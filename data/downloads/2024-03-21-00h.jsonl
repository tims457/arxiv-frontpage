{"created":"2024-03-19 17:59:56","title":"LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression","abstract":"This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.   We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.","sentences":["This paper focuses on task-agnostic prompt compression for better generalizability and efficiency.","Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.   ","To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset.","We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context.","Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.   ","We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH.","Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs.","Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x."],"url":"http://arxiv.org/abs/2403.12968v1","category":"cs.CL"}
{"created":"2024-03-19 17:59:52","title":"Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment","abstract":"This paper introduces a novel framework for virtual try-on, termed Wear-Any-Way. Different from previous methods, Wear-Any-Way is a customizable solution. Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style. To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios. To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations. With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style. For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc. Wear-Any-Way enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry.","sentences":["This paper introduces a novel framework for virtual try-on, termed Wear-Any-Way.","Different from previous methods, Wear-Any-Way is a customizable solution.","Besides generating high-fidelity results, our method supports users to precisely manipulate the wearing style.","To achieve this goal, we first construct a strong pipeline for standard virtual try-on, supporting single/multiple garment try-on and model-to-model settings in complicated scenarios.","To make it manipulable, we propose sparse correspondence alignment which involves point-based control to guide the generation for specific locations.","With this design, Wear-Any-Way gets state-of-the-art performance for the standard setting and provides a novel interaction form for customizing the wearing style.","For instance, it supports users to drag the sleeve to make it rolled up, drag the coat to make it open, and utilize clicks to control the style of tuck, etc.","Wear-Any-Way enables more liberated and flexible expressions of the attires, holding profound implications in the fashion industry."],"url":"http://arxiv.org/abs/2403.12965v1","category":"cs.CV"}
{"created":"2024-03-19 17:59:39","title":"Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models","abstract":"Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/DualAdapter.","sentences":["Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning.","In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't.","Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples.","In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks.","Our extensive experimental results across 15 datasets validate that the proposed DualAdapter outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency.","Code is available at https://github.com/zhangce01/DualAdapter."],"url":"http://arxiv.org/abs/2403.12964v1","category":"cs.CV"}
{"created":"2024-03-19 17:59:33","title":"FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis","abstract":"In this study, we delve into the generation of high-resolution images from pre-trained diffusion models, addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions. To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis. We replace the original convolutional layers in pre-trained diffusion models by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively. Further enhanced by a padding-then-crop strategy, our method can flexibly handle text-to-image generation of various aspect ratios. By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving an astonishing capacity of arbitrary-size, high-resolution, and high-quality generation. With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images. The code will be released at https://github.com/LeonHLJ/FouriScale.","sentences":["In this study, we delve into the generation of high-resolution images from pre-trained diffusion models, addressing persistent challenges, such as repetitive patterns and structural distortions, that emerge when models are applied beyond their trained resolutions.","To address this issue, we introduce an innovative, training-free approach FouriScale from the perspective of frequency domain analysis.","We replace the original convolutional layers in pre-trained diffusion models by incorporating a dilation technique along with a low-pass operation, intending to achieve structural consistency and scale consistency across resolutions, respectively.","Further enhanced by a padding-then-crop strategy, our method can flexibly handle text-to-image generation of various aspect ratios.","By using the FouriScale as guidance, our method successfully balances the structural integrity and fidelity of generated images, achieving an astonishing capacity of arbitrary-size, high-resolution, and high-quality generation.","With its simplicity and compatibility, our method can provide valuable insights for future explorations into the synthesis of ultra-high-resolution images.","The code will be released at https://github.com/LeonHLJ/FouriScale."],"url":"http://arxiv.org/abs/2403.12963v1","category":"cs.CV"}
{"created":"2024-03-19 17:59:09","title":"TexTile: A Differentiable Metric for Texture Tileability","abstract":"We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability). Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture. In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures. Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy. We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality. Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field.","sentences":["We introduce TexTile, a novel differentiable metric to quantify the degree upon which a texture image can be concatenated with itself without introducing repeating artifacts (i.e., the tileability).","Existing methods for tileable texture synthesis focus on general texture quality, but lack explicit analysis of the intrinsic repeatability properties of a texture.","In contrast, our TexTile metric effectively evaluates the tileable properties of a texture, opening the door to more informed synthesis and analysis of tileable textures.","Under the hood, TexTile is formulated as a binary classifier carefully built from a large dataset of textures of different styles, semantics, regularities, and human annotations.","Key to our method is a set of architectural modifications to baseline pre-train image classifiers to overcome their shortcomings at measuring tileability, along with a custom data augmentation and training regime aimed at increasing robustness and accuracy.","We demonstrate that TexTile can be plugged into different state-of-the-art texture synthesis methods, including diffusion-based strategies, and generate tileable textures while keeping or even improving the overall texture quality.","Furthermore, we show that TexTile can objectively evaluate any tileable texture synthesis method, whereas the current mix of existing metrics produces uncorrelated scores which heavily hinders progress in the field."],"url":"http://arxiv.org/abs/2403.12961v1","category":"cs.CV"}
{"created":"2024-03-19 17:58:04","title":"FaceXFormer: A Unified Transformer for Facial Analysis","abstract":"In this work, we introduce FaceXformer, an end-to-end unified transformer model for a comprehensive range of facial analysis tasks such as face parsing, landmark detection, head pose estimation, attributes recognition, and estimation of age, gender, race, and landmarks visibility. Conventional methods in face analysis have often relied on task-specific designs and preprocessing techniques, which limit their approach to a unified architecture. Unlike these conventional methods, our FaceXformer leverages a transformer-based encoder-decoder architecture where each task is treated as a learnable token, enabling the integration of multiple tasks within a single framework. Moreover, we propose a parameter-efficient decoder, FaceX, which jointly processes face and task tokens, thereby learning generalized and robust face representations across different tasks. To the best of our knowledge, this is the first work to propose a single model capable of handling all these facial analysis tasks using transformers. We conducted a comprehensive analysis of effective backbones for unified face task processing and evaluated different task queries and the synergy between them. We conduct experiments against state-of-the-art specialized models and previous multi-task models in both intra-dataset and cross-dataset evaluations across multiple benchmarks. Additionally, our model effectively handles images \"in-the-wild,\" demonstrating its robustness and generalizability across eight different tasks, all while maintaining the real-time performance of 37 FPS.","sentences":["In this work, we introduce FaceXformer, an end-to-end unified transformer model for a comprehensive range of facial analysis tasks such as face parsing, landmark detection, head pose estimation, attributes recognition, and estimation of age, gender, race, and landmarks visibility.","Conventional methods in face analysis have often relied on task-specific designs and preprocessing techniques, which limit their approach to a unified architecture.","Unlike these conventional methods, our FaceXformer leverages a transformer-based encoder-decoder architecture where each task is treated as a learnable token, enabling the integration of multiple tasks within a single framework.","Moreover, we propose a parameter-efficient decoder, FaceX, which jointly processes face and task tokens, thereby learning generalized and robust face representations across different tasks.","To the best of our knowledge, this is the first work to propose a single model capable of handling all these facial analysis tasks using transformers.","We conducted a comprehensive analysis of effective backbones for unified face task processing and evaluated different task queries and the synergy between them.","We conduct experiments against state-of-the-art specialized models and previous multi-task models in both intra-dataset and cross-dataset evaluations across multiple benchmarks.","Additionally, our model effectively handles images \"in-the-wild,\" demonstrating its robustness and generalizability across eight different tasks, all while maintaining the real-time performance of 37 FPS."],"url":"http://arxiv.org/abs/2403.12960v1","category":"cs.CV"}
{"created":"2024-03-19 17:58:02","title":"WHAC: World-grounded Humans and Cameras","abstract":"Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem. In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera. Our approach is founded on two key observations. Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth. Secondly, human motions inherently provide absolute spatial cues. By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques. Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories. Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework. We will make the code and dataset publicly available.","sentences":["Estimating human and camera trajectories with accurate scale in the world coordinate system from a monocular video is a highly desirable yet challenging and ill-posed problem.","In this study, we aim to recover expressive parametric human models (i.e., SMPL-X) and corresponding camera poses jointly, by leveraging the synergy between three critical players: the world, the human, and the camera.","Our approach is founded on two key observations.","Firstly, camera-frame SMPL-X estimation methods readily recover absolute human depth.","Secondly, human motions inherently provide absolute spatial cues.","By integrating these insights, we introduce a novel framework, referred to as WHAC, to facilitate world-grounded expressive human pose and shape estimation (EHPS) alongside camera pose estimation, without relying on traditional optimization techniques.","Additionally, we present a new synthetic dataset, WHAC-A-Mole, which includes accurately annotated humans and cameras, and features diverse interactive human motions as well as realistic camera trajectories.","Extensive experiments on both standard and newly established benchmarks highlight the superiority and efficacy of our framework.","We will make the code and dataset publicly available."],"url":"http://arxiv.org/abs/2403.12959v1","category":"cs.CV"}
{"created":"2024-03-19 17:57:52","title":"GVGEN: Text-to-3D Generation with Volumetric Representation","abstract":"In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities. To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input. We propose two innovative techniques:(1) Structured Volumetric Representation. We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume. This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians. To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods. Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency.","sentences":["In recent years, 3D Gaussian splatting has emerged as a powerful technique for 3D reconstruction and generation, known for its fast and high-quality rendering capabilities.","To address these shortcomings, this paper introduces a novel diffusion-based framework, GVGEN, designed to efficiently generate 3D Gaussian representations from text input.","We propose two innovative techniques:(1) Structured Volumetric Representation.","We first arrange disorganized 3D Gaussian points as a structured form GaussianVolume.","This transformation allows the capture of intricate texture details within a volume composed of a fixed number of Gaussians.","To better optimize the representation of these details, we propose a unique pruning and densifying method named the Candidate Pool Strategy, enhancing detail fidelity through selective optimization.","(2) Coarse-to-fine Generation Pipeline.","To simplify the generation of GaussianVolume and empower the model to generate instances with detailed 3D geometry, we propose a coarse-to-fine pipeline.","It initially constructs a basic geometric structure, followed by the prediction of complete Gaussian attributes.","Our framework, GVGEN, demonstrates superior performance in qualitative and quantitative assessments compared to existing 3D generation methods.","Simultaneously, it maintains a fast generation speed ($\\sim$7 seconds), effectively striking a balance between quality and efficiency."],"url":"http://arxiv.org/abs/2403.12957v1","category":"cs.CV"}
{"created":"2024-03-19 17:57:19","title":"Abiogenesis: a possible quantum interpretation of the telepoietic conjecture","abstract":"In the research on the origin of life, topics that can be considered reasonably shared by the generality of researchers are initially identified. It is then shown that the application of these principles to the results obtained with the IdLE-IdLA mathematical model for the simulation of aggregative processes, leads to the conclusion that the primordial formation of self-replicating structures is difficult to reconcile with deterministic aggregative dynamics in the classical sense. Regardless of the extent to which the process itself is governed by chance or by aggregative codes written in the laws of chemistry, no conventional causality is likely. The model itself suggests only one possible way out, consistent with thermodynamics: the existence of information sets rushing into the system in a different way from the perceived time stream. The possibilities offered by quantum mechanics and its most recent interpretations are consequently investigated to try to interpret, at the level of particle physics, the suggestion of IdLE-IdLA model. The attempt leads to a mutual accreditation of macroscopic telepoiesis and a kind of quantum retrocausality. The result is a vision of the natural world in which the coexistence of causal and retrocausal dynamics is presented as a possible interpretative key of the whole complex of vital manifestations.","sentences":["In the research on the origin of life, topics that can be considered reasonably shared by the generality of researchers are initially identified.","It is then shown that the application of these principles to the results obtained with the IdLE-IdLA mathematical model for the simulation of aggregative processes, leads to the conclusion that the primordial formation of self-replicating structures is difficult to reconcile with deterministic aggregative dynamics in the classical sense.","Regardless of the extent to which the process itself is governed by chance or by aggregative codes written in the laws of chemistry, no conventional causality is likely.","The model itself suggests only one possible way out, consistent with thermodynamics: the existence of information sets rushing into the system in a different way from the perceived time stream.","The possibilities offered by quantum mechanics and its most recent interpretations are consequently investigated to try to interpret, at the level of particle physics, the suggestion of IdLE-IdLA model.","The attempt leads to a mutual accreditation of macroscopic telepoiesis and a kind of quantum retrocausality.","The result is a vision of the natural world in which the coexistence of causal and retrocausal dynamics is presented as a possible interpretative key of the whole complex of vital manifestations."],"url":"http://arxiv.org/abs/2403.12955v1","category":"physics.bio-ph"}
{"created":"2024-03-19 17:54:34","title":"Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models","abstract":"Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements.","sentences":["Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting.","Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments.","To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs.","Our method is based on the notion of modulating per-class prototypes in the shared embedding space.","By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering.","At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy.","A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods.","Extensive evaluations across 15 datasets involving natural distribution shifts and cross-dataset generalization demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements."],"url":"http://arxiv.org/abs/2403.12952v1","category":"cs.CV"}
{"created":"2024-03-19 17:52:34","title":"Legendrian loops and cluster modular groups","abstract":"This work studies Legendrian loop actions on exact Lagrangian fillings of Legendrian links in $(\\R^3, \\xi_{\\st})$. By identifying the induced action of Legendrian loops as generators of cluster modular groups, we establish the existence of faithful group actions on the exact Lagrangian fillings of several families of Legendrian positive braid closures, including all positive torus links. In addition, we leverage a Nielsen-Thurston-like classification of cluster automorphisms to provide new combinatorial and algebraic tools for proving that a Legendrian loop action has infinite order.","sentences":["This work studies Legendrian loop actions on exact Lagrangian fillings of Legendrian links in $(\\R^3, \\xi_{\\st})$. By identifying the induced action of Legendrian loops as generators of cluster modular groups, we establish the existence of faithful group actions on the exact Lagrangian fillings of several families of Legendrian positive braid closures, including all positive torus links.","In addition, we leverage a Nielsen-Thurston-like classification of cluster automorphisms to provide new combinatorial and algebraic tools for proving that a Legendrian loop action has infinite order."],"url":"http://arxiv.org/abs/2403.12951v1","category":"math.SG"}
{"created":"2024-03-19 17:50:55","title":"Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion","abstract":"In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable. This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task. Such a generalization was not previously known and is likely to be of independent interest.","sentences":["In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm.","The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023).","The goal is to design algorithms without foreknowledge of the amount of change.   ","The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times.","Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention.","In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   ","Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable.","This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task.","Such a generalization was not previously known and is likely to be of independent interest."],"url":"http://arxiv.org/abs/2403.12950v1","category":"cs.LG"}
{"created":"2024-03-19 17:48:38","title":"DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset","abstract":"The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.","sentences":["The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies.","However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour.","As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity.","In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months.","We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability.","We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup."],"url":"http://arxiv.org/abs/2403.12945v1","category":"cs.RO"}
{"created":"2024-03-19 17:47:37","title":"Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers","abstract":"While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations. We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos. Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications. Project website: vid2robot.github.io","sentences":["While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans?","This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment.","We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots.","Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions.","This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory.","The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task.","To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment between human and robot video representations.","We evaluate Vid2Robot on real-world robots, demonstrating a 20% improvement in performance compared to other video-conditioned policies when using human demonstration videos.","Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions from one object to another, and long-horizon composition, thus showcasing its potential for real-world applications.","Project website: vid2robot.github.io"],"url":"http://arxiv.org/abs/2403.12943v1","category":"cs.RO"}
{"created":"2024-03-19 17:46:39","title":"Hidden Zeros in Scaffolded General Relativity and Exceptional Field Theories","abstract":"It was recently discovered by Arkani-Hamed et al that the colour-ordered scattering amplitudes of Tr$(\\Phi^3)$, the NLSM and scaffolded YM vanish at specific loci. We build on this observation and demonstrate that, beyond colour ordering, scattering amplitudes can display multiple instances of these hidden zeros. A first example are the flavour-ordered amplitudes of scaffolded General Relativity, corresponding to graviton exchange. The other two cases are multi-flavour DBI and the special Galileon. These are the natural generalisations of the NLSM with multiple instances of Adler's zero. The zero loci naturally define factorisation limits; in all three cases these include a metric-coupled cubic scalar theory. We comment on the general picture of hidden zeros and the relation to the double copy.","sentences":["It was recently discovered by Arkani-Hamed et al that the colour-ordered scattering amplitudes of Tr$(\\Phi^3)$, the NLSM and scaffolded YM vanish at specific loci.","We build on this observation and demonstrate that, beyond colour ordering, scattering amplitudes can display multiple instances of these hidden zeros.","A first example are the flavour-ordered amplitudes of scaffolded General Relativity, corresponding to graviton exchange.","The other two cases are multi-flavour DBI and the special Galileon.","These are the natural generalisations of the NLSM with multiple instances of Adler's zero.","The zero loci naturally define factorisation limits; in all three cases these include a metric-coupled cubic scalar theory.","We comment on the general picture of hidden zeros and the relation to the double copy."],"url":"http://arxiv.org/abs/2403.12939v1","category":"hep-th"}
{"created":"2024-03-19 17:43:08","title":"Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models","abstract":"Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions. The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public. With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient. This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases. We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data. Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision. The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes. Through our analysis, we demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information extraction, highlighting the potential of LLMs in revolutionising the way legal information is processed and utilised, offering significant implications for legal research and practice.","sentences":["Court transcripts and judgments are rich repositories of legal knowledge, detailing the intricacies of cases and the rationale behind judicial decisions.","The extraction of key information from these documents provides a concise overview of a case, crucial for both legal experts and the public.","With the advent of large language models (LLMs), automatic information extraction has become increasingly feasible and efficient.","This paper presents a comprehensive study on the application of GPT-4, a large language model, for automatic information extraction from UK Employment Tribunal (UKET) cases.","We meticulously evaluated GPT-4's performance in extracting critical information with a manual verification process to ensure the accuracy and relevance of the extracted data.","Our research is structured around two primary extraction tasks: the first involves a general extraction of eight key aspects that hold significance for both legal specialists and the general public, including the facts of the case, the claims made, references to legal statutes, references to precedents, general case outcomes and corresponding labels, detailed order and remedies and reasons for the decision.","The second task is more focused, aimed at analysing three of those extracted features, namely facts, claims and outcomes, in order to facilitate the development of a tool capable of predicting the outcome of employment law disputes.","Through our analysis, we demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information extraction, highlighting the potential of LLMs in revolutionising the way legal information is processed and utilised, offering significant implications for legal research and practice."],"url":"http://arxiv.org/abs/2403.12936v1","category":"cs.CL"}
{"created":"2024-03-19 17:37:18","title":"Segment Anything for comprehensive analysis of grapevine cluster architecture and berry properties","abstract":"Grape cluster architecture and compactness are complex traits influencing disease susceptibility, fruit quality, and yield. Evaluation methods for these traits include visual scoring, manual methodologies, and computer vision, with the latter being the most scalable approach. Most of the existing computer vision approaches for processing cluster images often rely on conventional segmentation or machine learning with extensive training and limited generalization. The Segment Anything Model (SAM), a novel foundation model trained on a massive image dataset, enables automated object segmentation without additional training. This study demonstrates out-of-the-box SAM's high accuracy in identifying individual berries in 2D cluster images. Using this model, we managed to segment approximately 3,500 cluster images, generating over 150,000 berry masks, each linked with spatial coordinates within their clusters. The correlation between human-identified berries and SAM predictions was very strong (Pearson r2=0.96). Although the visible berry count in images typically underestimates the actual cluster berry count due to visibility issues, we demonstrated that this discrepancy could be adjusted using a linear regression model (adjusted R2=0.87). We emphasized the critical importance of the angle at which the cluster is imaged, noting its substantial effect on berry counts and architecture. We proposed different approaches in which berry location information facilitated the calculation of complex features related to cluster architecture and compactness. Finally, we discussed SAM's potential integration into currently available pipelines for image generation and processing in vineyard conditions.","sentences":["Grape cluster architecture and compactness are complex traits influencing disease susceptibility, fruit quality, and yield.","Evaluation methods for these traits include visual scoring, manual methodologies, and computer vision, with the latter being the most scalable approach.","Most of the existing computer vision approaches for processing cluster images often rely on conventional segmentation or machine learning with extensive training and limited generalization.","The Segment Anything Model (SAM), a novel foundation model trained on a massive image dataset, enables automated object segmentation without additional training.","This study demonstrates out-of-the-box SAM's high accuracy in identifying individual berries in 2D cluster images.","Using this model, we managed to segment approximately 3,500 cluster images, generating over 150,000 berry masks, each linked with spatial coordinates within their clusters.","The correlation between human-identified berries and SAM predictions was very strong (Pearson r2=0.96).","Although the visible berry count in images typically underestimates the actual cluster berry count due to visibility issues, we demonstrated that this discrepancy could be adjusted using a linear regression model (adjusted R2=0.87).","We emphasized the critical importance of the angle at which the cluster is imaged, noting its substantial effect on berry counts and architecture.","We proposed different approaches in which berry location information facilitated the calculation of complex features related to cluster architecture and compactness.","Finally, we discussed SAM's potential integration into currently available pipelines for image generation and processing in vineyard conditions."],"url":"http://arxiv.org/abs/2403.12935v1","category":"cs.CV"}
{"created":"2024-03-19 17:36:28","title":"Zero-Reference Low-Light Enhancement via Physical Quadruple Priors","abstract":"Understanding illumination and reducing the need for supervision pose a significant challenge in low-light enhancement. Current approaches are highly sensitive to data usage during training and illumination-specific hyper-parameters, limiting their ability to handle unseen scenarios. In this paper, we propose a new zero-reference low-light enhancement framework trainable solely with normal light images. To accomplish this, we devise an illumination-invariant prior inspired by the theory of physical light transfer. This prior serves as the bridge between normal and low-light images. Then, we develop a prior-to-image framework trained without low-light data. During testing, this framework is able to restore our illumination-invariant prior back to images, automatically achieving low-light enhancement. Within this framework, we leverage a pretrained generative diffusion model for model ability, introduce a bypass decoder to handle detail distortion, as well as offer a lightweight version for practicality. Extensive experiments demonstrate our framework's superiority in various scenarios as well as good interpretability, robustness, and efficiency. Code is available on our project homepage: http://daooshee.github.io/QuadPrior-Website/","sentences":["Understanding illumination and reducing the need for supervision pose a significant challenge in low-light enhancement.","Current approaches are highly sensitive to data usage during training and illumination-specific hyper-parameters, limiting their ability to handle unseen scenarios.","In this paper, we propose a new zero-reference low-light enhancement framework trainable solely with normal light images.","To accomplish this, we devise an illumination-invariant prior inspired by the theory of physical light transfer.","This prior serves as the bridge between normal and low-light images.","Then, we develop a prior-to-image framework trained without low-light data.","During testing, this framework is able to restore our illumination-invariant prior back to images, automatically achieving low-light enhancement.","Within this framework, we leverage a pretrained generative diffusion model for model ability, introduce a bypass decoder to handle detail distortion, as well as offer a lightweight version for practicality.","Extensive experiments demonstrate our framework's superiority in various scenarios as well as good interpretability, robustness, and efficiency.","Code is available on our project homepage: http://daooshee.github.io/QuadPrior-Website/"],"url":"http://arxiv.org/abs/2403.12933v1","category":"cs.CV"}
{"created":"2024-03-19 17:34:27","title":"You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs","abstract":"We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis. This is achieved by integrating the diffusion process with GANs. Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning. In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training. Our code is provided at https://github.com/Luo-Yihong/YOSO.","sentences":["We introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis.","This is achieved by integrating the diffusion process with GANs.","Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning.","We show that our method can serve as a one-step generation model training from scratch with competitive performance.","Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning.","In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training.","Our code is provided at https://github.com/Luo-Yihong/YOSO."],"url":"http://arxiv.org/abs/2403.12931v1","category":"cs.CV"}
{"created":"2024-03-19 17:32:15","title":"Observation of the anomalous Nernst effect in altermagnetic candidate Mn5Si3","abstract":"The anomalous Nernst effect generates transverse voltage to the applied thermal gradient in magnetically ordered systems. The effect was previously considered excluded in compensated magnetic materials with collinear ordering. However, in the recently identified class of compensated magnetic materials, dubbed altermagnets, time-reversal symmetry breaking in the electronic band structure makes the presence of the anomalous Nernst effect possible despite the collinear spin arrangement. In this work, we investigate epitaxial Mn5Si3 thin films known to be an altermagnetic candidate. We show that the material manifests a sizable anomalous Nernst coefficient despite the small net magnetization of the films. The measured magnitudes of the anomalous Nernst coefficient reach a scale of microVolts per Kelvin. We support our magneto-thermoelectric measurements by density-functional theory calculations of the material's spin-split electronic structure, which allows for the finite Berry curvature in the reciprocal space. Furthermore, we present our calculations of the intrinsic Berry-curvature Nernst conductivity, which agree with our experimental observations.","sentences":["The anomalous Nernst effect generates transverse voltage to the applied thermal gradient in magnetically ordered systems.","The effect was previously considered excluded in compensated magnetic materials with collinear ordering.","However, in the recently identified class of compensated magnetic materials, dubbed altermagnets, time-reversal symmetry breaking in the electronic band structure makes the presence of the anomalous Nernst effect possible despite the collinear spin arrangement.","In this work, we investigate epitaxial Mn5Si3 thin films known to be an altermagnetic candidate.","We show that the material manifests a sizable anomalous Nernst coefficient despite the small net magnetization of the films.","The measured magnitudes of the anomalous Nernst coefficient reach a scale of microVolts per Kelvin.","We support our magneto-thermoelectric measurements by density-functional theory calculations of the material's spin-split electronic structure, which allows for the finite Berry curvature in the reciprocal space.","Furthermore, we present our calculations of the intrinsic Berry-curvature Nernst conductivity, which agree with our experimental observations."],"url":"http://arxiv.org/abs/2403.12929v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 17:32:01","title":"Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models","abstract":"Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content. This lends to it a quality of creativity which can be empowering in the early stages of design. In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas. Participants typically prompted in a straightforward manner with concise instructions. We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users. Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM.","sentences":["Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.","This lends to it a quality of creativity which can be empowering in the early stages of design.","In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them.","We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.","Participants typically prompted in a straightforward manner with concise instructions.","We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.","Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM."],"url":"http://arxiv.org/abs/2403.12928v1","category":"cs.HC"}
{"created":"2024-03-19 17:27:55","title":"Contextual AD Narration with Interleaved Multimodal Sequence","abstract":"The Audio Description (AD) task aims to generate descriptions of visual elements for visually impaired individuals to help them access long-form video contents, like movie. With video feature, text, character bank and context information as inputs, the generated ADs are able to correspond to the characters by name and provide reasonable, contextual descriptions to help audience understand the storyline of movie. To achieve this goal, we propose to leverage pre-trained foundation models through a simple and unified framework to generate ADs with interleaved multimodal sequence as input, termed as Uni-AD. To enhance the alignment of features across various modalities with finer granularity, we introduce a simple and lightweight module that maps video features into the textual feature space. Moreover, we also propose a character-refinement module to provide more precise information by identifying the main characters who play more significant role in the video context. With these unique designs, we further incorporate contextual information and a contrastive loss into our architecture to generate more smooth and contextual ADs. Experiments on the MAD-eval dataset show that Uni-AD can achieve state-of-the-art performance on AD generation, which demonstrates the effectiveness of our approach. Code will be available at https://github.com/MCG-NJU/Uni-AD.","sentences":["The Audio Description (AD) task aims to generate descriptions of visual elements for visually impaired individuals to help them access long-form video contents, like movie.","With video feature, text, character bank and context information as inputs, the generated ADs are able to correspond to the characters by name and provide reasonable, contextual descriptions to help audience understand the storyline of movie.","To achieve this goal, we propose to leverage pre-trained foundation models through a simple and unified framework to generate ADs with interleaved multimodal sequence as input, termed as Uni-AD.","To enhance the alignment of features across various modalities with finer granularity, we introduce a simple and lightweight module that maps video features into the textual feature space.","Moreover, we also propose a character-refinement module to provide more precise information by identifying the main characters who play more significant role in the video context.","With these unique designs, we further incorporate contextual information and a contrastive loss into our architecture to generate more smooth and contextual ADs.","Experiments on the MAD-eval dataset show that Uni-AD can achieve state-of-the-art performance on AD generation, which demonstrates the effectiveness of our approach.","Code will be available at https://github.com/MCG-NJU/Uni-AD."],"url":"http://arxiv.org/abs/2403.12922v1","category":"cs.CV"}
{"created":"2024-03-19 17:23:44","title":"Semantic Layering in Room Segmentation via LLMs","abstract":"In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation. Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation. By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation. Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies. The effectiveness of SeLRoS is verified through its application across 30 different 3D environments. Source code and experiment videos for this work are available at: https://sites.google.com/view/selros.","sentences":["In this paper, we introduce Semantic Layering in Room Segmentation via LLMs (SeLRoS), an advanced method for semantic room segmentation by integrating Large Language Models (LLMs) with traditional 2D map-based segmentation.","Unlike previous approaches that solely focus on the geometric segmentation of indoor environments, our work enriches segmented maps with semantic data, including object identification and spatial relationships, to enhance robotic navigation.","By leveraging LLMs, we provide a novel framework that interprets and organizes complex information about each segmented area, thereby improving the accuracy and contextual relevance of room segmentation.","Furthermore, SeLRoS overcomes the limitations of existing algorithms by using a semantic evaluation method to accurately distinguish true room divisions from those erroneously generated by furniture and segmentation inaccuracies.","The effectiveness of SeLRoS is verified through its application across 30 different 3D environments.","Source code and experiment videos for this work are available at: https://sites.google.com/view/selros."],"url":"http://arxiv.org/abs/2403.12920v1","category":"cs.RO"}
{"created":"2024-03-19 17:23:14","title":"Generalized Ramsey--Tur\u00e1n density for cliques","abstract":"We study the generalized Ramsey--Tur\\'an function $\\mathrm{RT}(n,K_s,K_t,o(n))$, which is the maximum possible number of copies of $K_s$ in an $n$-vertex $K_t$-free graph with independence number $o(n)$. The case when $s=2$ was settled by Erd{\\H{o}}s, S{\\'o}s, Bollob{\\'a}s, Hajnal, and Szemer\\'{e}di in the 1980s. We combinatorially resolve the general case for all $s\\ge 3$, showing that the (asymptotic) extremal graphs for this problem have simple (bounded) structures. In particular, it implies that the extremal structures follow a periodic pattern when $t$ is much larger than $s$. Our results disprove a conjecture of Balogh, Liu, and Sharifzadeh and show that a relaxed version does hold.","sentences":["We study the generalized Ramsey--Tur\\'an function $\\mathrm{RT}(n,K_s,K_t,o(n))$, which is the maximum possible number of copies of $K_s$ in an $n$-vertex $K_t$-free graph with independence number $o(n)$. The case when $s=2$ was settled by Erd{\\H{o}}s, S{\\'o}s, Bollob{\\'a}s, Hajnal, and Szemer\\'{e}di in the 1980s.","We combinatorially resolve the general case for all $s\\ge 3$, showing that the (asymptotic) extremal graphs for this problem have simple (bounded) structures.","In particular, it implies that the extremal structures follow a periodic pattern when $t$ is much larger than $s$. Our results disprove a conjecture of Balogh, Liu, and Sharifzadeh and show that a relaxed version does hold."],"url":"http://arxiv.org/abs/2403.12919v1","category":"math.CO"}
{"created":"2024-03-19 17:21:29","title":"Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts","abstract":"Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets.","sentences":["Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting.","Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights.","However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions.","To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs.","Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection.","Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting.","We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets."],"url":"http://arxiv.org/abs/2403.12918v1","category":"cs.CL"}
{"created":"2024-03-19 17:16:34","title":"Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts","abstract":"With fact-checking by professionals being difficult to scale on social media, algorithmic techniques have been considered. However, it is uncertain how the public may react to labels by automated fact-checkers. In this study, we investigate the use of automated warning labels derived from misinformation detection literature and investigate their effects on three forms of post engagement. Focusing on political posts, we also consider how partisanship affects engagement. In a two-phases within-subjects experiment with 200 participants, we found that the generic warnings suppressed intents to comment on and share posts, but not on the intent to like them. Furthermore, when different reasons for the labels were provided, their effects on post engagement were inconsistent, suggesting that the reasons could have undesirably motivated engagement instead. Partisanship effects were observed across the labels with higher engagement for politically congruent posts. We discuss the implications on the design and use of automated warning labels.","sentences":["With fact-checking by professionals being difficult to scale on social media, algorithmic techniques have been considered.","However, it is uncertain how the public may react to labels by automated fact-checkers.","In this study, we investigate the use of automated warning labels derived from misinformation detection literature and investigate their effects on three forms of post engagement.","Focusing on political posts, we also consider how partisanship affects engagement.","In a two-phases within-subjects experiment with 200 participants, we found that the generic warnings suppressed intents to comment on and share posts, but not on the intent to like them.","Furthermore, when different reasons for the labels were provided, their effects on post engagement were inconsistent, suggesting that the reasons could have undesirably motivated engagement instead.","Partisanship effects were observed across the labels with higher engagement for politically congruent posts.","We discuss the implications on the design and use of automated warning labels."],"url":"http://arxiv.org/abs/2403.12916v1","category":"cs.HC"}
{"created":"2024-03-19 17:12:58","title":"Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model","abstract":"We introduce the Pyramid Diffusion Model (PDM), a novel architecture designed for ultra-high-resolution image synthesis. PDM utilizes a pyramid latent representation, providing a broader design space that enables more flexible, structured, and efficient perceptual compression which enable AutoEncoder and Network of Diffusion to equip branches and deeper layers. To enhance PDM's capabilities for generative tasks, we propose the integration of Spatial-Channel Attention and Res-Skip Connection, along with the utilization of Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network and AutoEncoder. In summary, PDM achieves the synthesis of images with a 2K resolution for the first time, demonstrated on two new datasets comprising images of sizes 2048x2048 pixels and 2048x1024 pixels respectively. We believe that this work offers an alternative approach to designing scalable image generative models, while also providing incremental reinforcement for existing frameworks.","sentences":["We introduce the Pyramid Diffusion Model (PDM), a novel architecture designed for ultra-high-resolution image synthesis.","PDM utilizes a pyramid latent representation, providing a broader design space that enables more flexible, structured, and efficient perceptual compression which enable AutoEncoder and Network of Diffusion to equip branches and deeper layers.","To enhance PDM's capabilities for generative tasks, we propose the integration of Spatial-Channel Attention and Res-Skip Connection, along with the utilization of Spectral Norm and Decreasing Dropout Strategy for the Diffusion Network and AutoEncoder.","In summary, PDM achieves the synthesis of images with a 2K resolution for the first time, demonstrated on two new datasets comprising images of sizes 2048x2048 pixels and 2048x1024 pixels respectively.","We believe that this work offers an alternative approach to designing scalable image generative models, while also providing incremental reinforcement for existing frameworks."],"url":"http://arxiv.org/abs/2403.12915v1","category":"cs.CV"}
{"created":"2024-03-19 17:11:25","title":"Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers","abstract":"In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS). MIMS support both small peer-to-peer networks and large groups. Misinformation in the former may spread due to recipients' trust in the sender while in the latter, misinformation can directly reach a wide audience. The encryption of MIMS makes it difficult to address misinformation directly. As such, chatbots have become an alternative solution where users can disclose their chat content directly to fact checking services. To understand how effective fact checking chatbots are as an intervention and how trust in three different fact checkers (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents. We found mixed results for the fact checkers but support for the chatbot intervention overall. We also found a striking contradiction between participants' trust in the fact checkers and their behaviour towards them. Specifically, those who reported a high level of trust in the government performed worse and tended to follow the fact checking tool less when it was endorsed by the government.","sentences":["In Singapore, there has been a rise in misinformation on mobile instant messaging services (MIMS).","MIMS support both small peer-to-peer networks and large groups.","Misinformation in the former may spread due to recipients' trust in the sender while in the latter, misinformation can directly reach a wide audience.","The encryption of MIMS makes it difficult to address misinformation directly.","As such, chatbots have become an alternative solution where users can disclose their chat content directly to fact checking services.","To understand how effective fact checking chatbots are as an intervention and how trust in three different fact checkers (i.e., Government, News Outlets, and Artificial Intelligence) may affect this trust, we conducted a within-subjects experiment with 527 Singapore residents.","We found mixed results for the fact checkers but support for the chatbot intervention overall.","We also found a striking contradiction between participants' trust in the fact checkers and their behaviour towards them.","Specifically, those who reported a high level of trust in the government performed worse and tended to follow the fact checking tool less when it was endorsed by the government."],"url":"http://arxiv.org/abs/2403.12913v1","category":"cs.HC"}
{"created":"2024-03-19 17:08:24","title":"Yell At Your Robot: Improving On-the-Fly from Language Corrections","abstract":"Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.","sentences":["Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations.","However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.","Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback?","In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections.","We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions.","This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback.","Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.","Videos and code are available at https://yay-robot.github.io/."],"url":"http://arxiv.org/abs/2403.12910v1","category":"cs.RO"}
{"created":"2024-03-19 17:08:02","title":"Local reconstruction analysis of inverting the Radon transform in the plane from noisy discrete data","abstract":"In this paper, we investigate the reconstruction error, $N_\\e^{\\text{rec}}(x)$, when a linear, filtered back-projection (FBP) algorithm is applied to noisy, discrete Radon transform data with sampling step size $\\epsilon$ in two-dimensions. Specifically, we analyze $N_\\e^{\\text{rec}}(x)$ for $x$ in small, $O(\\e)$-sized neighborhoods around a generic fixed point, $x_0$, in the plane, where the measurement noise values, $\\eta_{k,j}$ (i.e., the errors in the sinogram space), are random variables. The latter are independent, but not necessarily identically distributed. We show, under suitable assumptions on the first three moments of the $\\eta_{k,j}$, that the following limit exists: $N^{\\text{rec}}(\\chx;x_0) = \\lim_{\\e\\to0}N_\\e^{\\text{rec}}(x_0+\\e\\chx)$, for $\\check x$ in a bounded domain. Here, $N_\\e^{\\text{rec}}$ and $ N^{\\text{rec}}$ are viewed as continuous random variables, and the limit is understood in the sense of distributions. Once the limit is established, we prove that $N^{\\text{rec}}$ is a zero mean Gaussian random field and compute explicitly its covariance. In addition, we validate our theory using numerical simulations and pseudo random noise.","sentences":["In this paper, we investigate the reconstruction error, $N_\\e^{\\text{rec}}(x)$, when a linear, filtered back-projection (FBP) algorithm is applied to noisy, discrete Radon transform data with sampling step size $\\epsilon$ in two-dimensions.","Specifically, we analyze $N_\\e^{\\text{rec}}(x)$ for $x$ in small, $O(\\e)$-sized neighborhoods around a generic fixed point, $x_0$, in the plane, where the measurement noise values, $\\eta_{k,j}$ (i.e., the errors in the sinogram space), are random variables.","The latter are independent, but not necessarily identically distributed.","We show, under suitable assumptions on the first three moments of the $\\eta_{k,j}$, that the following limit exists: $N^{\\text{rec}}(\\chx;x_0)","= \\lim_{\\e\\to0}N_\\e^{\\text{rec}}(x_0+\\e\\chx)$, for $\\check x$ in a bounded domain.","Here, $N_\\e^{\\text{rec}}$ and $ N^{\\text{rec}}$ are viewed as continuous random variables, and the limit is understood in the sense of distributions.","Once the limit is established, we prove that $N^{\\text{rec}}$ is a zero mean Gaussian random field and compute explicitly its covariance.","In addition, we validate our theory using numerical simulations and pseudo random noise."],"url":"http://arxiv.org/abs/2403.12909v1","category":"math.NA"}
{"created":"2024-03-19 17:02:07","title":"TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation","abstract":"Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets. We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model. Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability. Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds. Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions.","sentences":["Texturing 3D humans with semantic UV maps remains a challenge due to the difficulty of acquiring reasonably unfolded UV.","Despite recent text-to-3D advancements in supervising multi-view renderings using large text-to-image (T2I) models, issues persist with generation speed, text consistency, and texture quality, resulting in data scarcity among existing datasets.","We present TexDreamer, the first zero-shot multimodal high-fidelity 3D human texture generation model.","Utilizing an efficient texture adaptation finetuning strategy, we adapt large T2I model to a semantic UV structure while preserving its original generalization capability.","Leveraging a novel feature translator module, the trained model is capable of generating high-fidelity 3D human textures from either text or image within seconds.","Furthermore, we introduce ArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024) 3D human texture dataset which contains 50k high-fidelity textures with text descriptions."],"url":"http://arxiv.org/abs/2403.12906v1","category":"cs.CV"}
{"created":"2024-03-19 16:53:53","title":"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference","abstract":"The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data. This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence.","sentences":["The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure.","This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services.","Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency.","Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes.","Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data.","This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence."],"url":"http://arxiv.org/abs/2403.12900v1","category":"cs.DC"}
{"created":"2024-03-19 16:52:44","title":"Comments Concerning a Hypothetical Mesoscopic Dark Dimension","abstract":"Motivated by string-theoretic swampland conjectures, the existence of a dark fifth dimension, whose size is roughly 1 -- 10 microns, has been proposed. A great deal of supporting evidence has been presented, and definitive experimental tests are likely to be carried out. The basic idea is that the four-dimensional spacetime that we observe lives on a brane that is localized in the dark dimension. This short note points out that there are two distinct ways to realize such a scenario in string theory/M-theory. In the one considered previously the dark dimension is topologically a circle and our observable 4d spacetime is confined to a brane that is localized in a GUT-scale region of the circle. An alternative possibility is that the dark dimension is a line interval with branes attached at each end. This option would imply the existence of a parallel 4d spacetime microns away from us!","sentences":["Motivated by string-theoretic swampland conjectures, the existence of a dark fifth dimension, whose size is roughly 1 -- 10 microns, has been proposed.","A great deal of supporting evidence has been presented, and definitive experimental tests are likely to be carried out.","The basic idea is that the four-dimensional spacetime that we observe lives on a brane that is localized in the dark dimension.","This short note points out that there are two distinct ways to realize such a scenario in string theory/M-theory.","In the one considered previously the dark dimension is topologically a circle and our observable 4d spacetime is confined to a brane that is localized in a GUT-scale region of the circle.","An alternative possibility is that the dark dimension is a line interval with branes attached at each end.","This option would imply the existence of a parallel 4d spacetime microns away from us!"],"url":"http://arxiv.org/abs/2403.12899v1","category":"hep-th"}
{"created":"2024-03-19 16:52:39","title":"Plane Hamiltonian Cycles in Convex Drawings","abstract":"A conjecture by Rafla from 1988 asserts that every simple drawing of the complete graph $K_n$ admits a plane Hamiltonian cycle. It turned out that already the existence of much simpler non-crossing substructures in such drawings is hard to prove. Recent progress was made by Aichholzer et al. and by Suk and Zeng who proved the existence of a plane path of length $\\Omega(\\log n / \\log \\log n)$ and of a plane matching of size $\\Omega(n^{1/2})$ in every simple drawing of $K_{n}$.   Instead of studying simpler substructures, we prove Rafla's conjecture for the subclass of convex drawings, the most general class in the convexity hierarchy introduced by Arroyo et al. Moreover, we show that every convex drawing of $K_n$ contains a plane Hamiltonian path between each pair of vertices (Hamiltonian connectivity) and a plane $k$-cycle for each $3 \\leq k \\leq n$ (pancyclicity), and present further results on maximal plane subdrawings.","sentences":["A conjecture by Rafla from 1988 asserts that every simple drawing of the complete graph $K_n$ admits a plane Hamiltonian cycle.","It turned out that already the existence of much simpler non-crossing substructures in such drawings is hard to prove.","Recent progress was made by Aichholzer et al.","and by Suk and Zeng who proved the existence of a plane path of length $\\Omega(\\log n / \\log \\log n)$ and of a plane matching of size $\\Omega(n^{1/2})$ in every simple drawing of $K_{n}$.   Instead of studying simpler substructures, we prove Rafla's conjecture for the subclass of convex drawings, the most general class in the convexity hierarchy introduced by Arroyo et al.","Moreover, we show that every convex drawing of $K_n$ contains a plane Hamiltonian path between each pair of vertices (Hamiltonian connectivity) and a plane $k$-cycle for each $3 \\leq k \\leq n$ (pancyclicity), and present further results on maximal plane subdrawings."],"url":"http://arxiv.org/abs/2403.12898v1","category":"cs.CG"}
{"created":"2024-03-19 16:52:20","title":"On black bounce spacetimes in non-linear electrodynamics","abstract":"One of the main issues in gravitation is the presence of singularities in the most common spacetime solutions of General Relativity, as the case of black holes. A way of constructing regular solutions that remove space-like singularities consist on implement a bounce on such spacetime, leading what is usually known as black bounce spacetimes. Such spacetimes are known to describe regular black holes or traversable wormholes. However, one of the main issues lies on reconstructing the appropriate source that leads to such a solution. In this paper, a reconstruction method is implemented to show that such types of metrics can be well accommodated in non-linear electrodynamics with the presence of a scalar field. Some of the most important black bounces solutions are reconstructed in this framework, both in 3+1 as in 2+1 dimensions. For the first time in the literature, these solutions have an electrically charged source of matter from non-linear electrodynamics.","sentences":["One of the main issues in gravitation is the presence of singularities in the most common spacetime solutions of General Relativity, as the case of black holes.","A way of constructing regular solutions that remove space-like singularities consist on implement a bounce on such spacetime, leading what is usually known as black bounce spacetimes.","Such spacetimes are known to describe regular black holes or traversable wormholes.","However, one of the main issues lies on reconstructing the appropriate source that leads to such a solution.","In this paper, a reconstruction method is implemented to show that such types of metrics can be well accommodated in non-linear electrodynamics with the presence of a scalar field.","Some of the most important black bounces solutions are reconstructed in this framework, both in 3+1 as in 2+1 dimensions.","For the first time in the literature, these solutions have an electrically charged source of matter from non-linear electrodynamics."],"url":"http://arxiv.org/abs/2403.12897v1","category":"gr-qc"}
{"created":"2024-03-19 16:50:11","title":"Quantum Onsager relations","abstract":"Using quantum information geometry, I derive quantum generalizations of the Onsager rate equations, which model the dynamics of a system near a steady state. By redefining the currents, I propose a version of the equations with a symmetric transport tensor without assuming any detailed balance. With a more conventional definition of the currents, I propose another version where the transport tensor may not be symmetric and detailed balance is a sufficient, though not necessary, condition for the symmetry of the tensor. The relations and relative merits of the two versions are discussed.","sentences":["Using quantum information geometry, I derive quantum generalizations of the Onsager rate equations, which model the dynamics of a system near a steady state.","By redefining the currents, I propose a version of the equations with a symmetric transport tensor without assuming any detailed balance.","With a more conventional definition of the currents, I propose another version where the transport tensor may not be symmetric and detailed balance is a sufficient, though not necessary, condition for the symmetry of the tensor.","The relations and relative merits of the two versions are discussed."],"url":"http://arxiv.org/abs/2403.12896v1","category":"quant-ph"}
{"created":"2024-03-19 16:48:40","title":"mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding","abstract":"Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs. Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently. Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning. Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain. Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.","sentences":["Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts.","Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images.","In this work, we emphasize the importance of structure information in Visual Document Understanding and propose the Unified Structure Learning to boost the performance of MLLMs.","Our Unified Structure Learning comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image.","To better encode structure information, we design a simple and effective vision-to-text module H-Reducer, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently.","Furthermore, by constructing structure-aware text sequences and multi-grained pairs of texts and bounding boxes for publicly available text-rich images, we build a comprehensive training set DocStruct4M to support structure learning.","Finally, we construct a small but high-quality reasoning tuning dataset DocReason25K to trigger the detailed explanation ability in the document domain.","Our model DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks.","Our codes, models, and datasets are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5."],"url":"http://arxiv.org/abs/2403.12895v1","category":"cs.CV"}
{"created":"2024-03-19 16:45:45","title":"Wideband Modeling and Beamforming for Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain. Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations. We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies. With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers. Finally, we provide simulation results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS.","sentences":["This work studies the wideband modeling and beamforming design of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes and goes beyond conventional RIS with diagonal phase shift matrices to achieve enhanced channel gain.","Specifically, we investigate the response of BD-RIS in wideband systems by going back to its hardware circuit realizations.","We propose a novel wideband model which has simple expressions while capturing the response variations of BD-RIS for signals with different frequencies.","With this wideband model, we propose a BD-RIS design algorithm for an orthogonal frequency division multiplexing system to maximize the average rate over all subcarriers.","Finally, we provide simulation results to evaluate the performance of the proposed design and show the importance of wideband modeling for BD-RIS."],"url":"http://arxiv.org/abs/2403.12893v1","category":"cs.IT"}
{"created":"2024-03-19 16:40:57","title":"Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across Varied Bowl Configurations and Food Types","abstract":"In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF). The goal is to acquire (i.e., scoop) food items from a bowl. However, achieving robust and adaptive food manipulation is particularly challenging. To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping. Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors. We validate the effectiveness of our approach by conducting experiments on a real robot. We also compare its performance with a baseline. The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric. Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food.","sentences":["In this study, we introduce a novel visual imitation network with a spatial attention module for robotic assisted feeding (RAF).","The goal is to acquire (i.e., scoop) food items from a bowl.","However, achieving robust and adaptive food manipulation is particularly challenging.","To deal with this, we propose a framework that integrates visual perception with imitation learning to enable the robot to handle diverse scenarios during scooping.","Our approach, named AVIL (adaptive visual imitation learning), exhibits adaptability and robustness across different bowl configurations in terms of material, size, and position, as well as diverse food types including granular, semi-solid, and liquid, even in the presence of distractors.","We validate the effectiveness of our approach by conducting experiments on a real robot.","We also compare its performance with a baseline.","The results demonstrate improvement over the baseline across all scenarios, with an enhancement of up to 2.5 times in terms of a success metric.","Notably, our model, trained solely on data from a transparent glass bowl containing granular cereals, showcases generalization ability when tested zero-shot on other bowl configurations with different types of food."],"url":"http://arxiv.org/abs/2403.12891v1","category":"cs.RO"}
{"created":"2024-03-19 16:33:26","title":"EmoVOCA: Speech-Driven Emotional 3D Talking Heads","abstract":"The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available.","sentences":["The domain of 3D talking head generation has witnessed significant progress in recent years.","A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions.","Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions.","In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences.","To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face.","Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature.","Our code and pre-trained model will be made available."],"url":"http://arxiv.org/abs/2403.12886v1","category":"cs.CV"}
{"created":"2024-03-19 16:31:30","title":"HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning","abstract":"Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities. Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning. HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner. The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop. This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness. Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.","sentences":["Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities.","Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures.","To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning.","HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner.","The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop.","This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness.","Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets."],"url":"http://arxiv.org/abs/2403.12884v1","category":"cs.CV"}
{"created":"2024-03-19 16:26:10","title":"Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models","abstract":"Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.","sentences":["Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents.","How to integrate agent ability into general LLMs becomes a crucial and urgent problem.","This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations.","Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.","Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets.","With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark.","Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs.","The code will be available at https://github.com/InternLM/Agent-FLAN."],"url":"http://arxiv.org/abs/2403.12881v1","category":"cs.CL"}
{"created":"2024-03-19 16:24:54","title":"General properties of the electric Penrose process","abstract":"We consider the Penrose process with the charged particles in the Reissner-Nordstr\\\"{o}m background. Let parent particle 0 decay to particles 1 and 2. With the assumption that all three particles move in the same plane, the exact formulas for characteristics of particles 1 and 2 in terms of those of particle 0 are derived. We concentrate on scenarios in which particle 1 and 2 are ejected along the trajectory of particle 0. It is shown that such scenarios correspond to the extrema of energies $E_{1}$ or $E_{2}$ of daughter particles with respect to the angular momentum $L_{1}$ or $L_{2}$ of one of them. We derive bounds on the values of angular momenta $L_{1}$ and $L_{2}$. We give classification of these scenarios and discuss their properties including decay in the near-horizon region. The results are reformulated in terms of velocities of daughter particles in the center of mass frame. The approach is applicable also to collisional Penrose process, in which a combination of particles 1 and 2 is considered as one effective particle. If the mass of particle 0 $m_{0}\\rightarrow \\infty $, the situation corresponds to the Ba\\~{n}ados-Silk-West effect, the results agree with the ones known in literature before. In addition, we consider special cases when decay occurs in the turning point for one or all three particles.","sentences":["We consider the Penrose process with the charged particles in the Reissner-Nordstr\\\"{o}m background.","Let parent particle 0 decay to particles 1 and 2.","With the assumption that all three particles move in the same plane, the exact formulas for characteristics of particles 1 and 2 in terms of those of particle 0 are derived.","We concentrate on scenarios in which particle 1 and 2 are ejected along the trajectory of particle 0.","It is shown that such scenarios correspond to the extrema of energies $E_{1}$ or $E_{2}$ of daughter particles with respect to the angular momentum $L_{1}$ or $L_{2}$ of one of them.","We derive bounds on the values of angular momenta $L_{1}$ and $L_{2}$. We give classification of these scenarios and discuss their properties including decay in the near-horizon region.","The results are reformulated in terms of velocities of daughter particles in the center of mass frame.","The approach is applicable also to collisional Penrose process, in which a combination of particles 1 and 2 is considered as one effective particle.","If the mass of particle 0 $m_{0}\\rightarrow \\infty $, the situation corresponds to the Ba\\~{n}ados-Silk-West effect, the results agree with the ones known in literature before.","In addition, we consider special cases when decay occurs in the turning point for one or all three particles."],"url":"http://arxiv.org/abs/2403.12879v1","category":"gr-qc"}
{"created":"2024-03-19 16:21:40","title":"LAVA: Long-horizon Visual Action based Food Acquisition","abstract":"Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves. The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table. Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods. This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods. Long-horizon refers to the goal of \"clearing the bowl\" by sequentially acquiring the food from the bowl. LAVA employs a hierarchical policy for long-horizon food acquisition tasks. The framework uses high-level policy to determine primitives by leveraging ScoopNet. At the mid-level, LAVA finds parameters for primitives using vision. To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution. We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition. Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl. Code, datasets, videos, and supplementary materials can be found on our website.","sentences":["Robotic Assisted Feeding (RAF) addresses the fundamental need for individuals with mobility impairments to regain autonomy in feeding themselves.","The goal of RAF is to use a robot arm to acquire and transfer food to individuals from the table.","Existing RAF methods primarily focus on solid foods, leaving a gap in manipulation strategies for semi-solid and deformable foods.","This study introduces Long-horizon Visual Action (LAVA) based food acquisition of liquid, semisolid, and deformable foods.","Long-horizon refers to the goal of \"clearing the bowl\" by sequentially acquiring the food from the bowl.","LAVA employs a hierarchical policy for long-horizon food acquisition tasks.","The framework uses high-level policy to determine primitives by leveraging ScoopNet.","At the mid-level, LAVA finds parameters for primitives using vision.","To carry out sequential plans in the real world, LAVA delegates action execution which is driven by Low-level policy that uses parameters received from mid-level policy and behavior cloning ensuring precise trajectory execution.","We validate our approach on complex real-world acquisition trials involving granular, liquid, semisolid, and deformable food types along with fruit chunks and soup acquisition.","Across 46 bowls, LAVA acquires much more efficiently than baselines with a success rate of 89 +/- 4% and generalizes across realistic plate variations such as different positions, varieties, and amount of food in the bowl.","Code, datasets, videos, and supplementary materials can be found on our website."],"url":"http://arxiv.org/abs/2403.12876v1","category":"cs.RO"}
{"created":"2024-03-19 16:12:25","title":"Regularization in Spider-Style Strategy Discovery and Schedule Construction","abstract":"To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem. In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider. We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property.","sentences":["To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem.","In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider.","We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property."],"url":"http://arxiv.org/abs/2403.12869v1","category":"cs.AI"}
{"created":"2024-03-19 16:09:30","title":"PE-Planner: A Performance-Enhanced Quadrotor Motion Planner for Autonomous Flight in Complex and Dynamic Environments","abstract":"The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight. In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances. The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization. Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control. Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner. Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario.","sentences":["The role of a motion planner is pivotal in quadrotor applications, yet existing methods often struggle to adapt to complex environments, limiting their ability to achieve fast, safe, and robust flight.","In this letter, we introduce a performance-enhanced quadrotor motion planner designed for autonomous flight in complex environments including dense obstacles, dynamic obstacles, and unknown disturbances.","The global planner generates an initial trajectory through kinodynamic path searching and refines it using B-spline trajectory optimization.","Subsequently, the local planner takes into account the quadrotor dynamics, estimated disturbance, global reference trajectory, control cost, time cost, and safety constraints to generate real-time control inputs, utilizing the framework of model predictive contouring control.","Both simulations and real-world experiments corroborate the heightened robustness, safety, and speed of the proposed motion planner.","Additionally, our motion planner achieves flights at more than 6.8 m/s in a challenging and complex racing scenario."],"url":"http://arxiv.org/abs/2403.12865v1","category":"cs.RO"}
{"created":"2024-03-19 16:06:10","title":"Epistemology of Language Models: Do Language Models Have Holistic Knowledge?","abstract":"This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.","sentences":["This paper investigates the inherent knowledge in language models from the perspective of epistemological holism.","The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism.","These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise.","To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation.","In the abduction task, the language models explained situations while avoiding revising the core knowledge.","However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles."],"url":"http://arxiv.org/abs/2403.12862v1","category":"cs.CL"}
{"created":"2024-03-19 16:04:48","title":"Mutually orthogoval projective and affine spaces","abstract":"A recent paper showed how to find sets of finite affine or projective planes constructed on a common set of points, so that lines of one plane meet lines of a different plane in at most two points. In this paper, those results are generalized in two different ways to spaces of higher dimension. The simpler of the two generalizations admits many solutions, both affine and projective. For the stronger definition, where a line of one space must be an arc in the other, we show the existence of pairs of projective spaces of dimension one less than a prime.","sentences":["A recent paper showed how to find sets of finite affine or projective planes constructed on a common set of points, so that lines of one plane meet lines of a different plane in at most two points.","In this paper, those results are generalized in two different ways to spaces of higher dimension.","The simpler of the two generalizations admits many solutions, both affine and projective.","For the stronger definition, where a line of one space must be an arc in the other, we show the existence of pairs of projective spaces of dimension one less than a prime."],"url":"http://arxiv.org/abs/2403.12860v1","category":"math.CO"}
{"created":"2024-03-19 16:02:35","title":"Average circuit eigenvalue sampling on NISQ devices","abstract":"Average circuit eigenvalue sampling (ACES) was introduced by Flammia in arXiv:2108.05803 as a protocol to characterize the Pauli error channels of individual gates across the device simultaneously. The original paper posed using ACES to characterize near-term devices as an open problem. This work advances in this direction by presenting a full implementation of ACES for real devices and deploying it to Superstaq arXiv:2309.05157, along with a device-tailored resource estimation obtained through simulations and experiments. Our simulations show that ACES is able to estimate one- and two-qubit non-uniform Pauli error channels to an average eigenvalue absolute error of under $0.003$ and total variation distance of under 0.001 between simulated and reconstructed probability distributions over Pauli errors with $10^5$ shots per circuit using 5 circuits of depth 14. The question of estimating general error channels through twirling techniques in real devices remains open, as it is dependent on a device's native gates, but simulations with the Clifford set show results in agreement with reported hardware data. Experimental results on IBM's Algiers and Osaka devices are presented, where we characterize their error channels as Pauli channels without twirling.","sentences":["Average circuit eigenvalue sampling (ACES) was introduced by Flammia in arXiv:2108.05803 as a protocol to characterize the Pauli error channels of individual gates across the device simultaneously.","The original paper posed using ACES to characterize near-term devices as an open problem.","This work advances in this direction by presenting a full implementation of ACES for real devices and deploying it to Superstaq arXiv:2309.05157, along with a device-tailored resource estimation obtained through simulations and experiments.","Our simulations show that ACES is able to estimate one-","and two-qubit non-uniform Pauli error channels to an average eigenvalue absolute error of under $0.003$ and total variation distance of under 0.001 between simulated and reconstructed probability distributions over Pauli errors with $10^5$ shots per circuit using 5 circuits of depth 14.","The question of estimating general error channels through twirling techniques in real devices remains open, as it is dependent on a device's native gates, but simulations with the Clifford set show results in agreement with reported hardware data.","Experimental results on IBM's Algiers and Osaka devices are presented, where we characterize their error channels as Pauli channels without twirling."],"url":"http://arxiv.org/abs/2403.12857v1","category":"quant-ph"}
{"created":"2024-03-19 15:57:32","title":"RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems","abstract":"Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings.","sentences":["Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise.","Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks.","RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform.","Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP.","We demonstrate that RASP can enable a diverse range of useful tasks in home, office, lab, and other indoor settings."],"url":"http://arxiv.org/abs/2403.12853v1","category":"cs.RO"}
{"created":"2024-03-19 15:57:04","title":"Generative Enhancement for 3D Medical Images","abstract":"The limited availability of 3D medical image datasets, due to privacy concerns and high collection or annotation costs, poses significant challenges in the field of medical imaging. While a promising alternative is the use of synthesized medical data, there are few solutions for realistic 3D medical image synthesis due to difficulties in backbone design and fewer 3D training samples compared to 2D counterparts. In this paper, we propose GEM-3D, a novel generative approach to the synthesis of 3D medical images and the enhancement of existing datasets using conditional diffusion models. Our method begins with a 2D slice, noted as the informed slice to serve the patient prior, and propagates the generation process using a 3D segmentation mask. By decomposing the 3D medical images into masks and patient prior information, GEM-3D offers a flexible yet effective solution for generating versatile 3D images from existing datasets. GEM-3D can enable dataset enhancement by combining informed slice selection and generation at random positions, along with editable mask volumes to introduce large variations in diffusion sampling. Moreover, as the informed slice contains patient-wise information, GEM-3D can also facilitate counterfactual image synthesis and dataset-level de-enhancement with desired control. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D is capable of synthesizing high-quality 3D medical images with volumetric consistency, offering a straightforward solution for dataset enhancement during inference. The code is available at https://github.com/HKU-MedAI/GEM-3D.","sentences":["The limited availability of 3D medical image datasets, due to privacy concerns and high collection or annotation costs, poses significant challenges in the field of medical imaging.","While a promising alternative is the use of synthesized medical data, there are few solutions for realistic 3D medical image synthesis due to difficulties in backbone design and fewer 3D training samples compared to 2D counterparts.","In this paper, we propose GEM-3D, a novel generative approach to the synthesis of 3D medical images and the enhancement of existing datasets using conditional diffusion models.","Our method begins with a 2D slice, noted as the informed slice to serve the patient prior, and propagates the generation process using a 3D segmentation mask.","By decomposing the 3D medical images into masks and patient prior information, GEM-3D offers a flexible yet effective solution for generating versatile 3D images from existing datasets.","GEM-3D can enable dataset enhancement by combining informed slice selection and generation at random positions, along with editable mask volumes to introduce large variations in diffusion sampling.","Moreover, as the informed slice contains patient-wise information, GEM-3D can also facilitate counterfactual image synthesis and dataset-level de-enhancement with desired control.","Experiments on brain MRI and abdomen CT images demonstrate that GEM-3D is capable of synthesizing high-quality 3D medical images with volumetric consistency, offering a straightforward solution for dataset enhancement during inference.","The code is available at https://github.com/HKU-MedAI/GEM-3D."],"url":"http://arxiv.org/abs/2403.12852v1","category":"eess.IV"}
{"created":"2024-03-19 15:54:48","title":"Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation","abstract":"Compositional 3D scene synthesis has diverse applications across a spectrum of industries such as robotics, films, and video games, as it closely mirrors the complexity of real-world multi-object environments. Early works typically employ shape retrieval based frameworks which naturally suffer from limited shape diversity. Recent progresses have been made in shape generation with powerful generative models, such as diffusion models, which increases the shape fidelity. However, these approaches separately treat 3D shape generation and layout generation. The synthesized scenes are usually hampered by layout collision, which implies that the scene-level fidelity is still under-explored. In this paper, we aim at generating realistic and reasonable 3D scenes from scene graph. To enrich the representation capability of the given scene graph inputs, large language model is utilized to explicitly aggregate the global graph features with local relationship features. With a unified graph convolution network (GCN), graph features are extracted from scene graphs updated via joint layout-shape distribution. During scene generation, an IoU-based regularization loss is introduced to constrain the predicted 3D layouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3D scene synthesis, especially in terms of scene-level fidelity. The source code will be released after publication.","sentences":["Compositional 3D scene synthesis has diverse applications across a spectrum of industries such as robotics, films, and video games, as it closely mirrors the complexity of real-world multi-object environments.","Early works typically employ shape retrieval based frameworks which naturally suffer from limited shape diversity.","Recent progresses have been made in shape generation with powerful generative models, such as diffusion models, which increases the shape fidelity.","However, these approaches separately treat 3D shape generation and layout generation.","The synthesized scenes are usually hampered by layout collision, which implies that the scene-level fidelity is still under-explored.","In this paper, we aim at generating realistic and reasonable 3D scenes from scene graph.","To enrich the representation capability of the given scene graph inputs, large language model is utilized to explicitly aggregate the global graph features with local relationship features.","With a unified graph convolution network (GCN), graph features are extracted from scene graphs updated via joint layout-shape distribution.","During scene generation, an IoU-based regularization loss is introduced to constrain the predicted 3D layouts.","Benchmarked on the SG-FRONT dataset, our method achieves better 3D scene synthesis, especially in terms of scene-level fidelity.","The source code will be released after publication."],"url":"http://arxiv.org/abs/2403.12848v1","category":"cs.CV"}
{"created":"2024-03-19 15:54:38","title":"Policy Bifurcation in Safe Reinforcement Learning","abstract":"Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifurcated policy, we propose a safe RL algorithm called multimodal policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output. The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient. Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy's capability of exploring different modes. Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations.","sentences":["Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems.","Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations.","We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple.","Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state.","To train such a bifurcated policy, we propose a safe RL algorithm called multimodal policy optimization (MUPO), which utilizes a Gaussian mixture distribution as the policy output.","The bifurcated behavior can be achieved by selecting the Gaussian component with the highest mixing coefficient.","Besides, MUPO also integrates spectral normalization and forward KL divergence to enhance the policy's capability of exploring different modes.","Experiments with vehicle control tasks show that our algorithm successfully learns the bifurcated policy and ensures satisfying safety, while a continuous policy suffers from inevitable constraint violations."],"url":"http://arxiv.org/abs/2403.12847v2","category":"cs.LG"}
{"created":"2024-03-19 15:51:29","title":"Giant electrode effect on tunneling magnetoresistance and electroresistance in van der Waals intrinsic multiferroic tunnel junctions","abstract":"Van der Waals multiferroic tunnel junctions (vdW-MFTJs) with multiple nonvolatile resistive states are highly suitable for new physics and next-generation storage electronics. However, currently reported vdW-MFTJs are based on two types of materials, i.e., vdW ferromagnetic and ferroelectric materials, forming a multiferroic system. This undoubtedly introduces additional interfaces, increasing the complexity of experimental preparation. Herein, we engineer vdW intrinsic MFTJs utilizing bilayer VS$_2$. By employing the nonequilibrium Green's function combined with density functional theory, we systematically investigate the influence of three types of electrodes (including non-vdW pure metal Ag/Au, vdW metallic 1T-MoS$_2$/2H-PtTe$_2$, and vdW ferromagnetic metallic Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$) on the electronic transport properties of VS$_2$-based intrinsic MFTJs. We demonstrate that these MFTJs manifest a giant electrode-dependent electronic transport characteristic effect. Comprehensively comparing these electrode pairs, the Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$ electrode combination exhibits optimal transport properties, the maximum TMR (TER) can reach 10949\\% (69\\%) and the minimum resistance-area product (RA) is 0.45 $\\Omega$$\\mu$m$^{2}$, as well as the perfect spin filtering and negative differential resistance effects. More intriguingly, TMR (TER) can be further enhanced to 34000\\% (380\\%) by applying an external bias voltage (0.1 V), while RA can be reduced to 0.16 $\\Omega$$\\mu$m$^{2}$ under the influence of biaxial stress (-3\\%). Our proposed concept of designing vdW-MFTJs using intrinsic multiferroic materials points towards new avenues in experimental exploration.","sentences":["Van der Waals multiferroic tunnel junctions (vdW-MFTJs) with multiple nonvolatile resistive states are highly suitable for new physics and next-generation storage electronics.","However, currently reported vdW-MFTJs are based on two types of materials, i.e., vdW ferromagnetic and ferroelectric materials, forming a multiferroic system.","This undoubtedly introduces additional interfaces, increasing the complexity of experimental preparation.","Herein, we engineer vdW intrinsic MFTJs utilizing bilayer VS$_2$. By employing the nonequilibrium Green's function combined with density functional theory, we systematically investigate the influence of three types of electrodes (including non-vdW pure metal Ag/Au, vdW metallic 1T-MoS$_2$/2H-PtTe$_2$, and vdW ferromagnetic metallic Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$) on the electronic transport properties of VS$_2$-based intrinsic MFTJs.","We demonstrate that these MFTJs manifest a giant electrode-dependent electronic transport characteristic effect.","Comprehensively comparing these electrode pairs, the Fe$_3$GaTe$_2$/Fe$_3$GeTe$_2$ electrode combination exhibits optimal transport properties, the maximum TMR (TER) can reach 10949\\% (69\\%) and the minimum resistance-area product (RA) is 0.45 $\\Omega$$\\mu$m$^{2}$, as well as the perfect spin filtering and negative differential resistance effects.","More intriguingly, TMR (TER) can be further enhanced to 34000\\% (380\\%) by applying an external bias voltage (0.1 V), while RA can be reduced to 0.16 $\\Omega$$\\mu$m$^{2}$ under the influence of biaxial stress (-3\\%).","Our proposed concept of designing vdW-MFTJs using intrinsic multiferroic materials points towards new avenues in experimental exploration."],"url":"http://arxiv.org/abs/2403.12845v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 15:51:21","title":"MELTing point: Mobile Evaluation of Language Transformers","abstract":"Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models. Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound. Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost. Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience. Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost. We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments.","sentences":["Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''.","However, their runtime requirements have prevented them from being broadly deployed on mobile.","As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs).","To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices.","We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   ","Our analysis is the first systematic study of on-device LLM execution, quantifying performance, energy efficiency and accuracy across various state-of-the-art models and showcases the state of on-device intelligence in the era of hyperscale models.","Results highlight the performance heterogeneity across targets and corroborates that LLM inference is largely memory-bound.","Quantization drastically reduces memory requirements and renders execution viable, but at a non-negligible accuracy cost.","Drawing from its energy footprint and thermal behavior, the continuous execution of LLMs remains elusive, as both factors negatively affect user experience.","Last, our experience shows that the ecosystem is still in its infancy, and algorithmic as well as hardware breakthroughs can significantly shift the execution cost.","We expect NPU acceleration, and framework-hardware co-design to be the biggest bet towards efficient standalone execution, with the alternative of offloading tailored towards edge deployments."],"url":"http://arxiv.org/abs/2403.12844v1","category":"cs.LG"}
{"created":"2024-03-19 15:46:18","title":"Optical properties of Euler-Heisenberg black hole in the Cold Dark Matter Halo","abstract":"The optical properties of Euler-Heisenberg (EH) black hole (BH) surrounded by Cold Dark Matter (CDM) halo are investigated. By changing BH's parameters, we found that the radius of horizon r_{h} and radius of photon sphere r_{ph} will transparently increase as CDM halo parameters R and \\rho increase. To show the influence of CDM halo on the BH's optical characteristics, we took two sets of R and \\rho with prominent differences and plot the first four orders of images for thin accretion disk with different angle of inclination \\theta of observer. The images with light intensity distributions using Novikov-Thorne (N-T) model are also derived, as well as the effective potential, photon orbits. Especially, analysis of intersection behaviors between photon trajectories with different impact parameters and circular time-like orbits in accretion disk will help better understand the image of thin accretion disk. Our results showed that CDM halo will make BH become more larger and dimmer distinctly.","sentences":["The optical properties of Euler-Heisenberg (EH) black hole (BH) surrounded by Cold Dark Matter (CDM) halo are investigated.","By changing BH's parameters, we found that the radius of horizon r_{h} and radius of photon sphere r_{ph} will transparently increase as CDM halo parameters R and \\rho increase.","To show the influence of CDM halo on the BH's optical characteristics, we took two sets of R and \\rho with prominent differences and plot the first four orders of images for thin accretion disk with different angle of inclination \\theta of observer.","The images with light intensity distributions using Novikov-Thorne (N-T) model are also derived, as well as the effective potential, photon orbits.","Especially, analysis of intersection behaviors between photon trajectories with different impact parameters and circular time-like orbits in accretion disk will help better understand the image of thin accretion disk.","Our results showed that CDM halo will make BH become more larger and dimmer distinctly."],"url":"http://arxiv.org/abs/2403.12840v1","category":"gr-qc"}
{"created":"2024-03-19 15:43:16","title":"How Spammers and Scammers Leverage AI-Generated Images on Facebook for Audience Growth","abstract":"Much of the research and discourse on risks from artificial intelligence (AI) image generators, such as DALL-E and Midjourney, has centered around whether they could be used to inject false information into political discourse. We show that spammers and scammers - seemingly motivated by profit or clout, not ideology - are already using AI-generated images to gain significant traction on Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated images to users who neither follow the Pages posting the images nor realize that the images are AI-generated, highlighting the need for improved transparency and provenance standards as AI models proliferate.","sentences":["Much of the research and discourse on risks from artificial intelligence (AI) image generators, such as DALL-E and Midjourney, has centered around whether they could be used to inject false information into political discourse.","We show that spammers and scammers - seemingly motivated by profit or clout, not ideology - are already using AI-generated images to gain significant traction on Facebook.","At times, the Facebook Feed is recommending unlabeled AI-generated images to users who neither follow the Pages posting the images nor realize that the images are AI-generated, highlighting the need for improved transparency and provenance standards as AI models proliferate."],"url":"http://arxiv.org/abs/2403.12838v1","category":"cs.CY"}
{"created":"2024-03-19 15:42:20","title":"Predicting Winning Lottery Numbers","abstract":"We use mathematical statistics theory to derive the Compound-Dirichlet-Multinomial (CDM) prediction model. We then use this model to predict winning numbers for the 6-number, 5-number, pick-4 and pick-3 lottery games. We also develop a strategy which we call the 3-strategy, for generating profit by predicting winning numbers for the pick-3 lottery game.","sentences":["We use mathematical statistics theory to derive the Compound-Dirichlet-Multinomial (CDM) prediction model.","We then use this model to predict winning numbers for the 6-number, 5-number, pick-4 and pick-3 lottery games.","We also develop a strategy which we call the 3-strategy, for generating profit by predicting winning numbers for the pick-3 lottery game."],"url":"http://arxiv.org/abs/2403.12836v1","category":"math.PR"}
{"created":"2024-03-19 15:41:39","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","abstract":"Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions. Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning. Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text. An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering. We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents.","sentences":["Traditional approaches in physics-based motion generation, centered around imitation learning and reward shaping, often struggle to adapt to new scenarios.","To tackle this limitation, we propose AnySkill, a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions.","Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning.","Upon receiving an open-vocabulary textual instruction, AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text.","An important feature of our method is the use of image-based rewards for the high-level policy, which allows the agent to learn interactions with objects without manual reward engineering.","We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths, marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents."],"url":"http://arxiv.org/abs/2403.12835v1","category":"cs.CV"}
{"created":"2024-03-19 15:41:16","title":"Embarrassingly Simple Scribble Supervision for 3D Medical Segmentation","abstract":"Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field. Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets. Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative. We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems. To address these issues, we propose a comprehensive scribble supervision benchmark consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities. We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels. Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations. Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance. Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision. Source code as well as our extensive scribble benchmarking suite will be made publicly available upon publication.","sentences":["Traditionally, segmentation algorithms require dense annotations for training, demanding significant annotation efforts, particularly within the 3D medical imaging field.","Scribble-supervised learning emerges as a possible solution to this challenge, promising a reduction in annotation efforts when creating large-scale datasets.","Recently, a plethora of methods for optimized learning from scribbles have been proposed, but have so far failed to position scribble annotation as a beneficial alternative.","We relate this shortcoming to two major issues: 1) the complex nature of many methods which deeply ties them to the underlying segmentation model, thus preventing a migration to more powerful state-of-the-art models as the field progresses and 2) the lack of a systematic evaluation to validate consistent performance across the broader medical domain, resulting in a lack of trust when applying these methods to new segmentation problems.","To address these issues, we propose a comprehensive scribble supervision benchmark consisting of seven datasets covering a diverse set of anatomies and pathologies imaged with varying modalities.","We furthermore propose the systematic use of partial losses, i.e. losses that are only computed on annotated voxels.","Contrary to most existing methods, these losses can be seamlessly integrated into state-of-the-art segmentation methods, enabling them to learn from scribble annotations while preserving their original loss formulations.","Our evaluation using nnU-Net reveals that while most existing methods suffer from a lack of generalization, the proposed approach consistently delivers state-of-the-art performance.","Thanks to its simplicity, our approach presents an embarrassingly simple yet effective solution to the challenges of scribble supervision.","Source code as well as our extensive scribble benchmarking suite will be made publicly available upon publication."],"url":"http://arxiv.org/abs/2403.12834v1","category":"cs.CV"}
{"created":"2024-03-19 15:39:17","title":"On high-dimensional classification by sparse generalized Bayesian logistic regression","abstract":"This work addresses the problem of high-dimensional classification by exploring the generalized Bayesian logistic regression method under a sparsity-inducing prior distribution. The method involves utilizing a fractional power of the likelihood resulting the fractional posterior. Our study yields concentration results for the fractional posterior, not only on the joint distribution of the predictor and response variable but also for the regression coefficients. Significantly, we derive novel findings concerning misclassification excess risk bounds using sparse generalized Bayesian logistic regression. These results parallel recent findings for penalized methods in the frequentist literature. Furthermore, we extend our results to the scenario of model misspecification, which is of critical importance.","sentences":["This work addresses the problem of high-dimensional classification by exploring the generalized Bayesian logistic regression method under a sparsity-inducing prior distribution.","The method involves utilizing a fractional power of the likelihood resulting the fractional posterior.","Our study yields concentration results for the fractional posterior, not only on the joint distribution of the predictor and response variable but also for the regression coefficients.","Significantly, we derive novel findings concerning misclassification excess risk bounds using sparse generalized Bayesian logistic regression.","These results parallel recent findings for penalized methods in the frequentist literature.","Furthermore, we extend our results to the scenario of model misspecification, which is of critical importance."],"url":"http://arxiv.org/abs/2403.12832v1","category":"math.ST"}
{"created":"2024-03-19 15:36:22","title":"Kinetic-type Mean Field Games with Non-separable Local Hamiltonians","abstract":"We prove well-posedness of a class of kinetic-type Mean Field Games, which typically arise when agents control their acceleration. Such systems include independent variables representing the spatial position as well as velocity. We consider non-separable Hamiltonians without any structural conditions, which depend locally on the density variable. Our analysis is based on two main ingredients: an energy method for the forward-backward system in Sobolev spaces, on the one hand and on a suitable vector field method to control derivatives with respect to the velocity variable, on the other hand. The careful combination of these two techniques reveals interesting phenomena applicable for Mean Field Games involving general classes of drift-diffusion operators and nonlinearities. While many prior existence theories for general Mean Field Games systems take the final datum function to be smoothing, we can allow this function to be non-smoothing, i.e. also depending locally on the final measure. Our well-posedness results hold under an appropriate smallness condition, assumed jointly on the data.","sentences":["We prove well-posedness of a class of kinetic-type Mean Field Games, which typically arise when agents control their acceleration.","Such systems include independent variables representing the spatial position as well as velocity.","We consider non-separable Hamiltonians without any structural conditions, which depend locally on the density variable.","Our analysis is based on two main ingredients: an energy method for the forward-backward system in Sobolev spaces, on the one hand and on a suitable vector field method to control derivatives with respect to the velocity variable, on the other hand.","The careful combination of these two techniques reveals interesting phenomena applicable for Mean Field Games involving general classes of drift-diffusion operators and nonlinearities.","While many prior existence theories for general Mean Field Games systems take the final datum function to be smoothing, we can allow this function to be non-smoothing, i.e. also depending locally on the final measure.","Our well-posedness results hold under an appropriate smallness condition, assumed jointly on the data."],"url":"http://arxiv.org/abs/2403.12829v1","category":"math.AP"}
{"created":"2024-03-19 15:24:49","title":"Answer Set Programming for Flexible Payroll Management","abstract":"Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries. Moreover, the rules are often complex and change regularly. Therefore, payroll management systems must be flexible in design. In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard. It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios. We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances.","sentences":["Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries.","Moreover, the rules are often complex and change regularly.","Therefore, payroll management systems must be flexible in design.","In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard.","It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios.","We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances."],"url":"http://arxiv.org/abs/2403.12823v1","category":"cs.LO"}
{"created":"2024-03-19 15:24:17","title":"FORM-based global reliability sensitivity analysis of systems with multiple failure modes","abstract":"Global variance-based reliability sensitivity indices arise from a variance decomposition of the indicator function describing the failure event. The first-order indices reflect the main effect of each variable on the variance of the failure event and can be used for variable prioritization; the total-effect indices represent the total effect of each variable, including its interaction with other variables, and can be used for variable fixing. This contribution derives expressions for the variance-based reliability indices of systems with multiple failure modes that are based on the first-order reliability method (FORM). The derived expressions are a function of the FORM results and, hence, do not require additional expensive model evaluations. They do involve the evaluation of multinormal integrals, for which effective solutions are available. We demonstrate that the derived expressions enable an accurate estimation of variance-based reliability sensitivities for general system problems to which FORM is applicable.","sentences":["Global variance-based reliability sensitivity indices arise from a variance decomposition of the indicator function describing the failure event.","The first-order indices reflect the main effect of each variable on the variance of the failure event and can be used for variable prioritization; the total-effect indices represent the total effect of each variable, including its interaction with other variables, and can be used for variable fixing.","This contribution derives expressions for the variance-based reliability indices of systems with multiple failure modes that are based on the first-order reliability method (FORM).","The derived expressions are a function of the FORM results and, hence, do not require additional expensive model evaluations.","They do involve the evaluation of multinormal integrals, for which effective solutions are available.","We demonstrate that the derived expressions enable an accurate estimation of variance-based reliability sensitivities for general system problems to which FORM is applicable."],"url":"http://arxiv.org/abs/2403.12822v1","category":"stat.ME"}
{"created":"2024-03-19 15:21:10","title":"FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer","abstract":"The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.","sentences":["The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution.","Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets.","Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance.","For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture.","FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking.","Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models.","Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer."],"url":"http://arxiv.org/abs/2403.12821v1","category":"cs.LG"}
{"created":"2024-03-19 15:21:00","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","abstract":"Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselines and predictive realism successfully validate its generalization ability. Inference efficiency of the proposed model also defeats traditional physics simulation. This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination.","sentences":["Delicate cloth simulations have long been desired in computer graphics.","Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations.","Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics.","This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation.","The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics.","The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks.","The model is tested across different cloth animation cases, without training with new data.","Agreement with baselines and predictive realism successfully validate its generalization ability.","Inference efficiency of the proposed model also defeats traditional physics simulation.","This framework is also designed to easily integrate with other visual refinement techniques like wrinkle carving, which leaves significant chances to incorporate prevailing macing learning techniques in 3D cloth amination."],"url":"http://arxiv.org/abs/2403.12820v1","category":"cs.GR"}
{"created":"2024-03-19 15:18:03","title":"Two-dimensional inference of divertor plasma characteristics: advancements to a multi-instrument Bayesian analysis system","abstract":"An integrated data analysis system based on Bayesian inference has been developed for application to data from multiple diagnostics over the two-dimensional cross-section of tokamak divertors. Tests of the divertor multi-instrument Bayesian analysis system (D-MIBAS) on a synthetic data set (including realistic experimental uncertainties) generated from SOLPS-ITER predictions of the MAST-U divertor have been performed. The resulting inference was within 6\\%, 5\\% and 30\\% median absolute percentage error of the SOLPS-predicted electron temperature, electron density and neutral atomic hydrogen density, respectively, across a two-dimensional poloidal cross-section of the MAST-U Super-X outer divertor.   To accommodate molecular contributions to Balmer emission, an advanced emission model has been developed which is shown to be crucial for inference accuracy. Our D-MIBAS system utilises a mesh aligned to poloidal magnetic flux-surfaces, throughout the divertor, with plasma parameters assigned to each mesh vertex and collectively considered in the inference. This allowed comprehensive forward models to multiple diagnostics and the inclusion of expected physics. This is shown to be important for inference precision when including molecular contributions to Balmer emission. These developments pave the way for accurate two-dimensional electron temperature, electron density and neutral atomic hydrogen density inferences for MAST-U divertor experimental data for the first time.","sentences":["An integrated data analysis system based on Bayesian inference has been developed for application to data from multiple diagnostics over the two-dimensional cross-section of tokamak divertors.","Tests of the divertor multi-instrument Bayesian analysis system (D-MIBAS) on a synthetic data set (including realistic experimental uncertainties) generated from SOLPS-ITER predictions of the MAST-U divertor have been performed.","The resulting inference was within 6\\%, 5\\% and 30\\% median absolute percentage error of the SOLPS-predicted electron temperature, electron density and neutral atomic hydrogen density, respectively, across a two-dimensional poloidal cross-section of the MAST-U Super-X outer divertor.   ","To accommodate molecular contributions to Balmer emission, an advanced emission model has been developed which is shown to be crucial for inference accuracy.","Our D-MIBAS system utilises a mesh aligned to poloidal magnetic flux-surfaces, throughout the divertor, with plasma parameters assigned to each mesh vertex and collectively considered in the inference.","This allowed comprehensive forward models to multiple diagnostics and the inclusion of expected physics.","This is shown to be important for inference precision when including molecular contributions to Balmer emission.","These developments pave the way for accurate two-dimensional electron temperature, electron density and neutral atomic hydrogen density inferences for MAST-U divertor experimental data for the first time."],"url":"http://arxiv.org/abs/2403.12819v1","category":"physics.plasm-ph"}
{"created":"2024-03-19 15:15:19","title":"Re-identification from histopathology images","abstract":"In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases. These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks. This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy. We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD). We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue. We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset. Based on our findings, we formulated a risk assessment scheme to estimate the risk to the patient's privacy prior to publication.","sentences":["In numerous studies, deep learning algorithms have proven their potential for the analysis of histopathology images, for example, for revealing the subtypes of tumors or the primary origin of metastases.","These models require large datasets for training, which must be anonymized to prevent possible patient identity leaks.","This study demonstrates that even relatively simple deep learning algorithms can re-identify patients in large histopathology datasets with substantial accuracy.","We evaluated our algorithms on two TCIA datasets including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).","We also demonstrate the algorithm's performance on an in-house dataset of meningioma tissue.","We predicted the source patient of a slide with F1 scores of 50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31 % on our meningioma dataset.","Based on our findings, we formulated a risk assessment scheme to estimate the risk to the patient's privacy prior to publication."],"url":"http://arxiv.org/abs/2403.12816v1","category":"cs.CV"}
{"created":"2024-03-19 15:13:44","title":"A Unified Framework for Rerandomization using Quadratic Forms","abstract":"In the design stage of a randomized experiment, one way to ensure treatment and control groups exhibit similar covariate distributions is to randomize treatment until some prespecified level of covariate balance is satisfied. This experimental design strategy is known as rerandomization. Most rerandomization methods utilize balance metrics based on a quadratic form $v^TAv$ , where $v$ is a vector of covariate mean differences and $A$ is a positive semi-definite matrix. In this work, we derive general results for treatment-versus-control rerandomization schemes that employ quadratic forms for covariate balance. In addition to allowing researchers to quickly derive properties of rerandomization schemes not previously considered, our theoretical results provide guidance on how to choose the matrix $A$ in practice. We find the Mahalanobis and Euclidean distances optimize different measures of covariate balance. Furthermore, we establish how the covariates' eigenstructure and their relationship to the outcomes dictates which matrix $A$ yields the most precise mean-difference estimator for the average treatment effect. We find that the Euclidean distance is minimax optimal, in the sense that the mean-difference estimator's precision is never too far from the optimal choice, regardless of the relationship between covariates and outcomes. Our theoretical results are verified via simulation, where we find that rerandomization using the Euclidean distance has better performance in high-dimensional settings and typically achieves greater variance reduction to the mean-difference estimator than other quadratic forms.","sentences":["In the design stage of a randomized experiment, one way to ensure treatment and control groups exhibit similar covariate distributions is to randomize treatment until some prespecified level of covariate balance is satisfied.","This experimental design strategy is known as rerandomization.","Most rerandomization methods utilize balance metrics based on a quadratic form $v^TAv$ , where $v$ is a vector of covariate mean differences and $A$ is a positive semi-definite matrix.","In this work, we derive general results for treatment-versus-control rerandomization schemes that employ quadratic forms for covariate balance.","In addition to allowing researchers to quickly derive properties of rerandomization schemes not previously considered, our theoretical results provide guidance on how to choose the matrix $A$ in practice.","We find the Mahalanobis and Euclidean distances optimize different measures of covariate balance.","Furthermore, we establish how the covariates' eigenstructure and their relationship to the outcomes dictates which matrix $A$ yields the most precise mean-difference estimator for the average treatment effect.","We find that the Euclidean distance is minimax optimal, in the sense that the mean-difference estimator's precision is never too far from the optimal choice, regardless of the relationship between covariates and outcomes.","Our theoretical results are verified via simulation, where we find that rerandomization using the Euclidean distance has better performance in high-dimensional settings and typically achieves greater variance reduction to the mean-difference estimator than other quadratic forms."],"url":"http://arxiv.org/abs/2403.12815v1","category":"stat.ME"}
{"created":"2024-03-19 15:12:56","title":"Knowledge and Data Dual-Driven Channel Estimation and Feedback for Ultra-Massive MIMO Systems under Hybrid Field Beam Squint Effect","abstract":"Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance. To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks. Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated. A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers. In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively. Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional quantized bit vector for feedback, thereby reducing the feedback overhead substantially. Simulation results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios.","sentences":["Acquiring accurate channel state information (CSI) at an access point (AP) is challenging for wideband millimeter wave (mmWave) ultra-massive multiple-input and multiple-output (UMMIMO) systems, due to the high-dimensional channel matrices, hybrid near- and far- field channel feature, beam squint effects, and imperfect hardware constraints, such as low-resolution analog-to-digital converters, and in-phase and quadrature imbalance.","To overcome these challenges, this paper proposes an efficient downlink channel estimation (CE) and CSI feedback approach based on knowledge and data dual-driven deep learning (DL) networks.","Specifically, we first propose a data-driven residual neural network de-quantizer (ResNet-DQ) to pre-process the received pilot signals at user equipment (UEs), where the noise and distortion brought by imperfect hardware can be mitigated.","A knowledge-driven generalized multiple measurement vector learned approximate message passing (GMMV-LAMP) network is then developed to jointly estimate the channels by exploiting the approximately same physical angle shared by different subcarriers.","In particular, two wideband redundant dictionaries (WRDs) are proposed such that the measurement matrices of the GMMV-LAMP network can accommodate the far-field and near-field beam squint effect, respectively.","Finally, we propose an encoder at the UEs and a decoder at the AP by a data-driven CSI residual network (CSI-ResNet) to compress the CSI matrix into a low-dimensional quantized bit vector for feedback, thereby reducing the feedback overhead substantially.","Simulation results show that the proposed knowledge and data dual-driven approach outperforms conventional downlink CE and CSI feedback methods, especially in the case of low signal-to-noise ratios."],"url":"http://arxiv.org/abs/2403.12813v1","category":"cs.IT"}
{"created":"2024-03-19 15:09:00","title":"The Inflationary Butterfly Effect: Non-Perturbative Dynamics From Small-Scale Features","abstract":"For the first time, we investigate the non-perturbative dynamics of single field inflation with a departure from slow-roll. Using simulations, we find that oscillatory features in the potential can drastically alter the course of inflation, with major phenomenological implications. In certain cases, the entire Universe gets trapped in a forever inflating de Sitter state. In others, only some regions get stuck in a false vacuum, offering an alternative channel for primordial black hole formation. Analogous to the flap of a butterfly, these results show that small-scale phenomena can have profound consequences on the evolution of the entire Universe. This demonstrates the necessity of a non-perturbative approach in the exploration of the small-scale physics of inflation, particularly in the regime relevant for gravitational-wave astronomy.","sentences":["For the first time, we investigate the non-perturbative dynamics of single field inflation with a departure from slow-roll.","Using simulations, we find that oscillatory features in the potential can drastically alter the course of inflation, with major phenomenological implications.","In certain cases, the entire Universe gets trapped in a forever inflating de Sitter state.","In others, only some regions get stuck in a false vacuum, offering an alternative channel for primordial black hole formation.","Analogous to the flap of a butterfly, these results show that small-scale phenomena can have profound consequences on the evolution of the entire Universe.","This demonstrates the necessity of a non-perturbative approach in the exploration of the small-scale physics of inflation, particularly in the regime relevant for gravitational-wave astronomy."],"url":"http://arxiv.org/abs/2403.12811v1","category":"astro-ph.CO"}
{"created":"2024-03-19 15:07:22","title":"Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models","abstract":"In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.","sentences":["In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions.","Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction.","Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models.","On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored.","Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models.","We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models.","Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers.","Our code is available: https://github.com/casszhao/multilingual-faith."],"url":"http://arxiv.org/abs/2403.12809v1","category":"cs.CL"}
{"created":"2024-03-19 15:07:12","title":"Freshness-aware Block Propagation Optimization in 6G-based Web 3.0: An Evolutionary Game Approach","abstract":"Driven by the aspiration to establish a decentralized digital economy, Web 3.0 is emerging as the fundamental technology for digital transformation. Incorporating the promising sixth-generation (6G) technology with large bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds great potential in empowering users with enhanced data control and facilitating secure peer-to-peer transactions, especially in consumer electronics, through the utilization of blockchain technologies. However, 6G-based Web 3.0 is still in its infancy, such as ensuring block freshness and optimizing block propagation to improve blockchain performance. In this paper, we develop a freshness-aware block propagation optimization framework for 6G-based Web 3.0. We first propose a novel metric called Age of Block Information (AoBI) based on the concept of age of information to quantify block freshness. To make block propagation optimization tractable, we classify miners into five different states and propose a block propagation model for public blockchains inspired by epidemic models. Moreover, considering that the miners are bounded rational, we propose an incentive mechanism based on the evolutionary game for block propagation to improve block propagation efficiency. Numerical results demonstrate that compared with other block propagation mechanisms, the proposed scheme has a higher block forwarding probability, which improves block propagation efficiency.","sentences":["Driven by the aspiration to establish a decentralized digital economy, Web 3.0 is emerging as the fundamental technology for digital transformation.","Incorporating the promising sixth-generation (6G) technology with large bandwidth and space-air-ground integrated coverage, 6G-based Web 3.0 holds great potential in empowering users with enhanced data control and facilitating secure peer-to-peer transactions, especially in consumer electronics, through the utilization of blockchain technologies.","However, 6G-based Web 3.0 is still in its infancy, such as ensuring block freshness and optimizing block propagation to improve blockchain performance.","In this paper, we develop a freshness-aware block propagation optimization framework for 6G-based Web 3.0.","We first propose a novel metric called Age of Block Information (AoBI) based on the concept of age of information to quantify block freshness.","To make block propagation optimization tractable, we classify miners into five different states and propose a block propagation model for public blockchains inspired by epidemic models.","Moreover, considering that the miners are bounded rational, we propose an incentive mechanism based on the evolutionary game for block propagation to improve block propagation efficiency.","Numerical results demonstrate that compared with other block propagation mechanisms, the proposed scheme has a higher block forwarding probability, which improves block propagation efficiency."],"url":"http://arxiv.org/abs/2403.12807v1","category":"cs.GT"}
{"created":"2024-03-19 15:07:08","title":"VisualCritic: Making LMMs Perceive Visual Quality Like Humans","abstract":"At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer \"Yes!\". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.","sentences":["At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals.","However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception.","Can LMMs achieve this and show the same degree of generalization in this regard?","If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed.","In this paper, we explore this question and provide the answer \"Yes!\".","As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment.","VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models.","As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic.","Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images."],"url":"http://arxiv.org/abs/2403.12806v1","category":"cs.CV"}
{"created":"2024-03-19 15:06:53","title":"Contextual Moral Value Alignment Through Context-Based Aggregation","abstract":"Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI. Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance. In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation. Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input. The proposed system shows better results in term of alignment to human value compared to the state of the art.","sentences":["Developing value-aligned AI agents is a complex undertaking and an ongoing challenge in the field of AI.","Specifically within the domain of Large Language Models (LLMs), the capability to consolidate multiple independently trained dialogue agents, each aligned with a distinct moral value, into a unified system that can adapt to and be aligned with multiple moral values is of paramount importance.","In this paper, we propose a system that does contextual moral value alignment based on contextual aggregation.","Here, aggregation is defined as the process of integrating a subset of LLM responses that are best suited to respond to a user input, taking into account features extracted from the user's input.","The proposed system shows better results in term of alignment to human value compared to the state of the art."],"url":"http://arxiv.org/abs/2403.12805v1","category":"cs.AI"}
{"created":"2024-03-19 15:04:35","title":"DreamDA: Generative Data Augmentation with Diffusion Models","abstract":"The acquisition of large-scale, high-quality data is a resource-intensive and time-consuming endeavor. Compared to conventional Data Augmentation (DA) techniques (e.g. cropping and rotation), exploiting prevailing diffusion models for data generation has received scant attention in classification tasks. Existing generative DA methods either inadequately bridge the domain gap between real-world and synthesized images, or inherently suffer from a lack of diversity. To solve these issues, this paper proposes a new classification-oriented framework DreamDA, which enables data synthesis and label generation by way of diffusion models. DreamDA generates diverse samples that adhere to the original data distribution by considering training images in the original data as seeds and perturbing their reverse diffusion process. In addition, since the labels of the generated data may not align with the labels of their corresponding seed images, we introduce a self-training paradigm for generating pseudo labels and training classifiers using the synthesized data. Extensive experiments across four tasks and five datasets demonstrate consistent improvements over strong baselines, revealing the efficacy of DreamDA in synthesizing high-quality and diverse images with accurate labels. Our code will be available at https://github.com/yunxiangfu2001/DreamDA.","sentences":["The acquisition of large-scale, high-quality data is a resource-intensive and time-consuming endeavor.","Compared to conventional Data Augmentation (DA) techniques (e.g. cropping and rotation), exploiting prevailing diffusion models for data generation has received scant attention in classification tasks.","Existing generative DA methods either inadequately bridge the domain gap between real-world and synthesized images, or inherently suffer from a lack of diversity.","To solve these issues, this paper proposes a new classification-oriented framework DreamDA, which enables data synthesis and label generation by way of diffusion models.","DreamDA generates diverse samples that adhere to the original data distribution by considering training images in the original data as seeds and perturbing their reverse diffusion process.","In addition, since the labels of the generated data may not align with the labels of their corresponding seed images, we introduce a self-training paradigm for generating pseudo labels and training classifiers using the synthesized data.","Extensive experiments across four tasks and five datasets demonstrate consistent improvements over strong baselines, revealing the efficacy of DreamDA in synthesizing high-quality and diverse images with accurate labels.","Our code will be available at https://github.com/yunxiangfu2001/DreamDA."],"url":"http://arxiv.org/abs/2403.12803v1","category":"cs.CV"}
{"created":"2024-03-19 15:01:19","title":"RelationVLM: Making Large Vision-Language Models Understand Visual Relations","abstract":"The development of Large Vision-Language Models (LVLMs) is striving to catch up with the success of Large Language Models (LLMs), yet it faces more challenges to be resolved. Very recent works enable LVLMs to localize object-level visual contents and ground text to them. Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data. In this work, we present RelationVLM, a large vision-language model capable of comprehending various levels and types of relations whether across multiple images or within a video. Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms. Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive in-context capability of reasoning from few-shot examples by comparison. This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence.","sentences":["The development of Large Vision-Language Models (LVLMs) is striving to catch up with the success of Large Language Models (LLMs), yet it faces more challenges to be resolved.","Very recent works enable LVLMs to localize object-level visual contents and ground text to them.","Nonetheless, current LVLMs still struggle to precisely understand visual relations due to the lack of relevant data.","In this work, we present RelationVLM, a large vision-language model capable of comprehending various levels and types of relations whether across multiple images or within a video.","Specifically, we devise a multi-stage relation-aware training scheme and a series of corresponding data configuration strategies to bestow RelationVLM with the capabilities of understanding semantic relations, temporal associations and geometric transforms.","Extensive case studies and quantitative evaluations show RelationVLM has strong capability in understanding such relations and emerges impressive in-context capability of reasoning from few-shot examples by comparison.","This work fosters the advancements of LVLMs by enabling them to support a wider range of downstream applications toward artificial general intelligence."],"url":"http://arxiv.org/abs/2403.12801v1","category":"cs.CV"}
{"created":"2024-03-19 15:01:18","title":"Learning Neural Volumetric Pose Features for Camera Localization","abstract":"We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses. Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module. This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features. Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework. Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy.","sentences":["We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses.","Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module.","This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features.","Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework.","Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy."],"url":"http://arxiv.org/abs/2403.12800v1","category":"cs.CV"}
{"created":"2024-03-19 15:01:14","title":"Investigating Text Shortening Strategy in BERT: Truncation vs Summarization","abstract":"The parallelism of Transformer-based models comes at the cost of their input max-length. Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative. In this study, we investigate the performance of document truncation and summarization in text classification tasks. Each of the two was investigated with several variations. This study also investigated how close their performances are to the performance of full-text. We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests. This study shows how the summaries outperform the majority of truncation method variations and lose to only one. The best strategy obtained in this study is taking the head of the document. The second is extractive summarization. This study explains what happened to the result, leading to further research in order to exploit the potential of document summarization as a shortening alternative. The code and data used in this work are publicly available in https://github.com/mirzaalimm/TruncationVsSummarization.","sentences":["The parallelism of Transformer-based models comes at the cost of their input max-length.","Some studies proposed methods to overcome this limitation, but none of them reported the effectiveness of summarization as an alternative.","In this study, we investigate the performance of document truncation and summarization in text classification tasks.","Each of the two was investigated with several variations.","This study also investigated how close their performances are to the performance of full-text.","We used a dataset of summarization tasks based on Indonesian news articles (IndoSum) to do classification tests.","This study shows how the summaries outperform the majority of truncation method variations and lose to only one.","The best strategy obtained in this study is taking the head of the document.","The second is extractive summarization.","This study explains what happened to the result, leading to further research in order to exploit the potential of document summarization as a shortening alternative.","The code and data used in this work are publicly available in https://github.com/mirzaalimm/TruncationVsSummarization."],"url":"http://arxiv.org/abs/2403.12799v1","category":"cs.CL"}
{"created":"2024-03-19 14:52:53","title":"Modeling RIS from Electromagnetic Principles to Communication Systems--Part I: Synthesis and Characterization of a Scalable Anomalous Reflector","abstract":"This work aims to build connections between the electromagnetic and communication aspects of Reconfigurable Intelligent Surfaces (RIS) by proposing a methodology to combine outputs from electromagnetic RIS design into an RIS-tailored system-level simulator and a ray tracer. In this first part of the contribution, a periodic anomalous reflector is designed using an algebraic array antenna scattering synthesis technique that enables electromagnetically accurate modeling of scattering surfaces with both static and reconfigurable scattering characteristics. The multi-mode periodic structure, capable of scattering into several anomalous angles through manipulation of reactive loads, is then cropped into finite-sized arrays, and the quantization effects of the load reactances on the array scattering are analyzed. An experimental anomalous reflector is demonstrated with a comparison between simulated and measured scattering performance. In the second part, the simulated receiving and transmitting scattering patterns of the anomalous reflector are utilized to build an electromagnetically consistent path loss model of an RIS into a system-level simulator. Large-scale fading is analyzed in simple scenarios of RIS-assisted wireless networks to verify the communication model, and an indoor scenario measurement using the manufactured anomalous reflector sample to support the simulation analysis. After verifying the connections between electromagnetic and communication aspects through simulations and measurements, the proposed communication model can be used for a broad range of RIS designs to perform large-scale system-level and ray-tracing simulations in realistic scenarios.","sentences":["This work aims to build connections between the electromagnetic and communication aspects of Reconfigurable Intelligent Surfaces (RIS) by proposing a methodology to combine outputs from electromagnetic RIS design into an RIS-tailored system-level simulator and a ray tracer.","In this first part of the contribution, a periodic anomalous reflector is designed using an algebraic array antenna scattering synthesis technique that enables electromagnetically accurate modeling of scattering surfaces with both static and reconfigurable scattering characteristics.","The multi-mode periodic structure, capable of scattering into several anomalous angles through manipulation of reactive loads, is then cropped into finite-sized arrays, and the quantization effects of the load reactances on the array scattering are analyzed.","An experimental anomalous reflector is demonstrated with a comparison between simulated and measured scattering performance.","In the second part, the simulated receiving and transmitting scattering patterns of the anomalous reflector are utilized to build an electromagnetically consistent path loss model of an RIS into a system-level simulator.","Large-scale fading is analyzed in simple scenarios of RIS-assisted wireless networks to verify the communication model, and an indoor scenario measurement using the manufactured anomalous reflector sample to support the simulation analysis.","After verifying the connections between electromagnetic and communication aspects through simulations and measurements, the proposed communication model can be used for a broad range of RIS designs to perform large-scale system-level and ray-tracing simulations in realistic scenarios."],"url":"http://arxiv.org/abs/2403.12790v1","category":"physics.app-ph"}
{"created":"2024-03-19 14:52:51","title":"Bivariate temporal dependence via mixtures of rotated copulas","abstract":"Parametric bivariate copula families have been known to flexibly capture enough various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions. However, to the best of our knowledge, there is not a single parametric model adaptable enough to capture several of these features simultaneously. To address this, we propose a mixture of 4-way rotations of a parametric copula that is able to capture all these features. We illustrate the construction using the Clayton family but the concept is general and can be applied to other families. In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average type of relationship. The properties of the proposed model and its performance are examined using simulated and real data sets.","sentences":["Parametric bivariate copula families have been known to flexibly capture enough various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions.","However, to the best of our knowledge, there is not a single parametric model adaptable enough to capture several of these features simultaneously.","To address this, we propose a mixture of 4-way rotations of a parametric copula that is able to capture all these features.","We illustrate the construction using the Clayton family but the concept is general and can be applied to other families.","In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average type of relationship.","The properties of the proposed model and its performance are examined using simulated and real data sets."],"url":"http://arxiv.org/abs/2403.12789v1","category":"stat.ME"}
{"created":"2024-03-19 14:50:13","title":"Total Disentanglement of Font Images into Style and Character Class Features","abstract":"In this paper, we demonstrate a total disentanglement of font images. Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features. It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts. These disentangled features guarantee the reconstruction of the original font image. Various experiments have been conducted to understand the performance of total disentanglement. First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?'' Hofstadter (1985). Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation.","sentences":["In this paper, we demonstrate a total disentanglement of font images.","Total disentanglement is a neural network-based method for decomposing each font image nonlinearly and completely into its style and content (i.e., character class) features.","It uses a simple but careful training procedure to extract the common style feature from all `A'-`Z' images in the same font and the common content feature from all `A' (or another class) images in different fonts.","These disentangled features guarantee the reconstruction of the original font image.","Various experiments have been conducted to understand the performance of total disentanglement.","First, it is demonstrated that total disentanglement is achievable with very high accuracy; this is experimental proof of the long-standing open question, ``Does `A'-ness exist?''","Hofstadter (1985).","Second, it is demonstrated that the disentangled features produced by total disentanglement apply to a variety of tasks, including font recognition, character recognition, and one-shot font image generation."],"url":"http://arxiv.org/abs/2403.12784v1","category":"cs.CV"}
{"created":"2024-03-19 14:49:45","title":"Beyond Point Masses. II. Non-Keplerian Shape Effects are Detectable in Several TNO Binaries","abstract":"About 40 transneptunian binaries (TNBs) have fully determined orbits with about 10 others being solved except for breaking the mirror ambiguity. Despite decades of study almost all TNBs have only ever been analyzed with a model that assumes perfect Keplerian motion (e.g., two point masses). In reality, all TNB systems are non-Keplerian due to non-spherical shapes, possible presence of undetected system components, and/or solar perturbations. In this work, we focus on identifying candidates for detectable non-Keplerian motion based on sample of 45 well-characterized binaries. We use MultiMoon, a non-Keplerian Bayesian inference tool, to analyze published relative astrometry allowing for non-spherical shapes of each TNB system's primary. We first reproduce the results of previous Keplerian fitting efforts with MultiMoon, which serves as a comparison for the non-Keplerian fits and confirms that these fits are not biased by the assumption of a Keplerian orbit. We unambiguously detect non-Keplerian motion in 8 TNB systems across a range of primary radii, mutual orbit separations, and system masses. As a proof of concept for non-Keplerian fitting, we perform detailed fits for (66652) Borasisi-Pabu, possibly revealing a $J_2 \\approx 0.44$, implying Borasisi (and/or Pabu) may be a contact binary or an unresolved compact binary. However, full confirmation of this result will require new observations. This work begins the next generation of TNB analyses that go beyond the point mass assumption to provide unique and valuable information on the physical properties of TNBs with implications for their formation and evolution.","sentences":["About 40 transneptunian binaries (TNBs) have fully determined orbits with about 10 others being solved except for breaking the mirror ambiguity.","Despite decades of study almost all TNBs have only ever been analyzed with a model that assumes perfect Keplerian motion (e.g., two point masses).","In reality, all TNB systems are non-Keplerian due to non-spherical shapes, possible presence of undetected system components, and/or solar perturbations.","In this work, we focus on identifying candidates for detectable non-Keplerian motion based on sample of 45 well-characterized binaries.","We use MultiMoon, a non-Keplerian Bayesian inference tool, to analyze published relative astrometry allowing for non-spherical shapes of each TNB system's primary.","We first reproduce the results of previous Keplerian fitting efforts with MultiMoon, which serves as a comparison for the non-Keplerian fits and confirms that these fits are not biased by the assumption of a Keplerian orbit.","We unambiguously detect non-Keplerian motion in 8 TNB systems across a range of primary radii, mutual orbit separations, and system masses.","As a proof of concept for non-Keplerian fitting, we perform detailed fits for (66652) Borasisi-Pabu, possibly revealing a $J_2 \\approx 0.44$, implying Borasisi (and/or Pabu) may be a contact binary or an unresolved compact binary.","However, full confirmation of this result will require new observations.","This work begins the next generation of TNB analyses that go beyond the point mass assumption to provide unique and valuable information on the physical properties of TNBs with implications for their formation and evolution."],"url":"http://arxiv.org/abs/2403.12783v1","category":"astro-ph.EP"}
{"created":"2024-03-19 14:48:10","title":"Large-Scale RIS Enabled Air-Ground Channels: Near-Field Modeling and Analysis","abstract":"Existing works mainly rely on the far-field planar-wave-based channel model to assess the performance of reconfigurable intelligent surface (RIS)-enabled wireless communication systems. However, when the transmitter and receiver are in near-field ranges, this will result in relatively low computing accuracy. To tackle this challenge, we initially develop an analytical framework for sub-array partitioning. This framework divides the large-scale RIS array into multiple sub-arrays, effectively reducing modeling complexity while maintaining acceptable accuracy. Then, we develop a beam domain channel model based on the proposed sub-array partition framework for large-scale RIS-enabled UAV-to-vehicle communication systems, which can be used to efficiently capture the sparse features in RIS-enabled UAV-to-vehicle channels in both near-field and far-field ranges. Furthermore, some important propagation characteristics of the proposed channel model, including the spatial cross-correlation functions (CCFs), temporal auto-correlation functions (ACFs), frequency correlation functions (CFs), and channel capacities with respect to the different physical features of the RIS and non-stationary properties of the channel model are derived and analyzed. Finally, simulation results are provided to demonstrate that the proposed framework is helpful to achieve a good tradeoff between model complexity and accuracy for investigating the channel propagation characteristics, and therefore providing highly-efficient communications in RIS-enabled UAV-to-vehicle wireless networks.","sentences":["Existing works mainly rely on the far-field planar-wave-based channel model to assess the performance of reconfigurable intelligent surface (RIS)-enabled wireless communication systems.","However, when the transmitter and receiver are in near-field ranges, this will result in relatively low computing accuracy.","To tackle this challenge, we initially develop an analytical framework for sub-array partitioning.","This framework divides the large-scale RIS array into multiple sub-arrays, effectively reducing modeling complexity while maintaining acceptable accuracy.","Then, we develop a beam domain channel model based on the proposed sub-array partition framework for large-scale RIS-enabled UAV-to-vehicle communication systems, which can be used to efficiently capture the sparse features in RIS-enabled UAV-to-vehicle channels in both near-field and far-field ranges.","Furthermore, some important propagation characteristics of the proposed channel model, including the spatial cross-correlation functions (CCFs), temporal auto-correlation functions (ACFs), frequency correlation functions (CFs), and channel capacities with respect to the different physical features of the RIS and non-stationary properties of the channel model are derived and analyzed.","Finally, simulation results are provided to demonstrate that the proposed framework is helpful to achieve a good tradeoff between model complexity and accuracy for investigating the channel propagation characteristics, and therefore providing highly-efficient communications in RIS-enabled UAV-to-vehicle wireless networks."],"url":"http://arxiv.org/abs/2403.12781v1","category":"eess.SP"}
{"created":"2024-03-19 14:44:54","title":"Discover and Mitigate Multiple Biased Subgroups in Image Classifiers","abstract":"Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier. We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models. Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups. Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers. The code is available at https://github.com/ZhangAIPI/DIM.","sentences":["Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications.","Such subgroups are typically unknown due to the absence of subgroup labels.","Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness.","Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   ","In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers.","Our approach decomposes the image features into multiple components that represent multiple subgroups.","This decomposition is achieved via a bilinear dimension reduction method, Partial Least Square (PLS), guided by useful supervision from the image classifier.","We further interpret the semantic meaning of each subgroup component by generating natural language descriptions using vision-language foundation models.","Finally, DIM mitigates multiple biased subgroups simultaneously via two strategies, including the data- and model-centric strategies.","Extensive experiments on CIFAR-100 and Breeds datasets demonstrate the effectiveness of DIM in discovering and mitigating multiple biased subgroups.","Furthermore, DIM uncovers the failure modes of the classifier on Hard ImageNet, showcasing its broader applicability to understanding model bias in image classifiers.","The code is available at https://github.com/ZhangAIPI/DIM."],"url":"http://arxiv.org/abs/2403.12777v1","category":"cs.CV"}
{"created":"2024-03-19 14:44:45","title":"Automated Data Curation for Robust Language Model Fine-Tuning","abstract":"Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.   We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Automatically identifying which data to filter or correct is done via LLM-derived confidence estimates, to ensure only confident modifications to the dataset. Unlike existing data curation techniques, CLEAR is a comprehensive framework that can improve a dataset (and trained model outputs) without additional fine-tuning computations. We don't assume access to a stronger LLM than the model being fine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether CLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal that CLEAR consistently improves the performance of fine-tuned models across many datasets and models (like GPT-3.5 and Llama2).","sentences":["Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses.","Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with target responses, but real-world data tends to be noisy.","While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.   ","We introduce an automated data curation pipeline CLEAR (Confidence-based LLM Evaluation And Rectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure.","CLEAR estimates which training data is low-quality and either filters or corrects it.","Automatically identifying which data to filter or correct is done via LLM-derived confidence estimates, to ensure only confident modifications to the dataset.","Unlike existing data curation techniques, CLEAR is a comprehensive framework that can improve a dataset (and trained model outputs) without additional fine-tuning computations.","We don't assume access to a stronger LLM than the model being fine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether CLEAR can meaningfully improve the capabilities of any LLM.","Experiments reveal that CLEAR consistently improves the performance of fine-tuned models across many datasets and models (like GPT-3.5 and Llama2)."],"url":"http://arxiv.org/abs/2403.12776v1","category":"cs.CL"}
{"created":"2024-03-19 14:43:52","title":"Is open source software culture enough to make AI a common ?","abstract":"Language models (LM or LLM) are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users. Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation. In this contribution, we examine the concept of the commons and its relevance for thinking about LM. We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies. Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers.","sentences":["Language models (LM or LLM) are increasingly deployed in the field of artificial intelligence (AI) and its applications, but the question arises as to whether they can be a common resource managed and maintained by a community of users.","Indeed, the dominance of private companies with exclusive access to massive data and language processing resources can create inequalities and biases in LM, as well as obstacles to innovation for those who do not have the same resources necessary for their implementation.","In this contribution, we examine the concept of the commons and its relevance for thinking about LM.","We highlight the potential benefits of treating the data and resources needed to create LMs as commons, including increased accessibility, equity, and transparency in the development and use of AI technologies.","Finally, we present a case study centered on the Hugging Face platform, an open-source platform for deep learning designed to encourage collaboration and sharing among AI designers."],"url":"http://arxiv.org/abs/2403.12774v1","category":"cs.CY"}
{"created":"2024-03-19 14:34:44","title":"Multispectral Image Restoration by Generalized Opponent Transformation Total Variation","abstract":"Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks. Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization. The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain. Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images. We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration. To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM.","sentences":["Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks.","Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization.","The primary contribution of this paper is to propose and develop a new multispectral total variation regularization in a generalized opponent transformation domain instead of the original multispectral image domain.","Here opponent transformations for multispectral images are generalized from a well-known opponent transformation for color images.","We will explore the properties of generalized opponent transformation total variation (GOTTV) regularization and the corresponding optimization formula for multispectral image restoration.","To evaluate the effectiveness of the new GOTTV method, we provide numerical examples that showcase its superior performance compared to existing multispectral image total variation methods, using criteria such as MPSNR and MSSIM."],"url":"http://arxiv.org/abs/2403.12770v1","category":"cs.CV"}
{"created":"2024-03-19 14:32:28","title":"ContextVis: Envision Contextual Learning and Interaction with Generative Models","abstract":"ContextVis introduces a workflow by integrating generative models to create contextual learning materials. It aims to boost knowledge acquisition through the creation of resources with contextual cues. A case study on vocabulary learning demonstrates the effectiveness of generative models in developing educational resources that enrich language understanding and aid memory retention. The system combines an easy-to-use Dashboard for educators with an interactive Playground for learners, establishing a unified platform for content creation and interaction. Future work may expand to include a wider range of generative models, media formats, and customization features for educators.","sentences":["ContextVis introduces a workflow by integrating generative models to create contextual learning materials.","It aims to boost knowledge acquisition through the creation of resources with contextual cues.","A case study on vocabulary learning demonstrates the effectiveness of generative models in developing educational resources that enrich language understanding and aid memory retention.","The system combines an easy-to-use Dashboard for educators with an interactive Playground for learners, establishing a unified platform for content creation and interaction.","Future work may expand to include a wider range of generative models, media formats, and customization features for educators."],"url":"http://arxiv.org/abs/2403.12768v1","category":"cs.HC"}
{"created":"2024-03-19 14:31:59","title":"Dynamic Density Functional Theory with Inertia and Background Flow","abstract":"We present dynamic density functional theory (DDFT) incorporating general inhomogeneous, incompressible, time dependent background flows and inertia, describing externally driven passive colloidal systems out of equilibrium. We start by considering the underlying nonequilibrium Langevin dynamics, including the effect of the local velocity of the surrounding liquid bath, to obtain the nonlinear, nonlocal partial differential equations governing the evolution of the (coarse--grained) density and velocity fields describing the dynamics of colloids. Additionally, we show both with heuristic arguments, and by numerical solution, that our equations and solutions agree with existing DDFTs in the overdamped (high friction) limit. We provide numerical solutions that model the flow of hard spheres, in both unbounded and confined domains, and compare to previously--derived DDFTs with and without the background flow.","sentences":["We present dynamic density functional theory (DDFT) incorporating general inhomogeneous, incompressible, time dependent background flows and inertia, describing externally driven passive colloidal systems out of equilibrium.","We start by considering the underlying nonequilibrium Langevin dynamics, including the effect of the local velocity of the surrounding liquid bath, to obtain the nonlinear, nonlocal partial differential equations governing the evolution of the (coarse--grained) density and velocity fields describing the dynamics of colloids.","Additionally, we show both with heuristic arguments, and by numerical solution, that our equations and solutions agree with existing DDFTs in the overdamped (high friction) limit.","We provide numerical solutions that model the flow of hard spheres, in both unbounded and confined domains, and compare to previously--derived DDFTs with and without the background flow."],"url":"http://arxiv.org/abs/2403.12765v1","category":"cond-mat.soft"}
{"created":"2024-03-19 14:27:31","title":"BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs","abstract":"This paper presents a novel approach to generating behavior trees for robots using lightweight large language models (LLMs) with a maximum of 7 billion parameters. The study demonstrates that it is possible to achieve satisfying results with compact LLMs when fine-tuned on a specific dataset. The key contributions of this research include the creation of a fine-tuning dataset based on existing behavior trees using GPT-3.5 and a comprehensive comparison of multiple LLMs (namely llama2, llama-chat, and code-llama) across nine distinct tasks. To be thorough, we evaluated the generated behavior trees using static syntactical analysis, a validation system, a simulated environment, and a real robot. Furthermore, this work opens the possibility of deploying such solutions directly on the robot, enhancing its practical applicability. Findings from this study demonstrate the potential of LLMs with a limited number of parameters in generating effective and efficient robot behaviors.","sentences":["This paper presents a novel approach to generating behavior trees for robots using lightweight large language models (LLMs) with a maximum of 7 billion parameters.","The study demonstrates that it is possible to achieve satisfying results with compact LLMs when fine-tuned on a specific dataset.","The key contributions of this research include the creation of a fine-tuning dataset based on existing behavior trees using GPT-3.5 and a comprehensive comparison of multiple LLMs (namely llama2, llama-chat, and code-llama) across nine distinct tasks.","To be thorough, we evaluated the generated behavior trees using static syntactical analysis, a validation system, a simulated environment, and a real robot.","Furthermore, this work opens the possibility of deploying such solutions directly on the robot, enhancing its practical applicability.","Findings from this study demonstrate the potential of LLMs with a limited number of parameters in generating effective and efficient robot behaviors."],"url":"http://arxiv.org/abs/2403.12761v1","category":"cs.RO"}
{"created":"2024-03-19 14:27:24","title":"WaveFace: Authentic Face Restoration with Efficient Frequency Recovery","abstract":"Although diffusion models are rising as a powerful solution for blind face restoration, they are criticized for two problems: 1) slow training and inference speed, and 2) failure in preserving identity and recovering fine-grained facial details. In this work, we propose WaveFace to solve the problems in the frequency domain, where low- and high-frequency components decomposed by wavelet transformation are considered individually to maximize authenticity as well as efficiency. The diffusion model is applied to recover the low-frequency component only, which presents general information of the original image but 1/16 in size. To preserve the original identity, the generation is conditioned on the low-frequency component of low-quality images at each denoising step. Meanwhile, high-frequency components at multiple decomposition levels are handled by a unified network, which recovers complex facial details in a single step. Evaluations on four benchmark datasets show that: 1) WaveFace outperforms state-of-the-art methods in authenticity, especially in terms of identity preservation, and 2) authentic images are restored with the efficiency 10x faster than existing diffusion model-based BFR methods.","sentences":["Although diffusion models are rising as a powerful solution for blind face restoration, they are criticized for two problems: 1) slow training and inference speed, and 2) failure in preserving identity and recovering fine-grained facial details.","In this work, we propose WaveFace to solve the problems in the frequency domain, where low- and high-frequency components decomposed by wavelet transformation are considered individually to maximize authenticity as well as efficiency.","The diffusion model is applied to recover the low-frequency component only, which presents general information of the original image but 1/16 in size.","To preserve the original identity, the generation is conditioned on the low-frequency component of low-quality images at each denoising step.","Meanwhile, high-frequency components at multiple decomposition levels are handled by a unified network, which recovers complex facial details in a single step.","Evaluations on four benchmark datasets show that: 1) WaveFace outperforms state-of-the-art methods in authenticity, especially in terms of identity preservation, and 2) authentic images are restored with the efficiency 10x faster than existing diffusion model-based BFR methods."],"url":"http://arxiv.org/abs/2403.12760v1","category":"cs.CV"}
{"created":"2024-03-19 14:27:13","title":"Strong-coupling critical behavior in three-dimensional lattice Abelian gauge models with charged $N$-component scalar fields and $SO(N)$ symmetry","abstract":"We consider a three-dimensional lattice Abelian Higgs gauge model for a charged $N$-component scalar field ${\\phi}$, which is invariant under $SO(N)$ global transformations for generic values of the parameters. We focus on the strong-coupling regime, in which the kinetic Hamiltonian term for the gauge field is a small perturbation, which is irrelevant for the critical behavior. The Hamiltonian depends on a parameter $v$ which determines the global symmetry of the model and the symmetry of the low-temperature phases. We present renormalization-group predictions, based on a Landau-Ginzburg-Wilson effective description that relies on the identification of the appropriate order parameter and on the symmetry-breaking patterns that occur at the strong-coupling phase transitions. For $v=0$, the global symmetry group of the model is $SU(N)$; the corresponding model may undergo continuous transitions only for $N=2$. For $v\\not=0$, i.e., in the $SO(N)$ symmetric case, continuous transitions (in the Heisenberg universality class) are possible also for $N=3$ and 4. We perform Monte Carlo simulations for $N=2,3,4,6$, to verify the renormalization-group predictions. Finite-size scaling analyses of the numerical data are in full agreement.","sentences":["We consider a three-dimensional lattice Abelian Higgs gauge model for a charged $N$-component scalar field ${\\phi}$, which is invariant under $SO(N)$ global transformations for generic values of the parameters.","We focus on the strong-coupling regime, in which the kinetic Hamiltonian term for the gauge field is a small perturbation, which is irrelevant for the critical behavior.","The Hamiltonian depends on a parameter $v$ which determines the global symmetry of the model and the symmetry of the low-temperature phases.","We present renormalization-group predictions, based on a Landau-Ginzburg-Wilson effective description that relies on the identification of the appropriate order parameter and on the symmetry-breaking patterns that occur at the strong-coupling phase transitions.","For $v=0$, the global symmetry group of the model is $SU(N)$; the corresponding model may undergo continuous transitions only for $N=2$. For $v\\not=0$, i.e., in the $SO(N)$ symmetric case, continuous transitions (in the Heisenberg universality class) are possible also for $N=3$ and 4.","We perform Monte Carlo simulations for $N=2,3,4,6$, to verify the renormalization-group predictions.","Finite-size scaling analyses of the numerical data are in full agreement."],"url":"http://arxiv.org/abs/2403.12758v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-19 14:27:13","title":"Robust Numerical Methods for Nonlinear Regression","abstract":"Many scientific and engineering applications require fitting regression models that are nonlinear in the parameters. Advances in computer hardware and software in recent decades have made it easier to fit such models. Relative to fitting regression models that are linear in the parameters, however, fitting nonlinear regression models is more complicated. In particular, software like the $\\texttt{nls}$ R function requires care in how the model is parameterized and how initial values are chosen for the maximum likelihood iterations. Often special diagnostics are needed to detect and suggest approaches for dealing with identifiability problems that can arise with such model fitting. When using Bayesian inference, there is the added complication of having to specify (often noninformative or weakly informative) prior distributions. Generally, the details for these tasks must be determined for each new nonlinear regression model. This paper provides a step-by-step procedure for specifying these details for any appropriate nonlinear regression model. Following the procedure will result in a numerically robust algorithm for fitting the nonlinear regression model. We illustrate the methods with three different nonlinear models that are used in the analysis of experimental fatigue data and we include two detailed numerical examples.","sentences":["Many scientific and engineering applications require fitting regression models that are nonlinear in the parameters.","Advances in computer hardware and software in recent decades have made it easier to fit such models.","Relative to fitting regression models that are linear in the parameters, however, fitting nonlinear regression models is more complicated.","In particular, software like the $\\texttt{nls}$ R function requires care in how the model is parameterized and how initial values are chosen for the maximum likelihood iterations.","Often special diagnostics are needed to detect and suggest approaches for dealing with identifiability problems that can arise with such model fitting.","When using Bayesian inference, there is the added complication of having to specify (often noninformative or weakly informative) prior distributions.","Generally, the details for these tasks must be determined for each new nonlinear regression model.","This paper provides a step-by-step procedure for specifying these details for any appropriate nonlinear regression model.","Following the procedure will result in a numerically robust algorithm for fitting the nonlinear regression model.","We illustrate the methods with three different nonlinear models that are used in the analysis of experimental fatigue data and we include two detailed numerical examples."],"url":"http://arxiv.org/abs/2403.12759v1","category":"stat.ME"}
{"created":"2024-03-19 14:17:31","title":"Developing Algorithms for the Internet of Flying Things Through Environments With Varying Degrees of Realism","abstract":"This work discusses the benefits of having multiple simulated environments with different degrees of realism for the development of algorithms in scenarios populated by autonomous nodes capable of communication and mobility. This approach aids the development experience and generates robust algorithms. It also proposes GrADyS-SIM NextGen as a solution that enables development on a single programming language and toolset over multiple environments with varying levels of realism. Finally, we illustrate the usefulness of this approach with a toy problem that makes use of the simulation framework, taking advantage of the proposed environments to iteratively develop a robust solution.","sentences":["This work discusses the benefits of having multiple simulated environments with different degrees of realism for the development of algorithms in scenarios populated by autonomous nodes capable of communication and mobility.","This approach aids the development experience and generates robust algorithms.","It also proposes GrADyS-SIM NextGen as a solution that enables development on a single programming language and toolset over multiple environments with varying levels of realism.","Finally, we illustrate the usefulness of this approach with a toy problem that makes use of the simulation framework, taking advantage of the proposed environments to iteratively develop a robust solution."],"url":"http://arxiv.org/abs/2403.12753v1","category":"cs.NI"}
{"created":"2024-03-19 14:15:59","title":"Oscillatory integrals and weighted gradient flows","abstract":"We investigate estimating scalar oscillatory integrals by integrating by parts in directions based on $(x_1 \\partial_{x_1} f(x) ,..., x_n \\partial_{x_n}f(x))$, where $f(x)$ is the phase function. We prove a theorem which provides estimates that are uniform with respect to linear perturbations of the phase and investigate some consequences. When the phase function is quasi-homogeneous the theorem gives estimates for the associated surface measure Fourier transforms that are generally not too far off from being sharp. In addition, the theorem provides a new proof, up to endpoints, that the well-known oscillatory integral estimates of Varchenko [V] when the Newton polyhedron of the phase function is nondegenerate extend to corresponding bounds for surface measure Fourier transforms when the index is less than $\\frac{1}{2}$. A sharp version of this was originally proven in [G2].","sentences":["We investigate estimating scalar oscillatory integrals by integrating by parts in directions based on $(x_1 \\partial_{x_1} f(x) ,..., x_n \\partial_{x_n}f(x))$, where $f(x)$ is the phase function.","We prove a theorem which provides estimates that are uniform with respect to linear perturbations of the phase and investigate some consequences.","When the phase function is quasi-homogeneous the theorem gives estimates for the associated surface measure Fourier transforms that are generally not too far off from being sharp.","In addition, the theorem provides a new proof, up to endpoints, that the well-known oscillatory integral estimates of Varchenko [V] when the Newton polyhedron of the phase function is nondegenerate extend to corresponding bounds for surface measure Fourier transforms when the index is less than $\\frac{1}{2}$. A sharp version of this was originally proven in [G2]."],"url":"http://arxiv.org/abs/2403.12751v1","category":"math.CA"}
{"created":"2024-03-19 14:15:56","title":"Galois theory and homology in quasi-abelian functor categories","abstract":"Given a finite category T, we consider the functor category [T,A], where A can in particular be any quasi-abelian category. Examples of quasi-abelian categories are given by any abelian category but also by non-exact additive categories as the categories of torsion(-free) abelian groups, topological abelian groups, locally compact abelian groups, Banach spaces and Fr\\'echet spaces. In this situation, the categories of various internal categorical structures in A, such as the categories of internal n-fold groupoids, are equivalent to functor categories [T,A] for a suitable category T. For a replete full subcategory S of T, we define F to be the full subcategory of [T,A] whose objects are given by the functors G with G(X)=0 for all objects X not in S. We prove that F is a torsion-free Birkhoff subcategory of [T,A]. This allows us to study (higher) central extensions from categorical Galois theory in [T,A] with respect to F and generalized Hopf formulae for homology.","sentences":["Given a finite category T, we consider the functor category [T,A], where A can in particular be any quasi-abelian category.","Examples of quasi-abelian categories are given by any abelian category but also by non-exact additive categories as the categories of torsion(-free) abelian groups, topological abelian groups, locally compact abelian groups, Banach spaces and Fr\\'echet spaces.","In this situation, the categories of various internal categorical structures in A, such as the categories of internal n-fold groupoids, are equivalent to functor categories","[T,A] for a suitable category T. For a replete full subcategory S of T, we define F to be the full subcategory of [T,A] whose objects are given by the functors G with G(X)=0 for all objects X not in S.","We prove that F is a torsion-free Birkhoff subcategory of [T,A].","This allows us to study (higher) central extensions from categorical Galois theory in [T,A] with respect to F and generalized Hopf formulae for homology."],"url":"http://arxiv.org/abs/2403.12750v1","category":"math.CT"}
{"created":"2024-03-19 14:11:26","title":"Building Brain Tumor Segmentation Networks with User-Assisted Filter Estimation and Selection","abstract":"Brain tumor image segmentation is a challenging research topic in which deep-learning models have presented the best results. However, the traditional way of training those models from many pre-annotated images leaves several unanswered questions. Hence methodologies, such as Feature Learning from Image Markers (FLIM), have involved an expert in the learning loop to reduce human effort in data annotation and build models sufficiently deep for a given problem. FLIM has been successfully used to create encoders, estimating the filters of all convolutional layers from patches centered at marker voxels. In this work, we present Multi-Step (MS) FLIM - a user-assisted approach to estimating and selecting the most relevant filters from multiple FLIM executions. MS-FLIM is used only for the first convolutional layer, and the results already indicate improvement over FLIM. For evaluation, we build a simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma segmentation using T1Gd and FLAIR MRI scans, varying the encoder's training method, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two datasets. The results show that the sU-Net based on MS-FLIM outperforms the other training methods and achieves effectiveness within the standard deviations of the SOTA models.","sentences":["Brain tumor image segmentation is a challenging research topic in which deep-learning models have presented the best results.","However, the traditional way of training those models from many pre-annotated images leaves several unanswered questions.","Hence methodologies, such as Feature Learning from Image Markers (FLIM), have involved an expert in the learning loop to reduce human effort in data annotation and build models sufficiently deep for a given problem.","FLIM has been successfully used to create encoders, estimating the filters of all convolutional layers from patches centered at marker voxels.","In this work, we present Multi-Step (MS) FLIM - a user-assisted approach to estimating and selecting the most relevant filters from multiple FLIM executions.","MS-FLIM is used only for the first convolutional layer, and the results already indicate improvement over FLIM.","For evaluation, we build a simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma segmentation using T1Gd and FLAIR MRI scans, varying the encoder's training method, using FLIM, MS-FLIM, and backpropagation algorithm.","Also, we compared these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two datasets.","The results show that the sU-Net based on MS-FLIM outperforms the other training methods and achieves effectiveness within the standard deviations of the SOTA models."],"url":"http://arxiv.org/abs/2403.12748v1","category":"cs.CV"}
{"created":"2024-03-19 14:07:28","title":"Instructing Large Language Models to Identify and Ignore Irrelevant Conditions","abstract":"Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions. Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. However, they were seriously confused by the irrelevant conditions, resulting in low accuracy. In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions. It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question. Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths. Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot reasoning. We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement. We conduct extensive experiments on eight MWP datasets. I$^3$C can be combined with any CoT prompting methods to improve the performance of solving MWPs. Notably, with GPT-3.5-Turbo and I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1. Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.","sentences":["Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.","Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs.","However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.","In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.","It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.","Then it prompts LLMs to verify the irrelevant conditions.","Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.","Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot reasoning.","We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement.","We conduct extensive experiments on eight MWP datasets.","I$^3$C can be combined with any CoT prompting methods to improve the performance of solving MWPs.","Notably, with GPT-3.5-Turbo and I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1.","Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/."],"url":"http://arxiv.org/abs/2403.12744v1","category":"cs.CL"}
{"created":"2024-03-19 14:02:13","title":"Towards Controllable Face Generation with Semantic Latent Diffusion Models","abstract":"Semantic Image Synthesis (SIS) is among the most popular and effective techniques in the field of face generation and editing, thanks to its good generation quality and the versatility is brings along. Recent works attempted to go beyond the standard GAN-based framework, and started to explore Diffusion Models (DMs) for this task as these stand out with respect to GANs in terms of both quality and diversity. On the other hand, DMs lack in fine-grained controllability and reproducibility. To address that, in this paper we propose a SIS framework based on a novel Latent Diffusion Model architecture for human face generation and editing that is both able to reproduce and manipulate a real reference image and generate diversity-driven results. The proposed system utilizes both SPADE normalization and cross-attention layers to merge shape and style information and, by doing so, allows for a precise control over each of the semantic parts of the human face. This was not possible with previous methods in the state of the art. Finally, we performed an extensive set of experiments to prove that our model surpasses current state of the art, both qualitatively and quantitatively.","sentences":["Semantic Image Synthesis (SIS) is among the most popular and effective techniques in the field of face generation and editing, thanks to its good generation quality and the versatility is brings along.","Recent works attempted to go beyond the standard GAN-based framework, and started to explore Diffusion Models (DMs) for this task as these stand out with respect to GANs in terms of both quality and diversity.","On the other hand, DMs lack in fine-grained controllability and reproducibility.","To address that, in this paper we propose a SIS framework based on a novel Latent Diffusion Model architecture for human face generation and editing that is both able to reproduce and manipulate a real reference image and generate diversity-driven results.","The proposed system utilizes both SPADE normalization and cross-attention layers to merge shape and style information and, by doing so, allows for a precise control over each of the semantic parts of the human face.","This was not possible with previous methods in the state of the art.","Finally, we performed an extensive set of experiments to prove that our model surpasses current state of the art, both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2403.12743v1","category":"cs.CV"}
{"created":"2024-03-19 14:00:15","title":"Controllability and diffeomorphism groups on manifolds with boundary","abstract":"In this article we consider diffeomorphism groups of manifolds with smooth boundary. We show that the diffeomorphism groups of the manifold and its boundary fit into a short exact sequence which admits local sections. In other words, they form an infinite-dimensional fibre bundle. Manifolds with boundary are of interest in numerical analysis and with a view towards applications in machine learning we establish controllability results for families of vector fields. This generalises older results due to Agrachev and Caponigro in the boundary-less case. Our results show in particular that the diffeomorphism group of a manifold with smooth boundary is generated by the image of the exponential map.","sentences":["In this article we consider diffeomorphism groups of manifolds with smooth boundary.","We show that the diffeomorphism groups of the manifold and its boundary fit into a short exact sequence which admits local sections.","In other words, they form an infinite-dimensional fibre bundle.","Manifolds with boundary are of interest in numerical analysis and with a view towards applications in machine learning we establish controllability results for families of vector fields.","This generalises older results due to Agrachev and Caponigro in the boundary-less case.","Our results show in particular that the diffeomorphism group of a manifold with smooth boundary is generated by the image of the exponential map."],"url":"http://arxiv.org/abs/2403.12742v1","category":"math.DG"}
{"created":"2024-03-19 13:59:09","title":"Two-level systems and harmonic excitations in a mean-field anharmonic quantum glass","abstract":"Structural glasses display at low temperature a set of anomalies in thermodynamic observables. The prominent example is the linear-in-temperature scaling of the specific heat, at odds with the Debye cubic scaling found in crystals, due to acoustic phonons. Such an excess of specific heat in amorphous solids is thought of arising from phenomenological soft excitations dubbed tunneling two-level systems (TTLS). Their nature as well as their statistical properties remain elusive from a first-principle viewpoint. In this work we investigate the canonically quantized version of the KHGPS model, a mean-field glass model of coupled anharmonic oscillators, across its phase diagram, with an emphasis on the specific heat. The thermodynamics is solved in a semiclassical expansion. We show that in the replica-symmetric region of the model, up to the marginal glass transition line where replica symmetry gets continuously broken, a disordered version of the Debye approximation holds: the specific heat is dominated by harmonic vibrational excitations inducing a power-law scaling at the transition, ruled by random matrix theory. This mechanism generalizes a previous semiclassical argument in the literature. We then study the marginal glass phase where the semiclassical expansion becomes non-perturbative due to the emergence of instantons that overcome disordered Debye behavior. Inside the glass phase, a variational solution to the instanton approach provides the prevailing excitations as TTLS, which generate a linear specific heat. This phase thus hosts a mix of TTLS and harmonic excitations generated by interactions. We finally suggest to go beyond the variational approximation through an analogy with the spin-boson model.","sentences":["Structural glasses display at low temperature a set of anomalies in thermodynamic observables.","The prominent example is the linear-in-temperature scaling of the specific heat, at odds with the Debye cubic scaling found in crystals, due to acoustic phonons.","Such an excess of specific heat in amorphous solids is thought of arising from phenomenological soft excitations dubbed tunneling two-level systems (TTLS).","Their nature as well as their statistical properties remain elusive from a first-principle viewpoint.","In this work we investigate the canonically quantized version of the KHGPS model, a mean-field glass model of coupled anharmonic oscillators, across its phase diagram, with an emphasis on the specific heat.","The thermodynamics is solved in a semiclassical expansion.","We show that in the replica-symmetric region of the model, up to the marginal glass transition line where replica symmetry gets continuously broken, a disordered version of the Debye approximation holds: the specific heat is dominated by harmonic vibrational excitations inducing a power-law scaling at the transition, ruled by random matrix theory.","This mechanism generalizes a previous semiclassical argument in the literature.","We then study the marginal glass phase where the semiclassical expansion becomes non-perturbative due to the emergence of instantons that overcome disordered Debye behavior.","Inside the glass phase, a variational solution to the instanton approach provides the prevailing excitations as TTLS, which generate a linear specific heat.","This phase thus hosts a mix of TTLS and harmonic excitations generated by interactions.","We finally suggest to go beyond the variational approximation through an analogy with the spin-boson model."],"url":"http://arxiv.org/abs/2403.12740v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-19 13:56:34","title":"Path Planning in a dynamic environment using Spherical Particle Swarm Optimization","abstract":"Efficiently planning an Unmanned Aerial Vehicle (UAV) path is crucial, especially in dynamic settings where potential threats are prevalent. A Dynamic Path Planner (DPP) for UAV using the Spherical Vector-based Particle Swarm Optimisation (SPSO) technique is proposed in this study. The UAV is supposed to go from a starting point to an end point through an optimal path according to some flight criteria. Path length, Safety, Attitude and Path Smoothness are all taken into account upon deciding how an optimal path should be. The path is constructed as a set of way-points that stands as re-planning checkpoints. At each path way-point, threats are allowed some constrained random motion, where their exact positions are updated and fed to the SPSO-solver. Four test scenarios are carried out using real digital elevation models. Each test gives different priorities to path length and safety, in order to show how well the SPSO-DPP is capable of generating a safe yet efficient path segments. Finally, a comparison is made to reveal the persistent overall superior performance of the SPSO, in a dynamic environment, over both the Particle Swarm Optimisation (PSO) and the Genetic Algorithm (GA). The methods are compared directly, by averaging costs over multiple runs, and by considering different challenging levels of obstacle motion. SPSO outperformed both PSO and GA, showcasing cost reductions ranging from 330\\% to 675\\% compared to both algorithms.","sentences":["Efficiently planning an Unmanned Aerial Vehicle (UAV) path is crucial, especially in dynamic settings where potential threats are prevalent.","A Dynamic Path Planner (DPP) for UAV using the Spherical Vector-based Particle Swarm Optimisation (SPSO) technique is proposed in this study.","The UAV is supposed to go from a starting point to an end point through an optimal path according to some flight criteria.","Path length, Safety, Attitude and Path Smoothness are all taken into account upon deciding how an optimal path should be.","The path is constructed as a set of way-points that stands as re-planning checkpoints.","At each path way-point, threats are allowed some constrained random motion, where their exact positions are updated and fed to the SPSO-solver.","Four test scenarios are carried out using real digital elevation models.","Each test gives different priorities to path length and safety, in order to show how well the SPSO-DPP is capable of generating a safe yet efficient path segments.","Finally, a comparison is made to reveal the persistent overall superior performance of the SPSO, in a dynamic environment, over both the Particle Swarm Optimisation (PSO) and the Genetic Algorithm (GA).","The methods are compared directly, by averaging costs over multiple runs, and by considering different challenging levels of obstacle motion.","SPSO outperformed both PSO and GA, showcasing cost reductions ranging from 330\\% to 675\\% compared to both algorithms."],"url":"http://arxiv.org/abs/2403.12739v1","category":"cs.NE"}
{"created":"2024-03-19 13:54:14","title":"Navigating Compiler Errors with AI Assistance -- A Study of GPT Hints in an Introductory Programming Course","abstract":"We examined the efficacy of AI-assisted learning in an introductory programming course at the university level by using a GPT-4 model to generate personalized hints for compiler errors within a platform for automated assessment of programming assignments. The control group had no access to GPT hints. In the experimental condition GPT hints were provided when a compiler error was detected, for the first half of the problems in each module. For the latter half of the module, hints were disabled. Students highly rated the usefulness of GPT hints. In affect surveys, the experimental group reported significantly higher levels of focus and lower levels of confrustion (confusion and frustration) than the control group. For the six most commonly occurring error types we observed mixed results in terms of performance when access to GPT hints was enabled for the experimental group. However, in the absence of GPT hints, the experimental group's performance surpassed the control group for five out of the six error types.","sentences":["We examined the efficacy of AI-assisted learning in an introductory programming course at the university level by using a GPT-4 model to generate personalized hints for compiler errors within a platform for automated assessment of programming assignments.","The control group had no access to GPT hints.","In the experimental condition GPT hints were provided when a compiler error was detected, for the first half of the problems in each module.","For the latter half of the module, hints were disabled.","Students highly rated the usefulness of GPT hints.","In affect surveys, the experimental group reported significantly higher levels of focus and lower levels of confrustion (confusion and frustration) than the control group.","For the six most commonly occurring error types we observed mixed results in terms of performance when access to GPT hints was enabled for the experimental group.","However, in the absence of GPT hints, the experimental group's performance surpassed the control group for five out of the six error types."],"url":"http://arxiv.org/abs/2403.12737v1","category":"cs.SE"}
{"created":"2024-03-19 13:49:31","title":"Exact and Heuristic Computation of the Scanwidth of Directed Acyclic Graphs","abstract":"To measure the tree-likeness of a directed acyclic graph (DAG), a new width parameter that considers the directions of the arcs was recently introduced: scanwidth. We present the first algorithm that efficiently computes the exact scanwidth of general DAGs. For DAGs with one root and scanwidth $k$ it runs in $O(k \\cdot n^k \\cdot m)$ time. The algorithm also functions as an FPT algorithm with complexity $O(2^{4 \\ell - 1} \\cdot \\ell \\cdot n + n^2)$ for phylogenetic networks of level-$\\ell$, a type of DAG used to depict evolutionary relationships among species. Our algorithm performs well in practice, being able to compute the scanwidth of synthetic networks up to 30 reticulations and 100 leaves within 500 seconds. Furthermore, we propose a heuristic that obtains an average practical approximation ratio of 1.5 on these networks. While we prove that the scanwidth is bounded from below by the treewidth of the underlying undirected graph, experiments suggest that for networks the parameters are close in practice.","sentences":["To measure the tree-likeness of a directed acyclic graph (DAG), a new width parameter that considers the directions of the arcs was recently introduced: scanwidth.","We present the first algorithm that efficiently computes the exact scanwidth of general DAGs.","For DAGs with one root and scanwidth $k$ it runs in $O(k \\cdot n^k \\cdot m)$ time.","The algorithm also functions as an FPT algorithm with complexity $O(2^{4 \\ell - 1} \\cdot \\ell \\cdot n + n^2)$ for phylogenetic networks of level-$\\ell$, a type of DAG used to depict evolutionary relationships among species.","Our algorithm performs well in practice, being able to compute the scanwidth of synthetic networks up to 30 reticulations and 100 leaves within 500 seconds.","Furthermore, we propose a heuristic that obtains an average practical approximation ratio of 1.5 on these networks.","While we prove that the scanwidth is bounded from below by the treewidth of the underlying undirected graph, experiments suggest that for networks the parameters are close in practice."],"url":"http://arxiv.org/abs/2403.12734v1","category":"cs.DS"}
{"created":"2024-03-19 13:47:35","title":"Tighter Confidence Bounds for Sequential Kernel Regression","abstract":"Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and comparable computational cost. Our new confidence bounds can be used as a generic tool to design improved algorithms for other kernelised learning and decision-making problems.","sentences":["Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions.","In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms.","Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees.","In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression.","We prove that our new confidence bounds are always tighter than existing ones in this setting.","We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history.","When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and comparable computational cost.","Our new confidence bounds can be used as a generic tool to design improved algorithms for other kernelised learning and decision-making problems."],"url":"http://arxiv.org/abs/2403.12732v1","category":"stat.ML"}
{"created":"2024-03-19 13:46:59","title":"Extension of the characterization method for non-Gaussianity in gravitational wave detector with statistical hypothesis test","abstract":"In gravitational wave astronomy, non-Gaussian noise, such as scattered light noise disturbs stable interferometer operation, limiting the interferometer's sensitivity, and reducing the reliability of the analyses. In scattered light noise, the non-Gaussian noise dominates the sensitivity in a low frequency range of less than a few hundred Hz, which is sensitive to gravitational waves from compact binary coalescence. This non-Gaussian noise prevents reliable parameter estimation, since several analysis methods are optimized only for Gaussian noise. Therefore, identifying data contaminated by non-Gaussian noise is important. In this work, we extended the conventional method to evaluate non-Gaussian noise, Rayleigh statistic, by using a statistical hypothesis test to determine a threshold for non-Gaussian noise. First, we estimated the distribution of the Rayleigh statistic against Gaussian noise, called the background distribution, and validated that our extension serves as the hypothetical test. Moreover, we investigated the detection efficiency by assuming two non-Gaussian noise models. For example, for the model with strong scattered light noise, the true positive rate was always above 0.7 when the significance level was 0.05. The results showed that our extension can contribute to an initial detection of non-Gaussian noise and lead to further investigation of the origin of the non-Gaussian noise.","sentences":["In gravitational wave astronomy, non-Gaussian noise, such as scattered light noise disturbs stable interferometer operation, limiting the interferometer's sensitivity, and reducing the reliability of the analyses.","In scattered light noise, the non-Gaussian noise dominates the sensitivity in a low frequency range of less than a few hundred Hz, which is sensitive to gravitational waves from compact binary coalescence.","This non-Gaussian noise prevents reliable parameter estimation, since several analysis methods are optimized only for Gaussian noise.","Therefore, identifying data contaminated by non-Gaussian noise is important.","In this work, we extended the conventional method to evaluate non-Gaussian noise, Rayleigh statistic, by using a statistical hypothesis test to determine a threshold for non-Gaussian noise.","First, we estimated the distribution of the Rayleigh statistic against Gaussian noise, called the background distribution, and validated that our extension serves as the hypothetical test.","Moreover, we investigated the detection efficiency by assuming two non-Gaussian noise models.","For example, for the model with strong scattered light noise, the true positive rate was always above 0.7 when the significance level was 0.05.","The results showed that our extension can contribute to an initial detection of non-Gaussian noise and lead to further investigation of the origin of the non-Gaussian noise."],"url":"http://arxiv.org/abs/2403.12731v1","category":"gr-qc"}
{"created":"2024-03-19 13:45:34","title":"What Does Evaluation of Explainable Artificial Intelligence Actually Tell Us? A Case for Compositional and Contextual Validation of XAI Building Blocks","abstract":"Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed -- accounting for a diverse range of their engineering and social aspects -- in view of the anticipated use case.","sentences":["Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging.","In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols.","While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs.","Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed -- accounting for a diverse range of their engineering and social aspects -- in view of the anticipated use case."],"url":"http://arxiv.org/abs/2403.12730v1","category":"cs.HC"}
{"created":"2024-03-19 13:42:16","title":"Small Distance Increment Method for Measuring Complex Permittivity With mmWave Radar","abstract":"Measuring the complex permittivity of material is essential in many scenarios such as quality check and component analysis. Generally, measurement methods for characterizing the material are based on the usage of vector network analyzer, which is large and not easy for on-site measurement, especially in high frequency range such as millimeter wave (mmWave). In addition, some measurement methods require the destruction of samples, which is not suitable for non-destructive inspection. In this work, a small distance increment (SDI) method is proposed to non-destructively measure the complex permittivity of material. In SDI, the transmitter and receiver are formed as the monostatic radar, which is facing towards the material under test (MUT). During the measurement, the distance between radar and MUT changes with small increments and the signals are recorded at each position. A mathematical model is formulated to depict the relationship among the complex permittivity, distance increment, and measured signals. By fitting the model, the complex permittivity of MUT is estimated. To implement and evaluate the proposed SDI method, a commercial off-the-shelf mmWave radar is utilized and the measurement system is developed. Then, the evaluation was carried out on the acrylic plate. With the proposed method, the estimated complex permittivity of acrylic plate shows good agreement with the literature values, demonstrating the efficacy of SDI method for characterizing the complex permittivity of material.","sentences":["Measuring the complex permittivity of material is essential in many scenarios such as quality check and component analysis.","Generally, measurement methods for characterizing the material are based on the usage of vector network analyzer, which is large and not easy for on-site measurement, especially in high frequency range such as millimeter wave (mmWave).","In addition, some measurement methods require the destruction of samples, which is not suitable for non-destructive inspection.","In this work, a small distance increment (SDI) method is proposed to non-destructively measure the complex permittivity of material.","In SDI, the transmitter and receiver are formed as the monostatic radar, which is facing towards the material under test (MUT).","During the measurement, the distance between radar and MUT changes with small increments and the signals are recorded at each position.","A mathematical model is formulated to depict the relationship among the complex permittivity, distance increment, and measured signals.","By fitting the model, the complex permittivity of MUT is estimated.","To implement and evaluate the proposed SDI method, a commercial off-the-shelf mmWave radar is utilized and the measurement system is developed.","Then, the evaluation was carried out on the acrylic plate.","With the proposed method, the estimated complex permittivity of acrylic plate shows good agreement with the literature values, demonstrating the efficacy of SDI method for characterizing the complex permittivity of material."],"url":"http://arxiv.org/abs/2403.12726v1","category":"eess.SP"}
{"created":"2024-03-19 13:41:11","title":"Python Fuzzing for Trustworthy Machine Learning Frameworks","abstract":"Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems. Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software. Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python. We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is integrated in GitLab CI. To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, and related projects such as h5py. Applying our dynamic analysis pipeline to these targets, we were able to discover 3 new bugs and propose fixes for them.","sentences":["Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems.","Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software.","Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python.","We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset.","Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection.","Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly.","Furthermore, the proposed pipeline is integrated in GitLab CI.","To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, and related projects such as h5py.","Applying our dynamic analysis pipeline to these targets, we were able to discover 3 new bugs and propose fixes for them."],"url":"http://arxiv.org/abs/2403.12723v1","category":"cs.CR"}
{"created":"2024-03-19 13:29:44","title":"Shared Autonomy via Variable Impedance Control and Virtual Potential Fields for Encoding Human Demonstration","abstract":"This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture. For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner. Therefore, two key components are addressed in this work: motion generation and shared autonomy. We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance. Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority. Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces. System passivity is ensured by an energy-tank based task passivation strategy. The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot.","sentences":["This article introduces a framework for complex human-robot collaboration tasks, such as the co-manufacturing of furniture.","For these tasks, it is essential to encode tasks from human demonstration and reproduce these skills in a compliant and safe manner.","Therefore, two key components are addressed in this work: motion generation and shared autonomy.","We propose a motion generator based on a time-invariant potential field, capable of encoding wrench profiles, complex and closed-loop trajectories, and additionally incorporates obstacle avoidance.","Additionally, the paper addresses shared autonomy (SA) which enables synergetic collaboration between human operators and robots by dynamically allocating authority.","Variable impedance control (VIC) and force control are employed, where impedance and wrench are adapted based on the human-robot autonomy factor derived from interaction forces.","System passivity is ensured by an energy-tank based task passivation strategy.","The framework's efficacy is validated through simulations and an experimental study employing a Franka Emika Research 3 robot."],"url":"http://arxiv.org/abs/2403.12720v1","category":"cs.RO"}
{"created":"2024-03-19 13:28:03","title":"Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis","abstract":"Early detection of Alzheimer's disease's precursor stages is imperative for significantly enhancing patient outcomes and quality of life. This challenge is tackled through a semi-supervised multi-modal diagnosis framework. In particular, we introduce a new hypergraph framework that enables higher-order relations between multi-modal data, while utilising minimal labels. We first introduce a bilevel hypergraph optimisation framework that jointly learns a graph augmentation policy and a semi-supervised classifier. This dual learning strategy is hypothesised to enhance the robustness and generalisation capabilities of the model by fostering new pathways for information propagation. Secondly, we introduce a novel strategy for generating pseudo-labels more effectively via a gradient-driven flow. Our experimental results demonstrate the superior performance of our framework over current techniques in diagnosing Alzheimer's disease.","sentences":["Early detection of Alzheimer's disease's precursor stages is imperative for significantly enhancing patient outcomes and quality of life.","This challenge is tackled through a semi-supervised multi-modal diagnosis framework.","In particular, we introduce a new hypergraph framework that enables higher-order relations between multi-modal data, while utilising minimal labels.","We first introduce a bilevel hypergraph optimisation framework that jointly learns a graph augmentation policy and a semi-supervised classifier.","This dual learning strategy is hypothesised to enhance the robustness and generalisation capabilities of the model by fostering new pathways for information propagation.","Secondly, we introduce a novel strategy for generating pseudo-labels more effectively via a gradient-driven flow.","Our experimental results demonstrate the superior performance of our framework over current techniques in diagnosing Alzheimer's disease."],"url":"http://arxiv.org/abs/2403.12719v1","category":"cs.LG"}
{"created":"2024-03-19 13:26:34","title":"A New Reduction Method from Multivariate Polynomials to Univariate Polynomials","abstract":"Polynomial multiplication is a fundamental problem in symbolic computation. There are efficient methods for the multiplication of two univariate polynomials. However, there is rarely efficiently nontrivial method for the multiplication of two multivariate polynomials. Therefore, we consider a new multiplication mechanism that involves a) reversibly reducing multivariate polynomials into univariate polynomials, b) calculating the product of the derived univariate polynomials by the Toom-Cook or FFT algorithm, and c) correctly recovering the product of multivariate polynomials from the product of two univariate polynomials. This work focuses on step a), expecting the degrees of the derived univariate polynomials to be as small as possible. We propose iterative Kronecker substitution, where smaller substitution exponents are selected instead of standard Kronecker substitution. We also apply the Chinese remainder theorem to polynomial reduction and find its advantages in some cases. Afterwards, we provide a hybrid reduction combining the advantages of both reduction methods. Moreover, we compare these reduction methods in terms of lower and upper bounds of the degree of the product of two derived univariate polynomials, and their computational complexities. With randomly generated multivariate polynomials, experiments show that the degree of the product of two univariate polynomials derived from the hybrid reduction can be reduced even to approximately 3% that resulting from the standard Kronecker substitution, implying an efficient subsequent multiplication of two univariate polynomials.","sentences":["Polynomial multiplication is a fundamental problem in symbolic computation.","There are efficient methods for the multiplication of two univariate polynomials.","However, there is rarely efficiently nontrivial method for the multiplication of two multivariate polynomials.","Therefore, we consider a new multiplication mechanism that involves a) reversibly reducing multivariate polynomials into univariate polynomials, b) calculating the product of the derived univariate polynomials by the Toom-Cook or FFT algorithm, and c) correctly recovering the product of multivariate polynomials from the product of two univariate polynomials.","This work focuses on step a), expecting the degrees of the derived univariate polynomials to be as small as possible.","We propose iterative Kronecker substitution, where smaller substitution exponents are selected instead of standard Kronecker substitution.","We also apply the Chinese remainder theorem to polynomial reduction and find its advantages in some cases.","Afterwards, we provide a hybrid reduction combining the advantages of both reduction methods.","Moreover, we compare these reduction methods in terms of lower and upper bounds of the degree of the product of two derived univariate polynomials, and their computational complexities.","With randomly generated multivariate polynomials, experiments show that the degree of the product of two univariate polynomials derived from the hybrid reduction can be reduced even to approximately 3% that resulting from the standard Kronecker substitution, implying an efficient subsequent multiplication of two univariate polynomials."],"url":"http://arxiv.org/abs/2403.12716v1","category":"cs.CC"}
{"created":"2024-03-19 13:22:12","title":"On the use of the cumulant generating function for inference on time series","abstract":"We introduce innovative inference procedures for analyzing time series data. Our methodology enables density approximation and composite hypothesis testing based on Whittle's estimator, a widely applied M-estimator in the frequency domain. Its core feature involves the (general Legendre transform of the) cumulant generating function of the Whittle likelihood score, as obtained using an approximated distribution of the periodogram ordinates. We present a testing algorithm that significantly expands the applicability of the state-of-the-art saddlepoint test, while maintaining the numerical accuracy of the saddlepoint approximation. Additionally, we demonstrate connections between our findings and three other prevalent frequency domain approaches: the bootstrap, empirical likelihood, and exponential tilting. Numerical examples using both simulated and real data illustrate the advantages and accuracy of our methodology.","sentences":["We introduce innovative inference procedures for analyzing time series data.","Our methodology enables density approximation and composite hypothesis testing based on Whittle's estimator, a widely applied M-estimator in the frequency domain.","Its core feature involves the (general Legendre transform of the) cumulant generating function of the Whittle likelihood score, as obtained using an approximated distribution of the periodogram ordinates.","We present a testing algorithm that significantly expands the applicability of the state-of-the-art saddlepoint test, while maintaining the numerical accuracy of the saddlepoint approximation.","Additionally, we demonstrate connections between our findings and three other prevalent frequency domain approaches: the bootstrap, empirical likelihood, and exponential tilting.","Numerical examples using both simulated and real data illustrate the advantages and accuracy of our methodology."],"url":"http://arxiv.org/abs/2403.12714v1","category":"stat.ME"}
{"created":"2024-03-19 13:21:08","title":"Spanning Euler Tours in Hypergraphs","abstract":"Motivated by generalizations of de Bruijn cycles to various combinatorial structures (Chung, Diaconis, and Graham), we study various Euler tours in set systems. Let $\\mathcal{G}$ be a hypergraph whose corank and rank are $c\\geq 3$ and $k$, respetively. The minimum $t$-degree of $\\mathcal{G}$ is the fewest number of edges containing every $t$-subset of vertices. An Euler tour (family, respectively) in $\\mathcal{G}$ is a (family of, respectively) closed walk(s) that (jointly, respectively) traverses each edge of $\\mathcal{G}$ exactly once. An Euler tour is spanning if it traverses all the vertices of $\\mathcal{G}$. We show that $\\mathcal{G}$ has an Euler family if its incidence graph is $(1+\\lceil k/c \\rceil)$-edge-connected. Provided that the number of vertices of $\\mathcal{G}$ meets a reasonable lower bound, and either $2$-degree is at least $k$ or $t$-degree is at least one for $t\\geq 3$, we show that $\\mathcal{G}$ has a spanning Euler tour. To exhibit the usefulness of our results, we solve a number of open problems concerning ordering blocks of a design (these have applications in other fields such as erasure-correcting codes). Answering a question of Horan and Hurlbert, we show that a Steiner quadruple system of order $n$ has a (spanning) Euler tour if and only if $n\\geq 8$ and $n\\equiv 2,4 \\pmod 6$, and we prove a similar result for all Steiner systems, as well as all designs except for 2-designs whose index $\\lambda$ is less than the largest block size. We nearly solve a conjecture of Dewar and Stevens on the existence of universal cycles in pairwise balanced designs. Motivated by R.L. Graham's question on the existence of Hamiltonian cycles in block-intersection graphs of Steiner triple systems, we establish the Hamiltonicity of the block-intersection graph of a large family of (not necessarily uniform) designs. All our results are constructive and of polynomial time complexity.","sentences":["Motivated by generalizations of de Bruijn cycles to various combinatorial structures (Chung, Diaconis, and Graham), we study various Euler tours in set systems.","Let $\\mathcal{G}$ be a hypergraph whose corank and rank are $c\\geq 3$ and $k$, respetively.","The minimum $t$-degree of $\\mathcal{G}$ is the fewest number of edges containing every $t$-subset of vertices.","An Euler tour (family, respectively) in $\\mathcal{G}$ is a (family of, respectively) closed walk(s) that (jointly, respectively) traverses each edge of $\\mathcal{G}$ exactly once.","An Euler tour is spanning if it traverses all the vertices of $\\mathcal{G}$. We show that $\\mathcal{G}$ has an Euler family if its incidence graph is $(1+\\lceil k/c \\rceil)$-edge-connected.","Provided that the number of vertices of $\\mathcal{G}$ meets a reasonable lower bound, and either $2$-degree is at least $k$ or $t$-degree is at least one for $t\\geq 3$, we show that $\\mathcal{G}$ has a spanning Euler tour.","To exhibit the usefulness of our results, we solve a number of open problems concerning ordering blocks of a design (these have applications in other fields such as erasure-correcting codes).","Answering a question of Horan and Hurlbert, we show that a Steiner quadruple system of order $n$ has a (spanning) Euler tour if and only if $n\\geq 8$ and $n\\equiv 2,4 \\pmod 6$, and we prove a similar result for all Steiner systems, as well as all designs except for 2-designs whose index $\\lambda$ is less than the largest block size.","We nearly solve a conjecture of Dewar and Stevens on the existence of universal cycles in pairwise balanced designs.","Motivated by R.L. Graham's question on the existence of Hamiltonian cycles in block-intersection graphs of Steiner triple systems, we establish the Hamiltonicity of the block-intersection graph of a large family of (not necessarily uniform) designs.","All our results are constructive and of polynomial time complexity."],"url":"http://arxiv.org/abs/2403.12713v1","category":"math.CO"}
{"created":"2024-03-19 13:19:18","title":"Tests for categorical data beyond Pearson: A distance covariance and energy distance approach","abstract":"Categorical variables are of uttermost importance in biomedical research. When two of them are considered, it is often the case that one wants to test whether or not they are statistically dependent. We show weaknesses of classical methods -- such as Pearson's and the G-test -- and we propose testing strategies based on distances that lack those drawbacks. We first develop this theory for classical two-dimensional contingency tables, within the context of distance covariance, an association measure that characterises general statistical independence of two variables. We then apply the same fundamental ideas to one-dimensional tables, namely to the testing for goodness of fit to a discrete distribution, for which we resort to an analogous statistic called energy distance. We prove that our methodology has desirable theoretical properties, and we show how we can calibrate the null distribution of our test statistics without resorting to any resampling technique. We illustrate all this in simulations, as well as with some real data examples, demonstrating the adequate performance of our approach for biostatistical practice.","sentences":["Categorical variables are of uttermost importance in biomedical research.","When two of them are considered, it is often the case that one wants to test whether or not they are statistically dependent.","We show weaknesses of classical methods -- such as Pearson's and the G-test -- and we propose testing strategies based on distances that lack those drawbacks.","We first develop this theory for classical two-dimensional contingency tables, within the context of distance covariance, an association measure that characterises general statistical independence of two variables.","We then apply the same fundamental ideas to one-dimensional tables, namely to the testing for goodness of fit to a discrete distribution, for which we resort to an analogous statistic called energy distance.","We prove that our methodology has desirable theoretical properties, and we show how we can calibrate the null distribution of our test statistics without resorting to any resampling technique.","We illustrate all this in simulations, as well as with some real data examples, demonstrating the adequate performance of our approach for biostatistical practice."],"url":"http://arxiv.org/abs/2403.12711v1","category":"stat.ME"}
{"created":"2024-03-19 13:17:26","title":"Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition","abstract":"Concerns for the privacy of individuals captured in public imagery have led to privacy-preserving action recognition. Existing approaches often suffer from issues arising through obfuscation being applied globally and a lack of interpretability. Global obfuscation hides privacy sensitive regions, but also contextual regions important for action recognition. Lack of interpretability erodes trust in these new technologies. We highlight the limitations of current paradigms and propose a solution: Human selected privacy templates that yield interpretability by design, an obfuscation scheme that selectively hides attributes and also induces temporal consistency, which is important in action recognition. Our approach is architecture agnostic and directly modifies input imagery, while existing approaches generally require architecture training. Our approach offers more flexibility, as no retraining is required, and outperforms alternatives on three widely used datasets.","sentences":["Concerns for the privacy of individuals captured in public imagery have led to privacy-preserving action recognition.","Existing approaches often suffer from issues arising through obfuscation being applied globally and a lack of interpretability.","Global obfuscation hides privacy sensitive regions, but also contextual regions important for action recognition.","Lack of interpretability erodes trust in these new technologies.","We highlight the limitations of current paradigms and propose a solution: Human selected privacy templates that yield interpretability by design, an obfuscation scheme that selectively hides attributes and also induces temporal consistency, which is important in action recognition.","Our approach is architecture agnostic and directly modifies input imagery, while existing approaches generally require architecture training.","Our approach offers more flexibility, as no retraining is required, and outperforms alternatives on three widely used datasets."],"url":"http://arxiv.org/abs/2403.12710v1","category":"cs.CV"}
{"created":"2024-03-19 13:16:05","title":"Invariant Theory: a Third Lease of Life","abstract":"In 1993, just about a century after the epoch of Classical Invariant Theory and almost 30 years after Mumford's seminal book on Geometric Invariant Theory, Bernd Sturmfels approached the subject from a new, algorithmic perspective in his book on Algorithms in Invariant Theory. This article aims to highlight some of the developments that followed the book. Inspired by Bernd's style of teaching mathematics, the goal is neither comprehensiveness nor maximal generality, but to emphasize the main ideas and to convey the beauty of the subject. The article is intended as an invitation to invariant theory, and in particular to Bernd's work and the ideas inspired by it.","sentences":["In 1993, just about a century after the epoch of Classical Invariant Theory and almost 30 years after Mumford's seminal book on Geometric Invariant Theory, Bernd Sturmfels approached the subject from a new, algorithmic perspective in his book on Algorithms in Invariant Theory.","This article aims to highlight some of the developments that followed the book.","Inspired by Bernd's style of teaching mathematics, the goal is neither comprehensiveness nor maximal generality, but to emphasize the main ideas and to convey the beauty of the subject.","The article is intended as an invitation to invariant theory, and in particular to Bernd's work and the ideas inspired by it."],"url":"http://arxiv.org/abs/2403.12709v1","category":"math.AC"}
{"created":"2024-03-19 13:10:50","title":"On the openness of the idempotent barycenter map related to a t-norm","abstract":"We demonstrate that the idempotent barycenter map, associated with a t-norm $\\ast$, is open if and only if the map of max-$\\ast$ convex combination is open. As a corollary, we deduce that the idempotent barycenter map is open for spaces of idempotent measures associated with any t-norm $\\ast$. Nevertheless, we illustrate that the characteristics of the idempotent barycenter map, in general, depend on the specific t-norm being employed.","sentences":["We demonstrate that the idempotent barycenter map, associated with a t-norm $\\ast$, is open if and only if the map of max-$\\ast$ convex combination is open.","As a corollary, we deduce that the idempotent barycenter map is open for spaces of idempotent measures associated with any t-norm $\\ast$. Nevertheless, we illustrate that the characteristics of the idempotent barycenter map, in general, depend on the specific t-norm being employed."],"url":"http://arxiv.org/abs/2403.12708v1","category":"math.GN"}
{"created":"2024-03-19 13:09:19","title":"Selective Domain-Invariant Feature for Generalizable Deepfake Detection","abstract":"With diverse presentation forgery methods emerging continually, detecting the authenticity of images has drawn growing attention. Although existing methods have achieved impressive accuracy in training dataset detection, they still perform poorly in the unseen domain and suffer from forgery of irrelevant information such as background and identity, affecting generalizability. To solve this problem, we proposed a novel framework Selective Domain-Invariant Feature (SDIF), which reduces the sensitivity to face forgery by fusing content features and styles. Specifically, we first use a Farthest-Point Sampling (FPS) training strategy to construct a task-relevant style sample representation space for fusing with content features. Then, we propose a dynamic feature extraction module to generate features with diverse styles to improve the performance and effectiveness of the feature extractor. Finally, a domain separation strategy is used to retain domain-related features to help distinguish between real and fake faces. Both qualitative and quantitative results in existing benchmarks and proposals demonstrate the effectiveness of our approach.","sentences":["With diverse presentation forgery methods emerging continually, detecting the authenticity of images has drawn growing attention.","Although existing methods have achieved impressive accuracy in training dataset detection, they still perform poorly in the unseen domain and suffer from forgery of irrelevant information such as background and identity, affecting generalizability.","To solve this problem, we proposed a novel framework Selective Domain-Invariant Feature (SDIF), which reduces the sensitivity to face forgery by fusing content features and styles.","Specifically, we first use a Farthest-Point Sampling (FPS) training strategy to construct a task-relevant style sample representation space for fusing with content features.","Then, we propose a dynamic feature extraction module to generate features with diverse styles to improve the performance and effectiveness of the feature extractor.","Finally, a domain separation strategy is used to retain domain-related features to help distinguish between real and fake faces.","Both qualitative and quantitative results in existing benchmarks and proposals demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.12707v1","category":"cs.CV"}
{"created":"2024-03-19 13:08:54","title":"AnimateDiff-Lightning: Cross-Model Diffusion Distillation","abstract":"We present AnimateDiff-Lightning for lightning-fast video generation. Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation. We discuss our modifications to adapt it for the video modality. Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility. We are pleased to release our distilled AnimateDiff-Lightning model for the community's use.","sentences":["We present AnimateDiff-Lightning for lightning-fast video generation.","Our model uses progressive adversarial diffusion distillation to achieve new state-of-the-art in few-step video generation.","We discuss our modifications to adapt it for the video modality.","Furthermore, we propose to simultaneously distill the probability flow of multiple base diffusion models, resulting in a single distilled motion module with broader style compatibility.","We are pleased to release our distilled AnimateDiff-Lightning model for the community's use."],"url":"http://arxiv.org/abs/2403.12706v1","category":"cs.CV"}
{"created":"2024-03-19 13:08:23","title":"The ultrametric backbone is the union of all minimum spanning forests","abstract":"Minimum spanning trees and forests are powerful sparsification techniques that remove cycles from weighted graphs to minimize total edge weight while preserving node connectivity. They have applications in computer science, network science, and graph theory. Despite their utility and ubiquity, they have several limitations, including that they are only defined for undirected networks, they significantly alter dynamics on networks, and they do not generally preserve important network features such as shortest distances, shortest path distribution, and community structure. In contrast, distance backbones, which are subgraphs formed by all edges that obey a generalized triangle inequality, are well defined in both directed and undirected graphs and preserve those and other important network features. The backbone of a graph is defined with respect to a specified path-length operator that aggregates weights along a path to define its length, thereby associating a cost to indirect connections. The backbone is the union of all shortest paths between each pair of nodes according to the specified operator. One such operator, the max function, computes the length of a path as the largest weight of the edges that compose it (a weakest link criterion). It is the only operator that yields an algebraic structure for computing shortest paths that is consistent with De Morgan's laws. Applying this operator yields the ultrametric backbone of a graph in that (semi-triangular) edges whose weights are larger than the length of an indirect path connecting the same nodes (i.e., those that break the generalized triangle inequality based on max as a path-length operator) are removed. We show that the ultrametric backbone is the union of all minimum spanning forests in undriected graphs and provides a new generalization of minimum spanning trees to directed graphs.","sentences":["Minimum spanning trees and forests are powerful sparsification techniques that remove cycles from weighted graphs to minimize total edge weight while preserving node connectivity.","They have applications in computer science, network science, and graph theory.","Despite their utility and ubiquity, they have several limitations, including that they are only defined for undirected networks, they significantly alter dynamics on networks, and they do not generally preserve important network features such as shortest distances, shortest path distribution, and community structure.","In contrast, distance backbones, which are subgraphs formed by all edges that obey a generalized triangle inequality, are well defined in both directed and undirected graphs and preserve those and other important network features.","The backbone of a graph is defined with respect to a specified path-length operator that aggregates weights along a path to define its length, thereby associating a cost to indirect connections.","The backbone is the union of all shortest paths between each pair of nodes according to the specified operator.","One such operator, the max function, computes the length of a path as the largest weight of the edges that compose it (a weakest link criterion).","It is the only operator that yields an algebraic structure for computing shortest paths that is consistent with De Morgan's laws.","Applying this operator yields the ultrametric backbone of a graph in that (semi-triangular) edges whose weights are larger than the length of an indirect path connecting the same nodes (i.e., those that break the generalized triangle inequality based on max as a path-length operator) are removed.","We show that the ultrametric backbone is the union of all minimum spanning forests in undriected graphs and provides a new generalization of minimum spanning trees to directed graphs."],"url":"http://arxiv.org/abs/2403.12705v1","category":"cs.DM"}
{"created":"2024-03-19 12:55:18","title":"Optimal estimate of electromagnetic field concentration between two nearly-touching inclusions in the quasi-static regime","abstract":"We investigate the electromagnetic field concentration between two nearly-touching inclusions that possess high-contrast electric permittivities in the quasi-static regime. By using layer potential techniques and asymptotic analysis in the low-frequency regime, we derive low-frequency expansions that provide integral representations for the solutions of the Maxwell equations. For the leading-order term $\\bE_0$ of the asymptotic expansion of the electric field, we prove that it has the blow up order of $\\epsilon^{-1} |\\ln \\epsilon|^{-1}$ within the radial geometry, where $\\epsilon$ signifies the asymptotic distance between the inclusions. By delicate analysis of the integral operators involved, we further prove the boundedness of the first-order term $\\bE_1$. We also conduct extensive numerical experiments which not only corroborate the theoretical findings but also provide more discoveries on the field concentration in the general geometric setup. Our study provides the first treatment in the literature on field concentration between nearly-touching material inclusions for the full Maxwell system.","sentences":["We investigate the electromagnetic field concentration between two nearly-touching inclusions that possess high-contrast electric permittivities in the quasi-static regime.","By using layer potential techniques and asymptotic analysis in the low-frequency regime, we derive low-frequency expansions that provide integral representations for the solutions of the Maxwell equations.","For the leading-order term $\\bE_0$ of the asymptotic expansion of the electric field, we prove that it has the blow up order of $\\epsilon^{-1} |\\ln \\epsilon|^{-1}$ within the radial geometry, where $\\epsilon$ signifies the asymptotic distance between the inclusions.","By delicate analysis of the integral operators involved, we further prove the boundedness of the first-order term $\\bE_1$. We also conduct extensive numerical experiments which not only corroborate the theoretical findings but also provide more discoveries on the field concentration in the general geometric setup.","Our study provides the first treatment in the literature on field concentration between nearly-touching material inclusions for the full Maxwell system."],"url":"http://arxiv.org/abs/2403.12697v1","category":"math.AP"}
{"created":"2024-03-19 12:52:38","title":"Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency","abstract":"Medical image segmentation plays a vital role in clinic disease diagnosis and medical image analysis. However, labeling medical images for segmentation task is tough due to the indispensable domain expertise of radiologists. Furthermore, considering the privacy and sensitivity of medical images, it is impractical to build a centralized segmentation dataset from different medical institutions. Federated learning aims to train a shared model of isolated clients without local data exchange which aligns well with the scarcity and privacy characteristics of medical data. To solve the problem of labeling hard, many advanced semi-supervised methods have been proposed in a centralized data setting. As for federated learning, how to conduct semi-supervised learning under this distributed scenario is worth investigating. In this work, we propose a novel federated semi-supervised learning framework for medical image segmentation. The intra-client and inter-client consistency learning are introduced to smooth predictions at the data level and avoid confirmation bias of local models. They are achieved with the assistance of a Variational Autoencoder (VAE) trained collaboratively by clients. The added VAE model plays three roles: 1) extracting latent low-dimensional features of all labeled and unlabeled data; 2) performing a novel type of data augmentation in calculating intra-client consistency loss; 3) utilizing the generative ability of itself to conduct inter-client consistency distillation. The proposed framework is compared with other federated semi-supervised or self-supervised learning methods. The experimental results illustrate that our method outperforms the state-of-the-art method while avoiding a lot of computation and communication overhead.","sentences":["Medical image segmentation plays a vital role in clinic disease diagnosis and medical image analysis.","However, labeling medical images for segmentation task is tough due to the indispensable domain expertise of radiologists.","Furthermore, considering the privacy and sensitivity of medical images, it is impractical to build a centralized segmentation dataset from different medical institutions.","Federated learning aims to train a shared model of isolated clients without local data exchange which aligns well with the scarcity and privacy characteristics of medical data.","To solve the problem of labeling hard, many advanced semi-supervised methods have been proposed in a centralized data setting.","As for federated learning, how to conduct semi-supervised learning under this distributed scenario is worth investigating.","In this work, we propose a novel federated semi-supervised learning framework for medical image segmentation.","The intra-client and inter-client consistency learning are introduced to smooth predictions at the data level and avoid confirmation bias of local models.","They are achieved with the assistance of a Variational Autoencoder (VAE) trained collaboratively by clients.","The added VAE model plays three roles: 1) extracting latent low-dimensional features of all labeled and unlabeled data; 2) performing a novel type of data augmentation in calculating intra-client consistency loss; 3) utilizing the generative ability of itself to conduct inter-client consistency distillation.","The proposed framework is compared with other federated semi-supervised or self-supervised learning methods.","The experimental results illustrate that our method outperforms the state-of-the-art method while avoiding a lot of computation and communication overhead."],"url":"http://arxiv.org/abs/2403.12695v1","category":"eess.IV"}
{"created":"2024-03-19 12:51:45","title":"Performance, Knowledge Acquisition and Satisfaction in Self-selected Groups: Evidence from a Classroom Field Experiment","abstract":"We investigate how to efficiently set up work groups to boost group productivity, individual satisfaction, and learning. Therefore, we conduct a natural field experiment in a compulsory undergraduate course and study differences between self-selected and randomly assigned groups. We find that self-selected groups perform significantly worse on group assignments. Yet, students in self-selected groups learn more and are more satisfied than those in randomly assigned groups. The effect of allowing students to pick group members dominates the effect of different group compositions in self-selected groups: When controlling for the skill, gender, and home region composition of groups, the differences between self-selected and randomly formed groups persist almost unaltered. The distribution of GitHub commits per group reveals that the better average performance of randomly assigned groups is mainly driven by highly skilled individuals distributed over more groups due to the assignment mechanism. Moreover, these highly skilled individuals contribute more to the group in randomly formed groups. We argue that this mechanism explains why self-selected groups perform worse on the projects but acquire more knowledge than randomly formed groups. These findings are relevant for setting up workgroups in academic, business, and governmental organizations when tasks are not constrained to the skill set of specific individuals.","sentences":["We investigate how to efficiently set up work groups to boost group productivity, individual satisfaction, and learning.","Therefore, we conduct a natural field experiment in a compulsory undergraduate course and study differences between self-selected and randomly assigned groups.","We find that self-selected groups perform significantly worse on group assignments.","Yet, students in self-selected groups learn more and are more satisfied than those in randomly assigned groups.","The effect of allowing students to pick group members dominates the effect of different group compositions in self-selected groups: When controlling for the skill, gender, and home region composition of groups, the differences between self-selected and randomly formed groups persist almost unaltered.","The distribution of GitHub commits per group reveals that the better average performance of randomly assigned groups is mainly driven by highly skilled individuals distributed over more groups due to the assignment mechanism.","Moreover, these highly skilled individuals contribute more to the group in randomly formed groups.","We argue that this mechanism explains why self-selected groups perform worse on the projects but acquire more knowledge than randomly formed groups.","These findings are relevant for setting up workgroups in academic, business, and governmental organizations when tasks are not constrained to the skill set of specific individuals."],"url":"http://arxiv.org/abs/2403.12694v1","category":"econ.GN"}
{"created":"2024-03-19 12:49:25","title":"Efficient thermalization and universal quantum computing with quantum Gibbs samplers","abstract":"The preparation of thermal states of matter is a crucial task in quantum simulation. In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures, and for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice. Furthermore, we show the efficient adiabatic preparation of the associated purifications or ``thermofield double'' states. To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications. In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures logarithmic in the system's size is polynomially equivalent to standard quantum computation. On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then analysing it as perturbation of the Hamiltonian corresponding to infinite temperature. For low temperature, we instead perform a perturbation at zero temperature of the Laplace transform of the energy observable at fixed runtime, and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing. Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems.","sentences":["The preparation of thermal states of matter is a crucial task in quantum simulation.","In this work, we prove that a recently introduced, efficiently implementable dissipative evolution thermalizes to the Gibbs state in time scaling polynomially with system size at high enough temperatures, and for any Hamiltonian that satisfies a Lieb-Robinson bound, such as local Hamiltonians on a lattice.","Furthermore, we show the efficient adiabatic preparation of the associated purifications or ``thermofield double'' states.","To the best of our knowledge, these are the first results rigorously establishing the efficient preparation of high-temperature Gibbs states and their purifications.","In the low-temperature regime, we show that implementing this family of dissipative evolutions for inverse temperatures logarithmic in the system's size is polynomially equivalent to standard quantum computation.","On a technical level, for high temperatures, our proof makes use of the mapping of the generator of the evolution into a Hamiltonian, and then analysing it as perturbation of the Hamiltonian corresponding to infinite temperature.","For low temperature, we instead perform a perturbation at zero temperature of the Laplace transform of the energy observable at fixed runtime, and resort to circuit-to-Hamiltonian mappings akin to the proof of universality of quantum adiabatic computing.","Taken together, our results show that a family of quasi-local dissipative evolutions efficiently prepares a large class of quantum many-body states of interest, and has the potential to mirror the success of classical Monte Carlo methods for quantum many-body systems."],"url":"http://arxiv.org/abs/2403.12691v1","category":"quant-ph"}
{"created":"2024-03-19 12:49:09","title":"LNPT: Label-free Network Pruning and Training","abstract":"Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online guidance for network pruning and learning on smart devices with unlabeled data. Our results demonstrate the superiority of this approach over supervised training.","sentences":["Pruning before training enables the deployment of neural networks on smart devices.","By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices.","It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance.","However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance.","In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization.","Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance.","We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online guidance for network pruning and learning on smart devices with unlabeled data.","Our results demonstrate the superiority of this approach over supervised training."],"url":"http://arxiv.org/abs/2403.12690v1","category":"cs.LG"}
{"created":"2024-03-19 12:47:43","title":"SEVEN: Pruning Transformer Model by Reserving Sentinels","abstract":"Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved by SEVEN. Extensive experiments on various TM in natural language, question-answering, and image classification domains are conducted to validate the effectiveness of SEVEN. The results demonstrate significant improvements of SEVEN in multiple pruning scenarios and across different sparsity levels. Additionally, SEVEN exhibits robust performance under various fine-tuning strategies. The code is publicly available at https://github.com/xiaojinying/SEVEN.","sentences":["Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks.","However, their considerable parameter size restricts their applicability, particularly on mobile devices.","Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise.","This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance.","Symbolic Descent (SD) is a general approach for training and fine-tuning TM.","In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD.","We utilize this design to dynamically assess the importance scores of weights.","SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise.","These weights are tended to be preserved by SEVEN.","Extensive experiments on various TM in natural language, question-answering, and image classification domains are conducted to validate the effectiveness of SEVEN.","The results demonstrate significant improvements of SEVEN in multiple pruning scenarios and across different sparsity levels.","Additionally, SEVEN exhibits robust performance under various fine-tuning strategies.","The code is publicly available at https://github.com/xiaojinying/SEVEN."],"url":"http://arxiv.org/abs/2403.12688v1","category":"cs.LG"}
{"created":"2024-03-19 12:45:52","title":"Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision","abstract":"This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition. We propose a novel audio-visual method for compound expression recognition. Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules. Notably, our method does not use any training data specific to the target task. The method is evaluated in multi-corpus training and cross-corpus validation setups. Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions. The source code is publicly available.","sentences":["This paper presents the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition.","We propose a novel audio-visual method for compound expression recognition.","Our method relies on emotion recognition models that fuse modalities at the emotion probability level, while decisions regarding the prediction of compound expressions are based on predefined rules.","Notably, our method does not use any training data specific to the target task.","The method is evaluated in multi-corpus training and cross-corpus validation setups.","Our findings from the challenge demonstrate that the proposed method can potentially form a basis for development of intelligent tools for annotating audio-visual data in the context of human's basic and compound emotions.","The source code is publicly available."],"url":"http://arxiv.org/abs/2403.12687v1","category":"cs.CV"}
{"created":"2024-03-19 12:45:00","title":"Dynamic Manipulation of Deformable Objects using Imitation Learning with Adaptation to Hardware Constraints","abstract":"Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects. However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits. These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical. To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive. Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics. This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements. We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening. Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag. See supplementary material at https://sites.google.com/view/bilbo-bag.","sentences":["Imitation Learning (IL) is a promising paradigm for learning dynamic manipulation of deformable objects since it does not depend on difficult-to-create accurate simulations of such objects.","However, the translation of motions demonstrated by a human to a robot is a challenge for IL, due to differences in the embodiments and the robot's physical limits.","These limits are especially relevant in dynamic manipulation where high velocities and accelerations are typical.","To address this problem, we propose a framework that first maps a dynamic demonstration into a motion that respects the robot's constraints using a constrained Dynamic Movement Primitive.","Second, the resulting object state is further optimized by quasi-static refinement motions to optimize task performance metrics.","This allows both efficiently altering the object state by dynamic motions and stable small-scale refinements.","We evaluate the framework in the challenging task of bag opening, designing the system BILBO: Bimanual dynamic manipulation using Imitation Learning for Bag Opening.","Our results show that BILBO can successfully open a wide range of crumpled bags, using a demonstration with a single bag.","See supplementary material at https://sites.google.com/view/bilbo-bag."],"url":"http://arxiv.org/abs/2403.12685v1","category":"cs.RO"}
{"created":"2024-03-19 12:36:52","title":"Evidence of extreme ionization conditions and low metallicity in GHZ2/GLASS-z12 from a combined analysis of NIRSpec and MIRI observations","abstract":"GHZ2/GLASS-z12 has been recently observed by JWST with both NIRSpec and MIRI spectrographs, making it the most distant galaxy ($z_{spec}=12.34$) with complete spectroscopic coverage from rest-frame UV to optical. It is identified as a strong CIV$_{1549}$ emitter with many other detected emission lines (NIV], HeII, OIII], NIII], CIII], [OII], [NeIII], [OIII], and H$\\alpha$), including a remarkable OIII$_{1333}$ Bowen fluorescence line. We analyze in this paper the joint NIRSpec+MIRI spectral data set. Combining six optical diagnostics (namely R2, R3, R23, O32, Ne3O2, and Ne3O2Hd), we find extreme ionization conditions, with O32 $=1.39 \\pm 0.19$ and Ne3O2 $=0.37 \\pm 0.18$ in stark excess compared to typical values in the ISM at lower redshifts. These line properties are compatible either with an AGN or with a compact, very dense star-forming environment ($\\Sigma_{\\rm SFR}$ $\\sim 10^2$-$10^3$ Msun/yr/kpc$^2$), with a high ionization parameter ($\\log_{10}$(U) $=-1.75 \\pm 0.16$), a high ionizing photon production efficiency $\\log(\\xi_{\\rm ion}) = 25.7_{-0.1}^{+0.2}$, and a low, although not pristine, metal content ranging between $5\\%$ and $11\\%$ Z$_\\odot$, indicating a rapid enrichment of the ISM in the last few Myrs. These properties also suggest that a substantial amount of ionizing photons ($\\sim 10\\%$) are leaking outside. The general lessons learned from GHZ2 are the following: (i) the UV to optical combined nebular indicators are broadly in agreement with UV-only or optical-only indicators. (ii) UV+optical diagnostics fail to discriminate between an AGN and star-formation in a low metallicity, high density, and extreme ionization environment. (iii) comparing the nebular line ratios with local analogs may be approaching its limits at $z \\gtrsim 10$, as this approach is potentially challenged by the unique conditions of star formation experienced by galaxies at these extreme redshifts.","sentences":["GHZ2/GLASS-z12 has been recently observed by JWST with both NIRSpec and MIRI spectrographs, making it the most distant galaxy ($z_{spec}=12.34$) with complete spectroscopic coverage from rest-frame UV to optical.","It is identified as a strong CIV$_{1549}$ emitter with many other detected emission lines (NIV], HeII, OIII], NIII], CIII], [OII], [NeIII], [OIII], and H$\\alpha$), including a remarkable OIII$_{1333}$ Bowen fluorescence line.","We analyze in this paper the joint NIRSpec+MIRI spectral data set.","Combining six optical diagnostics (namely R2, R3, R23, O32, Ne3O2, and Ne3O2Hd), we find extreme ionization conditions, with O32 $=1.39 \\pm 0.19$ and Ne3O2 $=0.37 \\pm 0.18$ in stark excess compared to typical values in the ISM at lower redshifts.","These line properties are compatible either with an AGN or with a compact, very dense star-forming environment ($\\Sigma_{\\rm SFR}$ $\\sim 10^2$-$10^3$ Msun/yr/kpc$^2$), with a high ionization parameter ($\\log_{10}$(U) $=-1.75 \\pm 0.16$), a high ionizing photon production efficiency $\\log(\\xi_{\\rm ion})","= 25.7_{-0.1}^{+0.2}$, and a low, although not pristine, metal content ranging between $5\\%$ and $11\\%$ Z$_\\odot$, indicating a rapid enrichment of the ISM in the last few Myrs.","These properties also suggest that a substantial amount of ionizing photons ($\\sim 10\\%$) are leaking outside.","The general lessons learned from GHZ2 are the following: (i) the UV to optical combined nebular indicators are broadly in agreement with UV-only or optical-only indicators.","(ii) UV+optical diagnostics fail to discriminate between an AGN and star-formation in a low metallicity, high density, and extreme ionization environment.","(iii) comparing the nebular line ratios with local analogs may be approaching its limits at $z \\gtrsim 10$, as this approach is potentially challenged by the unique conditions of star formation experienced by galaxies at these extreme redshifts."],"url":"http://arxiv.org/abs/2403.12683v1","category":"astro-ph.GA"}
{"created":"2024-03-19 12:30:38","title":"Effective Metric Descriptions of Quantum Black Holes","abstract":"In a recent work [arXiv:2307.13489 [gr-qc]], we have described spherically symmetric and static quantum black holes as deformations of the classical Schwarzschild metric that depend on the physical distance to the horizon. We have developed a framework that allows to compute the latter in a self-consistent fashion from the deformed geometry, in the vicinity of the horizon. However, in this formalism, the distance can be replaced by other physical quantities, e.g. curvature invariants such as the Ricci- or Kretschmann scalar. Here, we therefore define a more general framework, which we call an \"effective metric description\" (EMD), that captures the deformed geometry based on a generic physical quantity. We develop in detail the Ricci- and Kretschmann scalar EMD, in particular demonstrating how to compute the geometry in a self-consistent manner. Moreover, we provide explicit relations that allow to express one EMD in terms of the others, thus demonstrating their equivalence.","sentences":["In a recent work [arXiv:2307.13489 [gr-qc]], we have described spherically symmetric and static quantum black holes as deformations of the classical Schwarzschild metric that depend on the physical distance to the horizon.","We have developed a framework that allows to compute the latter in a self-consistent fashion from the deformed geometry, in the vicinity of the horizon.","However, in this formalism, the distance can be replaced by other physical quantities, e.g. curvature invariants such as the Ricci- or Kretschmann scalar.","Here, we therefore define a more general framework, which we call an \"effective metric description\" (EMD), that captures the deformed geometry based on a generic physical quantity.","We develop in detail the Ricci- and Kretschmann scalar EMD, in particular demonstrating how to compute the geometry in a self-consistent manner.","Moreover, we provide explicit relations that allow to express one EMD in terms of the others, thus demonstrating their equivalence."],"url":"http://arxiv.org/abs/2403.12679v1","category":"gr-qc"}
{"created":"2024-03-19 12:24:20","title":"Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights","abstract":"The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use. Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of chatbots and the accuracy of retrieval systems.","sentences":["The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights.","Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights.","Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations.","The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances.","The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions.","A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use.","Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of chatbots and the accuracy of retrieval systems."],"url":"http://arxiv.org/abs/2403.12678v1","category":"cs.CL"}
{"created":"2024-03-19 12:23:48","title":"Causal Change Point Detection and Localization","abstract":"Detecting and localizing change points in sequential data is of interest in many areas of application. Various notions of change points have been proposed, such as changes in mean, variance, or the linear regression coefficient. In this work, we consider settings in which a response variable $Y$ and a set of covariates $X=(X^1,\\ldots,X^{d+1})$ are observed over time and aim to find changes in the causal mechanism generating $Y$ from $X$. More specifically, we assume $Y$ depends linearly on a subset of the covariates and aim to determine at what time points either the dependency on the subset or the subset itself changes. We call these time points causal change points (CCPs) and show that they form a subset of the commonly studied regression change points. We propose general methodology to both detect and localize CCPs. Although motivated by causality, we define CCPs without referencing an underlying causal model. The proposed definition of CCPs exploits a notion of invariance, which is a purely observational quantity but -- under additional assumptions -- has a causal meaning. For CCP localization, we propose a loss function that can be combined with existing multiple change point algorithms to localize multiple CCPs efficiently. We evaluate and illustrate our methods on simulated datasets.","sentences":["Detecting and localizing change points in sequential data is of interest in many areas of application.","Various notions of change points have been proposed, such as changes in mean, variance, or the linear regression coefficient.","In this work, we consider settings in which a response variable $Y$ and a set of covariates $X=(X^1,\\ldots,X^{d+1})$ are observed over time and aim to find changes in the causal mechanism generating $Y$ from $X$. More specifically, we assume $Y$ depends linearly on a subset of the covariates and aim to determine at what time points either the dependency on the subset or the subset itself changes.","We call these time points causal change points (CCPs) and show that they form a subset of the commonly studied regression change points.","We propose general methodology to both detect and localize CCPs.","Although motivated by causality, we define CCPs without referencing an underlying causal model.","The proposed definition of CCPs exploits a notion of invariance, which is a purely observational quantity but -- under additional assumptions -- has a causal meaning.","For CCP localization, we propose a loss function that can be combined with existing multiple change point algorithms to localize multiple CCPs efficiently.","We evaluate and illustrate our methods on simulated datasets."],"url":"http://arxiv.org/abs/2403.12677v1","category":"stat.ME"}
{"created":"2024-03-19 12:21:23","title":"In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing","abstract":"Most research on deformable linear object (DLO) manipulation assumes rigid grasping. However, beyond rigid grasping and re-grasping, in-hand following is also an essential skill that humans use to dexterously manipulate DLOs, which requires continuously changing the grasp point by in-hand sliding while holding the DLO to prevent it from falling. Achieving such a skill is very challenging for robots without using specially designed but not versatile end-effectors. Previous works have attempted using generic parallel grippers, but their robustness is unsatisfactory owing to the conflict between following and holding, which is hard to balance with a one-degree-of-freedom gripper. In this work, inspired by how humans use fingers to follow DLOs, we explore the usage of a generic dexterous hand with tactile sensing to imitate human skills and achieve robust in-hand DLO following. To enable the hardware system to function in the real world, we develop a framework that includes Cartesian-space arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and task-specific motion design. Experimental results demonstrate the significant superiority of our method over using parallel grippers, as well as its great robustness, generalizability, and efficiency.","sentences":["Most research on deformable linear object (DLO) manipulation assumes rigid grasping.","However, beyond rigid grasping and re-grasping, in-hand following is also an essential skill that humans use to dexterously manipulate DLOs, which requires continuously changing the grasp point by in-hand sliding while holding the DLO to prevent it from falling.","Achieving such a skill is very challenging for robots without using specially designed but not versatile end-effectors.","Previous works have attempted using generic parallel grippers, but their robustness is unsatisfactory owing to the conflict between following and holding, which is hard to balance with a one-degree-of-freedom gripper.","In this work, inspired by how humans use fingers to follow DLOs, we explore the usage of a generic dexterous hand with tactile sensing to imitate human skills and achieve robust in-hand DLO following.","To enable the hardware system to function in the real world, we develop a framework that includes Cartesian-space arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and task-specific motion design.","Experimental results demonstrate the significant superiority of our method over using parallel grippers, as well as its great robustness, generalizability, and efficiency."],"url":"http://arxiv.org/abs/2403.12676v1","category":"cs.RO"}
{"created":"2024-03-19 12:21:20","title":"Pragmatic Competence Evaluation of Large Language Models for Korean","abstract":"The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation. Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4. Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference. Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.","sentences":["The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.","Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean.","We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options.","Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4.","Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.","Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations."],"url":"http://arxiv.org/abs/2403.12675v1","category":"cs.CL"}
{"created":"2024-03-19 12:13:52","title":"Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine","abstract":"Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points. In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM. However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted. In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure. The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points. Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score based on simulated annealing, which is widely used for optimization problems. The proposed evaluation method was also validated using numerical experiments.","sentences":["Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for semi-supervised anomaly detection, where they are trained using only normal data points.","In GBRBM-based anomaly detection, normal and anomalous data are classified based on a score that is identical to an energy function of the marginal GBRBM.","However, the classification threshold is difficult to set to an appropriate value, as this score cannot be interpreted.","In this study, we propose a measure that improves score's interpretability based on its cumulative distribution, and establish a guideline for setting the threshold using the interpretable measure.","The results of numerical experiments show that the guideline is reasonable when setting the threshold solely using normal data points.","Moreover, because identifying the measure involves computationally infeasible evaluation of the minimum score value, we also propose an evaluation method for the minimum score based on simulated annealing, which is widely used for optimization problems.","The proposed evaluation method was also validated using numerical experiments."],"url":"http://arxiv.org/abs/2403.12672v1","category":"cs.LG"}
{"created":"2024-03-19 12:13:33","title":"Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering","abstract":"AI assistants for coding are on the rise. However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code. This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue. Then, we propose a systematic approach based on prompt-altering methods to achieve better code security of (even proprietary black-box) AI-based code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs. In sum, we propose and evaluate three prompt altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination. Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user. We assess the effectiveness of the proposed methods on the GitHub Copilot using the OpenVPN project in realistic scenarios, and we demonstrate that the proposed methods reduce the number of insecure generated code samples by up to 16\\% and increase the number of secure code by up to 8\\%. Since our approach does not require access to the internals of the AI models, it can be in general applied to any AI-based code synthesizer, not only GitHub Copilot.","sentences":["AI assistants for coding are on the rise.","However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code.","This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue.","Then, we propose a systematic approach based on prompt-altering methods to achieve better code security of (even proprietary black-box) AI-based code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs.","In sum, we propose and evaluate three prompt altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination.","Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user.","We assess the effectiveness of the proposed methods on the GitHub Copilot using the OpenVPN project in realistic scenarios, and we demonstrate that the proposed methods reduce the number of insecure generated code samples by up to 16\\% and increase the number of secure code by up to 8\\%.","Since our approach does not require access to the internals of the AI models, it can be in general applied to any AI-based code synthesizer, not only GitHub Copilot."],"url":"http://arxiv.org/abs/2403.12671v1","category":"cs.CR"}
{"created":"2024-03-19 12:11:57","title":"Driving Animatronic Robot Facial Expression From Speech","abstract":"Animatronic robots aim to enable natural human-robot interaction through lifelike facial expressions. However, generating realistic, speech-synchronized robot expressions is challenging due to the complexities of facial biomechanics and responsive motion synthesis. This paper presents a principled, skinning-centric approach to drive animatronic robot facial expressions from speech. The proposed approach employs linear blend skinning (LBS) as the core representation to guide tightly integrated innovations in embodiment design and motion synthesis. LBS informs the actuation topology, enables human expression retargeting, and allows speech-driven facial motion generation. The proposed approach is capable of generating highly realistic, real-time facial expressions from speech on an animatronic face, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction.","sentences":["Animatronic robots aim to enable natural human-robot interaction through lifelike facial expressions.","However, generating realistic, speech-synchronized robot expressions is challenging due to the complexities of facial biomechanics and responsive motion synthesis.","This paper presents a principled, skinning-centric approach to drive animatronic robot facial expressions from speech.","The proposed approach employs linear blend skinning (LBS) as the core representation to guide tightly integrated innovations in embodiment design and motion synthesis.","LBS informs the actuation topology, enables human expression retargeting, and allows speech-driven facial motion generation.","The proposed approach is capable of generating highly realistic, real-time facial expressions from speech on an animatronic face, significantly advancing robots' ability to replicate nuanced human expressions for natural interaction."],"url":"http://arxiv.org/abs/2403.12670v1","category":"cs.RO"}
{"created":"2024-03-19 12:08:02","title":"Time delay in the gravitational field of an axisymmetric body at rest with full mass and spin multipole structure","abstract":"The time delay of a light signal which propagates in the gravitational field of an isolated body is considered. The body can be of arbitrary but time-independent shape and inner structure and can be in uniform rotational motion, while the center of mass of the body is assumed to be at rest. The gravitational field is given in the post-Newtonian scheme and in terms of the full set of mass-multipoles and spin-multipoles of the body. The asymptotic configuration is considered, where source and observer are located at spatial infinity from the massive body. It is found that in this asymptotic limit the higher multipole terms of time delay are related to the higher multipole terms of total light deflection. Furthermore, it is shown that the gauge terms vanish in this asymptotic configuration. In case of an axisymmetric body in uniform rotational motion, the higher multipole terms of time delay can be expressed in terms of Chebyshev polynomials. This fact allows one to determine the upper limits of the time delay for higher multipoles. These upper limits represent a criterion to identify those multipoles which contribute significantly to the time delay for a given accuracy of time measurements. It is found that the first mass-multipoles with $l \\le 8$ and the first spin-multipoles with $l \\le 3$ are sufficient for an accuracy on the femto-second scale of accuracy in time measurements.","sentences":["The time delay of a light signal which propagates in the gravitational field of an isolated body is considered.","The body can be of arbitrary but time-independent shape and inner structure and can be in uniform rotational motion, while the center of mass of the body is assumed to be at rest.","The gravitational field is given in the post-Newtonian scheme and in terms of the full set of mass-multipoles and spin-multipoles of the body.","The asymptotic configuration is considered, where source and observer are located at spatial infinity from the massive body.","It is found that in this asymptotic limit the higher multipole terms of time delay are related to the higher multipole terms of total light deflection.","Furthermore, it is shown that the gauge terms vanish in this asymptotic configuration.","In case of an axisymmetric body in uniform rotational motion, the higher multipole terms of time delay can be expressed in terms of Chebyshev polynomials.","This fact allows one to determine the upper limits of the time delay for higher multipoles.","These upper limits represent a criterion to identify those multipoles which contribute significantly to the time delay for a given accuracy of time measurements.","It is found that the first mass-multipoles with $l \\le 8$ and the first spin-multipoles with $l \\le 3$ are sufficient for an accuracy on the femto-second scale of accuracy in time measurements."],"url":"http://arxiv.org/abs/2403.12669v1","category":"gr-qc"}
{"created":"2024-03-19 12:05:09","title":"ICE: Interactive 3D Game Character Editing via Dialogue","abstract":"Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters. However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification. In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process. In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players. Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round. To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner. Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results. Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE. Project page: https://iceedit.github.io/.","sentences":["Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters.","However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification.","In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process.","In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players.","Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round.","To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner.","Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results.","Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE.","Project page: https://iceedit.github.io/."],"url":"http://arxiv.org/abs/2403.12667v2","category":"cs.MM"}
{"created":"2024-03-19 11:56:21","title":"Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making","abstract":"In many applications, model ensembling proves to be better than a single predictive model. Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML). The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models. In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks. This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The given ensemble is analyzed from different perspectives. We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models. We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions. Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of variables. Summarizing obtained insights, we can investigate and adjust the weights with a modification tool to tune the ensemble in the desired way. The application provides the aforementioned aspects through dedicated interactive visualizations, making it accessible to a diverse audience. We believe the cattleia can support users in decision-making and deepen the comprehension of AutoML frameworks.","sentences":["In many applications, model ensembling proves to be better than a single predictive model.","Hence, it is the most common post-processing technique in Automated Machine Learning (AutoML).","The most popular frameworks use ensembles at the expense of reducing the interpretability of the final models.","In our work, we propose cattleia - an application that deciphers the ensembles for regression, multiclass, and binary classification tasks.","This tool works with models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML.","The given ensemble is analyzed from different perspectives.","We conduct a predictive performance investigation through evaluation metrics of the ensemble and its component models.","We extend the validation perspective by introducing new measures to assess the diversity and complementarity of the model predictions.","Moreover, we apply explainable artificial intelligence (XAI) techniques to examine the importance of variables.","Summarizing obtained insights, we can investigate and adjust the weights with a modification tool to tune the ensemble in the desired way.","The application provides the aforementioned aspects through dedicated interactive visualizations, making it accessible to a diverse audience.","We believe the cattleia can support users in decision-making and deepen the comprehension of AutoML frameworks."],"url":"http://arxiv.org/abs/2403.12664v1","category":"cs.LG"}
{"created":"2024-03-19 11:54:04","title":"Generic non-uniqueness of minimizing harmonic maps from a ball to a sphere","abstract":"In this note, we study non-uniqueness for minimizing harmonic maps from $B^3$ to $\\mathbb{S}^2$. We show that every boundary map can be modified to a boundary map that admits multiple minimizers of the Dirichlet energy by a small $W^{1,p}$-change for $p<2$. This strengthens a remark by the second-named author and Strzelecki. The main novel ingredient is a homotopy construction, which is the answer to an easier variant of a challenging question regarding the existence of a norm control for homotopies between $ W^{1,p} $ maps.","sentences":["In this note, we study non-uniqueness for minimizing harmonic maps from $B^3$ to $\\mathbb{S}^2$. We show that every boundary map can be modified to a boundary map that admits multiple minimizers of the Dirichlet energy by a small $W^{1,p}$-change for $p<2$. This strengthens a remark by the second-named author and Strzelecki.","The main novel ingredient is a homotopy construction, which is the answer to an easier variant of a challenging question regarding the existence of a norm control for homotopies between $ W^{1,p} $ maps."],"url":"http://arxiv.org/abs/2403.12662v1","category":"math.AP"}
{"created":"2024-03-19 11:49:35","title":"ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems","abstract":"Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online for ease of reproduction.","sentences":["Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations.","Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands.","This research area, particularly in the context of DRS, is nascent and faces three core challenges.","Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights.","Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS.","Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods.","To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS.","ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement.","Our code is available online for ease of reproduction."],"url":"http://arxiv.org/abs/2403.12660v1","category":"cs.IR"}
{"created":"2024-03-19 11:49:08","title":"Zeolite Adsorption Property Prediction using Deep Learning","abstract":"The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials. The existing configuration space for these materials is wide, while existing molecular simulation methods are computationally expensive. In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular simulations. To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo simulations, confirming that the model can be used for property prediction. Furthermore, we show that the model can be used for identifying adsorption sites. Finally, we evaluate the capability of our model for generating novel zeolite configurations by using it in combination with a genetic algorithm.","sentences":["The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials.","The existing configuration space for these materials is wide, while existing molecular simulation methods are computationally expensive.","In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular simulations.","To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo simulations.","The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo simulations, confirming that the model can be used for property prediction.","Furthermore, we show that the model can be used for identifying adsorption sites.","Finally, we evaluate the capability of our model for generating novel zeolite configurations by using it in combination with a genetic algorithm."],"url":"http://arxiv.org/abs/2403.12659v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 11:44:19","title":"ac Hall Effect and Photon Drag of Superconducting Condensate","abstract":"We suggest a theoretical description of the photogalvanic phenomena arising in superconducting condensates in the field of electromagnetic wave. The ac Hall effect and photon drag are shown to originate from the second-order nonlinear response of superconducting carriers caused by the suppression of their concentration due to the combined influence of the electron - hole asymmetry and charge imbalance generated by the incident electromagnetic wave. Starting from the time-dependent Ginzburg-Landau theory with the complex relaxation constant we develop a phenomenological description of these phenomena and investigate the resulting behavior of the dc supercurrent and second harmonic induced by microwave radiation incident on a superconductor surface.","sentences":["We suggest a theoretical description of the photogalvanic phenomena arising in superconducting condensates in the field of electromagnetic wave.","The ac Hall effect and photon drag are shown to originate from the second-order nonlinear response of superconducting carriers caused by the suppression of their concentration due to the combined influence of the electron - hole asymmetry and charge imbalance generated by the incident electromagnetic wave.","Starting from the time-dependent Ginzburg-Landau theory with the complex relaxation constant we develop a phenomenological description of these phenomena and investigate the resulting behavior of the dc supercurrent and second harmonic induced by microwave radiation incident on a superconductor surface."],"url":"http://arxiv.org/abs/2403.12656v1","category":"cond-mat.supr-con"}
{"created":"2024-03-19 11:42:13","title":"One-sided generalized Drazin-Riesz and one-sided generalized Drazin-meromorphic invertible operators","abstract":"The aim of this paper is to introduce and study left and right versions of the class of generalized Drazin-Riesz invertible operators, as well as left and right versions of the class of generalized Drazin-meromorphic invertible operators.","sentences":["The aim of this paper is to introduce and study left and right versions of the class of generalized Drazin-Riesz invertible operators, as well as left and right versions of the class of generalized Drazin-meromorphic invertible operators."],"url":"http://arxiv.org/abs/2403.12655v1","category":"math.FA"}
{"created":"2024-03-19 11:34:40","title":"Adaptive Multilevel Neural Networks for Parametric PDEs with Error Estimation","abstract":"To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented. It is constructed to map parameters of the model data to corresponding finite element solutions. To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM). It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network. The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network. This leads to a problem adapted representation of the solution on locally refined grids. Furthermore, each solution of the AFEM is discretized in a hierarchical basis. For the architecture, convolutional neural networks (CNNs) are chosen. The hierarchical basis then allows to handle sparse images for finely discretized meshes. Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively. This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs. The architecture is described and preliminary numerical examples are presented.","sentences":["To solve high-dimensional parameter-dependent partial differential equations (pPDEs), a neural network architecture is presented.","It is constructed to map parameters of the model data to corresponding finite element solutions.","To improve training efficiency and to enable control of the approximation error, the network mimics an adaptive finite element method (AFEM).","It outputs a coarse grid solution and a series of corrections as produced in an AFEM, allowing a tracking of the error decay over successive layers of the network.","The observed errors are measured by a reliable residual based a posteriori error estimator, enabling the reduction to only few parameters for the approximation in the output of the network.","This leads to a problem adapted representation of the solution on locally refined grids.","Furthermore, each solution of the AFEM is discretized in a hierarchical basis.","For the architecture, convolutional neural networks (CNNs) are chosen.","The hierarchical basis then allows to handle sparse images for finely discretized meshes.","Additionally, as corrections on finer levels decrease in amplitude, i.e., importance for the overall approximation, the accuracy of the network approximation is allowed to decrease successively.","This can either be incorporated in the number of generated high fidelity samples used for training or the size of the network components responsible for the fine grid outputs.","The architecture is described and preliminary numerical examples are presented."],"url":"http://arxiv.org/abs/2403.12650v1","category":"math.NA"}
{"created":"2024-03-19 11:34:15","title":"InBox: Recommendation with Knowledge Graph using Interest Box Embedding","abstract":"Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability. Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of knowledge graph data and interest connectivity. To address these limitations, we introduce a novel embedding-based model called InBox. Specifically, various knowledge graph entities and relations are embedded as points or boxes, while user interests are modeled as boxes encompassing interaction history. Representing interests as boxes enables containing collections of item points related to that interest. We further propose that an interest comprises diverse basic concepts, and box intersection naturally supports concept combination. Across three training steps, InBox significantly outperforms state-of-the-art methods like HAKG and KGIN on recommendation tasks. Further analysis provides meaningful insights into the variable value of different KG data for recommendations. In summary, InBox advances recommender systems through box-based interest and concept modeling for sophisticated knowledge graph exploitation.","sentences":["Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability.","Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items.","However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity.","This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way.","Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests.","This homogenization limits the precise exploitation of knowledge graph data and interest connectivity.","To address these limitations, we introduce a novel embedding-based model called InBox.","Specifically, various knowledge graph entities and relations are embedded as points or boxes, while user interests are modeled as boxes encompassing interaction history.","Representing interests as boxes enables containing collections of item points related to that interest.","We further propose that an interest comprises diverse basic concepts, and box intersection naturally supports concept combination.","Across three training steps, InBox significantly outperforms state-of-the-art methods like HAKG and KGIN on recommendation tasks.","Further analysis provides meaningful insights into the variable value of different KG data for recommendations.","In summary, InBox advances recommender systems through box-based interest and concept modeling for sophisticated knowledge graph exploitation."],"url":"http://arxiv.org/abs/2403.12649v1","category":"cs.IR"}
{"created":"2024-03-19 11:30:30","title":"Prompt-fused framework for Inductive Logical Query Answering","abstract":"Answering logical queries on knowledge graphs (KG) poses a significant challenge for machine reasoning. The primary obstacle in this task stems from the inherent incompleteness of KGs. Existing research has predominantly focused on addressing the issue of missing edges in KGs, thereby neglecting another aspect of incompleteness: the emergence of new entities. Furthermore, most of the existing methods tend to reason over each logical operator separately, rather than comprehensively analyzing the query as a whole during the reasoning process. In this paper, we propose a query-aware prompt-fused framework named Pro-QE, which could incorporate existing query embedding methods and address the embedding of emerging entities through contextual information aggregation. Additionally, a query prompt, which is generated by encoding the symbolic query, is introduced to gather information relevant to the query from a holistic perspective. To evaluate the efficacy of our model in the inductive setting, we introduce two new challenging benchmarks. Experimental results demonstrate that our model successfully handles the issue of unseen entities in logical queries. Furthermore, the ablation study confirms the efficacy of the aggregator and prompt components.","sentences":["Answering logical queries on knowledge graphs (KG) poses a significant challenge for machine reasoning.","The primary obstacle in this task stems from the inherent incompleteness of KGs.","Existing research has predominantly focused on addressing the issue of missing edges in KGs, thereby neglecting another aspect of incompleteness: the emergence of new entities.","Furthermore, most of the existing methods tend to reason over each logical operator separately, rather than comprehensively analyzing the query as a whole during the reasoning process.","In this paper, we propose a query-aware prompt-fused framework named Pro-QE, which could incorporate existing query embedding methods and address the embedding of emerging entities through contextual information aggregation.","Additionally, a query prompt, which is generated by encoding the symbolic query, is introduced to gather information relevant to the query from a holistic perspective.","To evaluate the efficacy of our model in the inductive setting, we introduce two new challenging benchmarks.","Experimental results demonstrate that our model successfully handles the issue of unseen entities in logical queries.","Furthermore, the ablation study confirms the efficacy of the aggregator and prompt components."],"url":"http://arxiv.org/abs/2403.12646v1","category":"cs.LG"}
{"created":"2024-03-19 11:24:14","title":"Automated Contrastive Learning Strategy Search for Time Series","abstract":"In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive pair construction and contrastive losses. Further, we introduce an efficient reinforcement learning algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space. Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task. From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets. We also provide empirical analysis as a guidance for future design of CLS.","sentences":["In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series.","Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks.","However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations.","In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL).","We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive pair construction and contrastive losses.","Further, we introduce an efficient reinforcement learning algorithm, which optimizes CLS from the performance on the validation tasks, to obtain more effective CLS within the space.","Experimental results on various real-world tasks and datasets demonstrate that AutoCL could automatically find the suitable CLS for a given dataset and task.","From the candidate CLS found by AutoCL on several public datasets/tasks, we compose a transferable Generally Good Strategy (GGS), which has a strong performance for other datasets.","We also provide empirical analysis as a guidance for future design of CLS."],"url":"http://arxiv.org/abs/2403.12641v1","category":"cs.LG"}
{"created":"2024-03-19 11:20:18","title":"Hardy inequalities for large fermionic systems","abstract":"Given $0<s<\\frac d2$ with $s\\leq 1$, we are interested in the large $N$-behavior of the optimal constant $\\kappa_N$ in the Hardy inequality $\\sum_{n=1}^N (-\\Delta_n)^s \\geq \\kappa_N \\sum_{n<m} |X_n-X_m|^{-2s}$, when restricted to antisymmetric functions. We show that $N^{1-\\frac{2s}d}\\kappa_N$ has a positive, finite limit given by a certain variational problem, thereby generalizing a result of Lieb and Yau related to the Chandrasekhar theory of gravitational collapse.","sentences":["Given $0<s<\\frac d2$ with $s\\leq 1$, we are interested in the large $N$-behavior of the optimal constant $\\kappa_N$ in the Hardy inequality $\\sum_{n=1}^N (-\\Delta_n)^s \\geq \\kappa_N \\sum_{n<m} |X_n-X_m|^{-2s}$, when restricted to antisymmetric functions.","We show that $N^{1-\\frac{2s}d}\\kappa_N$ has a positive, finite limit given by a certain variational problem, thereby generalizing a result of Lieb and Yau related to the Chandrasekhar theory of gravitational collapse."],"url":"http://arxiv.org/abs/2403.12640v1","category":"math.AP"}
{"created":"2024-03-19 11:16:14","title":"A Practical Guide to Statistical Distances for Evaluating Generative Models in Science","abstract":"Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision making and a model generating medical images. We showcase that distinct distances can give different results on similar data. Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science.","sentences":["Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes.","How do we evaluate the samples these models generate?","This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics.","We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\\'echet Inception Distance; FID).","We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls.","To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision making and a model generating medical images.","We showcase that distinct distances can give different results on similar data.","Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science."],"url":"http://arxiv.org/abs/2403.12636v1","category":"cs.LG"}
{"created":"2024-03-19 11:04:46","title":"Implementation of a GHZ-Teukolsky puncture scheme for gravitational self-force calculations","abstract":"Post-adiabatic models of extreme- and intermediate-mass-ratio inspirals will require calculations of second-order gravitational self-force effects in the spacetime of a spinning, Kerr black hole. We take a step toward such calculations by implementing the recently formulated Teukolsky puncture scheme with Green-Hollands-Zimmerman metric reconstruction [CQG 39, 015019 (2022)]. This scheme eliminates the critical obstacle of gauge singularities that arise in the standard no-string metric reconstruction. Our first proof-of-principle implementation is limited to the simple case of circular orbits in Schwarzschild spacetime, but the method also applies to generic orbits on a Kerr background. We conclude with a discussion of various approaches to the second-order self-force problem in Kerr.","sentences":["Post-adiabatic models of extreme- and intermediate-mass-ratio inspirals will require calculations of second-order gravitational self-force effects in the spacetime of a spinning, Kerr black hole.","We take a step toward such calculations by implementing the recently formulated Teukolsky puncture scheme with Green-Hollands-Zimmerman metric reconstruction [CQG 39, 015019 (2022)].","This scheme eliminates the critical obstacle of gauge singularities that arise in the standard no-string metric reconstruction.","Our first proof-of-principle implementation is limited to the simple case of circular orbits in Schwarzschild spacetime, but the method also applies to generic orbits on a Kerr background.","We conclude with a discussion of various approaches to the second-order self-force problem in Kerr."],"url":"http://arxiv.org/abs/2403.12634v1","category":"gr-qc"}
{"created":"2024-03-19 10:59:21","title":"PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic Glove Applications","abstract":"Controlling hand exoskeletons to assist individuals with grasping tasks poses a challenge due to the difficulty in understanding user intentions. We propose that most daily grasping tasks during activities of daily living (ADL) can be deduced by analyzing object geometries (simple and complex) from 3D point clouds. The study introduces PointGrasp, a real-time system designed for identifying household scenes semantically, aiming to support and enhance assistance during ADL for tailored end-to-end grasping tasks. The system comprises an RGB-D camera with an inertial measurement unit and a microprocessor integrated into a tendon-driven soft robotic glove. The RGB-D camera processes 3D scenes at a rate exceeding 30 frames per second. The proposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple and 0.11 $\\pm$ 0.06 cm for complex geometries. Within each mode, it identifies and pinpoints reachable objects. This system shows promise in end-to-end vision-driven robotic-assisted rehabilitation manual tasks.","sentences":["Controlling hand exoskeletons to assist individuals with grasping tasks poses a challenge due to the difficulty in understanding user intentions.","We propose that most daily grasping tasks during activities of daily living (ADL) can be deduced by analyzing object geometries (simple and complex) from 3D point clouds.","The study introduces PointGrasp, a real-time system designed for identifying household scenes semantically, aiming to support and enhance assistance during ADL for tailored end-to-end grasping tasks.","The system comprises an RGB-D camera with an inertial measurement unit and a microprocessor integrated into a tendon-driven soft robotic glove.","The RGB-D camera processes 3D scenes at a rate exceeding 30 frames per second.","The proposed pipeline demonstrates an average RMSE of 0.8 $\\pm$ 0.39 cm for simple and 0.11 $\\pm$ 0.06 cm for complex geometries.","Within each mode, it identifies and pinpoints reachable objects.","This system shows promise in end-to-end vision-driven robotic-assisted rehabilitation manual tasks."],"url":"http://arxiv.org/abs/2403.12631v1","category":"cs.RO"}
{"created":"2024-03-19 10:53:40","title":"Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code","abstract":"In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness. Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs). Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code. This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information. Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving. Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation. Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies. This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification. The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1","sentences":["In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness.","Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs).","Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code.","This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information.","Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving.","Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation.","Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies.","This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification.","The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1"],"url":"http://arxiv.org/abs/2403.12627v1","category":"cs.AI"}
{"created":"2024-03-19 10:50:34","title":"Large-scale metric objects filtering for binary classification with application to abnormal brain connectivity detection","abstract":"The classification of random objects within metric spaces without a vector structure has attracted increasing attention. However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications. To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space. Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility. It enjoys a model-free property, as its implementation does not rely on any specified classifier. Theoretically, it controls the false discovery rate while guaranteeing the sure screening property. Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors. When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition. Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities.","sentences":["The classification of random objects within metric spaces without a vector structure has attracted increasing attention.","However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications.","To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space.","Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility.","It enjoys a model-free property, as its implementation does not rely on any specified classifier.","Theoretically, it controls the false discovery rate while guaranteeing the sure screening property.","Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors.","When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition.","Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities."],"url":"http://arxiv.org/abs/2403.12624v1","category":"stat.ME"}
{"created":"2024-03-19 10:47:59","title":"All-order Feynman rule for leading-twist quark operators","abstract":"We present an expression for the Feynman rule for leading-twist quark operators with an arbitrary number of total derivatives and an arbitrary number of gluons. This provides a generalization of known results and constitutes a necessary ingredient in the computation of the matrix elements of the corresponding operators. The result is in a form appropriate for implementation in a computer algebra system. To illustrate the latter we provide implementations in Mathematica and FORM, which are made available at https://github.com/vtsam/NKLO.","sentences":["We present an expression for the Feynman rule for leading-twist quark operators with an arbitrary number of total derivatives and an arbitrary number of gluons.","This provides a generalization of known results and constitutes a necessary ingredient in the computation of the matrix elements of the corresponding operators.","The result is in a form appropriate for implementation in a computer algebra system.","To illustrate the latter we provide implementations in Mathematica and FORM, which are made available at https://github.com/vtsam/NKLO."],"url":"http://arxiv.org/abs/2403.12623v1","category":"hep-ph"}
{"created":"2024-03-19 10:46:44","title":"Phase Transition and Thermodynamic Stability in an Entropy-driven Universe","abstract":"Motivated by the notion that the mathematics of gravity can be reproduced from a statistical requirement of maximal entropy, we study the consequence of introducing an entropic source term in the Einstein-Hilbert action. For a spatially homogeneous cosmological system driven by this entropic source and enveloped by a time evolving apparent horizon, we formulate a modified version of the second law of thermodynamics. An explicit differential equation governing the internal entropy profile is found. Using a Hessian matrix analysis of the internal entropy we check the thermodynamic stability for a {\\Lambda}CDM cosmology, a unified cosmic expanson and a non-singular ekpyrotic bounce. We find the mathematical condition for a second order phase transition during these evolutions from the divergence of specific heat at constant volume. The condition is purely kinematic and quadratic in nature, relating the deceleration parameter and the jerk parameter that chalks out an interesting curve on the parameter space. This condition is valid even without the entropic source term and may be a general property of any phase transition.","sentences":["Motivated by the notion that the mathematics of gravity can be reproduced from a statistical requirement of maximal entropy, we study the consequence of introducing an entropic source term in the Einstein-Hilbert action.","For a spatially homogeneous cosmological system driven by this entropic source and enveloped by a time evolving apparent horizon, we formulate a modified version of the second law of thermodynamics.","An explicit differential equation governing the internal entropy profile is found.","Using a Hessian matrix analysis of the internal entropy we check the thermodynamic stability for a {\\Lambda}CDM cosmology, a unified cosmic expanson and a non-singular ekpyrotic bounce.","We find the mathematical condition for a second order phase transition during these evolutions from the divergence of specific heat at constant volume.","The condition is purely kinematic and quadratic in nature, relating the deceleration parameter and the jerk parameter that chalks out an interesting curve on the parameter space.","This condition is valid even without the entropic source term and may be a general property of any phase transition."],"url":"http://arxiv.org/abs/2403.12622v1","category":"gr-qc"}
{"created":"2024-03-19 10:45:19","title":"Optimisation of Gyrokinetic Microstability Using Adjoint Methods","abstract":"Microinstabilities drive turbulent fluctuations in inhomogeneous, magnetized plasmas. In the context of magnetic confinement fusion devices, this leads to an enhanced transport of particles, momentum, and energy, thereby degrading confinement. In this work, we elaborate on the application of the adjoint method to efficiently determine the variation of linear growth rates for plasma microstabilities concerning a general set of external parameters within the local $\\delta \\! f$-gyrokinetic model. We then offer numerical verification of this approach. When coupled with gradient-based techniques, this methodology can facilitate the optimization process for the microstability of the confined plasmas across a high-dimensional parameter space. We present a numerical demonstration wherein the ion-temperature gradient (ITG) instability growth rate in a tokamak plasma is minimized with respect to flux surface shaping parameters. The adjoint method approach demonstrates a significant computational speed-up compared to a finite-difference gradient calculation.","sentences":["Microinstabilities drive turbulent fluctuations in inhomogeneous, magnetized plasmas.","In the context of magnetic confinement fusion devices, this leads to an enhanced transport of particles, momentum, and energy, thereby degrading confinement.","In this work, we elaborate on the application of the adjoint method to efficiently determine the variation of linear growth rates for plasma microstabilities concerning a general set of external parameters within the local $\\delta \\!","f$-gyrokinetic model.","We then offer numerical verification of this approach.","When coupled with gradient-based techniques, this methodology can facilitate the optimization process for the microstability of the confined plasmas across a high-dimensional parameter space.","We present a numerical demonstration wherein the ion-temperature gradient (ITG) instability growth rate in a tokamak plasma is minimized with respect to flux surface shaping parameters.","The adjoint method approach demonstrates a significant computational speed-up compared to a finite-difference gradient calculation."],"url":"http://arxiv.org/abs/2403.12621v1","category":"physics.plasm-ph"}
{"created":"2024-03-19 10:40:18","title":"Near-Field Channel Estimation in Dual-Band XL-MIMO with Side Information-Assisted Compressed Sensing","abstract":"Near-field communication comes to be an indispensable part of the future sixth generation (6G) communications at the arrival of the forth-coming deployment of extremely large-scale multiple-input-multiple-output (XL-MIMO) systems. Due to the substantial number of antennas, the electromagnetic radiation field is modeled by the spherical waves instead of the conventional planar waves, leading to severe weak sparsity to angular-domain near-field channel. Therefore, the channel estimation reminiscent of the conventional compression sensing (CS) approaches in the angular domain, judiciously utilized for low pilot overhead, may result in unprecedented challenges. To this end, this paper proposes a brand-new near-field channel estimation scheme by exploiting the naturally occurring useful side information. Specifically, we formulate the dual-band near-field communication model based on the fact that high-frequency systems are likely to be deployed with lower-frequency systems. Representative side information, i.e., the structural characteristic information derived by the sparsity ambiguity and the out-of-band spatial information stemming from the lower-frequency channel, is explored and tailored to materialize exceptional near-field channel estimation. Furthermore, in-depth theoretical analyses are developed to guarantee the minimum estimation error, based on which a suite of algorithms leveraging the elaborating side information are proposed. Numerical simulations demonstrate that the designed algorithms provide more assured results than the off-the-shelf approaches in the context of the dual-band near-field communications in both on- and off-grid scenarios, where the angle of departures/arrivals are discretely or continuously distributed, respectively.","sentences":["Near-field communication comes to be an indispensable part of the future sixth generation (6G) communications at the arrival of the forth-coming deployment of extremely large-scale multiple-input-multiple-output (XL-MIMO) systems.","Due to the substantial number of antennas, the electromagnetic radiation field is modeled by the spherical waves instead of the conventional planar waves, leading to severe weak sparsity to angular-domain near-field channel.","Therefore, the channel estimation reminiscent of the conventional compression sensing (CS) approaches in the angular domain, judiciously utilized for low pilot overhead, may result in unprecedented challenges.","To this end, this paper proposes a brand-new near-field channel estimation scheme by exploiting the naturally occurring useful side information.","Specifically, we formulate the dual-band near-field communication model based on the fact that high-frequency systems are likely to be deployed with lower-frequency systems.","Representative side information, i.e., the structural characteristic information derived by the sparsity ambiguity and the out-of-band spatial information stemming from the lower-frequency channel, is explored and tailored to materialize exceptional near-field channel estimation.","Furthermore, in-depth theoretical analyses are developed to guarantee the minimum estimation error, based on which a suite of algorithms leveraging the elaborating side information are proposed.","Numerical simulations demonstrate that the designed algorithms provide more assured results than the off-the-shelf approaches in the context of the dual-band near-field communications in both on- and off-grid scenarios, where the angle of departures/arrivals are discretely or continuously distributed, respectively."],"url":"http://arxiv.org/abs/2403.12620v1","category":"eess.SP"}
{"created":"2024-03-19 10:38:54","title":"NewsCaption: Named-Entity aware Captioning for Out-of-Context Media","abstract":"With the increasing influence of social media, online misinformation has grown to become a societal issue. The motivation for our work comes from the threat caused by cheapfakes, where an unaltered image is described using a news caption in a new but false-context. The main challenge in detecting such out-of-context multimedia is the unavailability of large-scale datasets. Several detection methods employ randomly selected captions to generate out-of-context training inputs. However, these randomly matched captions are not truly representative of out-of-context scenarios due to inconsistencies between the image description and the matched caption. We aim to address these limitations by introducing a novel task of out-of-context caption generation. In this work, we propose a new method that generates a realistic out-of-context caption given visual and textual context. We also demonstrate that the semantics of the generated captions can be controlled using the textual context. We also evaluate our method against several baselines and our method improves over the image captioning baseline by 6.2% BLUE-4, 2.96% CiDEr, 11.5% ROUGE, and 7.3% METEOR","sentences":["With the increasing influence of social media, online misinformation has grown to become a societal issue.","The motivation for our work comes from the threat caused by cheapfakes, where an unaltered image is described using a news caption in a new but false-context.","The main challenge in detecting such out-of-context multimedia is the unavailability of large-scale datasets.","Several detection methods employ randomly selected captions to generate out-of-context training inputs.","However, these randomly matched captions are not truly representative of out-of-context scenarios due to inconsistencies between the image description and the matched caption.","We aim to address these limitations by introducing a novel task of out-of-context caption generation.","In this work, we propose a new method that generates a realistic out-of-context caption given visual and textual context.","We also demonstrate that the semantics of the generated captions can be controlled using the textual context.","We also evaluate our method against several baselines and our method improves over the image captioning baseline by 6.2% BLUE-4, 2.96% CiDEr, 11.5% ROUGE, and 7.3% METEOR"],"url":"http://arxiv.org/abs/2403.12618v1","category":"cs.MM"}
{"created":"2024-03-19 10:24:18","title":"Parameter estimation and singularity of laws on the path space for SDEs driven by Rosenblatt processes","abstract":"In this paper, we study parameter identification for solutions to (possibly non-linear) SDEs driven by additive Rosenblatt process and singularity of the induced laws on the path space. We propose a joint estimator for the drift parameter, diffusion intensity, and Hurst index that can be computed from discrete-time observations with a bounded time horizon and we prove its strong consistency (as well as the speed of convergence) under in-fill asymptotics with a fixed time horizon. As a consequence of this strong consistency, singularity of measures generated by the solutions with different drifts is shown. This results in the invalidity of a Girsanov-type theorem for Rosenblatt processes.","sentences":["In this paper, we study parameter identification for solutions to (possibly non-linear) SDEs driven by additive Rosenblatt process and singularity of the induced laws on the path space.","We propose a joint estimator for the drift parameter, diffusion intensity, and Hurst index that can be computed from discrete-time observations with a bounded time horizon and we prove its strong consistency (as well as the speed of convergence) under in-fill asymptotics with a fixed time horizon.","As a consequence of this strong consistency, singularity of measures generated by the solutions with different drifts is shown.","This results in the invalidity of a Girsanov-type theorem for Rosenblatt processes."],"url":"http://arxiv.org/abs/2403.12610v1","category":"math.PR"}
{"created":"2024-03-19 10:12:48","title":"Obtaining the Fourier spectrum via Fourier coefficients","abstract":"The Fourier spectrum is a family of dimensions that interpolates between the Fourier and Hausdorff dimensions and are defined in terms of certain energies which capture Fourier decay. In this paper we obtain a convenient discrete representation of those energies using the Fourier coefficients. As an example application, we use this representation to establish sharp bounds for the Fourier spectrum of a general measure with bounded support, improving previous estimates of the second-named author","sentences":["The Fourier spectrum is a family of dimensions that interpolates between the Fourier and Hausdorff dimensions and are defined in terms of certain energies which capture Fourier decay.","In this paper we obtain a convenient discrete representation of those energies using the Fourier coefficients.","As an example application, we use this representation to establish sharp bounds for the Fourier spectrum of a general measure with bounded support, improving previous estimates of the second-named author"],"url":"http://arxiv.org/abs/2403.12603v1","category":"math.CA"}
{"created":"2024-03-19 10:09:34","title":"Invariance of fixation probability in Moran-like Processes on graphs","abstract":"The now famous Isothermal Theorem was introduced in a Nature Communications article in 2005 by Nowak and Liebermann and has since been the founding document of the rich field of evolutionary graph theory. Unfortunately, the Isothermal Theorem has never been proven completely. The main argument, that the projection of the graph dynamics is a Markovian Birth-and-Death- Process, is not applicable in general and leaves the question open for the case, when the projection is not Markov. To complete the proof, we first generalise the model by Liebermann and Nowak, by introducing a non-uniform selection policy to select an individual for procreation. Then, we use a martingale argument to prove that selecting with a specific selection policy related to the weight matrix of the underlying population graph still gives rise to Moran fixation probability, even outside of the framework of the original Isothermal Theorem. Our proof includes and completes the proof of the Isothermal Theorem. We follow up with a small numerical study that shows that the set of spatial Moran Processes with Moran fixation probability is even richer than previously understood. The initial condition and the selection policy play an important role in this.","sentences":["The now famous Isothermal Theorem was introduced in a Nature Communications article in 2005 by Nowak and Liebermann and has since been the founding document of the rich field of evolutionary graph theory.","Unfortunately, the Isothermal Theorem has never been proven completely.","The main argument, that the projection of the graph dynamics is a Markovian Birth-and-Death- Process, is not applicable in general and leaves the question open for the case, when the projection is not Markov.","To complete the proof, we first generalise the model by Liebermann and Nowak, by introducing a non-uniform selection policy to select an individual for procreation.","Then, we use a martingale argument to prove that selecting with a specific selection policy related to the weight matrix of the underlying population graph still gives rise to Moran fixation probability, even outside of the framework of the original Isothermal Theorem.","Our proof includes and completes the proof of the Isothermal Theorem.","We follow up with a small numerical study that shows that the set of spatial Moran Processes with Moran fixation probability is even richer than previously understood.","The initial condition and the selection policy play an important role in this."],"url":"http://arxiv.org/abs/2403.12598v1","category":"math.PR"}
{"created":"2024-03-19 10:03:07","title":"Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs","abstract":"Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \\citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA.   We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by \\citet{hsieh2023distilling}.   Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt \\cite{chen2023program}, our model outperforms the recently introduced Gemini Ultra and GPT-4V.","sentences":["Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks.","However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements.","We propose a technique to transfer capabilities from LLMs to VLMs.","On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \\citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA.   ","We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \\citet{liu2023deplot}.","We then propose constructing a 20x larger dataset than the original training set.","To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts.","Lastly, our model is fine-tuned using the multitask loss introduced by \\citet{hsieh2023distilling}.   ","Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline.","When rationales are further refined with a simple program-of-thought prompt \\cite{chen2023program}, our model outperforms the recently introduced Gemini Ultra and GPT-4V."],"url":"http://arxiv.org/abs/2403.12596v1","category":"cs.CL"}
{"created":"2024-03-19 09:51:20","title":"Simulation of the Wave Turbulence of a Liquid Surface Using the Dynamic Conformal Transformation Method","abstract":"The dynamic conformal transformation method has been generalized for the first time to numerically simulate the capillary wave turbulence of a liquid surface in the plane symmetric anisotropic geometry. The model is strongly nonlinear and involves effects of surface tension, as well as energy dissipation and pumping. Simulation results have shown that the system of nonlinear capillary waves can pass to the quasistationary chaotic motion regime (wave turbulence). The calculated exponents of spectra do not coincide with those for the classical Zakharov-Filonenko spectrum for isotropic capillary turbulence but are in good agreement with the estimate obtained under the assumption of the dominant effect of five-wave resonant interactions.","sentences":["The dynamic conformal transformation method has been generalized for the first time to numerically simulate the capillary wave turbulence of a liquid surface in the plane symmetric anisotropic geometry.","The model is strongly nonlinear and involves effects of surface tension, as well as energy dissipation and pumping.","Simulation results have shown that the system of nonlinear capillary waves can pass to the quasistationary chaotic motion regime (wave turbulence).","The calculated exponents of spectra do not coincide with those for the classical Zakharov-Filonenko spectrum for isotropic capillary turbulence but are in good agreement with the estimate obtained under the assumption of the dominant effect of five-wave resonant interactions."],"url":"http://arxiv.org/abs/2403.12592v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 09:48:18","title":"FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting","abstract":"Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a relevant discrete set of actions. Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets. This approach relies on inherent computations made by the actor-critic DRL architecture. We demonstrate the validity of our approach with simulation results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition.","sentences":["Designing a humanoid locomotion controller is challenging and classically split up in sub-problems.","Footstep planning is one of those, where the sequence of footsteps is defined.","Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem.","In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*).","However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters.","In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference.","Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps.","In contrast, other methods necessitate the selection of a relevant discrete set of actions.","Second, we propose a forecasting method, allowing to quickly estimate the number of footsteps required to reach different candidates of local targets.","This approach relies on inherent computations made by the actor-critic DRL architecture.","We demonstrate the validity of our approach with simulation results, and by a deployment on a kid-size humanoid robot during the RoboCup 2023 competition."],"url":"http://arxiv.org/abs/2403.12589v1","category":"cs.RO"}
{"created":"2024-03-19 09:47:54","title":"Machine Learning of the Prime Distribution","abstract":"In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.","sentences":["In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem.","We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques.","Numerical experiments that we perform corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2403.12588v1","category":"cs.IT"}
{"created":"2024-03-19 09:47:16","title":"An Adaptive feature mode decomposition based on a novel health indicator for bearing fault diagnosis","abstract":"The vibration analysis of the bearing is very crucial because of its non-stationary nature and low signal-to-noise ratio. Therefore, a novel scheme for detecting bearing defects is put forward based on the extraction of single-valued neutrosophic cross-entropy (SVNCE) to address this issue. Initially, the artificial hummingbird algorithm (AHA) is used to make the feature mode decomposition (FMD) adaptive by optimizing its parameter based on a newly developed health indicator (HI) i.e. sparsity impact measure index (SIMI). This HI ensures full sparsity and impact properties simultaneously. The raw signals are disintegrated into different modes by adaptive FMD at optimal values of its parameters. The energy of these modes is calculated for different health conditions. The energy interval range has been decided based on energy eigen which are then transformed into single-valued neutrosophic sets (SVNSs) for unknown defect conditions. The minimum argument principle employs the least SVNCE values between SVNSs of testing samples (obtained from unknown bearing conditions) and SVNSs of training samples (obtained from known bearing conditions) to recognize the different defects in the bearing. It has been discovered that the suggested methodology is more adept at identifying the various bearing defects.","sentences":["The vibration analysis of the bearing is very crucial because of its non-stationary nature and low signal-to-noise ratio.","Therefore, a novel scheme for detecting bearing defects is put forward based on the extraction of single-valued neutrosophic cross-entropy (SVNCE) to address this issue.","Initially, the artificial hummingbird algorithm (AHA) is used to make the feature mode decomposition (FMD) adaptive by optimizing its parameter based on a newly developed health indicator (HI) i.e. sparsity impact measure index (SIMI).","This HI ensures full sparsity and impact properties simultaneously.","The raw signals are disintegrated into different modes by adaptive FMD at optimal values of its parameters.","The energy of these modes is calculated for different health conditions.","The energy interval range has been decided based on energy eigen which are then transformed into single-valued neutrosophic sets (SVNSs) for unknown defect conditions.","The minimum argument principle employs the least SVNCE values between SVNSs of testing samples (obtained from unknown bearing conditions) and SVNSs of training samples (obtained from known bearing conditions) to recognize the different defects in the bearing.","It has been discovered that the suggested methodology is more adept at identifying the various bearing defects."],"url":"http://arxiv.org/abs/2403.12586v1","category":"eess.SP"}
{"created":"2024-03-19 09:45:49","title":"Quantixar: High-performance Vector Data Management System","abstract":"Traditional database management systems need help efficiently represent and querying the complex, high-dimensional data prevalent in modern applications. Vector databases offer a solution by storing data as numerical vectors within a multi-dimensional space. This enables similarity-based search and analysis, such as image retrieval, recommendation engine generation, and natural language processing. This paper introduces Quantixar, a vector database project designed for efficiency in high-dimensional settings. Quantixar tackles the challenge of managing high-dimensional data by strategically combining advanced indexing and quantization techniques. It employs HNSW indexing for accelerated ANN search. Additionally, Quantixar incorporates binary and product quantization to compress high-dimensional vectors, reducing storage requirements and computational costs during search. The paper delves into Quantixar's architecture, specific implementation, and experimental methodology.","sentences":["Traditional database management systems need help efficiently represent and querying the complex, high-dimensional data prevalent in modern applications.","Vector databases offer a solution by storing data as numerical vectors within a multi-dimensional space.","This enables similarity-based search and analysis, such as image retrieval, recommendation engine generation, and natural language processing.","This paper introduces Quantixar, a vector database project designed for efficiency in high-dimensional settings.","Quantixar tackles the challenge of managing high-dimensional data by strategically combining advanced indexing and quantization techniques.","It employs HNSW indexing for accelerated ANN search.","Additionally, Quantixar incorporates binary and product quantization to compress high-dimensional vectors, reducing storage requirements and computational costs during search.","The paper delves into Quantixar's architecture, specific implementation, and experimental methodology."],"url":"http://arxiv.org/abs/2403.12583v1","category":"cs.DB"}
{"created":"2024-03-19 09:45:33","title":"AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework","abstract":"The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering. Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, large language models (LLMs) have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLMs still suffer from hallucinations and are unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data. It has a positive impact on training LLMs for completing financial analysis. We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques. Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis.","sentences":["The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering.","Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress.","However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes.","Also, they can not integrate textual information such as financial news or reports.","Meanwhile, large language models (LLMs) have remarkable textual understanding and generation ability.","But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLMs still suffer from hallucinations and are unable to keep up with the latest information.","To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data.","It has a positive impact on training LLMs for completing financial analysis.","We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques.","Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis."],"url":"http://arxiv.org/abs/2403.12582v1","category":"cs.CL"}
{"created":"2024-03-19 09:44:41","title":"Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection","abstract":"Industrial anomaly detection (IAD) has garnered significant attention and experienced rapid development. However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations. On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios. On the other hand, the research on various new practical anomaly detection settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results. Therefore, we propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets. It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets. To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics. In addition, beyond the general unsupervised anomaly detection setting, we propose a new setting for Fully Unsupervised Industrial Anomaly Detection (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value. Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging benchmark to promote the development of the IAD field.","sentences":["Industrial anomaly detection (IAD) has garnered significant attention and experienced rapid development.","However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations.","On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios.","On the other hand, the research on various new practical anomaly detection settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results.","Therefore, we propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets.","It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets.","To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics.","In addition, beyond the general unsupervised anomaly detection setting, we propose a new setting for Fully Unsupervised Industrial Anomaly Detection (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value.","Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging benchmark to promote the development of the IAD field."],"url":"http://arxiv.org/abs/2403.12580v1","category":"cs.CV"}
{"created":"2024-03-19 09:34:13","title":"Exact model reduction for conditional quantum dynamics","abstract":"Leveraging an algebraic approach built on minimal realizations and conditional expectations in quantum probability, we propose a method to reduce the dimension of quantum filters while maintaining the correct distributions on the outcomes of interest. The method is presented for general quantum systems whose dynamics depend on measurement outcomes, hinges on a system-theoretic observability analysis, and is tested on prototypical examples.","sentences":["Leveraging an algebraic approach built on minimal realizations and conditional expectations in quantum probability, we propose a method to reduce the dimension of quantum filters while maintaining the correct distributions on the outcomes of interest.","The method is presented for general quantum systems whose dynamics depend on measurement outcomes, hinges on a system-theoretic observability analysis, and is tested on prototypical examples."],"url":"http://arxiv.org/abs/2403.12575v1","category":"quant-ph"}
{"created":"2024-03-19 09:34:11","title":"EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks","abstract":"Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection. Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules. Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps. For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps. Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models.","sentences":["Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions.","However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed.","Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge.","In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler.","Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection.","Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules.","Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps.","For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps.","Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models."],"url":"http://arxiv.org/abs/2403.12574v1","category":"cs.CV"}
{"created":"2024-03-19 09:33:07","title":"Lifting Multi-View Detection and Tracking to the Bird's Eye View","abstract":"Taking advantage of multi-view aggregation presents a promising solution to tackle challenges such as occlusion and missed detection in multi-object tracking and detection. Recent advancements in multi-view detection and 3D object recognition have significantly improved performance by strategically projecting all views onto the ground plane and conducting detection analysis from a Bird's Eye View. In this paper, we compare modern lifting methods, both parameter-free and parameterized, to multi-view aggregation. Additionally, we present an architecture that aggregates the features of multiple times steps to learn robust detection and combines appearance- and motion-based cues for tracking. Most current tracking approaches either focus on pedestrians or vehicles. In our work, we combine both branches and add new challenges to multi-view detection with cross-scene setups. Our method generalizes to three public datasets across two domains: (1) pedestrian: Wildtrack and MultiviewX, and (2) roadside perception: Synthehicle, achieving state-of-the-art performance in detection and tracking. https://github.com/tteepe/TrackTacular","sentences":["Taking advantage of multi-view aggregation presents a promising solution to tackle challenges such as occlusion and missed detection in multi-object tracking and detection.","Recent advancements in multi-view detection and 3D object recognition have significantly improved performance by strategically projecting all views onto the ground plane and conducting detection analysis from a Bird's Eye View.","In this paper, we compare modern lifting methods, both parameter-free and parameterized, to multi-view aggregation.","Additionally, we present an architecture that aggregates the features of multiple times steps to learn robust detection and combines appearance- and motion-based cues for tracking.","Most current tracking approaches either focus on pedestrians or vehicles.","In our work, we combine both branches and add new challenges to multi-view detection with cross-scene setups.","Our method generalizes to three public datasets across two domains: (1) pedestrian: Wildtrack and MultiviewX, and (2) roadside perception: Synthehicle, achieving state-of-the-art performance in detection and tracking.","https://github.com/tteepe/TrackTacular"],"url":"http://arxiv.org/abs/2403.12573v1","category":"cs.CV"}
{"created":"2024-03-19 09:30:56","title":"Compound Expression Recognition via Multi Model Ensemble","abstract":"Compound Expression Recognition (CER) plays a crucial role in interpersonal interactions. Due to the existence of Compound Expressions , human emotional expressions are complex, requiring consideration of both local and global facial expressions to make judgments. In this paper, to address this issue, we propose a solution based on ensemble learning methods for Compound Expression Recognition. Specifically, our task is classification, where we train three expression classification models based on convolutional networks, Vision Transformers, and multi-scale local attention networks. Then, through model ensemble using late fusion, we merge the outputs of multiple models to predict the final result. Our method achieves high accuracy on RAF-DB and is able to recognize expressions through zero-shot on certain portions of C-EXPR-DB.","sentences":["Compound Expression Recognition (CER) plays a crucial role in interpersonal interactions.","Due to the existence of Compound Expressions , human emotional expressions are complex, requiring consideration of both local and global facial expressions to make judgments.","In this paper, to address this issue, we propose a solution based on ensemble learning methods for Compound Expression Recognition.","Specifically, our task is classification, where we train three expression classification models based on convolutional networks, Vision Transformers, and multi-scale local attention networks.","Then, through model ensemble using late fusion, we merge the outputs of multiple models to predict the final result.","Our method achieves high accuracy on RAF-DB and is able to recognize expressions through zero-shot on certain portions of C-EXPR-DB."],"url":"http://arxiv.org/abs/2403.12572v1","category":"cs.CV"}
{"created":"2024-03-19 09:28:19","title":"Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images","abstract":"Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains. However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection. This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection. Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels. This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images. The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training. Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively. Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD","sentences":["Recent advancements in large-scale visual-language pre-trained models have led to significant progress in zero-/few-shot anomaly detection within natural image domains.","However, the substantial domain divergence between natural and medical images limits the effectiveness of these methodologies in medical anomaly detection.","This paper introduces a novel lightweight multi-level adaptation and comparison framework to repurpose the CLIP model for medical anomaly detection.","Our approach integrates multiple residual adapters into the pre-trained visual encoder, enabling a stepwise enhancement of visual features across different levels.","This multi-level adaptation is guided by multi-level, pixel-wise visual-language feature alignment loss functions, which recalibrate the model's focus from object semantics in natural imagery to anomaly identification in medical images.","The adapted features exhibit improved generalization across various medical data types, even in zero-shot scenarios where the model encounters unseen medical modalities and anatomical regions during training.","Our experiments on medical anomaly detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art models, with an average AUC improvement of 6.24% and 7.33% for anomaly classification, 2.03% and 2.37% for anomaly segmentation, under the zero-shot and few-shot settings, respectively.","Source code is available at: https://github.com/MediaBrain-SJTU/MVFA-AD"],"url":"http://arxiv.org/abs/2403.12570v1","category":"cs.CV"}
{"created":"2024-03-19 09:25:49","title":"Recovering Composition Algebras from 3D Geometric Algebras","abstract":"Generalized Hurwitz theorem states that there are fifteen composition algebras for any given field: seven unital, six para-unital, and two non-unital algebras. In this article we explore the recovery of such algebras from 3D Geometric Algebras. Different involutions, such as reversion, inversion, Clifford conjugation, and full grade inversion, are introduced in order to recover the norm of all composition algebras. A special attention is given to composition algebras of dimension 8, i. e. octonions, para-octonions and Okubo algebra, for which the introduction of a different product is needed.","sentences":["Generalized Hurwitz theorem states that there are fifteen composition algebras for any given field: seven unital, six para-unital, and two non-unital algebras.","In this article we explore the recovery of such algebras from 3D Geometric Algebras.","Different involutions, such as reversion, inversion, Clifford conjugation, and full grade inversion, are introduced in order to recover the norm of all composition algebras.","A special attention is given to composition algebras of dimension 8, i. e. octonions, para-octonions and Okubo algebra, for which the introduction of a different product is needed."],"url":"http://arxiv.org/abs/2403.12569v1","category":"math.RA"}
{"created":"2024-03-19 09:22:50","title":"Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer IoT Devices","abstract":"Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices. For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential. However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues. In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference. We design a memory-efficient management method to support memory-demanding inference in TEEs. By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in the trusted operating system. Additionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny deep learning library, and Tinylibm (827 LoCs), a tiny math library, to support efficient inference in TEEs. We implemented a prototype on Raspberry Pi 3B+ and evaluated it using three well-known lightweight DNN models. The experimental results demonstrate that our design significantly improves inference speed by 3.13 times and reduces power consumption by over 66.5% compared to non-memory optimization method in TEEs.","sentences":["Edge intelligence enables resource-demanding Deep Neural Network (DNN) inference without transferring original data, addressing concerns about data privacy in consumer Internet of Things (IoT) devices.","For privacy-sensitive applications, deploying models in hardware-isolated trusted execution environments (TEEs) becomes essential.","However, the limited secure memory in TEEs poses challenges for deploying DNN inference, and alternative techniques like model partitioning and offloading introduce performance degradation and security issues.","In this paper, we present a novel approach for advanced model deployment in TrustZone that ensures comprehensive privacy preservation during model inference.","We design a memory-efficient management method to support memory-demanding inference in TEEs.","By adjusting the memory priority, we effectively mitigate memory leakage risks and memory overlap conflicts, resulting in 32 lines of code alterations in the trusted operating system.","Additionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny deep learning library, and Tinylibm (827 LoCs), a tiny math library, to support efficient inference in TEEs.","We implemented a prototype on Raspberry Pi 3B+ and evaluated it using three well-known lightweight DNN models.","The experimental results demonstrate that our design significantly improves inference speed by 3.13 times and reduces power consumption by over 66.5% compared to non-memory optimization method in TEEs."],"url":"http://arxiv.org/abs/2403.12568v1","category":"cs.CR"}
{"created":"2024-03-19 09:21:43","title":"NN-ETM: Enabling safe neural network-based event-triggering mechanisms for consensus problems","abstract":"In networked control applications, event-triggering mechanisms (ETMs) reduce the communication load while ensuring performance guarantees. However, the design of ETMs is becoming increasingly complex, particularly for decentralized multi-agent and consensus setups, where the condition used to trigger communication might incorporate the agent's local information and the information received from neighbors. This typically results in ad-hoc solutions, which may only work for the consensus protocols under consideration. In this work, we aim to safely incorporate neural networks in the ETM to provide a general and flexible solution while ensuring guaranteed performance. To decouple the stability analysis of the consensus protocol from the abstraction of the neural network in the ETM, we first derive design criteria for the consensus and ETM pair, which allow independent analysis of each element under mild constraints. As a result, we propose NN-ETM, a novel ETM featuring a neural network, which provides an all-purpose solution to optimize communication in consensus problems while preserving the stability guarantees of the consensus protocol.","sentences":["In networked control applications, event-triggering mechanisms (ETMs) reduce the communication load while ensuring performance guarantees.","However, the design of ETMs is becoming increasingly complex, particularly for decentralized multi-agent and consensus setups, where the condition used to trigger communication might incorporate the agent's local information and the information received from neighbors.","This typically results in ad-hoc solutions, which may only work for the consensus protocols under consideration.","In this work, we aim to safely incorporate neural networks in the ETM to provide a general and flexible solution while ensuring guaranteed performance.","To decouple the stability analysis of the consensus protocol from the abstraction of the neural network in the ETM, we first derive design criteria for the consensus and ETM pair, which allow independent analysis of each element under mild constraints.","As a result, we propose NN-ETM, a novel ETM featuring a neural network, which provides an all-purpose solution to optimize communication in consensus problems while preserving the stability guarantees of the consensus protocol."],"url":"http://arxiv.org/abs/2403.12567v1","category":"eess.SY"}
{"created":"2024-03-19 09:20:43","title":"Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai","abstract":"In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity. Specifically, we implement a prototype-based approach to pinpoint contexts that mirror similar user preferences. To amplify accuracy and interpretability, we employ JS divergence of PoI attributes such as categories and prices as a measure of similarity between contexts. A temporal graph integrating both prototype and context nodes helps incorporate temporal information. We then identify appropriate prototypes considering both target contexts and short-term user preferences. Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention. Since its inception in 2023, this strategy has been adopted in Meituan Waimai's display recommender system, leading to a 4.6% surge in CTR and a 4.2% boost in GMV.","sentences":["In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively.","Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs.","To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences.","In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences.","For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences.","We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts.","This approach eliminates the necessity to select a sub-sequence for every candidate PoI, thereby avoiding high time complexity.","Specifically, we implement a prototype-based approach to pinpoint contexts that mirror similar user preferences.","To amplify accuracy and interpretability, we employ JS divergence of PoI attributes such as categories and prices as a measure of similarity between contexts.","A temporal graph integrating both prototype and context nodes helps incorporate temporal information.","We then identify appropriate prototypes considering both target contexts and short-term user preferences.","Following this, we utilize contexts aligned with these prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores with target attention.","Since its inception in 2023, this strategy has been adopted in Meituan Waimai's display recommender system, leading to a 4.6% surge in CTR and a 4.2% boost in GMV."],"url":"http://arxiv.org/abs/2403.12566v1","category":"cs.IR"}
{"created":"2024-03-19 09:17:25","title":"Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service","abstract":"Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource. In Indonesian, only a few works were found on long-text classification using Transformers. Most only use a small amount of data and do not report any HPO. In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer. We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words. To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library. Using the best hack found, we then compare 512, 256, and 128 tokens length. We find that removing stopwords while keeping punctuation and low-frequency words is the best hack. Some of our setups manage to outperform taking 512 first tokens using a smaller 128 or 256 first tokens which manage to represent the same information while requiring less computational resources. The findings could help developers to efficiently pursue optimal performance of the models using limited resources.","sentences":["Many NLP researchers rely on free computational services, such as Google Colab, to fine-tune their Transformer models, causing a limitation for hyperparameter optimization (HPO) in long-text classification due to the method having quadratic complexity and needing a bigger resource.","In Indonesian, only a few works were found on long-text classification using Transformers.","Most only use a small amount of data and do not report any HPO.","In this study, using 18k news articles, we investigate which pretrained models are recommended to use based on the output length of the tokenizer.","We then compare some hacks to shorten and enrich the sequences, which are the removals of stopwords, punctuation, low-frequency words, and recurring words.","To get a fair comparison, we propose and run an efficient and dynamic HPO procedure that can be done gradually on a limited resource and does not require a long-running optimization library.","Using the best hack found, we then compare 512, 256, and 128 tokens length.","We find that removing stopwords while keeping punctuation and low-frequency words is the best hack.","Some of our setups manage to outperform taking 512 first tokens using a smaller 128 or 256 first tokens which manage to represent the same information while requiring less computational resources.","The findings could help developers to efficiently pursue optimal performance of the models using limited resources."],"url":"http://arxiv.org/abs/2403.12563v1","category":"cs.CL"}
{"created":"2024-03-19 09:17:18","title":"Equity through Access: A Case for Small-scale Deep Learning","abstract":"The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using pretrained models can significantly reduce the computational resources and data required. We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.","sentences":["The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute.","These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions.","These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South.","In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited.","To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score.","Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs.","In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models.","Furthermore, we show that using pretrained models can significantly reduce the computational resources and data required.","We hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints."],"url":"http://arxiv.org/abs/2403.12562v1","category":"cs.LG"}
{"created":"2024-03-19 09:15:06","title":"The SIS process on Erd\u00f6s-R\u00e9nyi graphs: determining the infected fraction","abstract":"The SIS process on a graph poses many challenges. An important problem is to identify characteristics of the metastable behaviour. Existing mean-field methods (such as Heterogeneous Mean Field and the N-intertwined Mean Field Approximation) overestimate the metastable infected fraction, because they ignore correlations. Especially in sparse graphs, this leads to serious inaccuracies. We propose quenched and annealed methods incorporating correlations and giving significantly more accurate approximations. We use the Erd\\\"os-R\\'enyi graph as a test case, but the methods can be generalized easily. Our methods are computationally very friendly and can be applied to fairly large graphs, in contrast with some other second-order mean field approximations.","sentences":["The SIS process on a graph poses many challenges.","An important problem is to identify characteristics of the metastable behaviour.","Existing mean-field methods (such as Heterogeneous Mean Field and the N-intertwined Mean Field Approximation) overestimate the metastable infected fraction, because they ignore correlations.","Especially in sparse graphs, this leads to serious inaccuracies.","We propose quenched and annealed methods incorporating correlations and giving significantly more accurate approximations.","We use the Erd\\\"os-R\\'enyi graph as a test case, but the methods can be generalized easily.","Our methods are computationally very friendly and can be applied to fairly large graphs, in contrast with some other second-order mean field approximations."],"url":"http://arxiv.org/abs/2403.12560v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-19 09:13:33","title":"On the orbit-induced spin density of tightly focused optical vortex beams: ellipticity and helicity","abstract":"It has recently been established that a linearly-polarized optical vortex possesses spin angular momentum density in the direction of propagation (longitudinal spin) under tight-focusing. The helicity of light has long been associated with longitudinal spin angular momentum. Here we show that the longitudinal spin density of linearly-polarized vortices is anomalous because it has no associated helicity. It was also recently determined that the polarization-independent helicity of tightly-focused optical vortices is associated with their transverse spin momentum density. The key finding of this work is the fact that, in general, longitudinal spin can not necessarily be associated with helicity, and transverse spin is in general not associated with a zero helicity, and such extraordinary behaviour manifests most clearly for optical vortices under non-paraxial conditions.","sentences":["It has recently been established that a linearly-polarized optical vortex possesses spin angular momentum density in the direction of propagation (longitudinal spin) under tight-focusing.","The helicity of light has long been associated with longitudinal spin angular momentum.","Here we show that the longitudinal spin density of linearly-polarized vortices is anomalous because it has no associated helicity.","It was also recently determined that the polarization-independent helicity of tightly-focused optical vortices is associated with their transverse spin momentum density.","The key finding of this work is the fact that, in general, longitudinal spin can not necessarily be associated with helicity, and transverse spin is in general not associated with a zero helicity, and such extraordinary behaviour manifests most clearly for optical vortices under non-paraxial conditions."],"url":"http://arxiv.org/abs/2403.12558v1","category":"physics.optics"}
{"created":"2024-03-19 09:10:36","title":"Numerical approximation of a class of constrained Hamilton-Jacobi equations","abstract":"In this paper, we introduce a framework for the discretization of a class of constrained Hamilton-Jacobi equations, a system coupling a Hamilton-Jacobi equation with a Lagrange multiplier determined by the constraint. The equation is non-local, and the constraint has bounded variations. We show that, under a set of general hypothesis, the approximation obtained with a finite-differences monotonic scheme, converges towards the viscosity solution of the constrained Hamilton-Jacobi equation.   Constrained Hamilton-Jacobi equations often arise as the long time and small mutation asymptotics of population models in quantitative genetics. As an example, we detail the construction of a scheme for the limit of an integral Lotka-Volterra equation. We also construct and analyze an Asymptotic-Preserving (AP) scheme for the model outside of the asymptotics. We prove that it is stable along the transition towards the asymptotics.   The theoretical analysis of the schemes is illustrated and discussed with numerical simulations. The AP scheme is also used to conjecture the asymptotic behavior of the integral Lotka-Volterra equation, when the environment varies in time.","sentences":["In this paper, we introduce a framework for the discretization of a class of constrained Hamilton-Jacobi equations, a system coupling a Hamilton-Jacobi equation with a Lagrange multiplier determined by the constraint.","The equation is non-local, and the constraint has bounded variations.","We show that, under a set of general hypothesis, the approximation obtained with a finite-differences monotonic scheme, converges towards the viscosity solution of the constrained Hamilton-Jacobi equation.   ","Constrained Hamilton-Jacobi equations often arise as the long time and small mutation asymptotics of population models in quantitative genetics.","As an example, we detail the construction of a scheme for the limit of an integral Lotka-Volterra equation.","We also construct and analyze an Asymptotic-Preserving (AP) scheme for the model outside of the asymptotics.","We prove that it is stable along the transition towards the asymptotics.   ","The theoretical analysis of the schemes is illustrated and discussed with numerical simulations.","The AP scheme is also used to conjecture the asymptotic behavior of the integral Lotka-Volterra equation, when the environment varies in time."],"url":"http://arxiv.org/abs/2403.12557v1","category":"math.NA"}
{"created":"2024-03-19 08:59:13","title":"Impact of non-zero strange quark mass $(m_{s}\\neq0)$ in $f(R,T)$ gravity admitting observational results of strange stars","abstract":"In this article we propose a new class of isotropic strange star using Buchdahl-I metric ansatz and MIT bag model equation of state in presence of non-zero strange quark mass $(m_{s})$ in the framework of modified $f(R,T)$ theory of gravity. The barotropic form of MIT bag model equation of state and a specific class of $f(R,T)$ model {\\it viz.}, $f(R,T)=R+2\\alpha_{c}T$ where $\\alpha_{c}$ is termed as the gravity-matter coupling constant, produces a tractable set of solutions of Einstein field equations. From the numerical limit of the coupling constant $(\\alpha_{c})$, we have considered a range of $\\alpha_{c}$ from -2.0 to 2.0. Maximum mass and radius in this model is found by numerically solving the TOV equations and we note that within the stability window imposed by energy per baryon calculation, for a particular bag constant $B=70~MeV/fm^{3}$, $m_{s}$ and $\\alpha_{c}$ act as a constraining factor. Interestingly, the increment of $m_{s}$ and $\\alpha_{c}$ results in a softer equation of state which leads to the decrease in the maximum mass and radius while negative values of $\\alpha_{c}$ leads to a stiffer equation of state thereby increasing the maximum mass and radius in the present model. For physical application, we consider EXO 1745-248 and tabulate the effects of $m_{s}$ and $\\alpha_{c}$ on its radius. Using the formalism, we have analysed the characteristic properties of EXO 1745-248. Apart from that, we have predicted the radii of a wide range of strange star candidates in the context of $f(R,T)$ gravity and the results agree well with the observed properties. We note that our model satisfies all the necessary energy conditions and stability criteria to emerge as a viable stellar configuration.","sentences":["In this article we propose a new class of isotropic strange star using Buchdahl-I metric ansatz and MIT bag model equation of state in presence of non-zero strange quark mass $(m_{s})$ in the framework of modified $f(R,T)$ theory of gravity.","The barotropic form of MIT bag model equation of state and a specific class of $f(R,T)$ model {\\it viz.","}, $f(R,T)=R+2\\alpha_{c}T$ where $\\alpha_{c}$ is termed as the gravity-matter coupling constant, produces a tractable set of solutions of Einstein field equations.","From the numerical limit of the coupling constant $(\\alpha_{c})$, we have considered a range of $\\alpha_{c}$ from -2.0 to 2.0.","Maximum mass and radius in this model is found by numerically solving the TOV equations and we note that within the stability window imposed by energy per baryon calculation, for a particular bag constant $B=70~MeV/fm^{3}$, $m_{s}$ and $\\alpha_{c}$ act as a constraining factor.","Interestingly, the increment of $m_{s}$ and $\\alpha_{c}$ results in a softer equation of state which leads to the decrease in the maximum mass and radius while negative values of $\\alpha_{c}$ leads to a stiffer equation of state thereby increasing the maximum mass and radius in the present model.","For physical application, we consider EXO 1745-248 and tabulate the effects of $m_{s}$ and $\\alpha_{c}$ on its radius.","Using the formalism, we have analysed the characteristic properties of EXO 1745-248.","Apart from that, we have predicted the radii of a wide range of strange star candidates in the context of $f(R,T)$ gravity and the results agree well with the observed properties.","We note that our model satisfies all the necessary energy conditions and stability criteria to emerge as a viable stellar configuration."],"url":"http://arxiv.org/abs/2403.12555v1","category":"gr-qc"}
{"created":"2024-03-19 08:54:52","title":"M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving","abstract":"End-to-end autonomous driving has witnessed remarkable progress. However, the extensive deployment of autonomous vehicles has yet to be realized, primarily due to 1) inefficient multi-modal environment perception: how to integrate data from multi-modal sensors more efficiently; 2) non-human-like scene understanding: how to effectively locate and predict critical risky agents in traffic scenarios like an experienced driver. To overcome these challenges, in this paper, we propose a Multi-Modal fusion transformer incorporating Driver Attention (M2DA) for autonomous driving. To better fuse multi-modal data and achieve higher alignment between different modalities, a novel Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By incorporating driver attention, we empower the human-like scene understanding ability to autonomous vehicles to identify crucial areas within complex scenarios precisely and ensure safety. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance with less data in closed-loop benchmarks. Source codes are available at https://anonymous.4open.science/r/M2DA-4772.","sentences":["End-to-end autonomous driving has witnessed remarkable progress.","However, the extensive deployment of autonomous vehicles has yet to be realized, primarily due to 1) inefficient multi-modal environment perception: how to integrate data from multi-modal sensors more efficiently; 2) non-human-like scene understanding: how to effectively locate and predict critical risky agents in traffic scenarios like an experienced driver.","To overcome these challenges, in this paper, we propose a Multi-Modal fusion transformer incorporating Driver Attention (M2DA) for autonomous driving.","To better fuse multi-modal data and achieve higher alignment between different modalities, a novel Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed.","By incorporating driver attention, we empower the human-like scene understanding ability to autonomous vehicles to identify crucial areas within complex scenarios precisely and ensure safety.","We conduct experiments on the CARLA simulator and achieve state-of-the-art performance with less data in closed-loop benchmarks.","Source codes are available at https://anonymous.4open.science/r/M2DA-4772."],"url":"http://arxiv.org/abs/2403.12552v1","category":"cs.CV"}
{"created":"2024-03-19 08:49:48","title":"RGBD GS-ICP SLAM","abstract":"Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications. Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation. In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits. Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system. Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods. Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map.","sentences":["Simultaneous Localization and Mapping (SLAM) with dense representation plays a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR) applications.","Recent advancements in dense representation SLAM have highlighted the potential of leveraging neural scene representation and 3D Gaussian representation for high-fidelity spatial representation.","In this paper, we propose a novel dense representation SLAM approach with a fusion of Generalized Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS).","In contrast to existing methods, we utilize a single Gaussian map for both tracking and mapping, resulting in mutual benefits.","Through the exchange of covariances between tracking and mapping processes with scale alignment techniques, we minimize redundant computations and achieve an efficient system.","Additionally, we enhance tracking accuracy and mapping quality through our keyframe selection methods.","Experimental results demonstrate the effectiveness of our approach, showing an incredibly fast speed up to 107 FPS (for the entire system) and superior quality of the reconstructed map."],"url":"http://arxiv.org/abs/2403.12550v1","category":"cs.CV"}
{"created":"2024-03-19 08:48:22","title":"Treewidth of generalized Hamming graph, bipartite Kneser graph and generalized Petersen graph","abstract":"Let $t,q$ and $n$ be positive integers. Write $[q] = \\{1,2,\\ldots,q\\}$. The generalized Hamming graph $H(t,q,n)$ is the graph whose vertex set is the cartesian product of $n$ copies of $[q]$$(q\\ge 2)$, where two vertices are adjacent if their Hamming distance is at most $t$. In particular, $H(1,q,n)$ is the well-known Hamming graph and $H(1,2,n)$ is the hypercube. In 2006, Chandran and Kavitha described the asymptotic value of $tw(H(1,q,n))$, where $tw(G)$ denotes the treewidth of $G$. In this paper, we give the exact pathwidth of $H(t,2,n)$ and show that $tw(H(t,q,n)) = \\Theta(tq^n/\\sqrt{n})$ when $n$ goes to infinity. Based on those results, we show that the treewidth of bipartite Kneser graph $BK(n,k)$ is $\\binom{n}{k} - 1$ when $n$ is sufficient large relative to $k$ and the bounds of $tw(BK(2k+1,k))$ are given. Moreover, we present the bounds of the treewidth of generalized Petersen graph.","sentences":["Let $t,q$ and $n$ be positive integers.","Write $[q] = \\{1,2,\\ldots,q\\}$. The generalized Hamming graph $H(t,q,n)$ is the graph whose vertex set is the cartesian product of $n$ copies of $[q]$$(q\\ge 2)$, where two vertices are adjacent if their Hamming distance is at most $t$. In particular, $H(1,q,n)$ is the well-known Hamming graph and $H(1,2,n)$ is the hypercube.","In 2006, Chandran and Kavitha described the asymptotic value of $tw(H(1,q,n))$, where $tw(G)$ denotes the treewidth of $G$. In this paper, we give the exact pathwidth of $H(t,2,n)$ and show that $tw(H(t,q,n)) = \\Theta(tq^n/\\sqrt{n})$ when $n$ goes to infinity.","Based on those results, we show that the treewidth of bipartite Kneser graph $BK(n,k)$ is $\\binom{n}{k} - 1$ when $n$ is sufficient large relative to $k$ and the bounds of $tw(BK(2k+1,k))$ are given.","Moreover, we present the bounds of the treewidth of generalized Petersen graph."],"url":"http://arxiv.org/abs/2403.12549v1","category":"math.CO"}
{"created":"2024-03-19 08:41:45","title":"Motivic Hilbert zeta functions for curve singularities and related invariants","abstract":"In the present paper, we show that the motivic Hilbert zeta function for a curve singularity yields the generating functions for Euler numbers of punctual Hilbert schemes when any punctual Hilbert scheme admits an affine cell decomposition. This fact allows us to derive the relations among the motivic Hilbert zeta function and other invariants such as the generating function for semi-modules of the semi-group of the singularity, the HOMFLY polynomial and the degrees of Severi strata of the miniversal deformation of the singularity.   As an application of the fact above, we also generalize Kawai's result regarding the generating function for the Euler numbers of a singular curve.","sentences":["In the present paper, we show that the motivic Hilbert zeta function for a curve singularity yields the generating functions for Euler numbers of punctual Hilbert schemes when any punctual Hilbert scheme admits an affine cell decomposition.","This fact allows us to derive the relations among the motivic Hilbert zeta function and other invariants such as the generating function for semi-modules of the semi-group of the singularity, the HOMFLY polynomial and the degrees of Severi strata of the miniversal deformation of the singularity.   ","As an application of the fact above, we also generalize Kawai's result regarding the generating function for the Euler numbers of a singular curve."],"url":"http://arxiv.org/abs/2403.12545v1","category":"math.AG"}
{"created":"2024-03-19 08:40:21","title":"AffineQuant: Affine Transformation Quantization for Large Language Models","abstract":"The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). This approach extends the optimization scope and thus significantly minimizing quantization errors. Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities. To ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method. This method initially focuses on optimizing the diagonal elements and gradually extends to the other elements. Such an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation. As a result, significant performance improvements are evident across different LLMs on diverse datasets. To illustrate, we attain a C4 perplexity of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model of W4A4 quantization without overhead. On zero-shot tasks, AffineQuant achieves an average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using 4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art benchmark for PTQ in LLMs.","sentences":["The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks.","Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training.","Existing PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights.","In this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant).","This approach extends the optimization scope and thus significantly minimizing quantization errors.","Additionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities.","To ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method.","This method initially focuses on optimizing the diagonal elements and gradually extends to the other elements.","Such an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation.","As a result, significant performance improvements are evident across different LLMs on diverse datasets.","To illustrate, we attain a C4 perplexity of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model of W4A4 quantization without overhead.","On zero-shot tasks, AffineQuant achieves an average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using 4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art benchmark for PTQ in LLMs."],"url":"http://arxiv.org/abs/2403.12544v1","category":"cs.LG"}
{"created":"2024-03-19 08:29:47","title":"Community detection by spectral methods in multi-layer networks","abstract":"Community detection in multi-layer networks is a crucial problem in network analysis. In this paper, we analyze the performance of two spectral clustering algorithms for community detection within the multi-layer degree-corrected stochastic block model (MLDCSBM) framework. One algorithm is based on the sum of adjacency matrices, while the other utilizes the debiased sum of squared adjacency matrices. We establish consistency results for community detection using these methods under MLDCSBM as the size of the network and/or the number of layers increases. Our theorems demonstrate the advantages of utilizing multiple layers for community detection. Moreover, our analysis indicates that spectral clustering with the debiased sum of squared adjacency matrices is generally superior to spectral clustering with the sum of adjacency matrices. Numerical simulations confirm that our algorithm, employing the debiased sum of squared adjacency matrices, surpasses existing methods for community detection in multi-layer networks. Finally, the analysis of several real-world multi-layer networks yields meaningful insights.","sentences":["Community detection in multi-layer networks is a crucial problem in network analysis.","In this paper, we analyze the performance of two spectral clustering algorithms for community detection within the multi-layer degree-corrected stochastic block model (MLDCSBM) framework.","One algorithm is based on the sum of adjacency matrices, while the other utilizes the debiased sum of squared adjacency matrices.","We establish consistency results for community detection using these methods under MLDCSBM as the size of the network and/or the number of layers increases.","Our theorems demonstrate the advantages of utilizing multiple layers for community detection.","Moreover, our analysis indicates that spectral clustering with the debiased sum of squared adjacency matrices is generally superior to spectral clustering with the sum of adjacency matrices.","Numerical simulations confirm that our algorithm, employing the debiased sum of squared adjacency matrices, surpasses existing methods for community detection in multi-layer networks.","Finally, the analysis of several real-world multi-layer networks yields meaningful insights."],"url":"http://arxiv.org/abs/2403.12540v1","category":"cs.SI"}
{"created":"2024-03-19 08:09:44","title":"To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions","abstract":"How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.","sentences":["How can a robot provide unobtrusive physical support within a group of humans?","We present Attentive Support, a novel interaction concept for robots to support a group of humans.","It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs).","In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group.","With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed."],"url":"http://arxiv.org/abs/2403.12533v1","category":"cs.RO"}
{"created":"2024-03-19 08:08:38","title":"Asymptotic Error Rates for Point Process Classification","abstract":"Point processes are finding growing applications in numerous fields, such as neuroscience, high frequency finance and social media. So classic problems of classification and clustering are of increasing interest. However, analytic study of misclassification error probability in multi-class classification has barely begun. In this paper, we tackle the multi-class likelihood classification problem for point processes and develop, for the first time, both asymptotic upper and lower bounds on the error rate in terms of computable pair-wise affinities. We apply these general results to classifying renewal processes. Under some technical conditions, we show that the bounds have exponential decay and give explicit associated constants. The results are illustrated with a non-trivial simulation.","sentences":["Point processes are finding growing applications in numerous fields, such as neuroscience, high frequency finance and social media.","So classic problems of classification and clustering are of increasing interest.","However, analytic study of misclassification error probability in multi-class classification has barely begun.","In this paper, we tackle the multi-class likelihood classification problem for point processes and develop, for the first time, both asymptotic upper and lower bounds on the error rate in terms of computable pair-wise affinities.","We apply these general results to classifying renewal processes.","Under some technical conditions, we show that the bounds have exponential decay and give explicit associated constants.","The results are illustrated with a non-trivial simulation."],"url":"http://arxiv.org/abs/2403.12531v1","category":"math.ST"}
{"created":"2024-03-19 08:08:12","title":"PCT: Perspective Cue Training Framework for Multi-Camera BEV Segmentation","abstract":"Generating annotations for bird's-eye-view (BEV) segmentation presents significant challenges due to the scenes' complexity and the high manual annotation cost. In this work, we address these challenges by leveraging the abundance of unlabeled data available. We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets. PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels. Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures. PCT can be applied to various settings where unlabeled data is available. In this paper, we applied PCT for semi-supervised learning (SSL) and unsupervised domain adaptation (UDA). Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework. Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art.","sentences":["Generating annotations for bird's-eye-view (BEV) segmentation presents significant challenges due to the scenes' complexity and the high manual annotation cost.","In this work, we address these challenges by leveraging the abundance of unlabeled data available.","We propose the Perspective Cue Training (PCT) framework, a novel training framework that utilizes pseudo-labels generated from unlabeled perspective images using publicly available semantic segmentation models trained on large street-view datasets.","PCT applies a perspective view task head to the image encoder shared with the BEV segmentation head, effectively utilizing the unlabeled data to be trained with the generated pseudo-labels.","Since image encoders are present in nearly all camera-based BEV segmentation architectures, PCT is flexible and applicable to various existing BEV architectures.","PCT can be applied to various settings where unlabeled data is available.","In this paper, we applied PCT for semi-supervised learning (SSL) and unsupervised domain adaptation (UDA).","Additionally, we introduce strong input perturbation through Camera Dropout (CamDrop) and feature perturbation via BEV Feature Dropout (BFD), which are crucial for enhancing SSL capabilities using our teacher-student framework.","Our comprehensive approach is simple and flexible but yields significant improvements over various baselines for SSL and UDA, achieving competitive performances even against the current state-of-the-art."],"url":"http://arxiv.org/abs/2403.12530v1","category":"cs.CV"}
{"created":"2024-03-19 08:05:49","title":"Contextualized Messages Boost Graph Representations","abstract":"Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs. GNNs generally follow the message-passing scheme to locally update node feature representations. A graph readout function is then employed to create a representation for the entire graph. Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics. Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-level - when the space of node feature representation is uncountable. From the results, a novel soft-isomorphic relational graph convolution network (SIR-GCN) is proposed that emphasizes non-linear and contextualized transformations of neighborhood feature representations. The mathematical relationship of SIR-GCN and three widely used GNNs is explored to highlight the contribution. Validation on synthetic datasets then demonstrates that SIR-GCN outperforms comparable models even in simple node and graph property prediction tasks.","sentences":["Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs.","GNNs generally follow the message-passing scheme to locally update node feature representations.","A graph readout function is then employed to create a representation for the entire graph.","Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics.","Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations.","Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations.","This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-level - when the space of node feature representation is uncountable.","From the results, a novel soft-isomorphic relational graph convolution network (SIR-GCN) is proposed that emphasizes non-linear and contextualized transformations of neighborhood feature representations.","The mathematical relationship of SIR-GCN and three widely used GNNs is explored to highlight the contribution.","Validation on synthetic datasets then demonstrates that SIR-GCN outperforms comparable models even in simple node and graph property prediction tasks."],"url":"http://arxiv.org/abs/2403.12529v1","category":"cs.LG"}
{"created":"2024-03-19 08:02:21","title":"Irreducible modules over N=2 superconformal algebras from algebraic D-modules","abstract":"In this paper, we introduce a family of functors denoted $\\mathscr{F}_b$ that act on algebraic D-modules and generate modules over N=2 superconformal algebras. We prove these functors preserve irreducibility for all values of $b$, with a few clear exceptions described. We also establish necessary and sufficient conditions to determine when two such functors are naturally isomorphic. Applying $\\mathscr{F}_b$ to N=1 super-Virasoro algebras recovers the functors previously introduced in \\cite{CDLP}. Our new functors also facilitate the recovery of specific irreducible modules over N=2 superconformal algebras, including intermediate series and $U(\\mathfrak{h})$-free modules. Additionally, our constructed functors produce several new irreducible modules for N=2 superconformal algebras.","sentences":["In this paper, we introduce a family of functors denoted $\\mathscr{F}_b$ that act on algebraic D-modules and generate modules over N=2 superconformal algebras.","We prove these functors preserve irreducibility for all values of $b$, with a few clear exceptions described.","We also establish necessary and sufficient conditions to determine when two such functors are naturally isomorphic.","Applying $\\mathscr{F}_b$ to N=1 super-Virasoro algebras recovers the functors previously introduced in \\cite{CDLP}.","Our new functors also facilitate the recovery of specific irreducible modules over N=2 superconformal algebras, including intermediate series and $U(\\mathfrak{h})$-free modules.","Additionally, our constructed functors produce several new irreducible modules for N=2 superconformal algebras."],"url":"http://arxiv.org/abs/2403.12527v1","category":"math.RT"}
{"created":"2024-03-19 07:53:58","title":"Freezing and BPS jumping","abstract":"We report a novel BPS jumping phenomenon of 5d $\\mathcal{N}=1$ supersymmetric gauge theories whose brane configuration is equipped with an O$7$-plane. The study of the relation between O$7{}^+$-plane and O$7{}^-$-plane reveals that such BPS jumps take place when the Higgsing is triggered near the O7-plane upon a particular parameter tuning of the theories. We propose two types of gauge theories whose BPS spectra jump. One is the SU($2N+8$) gauge theory with a symmetric hypermultiplet converted to the SU($2N$) gauge theory with an antisymmetric hypermultiplet. The other is pure SO($2N+8$) gauge theory jumping to pure Sp($N$) gauge theory. We explicitly confirm our proposal through the (un)refined instanton partition functions. Furthermore, we discuss feasible generalizations involving an O$p$-plane for supersymmetric gauge theories of eight supercharges in four and three dimensions ($p=6, 5$ respectively).","sentences":["We report a novel BPS jumping phenomenon of 5d $\\mathcal{N}=1$ supersymmetric gauge theories whose brane configuration is equipped with an O$7$-plane.","The study of the relation between O$7{}^+$-plane and O$7{}^-$-plane reveals that such BPS jumps take place when the Higgsing is triggered near the O7-plane upon a particular parameter tuning of the theories.","We propose two types of gauge theories whose BPS spectra jump.","One is the SU($2N+8$) gauge theory with a symmetric hypermultiplet converted to the SU($2N$) gauge theory with an antisymmetric hypermultiplet.","The other is pure SO($2N+8$) gauge theory jumping to pure Sp($N$) gauge theory.","We explicitly confirm our proposal through the (un)refined instanton partition functions.","Furthermore, we discuss feasible generalizations involving an O$p$-plane for supersymmetric gauge theories of eight supercharges in four and three dimensions ($p=6, 5$ respectively)."],"url":"http://arxiv.org/abs/2403.12525v1","category":"hep-th"}
{"created":"2024-03-19 16:56:47","title":"Measurement of vector boson production cross sections and their ratios using $pp$ collisions at $\\sqrt{s}=13.6$ TeV with the ATLAS detector","abstract":"Fiducial and total $W^\\pm$ and $Z$ boson cross sections, their ratios and the ratio of top-antitop-quark pair and $W$-boson fiducial cross sections are measured in proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}=13.6$ TeV, corresponding to an integrated luminosity of 29 fb$^{-1}$ of data collected in 2022 by the ATLAS experiment at the Large Hadron Collider. The measured fiducial cross-section values for $W^+\\to \\ell^+\\nu$, $W^-\\to \\ell^-\\bar{\\nu}$, and $Z\\to \\ell^+\\ell^-$ ($\\ell=e$ or $\\mu$) boson productions are $4250\\pm 150$ pb, $3310\\pm 120$ pb, and $744\\pm 20$ pb, respectively, where the uncertainty is the total uncertainty, including that arising from the luminosity of about 2.2%. The measurements are in agreement with Standard-Model predictions calculated at next-to-next-to-leading-order in $\\alpha_s$, next-to-next-to-leading logarithmic accuracy and next-to-leading-order electroweak accuracy.","sentences":["Fiducial and total $W^\\pm$ and $Z$ boson cross sections, their ratios and the ratio of top-antitop-quark pair and $W$-boson fiducial cross sections are measured in proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}=13.6$ TeV, corresponding to an integrated luminosity of 29 fb$^{-1}$ of data collected in 2022 by the ATLAS experiment at the Large Hadron Collider.","The measured fiducial cross-section values for $W^+\\to \\ell^+\\nu$, $W^-\\to \\ell^-\\bar{\\nu}$, and $Z\\to \\ell^+\\ell^-$ ($\\ell=e$ or $\\mu$) boson productions are $4250\\pm 150$ pb, $3310\\pm 120$ pb, and $744\\pm 20$ pb, respectively, where the uncertainty is the total uncertainty, including that arising from the luminosity of about 2.2%.","The measurements are in agreement with Standard-Model predictions calculated at next-to-next-to-leading-order in $\\alpha_s$, next-to-next-to-leading logarithmic accuracy and next-to-leading-order electroweak accuracy."],"url":"http://arxiv.org/abs/2403.12902v1","category":"hep-ex"}
{"created":"2024-03-19 16:25:30","title":"Clustered Mallows Model","abstract":"Rankings are a type of preference elicitation that arise in experiments where assessors arrange items, for example, in decreasing order of utility. Orderings of n items labelled {1,...,n} denoted are permutations that reflect strict preferences. For a number of reasons, strict preferences can be unrealistic assumptions for real data. For example, when items share common traits it may be reasonable to attribute them equal ranks. Also, there can be different importance attributions to decisions that form the ranking. In a situation with, for example, a large number of items, an assessor may wish to rank at top a certain number items; to rank other items at the bottom and to express indifference to all others. In addition, when aggregating opinions, a judging body might be decisive about some parts of the rank but ambiguous for others. In this paper we extend the well-known Mallows (Mallows, 1957) model (MM) to accommodate item indifference, a phenomenon that can be in place for a variety of reasons, such as those above mentioned.The underlying grouping of similar items motivates the proposed Clustered Mallows Model (CMM). The CMM can be interpreted as a Mallows distribution for tied ranks where ties are learned from the data. The CMM provides the flexibility to combine strict and indifferent relations, achieving a simpler and robust representation of rank collections in the form of ordered clusters. Bayesian inference for the CMM is in the class of doubly-intractable problems since the model's normalisation constant is not available in closed form. We overcome this challenge by sampling from the posterior with a version of the exchange algorithm \\citep{murray2006}. Real data analysis of food preferences and results of Formula 1 races are presented, illustrating the CMM in practical situations.","sentences":["Rankings are a type of preference elicitation that arise in experiments where assessors arrange items, for example, in decreasing order of utility.","Orderings of n items labelled {1,...,n} denoted are permutations that reflect strict preferences.","For a number of reasons, strict preferences can be unrealistic assumptions for real data.","For example, when items share common traits it may be reasonable to attribute them equal ranks.","Also, there can be different importance attributions to decisions that form the ranking.","In a situation with, for example, a large number of items, an assessor may wish to rank at top a certain number items; to rank other items at the bottom and to express indifference to all others.","In addition, when aggregating opinions, a judging body might be decisive about some parts of the rank but ambiguous for others.","In this paper we extend the well-known Mallows (Mallows, 1957) model (MM) to accommodate item indifference, a phenomenon that can be in place for a variety of reasons, such as those above mentioned.","The underlying grouping of similar items motivates the proposed Clustered Mallows Model (CMM).","The CMM can be interpreted as a Mallows distribution for tied ranks where ties are learned from the data.","The CMM provides the flexibility to combine strict and indifferent relations, achieving a simpler and robust representation of rank collections in the form of ordered clusters.","Bayesian inference for the CMM is in the class of doubly-intractable problems since the model's normalisation constant is not available in closed form.","We overcome this challenge by sampling from the posterior with a version of the exchange algorithm \\citep{murray2006}.","Real data analysis of food preferences and results of Formula 1 races are presented, illustrating the CMM in practical situations."],"url":"http://arxiv.org/abs/2403.12880v1","category":"stat.ME"}
{"created":"2024-03-19 16:21:42","title":"Boundary layers in thermal convection are fluctuation-dominated","abstract":"We study the dynamics of thermal and momentum boundary layers in three-dimensional direct numerical simulations of Rayleigh-B\\'enard convection for the Rayleigh number range $10^5 \\le Ra \\le 10^{11}$ and $Pr=0.7$. Using a Cartesian slab with horizontal periodic boundary conditions and an aspect ratio of 4, we obtain statistical homogeneity in the horizontal $x$- and $y$-directions, thus approximating an infinitely extended system. We observe that upon canonical use of long-time and area averages, a coherent mean flow is practically absent. Instead, the velocity field close to the wall is a collection of differently oriented local shear patches interspersed by shear-free, incoherent flow regions. These shear patches occupy an area fraction of approximately $40\\%$ for all $Ra$. Rather than resulting in a pronounced mean with small fluctuations about it, the velocity field is dominated by strong fluctuations of all three components around a non-existent or weak mean. This feature is particularly pronounced for $Ra \\ge 10^9$. We discuss the consequences of these observations for convection layers with larger aspect ratios, including boundary layer instabilities and the resulting turbulent heat transport.","sentences":["We study the dynamics of thermal and momentum boundary layers in three-dimensional direct numerical simulations of Rayleigh-B\\'enard convection for the Rayleigh number range $10^5 \\le Ra \\le 10^{11}$ and $Pr=0.7$. Using a Cartesian slab with horizontal periodic boundary conditions and an aspect ratio of 4, we obtain statistical homogeneity in the horizontal $x$- and $y$-directions, thus approximating an infinitely extended system.","We observe that upon canonical use of long-time and area averages, a coherent mean flow is practically absent.","Instead, the velocity field close to the wall is a collection of differently oriented local shear patches interspersed by shear-free, incoherent flow regions.","These shear patches occupy an area fraction of approximately $40\\%$ for all $Ra$. Rather than resulting in a pronounced mean with small fluctuations about it, the velocity field is dominated by strong fluctuations of all three components around a non-existent or weak mean.","This feature is particularly pronounced for $Ra \\ge 10^9$.","We discuss the consequences of these observations for convection layers with larger aspect ratios, including boundary layer instabilities and the resulting turbulent heat transport."],"url":"http://arxiv.org/abs/2403.12877v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 15:42:46","title":"Opti-Acoustic Semantic SLAM with Unknown Objects in Underwater Environments","abstract":"Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater. This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene. The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization. Probabilistic data association is used to determine observation to landmark correspondences. Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates. Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available. Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene.","sentences":["Despite recent advances in semantic Simultaneous Localization and Mapping (SLAM) for terrestrial and aerial applications, underwater semantic SLAM remains an open and largely unaddressed research problem due to the unique sensing modalities and the object classes found underwater.","This paper presents an object-based semantic SLAM method for underwater environments that can identify, localize, classify, and map a wide variety of marine objects without a priori knowledge of the object classes present in the scene.","The method performs unsupervised object segmentation and object-level feature aggregation, and then uses opti-acoustic sensor fusion for object localization.","Probabilistic data association is used to determine observation to landmark correspondences.","Given such correspondences, the method then jointly optimizes landmark and vehicle position estimates.","Indoor and outdoor underwater datasets with a wide variety of objects and challenging acoustic and lighting conditions are collected for evaluation and made publicly available.","Quantitative and qualitative results show the proposed method achieves reduced trajectory error compared to baseline methods, and is able to obtain comparable map accuracy to a baseline closed-set method that requires hand-labeled data of all objects in the scene."],"url":"http://arxiv.org/abs/2403.12837v1","category":"cs.RO"}
{"created":"2024-03-19 15:17:16","title":"Searching for Lepton Flavor Violation with the CMS Experiment","abstract":"Searches for lepton flavor violation (LFV) stand at the forefront of experimental particle physics research, offering a sensitive probe to many scenarios of physics beyond the Standard Model. The high proton-proton collision energy and luminosity provided by the CERN Large Hadron Collider (LHC) and the excellent CMS detector performance allow for an extensive program of LFV searches. This article reviews a broad range of LFV searches conducted at the CMS experiment using data collected in LHC Run 2, including $\\tau\\to3\\mu$ decays, Higgs boson decays, and top quark production and decays. In each analysis, the online and offline event selections, signal modeling, background suppression and estimation, and statistical interpretation are elucidated. These searches involve various final state particles in a large transverse momentum range, showcasing the capability of the CMS experiment in exploring fundamental questions in particle physics.","sentences":["Searches for lepton flavor violation (LFV) stand at the forefront of experimental particle physics research, offering a sensitive probe to many scenarios of physics beyond the Standard Model.","The high proton-proton collision energy and luminosity provided by the CERN Large Hadron Collider (LHC) and the excellent CMS detector performance allow for an extensive program of LFV searches.","This article reviews a broad range of LFV searches conducted at the CMS experiment using data collected in LHC Run 2, including $\\tau\\to3\\mu$ decays, Higgs boson decays, and top quark production and decays.","In each analysis, the online and offline event selections, signal modeling, background suppression and estimation, and statistical interpretation are elucidated.","These searches involve various final state particles in a large transverse momentum range, showcasing the capability of the CMS experiment in exploring fundamental questions in particle physics."],"url":"http://arxiv.org/abs/2403.12817v1","category":"hep-ex"}
{"created":"2024-03-19 14:18:59","title":"Ring nebulae around Wolf-Rayet stars in M33 as seen by SITELLE","abstract":"We have conducted an analysis of nebulae around Wolf-Rayet (WR) stars in M33 using data collected by the imaging Fourier transform spectrometer SITELLE at the Canada-France-Hawaii telescope as part of the SIGNALS Large Program. Of the 211 known Wolf-Rayet stars in M33, 178 are located in the fields observed in this study. We present the results of this analysis in the form of a comprehensive summary of all nebulae found around the observed WR stars. Based on three criteria we find to be the most effective for their detection, we detect a clear association with a circumstellar bubble around 33 of them (19\\%). Our results show that the presence of bubbles does not correlate with the spectral type of the central star. The mean diameter of the WR nebulae we have found is 21 parsecs.","sentences":["We have conducted an analysis of nebulae around Wolf-Rayet (WR) stars in M33 using data collected by the imaging Fourier transform spectrometer SITELLE at the Canada-France-Hawaii telescope as part of the SIGNALS Large Program.","Of the 211 known Wolf-Rayet stars in M33, 178 are located in the fields observed in this study.","We present the results of this analysis in the form of a comprehensive summary of all nebulae found around the observed WR stars.","Based on three criteria we find to be the most effective for their detection, we detect a clear association with a circumstellar bubble around 33 of them (19\\%).","Our results show that the presence of bubbles does not correlate with the spectral type of the central star.","The mean diameter of the WR nebulae we have found is 21 parsecs."],"url":"http://arxiv.org/abs/2403.12754v1","category":"astro-ph.SR"}
{"created":"2024-03-19 13:48:30","title":"Small Scale Reflection for the Working Lean User","abstract":"We present the design and implementation of the Small Scale Reflection proof methodology and tactic language (a.k.a. SSR) for the Lean 4 proof assistant. Like its Coq predecessor SSReflect, our Lean 4 implementation, dubbed LeanSSR, provides powerful rewriting principles and means for effective management of hypotheses in the proof context. Unlike SSReflect for Coq, LeanSSR does not require explicit switching between the logical and symbolic representation of a goal, allowing for even more concise proof scripts that seamlessly combine deduction steps with proofs by computation.   In this paper, we first provide a gentle introduction to the principles of structuring mechanised proofs using LeanSSR. Next, we show how the native support for metaprogramming in Lean 4 makes it possible to develop LeanSSR entirely within the proof assistant, greatly improving the overall experience of both tactic implementers and proof engineers. Finally, we demonstrate the utility of LeanSSR by conducting two case studies: (a) porting a collection of Coq lemmas about sequences from the widely used Mathematical Components library and (b) reimplementing proofs in the finite set library of Lean's mathlib4. Both case studies show significant reduction in proof sizes.","sentences":["We present the design and implementation of the Small Scale Reflection proof methodology and tactic language (a.k.a. SSR) for the Lean 4 proof assistant.","Like its Coq predecessor SSReflect, our Lean 4 implementation, dubbed LeanSSR, provides powerful rewriting principles and means for effective management of hypotheses in the proof context.","Unlike SSReflect for Coq, LeanSSR does not require explicit switching between the logical and symbolic representation of a goal, allowing for even more concise proof scripts that seamlessly combine deduction steps with proofs by computation.   ","In this paper, we first provide a gentle introduction to the principles of structuring mechanised proofs using LeanSSR.","Next, we show how the native support for metaprogramming in Lean 4 makes it possible to develop LeanSSR entirely within the proof assistant, greatly improving the overall experience of both tactic implementers and proof engineers.","Finally, we demonstrate the utility of LeanSSR by conducting two case studies: (a) porting a collection of Coq lemmas about sequences from the widely used Mathematical Components library and (b) reimplementing proofs in the finite set library of Lean's mathlib4.","Both case studies show significant reduction in proof sizes."],"url":"http://arxiv.org/abs/2403.12733v1","category":"cs.PL"}
{"created":"2024-03-19 13:30:47","title":"CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation","abstract":"This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community. A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles. Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts.","sentences":["This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space.","The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents.","The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology.","All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment.","The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora, with variations in the most prominent genre categories being well-explained by the economic strength of each language community.","A comparison of the distribution of genre categories across the corpora indicates that web corpora from less developed countries primarily consist of news articles.","Conversely, web corpora from economically more developed countries exhibit a smaller proportion of news content, with a greater presence of promotional and opinionated texts."],"url":"http://arxiv.org/abs/2403.12721v1","category":"cs.CL"}
{"created":"2024-03-19 13:01:58","title":"ReProbes: An Architecture for Reconfigurable and Adaptive Probes","abstract":"Modern distributed systems are highly dynamic and scalable, requiring monitoring solutions that can adapt to rapid changes. Monitoring systems that rely on external probes can only achieve adaptation through expensive operations such as deployment, undeployment, and reconfiguration. This poster paper introduces ReProbes, a class of adaptive monitoring probes that can handle rapid changes in data collection strategies. ReProbe offers controllable and configurable self-adaptive capabilities for data transmission, collection, and analysis methods. The resulting architecture can effectively enhance probe adaptability when qualitatively compared to state-of-the-art monitoring solutions.","sentences":["Modern distributed systems are highly dynamic and scalable, requiring monitoring solutions that can adapt to rapid changes.","Monitoring systems that rely on external probes can only achieve adaptation through expensive operations such as deployment, undeployment, and reconfiguration.","This poster paper introduces ReProbes, a class of adaptive monitoring probes that can handle rapid changes in data collection strategies.","ReProbe offers controllable and configurable self-adaptive capabilities for data transmission, collection, and analysis methods.","The resulting architecture can effectively enhance probe adaptability when qualitatively compared to state-of-the-art monitoring solutions."],"url":"http://arxiv.org/abs/2403.12703v1","category":"cs.SE"}
{"created":"2024-03-19 12:39:37","title":"Concepts and methods for predicting viral evolution","abstract":"The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year. These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies. Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution. Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes. From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year. Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection. Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app.","sentences":["The seasonal human influenza virus undergoes rapid evolution, leading to significant changes in circulating viral strains from year to year.","These changes are typically driven by adaptive mutations, particularly in the antigenic epitopes, the regions of the viral surface protein haemagglutinin targeted by human antibodies.","Here we describe a consistent set of methods for data-driven predictive analysis of viral evolution.","Our pipeline integrates four types of data: (1) sequence data of viral isolates collected on a worldwide scale, (2) epidemiological data on incidences, (3) antigenic characterization of circulating viruses, and (4) intrinsic viral phenotypes.","From the combined analysis of these data, we obtain estimates of relative fitness for circulating strains and predictions of clade frequencies for periods of up to one year.","Furthermore, we obtain comparative estimates of protection against future viral populations for candidate vaccine strains, providing a basis for pre-emptive vaccine strain selection.","Continuously updated predictions obtained from the prediction pipeline for influenza and SARS-CoV-2 are available on the website https://previr.app."],"url":"http://arxiv.org/abs/2403.12684v1","category":"q-bio.PE"}
{"created":"2024-03-19 10:11:14","title":"LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models","abstract":"Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications. However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture. Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types. To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper. LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams. Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs under the zero-shot setting, which aligns with real examinations, and compared their performance across different subjects. We also conduct an in-depth analysis to check whether GPT-4 can automatically score subjective predictions. Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese LLMs.","sentences":["Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications.","However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture.","Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue.","Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types.","To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper.","LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.","It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams.","Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of LLMs.","We have assessed 11 Chinese LLMs under the zero-shot setting, which aligns with real examinations, and compared their performance across different subjects.","We also conduct an in-depth analysis to check whether GPT-4 can automatically score subjective predictions.","Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese LLMs."],"url":"http://arxiv.org/abs/2403.12601v1","category":"cs.CL"}
{"created":"2024-03-19 07:50:32","title":"GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings","abstract":"Events describe the state changes of entities. In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent). Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language. There are two main problems in the current ERE works: a. Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event. b. The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored. To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly extract multiple event relations, we use Node Transformer and construct Task-specific Dynamic Event Graphs for each type of relation. Finally, we used a multi-task learning strategy to train the whole framework. Experimental results on the latest MAVEN-ERE dataset validate that GraphERE significantly outperforms existing methods. Further analyses indicate the effectiveness of the graph-enhanced event embeddings and the joint extraction strategy.","sentences":["Events describe the state changes of entities.","In a document, multiple events are connected by various relations (e.g., Coreference, Temporal, Causal, and Subevent).","Therefore, obtaining the connections between events through Event-Event Relation Extraction (ERE) is critical to understand natural language.","There are two main problems in the current ERE works: a.","Only embeddings of the event triggers are used for event feature representation, ignoring event arguments (e.g., time, place, person, etc.) and their structure within the event.","b.","The interconnection between relations (e.g., temporal and causal relations usually interact with each other ) is ignored.","To solve the above problems, this paper proposes a jointly multiple ERE framework called GraphERE based on Graph-enhanced Event Embeddings.","First, we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs; Then, to jointly extract multiple event relations, we use Node Transformer and construct Task-specific Dynamic Event Graphs for each type of relation.","Finally, we used a multi-task learning strategy to train the whole framework.","Experimental results on the latest MAVEN-ERE dataset validate that GraphERE significantly outperforms existing methods.","Further analyses indicate the effectiveness of the graph-enhanced event embeddings and the joint extraction strategy."],"url":"http://arxiv.org/abs/2403.12523v1","category":"cs.CL"}
{"created":"2024-03-19 07:44:18","title":"Multi-mode fault diagnosis datasets of gearbox under variable working conditions","abstract":"The gearbox is a critical component of electromechanical systems. The occurrence of multiple faults can significantly impact system accuracy and service life. The vibration signal of the gearbox is an effective indicator of its operational status and fault information. However, gearboxes in real industrial settings often operate under variable working conditions, such as varying speeds and loads. It is a significant and challenging research area to complete the gearbox fault diagnosis procedure under varying operating conditions using vibration signals. This data article presents vibration datasets collected from a gearbox exhibiting various fault degrees of severity and fault types, operating under diverse speed and load conditions. These faults are manually implanted into the gears or bearings through precise machining processes, which include health, missing teeth, wear, pitting, root cracks, and broken teeth. Several kinds of actual compound faults are also encompassed. The development of these datasets facilitates testing the effectiveness and reliability of newly developed fault diagnosis methods.","sentences":["The gearbox is a critical component of electromechanical systems.","The occurrence of multiple faults can significantly impact system accuracy and service life.","The vibration signal of the gearbox is an effective indicator of its operational status and fault information.","However, gearboxes in real industrial settings often operate under variable working conditions, such as varying speeds and loads.","It is a significant and challenging research area to complete the gearbox fault diagnosis procedure under varying operating conditions using vibration signals.","This data article presents vibration datasets collected from a gearbox exhibiting various fault degrees of severity and fault types, operating under diverse speed and load conditions.","These faults are manually implanted into the gears or bearings through precise machining processes, which include health, missing teeth, wear, pitting, root cracks, and broken teeth.","Several kinds of actual compound faults are also encompassed.","The development of these datasets facilitates testing the effectiveness and reliability of newly developed fault diagnosis methods."],"url":"http://arxiv.org/abs/2403.12521v1","category":"eess.SY"}
{"created":"2024-03-19 07:24:54","title":"Generalized Consistency Trajectory Models for Image Manipulation","abstract":"Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs. We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing. Code: \\url{https://github.com/1202kbs/GCTM}","sentences":["Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration.","The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks.","Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step.","However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations.","Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data.","Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbitrary distributions via ODEs.","We discuss the design space of GCTMs and demonstrate their efficacy in various image manipulation tasks such as image-to-image translation, restoration, and editing.","Code: \\url{https://github.com/1202kbs/GCTM}"],"url":"http://arxiv.org/abs/2403.12510v1","category":"cs.CV"}
{"created":"2024-03-19 07:10:58","title":"Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices","abstract":"Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.","sentences":["Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP).","Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations.","Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations.","These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities.","This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies.","Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs."],"url":"http://arxiv.org/abs/2403.12503v1","category":"cs.CR"}
{"created":"2024-03-19 07:07:13","title":"A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems","abstract":"Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation. The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits. First, having access to large contradiction data enables a comprehensive examination of their characteristics. Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training. Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses. In this paper, we build a large dataset of response generation models' contradictions for the first time. Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods.","sentences":["Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation.","The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits.","First, having access to large contradiction data enables a comprehensive examination of their characteristics.","Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training.","Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses.","In this paper, we build a large dataset of response generation models' contradictions for the first time.","Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses.","Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods."],"url":"http://arxiv.org/abs/2403.12500v1","category":"cs.CL"}
{"created":"2024-03-19 07:04:51","title":"WMMSE-Based Rate Maximization for RIS-Assisted MU-MIMO Systems","abstract":"Reconfigurable intelligent surface (RIS) technology, given its ability to favorably modify wireless communication environments, will play a pivotal role in the evolution of future communication systems. This paper proposes rate maximization techniques for both single-user and multiuser MIMO systems, based on the well-known weighted minimum mean square error (WMMSE) criterion. Using a suitable weight matrix, the WMMSE algorithm tackles an equivalent weighted mean square error (WMSE) minimization problem to achieve the sum-rate maximization. By considering a more practical RIS system model that employs a tensor-based representation enforced by the electromagnetic behavior exhibited by the RIS panel, we detail both the sum-rate maximizing and WMSE minimizing strategies for RIS phase shift optimization by deriving the closed-form gradient of the WMSE and the sum-rate with respect to the RIS phase shift vector. Our simulations reveal that the proposed rate maximization technique, rooted in the WMMSE algorithm, exhibits superior performance when compared to other benchmarks.","sentences":["Reconfigurable intelligent surface (RIS) technology, given its ability to favorably modify wireless communication environments, will play a pivotal role in the evolution of future communication systems.","This paper proposes rate maximization techniques for both single-user and multiuser MIMO systems, based on the well-known weighted minimum mean square error (WMMSE) criterion.","Using a suitable weight matrix, the WMMSE algorithm tackles an equivalent weighted mean square error (WMSE) minimization problem to achieve the sum-rate maximization.","By considering a more practical RIS system model that employs a tensor-based representation enforced by the electromagnetic behavior exhibited by the RIS panel, we detail both the sum-rate maximizing and WMSE minimizing strategies for RIS phase shift optimization by deriving the closed-form gradient of the WMSE and the sum-rate with respect to the RIS phase shift vector.","Our simulations reveal that the proposed rate maximization technique, rooted in the WMMSE algorithm, exhibits superior performance when compared to other benchmarks."],"url":"http://arxiv.org/abs/2403.12498v1","category":"cs.IT"}
{"created":"2024-03-19 06:54:33","title":"DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM","abstract":"We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-the-art methods, GPT-4V with our DetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS COCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val set for zero-shot referring expression comprehension, +14.5% AP on D-cube describe object detection FULL setting.","sentences":["We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini.","Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts.","Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs).","Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements.","The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases.","Compared to existing state-of-the-art methods, GPT-4V with our DetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS COCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val set for zero-shot referring expression comprehension, +14.5% AP on D-cube describe object detection FULL setting."],"url":"http://arxiv.org/abs/2403.12488v1","category":"cs.CV"}
{"created":"2024-03-19 06:43:46","title":"NTK-Guided Few-Shot Class Incremental Learning","abstract":"While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial network weights. Then they are carefully refined through curricular alignment, followed by the application of dual NTK regularization tailored specifically for both convolutional and linear layers. Through the combined effects of these measures, our network acquires robust NTK properties, significantly enhancing its foundational generalization. On popular FSCIL benchmark datasets, our NTK-FSCIL surpasses contemporary state-of-the-art approaches, elevating end-session accuracy by 2.9% to 8.7%.","sentences":["While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition.","In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK).","Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization.","To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network.","Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss.","Specifically, we initiate self-supervised pre-training on the base session to shape the initial network weights.","Then they are carefully refined through curricular alignment, followed by the application of dual NTK regularization tailored specifically for both convolutional and linear layers.","Through the combined effects of these measures, our network acquires robust NTK properties, significantly enhancing its foundational generalization.","On popular FSCIL benchmark datasets, our NTK-FSCIL surpasses contemporary state-of-the-art approaches, elevating end-session accuracy by 2.9% to 8.7%."],"url":"http://arxiv.org/abs/2403.12486v1","category":"cs.LG"}
{"created":"2024-03-19 06:39:47","title":"Embodied LLM Agents Learn to Cooperate in Organized Teams","abstract":"Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.","sentences":["Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks.","LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation.","However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation.","Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems.","Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors.","Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency."],"url":"http://arxiv.org/abs/2403.12482v1","category":"cs.AI"}
{"created":"2024-03-19 06:01:02","title":"When Do \"More Contexts\" Help with Sarcasm Recognition?","abstract":"Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words. Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models. While shown to be effective individually, no study has systematically evaluated their collective effectiveness. As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition. In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model. To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches. In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more contexts. We also identify inherent drawbacks of using more contexts, highlighting that in the pursuit of even better results, the model may need to adopt societal biases.","sentences":["Sarcasm recognition is challenging because it needs an understanding of the true intention, which is opposite to or different from the literal meaning of the words.","Prior work has addressed this challenge by developing a series of methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to models.","While shown to be effective individually, no study has systematically evaluated their collective effectiveness.","As a result, it remains unclear to what extent additional contexts can improve sarcasm recognition.","In this work, we explore the improvements that existing methods bring by incorporating more contexts into a model.","To this end, we develop a framework where we can integrate multiple contextual cues and test different approaches.","In evaluation with four approaches on three sarcasm recognition benchmarks, we achieve existing state-of-the-art performances and also demonstrate the benefits of sequentially adding more contexts.","We also identify inherent drawbacks of using more contexts, highlighting that in the pursuit of even better results, the model may need to adopt societal biases."],"url":"http://arxiv.org/abs/2403.12469v1","category":"cs.CL"}
{"created":"2024-03-19 05:37:26","title":"Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation. Recent advances in literature have studied the network representations of deep neural networks. However, there has been little work that studies representations learned by SNNs, especially using unsupervised local learning methods like spike-timing dependent plasticity (STDP). Recent work by \\cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD). Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods. We propose a novel reformulation of RSNNs using feedforward autoencoder networks with skip connections to help us compute the RTD for recurrent networks. Thus, we investigate the learning capabilities of RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics in learning such representations. We demonstrate that heterogeneous STDP in RSNNs yield distinct representations than their homogeneous and surrogate gradient-based supervised learning counterparts. Our results provide insights into the potential of heterogeneous SNN models, aiding the development of more efficient and biologically plausible hybrid artificial intelligence systems.","sentences":["Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation.","Recent advances in literature have studied the network representations of deep neural networks.","However, there has been little work that studies representations learned by SNNs, especially using unsupervised local learning methods like spike-timing dependent plasticity (STDP).","Recent work by \\cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD).","Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs).","This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods.","We propose a novel reformulation of RSNNs using feedforward autoencoder networks with skip connections to help us compute the RTD for recurrent networks.","Thus, we investigate the learning capabilities of RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics in learning such representations.","We demonstrate that heterogeneous STDP in RSNNs yield distinct representations than their homogeneous and surrogate gradient-based supervised learning counterparts.","Our results provide insights into the potential of heterogeneous SNN models, aiding the development of more efficient and biologically plausible hybrid artificial intelligence systems."],"url":"http://arxiv.org/abs/2403.12462v1","category":"cs.NE"}
{"created":"2024-03-19 05:30:50","title":"Non-negative Contrastive Learning","abstract":"Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these advantages enable NCL to outperform CL significantly on feature disentanglement, feature selection, as well as downstream classification tasks. At last, we show that NCL can be easily extended to other learning scenarios and benefit supervised learning as well. Code is available at https://github.com/PKU-ML/non_neg.","sentences":["Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner.","Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding.","In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features.","The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters.","NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL).","Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL.","Empirically, we show that these advantages enable NCL to outperform CL significantly on feature disentanglement, feature selection, as well as downstream classification tasks.","At last, we show that NCL can be easily extended to other learning scenarios and benefit supervised learning as well.","Code is available at https://github.com/PKU-ML/non_neg."],"url":"http://arxiv.org/abs/2403.12459v1","category":"cs.LG"}
{"created":"2024-03-19 05:23:48","title":"Deep Learning-Based CSI Feedback for RIS-Aided Massive MIMO Systems with Time Correlation","abstract":"In this paper, we consider an reconfigurable intelligent surface (RIS)-aided frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) downlink system.In the FDD systems, the downlink channel state information (CSI) should be sent to the base station through the feedback link. However, the overhead of CSI feedback occupies substantial uplink bandwidth resources in RIS-aided communication systems. In this work, we propose a deep learning (DL)-based scheme to reduce the overhead of CSI feedback by compressing the cascaded CSI. In the practical RIS-aided communication systems, the cascaded channel at the adjacent slots inevitably has time correlation. We use long short-term memory to learn time correlation, which can help the neural network to improve the recovery quality of the compressed CSI. Moreover, the attention mechanism is introduced to further improve the CSI recovery quality. Simulation results demonstrate that our proposed DLbased scheme can significantly outperform other DL-based methods in terms of the CSI recovery quality","sentences":["In this paper, we consider an reconfigurable intelligent surface (RIS)-aided frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) downlink system.","In the FDD systems, the downlink channel state information (CSI) should be sent to the base station through the feedback link.","However, the overhead of CSI feedback occupies substantial uplink bandwidth resources in RIS-aided communication systems.","In this work, we propose a deep learning (DL)-based scheme to reduce the overhead of CSI feedback by compressing the cascaded CSI.","In the practical RIS-aided communication systems, the cascaded channel at the adjacent slots inevitably has time correlation.","We use long short-term memory to learn time correlation, which can help the neural network to improve the recovery quality of the compressed CSI.","Moreover, the attention mechanism is introduced to further improve the CSI recovery quality.","Simulation results demonstrate that our proposed DLbased scheme can significantly outperform other DL-based methods in terms of the CSI recovery quality"],"url":"http://arxiv.org/abs/2403.12453v1","category":"eess.SP"}
{"created":"2024-03-19 05:21:20","title":"INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations","abstract":"Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency. Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies. In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by distilling vision foundation models into a scalable perception module. Moreover, we design a pipeline that uses large language models to generate concise and readable language explanations for policies and decisions. In experiments on nine Atari tasks, our approach demonstrates substantial performance gains over existing NSRL methods. We also showcase explanations for policies and decisions.","sentences":["Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies.","For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency.","Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies.","In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by distilling vision foundation models into a scalable perception module.","Moreover, we design a pipeline that uses large language models to generate concise and readable language explanations for policies and decisions.","In experiments on nine Atari tasks, our approach demonstrates substantial performance gains over existing NSRL methods.","We also showcase explanations for policies and decisions."],"url":"http://arxiv.org/abs/2403.12451v1","category":"cs.AI"}
{"created":"2024-03-19 05:17:47","title":"Do Generated Data Always Help Contrastive Learning?","abstract":"Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation. Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely data-centric strategy without introducing any extra computation cost. On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods. Notably, without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods. Code is available at https://github.com/PKU-ML/adainf.","sentences":["Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations.","With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized.","These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''.","However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning.","We investigate the causes behind this failure from the perspective of both data inflation and data augmentation.","For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa.","We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation.","Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely data-centric strategy without introducing any extra computation cost.","On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods.","Notably, without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods.","Code is available at https://github.com/PKU-ML/adainf."],"url":"http://arxiv.org/abs/2403.12448v1","category":"cs.LG"}
{"created":"2024-03-19 04:41:09","title":"Geometric Constraints in Deep Learning Frameworks: A Survey","abstract":"Stereophotogrammetry is an emerging technique of scene understanding. Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. Since then, thousands of approaches have been explored. The classic geometric techniques of Shape from Stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations. More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry. In this survey, we explore the overlap for geometric-based and deep learning-based frameworks. We compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. We present a new taxonomy for prevalent geometry enforcing constraints used in modern deep learning frameworks. We also present insightful observations and potential future research directions.","sentences":["Stereophotogrammetry is an emerging technique of scene understanding.","Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world.","Since then, thousands of approaches have been explored.","The classic geometric techniques of Shape from Stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations.","More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry.","In this survey, we explore the overlap for geometric-based and deep learning-based frameworks.","We compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems.","We present a new taxonomy for prevalent geometry enforcing constraints used in modern deep learning frameworks.","We also present insightful observations and potential future research directions."],"url":"http://arxiv.org/abs/2403.12431v1","category":"cs.CV"}
{"created":"2024-03-19 04:02:57","title":"STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model","abstract":"Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of STG networks. STG-Mamba is formulated as an Encoder-Decoder architecture, which takes GS3B as the basic module, for efficient sequential data modeling. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph Neural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothly fits in the context of selective state space evolution, and at the same time keeps linear complexity. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time.","sentences":["Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning.","In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time.","In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension.","In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of STG networks.","STG-Mamba is formulated as an Encoder-Decoder architecture, which takes GS3B as the basic module, for efficient sequential data modeling.","Furthermore, to strengthen GNN's ability of modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph Neural Networks (KFGN) for adaptive graph structure upgrading.","KFGN smoothly fits in the context of selective state space evolution, and at the same time keeps linear complexity.","Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba.","It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time."],"url":"http://arxiv.org/abs/2403.12418v1","category":"cs.LG"}
{"created":"2024-03-19 04:02:31","title":"On Predictive planning and counterfactual learning in active inference","abstract":"Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important. Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making. In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'. Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making. We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent. Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making.","sentences":["Given the rapid advancement of artificial intelligence, understanding the foundations of intelligent behaviour is increasingly important.","Active inference, regarded as a general theory of behaviour, offers a principled approach to probing the basis of sophistication in planning and decision-making.","In this paper, we examine two decision-making schemes in active inference based on 'planning' and 'learning from experience'.","Furthermore, we also introduce a mixed model that navigates the data-complexity trade-off between these strategies, leveraging the strengths of both to facilitate balanced decision-making.","We evaluate our proposed model in a challenging grid-world scenario that requires adaptability from the agent.","Additionally, our model provides the opportunity to analyze the evolution of various parameters, offering valuable insights and contributing to an explainable framework for intelligent decision-making."],"url":"http://arxiv.org/abs/2403.12417v1","category":"cs.AI"}
{"created":"2024-03-19 03:59:14","title":"Eye-gaze Guided Multi-modal Alignment Framework for Radiology","abstract":"In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut training costs. Our model demonstrates robust performance, outperforming other state-of-the-art methods in zero-shot classification and retrieval tasks. The incorporation of easily-obtained eye-gaze data during routine radiological diagnoses signifies a step towards minimizing manual annotation dependency. Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into multi-modal pre-training.","sentences":["In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge.","The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets.","This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology.","Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process.","Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations.","This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts.","We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut training costs.","Our model demonstrates robust performance, outperforming other state-of-the-art methods in zero-shot classification and retrieval tasks.","The incorporation of easily-obtained eye-gaze data during routine radiological diagnoses signifies a step towards minimizing manual annotation dependency.","Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into multi-modal pre-training."],"url":"http://arxiv.org/abs/2403.12416v1","category":"cs.CV"}
{"created":"2024-03-19 03:34:23","title":"Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion","abstract":"In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as a contextual Markov decision process. (ii) RallyNet leverages the experience to generate context as the agent's intent in the rally. (iii) To generate more realistic behavior, RallyNet leverages Geometric Brownian Motion (GBM) to model the interactions between players by introducing a valuable inductive bias for learning player behaviors. In this manner, RallyNet links player intents with interaction models with GBM, providing an understanding of interactions for sports analytics. We extensively validate RallyNet with the largest available real-world badminton dataset consisting of men's and women's singles, demonstrating its ability to imitate player behaviors. Results reveal RallyNet's superiority over offline imitation learning methods and state-of-the-art turn-based approaches, outperforming them by at least 16% in mean rule-based agent normalization score. Furthermore, we discuss various practical use cases to highlight RallyNet's applicability.","sentences":["In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players.","While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored.","Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches.","However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions.","In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as a contextual Markov decision process.","(ii) RallyNet leverages the experience to generate context as the agent's intent in the rally.","(iii) To generate more realistic behavior, RallyNet leverages Geometric Brownian Motion (GBM) to model the interactions between players by introducing a valuable inductive bias for learning player behaviors.","In this manner, RallyNet links player intents with interaction models with GBM, providing an understanding of interactions for sports analytics.","We extensively validate RallyNet with the largest available real-world badminton dataset consisting of men's and women's singles, demonstrating its ability to imitate player behaviors.","Results reveal RallyNet's superiority over offline imitation learning methods and state-of-the-art turn-based approaches, outperforming them by at least 16% in mean rule-based agent normalization score.","Furthermore, we discuss various practical use cases to highlight RallyNet's applicability."],"url":"http://arxiv.org/abs/2403.12406v1","category":"cs.AI"}
{"created":"2024-03-19 03:27:01","title":"Understanding Training-free Diffusion Guidance: Mechanisms and Limitations","abstract":"Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free diffusion guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of the operational mechanisms and fundamental limitations of training-free guidance. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free methods are more susceptible to adversarial gradients and exhibit slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.","sentences":["Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science.","Recently, several studies have proposed training-free diffusion guidance by using off-the-shelf networks pretrained on clean images.","This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance.","In this paper, we aim to develop a deeper understanding of the operational mechanisms and fundamental limitations of training-free guidance.","We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance.","To elucidate their drawbacks, we theoretically demonstrate that training-free methods are more susceptible to adversarial gradients and exhibit slower convergence rates compared to classifier guidance.","We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence.","Our experiments in image and motion generation confirm the efficacy of these techniques."],"url":"http://arxiv.org/abs/2403.12404v1","category":"cs.LG"}
{"created":"2024-03-19 03:22:35","title":"Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales","abstract":"Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable. Our comprehensive evaluation on a variety of social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability.","sentences":["Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content.","Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech.","Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design.","To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design.","Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable.","Our comprehensive evaluation on a variety of social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability."],"url":"http://arxiv.org/abs/2403.12403v1","category":"cs.CL"}
{"created":"2024-03-19 03:16:52","title":"Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing","abstract":"Despite the development of various deep learning methods for Wi-Fi sensing, package loss often results in noncontinuous estimation of the Channel State Information (CSI), which negatively impacts the performance of the learning models. To overcome this challenge, we propose a deep learning model based on Bidirectional Encoder Representations from Transformers (BERT) for CSI recovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner on the target dataset without the need for additional data. Furthermore, unlike traditional interpolation methods that focus on one subcarrier at a time, CSI-BERT captures the sequential relationships across different subcarriers. Experimental results demonstrate that CSI-BERT achieves lower error rates and faster speed compared to traditional interpolation methods, even when facing with high loss rates. Moreover, by harnessing the recovered CSI obtained from CSI-BERT, other deep learning models like Residual Network and Recurrent Neural Network can achieve an average increase in accuracy of approximately 15\\% in Wi-Fi sensing tasks. The collected dataset WiGesture and code for our model are publicly available at https://github.com/RS2002/CSI-BERT.","sentences":["Despite the development of various deep learning methods for Wi-Fi sensing, package loss often results in noncontinuous estimation of the Channel State Information (CSI), which negatively impacts the performance of the learning models.","To overcome this challenge, we propose a deep learning model based on Bidirectional Encoder Representations from Transformers (BERT) for CSI recovery, named CSI-BERT.","CSI-BERT can be trained in an self-supervised manner on the target dataset without the need for additional data.","Furthermore, unlike traditional interpolation methods that focus on one subcarrier at a time, CSI-BERT captures the sequential relationships across different subcarriers.","Experimental results demonstrate that CSI-BERT achieves lower error rates and faster speed compared to traditional interpolation methods, even when facing with high loss rates.","Moreover, by harnessing the recovered CSI obtained from CSI-BERT, other deep learning models like Residual Network and Recurrent Neural Network can achieve an average increase in accuracy of approximately 15\\% in Wi-Fi sensing tasks.","The collected dataset WiGesture and code for our model are publicly available at https://github.com/RS2002/CSI-BERT."],"url":"http://arxiv.org/abs/2403.12400v1","category":"cs.LG"}
{"created":"2024-03-19 03:11:42","title":"Hierarchical Digital Twin for Efficient 6G Network Orchestration via Adaptive Attribute Selection and Scalable Network Modeling","abstract":"Achieving a holistic and long-term understanding through accurate network modeling is essential for orchestrating future networks with increasing service diversity and infrastructure complexities. However, due to unselective data collection and uniform processing, traditional modeling approaches undermine the efficacy and timeliness of network orchestration. Additionally, temporal disparities arising from various modeling delays further impair the centralized decision-making with distributed models. In this paper, we propose a new hierarchical digital twin paradigm adapting to real-time network situations for problem-centered model construction. Specifically, we introduce an adaptive attribute selection mechanism that evaluates the distinct modeling values of diverse network attributes, considering their relevance to current network scenarios and inherent modeling complexity. By prioritizing critical attributes at higher layers, an efficient evaluation of network situations is achieved to identify target areas. Subsequently, scalable network modeling facilitates the inclusion of all identified elements at the lower layers, where more fine-grained digital twins are developed to generate targeted solutions for user association and power allocation. Furthermore, virtual-physical domain synchronization is implemented to maintain accurate temporal alignment between the digital twins and their physical counterparts, spanning from the construction to the utilization of the proposed paradigm. Extensive simulations validate the proposed approach, demonstrating its effectiveness in efficiently identifying pressing issues and delivering network orchestration solutions in complex 6G HetNets.","sentences":["Achieving a holistic and long-term understanding through accurate network modeling is essential for orchestrating future networks with increasing service diversity and infrastructure complexities.","However, due to unselective data collection and uniform processing, traditional modeling approaches undermine the efficacy and timeliness of network orchestration.","Additionally, temporal disparities arising from various modeling delays further impair the centralized decision-making with distributed models.","In this paper, we propose a new hierarchical digital twin paradigm adapting to real-time network situations for problem-centered model construction.","Specifically, we introduce an adaptive attribute selection mechanism that evaluates the distinct modeling values of diverse network attributes, considering their relevance to current network scenarios and inherent modeling complexity.","By prioritizing critical attributes at higher layers, an efficient evaluation of network situations is achieved to identify target areas.","Subsequently, scalable network modeling facilitates the inclusion of all identified elements at the lower layers, where more fine-grained digital twins are developed to generate targeted solutions for user association and power allocation.","Furthermore, virtual-physical domain synchronization is implemented to maintain accurate temporal alignment between the digital twins and their physical counterparts, spanning from the construction to the utilization of the proposed paradigm.","Extensive simulations validate the proposed approach, demonstrating its effectiveness in efficiently identifying pressing issues and delivering network orchestration solutions in complex 6G HetNets."],"url":"http://arxiv.org/abs/2403.12398v1","category":"cs.NI"}
{"created":"2024-03-19 02:59:58","title":"AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis","abstract":"Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field. The complexity of its structure and context necessitates advanced computational models for accurate analysis. In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text. To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry. The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet's gender classification (99.34\\% accuracy), and poetry sub-meter classification (97.79\\% accuracy). In addition, the model achieved an accuracy score in poems' rhyme classification (97.73\\% accuracy) which is almost equivalent to the best score reported in this study. Moreover, the proposed model significantly outperformed previous work and other comparative models in the tasks of poems' sentiment analysis, achieving an accuracy of 78.95\\%, and poetry meter classification (99.03\\% accuracy), while significantly expanding the scope of these two problems. The dataset used in this study, contains more than 2.09 million verses collected from online sources, each associated with various attributes such as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the effectiveness of the proposed model in understanding and analyzing Arabic poetry, achieving state-of-the-art results in several tasks and outperforming previous works and other language models included in the study. AraPoemBERT model is publicly available on \\url{https://huggingface.co/faisalq}.","sentences":["Arabic poetry, with its rich linguistic features and profound cultural significance, presents a unique challenge to the Natural Language Processing (NLP) field.","The complexity of its structure and context necessitates advanced computational models for accurate analysis.","In this paper, we introduce AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry text.","To demonstrate the effectiveness of the proposed model, we compared AraPoemBERT with 5 different Arabic language models on various NLP tasks related to Arabic poetry.","The new model outperformed all other models and achieved state-of-the-art results in most of the downstream tasks.","AraPoemBERT achieved unprecedented accuracy in two out of three novel tasks: poet's gender classification (99.34\\% accuracy), and poetry sub-meter classification (97.79\\% accuracy).","In addition, the model achieved an accuracy score in poems' rhyme classification (97.73\\% accuracy) which is almost equivalent to the best score reported in this study.","Moreover, the proposed model significantly outperformed previous work and other comparative models in the tasks of poems' sentiment analysis, achieving an accuracy of 78.95\\%, and poetry meter classification (99.03\\% accuracy), while significantly expanding the scope of these two problems.","The dataset used in this study, contains more than 2.09 million verses collected from online sources, each associated with various attributes such as meter, sub-meter, poet, rhyme, and topic.","The results demonstrate the effectiveness of the proposed model in understanding and analyzing Arabic poetry, achieving state-of-the-art results in several tasks and outperforming previous works and other language models included in the study.","AraPoemBERT model is publicly available on \\url{https://huggingface.co/faisalq}."],"url":"http://arxiv.org/abs/2403.12392v1","category":"cs.CL"}
{"created":"2024-03-19 02:59:50","title":"FairSTG: Countering performance heterogeneity via collaborative sample-level optimization","abstract":"Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples and challenging ones, and fairness objectives for immediately suppressing sample-level performance heterogeneity. Experiments on four spatiotemporal datasets demonstrate that our FairSTG significantly improves the fairness quality while maintaining comparable forecasting accuracy. Case studies show FairSTG can counter both spatial and temporal performance heterogeneity by our sample-level retrieval and compensation, and our work can potentially alleviate the risks on spatiotemporal resource allocation for underrepresented urban regions.","sentences":["Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites.","While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples.","In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications.","To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up.","Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples and challenging ones, and fairness objectives for immediately suppressing sample-level performance heterogeneity.","Experiments on four spatiotemporal datasets demonstrate that our FairSTG significantly improves the fairness quality while maintaining comparable forecasting accuracy.","Case studies show FairSTG can counter both spatial and temporal performance heterogeneity by our sample-level retrieval and compensation, and our work can potentially alleviate the risks on spatiotemporal resource allocation for underrepresented urban regions."],"url":"http://arxiv.org/abs/2403.12391v1","category":"cs.LG"}
{"created":"2024-03-19 02:57:07","title":"Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models","abstract":"Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.","sentences":["Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems.","Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems.","Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret.","In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches.","Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples.","The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown."],"url":"http://arxiv.org/abs/2403.12388v1","category":"cs.IR"}
{"created":"2024-03-19 02:52:58","title":"Pipelined Biomedical Event Extraction Rivaling Joint Learning","abstract":"Biomedical event extraction is an information extraction task to obtain events from biomedical text, whose targets include the type, the trigger, and the respective arguments involved in an event. Traditional biomedical event extraction usually adopts a pipelined approach, which contains trigger identification, argument role recognition, and finally event construction either using specific rules or by machine learning. In this paper, we propose an n-ary relation extraction method based on the BERT pre-training model to construct Binding events, in order to capture the semantic information about an event's context and its participants. The experimental results show that our method achieves promising results on the GE11 and GE13 corpora of the BioNLP shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates that by significantly improving theperformance of Binding events, the overall performance of the pipelined event extraction approach or even exceeds those of current joint learning methods.","sentences":["Biomedical event extraction is an information extraction task to obtain events from biomedical text, whose targets include the type, the trigger, and the respective arguments involved in an event.","Traditional biomedical event extraction usually adopts a pipelined approach, which contains trigger identification, argument role recognition, and finally event construction either using specific rules or by machine learning.","In this paper, we propose an n-ary relation extraction method based on the BERT pre-training model to construct Binding events, in order to capture the semantic information about an event's context and its participants.","The experimental results show that our method achieves promising results on the GE11 and GE13 corpora of the BioNLP shared task with F1 scores of 63.14% and 59.40%, respectively.","It demonstrates that by significantly improving theperformance of Binding events, the overall performance of the pipelined event extraction approach or even exceeds those of current joint learning methods."],"url":"http://arxiv.org/abs/2403.12386v1","category":"cs.CL"}
{"created":"2024-03-19 02:25:29","title":"Characteristic AI Agents via Large Language Models","abstract":"The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents. The benchmark is available at https://github.com/nuaa-nlp/Character100.","sentences":["The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems.","Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots.","While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce.","Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings.","Current investigations have primarily focused on act on roles with simple profiles.","In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics.","A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play.","With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings.","In addition, we devise a set of automatic metrics for quantitative performance evaluation.","The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents.","The benchmark is available at https://github.com/nuaa-nlp/Character100."],"url":"http://arxiv.org/abs/2403.12368v1","category":"cs.CL"}
{"created":"2024-03-19 02:23:12","title":"U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation","abstract":"Machine learning techniques have seen a tremendous rise in popularity in weather and climate sciences. Data assimilation (DA), which combines observations and numerical models, has great potential to incorporate machine learning and artificial intelligence (ML/AI) techniques. In this paper, we use U-Net, a type of convolutional neutral network (CNN), to predict the localized ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm. Using a 2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA experiments. The trained U-Nets are then used to predict the flow-dependent localized error covariance matrices in U-Net Kalman Filter (UNetKF) experiments, which are compared to traditional 3-dimensional variational (3DVar), ensemble 3DVar (En3DVar) and EnKF methods. The performance of UNetKF can match or exceed that of 3DVar, En3DVar or EnKF. We also demonstrate that trained U-Nets can be transferred to a higher-resolution model for UNetKF implementation, which again performs competitively to 3DVar and EnKF, particularly for small ensemble sizes.","sentences":["Machine learning techniques have seen a tremendous rise in popularity in weather and climate sciences.","Data assimilation (DA), which combines observations and numerical models, has great potential to incorporate machine learning and artificial intelligence (ML/AI) techniques.","In this paper, we use U-Net, a type of convolutional neutral network (CNN), to predict the localized ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm.","Using a 2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA experiments.","The trained U-Nets are then used to predict the flow-dependent localized error covariance matrices in U-Net Kalman Filter (UNetKF) experiments, which are compared to traditional 3-dimensional variational (3DVar), ensemble 3DVar (En3DVar) and EnKF methods.","The performance of UNetKF can match or exceed that of 3DVar, En3DVar or EnKF.","We also demonstrate that trained U-Nets can be transferred to a higher-resolution model for UNetKF implementation, which again performs competitively to 3DVar and EnKF, particularly for small ensemble sizes."],"url":"http://arxiv.org/abs/2403.12366v1","category":"cs.LG"}
{"created":"2024-03-19 02:19:02","title":"E-DoH: Elegantly Detecting the Depths of Open DoH Service on the Internet","abstract":"In recent years, DNS over Encrypted (DoE) methods have been regarded as a novel trend within the realm of the DNS ecosystem. In these DoE methods, DNS over HTTPS (DoH) provides encryption to protect data confidentiality while providing better obfuscation to avoid censorship by multiplexing port 443 with web services. This development introduced certain inconveniences in discovering publicly available DoH services. In this paper, we propose the E-DoH method for elegant and efficient DoH service detection. First, we optimized the probing mechanism to enable a single DoH connection to accomplish multiple tasks including service discovery, correctness validation and dependency construction. Second, we propose an efficient DoH detection tool. This tool can enhance probing efficiency while significantly reduce the required traffic volume. Third, based on the above optimization methods, we conducted an exploration of the IPv4 space and performed an in-depth analysis of DoH based on the collected information. Through experiments, our approach demonstrates a remarkable 80% improvement in time efficiency, and only requires 4%-20% traffic volume to complete the detection task. In wild detection, our approach discovered 46k DoH services, which nearly doubles the number discovered by the state-of-the-art. Based on the collected data, we present several intriguing conclusions about the current DoH service ecosystem.","sentences":["In recent years, DNS over Encrypted (DoE) methods have been regarded as a novel trend within the realm of the DNS ecosystem.","In these DoE methods, DNS over HTTPS (DoH) provides encryption to protect data confidentiality while providing better obfuscation to avoid censorship by multiplexing port 443 with web services.","This development introduced certain inconveniences in discovering publicly available DoH services.","In this paper, we propose the E-DoH method for elegant and efficient DoH service detection.","First, we optimized the probing mechanism to enable a single DoH connection to accomplish multiple tasks including service discovery, correctness validation and dependency construction.","Second, we propose an efficient DoH detection tool.","This tool can enhance probing efficiency while significantly reduce the required traffic volume.","Third, based on the above optimization methods, we conducted an exploration of the IPv4 space and performed an in-depth analysis of DoH based on the collected information.","Through experiments, our approach demonstrates a remarkable 80% improvement in time efficiency, and only requires 4%-20% traffic volume to complete the detection task.","In wild detection, our approach discovered 46k DoH services, which nearly doubles the number discovered by the state-of-the-art.","Based on the collected data, we present several intriguing conclusions about the current DoH service ecosystem."],"url":"http://arxiv.org/abs/2403.12363v1","category":"cs.CR"}
{"created":"2024-03-19 01:58:14","title":"Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation","abstract":"This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient data sampling and fast inference time. The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated data are available for training. Such device-informed simulated data are much easier to collect than real-world data but exhibit large distribution shifts from their real-world counterparts. To leverage such simulated data effectively, a hierarchical data augmentation strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented data is designed. Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference while attaining on-par performance with the state-of-the-art optimization-based methods.","sentences":["This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient data sampling and fast inference time.","The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated data are available for training.","Such device-informed simulated data are much easier to collect than real-world data but exhibit large distribution shifts from their real-world counterparts.","To leverage such simulated data effectively, a hierarchical data augmentation strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented data is designed.","Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference while attaining on-par performance with the state-of-the-art optimization-based methods."],"url":"http://arxiv.org/abs/2403.12354v1","category":"cs.LG"}
{"created":"2024-03-19 01:44:02","title":"A New Intelligent Reflecting Surface-Aided Electromagnetic Stealth Strategy","abstract":"Electromagnetic wave absorbing material (EWAM) plays an essential role in manufacturing stealth aircraft, which can achieve the electromagnetic stealth (ES) by reducing the strength of the signal reflected back to the radar system. However, the stealth performance is limited by the coating thickness, incident wave angles, and working frequencies. To tackle these limitations, we propose a new intelligent reflecting surface (IRS)-aided ES system where an IRS is deployed at the target to synergize with EWAM for effectively mitigating the echo signal and thus reducing the radar detection probability. Considering the monotonic relationship between the detection probability and the received signal-to-noise-ratio (SNR) at the radar, we formulate an optimization problem that minimizes the SNR under the reflection constraint of each IRS element, and a semi-closed-form solution is derived by using Karush-Kuhn-Tucker (KKT) conditions. Simulation results validate the superiority of the proposed IRS-aided ES system compared to various benchmarks.","sentences":["Electromagnetic wave absorbing material (EWAM) plays an essential role in manufacturing stealth aircraft, which can achieve the electromagnetic stealth (ES) by reducing the strength of the signal reflected back to the radar system.","However, the stealth performance is limited by the coating thickness, incident wave angles, and working frequencies.","To tackle these limitations, we propose a new intelligent reflecting surface (IRS)-aided ES system where an IRS is deployed at the target to synergize with EWAM for effectively mitigating the echo signal and thus reducing the radar detection probability.","Considering the monotonic relationship between the detection probability and the received signal-to-noise-ratio (SNR) at the radar, we formulate an optimization problem that minimizes the SNR under the reflection constraint of each IRS element, and a semi-closed-form solution is derived by using Karush-Kuhn-Tucker (KKT) conditions.","Simulation results validate the superiority of the proposed IRS-aided ES system compared to various benchmarks."],"url":"http://arxiv.org/abs/2403.12352v1","category":"eess.SP"}
{"created":"2024-03-19 00:24:56","title":"Metastability in Parabolic Equations and Diffusion Processes with a Small Parameter","abstract":"We study diffusion processes in $\\mathbb{R}^d$ that leave invariant a finite collection of manifolds (surfaces or points) in $\\mathbb{R}^d$ and small perturbations of such processes. Assuming certain ergodic properties at and near the invariant surfaces, we describe the rate at which the process gets attracted to or repelled from the surface, based on the local behavior of the coefficients. For processes that include, additionally, a small non-degenerate perturbation, we describe the metastable behavior. Namely, by allowing the time scale to depend on the size of the perturbation, we observe different asymptotic distributions of the process at different time scales.   Stated in PDE terms, the results provide the asymptotics, at different time scales, for the solution of the parabolic Cauchy problem when the operator that degenerates on a collection of surfaces is perturbed by a small non-degenerate term. This asymptotic behavior switches at a finite number of time scales that are calculated and does not depend on the perturbation.","sentences":["We study diffusion processes in $\\mathbb{R}^d$ that leave invariant a finite collection of manifolds (surfaces or points) in $\\mathbb{R}^d$ and small perturbations of such processes.","Assuming certain ergodic properties at and near the invariant surfaces, we describe the rate at which the process gets attracted to or repelled from the surface, based on the local behavior of the coefficients.","For processes that include, additionally, a small non-degenerate perturbation, we describe the metastable behavior.","Namely, by allowing the time scale to depend on the size of the perturbation, we observe different asymptotic distributions of the process at different time scales.   ","Stated in PDE terms, the results provide the asymptotics, at different time scales, for the solution of the parabolic Cauchy problem when the operator that degenerates on a collection of surfaces is perturbed by a small non-degenerate term.","This asymptotic behavior switches at a finite number of time scales that are calculated and does not depend on the perturbation."],"url":"http://arxiv.org/abs/2403.12333v1","category":"math.PR"}
{"created":"2024-03-18 23:32:08","title":"Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional Computing on Embedded Devices","abstract":"Alcohol consumption has a significant impact on individuals' health, with even more pronounced consequences when consumption becomes excessive. One approach to promoting healthier drinking habits is implementing just-in-time interventions, where timely notifications indicating intoxication are sent during heavy drinking episodes. However, the complexity or invasiveness of an intervention mechanism may deter an individual from using them in practice. Previous research tackled this challenge using collected motion data and conventional Machine Learning (ML) algorithms to classify heavy drinking episodes, but with impractical accuracy and computational efficiency for mobile devices. Consequently, we have elected to use Hyperdimensional Computing (HDC) to design a just-in-time intervention approach that is practical for smartphones, smart wearables, and IoT deployment. HDC is a framework that has proven results in processing real-time sensor data efficiently. This approach offers several advantages, including low latency, minimal power consumption, and high parallelism. We explore various HDC encoding designs and combine them with various HDC learning models to create an optimal and feasible approach for mobile devices. Our findings indicate an accuracy rate of 89\\%, which represents a substantial 12\\% improvement over the current state-of-the-art.","sentences":["Alcohol consumption has a significant impact on individuals' health, with even more pronounced consequences when consumption becomes excessive.","One approach to promoting healthier drinking habits is implementing just-in-time interventions, where timely notifications indicating intoxication are sent during heavy drinking episodes.","However, the complexity or invasiveness of an intervention mechanism may deter an individual from using them in practice.","Previous research tackled this challenge using collected motion data and conventional Machine Learning (ML) algorithms to classify heavy drinking episodes, but with impractical accuracy and computational efficiency for mobile devices.","Consequently, we have elected to use Hyperdimensional Computing (HDC) to design a just-in-time intervention approach that is practical for smartphones, smart wearables, and IoT deployment.","HDC is a framework that has proven results in processing real-time sensor data efficiently.","This approach offers several advantages, including low latency, minimal power consumption, and high parallelism.","We explore various HDC encoding designs and combine them with various HDC learning models to create an optimal and feasible approach for mobile devices.","Our findings indicate an accuracy rate of 89\\%, which represents a substantial 12\\% improvement over the current state-of-the-art."],"url":"http://arxiv.org/abs/2403.12323v1","category":"cs.LG"}
{"created":"2024-03-18 23:23:50","title":"Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training","abstract":"Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks. The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance. In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation. By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware. Extensive experiments demonstrate the effectiveness of the approximation technique in neural network training. This work underscores the potential of the likelihood ratio method in achieving high-performance neural network training, suggesting avenues for further exploration.","sentences":["Efficient and biologically plausible alternatives to backpropagation in neural network training remain a challenge due to issues such as high computational complexity and additional assumptions about neural networks, which limit scalability to deeper networks.","The likelihood ratio method offers a promising gradient estimation strategy but is constrained by significant memory consumption, especially when deploying multiple copies of data to reduce estimation variance.","In this paper, we introduce an approximation technique for the likelihood ratio (LR) method to alleviate computational and memory demands in gradient estimation.","By exploiting the natural parallelism during the backward pass using LR, we further provide a high-performance training strategy, which pipelines both the forward and backward pass, to make it more suitable for the computation on specialized hardware.","Extensive experiments demonstrate the effectiveness of the approximation technique in neural network training.","This work underscores the potential of the likelihood ratio method in achieving high-performance neural network training, suggesting avenues for further exploration."],"url":"http://arxiv.org/abs/2403.12320v1","category":"cs.LG"}
{"created":"2024-03-18 23:18:27","title":"Reinforcement Learning from Delayed Observations via World Models","abstract":"In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the first time showcasing delay-aware reinforcement learning on visual observations.","sentences":["In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them.","However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms.","In this paper, we focus on addressing observation delays in partially observable environments.","We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays.","By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases.","Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30.","Moreover, we evaluate our methods on visual input based delayed environment, for the first time showcasing delay-aware reinforcement learning on visual observations."],"url":"http://arxiv.org/abs/2403.12309v1","category":"cs.LG"}
{"created":"2024-03-18 23:18:16","title":"Gradient-based Fuzzy System Optimisation via Automatic Differentiation -- FuzzyR as a Use Case","abstract":"Since their introduction, fuzzy sets and systems have become an important area of research known for its versatility in modelling, knowledge representation and reasoning, and increasingly its potential within the context explainable AI. While the applications of fuzzy systems are diverse, there has been comparatively little advancement in their design from a machine learning perspective. In other words, while representations such as neural networks have benefited from a boom in learning capability driven by an increase in computational performance in combination with advances in their training mechanisms and available tool, in particular gradient descent, the impact on fuzzy system design has been limited. In this paper, we discuss gradient-descent-based optimisation of fuzzy systems, focussing in particular on automatic differentiation -- crucial to neural network learning -- with a view to free fuzzy system designers from intricate derivative computations, allowing for more focus on the functional and explainability aspects of their design. As a starting point, we present a use case in FuzzyR which demonstrates how current fuzzy inference system implementations can be adjusted to leverage powerful features of automatic differentiation tools sets, discussing its potential for the future of fuzzy system design.","sentences":["Since their introduction, fuzzy sets and systems have become an important area of research known for its versatility in modelling, knowledge representation and reasoning, and increasingly its potential within the context explainable AI.","While the applications of fuzzy systems are diverse, there has been comparatively little advancement in their design from a machine learning perspective.","In other words, while representations such as neural networks have benefited from a boom in learning capability driven by an increase in computational performance in combination with advances in their training mechanisms and available tool, in particular gradient descent, the impact on fuzzy system design has been limited.","In this paper, we discuss gradient-descent-based optimisation of fuzzy systems, focussing in particular on automatic differentiation -- crucial to neural network learning -- with a view to free fuzzy system designers from intricate derivative computations, allowing for more focus on the functional and explainability aspects of their design.","As a starting point, we present a use case in FuzzyR which demonstrates how current fuzzy inference system implementations can be adjusted to leverage powerful features of automatic differentiation tools sets, discussing its potential for the future of fuzzy system design."],"url":"http://arxiv.org/abs/2403.12308v1","category":"cs.AI"}
{"created":"2024-03-18 23:16:17","title":"Molecular Classification Using Hyperdimensional Graph Classification","abstract":"Our work introduces an innovative approach to graph learning by leveraging Hyperdimensional Computing. Graphs serve as a widely embraced method for conveying information, and their utilization in learning has gained significant attention. This is notable in the field of chemoinformatics, where learning from graph representations plays a pivotal role. An important application within this domain involves the identification of cancerous cells across diverse molecular structures.   We propose an HDC-based model that demonstrates comparable Area Under the Curve results when compared to state-of-the-art models like Graph Neural Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it outperforms previously proposed hyperdimensional computing graph learning methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x acceleration in the training phase and a 15x improvement in inference time compared to GNN and WL models. This not only underscores the efficacy of the HDC-based method, but also highlights its potential for expedited and resource-efficient graph learning.","sentences":["Our work introduces an innovative approach to graph learning by leveraging Hyperdimensional Computing.","Graphs serve as a widely embraced method for conveying information, and their utilization in learning has gained significant attention.","This is notable in the field of chemoinformatics, where learning from graph representations plays a pivotal role.","An important application within this domain involves the identification of cancerous cells across diverse molecular structures.   ","We propose an HDC-based model that demonstrates comparable Area Under the Curve results when compared to state-of-the-art models like Graph Neural Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL).","Moreover, it outperforms previously proposed hyperdimensional computing graph learning methods.","Furthermore, it achieves noteworthy speed enhancements, boasting a 40x acceleration in the training phase and a 15x improvement in inference time compared to GNN and WL models.","This not only underscores the efficacy of the HDC-based method, but also highlights its potential for expedited and resource-efficient graph learning."],"url":"http://arxiv.org/abs/2403.12307v1","category":"cs.LG"}
{"created":"2024-03-18 22:39:03","title":"Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach","abstract":"Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health. Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes. Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns. This study investigates the application of LLMs for extracting severity-related information for various SUD diagnoses from clinical notes. We propose a workflow employing zero-shot learning of LLMs with carefully crafted prompts and post-processing techniques. Through experimentation with Flan-T5, an open-source LLM, we demonstrate its superior recall compared to the rule-based approach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness of LLMs in extracting severity information, contributing to improved risk assessment and treatment planning for SUD patients.","sentences":["Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society.","SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health.","Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes.","Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language.","Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns.","This study investigates the application of LLMs for extracting severity-related information for various SUD diagnoses from clinical notes.","We propose a workflow employing zero-shot learning of LLMs with carefully crafted prompts and post-processing techniques.","Through experimentation with Flan-T5, an open-source LLM, we demonstrate its superior recall compared to the rule-based approach.","Focusing on 11 categories of SUD diagnoses, we show the effectiveness of LLMs in extracting severity information, contributing to improved risk assessment and treatment planning for SUD patients."],"url":"http://arxiv.org/abs/2403.12297v1","category":"cs.CL"}
{"created":"2024-03-18 22:37:37","title":"5G Cellular Systems Using UAVs, RES, and RISs (PL: Systemy kom\u00f3rkowe 5G wykorzystuj\u0105ce BSP, OZE oraz IPR)","abstract":"The paper considers energy consumption in 5G cellular networks powered by Renewable Energy Sources (RESs) and equipped with Reconfigurable Intelligent Surfaces (RISs) and tethered Unmanned Aerial Vehicles (UAVs), acting as mobile access points. The study was focused on the energy side of the Radio Access Network (RAN) located in the city of Pozna\\'n in Poland. The profit associated with the use of renewable energy generators, i.e. photovoltaic panels (PVP) for base stations (BSs) is presented in the form of two factors: the average number of UAV charges (ANUC) to provide continuous access to mobile services for connected user equipment (UE) terminals, and the average reduction in energy consumption (AREC) of the wireless system.","sentences":["The paper considers energy consumption in 5G cellular networks powered by Renewable Energy Sources (RESs) and equipped with Reconfigurable Intelligent Surfaces (RISs) and tethered Unmanned Aerial Vehicles (UAVs), acting as mobile access points.","The study was focused on the energy side of the Radio Access Network (RAN) located in the city of Pozna\\'n in Poland.","The profit associated with the use of renewable energy generators, i.e. photovoltaic panels (PVP) for base stations (BSs) is presented in the form of two factors: the average number of UAV charges (ANUC) to provide continuous access to mobile services for connected user equipment (UE) terminals, and the average reduction in energy consumption (AREC) of the wireless system."],"url":"http://arxiv.org/abs/2403.12296v1","category":"cs.NI"}
{"created":"2024-03-18 22:33:51","title":"A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2","abstract":"In this study we compared how well DALL-E 2 visually represented the meaning of linguistic prompts also given to young children in comprehension tests. Sentences representing fundamental components of grammatical knowledge were selected from assessment tests used with several hundred English-speaking children aged 2-7 years for whom we had collected original item-level data. DALL-E 2 was given these prompts five times to generate 20 cartoons per item, for 9 adult judges to score. Results revealed no conditions in which DALL-E 2-generated images that matched the semantic accuracy of children, even at the youngest age (2 years). DALL-E 2 failed to assign the appropriate roles in reversible forms; it failed on negation despite an easier contrastive prompt than the children received; it often assigned the adjective to the wrong noun; it ignored implicit agents in passives. This work points to a clear absence of compositional sentence representations for DALL-E 2.","sentences":["In this study we compared how well DALL-E 2 visually represented the meaning of linguistic prompts also given to young children in comprehension tests.","Sentences representing fundamental components of grammatical knowledge were selected from assessment tests used with several hundred English-speaking children aged 2-7 years for whom we had collected original item-level data.","DALL-E 2 was given these prompts five times to generate 20 cartoons per item, for 9 adult judges to score.","Results revealed no conditions in which DALL-E 2-generated images that matched the semantic accuracy of children, even at the youngest age (2 years).","DALL-E 2 failed to assign the appropriate roles in reversible forms; it failed on negation despite an easier contrastive prompt than the children received; it often assigned the adjective to the wrong noun; it ignored implicit agents in passives.","This work points to a clear absence of compositional sentence representations for DALL-E 2."],"url":"http://arxiv.org/abs/2403.12294v1","category":"cs.CL"}
{"created":"2024-03-18 22:31:44","title":"Photon statistics analysis of h-BN quantum emitters with pulsed and continuous-wave excitation","abstract":"We report on the quantum photon statistics of hexagonal boron nitride (h-BN) quantum emitters by analyzing the Mandel Q parameter. We have measured the Mandel Q parameter for h-BN quantum emitters under various temperatures and pump power excitation conditions. Under pulsed excitation we can achieve a Mandel Q of -0.002 and under continuous-wave (CW) excitation this parameter can reach -0.0025. We investigate the effect of cryogenic temperatures on Mandel Q and conclude that the photon statistics vary weakly with temperature. Through calculation of spontaneous emission from an excited two-level emitter model, we demonstrate good agreement between measured and calculated Mandel Q parameter when accounting for the experimental photon collection efficiency. Finally, we illustrate the usefulness of Mandel Q in quantum applications by the example of random number generation and analyze the effect of Mandel Q on the speed of generating random bits via this method.","sentences":["We report on the quantum photon statistics of hexagonal boron nitride (h-BN) quantum emitters by analyzing the Mandel Q parameter.","We have measured the Mandel Q parameter for h-BN quantum emitters under various temperatures and pump power excitation conditions.","Under pulsed excitation we can achieve a Mandel Q of -0.002 and under continuous-wave (CW) excitation this parameter can reach -0.0025.","We investigate the effect of cryogenic temperatures on Mandel Q and conclude that the photon statistics vary weakly with temperature.","Through calculation of spontaneous emission from an excited two-level emitter model, we demonstrate good agreement between measured and calculated Mandel Q parameter when accounting for the experimental photon collection efficiency.","Finally, we illustrate the usefulness of Mandel Q in quantum applications by the example of random number generation and analyze the effect of Mandel Q on the speed of generating random bits via this method."],"url":"http://arxiv.org/abs/2403.12291v1","category":"quant-ph"}
{"created":"2024-03-18 21:42:07","title":"Energy Consumption in Wireless Systems Equipped with RES, UAVs, and IRSs","abstract":"The paper considers the characteristics of the energy budget for mobile base stations (BSs) in the form of Unmanned Aerial Vehicles (UAVs) equipped with Radio Frequency (RF) transceivers, Intelligent Reconfigurable Surfaces (IRSs), and Renewable Energy Sources (RESs). The obtained results highlight the benefits and challenges related to using the aforementioned mobile base stations from the energy side. The research cases took into account two types of UAV devices - multirotor and fixed-wing (airplane-like).","sentences":["The paper considers the characteristics of the energy budget for mobile base stations (BSs) in the form of Unmanned Aerial Vehicles (UAVs) equipped with Radio Frequency (RF) transceivers, Intelligent Reconfigurable Surfaces (IRSs), and Renewable Energy Sources (RESs).","The obtained results highlight the benefits and challenges related to using the aforementioned mobile base stations from the energy side.","The research cases took into account two types of UAV devices - multirotor and fixed-wing (airplane-like)."],"url":"http://arxiv.org/abs/2403.12274v1","category":"cs.NI"}
{"created":"2024-03-18 21:04:02","title":"Parasitic Circus:On the Feasibility of Golden Free PCB Verification","abstract":"Printed circuit boards (PCBs) are an integral part of electronic systems. Hence, verifying their physical integrity in the presence of supply chain attacks (e.g., tampering and counterfeiting) is of utmost importance. Recently, tamper detection techniques grounded in impedance characterization of PCB's Power Delivery Network (PDN) have gained prominence due to their global detection coverage, non-invasive, and low-cost nature. Similar to other physical verification methods, these techniques rely on the existence of a physical golden sample for signature comparisons. However, having access to a physical golden sample for golden signature extraction is not feasible in many real-world scenarios. In this work, we assess the feasibility of eliminating a physical golden sample and replacing it with a simulated golden signature obtained by the PCB design files. By performing extensive simulation and measurements on an in-house designed PCB, we demonstrate how the parasitic impedance of the PCB components plays a major role in reaching a successful verification. Based on the obtained results and using statistical metrics, we show that we can mitigate the discrepancy between collected signatures from simulation and measurements.","sentences":["Printed circuit boards (PCBs) are an integral part of electronic systems.","Hence, verifying their physical integrity in the presence of supply chain attacks (e.g., tampering and counterfeiting) is of utmost importance.","Recently, tamper detection techniques grounded in impedance characterization of PCB's Power Delivery Network (PDN) have gained prominence due to their global detection coverage, non-invasive, and low-cost nature.","Similar to other physical verification methods, these techniques rely on the existence of a physical golden sample for signature comparisons.","However, having access to a physical golden sample for golden signature extraction is not feasible in many real-world scenarios.","In this work, we assess the feasibility of eliminating a physical golden sample and replacing it with a simulated golden signature obtained by the PCB design files.","By performing extensive simulation and measurements on an in-house designed PCB, we demonstrate how the parasitic impedance of the PCB components plays a major role in reaching a successful verification.","Based on the obtained results and using statistical metrics, we show that we can mitigate the discrepancy between collected signatures from simulation and measurements."],"url":"http://arxiv.org/abs/2403.12252v1","category":"cs.CR"}
{"created":"2024-03-18 20:47:10","title":"Reference-based Metrics Disprove Themselves in Question Generation","abstract":"Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.","sentences":["Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG).","In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics.","Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference.","A good metric was expected to grade a human-validated question no worse than generated questions.","However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves.","We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models.","These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references.","Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment."],"url":"http://arxiv.org/abs/2403.12242v1","category":"cs.CL"}
{"created":"2024-03-18 20:35:35","title":"Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments","abstract":"The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for the HPO process. The analysis of the results identifies the main culprit for performance degradation attributed to stacking fully connected layers. This paper identifies new avenues for improving RL-based HPO processes in resource-constrained environments.","sentences":["The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs).","The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment.","In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers.","These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch.","The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for the HPO process.","The analysis of the results identifies the main culprit for performance degradation attributed to stacking fully connected layers.","This paper identifies new avenues for improving RL-based HPO processes in resource-constrained environments."],"url":"http://arxiv.org/abs/2403.12237v1","category":"cs.LG"}
{"created":"2024-03-18 20:23:14","title":"Edge-Disjoint Spanning Trees on Star-Product Networks","abstract":"Star-product graphs are a natural extension of the Cartesian product, but have not been well-studied. We show that many important established and emerging network topologies, including HyperX, SlimFly, BundleFly, PolarStar, mesh, and torus, are in fact star-product graphs. While this connection was known for BundleFly and PolarStar, it was not for the others listed.   We extend a method of constructing maximal and near-maximal sets of edge-disjoint spanning trees on Cartesian products to the star product, thus obtain maximal or near-maximal sets of edge-disjoint spanning trees on new networks of importance, where such sets can improve bandwidth of collective operations and therefore accelerate many important workloads in high-performance computing.","sentences":["Star-product graphs are a natural extension of the Cartesian product, but have not been well-studied.","We show that many important established and emerging network topologies, including HyperX, SlimFly, BundleFly, PolarStar, mesh, and torus, are in fact star-product graphs.","While this connection was known for BundleFly and PolarStar, it was not for the others listed.   ","We extend a method of constructing maximal and near-maximal sets of edge-disjoint spanning trees on Cartesian products to the star product, thus obtain maximal or near-maximal sets of edge-disjoint spanning trees on new networks of importance, where such sets can improve bandwidth of collective operations and therefore accelerate many important workloads in high-performance computing."],"url":"http://arxiv.org/abs/2403.12231v1","category":"cs.NI"}
{"created":"2024-03-18 20:06:58","title":"Intervalence Plasmons in Boron-Doped Diamond","abstract":"Doped semiconductors are capable of exhibiting metallic-like properties ranging from superconductivity to tunable localized surface plasmon resonances. Diamond is a wide-bandgap semiconductor that is rendered electronically active by incorporating a hole dopant, boron. While the effects of boron doping on the electronic band structure of diamond are well-studied, any link between charge carriers and plasmons, which could facilitate optical applications, has never been shown. Here, we report intervalence plasmons in boron-doped diamond, defined as collective electronic excitations between the valence subbands, opened up by the presence of holes. Evidence for these low energy excitations is provided by scanning transmission electron microscope-valence electron energy loss spectroscopy and photoinduced force infrared spectroscopy. The measured loss and absorbance spectra are subsequently reproduced by first-principles calculations based on the contribution of intervalence band transitions to the dielectric function. Remarkably, the calculations also reveal that the real part of the dielectric function exhibits a resonance characteristic of metallicity (narrow-banded negative values of the dielectric function). The energy of the zero-crossing and the position of the loss peak are found to coincide, and both increase with the carrier density. Our results provide insight into a new mechanism for inducing plasmon-like behavior in doped semiconductors from intervalence band transitions, and the possibility of attaining such properties in diamond, a key emerging material for biomedical and quantum information technologies.","sentences":["Doped semiconductors are capable of exhibiting metallic-like properties ranging from superconductivity to tunable localized surface plasmon resonances.","Diamond is a wide-bandgap semiconductor that is rendered electronically active by incorporating a hole dopant, boron.","While the effects of boron doping on the electronic band structure of diamond are well-studied, any link between charge carriers and plasmons, which could facilitate optical applications, has never been shown.","Here, we report intervalence plasmons in boron-doped diamond, defined as collective electronic excitations between the valence subbands, opened up by the presence of holes.","Evidence for these low energy excitations is provided by scanning transmission electron microscope-valence electron energy loss spectroscopy and photoinduced force infrared spectroscopy.","The measured loss and absorbance spectra are subsequently reproduced by first-principles calculations based on the contribution of intervalence band transitions to the dielectric function.","Remarkably, the calculations also reveal that the real part of the dielectric function exhibits a resonance characteristic of metallicity (narrow-banded negative values of the dielectric function).","The energy of the zero-crossing and the position of the loss peak are found to coincide, and both increase with the carrier density.","Our results provide insight into a new mechanism for inducing plasmon-like behavior in doped semiconductors from intervalence band transitions, and the possibility of attaining such properties in diamond, a key emerging material for biomedical and quantum information technologies."],"url":"http://arxiv.org/abs/2403.12221v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 20:00:24","title":"Stein fillings vs. Milnor fibers","abstract":"Given a link of a normal surface singularity with its canonical contact structure, we compare the collection of its Stein fillings to its Milnor fillings (that is, Milnor fibers of possible smoothings). We prove that, unlike Stein fillings, Milnor fillings of a given link have bounded topology; for links of sandwiched singularities, we further establish that there are only finitely many Milnor fillings. We discuss some other obstructions for a Stein filling to be represented by a Milnor fiber, and for various types of singularities, including simple classes like cusps and triangle singularities, we produce Stein fillings that do not come from Milnor fibers or resolutions. Meanwhile, we discover that there are many contact 3-manifolds that admit infinitely many Stein fillings but do not admit arbitrarily large ones.","sentences":["Given a link of a normal surface singularity with its canonical contact structure, we compare the collection of its Stein fillings to its Milnor fillings (that is, Milnor fibers of possible smoothings).","We prove that, unlike Stein fillings, Milnor fillings of a given link have bounded topology; for links of sandwiched singularities, we further establish that there are only finitely many Milnor fillings.","We discuss some other obstructions for a Stein filling to be represented by a Milnor fiber, and for various types of singularities, including simple classes like cusps and triangle singularities, we produce Stein fillings that do not come from Milnor fibers or resolutions.","Meanwhile, we discover that there are many contact 3-manifolds that admit infinitely many Stein fillings but do not admit arbitrarily large ones."],"url":"http://arxiv.org/abs/2403.12216v1","category":"math.GT"}
{"created":"2024-03-18 19:53:56","title":"Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions","abstract":"Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models, we conduct an evaluation on the test dataset, employing performance and error metrics. Our findings reveal that BERT-based models consistently outperform T5-based models. Furthermore, while the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5. A manual analysis of sentences generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to 1.0, between the original and generated sentences. However, critical errors emerge as both models exhibit discrepancies, such as alterations to monetary and percentage values, underscoring the importance of accuracy and consistency in the financial domain. Despite these challenges, PTT5 and mT5 achieve impressive macro F1-scores of 98.52% and 98.85%, respectively, with our proposed approach. Furthermore, our study sheds light on notable disparities in memory and time consumption for inference across the models.","sentences":["Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents.","However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese.","This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks.","By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5).","Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models.","Following the fine-tuning of the models, we conduct an evaluation on the test dataset, employing performance and error metrics.","Our findings reveal that BERT-based models consistently outperform T5-based models.","Furthermore, while the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5.","A manual analysis of sentences generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to 1.0, between the original and generated sentences.","However, critical errors emerge as both models exhibit discrepancies, such as alterations to monetary and percentage values, underscoring the importance of accuracy and consistency in the financial domain.","Despite these challenges, PTT5 and mT5 achieve impressive macro F1-scores of 98.52% and 98.85%, respectively, with our proposed approach.","Furthermore, our study sheds light on notable disparities in memory and time consumption for inference across the models."],"url":"http://arxiv.org/abs/2403.12212v1","category":"cs.CL"}
{"created":"2024-03-18 19:51:55","title":"A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness","abstract":"Medical records often consist of different modalities, such as images, text, and tabular information. Integrating all modalities offers a holistic view of a patient's condition, while analyzing them longitudinally provides a better understanding of disease progression. However, real-world longitudinal medical records present challenges: 1) patients may lack some or all of the data for a specific timepoint, and 2) certain modalities or views might be absent for all patients during a particular period. In this work, we introduce a unified model for longitudinal multi-modal multi-view (MMMV) prediction with missingness. Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability. We conduct extensive experiments on the knee osteoarthritis dataset from the Osteoarthritis Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a future timepoint. We demonstrate the effectiveness of our method by comparing results from our unified model to specific models that use the same modality and view combinations during training and evaluation. We also show the benefit of having extended temporal data and provide post-hoc analysis for a deeper understanding of each modality/view's importance for different tasks.","sentences":["Medical records often consist of different modalities, such as images, text, and tabular information.","Integrating all modalities offers a holistic view of a patient's condition, while analyzing them longitudinally provides a better understanding of disease progression.","However, real-world longitudinal medical records present challenges: 1) patients may lack some or all of the data for a specific timepoint, and 2) certain modalities or views might be absent for all patients during a particular period.","In this work, we introduce a unified model for longitudinal multi-modal multi-view (MMMV) prediction with missingness.","Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability.","We conduct extensive experiments on the knee osteoarthritis dataset from the Osteoarthritis Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a future timepoint.","We demonstrate the effectiveness of our method by comparing results from our unified model to specific models that use the same modality and view combinations during training and evaluation.","We also show the benefit of having extended temporal data and provide post-hoc analysis for a deeper understanding of each modality/view's importance for different tasks."],"url":"http://arxiv.org/abs/2403.12211v1","category":"cs.CV"}
{"created":"2024-03-18 19:51:17","title":"Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning","abstract":"Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we address by using a system decomposition technique to compute what we call Decomposed Control Lyapunov Functions (DCLFs). We use the computed DCLF for reward shaping, which we show improves RL performance. Through multiple examples, we demonstrate the effectiveness of this approach, where our method finds a policy to successfully land a quadcopter in less than half the amount of real-world data required by the state-of-the-art Soft-Actor Critic algorithm.","sentences":["Recent methods using Reinforcement Learning (RL) have proven to be successful for training intelligent agents in unknown environments.","However, RL has not been applied widely in real-world robotics scenarios.","This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications.","In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity.","Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF.","Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure.","However, this class of methods becomes intractable on high-dimensional systems, a problem that we address by using a system decomposition technique to compute what we call Decomposed Control Lyapunov Functions (DCLFs).","We use the computed DCLF for reward shaping, which we show improves RL performance.","Through multiple examples, we demonstrate the effectiveness of this approach, where our method finds a policy to successfully land a quadcopter in less than half the amount of real-world data required by the state-of-the-art Soft-Actor Critic algorithm."],"url":"http://arxiv.org/abs/2403.12210v1","category":"eess.SY"}
{"created":"2024-03-18 19:44:30","title":"Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat?","abstract":"The evolution of artificial intelligence (AI) has catalyzed a transformation in digital content generation, with profound implications for cyber influence operations. This report delves into the potential and limitations of generative deep learning models, such as diffusion models, in fabricating convincing synthetic images. We critically assess the accessibility, practicality, and output quality of these tools and their implications in threat scenarios of deception, influence, and subversion. Notably, the report generates content for several hypothetical cyber influence operations to demonstrate the current capabilities and limitations of these AI-driven methods for threat actors. While generative models excel at producing illustrations and non-realistic imagery, creating convincing photo-realistic content remains a significant challenge, limited by computational resources and the necessity for human-guided refinement. Our exploration underscores the delicate balance between technological advancement and its potential for misuse, prompting recommendations for ongoing research, defense mechanisms, multi-disciplinary collaboration, and policy development. These recommendations aim to leverage AI's potential for positive impact while safeguarding against its risks to the integrity of information, especially in the context of cyber influence.","sentences":["The evolution of artificial intelligence (AI) has catalyzed a transformation in digital content generation, with profound implications for cyber influence operations.","This report delves into the potential and limitations of generative deep learning models, such as diffusion models, in fabricating convincing synthetic images.","We critically assess the accessibility, practicality, and output quality of these tools and their implications in threat scenarios of deception, influence, and subversion.","Notably, the report generates content for several hypothetical cyber influence operations to demonstrate the current capabilities and limitations of these AI-driven methods for threat actors.","While generative models excel at producing illustrations and non-realistic imagery, creating convincing photo-realistic content remains a significant challenge, limited by computational resources and the necessity for human-guided refinement.","Our exploration underscores the delicate balance between technological advancement and its potential for misuse, prompting recommendations for ongoing research, defense mechanisms, multi-disciplinary collaboration, and policy development.","These recommendations aim to leverage AI's potential for positive impact while safeguarding against its risks to the integrity of information, especially in the context of cyber influence."],"url":"http://arxiv.org/abs/2403.12207v1","category":"cs.CY"}
{"created":"2024-03-18 19:22:53","title":"Compositional learning of functions in humans and machines","abstract":"The ability to learn and compose functions is foundational to efficient learning and reasoning in humans, enabling flexible generalizations such as creating new dishes from known cooking processes. Beyond sequential chaining of functions, existing linguistics literature indicates that humans can grasp more complex compositions with interacting functions, where output production depends on context changes induced by different function orderings. Extending the investigation into the visual domain, we developed a function learning paradigm to explore the capacity of humans and neural network models in learning and reasoning with compositional functions under varied interaction conditions. Following brief training on individual functions, human participants were assessed on composing two learned functions, in ways covering four main interaction types, including instances in which the application of the first function creates or removes the context for applying the second function. Our findings indicate that humans can make zero-shot generalizations on novel visual function compositions across interaction conditions, demonstrating sensitivity to contextual changes. A comparison with a neural network model on the same task reveals that, through the meta-learning for compositionality (MLC) approach, a standard sequence-to-sequence Transformer can mimic human generalization patterns in composing functions.","sentences":["The ability to learn and compose functions is foundational to efficient learning and reasoning in humans, enabling flexible generalizations such as creating new dishes from known cooking processes.","Beyond sequential chaining of functions, existing linguistics literature indicates that humans can grasp more complex compositions with interacting functions, where output production depends on context changes induced by different function orderings.","Extending the investigation into the visual domain, we developed a function learning paradigm to explore the capacity of humans and neural network models in learning and reasoning with compositional functions under varied interaction conditions.","Following brief training on individual functions, human participants were assessed on composing two learned functions, in ways covering four main interaction types, including instances in which the application of the first function creates or removes the context for applying the second function.","Our findings indicate that humans can make zero-shot generalizations on novel visual function compositions across interaction conditions, demonstrating sensitivity to contextual changes.","A comparison with a neural network model on the same task reveals that, through the meta-learning for compositionality (MLC) approach, a standard sequence-to-sequence Transformer can mimic human generalization patterns in composing functions."],"url":"http://arxiv.org/abs/2403.12201v1","category":"cs.AI"}
{"created":"2024-03-18 19:14:38","title":"Empirical Analysis on CI/CD Pipeline Evolution in Machine Learning Projects","abstract":"The growing popularity of machine learning (ML) and the integration of ML components with other software artifacts has led to the use of continuous integration and delivery (CI/CD) tools, such as Travis CI, GitHub Actions, etc. that enable faster integration and testing for ML projects. Such CI/CD configurations and services require synchronization during the life cycle of the projects. Several works discussed how CI/CD configuration and services change during their usage in traditional software systems. However, there is very limited knowledge of how CI/CD configuration and services change in ML projects.   To fill this knowledge gap, this work presents the first empirical analysis of how CI/CD configuration evolves for ML software systems. We manually analyzed 343 commits collected from 508 open-source ML projects to identify common CI/CD configuration change categories in ML projects and devised a taxonomy of 14 co-changes in CI/CD and ML components. Moreover, we developed a CI/CD configuration change clustering tool that identified frequent CI/CD configuration change patterns in 15,634 commits. Furthermore, we measured the expertise of ML developers who modify CI/CD configurations. Based on this analysis, we found that 61.8% of commits include a change to the build policy and minimal changes related to performance and maintainability compared to general open-source projects. Additionally, the co-evolution analysis identified that CI/CD configurations, in many cases, changed unnecessarily due to bad practices such as the direct inclusion of dependencies and a lack of usage of standardized testing frameworks. More practices were found through the change patterns analysis consisting of using deprecated settings and reliance on a generic build language. Finally, our developer's expertise analysis suggests that experienced developers are more inclined to modify CI/CD configurations.","sentences":["The growing popularity of machine learning (ML) and the integration of ML components with other software artifacts has led to the use of continuous integration and delivery (CI/CD) tools, such as Travis CI, GitHub Actions, etc. that enable faster integration and testing for ML projects.","Such CI/CD configurations and services require synchronization during the life cycle of the projects.","Several works discussed how CI/CD configuration and services change during their usage in traditional software systems.","However, there is very limited knowledge of how CI/CD configuration and services change in ML projects.   ","To fill this knowledge gap, this work presents the first empirical analysis of how CI/CD configuration evolves for ML software systems.","We manually analyzed 343 commits collected from 508 open-source ML projects to identify common CI/CD configuration change categories in ML projects and devised a taxonomy of 14 co-changes in CI/CD and ML components.","Moreover, we developed a CI/CD configuration change clustering tool that identified frequent CI/CD configuration change patterns in 15,634 commits.","Furthermore, we measured the expertise of ML developers who modify CI/CD configurations.","Based on this analysis, we found that 61.8% of commits include a change to the build policy and minimal changes related to performance and maintainability compared to general open-source projects.","Additionally, the co-evolution analysis identified that CI/CD configurations, in many cases, changed unnecessarily due to bad practices such as the direct inclusion of dependencies and a lack of usage of standardized testing frameworks.","More practices were found through the change patterns analysis consisting of using deprecated settings and reliance on a generic build language.","Finally, our developer's expertise analysis suggests that experienced developers are more inclined to modify CI/CD configurations."],"url":"http://arxiv.org/abs/2403.12199v1","category":"cs.SE"}
{"created":"2024-03-18 19:11:34","title":"E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space","abstract":"Face inpainting, the technique of restoring missing or damaged regions in facial images, is pivotal for applications like face recognition in occluded scenarios and image analysis with poor-quality captures. This process not only needs to produce realistic visuals but also preserve individual identity characteristics. The aim of this paper is to inpaint a face given periocular region (eyes-to-face) through a proposed new Generative Adversarial Network (GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used. The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training. We further improve the StyleGAN output to find the optimal code in the latent space using a new optimization for GAN inversion technique. Our E2F-Net requires a minimum training process reducing the computational complexity as a secondary benefit. Through extensive experiments, we show that our method successfully reconstructs the whole face with high quality, surpassing current techniques, despite significantly less training and supervision efforts. We have generated seven eyes-to-face datasets based on well-known public face datasets for training and verifying our proposed methods. The code and datasets are publicly available.","sentences":["Face inpainting, the technique of restoring missing or damaged regions in facial images, is pivotal for applications like face recognition in occluded scenarios and image analysis with poor-quality captures.","This process not only needs to produce realistic visuals but also preserve individual identity characteristics.","The aim of this paper is to inpaint a face given periocular region (eyes-to-face) through a proposed new Generative Adversarial Network (GAN)-based model called Eyes-to-Face Network (E2F-Net).","The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used.","The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training.","We further improve the StyleGAN output to find the optimal code in the latent space using a new optimization for GAN inversion technique.","Our E2F-Net requires a minimum training process reducing the computational complexity as a secondary benefit.","Through extensive experiments, we show that our method successfully reconstructs the whole face with high quality, surpassing current techniques, despite significantly less training and supervision efforts.","We have generated seven eyes-to-face datasets based on well-known public face datasets for training and verifying our proposed methods.","The code and datasets are publicly available."],"url":"http://arxiv.org/abs/2403.12197v1","category":"cs.CV"}
{"created":"2024-03-18 19:10:12","title":"Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models","abstract":"The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests. Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support. Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results. The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.   We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and zero-shot-role-play-Chain of Thought (CoT) prompting techniques for ChatGPT. We studied 5,115 npm packages (of which 2,180 are malicious) and performed a baseline comparison of the GPT-3 and GPT-4 models with a static analysis tool. Our findings showed promising results for GPT models with low misclassification alert rates. Our baseline comparison demonstrates a notable improvement over static analysis in precision scores above 25% and F1 scores above 15%. We attained precision and F1 scores of 91% and 94%, respectively, for the GPT-3 model. Overall, GPT-4 demonstrates superior performance in precision (99%) and F1 (97%) scores, while GPT-3 presents a cost-effective balance between performance and expenditure.","sentences":["The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests.","Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support.","Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results.","The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.   ","We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and zero-shot-role-play-Chain of Thought (CoT) prompting techniques for ChatGPT.","We studied 5,115 npm packages (of which 2,180 are malicious) and performed a baseline comparison of the GPT-3 and GPT-4 models with a static analysis tool.","Our findings showed promising results for GPT models with low misclassification alert rates.","Our baseline comparison demonstrates a notable improvement over static analysis in precision scores above 25% and F1 scores above 15%.","We attained precision and F1 scores of 91% and 94%, respectively, for the GPT-3 model.","Overall, GPT-4 demonstrates superior performance in precision (99%) and F1 (97%) scores, while GPT-3 presents a cost-effective balance between performance and expenditure."],"url":"http://arxiv.org/abs/2403.12196v1","category":"cs.CR"}
{"created":"2024-03-18 18:52:04","title":"MAC Advice for Facility Location Mechanism Design","abstract":"Algorithms with predictions have attracted much attention in the last years across various domains, including variants of facility location, as a way to surpass traditional worst-case analyses. We study the $k$-facility location mechanism design problem, where the $n$ agents are strategic and might misreport their location.   Unlike previous models, where predictions are for the $k$ optimal facility locations, we receive $n$ predictions for the locations of each of the agents. However, these predictions are only \"mostly\" and \"approximately\" correct (or MAC for short) -- i.e., some $\\delta$-fraction of the predicted locations are allowed to be arbitrarily incorrect, and the remainder of the predictions are allowed to be correct up to an $\\varepsilon$-error. We make no assumption on the independence of the errors. Can such predictions allow us to beat the current best bounds for strategyproof facility location?   We show that the $1$-median (geometric median) of a set of points is naturally robust under corruptions, which leads to an algorithm for single-facility location with MAC predictions. We extend the robustness result to a \"balanced\" variant of the $k$ facilities case. Without balancedness, we show that robustness completely breaks down, even for the setting of $k=2$ facilities on a line. For this \"unbalanced\" setting, we devise a truthful random mechanism that outperforms the best known result of Lu et al. [2010], which does not use predictions. En route, we introduce the problem of \"second\" facility location (when the first facility's location is already fixed). Our findings on the robustness of the $1$-median and more generally $k$-medians may be of independent interest, as quantitative versions of classic breakdown-point results in robust statistics.","sentences":["Algorithms with predictions have attracted much attention in the last years across various domains, including variants of facility location, as a way to surpass traditional worst-case analyses.","We study the $k$-facility location mechanism design problem, where the $n$ agents are strategic and might misreport their location.   ","Unlike previous models, where predictions are for the $k$ optimal facility locations, we receive $n$ predictions for the locations of each of the agents.","However, these predictions are only \"mostly\" and \"approximately\" correct (or MAC for short) -- i.e., some $\\delta$-fraction of the predicted locations are allowed to be arbitrarily incorrect, and the remainder of the predictions are allowed to be correct up to an $\\varepsilon$-error.","We make no assumption on the independence of the errors.","Can such predictions allow us to beat the current best bounds for strategyproof facility location?   ","We show that the $1$-median (geometric median) of a set of points is naturally robust under corruptions, which leads to an algorithm for single-facility location with MAC predictions.","We extend the robustness result to a \"balanced\" variant of the $k$ facilities case.","Without balancedness, we show that robustness completely breaks down, even for the setting of $k=2$ facilities on a line.","For this \"unbalanced\" setting, we devise a truthful random mechanism that outperforms the best known result of Lu et al.","[2010], which does not use predictions.","En route, we introduce the problem of \"second\" facility location (when the first facility's location is already fixed).","Our findings on the robustness of the $1$-median and more generally $k$-medians may be of independent interest, as quantitative versions of classic breakdown-point results in robust statistics."],"url":"http://arxiv.org/abs/2403.12181v1","category":"cs.GT"}
{"created":"2024-03-18 18:49:20","title":"Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving","abstract":"The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to bridge the gaps between these topics and seek to answer the following research question: When and how can explanations improve safety of autonomous driving? In this regard, we first revisit established safety and state-of-the-art explainability techniques in autonomous driving. Furthermore, we present three critical case studies and show the pivotal role of explanations in enhancing self-driving safety. Finally, we describe our empirical investigation and reveal potential value, limitations, and caveats with practical explainable AI methods on their role of assuring safety and transparency for vehicle autonomy.","sentences":["The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices.","However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles.","Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents.","Such drawback raises serious safety concerns from societal and legal perspectives.","Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation.","However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art.","In this paper, we aim to bridge the gaps between these topics and seek to answer the following research question: When and how can explanations improve safety of autonomous driving?","In this regard, we first revisit established safety and state-of-the-art explainability techniques in autonomous driving.","Furthermore, we present three critical case studies and show the pivotal role of explanations in enhancing self-driving safety.","Finally, we describe our empirical investigation and reveal potential value, limitations, and caveats with practical explainable AI methods on their role of assuring safety and transparency for vehicle autonomy."],"url":"http://arxiv.org/abs/2403.12176v1","category":"cs.RO"}
{"created":"2024-03-18 18:45:56","title":"Exploring Conformational Landscapes Along Anharmonic Low-Frequency Vibrations","abstract":"We aim to automatize the identification of collective variables to simplify and speed up enhanced sampling simulations of conformational dynamics in biomolecules. We focus on anharmonic low-frequency vibrations that exhibit fluctuations on timescales faster than conformational transitions but describe a path of least resistance towards structural change. A key challenge is that harmonic approximations are ill-suited to characterize these vibrations, which are observed at far-infrared frequencies and are easily excited by thermal collisions at room temperature.   Here, we approached this problem with a frequency-selective anharmonic (FRESEAN) mode analysis that does not rely on harmonic approximations and successfully isolates anharmonic low-frequency vibrations from short molecular dynamics simulation trajectories. We applied FRESEAN mode analysis to simulations of alanine dipeptide, a common test system for enhanced sampling simulation protocols, and compare the performance of isolated low-frequency vibrations to conventional user-defined collective variables (here backbone dihedral angles) in enhanced sampling simulations.   The comparison shows that enhanced sampling along anharmonic low-frequency vibrations not only reproduces known conformational dynamics but can even further improve sampling of slow transitions compared to user-defined collective variables. Notably, free energy surfaces spanned by low-frequency anharmonic vibrational modes exhibit lower barriers associated with conformational transitions relative to representations in backbone dihedral space. We thus conclude that anharmonic low-frequency vibrations provide a promising path for highly effective and fully automated enhanced sampling simulations of conformational dynamics in biomolecules.","sentences":["We aim to automatize the identification of collective variables to simplify and speed up enhanced sampling simulations of conformational dynamics in biomolecules.","We focus on anharmonic low-frequency vibrations that exhibit fluctuations on timescales faster than conformational transitions but describe a path of least resistance towards structural change.","A key challenge is that harmonic approximations are ill-suited to characterize these vibrations, which are observed at far-infrared frequencies and are easily excited by thermal collisions at room temperature.   ","Here, we approached this problem with a frequency-selective anharmonic (FRESEAN) mode analysis that does not rely on harmonic approximations and successfully isolates anharmonic low-frequency vibrations from short molecular dynamics simulation trajectories.","We applied FRESEAN mode analysis to simulations of alanine dipeptide, a common test system for enhanced sampling simulation protocols, and compare the performance of isolated low-frequency vibrations to conventional user-defined collective variables (here backbone dihedral angles) in enhanced sampling simulations.   ","The comparison shows that enhanced sampling along anharmonic low-frequency vibrations not only reproduces known conformational dynamics but can even further improve sampling of slow transitions compared to user-defined collective variables.","Notably, free energy surfaces spanned by low-frequency anharmonic vibrational modes exhibit lower barriers associated with conformational transitions relative to representations in backbone dihedral space.","We thus conclude that anharmonic low-frequency vibrations provide a promising path for highly effective and fully automated enhanced sampling simulations of conformational dynamics in biomolecules."],"url":"http://arxiv.org/abs/2403.12174v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-18 18:45:28","title":"TnT-LLM: Text Mining at Scale with Large Language Models","abstract":"Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.","sentences":["Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application.","However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming.","This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable.","In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels.","We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case.","In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively.","In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale.","We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine.","Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale.","We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications."],"url":"http://arxiv.org/abs/2403.12173v1","category":"cs.CL"}
{"created":"2024-03-18 18:42:32","title":"Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection","abstract":"Skeleton-based video anomaly detection (SVAD) is a crucial task in computer vision. Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities, thereby enhancing safety. Achieving this demands a comprehensive understanding of human motions, both at body and region levels, while also accounting for the wide variations of performing a single action. However, existing studies fail to simultaneously address these crucial properties. This paper introduces a novel, practical and lightweight framework, namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD. GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data, the Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies between normal and abnormal motions, and the Graph-based Conditional Diffusion model to generate a wide spectrum of human motions. Extensive experiments on four widely used skeleton-based video datasets show that GiCiSAD outperforms existing methods with significantly fewer training parameters, establishing it as the new state-of-the-art.","sentences":["Skeleton-based video anomaly detection (SVAD) is a crucial task in computer vision.","Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities, thereby enhancing safety.","Achieving this demands a comprehensive understanding of human motions, both at body and region levels, while also accounting for the wide variations of performing a single action.","However, existing studies fail to simultaneously address these crucial properties.","This paper introduces a novel, practical and lightweight framework, namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD.","GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data, the Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies between normal and abnormal motions, and the Graph-based Conditional Diffusion model to generate a wide spectrum of human motions.","Extensive experiments on four widely used skeleton-based video datasets show that GiCiSAD outperforms existing methods with significantly fewer training parameters, establishing it as the new state-of-the-art."],"url":"http://arxiv.org/abs/2403.12172v1","category":"cs.CV"}
{"created":"2024-03-18 18:39:53","title":"EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models","abstract":"Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks. Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57% and 33%, respectively. We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs.","sentences":["Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs).","They are designed to bypass safeguards and elicit prohibited outputs.","However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations.","This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs.","It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator.","This modular framework enables researchers to easily construct attacks from combinations of novel and existing components.","So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs.","Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks.","Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57% and 33%, respectively.","We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs."],"url":"http://arxiv.org/abs/2403.12171v1","category":"cs.CL"}
{"created":"2024-03-18 18:23:36","title":"Intelligent Execution through Plan Analysis","abstract":"Intelligent robots need to generate and execute plans. In order to deal with the complexity of real environments, planning makes some assumptions about the world. When executing plans, the assumptions are usually not met. Most works have focused on the negative impact of this fact and the use of replanning after execution failures. Instead, we focus on the positive impact, or opportunities to find better plans. When planning, the proposed technique finds and stores those opportunities. Later, during execution, the monitoring system can use them to focus perception and repair the plan, instead of replanning from scratch. Experiments in several paradigmatic robotic tasks show how the approach outperforms standard replanning strategies.","sentences":["Intelligent robots need to generate and execute plans.","In order to deal with the complexity of real environments, planning makes some assumptions about the world.","When executing plans, the assumptions are usually not met.","Most works have focused on the negative impact of this fact and the use of replanning after execution failures.","Instead, we focus on the positive impact, or opportunities to find better plans.","When planning, the proposed technique finds and stores those opportunities.","Later, during execution, the monitoring system can use them to focus perception and repair the plan, instead of replanning from scratch.","Experiments in several paradigmatic robotic tasks show how the approach outperforms standard replanning strategies."],"url":"http://arxiv.org/abs/2403.12162v1","category":"cs.AI"}
{"created":"2024-03-18 18:19:08","title":"Effect of Leaders Voice on Financial Market: An Empirical Deep Learning Expedition on NASDAQ, NSE, and Beyond","abstract":"Financial market like the price of stock, share, gold, oil, mutual funds are affected by the news and posts on social media. In this work deep learning based models are proposed to predict the trend of financial market based on NLP analysis of the twitter handles of leaders of different fields. There are many models available to predict financial market based on only the historical data of the financial component but combining historical data with news and posts of the social media like Twitter is the main objective of the present work. Substantial improvement is shown in the result. The main features of the present work are: a) proposing completely generalized algorithm which is able to generate models for any twitter handle and any financial component, b) predicting the time window for a tweets effect on a stock price c) analyzing the effect of multiple twitter handles for predicting the trend. A detailed survey is done to find out the latest work in recent years in the similar field, find the research gap, and collect the required data for analysis and prediction. State-of-the-art algorithm is proposed and complete implementation with environment is given. An insightful trend of the result improvement considering the NLP analysis of twitter data on financial market components is shown. The Indian and USA financial markets are explored in the present work where as other markets can be taken in future. The socio-economic impact of the present work is discussed in conclusion.","sentences":["Financial market like the price of stock, share, gold, oil, mutual funds are affected by the news and posts on social media.","In this work deep learning based models are proposed to predict the trend of financial market based on NLP analysis of the twitter handles of leaders of different fields.","There are many models available to predict financial market based on only the historical data of the financial component but combining historical data with news and posts of the social media like Twitter is the main objective of the present work.","Substantial improvement is shown in the result.","The main features of the present work are: a) proposing completely generalized algorithm which is able to generate models for any twitter handle and any financial component, b) predicting the time window for a tweets effect on a stock price c) analyzing the effect of multiple twitter handles for predicting the trend.","A detailed survey is done to find out the latest work in recent years in the similar field, find the research gap, and collect the required data for analysis and prediction.","State-of-the-art algorithm is proposed and complete implementation with environment is given.","An insightful trend of the result improvement considering the NLP analysis of twitter data on financial market components is shown.","The Indian and USA financial markets are explored in the present work where as other markets can be taken in future.","The socio-economic impact of the present work is discussed in conclusion."],"url":"http://arxiv.org/abs/2403.12161v1","category":"cs.CE"}
{"created":"2024-03-18 18:14:35","title":"A study on fuzzy plane and its application on fuzzy plane fitting","abstract":"In this paper, I obtain an $S$-type fuzzy point when two fuzzy numbers for two independent variables and a corresponding fuzzy number for the dependent variable are given. A comprehensive study on a conceptualization of a fuzzy plane as a collection of fuzzy numbers, or fuzzy points is proposed. A perpendicular fuzzy distance from a fuzzy point to a fuzzy plane is also revisited. An application of the proposed fuzzy plane is made to fit a fuzzy plane to the available data sets of imprecise locations in $\\mathbb{R}^3$. Moreover, a degree of fuzzily fitted fuzzy plane to the given data sets of fuzzy points is defined. All the fuzzy geometric construction and characteristics of fuzzy planes are explored with the help of same and inverse points ideas. All the study is supported by numerical examples and illustrated by fuzzy geometrical figures. This study provides a framework for developing a fuzzy plane-fitting model that will benefit the fields of curve detecting and fitting, image processing for industrial and scientific applications, signal processing, and problems of shape recognition.","sentences":["In this paper, I obtain an $S$-type fuzzy point when two fuzzy numbers for two independent variables and a corresponding fuzzy number for the dependent variable are given.","A comprehensive study on a conceptualization of a fuzzy plane as a collection of fuzzy numbers, or fuzzy points is proposed.","A perpendicular fuzzy distance from a fuzzy point to a fuzzy plane is also revisited.","An application of the proposed fuzzy plane is made to fit a fuzzy plane to the available data sets of imprecise locations in $\\mathbb{R}^3$. Moreover, a degree of fuzzily fitted fuzzy plane to the given data sets of fuzzy points is defined.","All the fuzzy geometric construction and characteristics of fuzzy planes are explored with the help of same and inverse points ideas.","All the study is supported by numerical examples and illustrated by fuzzy geometrical figures.","This study provides a framework for developing a fuzzy plane-fitting model that will benefit the fields of curve detecting and fitting, image processing for industrial and scientific applications, signal processing, and problems of shape recognition."],"url":"http://arxiv.org/abs/2403.12157v1","category":"math.GM"}
{"created":"2024-03-18 18:09:47","title":"Routing and Scheduling in Answer Set Programming applied to Multi-Agent Path Finding: Preliminary Report","abstract":"We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding. The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. This also abolishes the need for fixed upper bounds on the length of plans. The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way. This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. We formally elaborate upon this idea and present several resulting ASP encodings. Finally, we demonstrate their effectiveness via an empirical analysis.","sentences":["We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding.","The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents.","This also abolishes the need for fixed upper bounds on the length of plans.","The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore.","While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way.","This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints.","We formally elaborate upon this idea and present several resulting ASP encodings.","Finally, we demonstrate their effectiveness via an empirical analysis."],"url":"http://arxiv.org/abs/2403.12153v1","category":"cs.AI"}
{"created":"2024-03-18 18:08:44","title":"Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification","abstract":"Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art performance achieved by the proposed approach.","sentences":["Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks.","However, the generation of such knowledge entails considerable human labor and time costs.","This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings.","To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task.","We thoroughly examine the behavior of the LLM through an extensive ablation study.","Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements.","Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art performance achieved by the proposed approach."],"url":"http://arxiv.org/abs/2403.12151v1","category":"cs.AI"}
{"created":"2024-03-18 18:01:01","title":"Graph Neural Networks for Learning Equivariant Representations of Neural Networks","abstract":"Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.","sentences":["Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors.","However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself.","In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry.","Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures.","We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods.","The source code is open-sourced at https://github.com/mkofinas/neural-graphs."],"url":"http://arxiv.org/abs/2403.12143v1","category":"cs.LG"}
{"created":"2024-03-18 17:59:04","title":"ROUTERBENCH: A Benchmark for Multi-LLM Routing System","abstract":"As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through ROUTERBENCH, highlighting their potentials and limitations within our evaluation framework. This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments. The code and data are available at https://github.com/withmartian/routerbench.","sentences":["As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical.","Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost.","This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs.","Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area.","To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies.","We further propose a theoretical framework for LLM routing, and deliver a comparative analysis of various routing approaches through ROUTERBENCH, highlighting their potentials and limitations within our evaluation framework.","This work not only formalizes and advances the development of LLM routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable LLM deployments.","The code and data are available at https://github.com/withmartian/routerbench."],"url":"http://arxiv.org/abs/2403.12031v1","category":"cs.LG"}
{"created":"2024-03-18 17:58:02","title":"Align and Distill: Unifying and Improving Domain Adaptive Object Detection","abstract":"Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin. ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel. Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research. Code and data are available: https://github.com/justinkay/aldi and https://github.com/visipedia/caltech-fish-counting.","sentences":["Object detectors often perform poorly on data that differs from their training set.","Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge.","Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks.","We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin.","ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel.","Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research.","Code and data are available: https://github.com/justinkay/aldi and https://github.com/visipedia/caltech-fish-counting."],"url":"http://arxiv.org/abs/2403.12029v1","category":"cs.CV"}
{"created":"2024-03-18 17:57:30","title":"Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and Detail","abstract":"3D human body reconstruction has been a challenge in the field of computer vision. Previous methods are often time-consuming and difficult to capture the detailed appearance of the human body. In this paper, we propose a new method called \\emph{Ultraman} for fast reconstruction of textured 3D human models from a single image. Compared to existing techniques, \\emph{Ultraman} greatly improves the reconstruction speed and accuracy while preserving high-quality texture details. We present a set of new frameworks for human reconstruction consisting of three parts, geometric reconstruction, texture generation and texture mapping. Firstly, a mesh reconstruction framework is used, which accurately extracts 3D human shapes from a single image. At the same time, we propose a method to generate a multi-view consistent image of the human body based on a single image. This is finally combined with a novel texture mapping method to optimize texture details and ensure color consistency during reconstruction. Through extensive experiments and evaluations, we demonstrate the superior performance of \\emph{Ultraman} on various standard datasets. In addition, \\emph{Ultraman} outperforms state-of-the-art methods in terms of human rendering quality and speed. Upon acceptance of the article, we will make the code and data publicly available.","sentences":["3D human body reconstruction has been a challenge in the field of computer vision.","Previous methods are often time-consuming and difficult to capture the detailed appearance of the human body.","In this paper, we propose a new method called \\emph{Ultraman} for fast reconstruction of textured 3D human models from a single image.","Compared to existing techniques, \\emph{Ultraman} greatly improves the reconstruction speed and accuracy while preserving high-quality texture details.","We present a set of new frameworks for human reconstruction consisting of three parts, geometric reconstruction, texture generation and texture mapping.","Firstly, a mesh reconstruction framework is used, which accurately extracts 3D human shapes from a single image.","At the same time, we propose a method to generate a multi-view consistent image of the human body based on a single image.","This is finally combined with a novel texture mapping method to optimize texture details and ensure color consistency during reconstruction.","Through extensive experiments and evaluations, we demonstrate the superior performance of \\emph{Ultraman} on various standard datasets.","In addition, \\emph{Ultraman} outperforms state-of-the-art methods in terms of human rendering quality and speed.","Upon acceptance of the article, we will make the code and data publicly available."],"url":"http://arxiv.org/abs/2403.12028v1","category":"cs.CV"}
{"created":"2024-03-18 17:57:09","title":"From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models","abstract":"Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics and sources of both charts and textual inputs. Modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance. Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance. Challenges and future directions are addressed in a dedicated section, highlighting issues such as domain-specific charts, lack of efforts in evaluation, and agent-oriented settings. This survey paper serves to provide valuable insights and directions for future research in chart understanding leveraging large foundation models. The studies mentioned in this paper, along with emerging new research, will be continually updated at: https://github.com/khuangaf/Awesome-Chart-Understanding.","sentences":["Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making.","Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years.","Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks.","This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models.","The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks.","In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics and sources of both charts and textual inputs.","Modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance.","Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance.","Challenges and future directions are addressed in a dedicated section, highlighting issues such as domain-specific charts, lack of efforts in evaluation, and agent-oriented settings.","This survey paper serves to provide valuable insights and directions for future research in chart understanding leveraging large foundation models.","The studies mentioned in this paper, along with emerging new research, will be continually updated at: https://github.com/khuangaf/Awesome-Chart-Understanding."],"url":"http://arxiv.org/abs/2403.12027v1","category":"cs.CL"}
{"created":"2024-03-18 17:57:02","title":"FlexCap: Generating Rich, Localized, and Flexible Captions in Images","abstract":"We introduce a versatile $\\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQA datasets. We also demonstrate a $\\textit{localize-then-describe}$ approach with FlexCap can be better at open-ended object detection than a $\\textit{describe-then-localize}$ approach with other VLMs. We highlight a novel characteristic of FlexCap, which is its ability to extract diverse visual information through prefix conditioning. Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks such as image labeling, object attribute recognition, and visual dialog. Project webpage: https://flex-cap.github.io .","sentences":["We introduce a versatile $\\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths.","The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions.","To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images.","This flexible-captioning capability has several valuable applications.   ","First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset.","Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model.","The resulting system achieves state-of-the-art zero-shot performance on a number of VQA datasets.","We also demonstrate a $\\textit{localize-then-describe}$ approach with FlexCap can be better at open-ended object detection than a $\\textit{describe-then-localize}$ approach with other VLMs.","We highlight a novel characteristic of FlexCap, which is its ability to extract diverse visual information through prefix conditioning.","Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks such as image labeling, object attribute recognition, and visual dialog.","Project webpage: https://flex-cap.github.io ."],"url":"http://arxiv.org/abs/2403.12026v1","category":"cs.CV"}
{"created":"2024-03-18 17:56:37","title":"A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models","abstract":"Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounded in an iterative participatory approach and review of possible biases in Med-PaLM 2 answers to adversarial queries. Through our empirical study, we find that the use of a collection of datasets curated through a variety of methodologies, coupled with a thorough evaluation protocol that leverages multiple assessment rubric designs and diverse rater groups, surfaces biases that may be missed via narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment of an AI system promotes equitable health outcomes. We hope the broader community leverages and builds on these tools and methods towards realizing a shared goal of LLMs that promote accessible and equitable healthcare for all.","sentences":["Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities.","Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity.","In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date.","Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries.","Both our human assessment framework and dataset design process are grounded in an iterative participatory approach and review of possible biases in Med-PaLM 2 answers to adversarial queries.","Through our empirical study, we find that the use of a collection of datasets curated through a variety of methodologies, coupled with a thorough evaluation protocol that leverages multiple assessment rubric designs and diverse rater groups, surfaces biases that may be missed via narrower evaluation approaches.","Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise.","We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment of an AI system promotes equitable health outcomes.","We hope the broader community leverages and builds on these tools and methods towards realizing a shared goal of LLMs that promote accessible and equitable healthcare for all."],"url":"http://arxiv.org/abs/2403.12025v1","category":"cs.CY"}
{"created":"2024-03-18 17:52:57","title":"Supervised Fine-Tuning as Inverse Reinforcement Learning","abstract":"The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets. Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine.","sentences":["The prevailing approach to aligning Large Language Models (LLMs) typically relies on human or AI feedback and assumes access to specific types of preference datasets.","In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic.","We build a sequential decision-making framework to formulate the problem of aligning LLMs using demonstration datasets.","Drawing insights from inverse reinforcement learning and imitation learning, we introduce various approaches for divergence minimization in the LLM alignment tasks.","Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches.","Inclusively, we examine the pros and cons of the classical supervised fine-tuning method, elaborating on scenarios where different methods shine."],"url":"http://arxiv.org/abs/2403.12017v1","category":"cs.LG"}
{"created":"2024-03-18 17:51:16","title":"EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents","abstract":"Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We show qualitatively how the LLM adapts training environments to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls. Lastly, we present detailed ablation studies for our design choices.","sentences":["Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment.","Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.","Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at?","We propose EnvGen, a novel framework to address this question.","First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel.","Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.).","Next, we train a small RL agent in a mixture of the original and LLM-generated environments.","Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance.","We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments.","We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster.","We show qualitatively how the LLM adapts training environments to help improve RL agents' weaker skills over time.","Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls.","Lastly, we present detailed ablation studies for our design choices."],"url":"http://arxiv.org/abs/2403.12014v1","category":"cs.CL"}
{"created":"2024-03-18 17:48:31","title":"HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data","abstract":"3D hand-object interaction data is scarce due to the hardware constraints in scaling up the data collection process. In this paper, we propose HOIDiffusion for generating realistic and diverse 3D hand-object interaction data. Our model is a conditional diffusion model that takes both the 3D hand-object geometric structure and text description as inputs for image synthesis. This offers a more controllable and realistic synthesis as we can specify the structure and style inputs in a disentangled manner. HOIDiffusion is trained by leveraging a diffusion model pre-trained on large-scale natural images and a few 3D human demonstrations. Beyond controllable image synthesis, we adopt the generated 3D data for learning 6D object pose estimation and show its effectiveness in improving perception systems. Project page: https://mq-zhang1.github.io/HOIDiffusion","sentences":["3D hand-object interaction data is scarce due to the hardware constraints in scaling up the data collection process.","In this paper, we propose HOIDiffusion for generating realistic and diverse 3D hand-object interaction data.","Our model is a conditional diffusion model that takes both the 3D hand-object geometric structure and text description as inputs for image synthesis.","This offers a more controllable and realistic synthesis as we can specify the structure and style inputs in a disentangled manner.","HOIDiffusion is trained by leveraging a diffusion model pre-trained on large-scale natural images and a few 3D human demonstrations.","Beyond controllable image synthesis, we adopt the generated 3D data for learning 6D object pose estimation and show its effectiveness in improving perception systems.","Project page: https://mq-zhang1.github.io/HOIDiffusion"],"url":"http://arxiv.org/abs/2403.12011v1","category":"cs.CV"}
{"created":"2024-03-18 17:48:15","title":"VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model","abstract":"Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV.","sentences":["Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content.","Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency.","This paper introduces a novel framework that makes fundamental contributions to both questions.","Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models.","Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency.","Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap.","To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images.","As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds.","Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency.","By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects.","Our project page is aigc3d.github.io/VideoMV."],"url":"http://arxiv.org/abs/2403.12010v1","category":"cs.CV"}
{"created":"2024-03-18 17:47:39","title":"Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks","abstract":"In the realm of skin lesion image classification, the intricate spatial and semantic features pose significant challenges for conventional Convolutional Neural Network (CNN)-based methodologies. These challenges are compounded by the imbalanced nature of skin lesion datasets, which hampers the ability of models to learn minority class features effectively. Despite augmentation strategies, such as those using Generative Adversarial Networks (GANs), previous attempts have not fully addressed these complexities. This study introduces an innovative approach by integrating Graph Neural Networks (GNNs) with Capsule Networks to enhance classification performance. GNNs, known for their proficiency in handling graph-structured data, offer an advanced mechanism for capturing complex patterns and relationships beyond the capabilities of traditional CNNs. Capsule Networks further contribute by providing superior recognition of spatial hierarchies within images. Our research focuses on evaluating and enhancing the Tiny Pyramid Vision GNN (Tiny Pyramid ViG) architecture by incorporating it with a Capsule Network. This hybrid model was applied to the MNIST:HAM10000 dataset, a comprehensive skin lesion dataset designed for benchmarking classification models. After 75 epochs of training, our model achieved a significant accuracy improvement, reaching 89.23% and 95.52%, surpassing established benchmarks such as GoogLeNet (83.94%), InceptionV3 (86.82%), MobileNet V3 (89.87%), EfficientNet-B7 (92.07%), ResNet18 (92.22%), ResNet34 (91.90%), ViT-Base (73.70%), and IRv2-SA (93.47%) on the same dataset. This outcome underscores the potential of our approach in overcoming the inherent challenges of skin lesion classification, contributing to the advancement of image-based diagnosis in dermatology.","sentences":["In the realm of skin lesion image classification, the intricate spatial and semantic features pose significant challenges for conventional Convolutional Neural Network (CNN)-based methodologies.","These challenges are compounded by the imbalanced nature of skin lesion datasets, which hampers the ability of models to learn minority class features effectively.","Despite augmentation strategies, such as those using Generative Adversarial Networks (GANs), previous attempts have not fully addressed these complexities.","This study introduces an innovative approach by integrating Graph Neural Networks (GNNs) with Capsule Networks to enhance classification performance.","GNNs, known for their proficiency in handling graph-structured data, offer an advanced mechanism for capturing complex patterns and relationships beyond the capabilities of traditional CNNs.","Capsule Networks further contribute by providing superior recognition of spatial hierarchies within images.","Our research focuses on evaluating and enhancing the Tiny Pyramid Vision GNN (Tiny Pyramid ViG) architecture by incorporating it with a Capsule Network.","This hybrid model was applied to the MNIST:HAM10000 dataset, a comprehensive skin lesion dataset designed for benchmarking classification models.","After 75 epochs of training, our model achieved a significant accuracy improvement, reaching 89.23% and 95.52%, surpassing established benchmarks such as GoogLeNet (83.94%), InceptionV3 (86.82%), MobileNet V3 (89.87%), EfficientNet-B7 (92.07%), ResNet18 (92.22%), ResNet34 (91.90%), ViT-Base (73.70%), and IRv2-SA (93.47%) on the same dataset.","This outcome underscores the potential of our approach in overcoming the inherent challenges of skin lesion classification, contributing to the advancement of image-based diagnosis in dermatology."],"url":"http://arxiv.org/abs/2403.12009v2","category":"cs.CV"}
{"created":"2024-03-18 17:42:27","title":"Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023","abstract":"Visualization for explainable and trustworthy machine learning remains one of the most important and heavily researched fields within information visualization and visual analytics with various application domains, such as medicine, finance, and bioinformatics. After our 2020 state-of-the-art report comprising 200 techniques, we have persistently collected peer-reviewed articles describing visualization techniques, categorized them based on the previously established categorization schema consisting of 119 categories, and provided the resulting collection of 542 techniques in an online survey browser. In this survey article, we present the updated findings of new analyses of this dataset as of fall 2023 and discuss trends, insights, and eight open challenges for using visualizations in machine learning. Our results corroborate the rapidly growing trend of visualization techniques for increasing trust in machine learning models in the past three years, with visualization found to help improve popular model explainability methods and check new deep learning architectures, for instance.","sentences":["Visualization for explainable and trustworthy machine learning remains one of the most important and heavily researched fields within information visualization and visual analytics with various application domains, such as medicine, finance, and bioinformatics.","After our 2020 state-of-the-art report comprising 200 techniques, we have persistently collected peer-reviewed articles describing visualization techniques, categorized them based on the previously established categorization schema consisting of 119 categories, and provided the resulting collection of 542 techniques in an online survey browser.","In this survey article, we present the updated findings of new analyses of this dataset as of fall 2023 and discuss trends, insights, and eight open challenges for using visualizations in machine learning.","Our results corroborate the rapidly growing trend of visualization techniques for increasing trust in machine learning models in the past three years, with visualization found to help improve popular model explainability methods and check new deep learning architectures, for instance."],"url":"http://arxiv.org/abs/2403.12005v1","category":"cs.HC"}
{"created":"2024-03-18 17:38:53","title":"DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing","abstract":"Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score distillation. Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in altering appearances while accurately preserving the original structure and motion.","sentences":["Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion.","Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion.","Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation.","To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score distillation.","Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks.","Through extensive comparisons with leading methods, our approach demonstrates its superiority in altering appearances while accurately preserving the original structure and motion."],"url":"http://arxiv.org/abs/2403.12002v1","category":"cs.CV"}
{"created":"2024-03-18 17:35:02","title":"Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance","abstract":"Deep learning-based probabilistic models of musical data are producing increasingly realistic results and promise to enter creative workflows of many kinds. Yet they have been little-studied in a performance setting, where the results of user actions typically ought to feel instantaneous. To enable such study, we designed Notochord, a deep probabilistic model for sequences of structured events, and trained an instance of it on the Lakh MIDI dataset. Our probabilistic formulation allows interpretable interventions at a sub-event level, which enables one model to act as a backbone for diverse interactive musical functions including steerable generation, harmonization, machine improvisation, and likelihood-based interfaces. Notochord can generate polyphonic and multi-track MIDI, and respond to inputs with latency below ten milliseconds. Training code, model checkpoints and interactive examples are provided as open source software.","sentences":["Deep learning-based probabilistic models of musical data are producing increasingly realistic results and promise to enter creative workflows of many kinds.","Yet they have been little-studied in a performance setting, where the results of user actions typically ought to feel instantaneous.","To enable such study, we designed Notochord, a deep probabilistic model for sequences of structured events, and trained an instance of it on the Lakh MIDI dataset.","Our probabilistic formulation allows interpretable interventions at a sub-event level, which enables one model to act as a backbone for diverse interactive musical functions including steerable generation, harmonization, machine improvisation, and likelihood-based interfaces.","Notochord can generate polyphonic and multi-track MIDI, and respond to inputs with latency below ten milliseconds.","Training code, model checkpoints and interactive examples are provided as open source software."],"url":"http://arxiv.org/abs/2403.12000v1","category":"cs.SD"}
{"created":"2024-03-18 17:34:29","title":"HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs","abstract":"The hybrid deep models of Vision Transformer (ViT) and Convolution Neural Network (CNN) have emerged as a powerful class of backbones for vision tasks. Scaling up the input resolution of such hybrid backbones naturally strengthes model capacity, but inevitably suffers from heavy computational cost that scales quadratically. Instead, we present a new hybrid backbone with HIgh-Resolution Inputs (namely HIRI-ViT), that upgrades prevalent four-stage ViT to five-stage ViT tailored for high-resolution inputs. HIRI-ViT is built upon the seminal idea of decomposing the typical CNN operations into two parallel CNN branches in a cost-efficient manner. One high-resolution branch directly takes primary high-resolution features as inputs, but uses less convolution operations. The other low-resolution branch first performs down-sampling and then utilizes more convolution operations over such low-resolution features. Experiments on both recognition task (ImageNet-1K dataset) and dense prediction tasks (COCO and ADE20K datasets) demonstrate the superiority of HIRI-ViT. More remarkably, under comparable computational cost ($\\sim$5.0 GFLOPs), HIRI-ViT achieves to-date the best published Top-1 accuracy of 84.3% on ImageNet with 448$\\times$448 inputs, which absolutely improves 83.4% of iFormer-S by 0.9% with 224$\\times$224 inputs.","sentences":["The hybrid deep models of Vision Transformer (ViT) and Convolution Neural Network (CNN) have emerged as a powerful class of backbones for vision tasks.","Scaling up the input resolution of such hybrid backbones naturally strengthes model capacity, but inevitably suffers from heavy computational cost that scales quadratically.","Instead, we present a new hybrid backbone with HIgh-Resolution Inputs (namely HIRI-ViT), that upgrades prevalent four-stage ViT to five-stage ViT tailored for high-resolution inputs.","HIRI-ViT is built upon the seminal idea of decomposing the typical CNN operations into two parallel CNN branches in a cost-efficient manner.","One high-resolution branch directly takes primary high-resolution features as inputs, but uses less convolution operations.","The other low-resolution branch first performs down-sampling and then utilizes more convolution operations over such low-resolution features.","Experiments on both recognition task (ImageNet-1K dataset) and dense prediction tasks (COCO and ADE20K datasets) demonstrate the superiority of HIRI-ViT. More remarkably, under comparable computational cost ($\\sim$5.0 GFLOPs), HIRI-ViT achieves to-date the best published Top-1 accuracy of 84.3% on ImageNet with 448$\\times$448 inputs, which absolutely improves 83.4% of iFormer-S by 0.9% with 224$\\times$224 inputs."],"url":"http://arxiv.org/abs/2403.11999v1","category":"cs.CV"}
{"created":"2024-03-18 17:30:27","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","abstract":"Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII painting, where the resulting composite reflects a balance of chaos and order, with features like adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across physical, biological, and artistic spheres, revealing a nuanced ontology of immanence and material flux that resonates with postmodern philosophy, and positions these interconnections within a heterarchical framework. Our findings reveal the dynamic, context-dependent interplay of entities beyond traditional hierarchical paradigms, emphasizing the significant role of individual components and their fluctuative relationships within the system. Our predictions achieve a far higher degree of novelty, technical detail and explorative capacity than conventional generative AI methods. The approach establishes a widely useful framework for innovation by revealing hidden connections that facilitate discovery.","sentences":["Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature.","Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors.","One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping.","The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII painting, where the resulting composite reflects a balance of chaos and order, with features like adjustable porosity, mechanical strength, and complex patterned chemical functionalization.","We uncover other isomorphisms across physical, biological, and artistic spheres, revealing a nuanced ontology of immanence and material flux that resonates with postmodern philosophy, and positions these interconnections within a heterarchical framework.","Our findings reveal the dynamic, context-dependent interplay of entities beyond traditional hierarchical paradigms, emphasizing the significant role of individual components and their fluctuative relationships within the system.","Our predictions achieve a far higher degree of novelty, technical detail and explorative capacity than conventional generative AI methods.","The approach establishes a widely useful framework for innovation by revealing hidden connections that facilitate discovery."],"url":"http://arxiv.org/abs/2403.11996v1","category":"cs.LG"}
{"created":"2024-03-18 17:21:35","title":"Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching","abstract":"Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to identify the themes they express. More generally, this work illustrates how to use the combination of NLP techniques and LLMs to generate a codebook for SETs. We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings.","sentences":["Feedback is a critical aspect of improvement.","Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights.","Consider student evaluations of teaching (SETs), which are important sources of feedback for educators.","They can give instructors insights into what worked during a semester.","A collection of SETs can also be useful to administrators as signals for courses or entire programs.","However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze.","In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs).","We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university.","We show that the method can be used to extract, embed, cluster, and summarize the SETs to identify the themes they express.","More generally, this work illustrates how to use the combination of NLP techniques and LLMs to generate a codebook for SETs.","We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings."],"url":"http://arxiv.org/abs/2403.11984v1","category":"cs.CL"}
{"created":"2024-03-18 17:08:56","title":"Mechanisms enabling efficient fountain-flow supracellular migration","abstract":"In a prototypical mode of single-cell migration, retrograde cytoskeletal flow is mechanically coupled to the environment, propels the cell, and is sustained by an anterograde cytosolic flow of disassembled cytoskeletal components. Supracellular collectives also develop fountain-flows to migrate, but the opposing cellular streams interact with the environment producing conflicting forces. To understand the biophysical constraints of fountain-flow supracellular migration, we develop an active gel model of a cell cluster driven by a polarized peripheral contractile cable. While the model develops fountain-flows and directed migration, efficiency and cluster velocity are extremely small compared to observations. We find that patterned friction or cluster-polarized single-cell directed migration, both suggested by contact inhibition of locomotion, rescue robust and efficient supracellular migration.","sentences":["In a prototypical mode of single-cell migration, retrograde cytoskeletal flow is mechanically coupled to the environment, propels the cell, and is sustained by an anterograde cytosolic flow of disassembled cytoskeletal components.","Supracellular collectives also develop fountain-flows to migrate, but the opposing cellular streams interact with the environment producing conflicting forces.","To understand the biophysical constraints of fountain-flow supracellular migration, we develop an active gel model of a cell cluster driven by a polarized peripheral contractile cable.","While the model develops fountain-flows and directed migration, efficiency and cluster velocity are extremely small compared to observations.","We find that patterned friction or cluster-polarized single-cell directed migration, both suggested by contact inhibition of locomotion, rescue robust and efficient supracellular migration."],"url":"http://arxiv.org/abs/2403.11969v1","category":"physics.bio-ph"}
{"created":"2024-03-18 17:05:24","title":"Informed Spectral Normalized Gaussian Processes for Trajectory Prediction","abstract":"Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction problem in autonomous driving by integrating prior drivability knowledge. On two public datasets, we investigate its performance under diminishing training data and across locations, and thereby demonstrate an increase in data-efficiency and robustness to location-transfers over non-informed and informed baselines.","sentences":["Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning.","Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency.","However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times.","Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs).","We propose a novel regularization-based continual learning method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks.","Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion.","We apply our informed SNGP model to the trajectory prediction problem in autonomous driving by integrating prior drivability knowledge.","On two public datasets, we investigate its performance under diminishing training data and across locations, and thereby demonstrate an increase in data-efficiency and robustness to location-transfers over non-informed and informed baselines."],"url":"http://arxiv.org/abs/2403.11966v1","category":"cs.LG"}
{"created":"2024-03-18 17:04:33","title":"Probabilistic Calibration by Design for Neural Network Regression","abstract":"Generating calibrated and sharp neural network predictive distributions for regression problems is essential for optimal decision-making in many real-world applications. To address the miscalibration issue of neural networks, various methods have been proposed to improve calibration, including post-hoc methods that adjust predictions after training and regularization methods that act during training. While post-hoc methods have shown better improvement in calibration compared to regularization methods, the post-hoc step is completely independent of model training. We introduce a novel end-to-end model training procedure called Quantile Recalibration Training, integrating post-hoc calibration directly into the training process without additional parameters. We also present a unified algorithm that includes our method and other post-hoc and regularization methods, as particular cases. We demonstrate the performance of our method in a large-scale experiment involving 57 tabular regression datasets, showcasing improved predictive accuracy while maintaining calibration. We also conduct an ablation study to evaluate the significance of different components within our proposed method, as well as an in-depth analysis of the impact of the base model and different hyperparameters on predictive accuracy.","sentences":["Generating calibrated and sharp neural network predictive distributions for regression problems is essential for optimal decision-making in many real-world applications.","To address the miscalibration issue of neural networks, various methods have been proposed to improve calibration, including post-hoc methods that adjust predictions after training and regularization methods that act during training.","While post-hoc methods have shown better improvement in calibration compared to regularization methods, the post-hoc step is completely independent of model training.","We introduce a novel end-to-end model training procedure called Quantile Recalibration Training, integrating post-hoc calibration directly into the training process without additional parameters.","We also present a unified algorithm that includes our method and other post-hoc and regularization methods, as particular cases.","We demonstrate the performance of our method in a large-scale experiment involving 57 tabular regression datasets, showcasing improved predictive accuracy while maintaining calibration.","We also conduct an ablation study to evaluate the significance of different components within our proposed method, as well as an in-depth analysis of the impact of the base model and different hyperparameters on predictive accuracy."],"url":"http://arxiv.org/abs/2403.11964v1","category":"cs.LG"}
{"created":"2024-03-18 17:02:41","title":"Transfer Learning Beyond Bounded Density Ratios","abstract":"We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.   In this work, we focus on transfer learning over the class of low-degree polynomial estimators. Our main result is a general transfer inequality over the domain $\\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measure and the inverse ratio $dP/dQ$ is bounded. To demonstrate the applicability of our inequality, we obtain new results in the settings of: (1) the classical truncated regression setting, where $dQ/dP$ equals infinity, and (2) the more recent out-of-distribution generalization setting for in-context learning linear functions with transformers. We also provide a discrete analogue of our transfer inequality on the Boolean Hypercube $\\{-1,1\\}^n$, and study its connections with the recent problem of Generalization on the Unseen of Abbe, Bengio, Lotfi and Rizk (ICML, 2023). Our main conceptual contribution is that the maximum influence of the error of the estimator $\\widehat{f}-f^*$ under $Q$, $\\mathrm{I}_{\\max}(\\widehat{f}-f^*)$, acts as a sufficient condition for transferability; when $\\mathrm{I}_{\\max}(\\widehat{f}-f^*)$ is appropriately bounded, transfer is possible over the Boolean domain.","sentences":["We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that transfer learning happens when the density ratio $dQ/dP$ is bounded.","Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but transfer learning is possible.   ","In this work, we focus on transfer learning over the class of low-degree polynomial estimators.","Our main result is a general transfer inequality over the domain $\\mathbb{R}^n$, proving that non-trivial transfer learning for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded.","For instance, it always applies if $Q$ is a log-concave measure and the inverse ratio $dP/dQ$ is bounded.","To demonstrate the applicability of our inequality, we obtain new results in the settings of: (1) the classical truncated regression setting, where $dQ/dP$ equals infinity, and (2) the more recent out-of-distribution generalization setting for in-context learning linear functions with transformers.","We also provide a discrete analogue of our transfer inequality on the Boolean Hypercube $\\{-1,1\\}^n$, and study its connections with the recent problem of Generalization on the Unseen of Abbe, Bengio, Lotfi and Rizk (ICML, 2023).","Our main conceptual contribution is that the maximum influence of the error of the estimator $\\widehat{f}-f^*$ under $Q$, $\\mathrm{I}_{\\max}(\\widehat{f}-f^*)$, acts as a sufficient condition for transferability; when $\\mathrm{I}_{\\max}(\\widehat{f}-f^*)$ is appropriately bounded, transfer is possible over the Boolean domain."],"url":"http://arxiv.org/abs/2403.11963v1","category":"cs.LG"}
{"created":"2024-03-18 16:58:23","title":"Enhanced Event-Based Video Reconstruction with Motion Compensation","abstract":"Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands. A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture. However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion. To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality. A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation. The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation. We also introduce an iterative training framework for this combined system. Results demonstrate that our approach achieves state-of-the-art reconstruction accuracy and simultaneously provides reliable dense flow estimation. Furthermore, our model exhibits flexibility in that it can integrate different flow networks, suggesting its potential for further performance enhancement.","sentences":["Deep neural networks for event-based video reconstruction often suffer from a lack of interpretability and have high memory demands.","A lightweight network called CISTA-LSTC has recently been introduced showing that high-quality reconstruction can be achieved through the systematic design of its architecture.","However, its modelling assumption that input signals and output reconstructed frame share the same sparse representation neglects the displacement caused by motion.","To address this, we propose warping the input intensity frames and sparse codes to enhance reconstruction quality.","A CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC for motion compensation.","The system relies solely on events, in which predicted flow aids in reconstruction and then reconstructed frames are used to facilitate flow estimation.","We also introduce an iterative training framework for this combined system.","Results demonstrate that our approach achieves state-of-the-art reconstruction accuracy and simultaneously provides reliable dense flow estimation.","Furthermore, our model exhibits flexibility in that it can integrate different flow networks, suggesting its potential for further performance enhancement."],"url":"http://arxiv.org/abs/2403.11961v1","category":"cs.CV"}
{"created":"2024-03-18 16:57:16","title":"CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation","abstract":"Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could result in overfitting and make the model vulnerable to noises. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective, which shows the causal relationships among the input, output, embeddings and confounders. Next, we show how to block the confounders via the frontdoor adjustment. Based on the results of the frontdoor adjustment, we introduce a novel Causality-Aware SPatiotEmpoRal graph neural network (CASPER), which contains a novel Spatiotemporal Causal Attention (SCA) and a Prompt Based Decoder (PBD). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper outperforms the baselines and effectively discovers causal relationships.","sentences":["Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations.","The collected data usually contains missing values due to various failures, which have significant impact on data analysis.","To impute the missing values, a lot of methods have been introduced.","When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship.","During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network.","These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output.","Over-exploiting these non-causal correlations could result in overfitting and make the model vulnerable to noises.","In this paper, we first revisit spatiotemporal time series imputation from a causal perspective, which shows the causal relationships among the input, output, embeddings and confounders.","Next, we show how to block the confounders via the frontdoor adjustment.","Based on the results of the frontdoor adjustment, we introduce a novel Causality-Aware SPatiotEmpoRal graph neural network (CASPER), which contains a novel Spatiotemporal Causal Attention (SCA) and a Prompt Based Decoder (PBD).","PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings.","Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients.","We evaluate Casper on three real-world datasets, and the experimental results show that Casper outperforms the baselines and effectively discovers causal relationships."],"url":"http://arxiv.org/abs/2403.11960v1","category":"cs.LG"}
{"created":"2024-03-18 16:56:47","title":"IVAC-P2L: Enhancing Video Action Counting through Irregular Repetition Priors","abstract":"Video Action Counting (VAC) is crucial in analyzing sports, fitness, and everyday activities by quantifying repetitive actions in videos. However, traditional VAC methods have overlooked the complexity of action repetitions, such as interruptions and the variability in cycle duration. Our research addresses the shortfall by introducing a novel approach to VAC, called Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular repetition patterns in videos, which we define through two primary aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures homogeneity in the spatial-temporal representations of cycle segments, signifying action uniformity within cycles. Cycle-interval inconsistency highlights the importance of distinguishing between cycle segments and intervals based on their inherent content differences. To encapsulate these principles, we propose a new methodology that includes consistency and inconsistency modules, supported by a unique pull-push loss (P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence among cycle segment features and a push loss to clearly distinguish features of cycle segments from interval segments. Empirical evaluations conducted on the RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in VAC task performance. Furthermore, the model demonstrates exceptional adaptability and generalization across various video contents, outperforming existing models on two additional datasets, UCFRep and Countix, without the need for dataset-specific optimization. These results confirm the efficacy of our approach in addressing irregular repetitions in videos and pave the way for further advancements in video analysis and understanding.","sentences":["Video Action Counting (VAC) is crucial in analyzing sports, fitness, and everyday activities by quantifying repetitive actions in videos.","However, traditional VAC methods have overlooked the complexity of action repetitions, such as interruptions and the variability in cycle duration.","Our research addresses the shortfall by introducing a novel approach to VAC, called Irregular Video Action Counting (IVAC).","IVAC prioritizes modeling irregular repetition patterns in videos, which we define through two primary aspects: Inter-cycle Consistency and Cycle-interval Inconsistency.","Inter-cycle Consistency ensures homogeneity in the spatial-temporal representations of cycle segments, signifying action uniformity within cycles.","Cycle-interval inconsistency highlights the importance of distinguishing between cycle segments and intervals based on their inherent content differences.","To encapsulate these principles, we propose a new methodology that includes consistency and inconsistency modules, supported by a unique pull-push loss (P2L) mechanism.","The IVAC-P2L model applies a pull loss to promote coherence among cycle segment features and a push loss to clearly distinguish features of cycle segments from interval segments.","Empirical evaluations conducted on the RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in VAC task performance.","Furthermore, the model demonstrates exceptional adaptability and generalization across various video contents, outperforming existing models on two additional datasets, UCFRep and Countix, without the need for dataset-specific optimization.","These results confirm the efficacy of our approach in addressing irregular repetitions in videos and pave the way for further advancements in video analysis and understanding."],"url":"http://arxiv.org/abs/2403.11959v1","category":"cs.CV"}
{"created":"2024-03-18 16:52:49","title":"Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment","abstract":"With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video's corresponding mean opinion score. Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language model to give the prediction score. Experimental results show that T2VQA outperforms existing T2V metrics and SOTA video quality assessment models. Quantitative analysis indicates that T2VQA is capable of giving subjective-align predictions, validating its effectiveness. The dataset and code will be released at https://github.com/QMME/T2VQA.","sentences":["With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives.","Among them, Text-to-Video (T2V) generation has received widespread attention.","Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively.","To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date.","The dataset is composed of 10,000 videos generated by 9 different T2V models.","We also conduct a subjective study to obtain each video's corresponding mean opinion score.","Based on T2VQA-DB, we propose a novel transformer-based model for subjective-aligned Text-to-Video Quality Assessment (T2VQA).","The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a large language model to give the prediction score.","Experimental results show that T2VQA outperforms existing T2V metrics and SOTA video quality assessment models.","Quantitative analysis indicates that T2VQA is capable of giving subjective-align predictions, validating its effectiveness.","The dataset and code will be released at https://github.com/QMME/T2VQA."],"url":"http://arxiv.org/abs/2403.11956v2","category":"cs.CV"}
{"created":"2024-03-18 16:36:54","title":"Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling","abstract":"Facial Expression Recognition (FER) plays a crucial role in computer vision and finds extensive applications across various fields. This paper aims to present our approach for the upcoming 6th Affective Behavior Analysis in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial expression recognition task, The limited size of the FER dataset poses a challenge to the expression recognition model's generalization ability, resulting in subpar recognition performance. To address this problem, we employ a semi-supervised learning technique to generate expression category pseudo-labels for unlabeled face data. At the same time, we uniformly sampled the labeled facial expression samples and implemented a debiased feedback learning strategy to address the problem of category imbalance in the dataset and the possible data bias in semi-supervised learning. Moreover, to further compensate for the limitation and bias of features obtained only from static images, we introduced a Temporal Encoder to learn and capture temporal relationships between neighbouring expression image features. In the 6th ABAW competition, our method achieved outstanding results on the official validation set, a result that fully confirms the effectiveness and competitiveness of our proposed method.","sentences":["Facial Expression Recognition (FER) plays a crucial role in computer vision and finds extensive applications across various fields.","This paper aims to present our approach for the upcoming 6th Affective Behavior Analysis in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024.","In the facial expression recognition task, The limited size of the FER dataset poses a challenge to the expression recognition model's generalization ability, resulting in subpar recognition performance.","To address this problem, we employ a semi-supervised learning technique to generate expression category pseudo-labels for unlabeled face data.","At the same time, we uniformly sampled the labeled facial expression samples and implemented a debiased feedback learning strategy to address the problem of category imbalance in the dataset and the possible data bias in semi-supervised learning.","Moreover, to further compensate for the limitation and bias of features obtained only from static images, we introduced a Temporal Encoder to learn and capture temporal relationships between neighbouring expression image features.","In the 6th ABAW competition, our method achieved outstanding results on the official validation set, a result that fully confirms the effectiveness and competitiveness of our proposed method."],"url":"http://arxiv.org/abs/2403.11942v2","category":"cs.CV"}
{"created":"2024-03-18 16:34:38","title":"AI-Assisted Cervical Cancer Screening","abstract":"Visual Inspection with Acetic Acid (VIA) remains the most feasible cervical cancer screening test in resource-constrained settings of low- and middle-income countries (LMICs), which are often performed screening camps or primary/community health centers by nurses instead of the preferred but unavailable expert Gynecologist. To address the highly subjective nature of the test, various handheld devices integrating cameras or smartphones have been recently explored to capture cervical images during VIA and aid decision-making via telemedicine or AI models. Most studies proposing AI models retrospectively use a relatively small number of already collected images from specific devices, digital cameras, or smartphones; the challenges and protocol for quality image acquisition during VIA in resource-constrained camp settings, challenges in getting gold standard, data imbalance, etc. are often overlooked. We present a novel approach and describe the end-to-end design process to build a robust smartphone-based AI-assisted system that does not require buying a separate integrated device: the proposed protocol for quality image acquisition in resource-constrained settings, dataset collected from 1,430 women during VIA performed by nurses in screening camps, preprocessing pipeline, and training and evaluation of a deep-learning-based classification model aimed to identify (pre)cancerous lesions. Our work shows that the readily available smartphones and a suitable protocol can capture the cervix images with the required details for the VIA test well; the deep-learning-based classification model provides promising results to assist nurses in VIA screening; and provides a direction for large-scale data collection and validation in resource-constrained settings.","sentences":["Visual Inspection with Acetic Acid (VIA) remains the most feasible cervical cancer screening test in resource-constrained settings of low- and middle-income countries (LMICs), which are often performed screening camps or primary/community health centers by nurses instead of the preferred but unavailable expert Gynecologist.","To address the highly subjective nature of the test, various handheld devices integrating cameras or smartphones have been recently explored to capture cervical images during VIA and aid decision-making via telemedicine or AI models.","Most studies proposing AI models retrospectively use a relatively small number of already collected images from specific devices, digital cameras, or smartphones; the challenges and protocol for quality image acquisition during VIA in resource-constrained camp settings, challenges in getting gold standard, data imbalance, etc. are often overlooked.","We present a novel approach and describe the end-to-end design process to build a robust smartphone-based AI-assisted system that does not require buying a separate integrated device: the proposed protocol for quality image acquisition in resource-constrained settings, dataset collected from 1,430 women during VIA performed by nurses in screening camps, preprocessing pipeline, and training and evaluation of a deep-learning-based classification model aimed to identify (pre)cancerous lesions.","Our work shows that the readily available smartphones and a suitable protocol can capture the cervix images with the required details for the VIA test well; the deep-learning-based classification model provides promising results to assist nurses in VIA screening; and provides a direction for large-scale data collection and validation in resource-constrained settings."],"url":"http://arxiv.org/abs/2403.11936v1","category":"eess.IV"}
{"created":"2024-03-18 16:33:43","title":"HyperColorization: Propagating spatially sparse noisy spectral clues for reconstructing hyperspectral images","abstract":"Hyperspectral cameras face challenging spatial-spectral resolution trade-offs and are more affected by shot noise than RGB photos taken over the same total exposure time. Here, we present a colorization algorithm to reconstruct hyperspectral images from a grayscale guide image and spatially sparse spectral clues. We demonstrate that our algorithm generalizes to varying spectral dimensions for hyperspectral images, and show that colorizing in a low-rank space reduces compute time and the impact of shot noise. To enhance robustness, we incorporate guided sampling, edge-aware filtering, and dimensionality estimation techniques. Our method surpasses previous algorithms in various performance metrics, including SSIM, PSNR, GFC, and EMD, which we analyze as metrics for characterizing hyperspectral image quality. Collectively, these findings provide a promising avenue for overcoming the time-space-wavelength resolution trade-off by reconstructing a dense hyperspectral image from samples obtained by whisk or push broom scanners, as well as hybrid spatial-spectral computational imaging systems.","sentences":["Hyperspectral cameras face challenging spatial-spectral resolution trade-offs and are more affected by shot noise than RGB photos taken over the same total exposure time.","Here, we present a colorization algorithm to reconstruct hyperspectral images from a grayscale guide image and spatially sparse spectral clues.","We demonstrate that our algorithm generalizes to varying spectral dimensions for hyperspectral images, and show that colorizing in a low-rank space reduces compute time and the impact of shot noise.","To enhance robustness, we incorporate guided sampling, edge-aware filtering, and dimensionality estimation techniques.","Our method surpasses previous algorithms in various performance metrics, including SSIM, PSNR, GFC, and EMD, which we analyze as metrics for characterizing hyperspectral image quality.","Collectively, these findings provide a promising avenue for overcoming the time-space-wavelength resolution trade-off by reconstructing a dense hyperspectral image from samples obtained by whisk or push broom scanners, as well as hybrid spatial-spectral computational imaging systems."],"url":"http://arxiv.org/abs/2403.11935v1","category":"cs.CV"}
{"created":"2024-03-18 16:33:29","title":"High-energy physics image classification: A Survey of Jet Applications","abstract":"In recent times, the fields of high-energy physics (HEP) experimentation and phenomenological studies have seen the integration of machine learning (ML) and its specialized branch, deep learning (DL). This survey offers a comprehensive assessment of these applications within the realm of various DL approaches. The initial segment of the paper introduces the fundamentals encompassing diverse particle physics types and establishes criteria for evaluating particle physics in tandem with learning models. Following this, a comprehensive taxonomy is presented for representing HEP images, encompassing accessible datasets, intricate details of preprocessing techniques, and methods of feature extraction and selection. Subsequently, the focus shifts to an exploration of available artificial intelligence (AI) models tailored to HEP images, along with a concentrated examination of HEP image classification pertaining to Jet particles. Within this review, a profound investigation is undertaken into distinct ML and DL proposed state-of-the art (SOTA) techniques, underscoring their implications for HEP inquiries. The discussion delves into specific applications in substantial detail, including Jet tagging, Jet tracking, particle classification, and more. The survey culminates with an analysis concerning the present status of HEP grounded in DL methodologies, encompassing inherent challenges and prospective avenues for future research endeavors.","sentences":["In recent times, the fields of high-energy physics (HEP) experimentation and phenomenological studies have seen the integration of machine learning (ML) and its specialized branch, deep learning (DL).","This survey offers a comprehensive assessment of these applications within the realm of various DL approaches.","The initial segment of the paper introduces the fundamentals encompassing diverse particle physics types and establishes criteria for evaluating particle physics in tandem with learning models.","Following this, a comprehensive taxonomy is presented for representing HEP images, encompassing accessible datasets, intricate details of preprocessing techniques, and methods of feature extraction and selection.","Subsequently, the focus shifts to an exploration of available artificial intelligence (AI) models tailored to HEP images, along with a concentrated examination of HEP image classification pertaining to Jet particles.","Within this review, a profound investigation is undertaken into distinct ML and DL proposed state-of-the art (SOTA) techniques, underscoring their implications for HEP inquiries.","The discussion delves into specific applications in substantial detail, including Jet tagging, Jet tracking, particle classification, and more.","The survey culminates with an analysis concerning the present status of HEP grounded in DL methodologies, encompassing inherent challenges and prospective avenues for future research endeavors."],"url":"http://arxiv.org/abs/2403.11934v1","category":"hep-ph"}
{"created":"2024-03-18 16:32:16","title":"Recovering the activity parameters of an active fluid confined in a sphere","abstract":"The properties of an active fluid, for example, a bacterial bath or a collection of microtubules and molecular motors, can be accessed through the dynamics of passive particle probes. Here, in the perspective of analyzing experimental situations of confinement in droplets, we consider the kinematics of a negatively buoyant probe particle in an active fluid, both confined within a spherical domain. The active bath generates a fluctuating flow that pushes the particle with a velocity that is modeled as a colored stochastic noise, characterized by two parameters, the intensity and memory time of the active flow. When the particle departs a little from the bottom of the spherical domain, the configuration is well approximated by a particle in a two-dimensional harmonic trap subjected to the colored noise, in which case an analytical solution exists, which is the base for quantitative analysis. We numerically simulate the dynamics of the particle and use the planar, two-dimensional mean square displacement to recover the activity parameters of the bath. This approach yields satisfactory results as long as the particle remains relatively confined, that is, as long as the intensity of the colored noise remains low.","sentences":["The properties of an active fluid, for example, a bacterial bath or a collection of microtubules and molecular motors, can be accessed through the dynamics of passive particle probes.","Here, in the perspective of analyzing experimental situations of confinement in droplets, we consider the kinematics of a negatively buoyant probe particle in an active fluid, both confined within a spherical domain.","The active bath generates a fluctuating flow that pushes the particle with a velocity that is modeled as a colored stochastic noise, characterized by two parameters, the intensity and memory time of the active flow.","When the particle departs a little from the bottom of the spherical domain, the configuration is well approximated by a particle in a two-dimensional harmonic trap subjected to the colored noise, in which case an analytical solution exists, which is the base for quantitative analysis.","We numerically simulate the dynamics of the particle and use the planar, two-dimensional mean square displacement to recover the activity parameters of the bath.","This approach yields satisfactory results as long as the particle remains relatively confined, that is, as long as the intensity of the colored noise remains low."],"url":"http://arxiv.org/abs/2403.11933v1","category":"cond-mat.soft"}
{"created":"2024-03-18 16:19:30","title":"Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration and Data-driven Analysis--Full Version","abstract":"In Bangladesh, agriculture is a crucial driver for addressing Sustainable Development Goal 1 (No Poverty) and 2 (Zero Hunger), playing a fundamental role in the economy and people's livelihoods. To enhance the sustainability and resilience of the agriculture industry through data-driven insights, the Bangladesh Bureau of Statistics and other organizations consistently collect and publish agricultural data on the Web. Nevertheless, the current datasets encounter various challenges: 1) they are presented in an unsustainable, static, read-only, and aggregated format, 2) they do not conform to the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles, and 3) they do not facilitate interactive analysis and integration with other data sources. In this paper, we present a thorough solution, delineating a systematic procedure for developing BDAKG: a knowledge graph that semantically and analytically integrates agriculture data in Bangladesh. BDAKG incorporates multidimensional semantics, is linked with external knowledge graphs, is compatible with OLAP, and adheres to the FAIR principles. Our experimental evaluation centers on evaluating the integration process and assessing the quality of the resultant knowledge graph in terms of completeness, timeliness, FAIRness, OLAP compatibility and data-driven analysis. Our federated data analysis recommend a strategic approach focused on decreasing CO$_2$ emissions, fostering economic growth, and promoting sustainable forestry.","sentences":["In Bangladesh, agriculture is a crucial driver for addressing Sustainable Development Goal 1 (No Poverty) and 2 (Zero Hunger), playing a fundamental role in the economy and people's livelihoods.","To enhance the sustainability and resilience of the agriculture industry through data-driven insights, the Bangladesh Bureau of Statistics and other organizations consistently collect and publish agricultural data on the Web.","Nevertheless, the current datasets encounter various challenges: 1) they are presented in an unsustainable, static, read-only, and aggregated format, 2) they do not conform to the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles, and 3) they do not facilitate interactive analysis and integration with other data sources.","In this paper, we present a thorough solution, delineating a systematic procedure for developing BDAKG: a knowledge graph that semantically and analytically integrates agriculture data in Bangladesh.","BDAKG incorporates multidimensional semantics, is linked with external knowledge graphs, is compatible with OLAP, and adheres to the FAIR principles.","Our experimental evaluation centers on evaluating the integration process and assessing the quality of the resultant knowledge graph in terms of completeness, timeliness, FAIRness, OLAP compatibility and data-driven analysis.","Our federated data analysis recommend a strategic approach focused on decreasing CO$_2$ emissions, fostering economic growth, and promoting sustainable forestry."],"url":"http://arxiv.org/abs/2403.11920v2","category":"cs.CY"}
{"created":"2024-03-18 16:19:15","title":"A Coq Mechanization of JavaScript Regular Expression Semantics","abstract":"We present an executable, proven-safe, faithful, and future-proof Coq mechanization of JavaScript regular expression (regex) matching, as specified by the last published edition of ECMA-262 section 22.2. This is, to our knowledge, the first time that an industrial-strength regex language has been faithfully mechanized in an interactive theorem prover. We highlight interesting challenges that arose in the process (including issues of encoding, corner cases, and executability), and we document the steps that we took to ensure that the result is straightforwardly auditable and that our understanding of the spec aligns with existing implementations.   We demonstrate the usability and versatility of the mechanization through a broad collection of analyses, case studies, and experiments: we prove that JavaScript regex matching always terminates and is safe (no assertion failures); we identifying subtle corner cases that led to mistakes in previous publications; we verify an optimization extracted from a state-of-the-art regex engine; we show that some classic properties described in automata textbooks and used in derivatives-based matchers do not hold in JavaScript regexes; and we demonstrate that the cost of updating the mechanization to account for changes in the original specification is reasonably low.   Our mechanization can be extracted to OCaml and linked with Unicode libraries to produce an executable engine that passes the relevant parts of the official Test262 conformance test suite.","sentences":["We present an executable, proven-safe, faithful, and future-proof Coq mechanization of JavaScript regular expression (regex) matching, as specified by the last published edition of ECMA-262 section 22.2.","This is, to our knowledge, the first time that an industrial-strength regex language has been faithfully mechanized in an interactive theorem prover.","We highlight interesting challenges that arose in the process (including issues of encoding, corner cases, and executability), and we document the steps that we took to ensure that the result is straightforwardly auditable and that our understanding of the spec aligns with existing implementations.   ","We demonstrate the usability and versatility of the mechanization through a broad collection of analyses, case studies, and experiments: we prove that JavaScript regex matching always terminates and is safe (no assertion failures); we identifying subtle corner cases that led to mistakes in previous publications; we verify an optimization extracted from a state-of-the-art regex engine; we show that some classic properties described in automata textbooks and used in derivatives-based matchers do not hold in JavaScript regexes; and we demonstrate that the cost of updating the mechanization to account for changes in the original specification is reasonably low.   ","Our mechanization can be extracted to OCaml and linked with Unicode libraries to produce an executable engine that passes the relevant parts of the official Test262 conformance test suite."],"url":"http://arxiv.org/abs/2403.11919v1","category":"cs.PL"}
{"created":"2024-03-18 16:12:32","title":"A $C^\\ast$-algebraic view on the interaction of real- and reciprocal space topology in skyrmion crystals","abstract":"Understanding the interaction of real- and reciprocal space topology in skyrmion crystals is an open problem. We approach it from the viewpoint of $C^\\ast$-algebras and calculate all admissible Chern numbers of a strongly coupled tight-binding skyrmion system on a triangular lattice as a function of Fermi energy and texture parameters. Our analysis reveals the topological complexity of electronic states coupled to spin textures, and the failure of the adiabatic picture to account for it in terms of emergent electromagnetism. On the contrary, we explain the discontinuous jumps in the real-space winding number in terms of collective evolution in real-, reciprocal, and mixed space Chern numbers. Our work sets the stage for further research on topological dynamics in complex dynamic spin textures coupled to external fields.","sentences":["Understanding the interaction of real- and reciprocal space topology in skyrmion crystals is an open problem.","We approach it from the viewpoint of $C^\\ast$-algebras and calculate all admissible Chern numbers of a strongly coupled tight-binding skyrmion system on a triangular lattice as a function of Fermi energy and texture parameters.","Our analysis reveals the topological complexity of electronic states coupled to spin textures, and the failure of the adiabatic picture to account for it in terms of emergent electromagnetism.","On the contrary, we explain the discontinuous jumps in the real-space winding number in terms of collective evolution in real-, reciprocal, and mixed space Chern numbers.","Our work sets the stage for further research on topological dynamics in complex dynamic spin textures coupled to external fields."],"url":"http://arxiv.org/abs/2403.11912v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 16:06:30","title":"Tur[k]ingBench: A Challenge Benchmark for Web Agents","abstract":"Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instances distributed across 158 tasks.   Additionally, to facilitate the evaluation on TurkingBench, we develop an evaluation framework that connects the responses of chatbots to modifications on web pages (modifying a text box, checking a radio, etc.). We evaluate the performance of state-of-the-art models, including language-only, vision-only, and layout-only models, and their combinations, on this benchmark. Our findings reveal that these models perform significantly better than random chance, yet considerable room exists for improvement. We hope this benchmark will help facilitate the evaluation and development of web-based agents.","sentences":["Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form.","However, there is more to the world than raw text.","For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions.","Can state-of-the-art multi-modal models generalize to such complex domains?   ","To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context.","Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes.","The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task.","This benchmark contains 32.2K instances distributed across 158 tasks.   ","Additionally, to facilitate the evaluation on TurkingBench, we develop an evaluation framework that connects the responses of chatbots to modifications on web pages (modifying a text box, checking a radio, etc.).","We evaluate the performance of state-of-the-art models, including language-only, vision-only, and layout-only models, and their combinations, on this benchmark.","Our findings reveal that these models perform significantly better than random chance, yet considerable room exists for improvement.","We hope this benchmark will help facilitate the evaluation and development of web-based agents."],"url":"http://arxiv.org/abs/2403.11905v1","category":"cs.AI"}
{"created":"2024-03-18 16:01:42","title":"Larimar: Large Language Models with Episodic Memory Control","abstract":"Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.","sentences":["Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today.","This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory.","Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning.","Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general.","We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness."],"url":"http://arxiv.org/abs/2403.11901v1","category":"cs.LG"}
{"created":"2024-03-18 15:53:33","title":"From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?","abstract":"Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term \"XIAI\" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore \"global\" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Important opportunities were raised such as using \"attention\" to enhance multi-modal XIAI for personalized medicine and combine DL with causal reasoning. Our discussion encourages the integration of XIAI in LLMs and domain-specific smaller models. Our review can stimulate further research and benchmarks toward improving inherent IAI and engaging complex NLP in healthcare.","sentences":["Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks.","Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making.","This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP.","The term \"XIAI\" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI.","Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global).","Our analysis shows that attention mechanisms were the most dominant emerging IAI.","Moreover, IAI is increasingly used against XAI.","The major challenges identified are that most XIAI do not explore \"global\" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks.","Important opportunities were raised such as using \"attention\" to enhance multi-modal XIAI for personalized medicine and combine DL with causal reasoning.","Our discussion encourages the integration of XIAI in LLMs and domain-specific smaller models.","Our review can stimulate further research and benchmarks toward improving inherent IAI and engaging complex NLP in healthcare."],"url":"http://arxiv.org/abs/2403.11894v1","category":"cs.CL"}
{"created":"2024-03-18 15:48:24","title":"N-Modal Contrastive Losses with Applications to Social Media Data in Trimodal Space","abstract":"The social media landscape of conflict dynamics has grown increasingly multi-modal. Recent advancements in model architectures such as CLIP have enabled researchers to begin studying the interplay between the modalities of text and images in a shared latent space. However, CLIP models fail to handle situations on social media when modalities present in a post expand above two. Social media dynamics often require understanding the interplay between not only text and images, but video as well. In this paper we explore an extension of the contrastive loss function to allow for any number of modalities, and demonstrate its usefulness in trimodal spaces on social media. By extending CLIP into three dimensions we can further aide understanding social media landscapes where all three modalities are present (an increasingly common situation). We use a newly collected public data set of Telegram posts containing all three modalities to train, and then demonstrate the usefulness of, a trimodal model in two OSINT scenarios: classifying a social media artifact post as either pro-Russian or pro-Ukrainian and identifying which account a given artifact originated from. While trimodal CLIP models have been explored before (though not on social media data), we also display a novel quadmodal CLIP model. This model can learn the interplay between text, image, video, and audio. We demonstrate new state-of-the-art baseline results on retrieval for quadmodel models moving forward.","sentences":["The social media landscape of conflict dynamics has grown increasingly multi-modal.","Recent advancements in model architectures such as CLIP have enabled researchers to begin studying the interplay between the modalities of text and images in a shared latent space.","However, CLIP models fail to handle situations on social media when modalities present in a post expand above two.","Social media dynamics often require understanding the interplay between not only text and images, but video as well.","In this paper we explore an extension of the contrastive loss function to allow for any number of modalities, and demonstrate its usefulness in trimodal spaces on social media.","By extending CLIP into three dimensions we can further aide understanding social media landscapes where all three modalities are present (an increasingly common situation).","We use a newly collected public data set of Telegram posts containing all three modalities to train, and then demonstrate the usefulness of, a trimodal model in two OSINT scenarios: classifying a social media artifact post as either pro-Russian or pro-Ukrainian and identifying which account a given artifact originated from.","While trimodal CLIP models have been explored before (though not on social media data), we also display a novel quadmodal CLIP model.","This model can learn the interplay between text, image, video, and audio.","We demonstrate new state-of-the-art baseline results on retrieval for quadmodel models moving forward."],"url":"http://arxiv.org/abs/2403.12747v1","category":"cs.CV"}
{"created":"2024-03-18 15:40:36","title":"SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules","abstract":"Low-rank adaptation (LoRA) and its variants are widely employed in fine-tuning large models, including large language models for natural language processing and diffusion models for computer vision. This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings. Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for transfer learning tasks especially in the extremely few-parameter regimes.","sentences":["Low-rank adaptation (LoRA) and its variants are widely employed in fine-tuning large models, including large language models for natural language processing and diffusion models for computer vision.","This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings.","Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for transfer learning tasks especially in the extremely few-parameter regimes."],"url":"http://arxiv.org/abs/2403.11887v1","category":"cs.CV"}
{"created":"2024-03-18 15:39:14","title":"QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction","abstract":"Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.","sentences":["Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success.","However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered.","In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction.","We introduce an environmental feedback-based self-correction method called ERASER.","Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary.","Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1.","Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs.","By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach."],"url":"http://arxiv.org/abs/2403.11886v1","category":"cs.CL"}
{"created":"2024-03-18 15:33:06","title":"ReGenNet: Towards Human Action-Reaction Synthesis","abstract":"Humans constantly interact with their surrounding environments. Current human-centric generative models mainly focus on synthesizing humans plausibly interacting with static scenes and objects, while the dynamic human action-reaction synthesis for ubiquitous causal human-human interactions is less explored. Human-human interactions can be regarded as asymmetric with actors and reactors in atomic interaction periods. In this paper, we comprehensively analyze the asymmetric, dynamic, synchronous, and detailed nature of human-human interactions and propose the first multi-setting human action-reaction synthesis benchmark to generate human reactions conditioned on given human actions. To begin with, we propose to annotate the actor-reactor order of the interaction sequences for the NTU120, InterHuman, and Chi3D datasets. Based on them, a diffusion-based generative model with a Transformer decoder architecture called ReGenNet together with an explicit distance-based interaction loss is proposed to predict human reactions in an online manner, where the future states of actors are unavailable to reactors. Quantitative and qualitative results show that our method can generate instant and plausible human reactions compared to the baselines, and can generalize to unseen actor motions and viewpoint changes.","sentences":["Humans constantly interact with their surrounding environments.","Current human-centric generative models mainly focus on synthesizing humans plausibly interacting with static scenes and objects, while the dynamic human action-reaction synthesis for ubiquitous causal human-human interactions is less explored.","Human-human interactions can be regarded as asymmetric with actors and reactors in atomic interaction periods.","In this paper, we comprehensively analyze the asymmetric, dynamic, synchronous, and detailed nature of human-human interactions and propose the first multi-setting human action-reaction synthesis benchmark to generate human reactions conditioned on given human actions.","To begin with, we propose to annotate the actor-reactor order of the interaction sequences for the NTU120, InterHuman, and Chi3D datasets.","Based on them, a diffusion-based generative model with a Transformer decoder architecture called ReGenNet together with an explicit distance-based interaction loss is proposed to predict human reactions in an online manner, where the future states of actors are unavailable to reactors.","Quantitative and qualitative results show that our method can generate instant and plausible human reactions compared to the baselines, and can generalize to unseen actor motions and viewpoint changes."],"url":"http://arxiv.org/abs/2403.11882v1","category":"cs.CV"}
{"created":"2024-03-18 15:32:02","title":"Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton","abstract":"In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.","sentences":["In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild.","Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements.","We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis.","Additionally, we incorporate a pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model.","Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data.","Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline."],"url":"http://arxiv.org/abs/2403.11879v1","category":"cs.SD"}
{"created":"2024-03-18 15:23:09","title":"Rapidly Deployable Intelligent 5G Aerial Neutral Host Networks: an O-RAN-Based Approach","abstract":"Arxiv is acting weird and throwing error: \"Bad character(s) in field Abstract.\" for no reason. Please refer to the manuscript.","sentences":["Arxiv is acting weird and throwing error: \"Bad character(s) in field Abstract.\"","for no reason.","Please refer to the manuscript."],"url":"http://arxiv.org/abs/2403.11869v1","category":"cs.NI"}
{"created":"2024-03-18 15:18:55","title":"Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging","abstract":"Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: https://mert-o.github.io/ThermalNeRF/.","sentences":["Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images.","In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning.","Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality.","We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations.","For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total.","We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images.","Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB.","Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps.","Project page: https://mert-o.github.io/ThermalNeRF/."],"url":"http://arxiv.org/abs/2403.11865v1","category":"cs.CV"}
{"created":"2024-03-18 15:08:01","title":"GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture","abstract":"In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model's score was weighted by percentage to obtain a final score. The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions.","sentences":["In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent.","We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google.","Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge.","We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness.","Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action.","Each model's score was weighted by percentage to obtain a final score.","The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories.","Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions."],"url":"http://arxiv.org/abs/2403.11858v1","category":"cs.CL"}
{"created":"2024-03-18 15:02:46","title":"Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay","abstract":"This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehicles. We demonstrate the effectiveness of our method through extensive simulations generated from real traffic data and compare its performance with existing approaches. L3IS shows a 99.90% success rate in a challenging on-ramp merging case generated from the real US Highway 101 data. We further perform a sensitivity analysis on AL3IS to evaluate its robustness against varying observation delays, which demonstrates an acceptable performance of 93.84% success rate in 1-second V2V communication delay.","sentences":["This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway.","We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles.","We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays.","By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehicles.","We demonstrate the effectiveness of our method through extensive simulations generated from real traffic data and compare its performance with existing approaches.","L3IS shows a 99.90% success rate in a challenging on-ramp merging case generated from the real US Highway 101 data.","We further perform a sensitivity analysis on AL3IS to evaluate its robustness against varying observation delays, which demonstrates an acceptable performance of 93.84% success rate in 1-second V2V communication delay."],"url":"http://arxiv.org/abs/2403.11852v2","category":"cs.RO"}
{"created":"2024-03-18 14:53:48","title":"Fuzzy Rough Choquet Distances for Classification","abstract":"This paper introduces a novel Choquet distance using fuzzy rough set based measures. The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral. This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance. We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours). The paper examines two fuzzy rough set based measures that are based on the positive region. Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences.","sentences":["This paper introduces a novel Choquet distance using fuzzy rough set based measures.","The proposed distance measure combines the attribute information received from fuzzy rough set theory with the flexibility of the Choquet integral.","This approach is designed to adeptly capture non-linear relationships within the data, acknowledging the interplay of the conditional attributes towards the decision attribute and resulting in a more flexible and accurate distance.","We explore its application in the context of machine learning, with a specific emphasis on distance-based classification approaches (e.g. k-nearest neighbours).","The paper examines two fuzzy rough set based measures that are based on the positive region.","Moreover, we explore two procedures for monotonizing the measures derived from fuzzy rough set theory, making them suitable for use with the Choquet integral, and investigate their differences."],"url":"http://arxiv.org/abs/2403.11843v1","category":"cs.LG"}
{"created":"2024-03-18 14:51:19","title":"Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data","abstract":"In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variables that mediate the effect of actions on system dynamics, it is sufficient to learn a lower bound of the mediator distribution function, instead of the Q-function, to partially mitigate the issue of distributional shift. This insight significantly simplifies our algorithm, by circumventing the challenging task of sequential uncertainty quantification for the estimated Q-function. Moreover, we provide theoretical guarantees for the algorithms we propose, and demonstrate their efficacy through simulations, as well as real-world experiments utilizing offline datasets from a leading ride-hailing platform.","sentences":["In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget.","As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning.","However, most existing offline reinforcement learning (RL) methods depend on two key assumptions--unconfoundedness and positivity--which frequently do not hold in observational data contexts.","Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL).","We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data.","Our key observation is that, by incorporating auxiliary variables that mediate the effect of actions on system dynamics, it is sufficient to learn a lower bound of the mediator distribution function, instead of the Q-function, to partially mitigate the issue of distributional shift.","This insight significantly simplifies our algorithm, by circumventing the challenging task of sequential uncertainty quantification for the estimated Q-function.","Moreover, we provide theoretical guarantees for the algorithms we propose, and demonstrate their efficacy through simulations, as well as real-world experiments utilizing offline datasets from a leading ride-hailing platform."],"url":"http://arxiv.org/abs/2403.11841v1","category":"stat.ML"}
{"created":"2024-03-18 14:48:29","title":"Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models","abstract":"Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the process implemented in the second stage. Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library. Furthermore, it incorporates safety expertise from a safety-trained LLM through a lightweight retrieval model. We evaluated our approach on three benchmarks, demonstrating significant improvements in LLM security and quality. Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.","sentences":["Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues.","One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training.","To address these, we introduce Guide-Align, a two-stage approach.","Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval.","Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values.","An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the process implemented in the second stage.","Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library.","Furthermore, it incorporates safety expertise from a safety-trained LLM through a lightweight retrieval model.","We evaluated our approach on three benchmarks, demonstrating significant improvements in LLM security and quality.","Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities."],"url":"http://arxiv.org/abs/2403.11838v1","category":"cs.CL"}
{"created":"2024-03-18 14:47:03","title":"Agent3D-Zero: An Agent for Zero-shot 3D Understanding","abstract":"The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence. The current common practice is to finetune Large Language Models (LLMs) with 3D data and texts to enable 3D understanding. Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data. Alternatively, in this work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework addressing the 3D scene understanding in a zero-shot manner. The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes. By consolidating this idea, we propose a novel way to make use of a Large Visual Language Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding. Specifically, given an input 3D scene, Agent3D-Zero first processes a bird's-eye view image with custom-designed visual prompts, then iteratively chooses the next viewpoints to observe and summarize the underlying knowledge. A distinctive advantage of Agent3D-Zero is the introduction of novel visual prompts, which significantly unleash the VLMs' ability to identify the most informative viewpoints and thus facilitate observing 3D scenes. Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments.","sentences":["The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence.","The current common practice is to finetune Large Language Models (LLMs) with 3D data and texts to enable 3D understanding.","Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data.","Alternatively, in this work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework addressing the 3D scene understanding in a zero-shot manner.","The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes.","By consolidating this idea, we propose a novel way to make use of a Large Visual Language Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding.","Specifically, given an input 3D scene, Agent3D-Zero first processes a bird's-eye view image with custom-designed visual prompts, then iteratively chooses the next viewpoints to observe and summarize the underlying knowledge.","A distinctive advantage of Agent3D-Zero is the introduction of novel visual prompts, which significantly unleash the VLMs' ability to identify the most informative viewpoints and thus facilitate observing 3D scenes.","Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments."],"url":"http://arxiv.org/abs/2403.11835v1","category":"cs.CV"}
{"created":"2024-03-18 14:40:33","title":"Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks","abstract":"Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive research has shown their vulnerability to adversarial attacks, which involve subtle perturbations to the inputs of the models aimed at compromising their performance. Recent proposals have effectively leveraged Graph Neural Networks (GNN) to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness. However, the adoption of GNN-based NIDS introduces new types of risks. In this paper, we propose the first formalization of adversarial attacks specifically tailored for GNN in network intrusion detection. Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios. As a final contribution, we conduct an extensive experimental campaign in which we launch the proposed attacks against state-of-the-art GNN-based NIDS. Our findings demonstrate the increased robustness of the models against classical feature-based adversarial attacks, while highlighting their susceptibility to structure-based attacks.","sentences":["Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS).","Nevertheless, extensive research has shown their vulnerability to adversarial attacks, which involve subtle perturbations to the inputs of the models aimed at compromising their performance.","Recent proposals have effectively leveraged Graph Neural Networks (GNN) to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness.","However, the adoption of GNN-based NIDS introduces new types of risks.","In this paper, we propose the first formalization of adversarial attacks specifically tailored for GNN in network intrusion detection.","Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios.","As a final contribution, we conduct an extensive experimental campaign in which we launch the proposed attacks against state-of-the-art GNN-based NIDS.","Our findings demonstrate the increased robustness of the models against classical feature-based adversarial attacks, while highlighting their susceptibility to structure-based attacks."],"url":"http://arxiv.org/abs/2403.11830v1","category":"cs.CR"}
{"created":"2024-03-18 14:36:28","title":"A meshless and binless approach to compute statistics in 3D Ensemble PTV","abstract":"We propose a method to obtain superresolution of turbulent statistics for three-dimensional ensemble particle tracking velocimetry (EPTV). The method is ''meshless'' because it does not require the definition of a grid for computing derivatives, and it is ''binless'' because it does not require the definition of bins to compute local statistics. The method combines the constrained radial basis function (RBF) formalism introduced Sperotto et al. (Meas Sci Technol, 33:094005, 2022) with a kernel estimate approach for the ensemble averaging of the RBF regressions. The computational cost for the RBF regression is alleviated using the partition of unity method (PUM). Three test cases are considered: (1) a 1D illustrative problem on a Gaussian process, (2) a 3D synthetic test case reproducing a 3D jet-like flow, and (3) an experimental dataset collected for an underwater jet flow at $\\text{Re} = 6750$ using a four-camera 3D PTV system. For each test case, the method performances are compared to traditional binning approaches such as Gaussian weighting (Ag\\\"u\\'i and Jim\\'enez, JFM, 185:447-468, 1987), local polynomial fitting (Ag\\\"uera et al, Meas Sci Technol, 27:124011, 2016), as well as a binned version of the RBF statistics.","sentences":["We propose a method to obtain superresolution of turbulent statistics for three-dimensional ensemble particle tracking velocimetry (EPTV).","The method is ''meshless'' because it does not require the definition of a grid for computing derivatives, and it is ''binless'' because it does not require the definition of bins to compute local statistics.","The method combines the constrained radial basis function (RBF) formalism introduced Sperotto et al.","(Meas Sci Technol, 33:094005, 2022) with a kernel estimate approach for the ensemble averaging of the RBF regressions.","The computational cost for the RBF regression is alleviated using the partition of unity method (PUM).","Three test cases are considered: (1) a 1D illustrative problem on a Gaussian process, (2) a 3D synthetic test case reproducing a 3D jet-like flow, and (3) an experimental dataset collected for an underwater jet flow at $\\text{Re} = 6750$ using a four-camera 3D PTV system.","For each test case, the method performances are compared to traditional binning approaches such as Gaussian weighting (Ag\\\"u\\'i and Jim\\'enez, JFM, 185:447-468, 1987), local polynomial fitting (Ag\\\"uera et al, Meas Sci Technol, 27:124011, 2016), as well as a binned version of the RBF statistics."],"url":"http://arxiv.org/abs/2403.11828v1","category":"physics.flu-dyn"}
{"created":"2024-03-18 14:24:20","title":"Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics","abstract":"Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets before discussing techniques to optimize text-to-image synthesis models towards quality and human preferences. Ultimately, we derive guidelines for improving text-to-image evaluation and discuss the open challenges and current limitations.","sentences":["Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models.","These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases.","As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments.","Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents.","In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics.","We also review frequently adopted text-image benchmark datasets before discussing techniques to optimize text-to-image synthesis models towards quality and human preferences.","Ultimately, we derive guidelines for improving text-to-image evaluation and discuss the open challenges and current limitations."],"url":"http://arxiv.org/abs/2403.11821v1","category":"cs.CV"}
{"created":"2024-03-18 14:04:47","title":"How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments","abstract":"Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other models on GAMA-Bench, achieving a score of 72.5. Moreover, the increasingly higher scores across the three iterations of GPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model's intelligence with each update. The code and experimental results are made publicly available via https://github.com/CUHK-ARISE/GAMABench.","sentences":["Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs).","Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory.","We focus specifically on games that support the participation of more than two agents simultaneously.","Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games.","We design a scoring scheme to assess a model's performance in these games quantitatively.","Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies.","Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited.","However, its performance can be improved through approaches such as Chain-of-Thought.","Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other models on GAMA-Bench, achieving a score of 72.5.","Moreover, the increasingly higher scores across the three iterations of GPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model's intelligence with each update.","The code and experimental results are made publicly available via https://github.com/CUHK-ARISE/GAMABench."],"url":"http://arxiv.org/abs/2403.11807v1","category":"cs.AI"}
{"created":"2024-03-18 14:03:19","title":"Debris disks around main-sequence stars","abstract":"'Debris disks' are collections of small bodies around stars, such as the Asteroid Belt and Kuiper Belt in our Solar System. These disks are composed of objects smaller than planets, including asteroids, comets, dust, and dwarf planets. We detect debris disks around a significant fraction of stars, and these disks appear to be common components of planetary systems. Extrasolar debris disks have a broad range of locations, shapes and features. This chapter provides an introduction to debris disks around main-sequence stars. It summarises our understanding of the field, and covers a wide range of concepts from observations and theory. It describes how we detect extrasolar debris disks, what we see, and what these observations tell us. It also describes how debris disks evolve, and how they interact with planets. The chapter concludes by discussing several unsolved questions in debris-disk science.","sentences":["'Debris disks' are collections of small bodies around stars, such as the Asteroid Belt and Kuiper Belt in our Solar System.","These disks are composed of objects smaller than planets, including asteroids, comets, dust, and dwarf planets.","We detect debris disks around a significant fraction of stars, and these disks appear to be common components of planetary systems.","Extrasolar debris disks have a broad range of locations, shapes and features.","This chapter provides an introduction to debris disks around main-sequence stars.","It summarises our understanding of the field, and covers a wide range of concepts from observations and theory.","It describes how we detect extrasolar debris disks, what we see, and what these observations tell us.","It also describes how debris disks evolve, and how they interact with planets.","The chapter concludes by discussing several unsolved questions in debris-disk science."],"url":"http://arxiv.org/abs/2403.11804v1","category":"astro-ph.EP"}
{"created":"2024-03-18 14:01:45","title":"Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models","abstract":"While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in the long context from 4K to 128K. We further present two intriguing analyses regarding the behavior of LLMs processing long context.","sentences":["While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat).","To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars.","The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task.","Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat.","The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in the long context from 4K to 128K.","We further present two intriguing analyses regarding the behavior of LLMs processing long context."],"url":"http://arxiv.org/abs/2403.11802v1","category":"cs.CL"}
{"created":"2024-03-18 13:50:50","title":"Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus","abstract":"The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.","sentences":["The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process.","We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner.","ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans.","Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity.","Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning."],"url":"http://arxiv.org/abs/2403.11793v1","category":"cs.CL"}
{"created":"2024-03-18 13:47:18","title":"Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling","abstract":"Shape reconstruction from imaging volumes is a recurring need in medical image analysis. Common workflows start with a segmentation step, followed by careful post-processing and,finally, ad hoc meshing algorithms. As this sequence can be timeconsuming, neural networks are trained to reconstruct shapes through template deformation. These networks deliver state-ofthe-art results without manual intervention, but, so far, they have primarily been evaluated on anatomical shapes with little topological variety between individuals. In contrast, other works favor learning implicit shape models, which have multiple benefits for meshing and visualization. Our work follows this direction by introducing deep medial voxels, a semi-implicit representation that faithfully approximates the topological skeleton from imaging volumes and eventually leads to shape reconstruction via convolution surfaces. Our reconstruction technique shows potential for both visualization and computer simulations.","sentences":["Shape reconstruction from imaging volumes is a recurring need in medical image analysis.","Common workflows start with a segmentation step, followed by careful post-processing and,finally, ad hoc meshing algorithms.","As this sequence can be timeconsuming, neural networks are trained to reconstruct shapes through template deformation.","These networks deliver state-ofthe-art results without manual intervention, but, so far, they have primarily been evaluated on anatomical shapes with little topological variety between individuals.","In contrast, other works favor learning implicit shape models, which have multiple benefits for meshing and visualization.","Our work follows this direction by introducing deep medial voxels, a semi-implicit representation that faithfully approximates the topological skeleton from imaging volumes and eventually leads to shape reconstruction via convolution surfaces.","Our reconstruction technique shows potential for both visualization and computer simulations."],"url":"http://arxiv.org/abs/2403.11790v1","category":"cs.CV"}
{"created":"2024-03-18 13:44:48","title":"Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models","abstract":"Extracting hyper-relations is crucial for constructing comprehensive knowledge graphs, but there are limited supervised methods available for this task. To address this gap, we introduce a zero-shot prompt-based method using OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text. Comparing our model with a baseline, we achieved promising results, with a recall of 0.77. Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.","sentences":["Extracting hyper-relations is crucial for constructing comprehensive knowledge graphs, but there are limited supervised methods available for this task.","To address this gap, we introduce a zero-shot prompt-based method using OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.","Comparing our model with a baseline, we achieved promising results, with a recall of 0.77.","Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area."],"url":"http://arxiv.org/abs/2403.11786v1","category":"cs.CL"}
{"created":"2024-03-18 13:41:48","title":"mqdtfit: A collection of Python functions for empirical multichannel quantum defect calculations","abstract":"The Python functions distributed with this article can be used for calculating the parameters of multichannel quantum defect theory models describing excited bound states of complex atoms. These parameters are obtained by fitting a model to experimental data provided by the user. The two main formulations of the theory are supported, namely the one in which the parameters of the model are a set of eigen channel quantum defects and a transformation matrix, and the one where these parameters are the elements of a reactance matrix. The distribution includes programs for calculating theoretical energy levels, calculating mixing coefficients and channel fractions and producing Lu-Fano plots.","sentences":["The Python functions distributed with this article can be used for calculating the parameters of multichannel quantum defect theory models describing excited bound states of complex atoms.","These parameters are obtained by fitting a model to experimental data provided by the user.","The two main formulations of the theory are supported, namely the one in which the parameters of the model are a set of eigen channel quantum defects and a transformation matrix, and the one where these parameters are the elements of a reactance matrix.","The distribution includes programs for calculating theoretical energy levels, calculating mixing coefficients and channel fractions and producing Lu-Fano plots."],"url":"http://arxiv.org/abs/2403.11783v1","category":"physics.atom-ph"}
{"created":"2024-03-18 13:39:05","title":"Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt","abstract":"Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.github.io .","sentences":["Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly.","We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language.","We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy.","Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research.","Experiments show that our model achieves favorable controlling ability and audio quality.","Audio samples are available at http://prompt-singer.github.io ."],"url":"http://arxiv.org/abs/2403.11780v1","category":"cs.SD"}
{"created":"2024-03-18 13:30:12","title":"S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention","abstract":"Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classification and reveal an influence of the length of the pre-training examples but not of the mask size on the downstream performance.","sentences":["Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs).","In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains.","However, its application to EEG signals remains largely unexplored.","In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification.","The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP.","Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding.","Notably, our results highlight the importance of spatial filtering for accurate downstream classification and reveal an influence of the length of the pre-training examples but not of the mask size on the downstream performance."],"url":"http://arxiv.org/abs/2403.11772v1","category":"cs.LG"}
{"created":"2024-03-18 13:22:22","title":"RIS-aided Single-frequency 3D Imaging by Exploiting Multi-view Image Correlations","abstract":"Retrieving range information in three-dimensional (3D) radio imaging is particularly challenging due to the limited communication bandwidth and pilot resources. To address this issue, we consider a reconfigurable intelligent surface (RIS)-aided uplink communication scenario, generating multiple measurements through RIS phase adjustment. This study successfully realizes 3D single-frequency imaging by exploiting the near-field multi-view image correlations deduced from user mobility. We first highlight the significance of considering anisotropy in multi-view image formation by investigating radar cross-section properties and diffraction resolution limits. We then propose a novel model for joint multi-view 3D imaging that incorporates occlusion effects and anisotropic scattering. These factors lead to slow image support variation and smooth coefficient evolution, which are mathematically modeled as Markov processes. Based on this model, we employ the Expectation Maximization-Turbo-Generalized Approximate Message Passing algorithm for joint multi-view single-frequency 3D imaging with limited measurements. Simulation results reveal the superiority of joint multi-view imaging in terms of enhanced imaging ranges, accuracies, and anisotropy characterization compared to single-view imaging. Combining adjacent observations for joint multi-view imaging enables a reduction in the measurement overhead by 80%.","sentences":["Retrieving range information in three-dimensional (3D) radio imaging is particularly challenging due to the limited communication bandwidth and pilot resources.","To address this issue, we consider a reconfigurable intelligent surface (RIS)-aided uplink communication scenario, generating multiple measurements through RIS phase adjustment.","This study successfully realizes 3D single-frequency imaging by exploiting the near-field multi-view image correlations deduced from user mobility.","We first highlight the significance of considering anisotropy in multi-view image formation by investigating radar cross-section properties and diffraction resolution limits.","We then propose a novel model for joint multi-view 3D imaging that incorporates occlusion effects and anisotropic scattering.","These factors lead to slow image support variation and smooth coefficient evolution, which are mathematically modeled as Markov processes.","Based on this model, we employ the Expectation Maximization-Turbo-Generalized Approximate Message Passing algorithm for joint multi-view single-frequency 3D imaging with limited measurements.","Simulation results reveal the superiority of joint multi-view imaging in terms of enhanced imaging ranges, accuracies, and anisotropy characterization compared to single-view imaging.","Combining adjacent observations for joint multi-view imaging enables a reduction in the measurement overhead by 80%."],"url":"http://arxiv.org/abs/2403.11764v1","category":"cs.IT"}
{"created":"2024-03-18 13:03:24","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","abstract":"Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier. MPVR generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively","sentences":["Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs).","To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks.","However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest.","To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR).","Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier.","MPVR generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs.","For example, MPVR obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively"],"url":"http://arxiv.org/abs/2403.11755v2","category":"cs.CV"}
{"created":"2024-03-18 12:47:19","title":"A path-dependent PDE solver based on signature kernels","abstract":"We develop a provably convergent kernel-based solver for path-dependent PDEs (PPDEs). Our numerical scheme leverages signature kernels, a recently introduced class of kernels on path-space. Specifically, we solve an optimal recovery problem by approximating the solution of a PPDE with an element of minimal norm in the signature reproducing kernel Hilbert space (RKHS) constrained to satisfy the PPDE at a finite collection of collocation paths. In the linear case, we show that the optimisation has a unique closed-form solution expressed in terms of signature kernel evaluations at the collocation paths. We prove consistency of the proposed scheme, guaranteeing convergence to the PPDE solution as the number of collocation points increases. Finally, several numerical examples are presented, in particular in the context of option pricing under rough volatility. Our numerical scheme constitutes a valid alternative to the ubiquitous Monte Carlo methods.","sentences":["We develop a provably convergent kernel-based solver for path-dependent PDEs (PPDEs).","Our numerical scheme leverages signature kernels, a recently introduced class of kernels on path-space.","Specifically, we solve an optimal recovery problem by approximating the solution of a PPDE with an element of minimal norm in the signature reproducing kernel Hilbert space (RKHS) constrained to satisfy the PPDE at a finite collection of collocation paths.","In the linear case, we show that the optimisation has a unique closed-form solution expressed in terms of signature kernel evaluations at the collocation paths.","We prove consistency of the proposed scheme, guaranteeing convergence to the PPDE solution as the number of collocation points increases.","Finally, several numerical examples are presented, in particular in the context of option pricing under rough volatility.","Our numerical scheme constitutes a valid alternative to the ubiquitous Monte Carlo methods."],"url":"http://arxiv.org/abs/2403.11738v1","category":"math.NA"}
{"created":"2024-03-18 12:42:53","title":"Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$","abstract":"GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational GNNs. When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] architecture is the original R-GNN architecture with a suitable transformation applied to the input states only. Experimental results illustrate the clear performance gains of R-GNN[$1$] and R-GNN[$2$] over plain R-GNNs, and also over edge transformers that also approximate $3$-GNNs.","sentences":["GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting.","This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings.","Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical.","In this work, we introduce a parameterized version of relational GNNs.","When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings.","For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains.","Furthermore, the new R-GNN[$t$] architecture is the original R-GNN architecture with a suitable transformation applied to the input states only.","Experimental results illustrate the clear performance gains of R-GNN[$1$] and R-GNN[$2$] over plain R-GNNs, and also over edge transformers that also approximate $3$-GNNs."],"url":"http://arxiv.org/abs/2403.11734v1","category":"cs.AI"}
{"created":"2024-03-19 17:50:40","title":"A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks","abstract":"The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT). However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG. This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase. Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse. Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation. Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage. To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching. Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark.","sentences":["The 6TiSCH protocol stack was proposed to ensure high-performance communications in the Industrial Internet of Things (IIoT).","However, the lack of sufficient time slots for nodes outside the 6TiSCH's Destination Oriented Directed Acyclic Graph (DODAG) to transmit their Destination Advertisement Object (DAO) messages and cell reservation requests significantly hinders their integration into the DODAG.","This oversight not only prolongs the device's join time but also increases energy consumption during the network formation phase.","Moreover, challenges emerge due to the substantial number of control packets employed by both the 6TiSCH Scheduling Function (SF) and routing protocol (RPL), thus draining more energy resources, increasing medium contention, and decreasing spatial reuse.","Furthermore, an SF that overlooks previously allocated slots when assigning new ones to the same node may increase jitter, and more complications ensue when it neglects the state of the TSCH queue, thus leading to packet dropping due to queue saturation.","Additional complexity arises when the RPL disregards the new parent's schedule saturation during parent switching, which results in inefficient energy and time usage.","To address these issues, we introduce in this paper novel mechanisms, strategically situated at the intersection of SF and RPL that are designed to balance the control packet distribution and adaptively manage parent switching.","Our proposal, implemented within the 6TiSCH simulator, demonstrates significant improvements across vital performance metrics, such as node's joining time, jitter, latency, energy consumption, and amount of traffic, in comparison to the conventional 6TiSCH benchmark."],"url":"http://arxiv.org/abs/2403.12949v1","category":"cs.NI"}
{"created":"2024-03-19 17:48:42","title":"Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes","abstract":"In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed variance estimator.","sentences":["In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy.","To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data.","We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\\tilde{O}(d)$, where $d$ is the feature dimension.","We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed variance estimator."],"url":"http://arxiv.org/abs/2403.12946v1","category":"cs.LG"}
{"created":"2024-03-19 17:47:36","title":"The physics of Core-Collapse Supernovae: explosion mechanism and explosive nucleosynthesis","abstract":"Recent developments in multi-dimensional simulations of core-collapse supernovae have considerably improved our understanding of this complex phenomenon. In addition to that, one-dimensional (1D) studies have been employed to study the explosion mechanism and its causal connection to the pre-collapse structure of the star, as well as to explore the vast parameter space of supernovae. Nonetheless, many uncertainties still affect the late stages of the evolution of massive stars, their collapse, and the subsequent shock propagation. In this review, we will briefly summarize the state-of-the-art of both 1D and 3D simulations and how they can be employed to study the evolution of massive stars, supernova explosions, and shock propagation, focusing on the uncertainties that affect each of these phases. Finally, we will illustrate the typical nucleosynthesis products that emerge from the explosion.","sentences":["Recent developments in multi-dimensional simulations of core-collapse supernovae have considerably improved our understanding of this complex phenomenon.","In addition to that, one-dimensional (1D) studies have been employed to study the explosion mechanism and its causal connection to the pre-collapse structure of the star, as well as to explore the vast parameter space of supernovae.","Nonetheless, many uncertainties still affect the late stages of the evolution of massive stars, their collapse, and the subsequent shock propagation.","In this review, we will briefly summarize the state-of-the-art of both 1D and 3D simulations and how they can be employed to study the evolution of massive stars, supernova explosions, and shock propagation, focusing on the uncertainties that affect each of these phases.","Finally, we will illustrate the typical nucleosynthesis products that emerge from the explosion."],"url":"http://arxiv.org/abs/2403.12942v1","category":"astro-ph.SR"}
{"created":"2024-03-19 17:43:57","title":"Neural Differential Algebraic Equations","abstract":"Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system.","sentences":["Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints.","Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships.","Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs.","This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains.","In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks.","Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes.","Our experiments demonstrate the proposed method's robustness to noise and extrapolation ability to (i) learn the behaviors of the system components and their interaction physics and (ii) disambiguate between data trends and mechanistic relationships contained in the system."],"url":"http://arxiv.org/abs/2403.12938v1","category":"cs.LG"}
{"created":"2024-03-19 17:43:48","title":"Elucidating the Nature of $\u03c0$-hydrogen Bonding in Liquid Water and Ammonia","abstract":"Aromatic compounds form an unusual kind of hydrogen bond with water and ammonia molecules, known as the $\\pi$-hydrogen bond. In this work, we report ab initio path integral molecular dynamics simulations enhanced by machine-learning potentials to study the structural, dynamical, and spectroscopic properties of solutions of benzene in liquid water and ammonia. Specifically, we model the spatial distribution functions of the solvents around the benzene molecule, establish the $\\pi$-hydrogen bonding interaction as a prominent structural motive, and set up existence criteria to distinguish the $\\pi$-hydrogen bonded configurations. These serve as a structural basis to calculate binding affinities of the solvent molecules in $\\pi$hydrogen bonds, identify an anticooperativity effect across the aromatic ring in water (but not ammonia), and estimate $\\pi$-hydrogen bond lifetimes in both solvents. Finally, we model hydration-shell-resolved vibrational spectra to clearly identify the vibrational signature of this structural motif in our simulations. These decomposed spectra corroborate previous experimental findings for benzene in water, offer additional insights, and further emphasize the contrast between $\\pi$-hydrogen bonds in water and in ammonia. Our simulations provide a comprehensive picture of the studied phenomenon and, at the same time, serve as a meaningful \\textit{ab initio} reference for an accurate description of $\\pi$-hydrogen bonding using empirical force fields in more complex situations, such as the hydration of biological interfaces.","sentences":["Aromatic compounds form an unusual kind of hydrogen bond with water and ammonia molecules, known as the $\\pi$-hydrogen bond.","In this work, we report ab initio path integral molecular dynamics simulations enhanced by machine-learning potentials to study the structural, dynamical, and spectroscopic properties of solutions of benzene in liquid water and ammonia.","Specifically, we model the spatial distribution functions of the solvents around the benzene molecule, establish the $\\pi$-hydrogen bonding interaction as a prominent structural motive, and set up existence criteria to distinguish the $\\pi$-hydrogen bonded configurations.","These serve as a structural basis to calculate binding affinities of the solvent molecules in $\\pi$hydrogen bonds, identify an anticooperativity effect across the aromatic ring in water (but not ammonia), and estimate $\\pi$-hydrogen bond lifetimes in both solvents.","Finally, we model hydration-shell-resolved vibrational spectra to clearly identify the vibrational signature of this structural motif in our simulations.","These decomposed spectra corroborate previous experimental findings for benzene in water, offer additional insights, and further emphasize the contrast between $\\pi$-hydrogen bonds in water and in ammonia.","Our simulations provide a comprehensive picture of the studied phenomenon and, at the same time, serve as a meaningful \\textit{ab initio} reference for an accurate description of $\\pi$-hydrogen bonding using empirical force fields in more complex situations, such as the hydration of biological interfaces."],"url":"http://arxiv.org/abs/2403.12937v1","category":"physics.chem-ph"}
{"created":"2024-03-19 17:32:53","title":"Identifiability and Observability of Nonsmooth Systems via Taylor-like Approximations","abstract":"New sensitivity-based methods are developed for determining identifiability and observability of nonsmooth input-output systems. More specifically, lexicographic calculus is used to construct nonsmooth sensitivity rank condition (SERC) tests, which we call lexicographic SERC (L-SERC) tests. The introduced L-SERC tests are: (i) practically implementable and amenable to large-scale problems; (ii) accurate since they directly treat the nonsmoothness while avoiding, e.g., smoothing approximations; and (iii) analogous to (and indeed recover) their smooth counterparts. To accomplish this, a first-order Taylor-like approximation theory is developed using lexicographic differentiation to directly treat nonsmooth functions. A practically implementable algorithm is proposed that determines partial structural identifiability or observability, a useful characterization in the nonsmooth setting. Lastly, the theory is illustrated through an application in climate modeling.","sentences":["New sensitivity-based methods are developed for determining identifiability and observability of nonsmooth input-output systems.","More specifically, lexicographic calculus is used to construct nonsmooth sensitivity rank condition (SERC) tests, which we call lexicographic SERC (L-SERC) tests.","The introduced L-SERC tests are: (i) practically implementable and amenable to large-scale problems; (ii) accurate since they directly treat the nonsmoothness while avoiding, e.g., smoothing approximations; and (iii) analogous to (and indeed recover) their smooth counterparts.","To accomplish this, a first-order Taylor-like approximation theory is developed using lexicographic differentiation to directly treat nonsmooth functions.","A practically implementable algorithm is proposed that determines partial structural identifiability or observability, a useful characterization in the nonsmooth setting.","Lastly, the theory is illustrated through an application in climate modeling."],"url":"http://arxiv.org/abs/2403.12930v1","category":"math.OC"}
{"created":"2024-03-19 17:29:13","title":"Decoherence-free algebras in quantum dynamics","abstract":"In this Article we analyze the algebraic properties of the asymptotic dynamics of finite-dimensional open quantum systems in the Heisenberg picture. In particular, a natural product (Choi-Effros product) can be defined in the asymptotic regime. Motivated by this structure, we introduce a new space called the Choi-Effros decoherence-free algebra. Interestingly, this space is both a C* -algebra with respect to the composition product, and a B* -algebra with respect to the Choi-Effros product. Moreover, such space admits a direct-sum decomposition revealing a clear relationship with the attractor subspace of the dynamics. In particular, the equality between the attractor subspace and the Choi-Effros decoherence-free algebra is a necessary and sufficient condition for a faithful dynamics. Finally, we show how all the findings do not rely on complete positivity but on the much weaker Schwarz property.","sentences":["In this Article we analyze the algebraic properties of the asymptotic dynamics of finite-dimensional open quantum systems in the Heisenberg picture.","In particular, a natural product (Choi-Effros product) can be defined in the asymptotic regime.","Motivated by this structure, we introduce a new space called the Choi-Effros decoherence-free algebra.","Interestingly, this space is both a C* -algebra with respect to the composition product, and a B* -algebra with respect to the Choi-Effros product.","Moreover, such space admits a direct-sum decomposition revealing a clear relationship with the attractor subspace of the dynamics.","In particular, the equality between the attractor subspace and the Choi-Effros decoherence-free algebra is a necessary and sufficient condition for a faithful dynamics.","Finally, we show how all the findings do not rely on complete positivity but on the much weaker Schwarz property."],"url":"http://arxiv.org/abs/2403.12926v1","category":"quant-ph"}
{"created":"2024-03-19 17:29:05","title":"On the $N$-waves hierarchy with constant boundary conditions. Spectral properties","abstract":"The paper is devoted to $N$-wave equations with constant boundary conditions related to symplectic Lie algebras. We study the spectral properties of a class of Lax operators $L$, whose potentials $Q(x,t)$ tend to constants $Q_\\pm$ for $x\\to \\pm \\infty$. For special choices of $Q_\\pm$ we outline the spectral properties of $L$, the direct scattering transform and construct its fundamental analytic solutions. We generalise Wronskian relations for the case of CBC -- this allows us to analyse the mapping between the scattering data and the $x$-derivative of the potential $Q_x$. Next, using the Wronskian relations we derive the dispersion laws for the $N$-wave hierarchy and describe the NLEE related to the given Lax operator.","sentences":["The paper is devoted to $N$-wave equations with constant boundary conditions related to symplectic Lie algebras.","We study the spectral properties of a class of Lax operators $L$, whose potentials $Q(x,t)$ tend to constants $Q_\\pm$ for $x\\to \\pm \\infty$. For special choices of $Q_\\pm$ we outline the spectral properties of $L$, the direct scattering transform and construct its fundamental analytic solutions.","We generalise Wronskian relations for the case of CBC -- this allows us to analyse the mapping between the scattering data and the $x$-derivative of the potential $Q_x$. Next, using the Wronskian relations we derive the dispersion laws for the $N$-wave hierarchy and describe the NLEE related to the given Lax operator."],"url":"http://arxiv.org/abs/2403.12925v1","category":"nlin.SI"}
{"created":"2024-03-19 17:05:47","title":"Discovery of a hot post-AGB star in Galactic globular cluster E3","abstract":"We report a new hot-PAGB star identified in the Galactic GC E3, which is one of the first to show a binary signature among the identified PAGB stars of GCs. We present a detailed photometric and spectroscopic analysis to study the evolutionary status of the newly identified hot-PAGB star. We use Gaia EDR3 proper motion (PM) and parallax measurements to confirm the cluster membership. We supplement the photometric observations with radial velocities (RVs) from multi-epoch high resolution (R$\\sim$28000) spectroscopic observations. We fitted the spectral energy distribution of 30 photometric fluxes of the PAGB star from ultraviolet to infra-red passbands with the TLUSTY BSTAR2006 stellar atmosphere model to derive its physical parameters, \\Teff, $\\log g$, \\Lbol, and R. We derive the chemical abundances of nine elements (He, C, N, O, Ne, Al, Si, S, and Fe) using spectroscopic measurements. The derived chemical abundances, kinematic information, and stellar parameters are used to study the evolution history of the star. The PM and parallax of the identified PAGB star match well with cluster parameters, which confirms its cluster membership. We find that the RVs vary over $\\sim$6 \\kms\\ between the two epochs. This is an indication of the star being in a binary orbit. A simulation of possible binary systems with the observed RVs suggests a binary period of either 39.12 days or 17.83 days with mass ratio, q$\\geq$1.0. The [Fe/H] derived using the high-resolution spectra is $\\sim -$0.7 dex matches well with the cluster metallicity. Various PAGB evolutionary tracks on the H-R diagram suggest a current mass of the identified PAGB star in the range 0.51$-$0.55 \\Msun. The star is enriched with C and O abundances, showing similar CNO abundances compared to the other PAGB stars of GCs with the evidence of 3rd dredge-up on the AGB phase.","sentences":["We report a new hot-PAGB star identified in the Galactic GC E3, which is one of the first to show a binary signature among the identified PAGB stars of GCs.","We present a detailed photometric and spectroscopic analysis to study the evolutionary status of the newly identified hot-PAGB star.","We use Gaia EDR3 proper motion (PM) and parallax measurements to confirm the cluster membership.","We supplement the photometric observations with radial velocities (RVs) from multi-epoch high resolution (R$\\sim$28000) spectroscopic observations.","We fitted the spectral energy distribution of 30 photometric fluxes of the PAGB star from ultraviolet to infra-red passbands with the TLUSTY BSTAR2006 stellar atmosphere model to derive its physical parameters, \\Teff, $\\log g$, \\Lbol, and R. We derive the chemical abundances of nine elements (He, C, N, O, Ne, Al, Si, S, and Fe) using spectroscopic measurements.","The derived chemical abundances, kinematic information, and stellar parameters are used to study the evolution history of the star.","The PM and parallax of the identified PAGB star match well with cluster parameters, which confirms its cluster membership.","We find that the RVs vary over $\\sim$6 \\kms\\ between the two epochs.","This is an indication of the star being in a binary orbit.","A simulation of possible binary systems with the observed RVs suggests a binary period of either 39.12 days or 17.83 days with mass ratio, q$\\geq$1.0.","The [Fe/H] derived using the high-resolution spectra is $\\sim -$0.7 dex matches well with the cluster metallicity.","Various PAGB evolutionary tracks on the H-R diagram suggest a current mass of the identified PAGB star in the range 0.51$-$0.55 \\Msun.","The star is enriched with C and O abundances, showing similar CNO abundances compared to the other PAGB stars of GCs with the evidence of 3rd dredge-up on the AGB phase."],"url":"http://arxiv.org/abs/2403.12907v1","category":"astro-ph.SR"}
{"created":"2024-03-19 17:01:09","title":"Ziv-Zakai-Optimal OFDM Resource Allocation for Time-of-Arrival Estimation","abstract":"This paper presents methods of optimizing the placement and power allocations of pilots in an orthogonal frequency-division multiplexing (OFDM) signal to minimize time-of-arrival (TOA) estimation errors under power and resource allocation constraints. TOA errors in this optimization are quantified through the Ziv-Zakai bound (ZZB), which captures error thresholding effects caused by sidelobes in the signal's autocorrelation function (ACF) which are not captured by the Cramer-Rao lower bound. This paper is the first to solve for these ZZB-optimal allocations in the context of OFDM signals, under integer resource allocation constraints, and under both coherent and noncoherent reception. Under convex constraints, the optimization of the ZZB is proven to be convex; under integer constraints, the optimization is lower bounded by a convex relaxation and a branch-and-bound algorithm is proposed for efficiently allocating pilot resources. These allocations are evaluated by their ZZBs and ACFs, compared against a typical uniform allocation, and deployed on a software-defined radio TOA measurement platform to demonstrate their applicability in real-world systems.","sentences":["This paper presents methods of optimizing the placement and power allocations of pilots in an orthogonal frequency-division multiplexing (OFDM) signal to minimize time-of-arrival (TOA) estimation errors under power and resource allocation constraints.","TOA errors in this optimization are quantified through the Ziv-Zakai bound (ZZB), which captures error thresholding effects caused by sidelobes in the signal's autocorrelation function (ACF) which are not captured by the Cramer-Rao lower bound.","This paper is the first to solve for these ZZB-optimal allocations in the context of OFDM signals, under integer resource allocation constraints, and under both coherent and noncoherent reception.","Under convex constraints, the optimization of the ZZB is proven to be convex; under integer constraints, the optimization is lower bounded by a convex relaxation and a branch-and-bound algorithm is proposed for efficiently allocating pilot resources.","These allocations are evaluated by their ZZBs and ACFs, compared against a typical uniform allocation, and deployed on a software-defined radio TOA measurement platform to demonstrate their applicability in real-world systems."],"url":"http://arxiv.org/abs/2403.12905v1","category":"eess.SP"}
{"created":"2024-03-19 16:56:26","title":"Automated Tabletop Exfoliation and Identification of Monolayer Graphene Flakes","abstract":"The discovery of graphene, one of the most-studied materials in condensed matter physics due to its singular mechanical, optical, and electronic properties, was enabled by manual ``Scotch Tape'' exfoliation. Nearly two decades later, this method is still widely used to obtain chemically-pristine flakes of graphene and other 2D van der Waals materials. Unfortunately, the yield of large, pristine flakes with uniform thickness is inconsistent. Thus, significant time and effort are required to exfoliate and locate flakes suitable for fabricating multilayer van der Waals heterostructures. Here, we describe a relatively affordable tabletop device (the ''eXfoliator'') that can reproducibly control key parameters and largely automate the exfoliation process. In a typical exfoliation run, the eXfoliator produces 3 or more large ($\\ge400\\ \\mu$m$^2$) high-quality monolayers, allowing new users to produce large pristine graphene monolayers at a rate comparable to manual exfoliation by an experienced user. We use an automated mapping system and computer vision algorithm to locate candidate flakes.","sentences":["The discovery of graphene, one of the most-studied materials in condensed matter physics due to its singular mechanical, optical, and electronic properties, was enabled by manual ``Scotch Tape'' exfoliation.","Nearly two decades later, this method is still widely used to obtain chemically-pristine flakes of graphene and other 2D van der Waals materials.","Unfortunately, the yield of large, pristine flakes with uniform thickness is inconsistent.","Thus, significant time and effort are required to exfoliate and locate flakes suitable for fabricating multilayer van der Waals heterostructures.","Here, we describe a relatively affordable tabletop device (the ''eXfoliator'') that can reproducibly control key parameters and largely automate the exfoliation process.","In a typical exfoliation run, the eXfoliator produces 3 or more large ($\\ge400\\ \\mu$m$^2$) high-quality monolayers, allowing new users to produce large pristine graphene monolayers at a rate comparable to manual exfoliation by an experienced user.","We use an automated mapping system and computer vision algorithm to locate candidate flakes."],"url":"http://arxiv.org/abs/2403.12901v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-19 16:44:53","title":"Uoc luong kenh truyen trong he thong da robot su dung SDR","abstract":"This study focuses on developing an experimental system for estimating communication channels in a multi-robot mobile system using software-defined radio (SDR) devices. The system consists of two mobile robots programmed for two scenarios: one where the robot remains stationary and another where it follows a predefined trajectory. Communication within the system is conducted through orthogonal frequency-division multiplexing (OFDM) to mitigate the effects of multipath propagation in indoor environments. The system's performance is evaluated using the bit error rate (BER). Connections related to robot motion and communication are implemented using Raspberry Pi 3 and BladeRF x115, respectively. The least squares (LS) technique is employed to estimate the channel with a bit error rate of approximately 10^(-2).","sentences":["This study focuses on developing an experimental system for estimating communication channels in a multi-robot mobile system using software-defined radio (SDR) devices.","The system consists of two mobile robots programmed for two scenarios: one where the robot remains stationary and another where it follows a predefined trajectory.","Communication within the system is conducted through orthogonal frequency-division multiplexing (OFDM) to mitigate the effects of multipath propagation in indoor environments.","The system's performance is evaluated using the bit error rate (BER).","Connections related to robot motion and communication are implemented using Raspberry Pi 3 and BladeRF x115, respectively.","The least squares (LS) technique is employed to estimate the channel with a bit error rate of approximately 10^(-2)."],"url":"http://arxiv.org/abs/2403.12892v1","category":"cs.RO"}
{"created":"2024-03-19 16:36:31","title":"Electrical readout of spins in the absence of spin blockade","abstract":"In semiconductor nanostructures, spin blockade (SB) is the most scalable mechanism for electrical spin readout requiring only two bound spins for its implementation which, in conjunction with charge sensing techniques, has led to high-fidelity readout of spins in semiconductor-based quantum processors. However, various mechanisms may lift SB, such as strong spin-orbit coupling (SOC) or low-lying excited states, hence posing challenges to perform spin readout at scale and with high fidelity in such systems. Here, we present a method, based on the dependence of the two-spin system polarizability on energy detuning, to perform spin state readout even when SB lifting mechanisms are dominant. It leverages SB lifting as a resource to detect different spin measurement outcomes selectively and positively. We demonstrate the method using a hybrid system formed by a quantum dot (QD) and a Boron acceptor in a silicon p-type transistor and show spin selective and positive readout of different spin states under SB lifting conditions due to (i) SOC and (ii) low-lying orbital states in the QD. We further use the method to determine the detuning-dependent spin relaxation time of 0.1-8~$\\mu$s. Our method should help perform high-fidelity projective spin measurements in systems subject to strong SOC and may facilitate quantum tomography and state leakage studies.","sentences":["In semiconductor nanostructures, spin blockade (SB) is the most scalable mechanism for electrical spin readout requiring only two bound spins for its implementation which, in conjunction with charge sensing techniques, has led to high-fidelity readout of spins in semiconductor-based quantum processors.","However, various mechanisms may lift SB, such as strong spin-orbit coupling (SOC) or low-lying excited states, hence posing challenges to perform spin readout at scale and with high fidelity in such systems.","Here, we present a method, based on the dependence of the two-spin system polarizability on energy detuning, to perform spin state readout even when SB lifting mechanisms are dominant.","It leverages SB lifting as a resource to detect different spin measurement outcomes selectively and positively.","We demonstrate the method using a hybrid system formed by a quantum dot (QD) and a Boron acceptor in a silicon p-type transistor and show spin selective and positive readout of different spin states under SB lifting conditions due to (i) SOC and (ii) low-lying orbital states in the QD.","We further use the method to determine the detuning-dependent spin relaxation time of 0.1-8~$\\mu$s.","Our method should help perform high-fidelity projective spin measurements in systems subject to strong SOC and may facilitate quantum tomography and state leakage studies."],"url":"http://arxiv.org/abs/2403.12888v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-19 16:03:03","title":"Primal Methods for Variational Inequality Problems with Functional Constraints","abstract":"Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequality problems, without necessitating any information on the optimal Lagrange multipliers. We establish a non-asymptotic convergence analysis of the algorithm for variational inequality problems with monotone operators under smooth constraints. Remarkably, our algorithms match the complexity of projection-based methods in terms of operator queries for both monotone and strongly monotone settings, while utilizing significantly cheaper oracles based on quadratic programming. Furthermore, we provide several numerical examples to evaluate the efficacy of our algorithms.","sentences":["Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research.","First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability.","However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints.","Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function.","These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers.","In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequality problems, without necessitating any information on the optimal Lagrange multipliers.","We establish a non-asymptotic convergence analysis of the algorithm for variational inequality problems with monotone operators under smooth constraints.","Remarkably, our algorithms match the complexity of projection-based methods in terms of operator queries for both monotone and strongly monotone settings, while utilizing significantly cheaper oracles based on quadratic programming.","Furthermore, we provide several numerical examples to evaluate the efficacy of our algorithms."],"url":"http://arxiv.org/abs/2403.12859v1","category":"math.OC"}
{"created":"2024-03-19 15:54:56","title":"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a Multi-Objective Genetic Algorithm","abstract":"Augmented Reality (AR) and Virtual Reality (VR) systems involve computationally intensive image processing algorithms that can burden end-devices with limited resources, leading to poor performance in providing low latency services. Edge-to-cloud computing overcomes the limitations of end-devices by offloading their computations to nearby edge devices or remote cloud servers. Although this proves to be sufficient for many applications, optimal placement of latency sensitive AR/VR services in edge-to-cloud infrastructures (to provide desirable service response times and reliability) remain a formidable challenging. To address this challenge, this paper develops a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of AR/VR-based services in multi-tier edge-to-cloud environments. The primary objective of the proposed MOGA is to minimize the response time of all running services, while maximizing the reliability of the underlying system from both software and hardware perspectives. To evaluate its performance, we mathematically modeled all components and developed a tailor-made simulator to assess its effectiveness on various scales. MOGA was compared with several heuristics to prove that intuitive solutions, which are usually assumed sufficient, are not efficient enough for the stated problem. The experimental results indicated that MOGA can significantly reduce the response time of deployed services by an average of 67\\% on different scales, compared to other heuristic methods. MOGA also ensures reliability of the 97\\% infrastructure (hardware) and 95\\% services (software).","sentences":["Augmented Reality (AR) and Virtual Reality (VR) systems involve computationally intensive image processing algorithms that can burden end-devices with limited resources, leading to poor performance in providing low latency services.","Edge-to-cloud computing overcomes the limitations of end-devices by offloading their computations to nearby edge devices or remote cloud servers.","Although this proves to be sufficient for many applications, optimal placement of latency sensitive AR/VR services in edge-to-cloud infrastructures (to provide desirable service response times and reliability) remain a formidable challenging.","To address this challenge, this paper develops a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of AR/VR-based services in multi-tier edge-to-cloud environments.","The primary objective of the proposed MOGA is to minimize the response time of all running services, while maximizing the reliability of the underlying system from both software and hardware perspectives.","To evaluate its performance, we mathematically modeled all components and developed a tailor-made simulator to assess its effectiveness on various scales.","MOGA was compared with several heuristics to prove that intuitive solutions, which are usually assumed sufficient, are not efficient enough for the stated problem.","The experimental results indicated that MOGA can significantly reduce the response time of deployed services by an average of 67\\% on different scales, compared to other heuristic methods.","MOGA also ensures reliability of the 97\\% infrastructure (hardware) and 95\\% services (software)."],"url":"http://arxiv.org/abs/2403.12849v1","category":"cs.DC"}
{"created":"2024-03-19 15:49:32","title":"The Interplay Between Symmetries and Impact Effects on Hybrid Mechanical Systems","abstract":"Hybrid systems are dynamical systems with continuous-time and discrete-time components in their dynamics. When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the discrete-time transition of the dynamics: interior impacts and exterior impacts. In this paper we define hybrid systems on principal bundles, study the underlying geometry on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle.","sentences":["Hybrid systems are dynamical systems with continuous-time and discrete-time components in their dynamics.","When hybrid systems are defined on a principal bundle we are able to define two classes of impacts for the discrete-time transition of the dynamics: interior impacts and exterior impacts.","In this paper we define hybrid systems on principal bundles, study the underlying geometry on the switching surface where impacts occur and we find conditions for which both exterior and interior impacts are preserved by the mechanical connection induced in the principal bundle."],"url":"http://arxiv.org/abs/2403.12842v1","category":"cs.RO"}
{"created":"2024-03-19 15:48:55","title":"Episodicity in accretion-ejection processes associated with IRAS 15398-3359","abstract":"The protostar IRAS 15398-3359 is associated with a bipolar molecular outflow ejected in an nearly northeast-southwest (NE-SW) direction which has been extensively studied. It has been suggested previous episodic accretion events by this source. Furthermore, the analysis of the morphology and kinematics of the molecular outflow revealed the presence of four $^{12}$CO(2-1) bipolar elliptical shock-like structures identified in both lobes. These structures seem to trace different ejections inclined $\\sim$10\\deg on the plane of the sky from each other. This led to the hypothesis that the outflow axis likely precesses and launches material episodically. We analyze ALMA archive observations in Band 6, revealing the presence of low-velocity ($<3.5$km s$^{-1}$) emission from the line $^{12}$CO(2-1) to the south and north of the protostar. We study the morphology and kinematics of the gas, which seems to support the hypothesis of a precessing episodic outflow. The ALMA observations reveal a north-south (N-S) outflow most likely associated with the IRAS 15398-3359 protostellar system. This outflow could be older than the well-studied NE-SW outflow. The orientation of the N-S outflow is 50\\deg - 60\\deg on the plane of the sky away from that of the NE-SW outflow. We also analyze the Spectral Energy Distribution of a far away young star and preliminary discard it as the driver of the SE outflow remnants. The new observations support the hypothesis of strong episodic accretion-ejection events in IRAS 15398-3359, accompanied by dramatic changes in the orientation of its ejection axis, implying that all the outflows in the region may have been driven by the same protostar.","sentences":["The protostar IRAS 15398-3359 is associated with a bipolar molecular outflow ejected in an nearly northeast-southwest (NE-SW) direction which has been extensively studied.","It has been suggested previous episodic accretion events by this source.","Furthermore, the analysis of the morphology and kinematics of the molecular outflow revealed the presence of four $^{12}$CO(2-1) bipolar elliptical shock-like structures identified in both lobes.","These structures seem to trace different ejections inclined $\\sim$10\\deg on the plane of the sky from each other.","This led to the hypothesis that the outflow axis likely precesses and launches material episodically.","We analyze ALMA archive observations in Band 6, revealing the presence of low-velocity ($<3.5$km s$^{-1}$) emission from the line $^{12}$CO(2-1) to the south and north of the protostar.","We study the morphology and kinematics of the gas, which seems to support the hypothesis of a precessing episodic outflow.","The ALMA observations reveal a north-south (N-S) outflow most likely associated with the IRAS 15398-3359 protostellar system.","This outflow could be older than the well-studied NE-SW outflow.","The orientation of the N-S outflow is 50\\deg - 60\\deg on the plane of the sky away from that of the NE-SW outflow.","We also analyze the Spectral Energy Distribution of a far away young star and preliminary discard it as the driver of the SE outflow remnants.","The new observations support the hypothesis of strong episodic accretion-ejection events in IRAS 15398-3359, accompanied by dramatic changes in the orientation of its ejection axis, implying that all the outflows in the region may have been driven by the same protostar."],"url":"http://arxiv.org/abs/2403.12841v1","category":"astro-ph.SR"}
{"created":"2024-03-19 15:45:54","title":"Global-guided Focal Neural Radiance Field for Large-scale Scene Rendering","abstract":"Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: https://shaomq2187.github.io/GF-NeRF/","sentences":["Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes.","However, their limited model capacity typically results in blurred rendering results.","Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs.","These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene.","Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity.","In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes.","Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy.","The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders.","Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency.","Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes.","Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes.","We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets.","Our project page: https://shaomq2187.github.io/GF-NeRF/"],"url":"http://arxiv.org/abs/2403.12839v1","category":"cs.CV"}
{"created":"2024-03-19 15:39:16","title":"Electronic instability, layer selectivity and Fermi arcs in La$_3$Ni$_2$O$_7$","abstract":"Using advanced dynamical mean-field theory on a realistic level we study the normal-state correlated electronic structure of the high-pressure superconductor La$_3$Ni$_2$O$_7$ and compare the features of the conventional bilayer (2222) Ruddelsden-Popper crystal structure with those of a newly-identified monolayer-trilayer (1313) alternation. Both structural cases display Ni-$d_{z^2}$ flat-band character at low-energy, which drives an electronic instability with a wave vector ${\\bf q_{\\rm I}}=(0.25,0.25,q_z)$ at ambient pressure, in line with recent experimental findings. The 1313 electronic structure exhibits significant layer selectivity, rendering especially the monolayer part to be Mott-critical. At high pressure, this layer selectivity weakens and the 1313 fermiology displays arcs reminiscent to those of high-$T_c$ cuprates. In contrast to dominant inter-site self-energy effects in the latter systems, here the Fermi arcs are the result of the multiorbital and multilayer interplay within a correlated flat-band scenario.","sentences":["Using advanced dynamical mean-field theory on a realistic level we study the normal-state correlated electronic structure of the high-pressure superconductor La$_3$Ni$_2$O$_7$ and compare the features of the conventional bilayer (2222) Ruddelsden-Popper crystal structure with those of a newly-identified monolayer-trilayer (1313) alternation.","Both structural cases display Ni-$d_{z^2}$ flat-band character at low-energy, which drives an electronic instability with a wave vector ${\\bf q_{\\rm I}}=(0.25,0.25,q_z)$ at ambient pressure, in line with recent experimental findings.","The 1313 electronic structure exhibits significant layer selectivity, rendering especially the monolayer part to be Mott-critical.","At high pressure, this layer selectivity weakens and the 1313 fermiology displays arcs reminiscent to those of high-$T_c$ cuprates.","In contrast to dominant inter-site self-energy effects in the latter systems, here the Fermi arcs are the result of the multiorbital and multilayer interplay within a correlated flat-band scenario."],"url":"http://arxiv.org/abs/2403.12831v1","category":"cond-mat.str-el"}
{"created":"2024-03-19 15:12:11","title":"The Emergence of Hardware Fuzzing: A Critical Review of its Significance","abstract":"In recent years, there has been a notable surge in attention towards hardware security, driven by the increasing complexity and integration of processors, SoCs, and third-party IPs aimed at delivering advanced solutions. However, this complexity also introduces vulnerabilities and bugs into hardware systems, necessitating early detection during the IC design cycle to uphold system integrity and mitigate re-engineering costs. While the Design Verification (DV) community employs dynamic and formal verification strategies, they encounter challenges such as scalability for intricate designs and significant human intervention, leading to prolonged verification durations. As an alternative approach, hardware fuzzing, inspired by software testing methodologies, has gained prominence for its efficacy in identifying bugs within complex hardware designs. Despite the introduction of various hardware fuzzing techniques, obstacles such as inefficient conversion of hardware modules into software models impede their effectiveness. This Systematization of Knowledge (SoK) initiative delves into the fundamental principles of existing hardware fuzzing, methodologies, and their applicability across diverse hardware designs. Additionally, it evaluates factors such as the utilization of golden reference models (GRMs), coverage metrics, and toolchains to gauge their potential for broader adoption, akin to traditional formal verification methods. Furthermore, this work examines the reliability of existing hardware fuzzing techniques in identifying vulnerabilities and identifies research gaps for future advancements in design verification techniques.","sentences":["In recent years, there has been a notable surge in attention towards hardware security, driven by the increasing complexity and integration of processors, SoCs, and third-party IPs aimed at delivering advanced solutions.","However, this complexity also introduces vulnerabilities and bugs into hardware systems, necessitating early detection during the IC design cycle to uphold system integrity and mitigate re-engineering costs.","While the Design Verification (DV) community employs dynamic and formal verification strategies, they encounter challenges such as scalability for intricate designs and significant human intervention, leading to prolonged verification durations.","As an alternative approach, hardware fuzzing, inspired by software testing methodologies, has gained prominence for its efficacy in identifying bugs within complex hardware designs.","Despite the introduction of various hardware fuzzing techniques, obstacles such as inefficient conversion of hardware modules into software models impede their effectiveness.","This Systematization of Knowledge (SoK) initiative delves into the fundamental principles of existing hardware fuzzing, methodologies, and their applicability across diverse hardware designs.","Additionally, it evaluates factors such as the utilization of golden reference models (GRMs), coverage metrics, and toolchains to gauge their potential for broader adoption, akin to traditional formal verification methods.","Furthermore, this work examines the reliability of existing hardware fuzzing techniques in identifying vulnerabilities and identifies research gaps for future advancements in design verification techniques."],"url":"http://arxiv.org/abs/2403.12812v1","category":"cs.CR"}
{"created":"2024-03-19 15:07:31","title":"Stability restoration by asymmetric nonlinear states in non-Hermitian double-well potentials","abstract":"We introduce a class of one-dimensional complex optical potentials that feature a nonlinearity-induced stability restoration, i.e., the existence of stable nonlinear modes propagating in a waveguide whose linear eigenmodes are unstable. The optical potential is an even function of the transverse coordinate, i.e., the system is parity symmetric but not parity-time symmetric. The stability restoration occurs for asymmetric stationary nonlinear modes that do not respect the parity symmetry. Stable nonlinear states exist either for focusing and defocusing nonlinearities. On the qualitative level the stability restoration cab be analyzed using a simple bimodal system. Its solutions enable systematic construction of stable stationary modes and more complex patterns with intensity periodically oscillating along the propagation distance.","sentences":["We introduce a class of one-dimensional complex optical potentials that feature a nonlinearity-induced stability restoration, i.e., the existence of stable nonlinear modes propagating in a waveguide whose linear eigenmodes are unstable.","The optical potential is an even function of the transverse coordinate, i.e., the system is parity symmetric but not parity-time symmetric.","The stability restoration occurs for asymmetric stationary nonlinear modes that do not respect the parity symmetry.","Stable nonlinear states exist either for focusing and defocusing nonlinearities.","On the qualitative level the stability restoration cab be analyzed using a simple bimodal system.","Its solutions enable systematic construction of stable stationary modes and more complex patterns with intensity periodically oscillating along the propagation distance."],"url":"http://arxiv.org/abs/2403.12810v1","category":"physics.optics"}
{"created":"2024-03-19 15:00:53","title":"Introducing Combi-Stations in Robotic Mobile Fulfilment Systems: A Queueing-Theory-Based Efficiency Analysis","abstract":"In the era of digital commerce, the surge in online shopping and the expectation for rapid delivery have placed unprecedented demands on warehouse operations. The traditional method of order fulfilment, where human order pickers traverse large storage areas to pick items, has become a bottleneck, consuming valuable time and resources. Robotic Mobile Fulfilment Systems (RMFS) offer a solution by using robots to transport storage racks directly to human-operated picking stations, eliminating the need for pickers to travel. This paper introduces combi-stations, a novel type of station that enables both item picking and replenishment, as opposed to traditional separate stations. We analyse the efficiency of combi-stations using queueing theory and demonstrate their potential to streamline warehouse operations. Our results suggest that combi-stations can reduce the number of robots required for stability and significantly reduce order turnover time, indicating a promising direction for future warehouse automation.","sentences":["In the era of digital commerce, the surge in online shopping and the expectation for rapid delivery have placed unprecedented demands on warehouse operations.","The traditional method of order fulfilment, where human order pickers traverse large storage areas to pick items, has become a bottleneck, consuming valuable time and resources.","Robotic Mobile Fulfilment Systems (RMFS) offer a solution by using robots to transport storage racks directly to human-operated picking stations, eliminating the need for pickers to travel.","This paper introduces combi-stations, a novel type of station that enables both item picking and replenishment, as opposed to traditional separate stations.","We analyse the efficiency of combi-stations using queueing theory and demonstrate their potential to streamline warehouse operations.","Our results suggest that combi-stations can reduce the number of robots required for stability and significantly reduce order turnover time, indicating a promising direction for future warehouse automation."],"url":"http://arxiv.org/abs/2403.12798v1","category":"cs.RO"}
{"created":"2024-03-19 14:57:28","title":"Optical Atomic Clock Interrogation Via an Integrated Spiral Cavity Laser","abstract":"Optical atomic clocks have demonstrated revolutionary advances in precision timekeeping, but their applicability to the real world is critically dependent on whether such clocks can operate outside a laboratory setting. The challenge to clock portability stems from the many obstacles not only in miniaturizing the underlying components of the clock $-$ namely the ultrastable laser, the frequency comb, and the atomic reference itself $-$ but also in making the clock resilient to environmental fluctuations. Photonic integration offers one compelling solution to simultaneously address the problems of miniaturization and ruggedization, but brings with it a new set of challenges in recreating the functionality of an optical clock using chip-scale building blocks. The clock laser used for atom interrogation is one particular point of uncertainty, as the performance of the meticulously-engineered bulk-cavity stabilized lasers would be exceptionally difficult to transfer to chip. Here we demonstrate that a chip-integrated ultrahigh quality factor (Q) spiral cavity, when interfaced with a 1348 nm seed laser, reaches a fractional frequency instability of $7.5 \\times 10^{-14}$, meeting the stability requirements for interrogating the narrow-linewidth transition of $^{88}$Sr$^+$ upon frequency doubling to 674 nm. In addition to achieving the record for laser stability on chip, we use this laser to showcase the operation of a Sr-ion clock with short-term instability averaging down as $3.9 \\times 10^{-14} / \\sqrt{\\tau}$, where $\\tau$ is the averaging time. Our demonstration of an optical atomic clock interrogated by an integrated spiral cavity laser opens the door for future advanced clock systems to be entirely constructed using lightweight, portable, and mass-manufacturable integrated optics and electronics.","sentences":["Optical atomic clocks have demonstrated revolutionary advances in precision timekeeping, but their applicability to the real world is critically dependent on whether such clocks can operate outside a laboratory setting.","The challenge to clock portability stems from the many obstacles not only in miniaturizing the underlying components of the clock $-$ namely the ultrastable laser, the frequency comb, and the atomic reference itself $-$ but also in making the clock resilient to environmental fluctuations.","Photonic integration offers one compelling solution to simultaneously address the problems of miniaturization and ruggedization, but brings with it a new set of challenges in recreating the functionality of an optical clock using chip-scale building blocks.","The clock laser used for atom interrogation is one particular point of uncertainty, as the performance of the meticulously-engineered bulk-cavity stabilized lasers would be exceptionally difficult to transfer to chip.","Here we demonstrate that a chip-integrated ultrahigh quality factor (Q) spiral cavity, when interfaced with a 1348 nm seed laser, reaches a fractional frequency instability of $7.5 \\times 10^{-14}$, meeting the stability requirements for interrogating the narrow-linewidth transition of $^{88}$Sr$^+$ upon frequency doubling to 674 nm.","In addition to achieving the record for laser stability on chip, we use this laser to showcase the operation of a Sr-ion clock with short-term instability averaging down as $3.9 \\times 10^{-14} / \\sqrt{\\tau}$, where $\\tau$ is the averaging time.","Our demonstration of an optical atomic clock interrogated by an integrated spiral cavity laser opens the door for future advanced clock systems to be entirely constructed using lightweight, portable, and mass-manufacturable integrated optics and electronics."],"url":"http://arxiv.org/abs/2403.12794v1","category":"physics.atom-ph"}
{"created":"2024-03-19 14:54:50","title":"Importance sampling for rare event tracking within the ensemble Kalman filtering framework","abstract":"In this work we employ importance sampling (IS) techniques to track a small over-threshold probability of a running maximum associated with the solution of a stochastic differential equation (SDE) within the framework of ensemble Kalman filtering (EnKF). Between two observation times of the EnKF, we propose to use IS with respect to the initial condition of the SDE, IS with respect to the Wiener process via a stochastic optimal control formulation, and combined IS with respect to both initial condition and Wiener process. Both IS strategies require the approximation of the solution of Kolmogorov Backward equation (KBE) with boundary conditions. In multidimensional settings, we employ a Markovian projection dimension reduction technique to obtain an approximation of the solution of the KBE by just solving a one dimensional PDE. The proposed ideas are tested on two illustrative examples: Double Well SDE and Langevin dynamics, and showcase a significant variance reduction compared to the standard Monte Carlo method and another sampling-based IS technique, namely, multilevel cross entropy.","sentences":["In this work we employ importance sampling (IS) techniques to track a small over-threshold probability of a running maximum associated with the solution of a stochastic differential equation (SDE) within the framework of ensemble Kalman filtering (EnKF).","Between two observation times of the EnKF, we propose to use IS with respect to the initial condition of the SDE, IS with respect to the Wiener process via a stochastic optimal control formulation, and combined IS with respect to both initial condition and Wiener process.","Both IS strategies require the approximation of the solution of Kolmogorov Backward equation (KBE) with boundary conditions.","In multidimensional settings, we employ a Markovian projection dimension reduction technique to obtain an approximation of the solution of the KBE by just solving a one dimensional PDE.","The proposed ideas are tested on two illustrative examples: Double Well SDE and Langevin dynamics, and showcase a significant variance reduction compared to the standard Monte Carlo method and another sampling-based IS technique, namely, multilevel cross entropy."],"url":"http://arxiv.org/abs/2403.12793v1","category":"math.NA"}
{"created":"2024-03-19 14:53:11","title":"2-balanced sequences coding rectangle exchange transformation","abstract":"We define a new class of ternary sequences that are 2-balanced. These sequences are obtained by colouring of Sturmian sequences. We show that the class contains sequences of any given letter frequencies. We provide an upper bound on factor and abelian complexity of these sequences. Using the interpretation by rectangle exchange transformation, we prove that for almost all triples of letter frequencies, the upper bound on factor and abelian complexity is reached. The bound on factor complexity is given using a number-theoretical function which we compute explicitly for a class of parameters.","sentences":["We define a new class of ternary sequences that are 2-balanced.","These sequences are obtained by colouring of Sturmian sequences.","We show that the class contains sequences of any given letter frequencies.","We provide an upper bound on factor and abelian complexity of these sequences.","Using the interpretation by rectangle exchange transformation, we prove that for almost all triples of letter frequencies, the upper bound on factor and abelian complexity is reached.","The bound on factor complexity is given using a number-theoretical function which we compute explicitly for a class of parameters."],"url":"http://arxiv.org/abs/2403.12791v1","category":"math.CO"}
{"created":"2024-03-19 14:50:58","title":"Beyond Point Masses. IV. TNO Altjira is Likely a Hierarchical Triple Discovered Through Non-Keplerian Motion","abstract":"Dynamically studying Trans-Neptunian Object (TNO) binaries allows us to measure masses and orbits. Most of the known objects appear to have only two components, except (47171) Lempo which is the single known hierarchical triple system with three similar-mass components. Though hundreds of TNOs have been imaged with high-resolution telescopes, no other hierarchical triples (or trinaries) have been found among solar system small bodies, even though they are predicted in planetesimal formation models such as gravitational collapse after the streaming instability. By going beyond the point-mass assumption and modeling TNO orbits as non-Keplerian, we open a new window into the shapes and spins of the components, including the possible presence of unresolved ``inner'' binaries. Here we present evidence for a new hierarchical triple, (148780) Altjira (2001 UQ$_{18}$), based on non-Keplerian dynamical modeling of the two observed components. We incorporate two recent Hubble Space Telescope (HST) observations, leading to a 17 year observational baseline. We present a new open-source Bayesian Point Spread Function (PSF) fitting code called nPSF that provides precise relative astrometry and uncertainties for single images. Our non-Keplerian analysis measures a statistically-significant ($\\sim$2.5-$\\sigma$) non-spherical shape for Altjira. The measured $J_2$ is best explained as an unresolved inner binary and an example hierarchical triple model gives the best fit to the observed astrometry. Using an updated non-Keplerian ephemeris (which is significantly different from the Keplerian predictions), we show that the predicted mutual event season for Altjira has already begun with several excellent opportunities for observations through $\\sim$2030.","sentences":["Dynamically studying Trans-Neptunian Object (TNO) binaries allows us to measure masses and orbits.","Most of the known objects appear to have only two components, except (47171) Lempo which is the single known hierarchical triple system with three similar-mass components.","Though hundreds of TNOs have been imaged with high-resolution telescopes, no other hierarchical triples (or trinaries) have been found among solar system small bodies, even though they are predicted in planetesimal formation models such as gravitational collapse after the streaming instability.","By going beyond the point-mass assumption and modeling TNO orbits as non-Keplerian, we open a new window into the shapes and spins of the components, including the possible presence of unresolved ``inner'' binaries.","Here we present evidence for a new hierarchical triple, (148780)","Altjira (2001 UQ$_{18}$), based on non-Keplerian dynamical modeling of the two observed components.","We incorporate two recent Hubble Space Telescope (HST) observations, leading to a 17 year observational baseline.","We present a new open-source Bayesian Point Spread Function (PSF) fitting code called nPSF that provides precise relative astrometry and uncertainties for single images.","Our non-Keplerian analysis measures a statistically-significant ($\\sim$2.5-$\\sigma$) non-spherical shape for Altjira.","The measured $J_2$ is best explained as an unresolved inner binary and an example hierarchical triple model gives the best fit to the observed astrometry.","Using an updated non-Keplerian ephemeris (which is significantly different from the Keplerian predictions), we show that the predicted mutual event season for Altjira has already begun with several excellent opportunities for observations through $\\sim$2030."],"url":"http://arxiv.org/abs/2403.12786v1","category":"astro-ph.EP"}
{"created":"2024-03-19 14:50:47","title":"Beyond Point Masses. I. New Non-Keplerian Modeling Tools Applied to Trans-Neptunian Triple (47171) Lempo","abstract":"Many details of the formation and evolution of the solar system are best inferred by understanding the orbital and physical properties of small bodies in the solar system. For example, small body binaries are particularly valuable for measuring masses. By extending the models of small body binaries beyond point masses, new information about the shape and spin orientation becomes available. This is particularly informative for Trans-Neptunian multiples (two or more components) where shapes and spin orientations are poorly understood. Going beyond point masses requires modeling tools that no longer assume fixed Keplerian orbits. To this end, we have developed a new n-quadrupole integrator SPINNY (SPIN+N-bodY) and pair it with a Bayesian parameter inference tool MultiMoon, both of which are publicly available. We describe these tools and how they can be used to learn more about solar system small body multiple systems. We then apply them to the unique Trans-Neptunian hierarchical triple system (47171) Lempo, finding a three-point-mass solution for the first time. This solution has two surprises: unequal densities of the inner components and a dynamical configuration apparently unstable on the age of the solar system.","sentences":["Many details of the formation and evolution of the solar system are best inferred by understanding the orbital and physical properties of small bodies in the solar system.","For example, small body binaries are particularly valuable for measuring masses.","By extending the models of small body binaries beyond point masses, new information about the shape and spin orientation becomes available.","This is particularly informative for Trans-Neptunian multiples (two or more components) where shapes and spin orientations are poorly understood.","Going beyond point masses requires modeling tools that no longer assume fixed Keplerian orbits.","To this end, we have developed a new n-quadrupole integrator SPINNY (SPIN+N-bodY) and pair it with a Bayesian parameter inference tool MultiMoon, both of which are publicly available.","We describe these tools and how they can be used to learn more about solar system small body multiple systems.","We then apply them to the unique Trans-Neptunian hierarchical triple system (47171) Lempo, finding a three-point-mass solution for the first time.","This solution has two surprises: unequal densities of the inner components and a dynamical configuration apparently unstable on the age of the solar system."],"url":"http://arxiv.org/abs/2403.12785v1","category":"astro-ph.EP"}
{"created":"2024-03-19 14:48:37","title":"Beyond Point Masses. III. Detecting Haumea's Nonspherical Gravitational Field","abstract":"The dwarf planet Haumea is one of the most compelling transneptunian objects (TNOs) to study, hosting two small, dynamically interacting satellites, a family of nearby spectrally unique objects, and a ring system. Haumea itself is extremely oblate due to its 3.9 hour rotation period. Understanding the orbits of Haumea's satellites, named Hi'iaka and Namaka, requires detailed modeling of both satellite-satellite gravitational interactions and satellite interactions with Haumea's nonspherical gravitational field (parameterized here as $J_2$). Understanding both of these effects allows for a detailed probe of the satellites' masses and Haumea's $J_2$ and spin pole. Measuring Haumea's $J_2$ provides information about Haumea's interior, possibly determining the extent of past differentation. In an effort to understand the Haumea system, we have performed detailed non-Keplerian orbit fitting of Haumea's satellites using a decade of new ultra-precise observations. Our fits detect Haumea's $J_2$ and spin pole at $\\gtrsim2.5\\sigma$ confidence. Degeneracies present in the dynamics prevent us from precisely measuring Haumea's $J_2$ with the current data, but future observations should enable a precise measurement. Our dynamically determined spin pole shows excellent agreement with past results, illustrating the strength of non-Keplerian orbit fitting. We also explore the spin-orbit dynamics of Haumea and its satellites, showing that axial precession of Hi'iaka may be detectable over decadal timescales. Finally, we present an ephemeris of the Haumea system over the coming decade, enabling high-quality observations of Haumea and its satellites for years to come.","sentences":["The dwarf planet Haumea is one of the most compelling transneptunian objects (TNOs) to study, hosting two small, dynamically interacting satellites, a family of nearby spectrally unique objects, and a ring system.","Haumea itself is extremely oblate due to its 3.9 hour rotation period.","Understanding the orbits of Haumea's satellites, named Hi'iaka and Namaka, requires detailed modeling of both satellite-satellite gravitational interactions and satellite interactions with Haumea's nonspherical gravitational field (parameterized here as $J_2$).","Understanding both of these effects allows for a detailed probe of the satellites' masses and Haumea's $J_2$ and spin pole.","Measuring Haumea's $J_2$ provides information about Haumea's interior, possibly determining the extent of past differentation.","In an effort to understand the Haumea system, we have performed detailed non-Keplerian orbit fitting of Haumea's satellites using a decade of new ultra-precise observations.","Our fits detect Haumea's $J_2$ and spin pole at $\\gtrsim2.5\\sigma$ confidence.","Degeneracies present in the dynamics prevent us from precisely measuring Haumea's $J_2$ with the current data, but future observations should enable a precise measurement.","Our dynamically determined spin pole shows excellent agreement with past results, illustrating the strength of non-Keplerian orbit fitting.","We also explore the spin-orbit dynamics of Haumea and its satellites, showing that axial precession of Hi'iaka may be detectable over decadal timescales.","Finally, we present an ephemeris of the Haumea system over the coming decade, enabling high-quality observations of Haumea and its satellites for years to come."],"url":"http://arxiv.org/abs/2403.12782v1","category":"astro-ph.EP"}
{"created":"2024-03-19 14:45:17","title":"ViTGaze: Gaze Following with Interaction Features in Vision Transformers","abstract":"Gaze following aims to interpret human-scene interactions by predicting the person's focal point of gaze. Prevailing approaches often use multi-modality inputs, most of which adopt a two-stage framework. Hence their performance highly depends on the previous prediction accuracy. Others use a single-modality approach with complex decoders, increasing network computational load. Inspired by the remarkable success of pre-trained plain Vision Transformers (ViTs), we introduce a novel single-modality gaze following framework, ViTGaze. In contrast to previous methods, ViTGaze creates a brand new gaze following framework based mainly on powerful encoders (dec. param. less than 1%). Our principal insight lies in that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes. Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from self-attention maps. Furthermore, our investigation reveals that ViT with self-supervised pre-training exhibits an enhanced ability to extract correlated information. A large number of experiments have been conducted to demonstrate the performance of the proposed method. Our method achieves state-of-the-art (SOTA) performance among all single-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and very comparable performance against multi-modality methods with 59% number of parameters less.","sentences":["Gaze following aims to interpret human-scene interactions by predicting the person's focal point of gaze.","Prevailing approaches often use multi-modality inputs, most of which adopt a two-stage framework.","Hence their performance highly depends on the previous prediction accuracy.","Others use a single-modality approach with complex decoders, increasing network computational load.","Inspired by the remarkable success of pre-trained plain Vision Transformers (ViTs), we introduce a novel single-modality gaze following framework, ViTGaze.","In contrast to previous methods, ViTGaze creates a brand new gaze following framework based mainly on powerful encoders (dec. param.","less than 1%).","Our principal insight lies in that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes.","Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from self-attention maps.","Furthermore, our investigation reveals that ViT with self-supervised pre-training exhibits an enhanced ability to extract correlated information.","A large number of experiments have been conducted to demonstrate the performance of the proposed method.","Our method achieves state-of-the-art (SOTA) performance among all single-modality methods (3.4% improvement on AUC, 5.1% improvement on AP) and very comparable performance against multi-modality methods with 59% number of parameters less."],"url":"http://arxiv.org/abs/2403.12778v1","category":"cs.CV"}
{"created":"2024-03-19 14:34:57","title":"TYC 3340-2437-1: A Quadruple System with A Massive Star","abstract":"Hierarchical massive quadruple systems are ideal laboratories for examining the theories of star formation, dynamical evolution, and stellar evolution. The successive mergers of hierarchical quadruple systems might explain the mass gap between neutron stars and black holes. Looking for light curves of O-type binaries identified by LAMOST, we find a (2+2) quadruple system: TYC 3340-2437-1, located in the stellar bow-shock nebula (SBN). It has a probability of over 99.99\\% being a quadruple system derived from the surface density of the vicinity stars. Its inner orbital periods are 3.390602(89) days and 2.4378(16) days, respectively, and the total mass is about (11.47 + 5.79) + (5.2 + 2.02) = 24.48 $M_{\\odot}$. The line-of-sight inclinations of the inner binaries, B$_1$ and B$_2$, are 55.94 and 78.2 degrees, respectively, indicating that they are not co-planar. Based on observations spanning 34 months and the significance of the astrometric excess noise ($D>2$) in Gaia DR3 data, we guess that its outer orbital period might be a few years. If it were true, the quadruple system might form through the disk fragmentation mechanism with outer eccentric greater than zero. This eccentricity could be the cause of both the arc-like feature of the SBN and the noncoplanarity of the inner orbit. The outer orbital period and outer eccentric could be determined with the release of future epoch astrometric data of Gaia.","sentences":["Hierarchical massive quadruple systems are ideal laboratories for examining the theories of star formation, dynamical evolution, and stellar evolution.","The successive mergers of hierarchical quadruple systems might explain the mass gap between neutron stars and black holes.","Looking for light curves of O-type binaries identified by LAMOST, we find a (2+2) quadruple system: TYC 3340-2437-1, located in the stellar bow-shock nebula (SBN).","It has a probability of over 99.99\\% being a quadruple system derived from the surface density of the vicinity stars.","Its inner orbital periods are 3.390602(89) days and 2.4378(16) days, respectively, and the total mass is about (11.47 + 5.79)","+ (5.2 + 2.02) = 24.48 $M_{\\odot}$. The line-of-sight inclinations of the inner binaries, B$_1$ and B$_2$, are 55.94 and 78.2 degrees, respectively, indicating that they are not co-planar.","Based on observations spanning 34 months and the significance of the astrometric excess noise ($D>2$) in Gaia DR3 data, we guess that its outer orbital period might be a few years.","If it were true, the quadruple system might form through the disk fragmentation mechanism with outer eccentric greater than zero.","This eccentricity could be the cause of both the arc-like feature of the SBN and the noncoplanarity of the inner orbit.","The outer orbital period and outer eccentric could be determined with the release of future epoch astrometric data of Gaia."],"url":"http://arxiv.org/abs/2403.12771v1","category":"astro-ph.SR"}
{"created":"2024-03-19 14:33:16","title":"Understanding the Factors Influencing Self-Managed Enterprises of Crowdworkers: A Comprehensive Review","abstract":"This paper investigates the shift in crowdsourcing towards self-managed enterprises of crowdworkers (SMECs), diverging from traditional platform-controlled models. It reviews the literature to understand the foundational aspects of this shift, focusing on identifying key factors that may explain the rise of SMECs, particularly concerning power dynamics and tensions between Online Labor Platforms (OLPs) and crowdworkers. The study aims to guide future research and inform policy and platform development, emphasizing the importance of fair labor practices in this evolving landscape.","sentences":["This paper investigates the shift in crowdsourcing towards self-managed enterprises of crowdworkers (SMECs), diverging from traditional platform-controlled models.","It reviews the literature to understand the foundational aspects of this shift, focusing on identifying key factors that may explain the rise of SMECs, particularly concerning power dynamics and tensions between Online Labor Platforms (OLPs) and crowdworkers.","The study aims to guide future research and inform policy and platform development, emphasizing the importance of fair labor practices in this evolving landscape."],"url":"http://arxiv.org/abs/2403.12769v1","category":"cs.HC"}
{"created":"2024-03-19 14:32:21","title":"Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation","abstract":"Acquiring pixel-level annotations is often limited in applications such as histology studies that require domain expertise. Various semi-supervised learning approaches have been developed to work with limited ground truth annotations, such as the popular teacher-student models. However, hierarchical prediction uncertainty within the student model (intra-uncertainty) and image prediction uncertainty (inter-uncertainty) have not been fully utilized by existing methods. To address these issues, we first propose a novel inter- and intra-uncertainty regularization method to measure and constrain both inter- and intra-inconsistencies in the teacher-student architecture. We also propose a new two-stage network with pseudo-mask guided feature aggregation (PG-FANet) as the segmentation model. The two-stage structure complements with the uncertainty regularization strategy to avoid introducing extra modules in solving uncertainties and the aggregation mechanisms enable multi-scale and multi-stage feature integration. Comprehensive experimental results over the MoNuSeg and CRAG datasets show that our PG-FANet outperforms other state-of-the-art methods and our semi-supervised learning framework yields competitive performance with a limited amount of labeled data.","sentences":["Acquiring pixel-level annotations is often limited in applications such as histology studies that require domain expertise.","Various semi-supervised learning approaches have been developed to work with limited ground truth annotations, such as the popular teacher-student models.","However, hierarchical prediction uncertainty within the student model (intra-uncertainty) and image prediction uncertainty (inter-uncertainty) have not been fully utilized by existing methods.","To address these issues, we first propose a novel inter- and intra-uncertainty regularization method to measure and constrain both inter- and intra-inconsistencies in the teacher-student architecture.","We also propose a new two-stage network with pseudo-mask guided feature aggregation (PG-FANet) as the segmentation model.","The two-stage structure complements with the uncertainty regularization strategy to avoid introducing extra modules in solving uncertainties and the aggregation mechanisms enable multi-scale and multi-stage feature integration.","Comprehensive experimental results over the MoNuSeg and CRAG datasets show that our PG-FANet outperforms other state-of-the-art methods and our semi-supervised learning framework yields competitive performance with a limited amount of labeled data."],"url":"http://arxiv.org/abs/2403.12767v1","category":"cs.CV"}
{"created":"2024-03-19 14:29:57","title":"Smooth helically symmetric transonic flows with nonzero vorticity in a concentric cylinder","abstract":"This paper concerns the structural stability of smooth cylindrical symmetric transonic flows in a concentric cylinder under helically symmetric perturbation of suitable boundary conditions. The deformation-curl decomposition developed by the second author and his collaborator is utilized to effectively decouple the elliptic-hyperbolic mixed structure in the steady compressible Euler equation. A key parameter in the helical symmetry is the step (denoted by $\\sigma$), which denotes the magnitude of the translation along the symmetry axis after rotating one full turn. It is shown that the step determines the type of the first order partial differential system satisfied by the radial and vertical velocity. There exists a critical number $\\sigma_{*}$ depending only on the background transonic flows, such that if $0<\\sigma<\\sigma_{*}$, one can prove the existence and uniqueness of smooth helically symmetric transonic flows with nonzero vorticity.","sentences":["This paper concerns the structural stability of smooth cylindrical symmetric transonic flows in a concentric cylinder under helically symmetric perturbation of suitable boundary conditions.","The deformation-curl decomposition developed by the second author and his collaborator is utilized to effectively decouple the elliptic-hyperbolic mixed structure in the steady compressible Euler equation.","A key parameter in the helical symmetry is the step (denoted by $\\sigma$), which denotes the magnitude of the translation along the symmetry axis after rotating one full turn.","It is shown that the step determines the type of the first order partial differential system satisfied by the radial and vertical velocity.","There exists a critical number $\\sigma_{*}$ depending only on the background transonic flows, such that if $0<\\sigma<\\sigma_{*}$, one can prove the existence and uniqueness of smooth helically symmetric transonic flows with nonzero vorticity."],"url":"http://arxiv.org/abs/2403.12762v1","category":"math.AP"}
{"created":"2024-03-19 14:26:50","title":"Hurwitz moduli varieties parameterizing pointed covers of an algebraic curve with a fixed monodromy group","abstract":"Given a smooth, projective curve $Y$, a point $y_0 \\in Y$, a positive integer $n$, and a transitive subgroup $G$ of the symmetric group $S_{d}$ we study smooth, proper families, parameterized by algebraic varieties, of pointed degree $d$ covers of $(Y,y_0)$, $(X,x_{0})\\to (Y,y_0)$, branched in $n$ points of $Y\\setminus y_{0}$, whose monodromy group equals $G$. We construct explicitly a family parameterized by a Hurwitz space and prove that it is universal. We use classical tools of algebraic topology and of complex algebraic geometry.","sentences":["Given a smooth, projective curve $Y$, a point $y_0 \\in Y$, a positive integer $n$, and a transitive subgroup $G$ of the symmetric group $S_{d}$ we study smooth, proper families, parameterized by algebraic varieties, of pointed degree $d$ covers of $(Y,y_0)$, $(X,x_{0})\\to (Y,y_0)$, branched in $n$ points of $Y\\setminus y_{0}$, whose monodromy group equals $G$. We construct explicitly a family parameterized by a Hurwitz space and prove that it is universal.","We use classical tools of algebraic topology and of complex algebraic geometry."],"url":"http://arxiv.org/abs/2403.12756v1","category":"math.AG"}
{"created":"2024-03-19 14:24:43","title":"Fast event-driven simulations for soft spheres: from dynamics to Laves phase nucleation","abstract":"Conventional molecular dynamics (MD) simulations struggle when simulating particles with steeply varying interaction potentials, due to the need to use a very short time step. Here, we demonstrate that an event-driven Monte Carlo (EDMC) approach first introduced by Peters and de With [Phys. Rev. E 85, 026703 (2012)] represents an excellent substitute for MD in the canonical ensemble. In addition to correctly reproducing the static thermodynamic properties of the system, the EDMC method closely mimics the dynamics of systems of particles interacting via the steeply repulsive Weeks-Chandler-Andersen (WCA) potential. In comparison to time-driven MD simulations, EDMC runs faster by over an order of magnitude at sufficiently low temperatures. Moreover, the lack of a finite time step in EDMC circumvents the need to trade accuracy against simulation speed associated with the choice of time step in MD. We showcase the usefulness of this model to explore the phase behavior of the WCA model at extremely low temperatures, and to demonstrate that spontaneous nucleation and growth of the Laves phases is possible at temperatures significantly lower than previously reported.","sentences":["Conventional molecular dynamics (MD) simulations struggle when simulating particles with steeply varying interaction potentials, due to the need to use a very short time step.","Here, we demonstrate that an event-driven Monte Carlo (EDMC) approach first introduced by Peters and de With [Phys.","Rev. E 85, 026703 (2012)] represents an excellent substitute for MD in the canonical ensemble.","In addition to correctly reproducing the static thermodynamic properties of the system, the EDMC method closely mimics the dynamics of systems of particles interacting via the steeply repulsive Weeks-Chandler-Andersen (WCA) potential.","In comparison to time-driven MD simulations, EDMC runs faster by over an order of magnitude at sufficiently low temperatures.","Moreover, the lack of a finite time step in EDMC circumvents the need to trade accuracy against simulation speed associated with the choice of time step in MD.","We showcase the usefulness of this model to explore the phase behavior of the WCA model at extremely low temperatures, and to demonstrate that spontaneous nucleation and growth of the Laves phases is possible at temperatures significantly lower than previously reported."],"url":"http://arxiv.org/abs/2403.12755v1","category":"cond-mat.soft"}
{"created":"2024-03-19 13:54:34","title":"A new framework for constrained optimization via feedback control of Lagrange multipliers","abstract":"The continuous-time analysis of existing iterative algorithms for optimization has a long history. This work proposes a novel continuous-time control-theoretic framework for equality-constrained optimization. The key idea is to design a feedback control system where the Lagrange multipliers are the control input, and the output represents the constraints. The system converges to a stationary point of the constrained optimization problem through suitable regulation. Regarding the Lagrange multipliers, we consider two control laws: proportional-integral control and feedback linearization. These choices give rise to a family of different methods. We rigorously develop the related algorithms, theoretically analyze their convergence and present several numerical experiments to support their effectiveness concerning the state-of-the-art approaches.","sentences":["The continuous-time analysis of existing iterative algorithms for optimization has a long history.","This work proposes a novel continuous-time control-theoretic framework for equality-constrained optimization.","The key idea is to design a feedback control system where the Lagrange multipliers are the control input, and the output represents the constraints.","The system converges to a stationary point of the constrained optimization problem through suitable regulation.","Regarding the Lagrange multipliers, we consider two control laws: proportional-integral control and feedback linearization.","These choices give rise to a family of different methods.","We rigorously develop the related algorithms, theoretically analyze their convergence and present several numerical experiments to support their effectiveness concerning the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.12738v1","category":"math.OC"}
{"created":"2024-03-19 13:41:49","title":"Some geometric and topological data-driven methods in robot motion path planning","abstract":"Motion path planning is an intrinsically geometric problem which is central for design of robot systems. Since the early years of AI, robotics together with computer vision have been the areas of computer science that drove its development. Many questions that arise, such as existence, optimality, and diversity of motion paths in the configuration space that describes feasible robot configurations, are of topological nature. The recent advances in topological data analysis and related metric geometry, topology and combinatorics have provided new tools to address these engineering tasks. We will survey some questions, issues, recent work and promising directions in data-driven geometric and topological methods with some emphasis on the use of discrete Morse theory.","sentences":["Motion path planning is an intrinsically geometric problem which is central for design of robot systems.","Since the early years of AI, robotics together with computer vision have been the areas of computer science that drove its development.","Many questions that arise, such as existence, optimality, and diversity of motion paths in the configuration space that describes feasible robot configurations, are of topological nature.","The recent advances in topological data analysis and related metric geometry, topology and combinatorics have provided new tools to address these engineering tasks.","We will survey some questions, issues, recent work and promising directions in data-driven geometric and topological methods with some emphasis on the use of discrete Morse theory."],"url":"http://arxiv.org/abs/2403.12725v1","category":"math.AT"}
{"created":"2024-03-19 13:27:38","title":"Emergence of dynamical networks in termites","abstract":"Termites form complex dynamical trail networks from simple individual rules when exploring their environment. To help identify those simple rules, we reconstructed trail networks from time-lapse images of roaming termites. We quantified the trails' frequentations over time and compared them to the ones obtained by a null model. Arena borders were preferred in both simulated and observed data. Yet, the amplification phenomenon was higher with real termites, underlining the role of pheromones.","sentences":["Termites form complex dynamical trail networks from simple individual rules when exploring their environment.","To help identify those simple rules, we reconstructed trail networks from time-lapse images of roaming termites.","We quantified the trails' frequentations over time and compared them to the ones obtained by a null model.","Arena borders were preferred in both simulated and observed data.","Yet, the amplification phenomenon was higher with real termites, underlining the role of pheromones."],"url":"http://arxiv.org/abs/2403.12718v1","category":"nlin.AO"}
{"created":"2024-03-19 13:25:59","title":"Bayesian uncertainty evaluation applied to the tilted-wave interferometer","abstract":"The tilted-wave interferometer is a promising technique for the development of a reference measurement system for the highly accurate form measurement of aspheres and freeform surfaces. The technique combines interferometric measurements, acquired with a special setup, and sophisticated mathematical evaluation procedures. To determine the form of the surface under test, a computational model is required that closely mimics the measurement process of the physical measurement instruments. The parameters of the computational model, comprising the surface under test sought, are then tuned by solving an inverse problem. Due to this embedded structure of the real experiment and computational model and the overall complexity, a thorough uncertainty evaluation is challenging. In this work, a Bayesian approach is proposed to tackle the inverse problem, based on a statistical model derived from the computational model of the tilted-wave interferometer. Such a procedure naturally allows for uncertainty quantification to be made. We present an approximate inference scheme to efficiently sample quantities of the posterior using Monte Carlo sampling involving the statistical model. In particular, the methodology derived is applied to the tilted-wave interferometer to obtain an estimate and corresponding uncertainty of the pixel-by-pixel form of the surface under test for two typical surfaces taking into account a number of key influencing factors. A statistical analysis using experimental design is employed to identify main influencing factors and a subsequent analysis confirms the efficacy of the method derived.","sentences":["The tilted-wave interferometer is a promising technique for the development of a reference measurement system for the highly accurate form measurement of aspheres and freeform surfaces.","The technique combines interferometric measurements, acquired with a special setup, and sophisticated mathematical evaluation procedures.","To determine the form of the surface under test, a computational model is required that closely mimics the measurement process of the physical measurement instruments.","The parameters of the computational model, comprising the surface under test sought, are then tuned by solving an inverse problem.","Due to this embedded structure of the real experiment and computational model and the overall complexity, a thorough uncertainty evaluation is challenging.","In this work, a Bayesian approach is proposed to tackle the inverse problem, based on a statistical model derived from the computational model of the tilted-wave interferometer.","Such a procedure naturally allows for uncertainty quantification to be made.","We present an approximate inference scheme to efficiently sample quantities of the posterior using Monte Carlo sampling involving the statistical model.","In particular, the methodology derived is applied to the tilted-wave interferometer to obtain an estimate and corresponding uncertainty of the pixel-by-pixel form of the surface under test for two typical surfaces taking into account a number of key influencing factors.","A statistical analysis using experimental design is employed to identify main influencing factors and a subsequent analysis confirms the efficacy of the method derived."],"url":"http://arxiv.org/abs/2403.12715v1","category":"physics.optics"}
{"created":"2024-03-19 12:58:46","title":"Unraveling the dynamics of magnetization in topological insulator-ferromagnet heterostructures via spin-orbit torque","abstract":"Spin-orbit coupling stands as a pivotal determinant in the realm of condensed matter physics. In recent, its profound influence on spin dynamics opens up a captivating arena with promising applications. Notably, the topological insulator-ferromagnet heterostructure has been recognized for inducing spin dynamics through applied current, driven by spin-orbit torque. Building upon recent observations revealing spin flip signals within this heterostructure, our study elucidates the conditions governing spin flips by studying the magnetization dynamics. We establish that the interplay between spin-anisotropy and spin-orbit torque plays a crucial role in shaping the physics of magnetization dynamics within the heterostructure. Furthermore, we categorize various modes of magnetization dynamics, constructing a comprehensive phase diagram across distinct energy scales, damping constants, and applied frequencies. This research not only offers insights into controlling spin direction but also charts a new pathway to the practical application of spin-orbit coupled systems.","sentences":["Spin-orbit coupling stands as a pivotal determinant in the realm of condensed matter physics.","In recent, its profound influence on spin dynamics opens up a captivating arena with promising applications.","Notably, the topological insulator-ferromagnet heterostructure has been recognized for inducing spin dynamics through applied current, driven by spin-orbit torque.","Building upon recent observations revealing spin flip signals within this heterostructure, our study elucidates the conditions governing spin flips by studying the magnetization dynamics.","We establish that the interplay between spin-anisotropy and spin-orbit torque plays a crucial role in shaping the physics of magnetization dynamics within the heterostructure.","Furthermore, we categorize various modes of magnetization dynamics, constructing a comprehensive phase diagram across distinct energy scales, damping constants, and applied frequencies.","This research not only offers insights into controlling spin direction but also charts a new pathway to the practical application of spin-orbit coupled systems."],"url":"http://arxiv.org/abs/2403.12701v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-19 12:56:02","title":"System Support for Environmentally Sustainable Computing in Data Centers","abstract":"Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability. While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact. This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation. We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices. We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain.","sentences":["Modern data centers suffer from a growing carbon footprint due to insufficient support for environmental sustainability.","While hardware accelerators and renewable energy have been utilized to enhance sustainability, addressing Quality of Service (QoS) degradation caused by renewable energy supply and hardware recycling remains challenging: (1) prior accelerators exhibit significant carbon footprints due to limited reconfigurability and inability to adapt to renewable energy fluctuations; (2) integrating recycled NAND flash chips in data centers poses challenges due to their short lifetime, increasing energy consumption; (3) the absence of a sustainability estimator impedes data centers and users in evaluating and improving their environmental impact.","This study aims to improve system support for environmentally sustainable data centers by proposing a reconfigurable hardware accelerator for intensive computing primitives and developing a fractional NAND flash cell to extend the lifetime of recycled flash chips while supporting graceful capacity degradation.","We also introduce a sustainability estimator to evaluate user task energy consumption and promote sustainable practices.","We present our preliminary results and recognize this as an ongoing initiative with significant potential to advance environmentally sustainable computing in data centers and stimulate further exploration in this critical research domain."],"url":"http://arxiv.org/abs/2403.12698v1","category":"cs.AR"}
{"created":"2024-03-19 12:52:46","title":"Bayesian estimation and uncertainty quantification of a temperature-dependent thermal conductivity","abstract":"We consider the problem of estimating a temperature-dependent thermal conductivity model (curve) from temperature measurements. We apply a Bayesian estimation approach that takes into account measurement errors and limited prior information of system properties. The approach intertwines system simulation and Markov chain Monte Carlo (MCMC) sampling. We investigate the impact of assuming different model classes -- cubic polynomials and piecewise linear functions -- their parametrization, and different types of prior information -- ranging from uninformative to informative. Piecewise linear functions require more parameters (conductivity values) to be estimated than the four parameters (coefficients or conductivity values) needed for cubic polynomials. The former model class is more flexible, but the latter requires less MCMC samples. While parametrizing polynomials with coefficients may feel more natural, it turns out that parametrizing them using conductivity values is far more natural for the specification of prior information. Robust estimation is possible for all model classes and parametrizations, as long as the prior information is accurate or not too informative. Gaussian Markov random field priors are especially well-suited for piecewise linear functions.","sentences":["We consider the problem of estimating a temperature-dependent thermal conductivity model (curve) from temperature measurements.","We apply a Bayesian estimation approach that takes into account measurement errors and limited prior information of system properties.","The approach intertwines system simulation and Markov chain Monte Carlo (MCMC) sampling.","We investigate the impact of assuming different model classes -- cubic polynomials and piecewise linear functions -- their parametrization, and different types of prior information -- ranging from uninformative to informative.","Piecewise linear functions require more parameters (conductivity values) to be estimated than the four parameters (coefficients or conductivity values) needed for cubic polynomials.","The former model class is more flexible, but the latter requires less MCMC samples.","While parametrizing polynomials with coefficients may feel more natural, it turns out that parametrizing them using conductivity values is far more natural for the specification of prior information.","Robust estimation is possible for all model classes and parametrizations, as long as the prior information is accurate or not too informative.","Gaussian Markov random field priors are especially well-suited for piecewise linear functions."],"url":"http://arxiv.org/abs/2403.12696v1","category":"cs.CE"}
{"created":"2024-03-19 12:51:39","title":"As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?","abstract":"Foundation models pre-trained on web-scale vision-language data, such as CLIP, are widely used as cornerstones of powerful machine learning systems. While pre-training offers clear advantages for downstream learning, it also endows downstream models with shared adversarial vulnerabilities that can be easily identified through the open-sourced foundation model. In this work, we expose such vulnerabilities in CLIP's downstream models and show that foundation models can serve as a basis for attacking their downstream systems. In particular, we propose a simple yet effective adversarial attack strategy termed Patch Representation Misalignment (PRM). Solely based on open-sourced CLIP vision encoders, this method produces adversaries that simultaneously fool more than 20 downstream models spanning 4 common vision-language tasks (semantic segmentation, object detection, image captioning and visual question-answering). Our findings highlight the concerning safety risks introduced by the extensive usage of public foundational models in the development of downstream systems, calling for extra caution in these scenarios.","sentences":["Foundation models pre-trained on web-scale vision-language data, such as CLIP, are widely used as cornerstones of powerful machine learning systems.","While pre-training offers clear advantages for downstream learning, it also endows downstream models with shared adversarial vulnerabilities that can be easily identified through the open-sourced foundation model.","In this work, we expose such vulnerabilities in CLIP's downstream models and show that foundation models can serve as a basis for attacking their downstream systems.","In particular, we propose a simple yet effective adversarial attack strategy termed Patch Representation Misalignment (PRM).","Solely based on open-sourced CLIP vision encoders, this method produces adversaries that simultaneously fool more than 20 downstream models spanning 4 common vision-language tasks (semantic segmentation, object detection, image captioning and visual question-answering).","Our findings highlight the concerning safety risks introduced by the extensive usage of public foundational models in the development of downstream systems, calling for extra caution in these scenarios."],"url":"http://arxiv.org/abs/2403.12693v1","category":"cs.CV"}
{"created":"2024-03-19 12:47:55","title":"Stabilizing DG Methods Using Dafermos' Entropy Rate Criterion: III -- Unstructured Grids","abstract":"The approach presented in the second installment of this series is extended to multidimensional systems of conservation laws that are approximated via a Discontinuous Galerkin method on unstructured (triangular) grids. Special attention is paid to predicting the entropy dissipation from boundaries. The resulting schemes are free of tunable viscosity parameters and tested on the Euler equations. The trinity of testcases is the spreading of thermal energy from a point source, transsonic and supersonic flows around airfoils, and supersonic air inlets.","sentences":["The approach presented in the second installment of this series is extended to multidimensional systems of conservation laws that are approximated via a Discontinuous Galerkin method on unstructured (triangular) grids.","Special attention is paid to predicting the entropy dissipation from boundaries.","The resulting schemes are free of tunable viscosity parameters and tested on the Euler equations.","The trinity of testcases is the spreading of thermal energy from a point source, transsonic and supersonic flows around airfoils, and supersonic air inlets."],"url":"http://arxiv.org/abs/2403.12689v1","category":"math.NA"}
{"created":"2024-03-19 12:21:16","title":"Some properties of Higman-Thompson monoids and digital circuits","abstract":"We define various monoid versions of the R. Thompson group $V$, and prove connections with monoids of acyclic digital circuits. We show that the monoid $M_{2,1}$ (based on partial functions) is not embeddable into Thompson's monoid ${\\sf tot}M_{2,1}$, but that ${\\sf tot}M_{2,1}$ has a submonoid that maps homomorphically onto $M_{2,1}$. This leads to an efficient completion algorithm for partial functions and partial circuits. We show that the union of partial circuits with disjoint domains is an element of $M_{2,1}$, and conversely, every element of $M_{2,1}$ can be decomposed efficiently into a union of partial circuits with disjoint domains.","sentences":["We define various monoid versions of the R. Thompson group $V$, and prove connections with monoids of acyclic digital circuits.","We show that the monoid $M_{2,1}$ (based on partial functions) is not embeddable into Thompson's monoid ${\\sf tot}M_{2,1}$, but that ${\\sf tot}M_{2,1}$ has a submonoid that maps homomorphically onto $M_{2,1}$. This leads to an efficient completion algorithm for partial functions and partial circuits.","We show that the union of partial circuits with disjoint domains is an element of $M_{2,1}$, and conversely, every element of $M_{2,1}$ can be decomposed efficiently into a union of partial circuits with disjoint domains."],"url":"http://arxiv.org/abs/2403.12674v1","category":"math.GR"}
{"created":"2024-03-19 12:00:42","title":"Diffusion, viscosity and linear rheology of valence-limited disordered fluids","abstract":"We numerically investigate the dynamics and linear rheology of disordered systems made of patchy particles, focussing on the role of valence, temperature and bonding mechanism. We demonstrate that the dynamics is enslaved to bonding, giving rise to an activated behaviour at low temperature. By independently computing the diffusion constant and the viscosity from the simulations, we also confirm the validity of the Stokes-Einstein relation in valence-limited systems, with two caveats: (i) the diffusion constant requires a finite-size correction, at least at the intermediate density we investigate, and (ii) there is the onset of a breakdown that appears at the lowest temperatures considered. Finally, our results show that the storage and loss moduli of mixtures of divalent and $M$-valent particles exhibit an apparent power-law dependence on frequency, hinting at the possibility of using the composition to finely tune the rheological response of these materials. Our results compare well with literature experimental data on valence-limited DNA nanostars. In addition, the wealth of data we present and analyse here will help to develop and test theoretical frameworks aimed at describing the dynamics of flexible limited-valence particles that self-assemble into disordered networks.","sentences":["We numerically investigate the dynamics and linear rheology of disordered systems made of patchy particles, focussing on the role of valence, temperature and bonding mechanism.","We demonstrate that the dynamics is enslaved to bonding, giving rise to an activated behaviour at low temperature.","By independently computing the diffusion constant and the viscosity from the simulations, we also confirm the validity of the Stokes-Einstein relation in valence-limited systems, with two caveats: (i) the diffusion constant requires a finite-size correction, at least at the intermediate density we investigate, and (ii) there is the onset of a breakdown that appears at the lowest temperatures considered.","Finally, our results show that the storage and loss moduli of mixtures of divalent and $M$-valent particles exhibit an apparent power-law dependence on frequency, hinting at the possibility of using the composition to finely tune the rheological response of these materials.","Our results compare well with literature experimental data on valence-limited DNA nanostars.","In addition, the wealth of data we present and analyse here will help to develop and test theoretical frameworks aimed at describing the dynamics of flexible limited-valence particles that self-assemble into disordered networks."],"url":"http://arxiv.org/abs/2403.12665v1","category":"cond-mat.soft"}
{"created":"2024-03-19 11:56:15","title":"Renormalization of networks with weak geometric coupling","abstract":"The Renormalization Group is crucial for understanding systems across scales, including complex networks. Renormalizing networks via network geometry, a framework in which their topology is based on the location of nodes in a hidden metric space, is one of the foundational approaches. However, the current methods assume that the geometric coupling is strong, neglecting weak coupling in many real networks. This paper extends renormalization to weak geometric coupling, showing that geometric information is essential to preserve self-similarity. Our results underline the importance of geometric effects on network topology even when the coupling to the underlying space is weak.","sentences":["The Renormalization Group is crucial for understanding systems across scales, including complex networks.","Renormalizing networks via network geometry, a framework in which their topology is based on the location of nodes in a hidden metric space, is one of the foundational approaches.","However, the current methods assume that the geometric coupling is strong, neglecting weak coupling in many real networks.","This paper extends renormalization to weak geometric coupling, showing that geometric information is essential to preserve self-similarity.","Our results underline the importance of geometric effects on network topology even when the coupling to the underlying space is weak."],"url":"http://arxiv.org/abs/2403.12663v1","category":"physics.soc-ph"}
{"created":"2024-03-19 11:48:18","title":"Local spectral estimates and quantitative weak mixing for substitution $\\mathbb{Z}$-actions","abstract":"The paper investigates H\\\"older and log-H\\\"older regularity of spectral measures for weakly mixing substitutions and the related question of quantitative weak mixing. It is assumed that the substitution is primitive, aperiodic, and its substitution matrix is irreducible over the rationals. In the case when there are no eigenvalues of the substitution matrix on the unit circle, our main theorem says that a weakly mixing substitution $\\mathbb{Z}$-action has uniformly log-H\\\"older regular spectral measures, and hence admits power-logarithmic bounds for the rate of weak mixing. In the more delicate Salem substitution case, our second main result says that H\\\"older regularity holds for algebraic spectral parameters, but the H\\\"older exponent cannot be chosen uniformly.","sentences":["The paper investigates H\\\"older and log-H\\\"older regularity of spectral measures for weakly mixing substitutions and the related question of quantitative weak mixing.","It is assumed that the substitution is primitive, aperiodic, and its substitution matrix is irreducible over the rationals.","In the case when there are no eigenvalues of the substitution matrix on the unit circle, our main theorem says that a weakly mixing substitution $\\mathbb{Z}$-action has uniformly log-H\\\"older regular spectral measures, and hence admits power-logarithmic bounds for the rate of weak mixing.","In the more delicate Salem substitution case, our second main result says that H\\\"older regularity holds for algebraic spectral parameters, but the H\\\"older exponent cannot be chosen uniformly."],"url":"http://arxiv.org/abs/2403.12657v1","category":"math.DS"}
{"created":"2024-03-19 11:31:28","title":"Revisiting Local Computation of PageRank: Simple and Optimal","abstract":"We revisit the classic local graph exploration algorithm ApproxContributions proposed by Andersen, Borgs, Chayes, Hopcroft, Mirrokni, and Teng (WAW '07, Internet Math. '08) for computing an $\\epsilon$-approximation of the PageRank contribution vector for a target node $t$ on a graph with $n$ nodes and $m$ edges. We give a worst-case complexity bound of ApproxContributions as $O(n\\pi(t)/\\epsilon\\cdot\\min(\\Delta_{in},\\Delta_{out},\\sqrt{m}))$, where $\\pi(t)$ is the PageRank score of $t$, and $\\Delta_{in}$ and $\\Delta_{out}$ are the maximum in-degree and out-degree of the graph, resp. We also give a lower bound of $\\Omega(\\min(\\Delta_{in}/\\delta,\\Delta_{out}/\\delta,\\sqrt{m}/\\delta,m))$ for detecting the $\\delta$-contributing set of $t$, showing that the simple ApproxContributions algorithm is already optimal.   We also investigate the computational complexity of locally estimating a node's PageRank centrality. We improve the best-known upper bound of $\\widetilde{O}(n^{2/3}\\cdot\\min(\\Delta_{out}^{1/3},m^{1/6}))$ given by Bressan, Peserico, and Pretto (SICOMP '23) to $O(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ by simply combining ApproxContributions with the Monte Carlo simulation method. We also improve their lower bound of $\\Omega(\\min(n^{1/2}\\Delta_{out}^{1/2},n^{1/3}m^{1/3}))$ to $\\Omega(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ if $\\min(\\Delta_{in},\\Delta_{out})=\\Omega(n^{1/3})$, and to $\\Omega(n^{1/2-\\gamma}(\\min(\\Delta_{in},\\Delta_{out}))^{1/2+\\gamma})$ if $\\min(\\Delta_{in},\\Delta_{out})=o(n^{1/3})$, where $\\gamma>0$ is an arbitrarily small constant. Our matching upper and lower bounds resolve the open problem of whether one can tighten the bounds given by Bressan, Peserico, and Pretto (FOCS '18, SICOMP '23). Remarkably, the techniques and analyses for proving all our results are surprisingly simple.","sentences":["We revisit the classic local graph exploration algorithm ApproxContributions proposed by Andersen, Borgs, Chayes, Hopcroft, Mirrokni, and Teng (WAW '07, Internet Math.","'08) for computing an $\\epsilon$-approximation of the PageRank contribution vector for a target node $t$ on a graph with $n$ nodes and $m$ edges.","We give a worst-case complexity bound of ApproxContributions as $O(n\\pi(t)/\\epsilon\\cdot\\min(\\Delta_{in},\\Delta_{out},\\sqrt{m}))$, where $\\pi(t)$ is the PageRank score of $t$, and $\\Delta_{in}$ and $\\Delta_{out}$ are the maximum in-degree and out-degree of the graph, resp.","We also give a lower bound of $\\Omega(\\min(\\Delta_{in}/\\delta,\\Delta_{out}/\\delta,\\sqrt{m}/\\delta,m))$ for detecting the $\\delta$-contributing set of $t$, showing that the simple ApproxContributions algorithm is already optimal.   ","We also investigate the computational complexity of locally estimating a node's PageRank centrality.","We improve the best-known upper bound of $\\widetilde{O}(n^{2/3}\\cdot\\min(\\Delta_{out}^{1/3},m^{1/6}))$ given by Bressan, Peserico, and Pretto (SICOMP '23) to $O(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ by simply combining ApproxContributions with the Monte Carlo simulation method.","We also improve their lower bound of $\\Omega(\\min(n^{1/2}\\Delta_{out}^{1/2},n^{1/3}m^{1/3}))$ to $\\Omega(n^{1/2}\\cdot\\min(\\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}))$ if $\\min(\\Delta_{in},\\Delta_{out})=\\Omega(n^{1/3})$, and to $\\Omega(n^{1/2-\\gamma}(\\min(\\Delta_{in},\\Delta_{out}))^{1/2+\\gamma})$ if $\\min(\\Delta_{in},\\Delta_{out})=o(n^{1/3})$, where $\\gamma>0$ is an arbitrarily small constant.","Our matching upper and lower bounds resolve the open problem of whether one can tighten the bounds given by Bressan, Peserico, and Pretto (FOCS '18, SICOMP '23).","Remarkably, the techniques and analyses for proving all our results are surprisingly simple."],"url":"http://arxiv.org/abs/2403.12648v1","category":"cs.DS"}
{"created":"2024-03-19 11:31:13","title":"Uncertainty in the financial market and application to forecastabnormal financial fluctuations","abstract":"The integration and innovation of finance and technology have gradually transformed the financial system into a complex one. Analyses of the causesd of abnormal fluctuations in the financial market to extract early warning indicators revealed that most early warning systems are qualitative and causal. However, these models cannot be used to forecast the risk of the financial market benchmark. Therefore, from a quantitative analysis perspective, we focus on the mean and volatility uncertainties of the stock index (benchmark) and then construct three early warning indicators: mean uncertainty, volatility uncertainty, and ALM-G-value at risk. Based on the novel warning indicators, we establish a new abnormal fluctuations warning model, which will provide a short-term warning for the country, society, and individuals to reflect in advance.","sentences":["The integration and innovation of finance and technology have gradually transformed the financial system into a complex one.","Analyses of the causesd of abnormal fluctuations in the financial market to extract early warning indicators revealed that most early warning systems are qualitative and causal.","However, these models cannot be used to forecast the risk of the financial market benchmark.","Therefore, from a quantitative analysis perspective, we focus on the mean and volatility uncertainties of the stock index (benchmark) and then construct three early warning indicators: mean uncertainty, volatility uncertainty, and ALM-G-value at risk.","Based on the novel warning indicators, we establish a new abnormal fluctuations warning model, which will provide a short-term warning for the country, society, and individuals to reflect in advance."],"url":"http://arxiv.org/abs/2403.12647v1","category":"q-fin.RM"}
{"created":"2024-03-19 11:30:03","title":"When Does Your Brain Know You? Segment Length and Its Impact on EEG-based Biometric Authentication Accuracy","abstract":"In the quest for optimal EEG-based biometric authentication, this study investigates the pivotal balance for accurate identification without sacrificing performance or adding unnecessary computational complexity. Through a methodical exploration of segment durations, and employing a variety of sophisticated machine learning models, the research seeks to pinpoint a threshold where EEG data provides maximum informational yield for authentication purposes. The findings are set to advance the field of non-invasive biometric technologies, proposing a practical approach to secure and user-friendly identity verification systems while also raising considerations for the real-world application of EEG-based biometric authentication beyond controlled environments.","sentences":["In the quest for optimal EEG-based biometric authentication, this study investigates the pivotal balance for accurate identification without sacrificing performance or adding unnecessary computational complexity.","Through a methodical exploration of segment durations, and employing a variety of sophisticated machine learning models, the research seeks to pinpoint a threshold where EEG data provides maximum informational yield for authentication purposes.","The findings are set to advance the field of non-invasive biometric technologies, proposing a practical approach to secure and user-friendly identity verification systems while also raising considerations for the real-world application of EEG-based biometric authentication beyond controlled environments."],"url":"http://arxiv.org/abs/2403.12644v1","category":"cs.CR"}
{"created":"2024-03-19 11:17:34","title":"An example of the convergence of hydrodynamics in strong external fields","abstract":"The AdS/CFT correspondence is used to provide an estimate of the radius of convergence of the linearized gradient expansion of the hydrodynamic description of $\\mathcal{N}=4$ SYM theory minimally coupled to $U(1)$ gauge theory subjected to strong magnetic fields. The results of this work demonstrate that the dispersion relations of hydrodynamic modes continue to converge for magnetic field strengths far beyond those values for which a hydrodynamic description is expected. For magnetic field strengths much larger then the temperature scale the bulk dual interpolates between AdS$_{4+1}$ and $\\text{BTZ}_{2+1}\\times \\mathbb{R}^2$ and may be regarded as a renormalization group flow of the $3+1$ CFT dimensional theory to a $1+1$ CFT. In this regime, we clarify past literature on the QNM spectrum of bulk scalar fields by introducing a new way to classify the behavior of QNM collisions in the complex frequency and momentum plane.","sentences":["The AdS/CFT correspondence is used to provide an estimate of the radius of convergence of the linearized gradient expansion of the hydrodynamic description of $\\mathcal{N}=4$ SYM theory minimally coupled to $U(1)$ gauge theory subjected to strong magnetic fields.","The results of this work demonstrate that the dispersion relations of hydrodynamic modes continue to converge for magnetic field strengths far beyond those values for which a hydrodynamic description is expected.","For magnetic field strengths much larger then the temperature scale the bulk dual interpolates between AdS$_{4+1}$ and $\\text{BTZ}_{2+1}\\times \\mathbb{R}^2$ and may be regarded as a renormalization group flow of the $3+1$ CFT dimensional theory to a $1+1$ CFT.","In this regime, we clarify past literature on the QNM spectrum of bulk scalar fields by introducing a new way to classify the behavior of QNM collisions in the complex frequency and momentum plane."],"url":"http://arxiv.org/abs/2403.12638v1","category":"hep-th"}
{"created":"2024-03-19 11:05:56","title":"Compositeness of near-threshold $s$-wave resonances","abstract":"The near-threshold clustering phenomenon is well understood by the low-energy universality, for shallow bound states below the threshold. Nevertheless, the characteristics of resonances slightly above the threshold still lack thorough elucidation. We introduce a novel probabilistic interpretation scheme for complex compositeness of resonances, in which the resonances with unphysically large decay widths are inherently excluded. Employing this scheme to analyze resonances via the effective range expansion, we demonstrate that near-threshold resonances have small composite fraction, in sharp contrast to shallow bound states below the threshold.","sentences":["The near-threshold clustering phenomenon is well understood by the low-energy universality, for shallow bound states below the threshold.","Nevertheless, the characteristics of resonances slightly above the threshold still lack thorough elucidation.","We introduce a novel probabilistic interpretation scheme for complex compositeness of resonances, in which the resonances with unphysically large decay widths are inherently excluded.","Employing this scheme to analyze resonances via the effective range expansion, we demonstrate that near-threshold resonances have small composite fraction, in sharp contrast to shallow bound states below the threshold."],"url":"http://arxiv.org/abs/2403.12635v1","category":"hep-ph"}
{"created":"2024-03-19 11:00:48","title":"State Estimation Using Single Body-Frame Bearing Measurements","abstract":"This paper addresses the problem of simultaneous estimation of the position, linear velocity and orientation of a rigid body using single bearing measurements. We introduce a Riccati observer-based estimator that fuses measurements from a 3-axis accelerometer, a 3-axis gyroscope, a single body-frame vector observation (e.g., magnetometer), and a single bearing-to-landmark measurement to obtain the full vehicle's state (position, velocity, orientation). The proposed observer guarantees global exponential convergence under some persistency of excitation (PE) condition on the vehicle's motion. Simulation results are presented to show the effectiveness of the proposed approach.","sentences":["This paper addresses the problem of simultaneous estimation of the position, linear velocity and orientation of a rigid body using single bearing measurements.","We introduce a Riccati observer-based estimator that fuses measurements from a 3-axis accelerometer, a 3-axis gyroscope, a single body-frame vector observation (e.g., magnetometer), and a single bearing-to-landmark measurement to obtain the full vehicle's state (position, velocity, orientation).","The proposed observer guarantees global exponential convergence under some persistency of excitation (PE) condition on the vehicle's motion.","Simulation results are presented to show the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2403.12633v1","category":"eess.SY"}
{"created":"2024-03-19 10:57:03","title":"The structure of the f_0(980) from system size dependent hadronic resonance ratios in p+p, p+Pb, and Pb+Pb collisions at the LHC","abstract":"It is shown that the hadronic phase in ultra-relativistic heavy ion collisions can be used to understand the properties of the $f_0(980)$ resonance. In particular it is shown that the centrality dependence of the $f_0(980)/\\pi$ and $f_0(980)/\\phi$ ratios depends strongly on the $f_0(980)\\rightarrow \\overline{K}+K$ branching ratio and whether the $f_0(980)$ is produced as a $\\left | \\overline{q}q \\right \\rangle$ or $\\left | \\overline{s}s \\right \\rangle$ state. These conclusions are drawn from calculations within the partial chemical equilibrium of the HRG model within Thermal-FIST as well as with the fully non-equilibrium hybrid-transport approach UrQMD. Our findings show how the hadronic phase in heavy ion collisions can be used for studies of exotic hadron properties otherwise possible only in dedicated experiments such as PANDA.","sentences":["It is shown that the hadronic phase in ultra-relativistic heavy ion collisions can be used to understand the properties of the $f_0(980)$ resonance.","In particular it is shown that the centrality dependence of the $f_0(980)/\\pi$ and $f_0(980)/\\phi$ ratios depends strongly on the $f_0(980)\\rightarrow \\overline{K}+K$ branching ratio and whether the $f_0(980)$ is produced as a $\\left | \\overline{q}q \\right \\rangle$ or $\\left | \\overline{s}s \\right \\rangle$ state.","These conclusions are drawn from calculations within the partial chemical equilibrium of the HRG model within Thermal-FIST as well as with the fully non-equilibrium hybrid-transport approach UrQMD.","Our findings show how the hadronic phase in heavy ion collisions can be used for studies of exotic hadron properties otherwise possible only in dedicated experiments such as PANDA."],"url":"http://arxiv.org/abs/2403.12629v1","category":"hep-ph"}
{"created":"2024-03-19 10:53:34","title":"On an integrable class of Chebyshev nets","abstract":"We consider integrable curve nets in Euclidean space as a particular integrable geometry invariant with respect to rigid motions and net-preserving reparameterisations. For the purpose of their description, we first give an overview of the most important second-order invariants and relations among them. As a particular integrable example, we study curve nets satisfying an $\\mathbb R$-linear relation between the Schief curvature of the net and the Gauss curvature of the supporting surface. Starting with an $\\mathfrak{so}(3)$-valued zero-curvature representation, associated with an elliptic spectral curve, we reveal two cases when the curve degenerates. In one of these cases, when the curvatures are proportional (concordant nets), we find a correspondence to pairs of pseudospherical surfaces of equal negative constant Gaussian curvatures. The construction generalises the well-known correspondence between translation surfaces and pairs of curves.","sentences":["We consider integrable curve nets in Euclidean space as a particular integrable geometry invariant with respect to rigid motions and net-preserving reparameterisations.","For the purpose of their description, we first give an overview of the most important second-order invariants and relations among them.","As a particular integrable example, we study curve nets satisfying an $\\mathbb R$-linear relation between the Schief curvature of the net and the Gauss curvature of the supporting surface.","Starting with an $\\mathfrak{so}(3)$-valued zero-curvature representation, associated with an elliptic spectral curve, we reveal two cases when the curve degenerates.","In one of these cases, when the curvatures are proportional (concordant nets), we find a correspondence to pairs of pseudospherical surfaces of equal negative constant Gaussian curvatures.","The construction generalises the well-known correspondence between translation surfaces and pairs of curves."],"url":"http://arxiv.org/abs/2403.12626v1","category":"math.DG"}
{"created":"2024-03-19 10:52:12","title":"Existence, uniqueness and characterisation of local minimisers in higher order Calculus of Variations in $\\mathrm L^{\\infty}$","abstract":"We study variational problems for second order supremal functionals $\\mathrm F_\\infty(u)= \\|F(\\cdot,u,\\mathrm D u,\\mathrm{A}\\!:\\!\\mathrm D^2u)\\|_{\\mathrm L^{\\infty}(\\Omega)}$, where $F$ satisfies certain natural assumptions, $\\mathrm A$ is a positive matrix, and $\\Omega \\Subset \\mathbb R^n$. Higher order problems are very novel in the Calculus of Variations in $\\mathrm L^{\\infty}$, and exhibit a strikingly different behaviour compared to first order problems, for which there exists an established theory, pioneered by Aronsson in 1960s. The aim of this paper is to develop a complete theory for $\\mathrm F_\\infty$. We prove that, under appropriate conditions, ``localised\" minimisers can be characterised as solutions to a nonlinear system of PDEs, which is different from the corresponding Aronsson equation for $\\mathrm F_\\infty$; the latter is only a necessary, but not a sufficient condition for minimality. We also establish the existence and uniqueness of localised minimisers subject to Dirichlet conditions on $\\partial \\Omega$, and also their partial regularity outside a singular set of codimension one, which may be non-empty even if $n=1$.","sentences":["We study variational problems for second order supremal functionals $\\mathrm F_\\infty(u)= \\|F(\\cdot,u,\\mathrm D u,\\mathrm{A}\\!:\\!\\mathrm D^2u)\\|_{\\mathrm L^{\\infty}(\\Omega)}$, where $F$ satisfies certain natural assumptions, $\\mathrm A$ is a positive matrix, and $\\Omega \\Subset \\mathbb R^n$. Higher order problems are very novel in the Calculus of Variations in $\\mathrm L^{\\infty}$, and exhibit a strikingly different behaviour compared to first order problems, for which there exists an established theory, pioneered by Aronsson in 1960s.","The aim of this paper is to develop a complete theory for $\\mathrm F_\\infty$. We prove that, under appropriate conditions, ``localised\" minimisers can be characterised as solutions to a nonlinear system of PDEs, which is different from the corresponding Aronsson equation for $\\mathrm F_\\infty$; the latter is only a necessary, but not a sufficient condition for minimality.","We also establish the existence and uniqueness of localised minimisers subject to Dirichlet conditions on $\\partial \\Omega$, and also their partial regularity outside a singular set of codimension one, which may be non-empty even if $n=1$."],"url":"http://arxiv.org/abs/2403.12625v1","category":"math.AP"}
{"created":"2024-03-19 10:40:03","title":"Detection of Malicious Agents in Social Learning","abstract":"Social learning is a non-Bayesian framework for distributed hypothesis testing aimed at learning the true state of the environment. Traditionally, the agents are assumed to receive observations conditioned on the same true state, although it is also possible to examine the case of heterogeneous models across the graph. One important special case is when heterogeneity is caused by the presence of malicious agents whose goal is to move the agents towards a wrong hypothesis. In this work, we propose an algorithm that allows to discover the true state of every individual agent based on the sequence of their beliefs. In so doing, the methodology is also able to locate malicious behavior.","sentences":["Social learning is a non-Bayesian framework for distributed hypothesis testing aimed at learning the true state of the environment.","Traditionally, the agents are assumed to receive observations conditioned on the same true state, although it is also possible to examine the case of heterogeneous models across the graph.","One important special case is when heterogeneity is caused by the presence of malicious agents whose goal is to move the agents towards a wrong hypothesis.","In this work, we propose an algorithm that allows to discover the true state of every individual agent based on the sequence of their beliefs.","In so doing, the methodology is also able to locate malicious behavior."],"url":"http://arxiv.org/abs/2403.12619v1","category":"cs.SI"}
{"created":"2024-03-19 10:34:39","title":"Quantitative homogenization of the compressible Navier-Stokes equations towards Darcy's law","abstract":"We consider the solutions $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ to the compressible Navier-Stokes equations (NSE) in a domain periodically perforated by holes of diameter $\\varepsilon>0$. We focus on the case where the diameter of the holes is of the same order as the distance between neighboring holes. This is the same setting investigated in the paper by Masmoudi [\\url{http://www.numdam.org/article/COCV_2002__8__885_0.pdf}], where convergence $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ of the system to the porous medium equation has been shown. We prove a quantitative version of this convergence result provided that the solution of the limiting system is sufficiently regular. The proof builds on the relative energy inequality satisfied by the compressible NSE.","sentences":["We consider the solutions $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ to the compressible Navier-Stokes equations (NSE) in a domain periodically perforated by holes of diameter $\\varepsilon>0$. We focus on the case where the diameter of the holes is of the same order as the distance between neighboring holes.","This is the same setting investigated in the paper by Masmoudi [\\url{http://www.numdam.org/article/COCV_2002__8__885_0.pdf}], where convergence $\\rho_\\varepsilon, \\mathbf{u}_\\varepsilon$ of the system to the porous medium equation has been shown.","We prove a quantitative version of this convergence result provided that the solution of the limiting system is sufficiently regular.","The proof builds on the relative energy inequality satisfied by the compressible NSE."],"url":"http://arxiv.org/abs/2403.12616v1","category":"math.AP"}
{"created":"2024-03-19 10:24:47","title":"MOCCA: A Fast Algorithm for Parallel MRI Reconstruction Using Model Based Coil Calibration","abstract":"We propose a new fast algorithm for simultaneous recovery of the coil sensitivities and the magnetization image from incomplete Fourier measurements in parallel MRI. Our approach is based on suitable parameter models for both, the magnetization image and the sensitivities. The derived MOCCA algorithm provides perfect reconstruction results if the model assumptions are satisfied. Moreover, it has low computational complexity and achieves very good performance for incomplete MRI data. We present a complete mathematical analysis of the proposed reconstruction method. Most importantly, MOCCA leads to a better understanding of the connections between subspace methods and sensitivity modeling which will provide us the with the opportunity to improve also existing algorithms as ESPIRiT.","sentences":["We propose a new fast algorithm for simultaneous recovery of the coil sensitivities and the magnetization image from incomplete Fourier measurements in parallel MRI.","Our approach is based on suitable parameter models for both, the magnetization image and the sensitivities.","The derived MOCCA algorithm provides perfect reconstruction results if the model assumptions are satisfied.","Moreover, it has low computational complexity and achieves very good performance for incomplete MRI data.","We present a complete mathematical analysis of the proposed reconstruction method.","Most importantly, MOCCA leads to a better understanding of the connections between subspace methods and sensitivity modeling which will provide us the with the opportunity to improve also existing algorithms as ESPIRiT."],"url":"http://arxiv.org/abs/2403.12611v1","category":"math.NA"}
{"created":"2024-03-19 10:24:15","title":"SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition","abstract":"As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades. While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data. This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem. We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively. We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN). We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol.","sentences":["As emotions play a central role in human communication, automatic emotion recognition has attracted increasing attention in the last two decades.","While multimodal systems enjoy high performances on lab-controlled data, they are still far from providing ecological validity on non-lab-controlled, namely 'in-the-wild' data.","This work investigates audiovisual deep learning approaches for emotion recognition in-the-wild problem.","We particularly explore the effectiveness of architectures based on fine-tuned Convolutional Neural Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio modality, respectively.","We compare alternative temporal modeling and fusion strategies using the embeddings from these multi-stage trained modality-specific Deep Neural Networks (DNN).","We report results on the AffWild2 dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge protocol."],"url":"http://arxiv.org/abs/2403.12609v1","category":"cs.LG"}
{"created":"2024-03-19 10:24:14","title":"Spectrum and extension of the inverse-Compton emission of the Crab Nebula from a combined Fermi-LAT and H.E.S.S. analysis","abstract":"The Crab Nebula is a unique laboratory for studying the acceleration of electrons and positrons through their non-thermal radiation. Observations of very-high-energy $\\gamma$ rays from the Crab Nebula have provided important constraints for modelling its broadband emission. We present the first fully self-consistent analysis of the Crab Nebula's $\\gamma$-ray emission between 1 GeV and $\\sim$100 TeV, that is over five orders of magnitude in energy. Using the open-source software package Gammapy, we combine 11.4 yr of data from the Fermi Large Area Telescope and 80 h of High Energy Stereoscopic System (H.E.S.S.) data at the event level and provide a measurement of the spatial extension of the nebula and its energy spectrum. We find evidence for a shrinking of the nebula with increasing $\\gamma$-ray energy. Furthermore, we fit several phenomenological models to the measured data, finding that none of them can fully describe the spatial extension and the spectral energy distribution at the same time. Especially the extension measured at TeV energies appears too large when compared to the X-ray emission. Our measurements probe the structure of the magnetic field between the pulsar wind termination shock and the dust torus, and we conclude that the magnetic field strength decreases with increasing distance from the pulsar. We complement our study with a careful assessment of systematic uncertainties.","sentences":["The Crab Nebula is a unique laboratory for studying the acceleration of electrons and positrons through their non-thermal radiation.","Observations of very-high-energy $\\gamma$ rays from the Crab Nebula have provided important constraints for modelling its broadband emission.","We present the first fully self-consistent analysis of the Crab Nebula's $\\gamma$-ray emission between 1 GeV and $\\sim$100 TeV, that is over five orders of magnitude in energy.","Using the open-source software package Gammapy, we combine 11.4 yr of data from the Fermi Large Area Telescope and 80 h of High Energy Stereoscopic System (H.E.S.S.) data at the event level and provide a measurement of the spatial extension of the nebula and its energy spectrum.","We find evidence for a shrinking of the nebula with increasing $\\gamma$-ray energy.","Furthermore, we fit several phenomenological models to the measured data, finding that none of them can fully describe the spatial extension and the spectral energy distribution at the same time.","Especially the extension measured at TeV energies appears too large when compared to the X-ray emission.","Our measurements probe the structure of the magnetic field between the pulsar wind termination shock and the dust torus, and we conclude that the magnetic field strength decreases with increasing distance from the pulsar.","We complement our study with a careful assessment of systematic uncertainties."],"url":"http://arxiv.org/abs/2403.12608v1","category":"astro-ph.HE"}
{"created":"2024-03-19 10:17:26","title":"On the Effectiveness of Heterogeneous Ensemble Methods for Re-identification","abstract":"In this contribution, we introduce a novel ensemble method for the re-identification of industrial entities, using images of chipwood pallets and galvanized metal plates as dataset examples. Our algorithms replace commonly used, complex siamese neural networks with an ensemble of simplified, rudimentary models, providing wider applicability, especially in hardware-restricted scenarios. Each ensemble sub-model uses different types of extracted features of the given data as its input, allowing for the creation of effective ensembles in a fraction of the training duration needed for more complex state-of-the-art models. We reach state-of-the-art performance at our task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%, and introduce five distinct feature extraction approaches, and study their combination using different ensemble methods.","sentences":["In this contribution, we introduce a novel ensemble method for the re-identification of industrial entities, using images of chipwood pallets and galvanized metal plates as dataset examples.","Our algorithms replace commonly used, complex siamese neural networks with an ensemble of simplified, rudimentary models, providing wider applicability, especially in hardware-restricted scenarios.","Each ensemble sub-model uses different types of extracted features of the given data as its input, allowing for the creation of effective ensembles in a fraction of the training duration needed for more complex state-of-the-art models.","We reach state-of-the-art performance at our task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%, and introduce five distinct feature extraction approaches, and study their combination using different ensemble methods."],"url":"http://arxiv.org/abs/2403.12606v1","category":"cs.LG"}
{"created":"2024-03-19 10:14:48","title":"A Benchmark for Data Management Challenges in Microservices","abstract":"Microservice architectures emerged as a popular architecture for designing scalable distributed applications. Although microservices have been extensively employed in industry settings for over a decade, there is little understanding of the data management challenges that arise in these applications. As a result, it is difficult to advance data system technologies for supporting microservice applications. To fill this gap, we present Online Marketplace, a microservice benchmark that incorporates core data management challenges that existing benchmarks have not sufficiently addressed. These challenges include transaction processing, query processing, event processing, constraint enforcement, and data replication. We have defined criteria for various data management issues to enable proper comparison across data systems and platforms.   After specifying the benchmark, we present the challenges we faced in creating workloads that accurately reflect the dynamic state of the microservices. We also discuss implementation issues that we encountered when developing Online Marketplace in state-of-the-art data platforms, which prevented us from meeting the specified data management requirements and criteria. Our evaluation demonstrates that the benchmark is a valuable tool for testing important properties sought by microservice practitioners. As a result, our proposed benchmark will facilitate the design of future data systems to meet the expectations of microservice practitioners.","sentences":["Microservice architectures emerged as a popular architecture for designing scalable distributed applications.","Although microservices have been extensively employed in industry settings for over a decade, there is little understanding of the data management challenges that arise in these applications.","As a result, it is difficult to advance data system technologies for supporting microservice applications.","To fill this gap, we present Online Marketplace, a microservice benchmark that incorporates core data management challenges that existing benchmarks have not sufficiently addressed.","These challenges include transaction processing, query processing, event processing, constraint enforcement, and data replication.","We have defined criteria for various data management issues to enable proper comparison across data systems and platforms.   ","After specifying the benchmark, we present the challenges we faced in creating workloads that accurately reflect the dynamic state of the microservices.","We also discuss implementation issues that we encountered when developing Online Marketplace in state-of-the-art data platforms, which prevented us from meeting the specified data management requirements and criteria.","Our evaluation demonstrates that the benchmark is a valuable tool for testing important properties sought by microservice practitioners.","As a result, our proposed benchmark will facilitate the design of future data systems to meet the expectations of microservice practitioners."],"url":"http://arxiv.org/abs/2403.12605v1","category":"cs.DB"}
{"created":"2024-03-19 10:12:30","title":"Integrated distributed sensing and quantum communication networks","abstract":"The integration of sensing and communication can achieve ubiquitous sensing while enabling ubiquitous communication. Within the gradually improving global communication, the integrated sensing and communication (ISAC) system based on optical fibers can accomplish various functionalities, such as urban structure imaging, seismic wave detection, and pipeline safety monitoring. With the development of quantum communication, quantum networks based on optical fiber are gradually being established. In this paper, we propose an integrated sensing and quantum network (ISAQN) scheme, which can achieve secure key distribution among multiple nodes and distributed sensing under the standard quantum limit. CV-QKD protocol and the round-trip multi-band structure are adopted to achieve the multi-node secure key distribution. Meanwhile, the spectrum phase monitoring (SPM) protocol is proposed to realize distributed sensing. It determines which node is vibrating by monitoring the frequency spectrum and restores the vibration waveform by monitoring the phase change. The scheme is experimentally demonstrated by simulating the vibration in a star structure network. Experimental results indicate that this multi-user quantum network can achieve a secret key rate (SKR) of approximately 0.7 $\\rm{Mbits/s}$ for each user under 10 $\\rm{km}$ standard fiber transmission and its network capacity is 8. In terms of distributed sensing, it can achieve a vibration response bandwidth ranging from 1 $\\rm{Hz}$ to 2 $\\rm{kHz}$, a strain resolution of 0.50 $\\rm{n}$$\\varepsilon$$/\\sqrt{\\rm{Hz}}$, and a spatial resolution of 0.20 $\\rm{m}$ under shot-noise-limited detection. The proposed ISAQN scheme enables simultaneous quantum communication and distributed sensing in a multi-point network, laying a foundation for future large-scale quantum networks and high-precision sensing networks.","sentences":["The integration of sensing and communication can achieve ubiquitous sensing while enabling ubiquitous communication.","Within the gradually improving global communication, the integrated sensing and communication (ISAC) system based on optical fibers can accomplish various functionalities, such as urban structure imaging, seismic wave detection, and pipeline safety monitoring.","With the development of quantum communication, quantum networks based on optical fiber are gradually being established.","In this paper, we propose an integrated sensing and quantum network (ISAQN) scheme, which can achieve secure key distribution among multiple nodes and distributed sensing under the standard quantum limit.","CV-QKD protocol and the round-trip multi-band structure are adopted to achieve the multi-node secure key distribution.","Meanwhile, the spectrum phase monitoring (SPM) protocol is proposed to realize distributed sensing.","It determines which node is vibrating by monitoring the frequency spectrum and restores the vibration waveform by monitoring the phase change.","The scheme is experimentally demonstrated by simulating the vibration in a star structure network.","Experimental results indicate that this multi-user quantum network can achieve a secret key rate (SKR) of approximately 0.7 $\\rm{Mbits/s}$ for each user under 10 $\\rm{km}$ standard fiber transmission and its network capacity is 8.","In terms of distributed sensing, it can achieve a vibration response bandwidth ranging from 1 $\\rm{Hz}$ to 2 $\\rm{kHz}$, a strain resolution of 0.50 $\\rm{n}$$\\varepsilon$$/\\sqrt{\\rm{Hz}}$, and a spatial resolution of 0.20 $\\rm{m}$ under shot-noise-limited detection.","The proposed ISAQN scheme enables simultaneous quantum communication and distributed sensing in a multi-point network, laying a foundation for future large-scale quantum networks and high-precision sensing networks."],"url":"http://arxiv.org/abs/2403.12602v1","category":"quant-ph"}
{"created":"2024-03-19 10:09:41","title":"Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance","abstract":"Rental assistance programs provide individuals with financial assistance to prevent housing instabilities caused by evictions and avert homelessness. Since these programs operate under resource constraints, they must decide who to prioritize. Typically, funding is distributed by a reactive or first-come-first serve allocation process that does not systematically consider risk of future homelessness. We partnered with Allegheny County, PA to explore a proactive allocation approach that prioritizes individuals facing eviction based on their risk of future homelessness. Our ML system that uses state and county administrative data to accurately identify individuals in need of support outperforms simpler prioritization approaches by at least 20% while being fair and equitable across race and gender. Furthermore, our approach would identify 28% of individuals who are overlooked by the current process and end up homeless. Beyond improvements to the rental assistance program in Allegheny County, this study can inform the development of evidence-based decision support tools in similar contexts, including lessons about data needs, model design, evaluation, and field validation.","sentences":["Rental assistance programs provide individuals with financial assistance to prevent housing instabilities caused by evictions and avert homelessness.","Since these programs operate under resource constraints, they must decide who to prioritize.","Typically, funding is distributed by a reactive or first-come-first serve allocation process that does not systematically consider risk of future homelessness.","We partnered with Allegheny County, PA to explore a proactive allocation approach that prioritizes individuals facing eviction based on their risk of future homelessness.","Our ML system that uses state and county administrative data to accurately identify individuals in need of support outperforms simpler prioritization approaches by at least 20% while being fair and equitable across race and gender.","Furthermore, our approach would identify 28% of individuals who are overlooked by the current process and end up homeless.","Beyond improvements to the rental assistance program in Allegheny County, this study can inform the development of evidence-based decision support tools in similar contexts, including lessons about data needs, model design, evaluation, and field validation."],"url":"http://arxiv.org/abs/2403.12599v1","category":"cs.CY"}
{"created":"2024-03-19 10:08:14","title":"Effects of forward disorder on quasi-1D superconductors","abstract":"We study the competition between disorder and singlet superconductivity in a quasi-1d system. We investigate the applicability of the Anderson theorem, namely that time-reversal conserving (non-magnetic) disorder does not impact the critical temperature, by opposition to time-reversal breaking disorder (magnetic). To do so we examine a quasi-1d system of spin 1/2 fermions with attractive interactions and forward scattering disorder using field theory (bosonization). By computing the superconducting critical temperature ($T_c$), we find that for non-magnetic disorder the Anderson theorem also holds in the quasi-1D geometry. On the contrary, magnetic disorder has an impact on the critical temperature, that we investigate by deriving renormalization group (RG) equations describing the competition between the disorder and the interactions. Computing the critical temperature as a function of disorder strength, we see that different regimes arise depending on the strength of interactions. We discuss possible platforms where to observe this in cold atoms and condensed matter.","sentences":["We study the competition between disorder and singlet superconductivity in a quasi-1d system.","We investigate the applicability of the Anderson theorem, namely that time-reversal conserving (non-magnetic) disorder does not impact the critical temperature, by opposition to time-reversal breaking disorder (magnetic).","To do so we examine a quasi-1d system of spin 1/2 fermions with attractive interactions and forward scattering disorder using field theory (bosonization).","By computing the superconducting critical temperature ($T_c$), we find that for non-magnetic disorder the Anderson theorem also holds in the quasi-1D geometry.","On the contrary, magnetic disorder has an impact on the critical temperature, that we investigate by deriving renormalization group (RG) equations describing the competition between the disorder and the interactions.","Computing the critical temperature as a function of disorder strength, we see that different regimes arise depending on the strength of interactions.","We discuss possible platforms where to observe this in cold atoms and condensed matter."],"url":"http://arxiv.org/abs/2403.12597v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-19 10:01:25","title":"Ensuring Solution Uniqueness in Fixed-Point-Based Harmonic Power Flow Analysis with Converter-Interfaced Resources: Ex-post Conditions","abstract":"Recently, the authors of this paper proposed a method for the Harmonic Power-Flow (HPF) calculus in polyphase grids with widespread deployment of Converter-Interfaced Distributed Energy Resources (CIDERs). The HPF problem was formulated by integrating the hybrid nodal equations of the grid with a detailed representation of the CIDERs hardware, sensing, and controls as Linear Time-Periodic (LTP) systems, and solving the resulting mismatch equations using the Newton-Raphson (NR) method. This work introduces a novel problem formulation based on the fixed-point algorithm that, combined with the contraction property of the HPF problem, provides insights into the uniqueness of its solution. Notably, the effectiveness of the fixed-point formulation and the uniqueness of the solution are evaluated through numerical analyses conducted on a modified version of the CIGRE low-voltage benchmark microgrid.","sentences":["Recently, the authors of this paper proposed a method for the Harmonic Power-Flow (HPF) calculus in polyphase grids with widespread deployment of Converter-Interfaced Distributed Energy Resources (CIDERs).","The HPF problem was formulated by integrating the hybrid nodal equations of the grid with a detailed representation of the CIDERs hardware, sensing, and controls as Linear Time-Periodic (LTP) systems, and solving the resulting mismatch equations using the Newton-Raphson (NR) method.","This work introduces a novel problem formulation based on the fixed-point algorithm that, combined with the contraction property of the HPF problem, provides insights into the uniqueness of its solution.","Notably, the effectiveness of the fixed-point formulation and the uniqueness of the solution are evaluated through numerical analyses conducted on a modified version of the CIGRE low-voltage benchmark microgrid."],"url":"http://arxiv.org/abs/2403.12595v1","category":"eess.SY"}
{"created":"2024-03-19 09:52:03","title":"Discrepancies in dynamic yield stress measurements of cement pastes","abstract":"The dynamic yield stress associated with the flow cessation of cement pastes is measured using a rheometer equipped with various shear geometries such as vane, helical, sandblasted co-axial cylinders, and serrated parallel plates, as well as with the mini-cone spread test. Discrepancies in yield stress values are observed for cement pastes at various volume fractions, with one to two orders of magnitude difference between vane, helical and mini-cone spread measurements on the one hand, and co-axial cylinder and parallel plate measurements on the other hand. To understand this discrepancy, the flow profile of a cement paste in the parallel-plate geometry is investigated with a high-speed camera, revealing the rapid formation of an un-sheared band near the static bottom plate. The width of this band depends upon the rotational velocity of the top plate, and upon the shear time. Recalculation of shear stress shows that the reduced sheared gap alone cannot explain the low measured yield stress. Further exploration suggests the formation of zones with lower particle content, possibly linked to cement particle sedimentation. Here, we argue that the complex nature of cement pastes, composed of negatively buoyant non-Brownian particles with attractive interactions due to highly charged nano-size hydration products, accounts for their complex rheological behavior.","sentences":["The dynamic yield stress associated with the flow cessation of cement pastes is measured using a rheometer equipped with various shear geometries such as vane, helical, sandblasted co-axial cylinders, and serrated parallel plates, as well as with the mini-cone spread test.","Discrepancies in yield stress values are observed for cement pastes at various volume fractions, with one to two orders of magnitude difference between vane, helical and mini-cone spread measurements on the one hand, and co-axial cylinder and parallel plate measurements on the other hand.","To understand this discrepancy, the flow profile of a cement paste in the parallel-plate geometry is investigated with a high-speed camera, revealing the rapid formation of an un-sheared band near the static bottom plate.","The width of this band depends upon the rotational velocity of the top plate, and upon the shear time.","Recalculation of shear stress shows that the reduced sheared gap alone cannot explain the low measured yield stress.","Further exploration suggests the formation of zones with lower particle content, possibly linked to cement particle sedimentation.","Here, we argue that the complex nature of cement pastes, composed of negatively buoyant non-Brownian particles with attractive interactions due to highly charged nano-size hydration products, accounts for their complex rheological behavior."],"url":"http://arxiv.org/abs/2403.12593v1","category":"cond-mat.soft"}
{"created":"2024-03-19 09:50:32","title":"Efficient pore space characterization based on the curvature of the distance map","abstract":"Media classification and the construction of pore network models from binary images of porous media hinges on accurately characterizing the pore space. We present an efficient method for (i) locating critical points, that is, pore body and throat centers, and (ii) partitioning of the pore space using information on the curvature of the distance map (DM) of the binary image. Specifically, we use the local maxima and minima of the determinant map of the Hessian matrix of the DM to locate the center of pore bodies and throats. The locating step provides structural information on the pore system, such as pore body and throat size distributions and the mean coordination number. The partitioning step is based on the eigenvalues of the Hessian, rather than the DM, to characterize the pore space using either watershed or medial axis transforms. This strategy eliminates the common problem of saddle-induced over-partitioning shared by all traditional marker-based watershed methods and represents an efficient method to determine the skeleton of the pore space without the need for morphological reconstruction.","sentences":["Media classification and the construction of pore network models from binary images of porous media hinges on accurately characterizing the pore space.","We present an efficient method for (i) locating critical points, that is, pore body and throat centers, and (ii) partitioning of the pore space using information on the curvature of the distance map (DM) of the binary image.","Specifically, we use the local maxima and minima of the determinant map of the Hessian matrix of the DM to locate the center of pore bodies and throats.","The locating step provides structural information on the pore system, such as pore body and throat size distributions and the mean coordination number.","The partitioning step is based on the eigenvalues of the Hessian, rather than the DM, to characterize the pore space using either watershed or medial axis transforms.","This strategy eliminates the common problem of saddle-induced over-partitioning shared by all traditional marker-based watershed methods and represents an efficient method to determine the skeleton of the pore space without the need for morphological reconstruction."],"url":"http://arxiv.org/abs/2403.12591v1","category":"physics.geo-ph"}
{"created":"2024-03-19 09:47:08","title":"LASPA: Latent Spatial Alignment for Fast Training-free Single Image Editing","abstract":"We present a novel, training-free approach for textual editing of real images using diffusion models. Unlike prior methods that rely on computationally expensive finetuning, our approach leverages LAtent SPatial Alignment (LASPA) to efficiently preserve image details. We demonstrate how the diffusion process is amenable to spatial guidance using a reference image, leading to semantically coherent edits. This eliminates the need for complex optimization and costly model finetuning, resulting in significantly faster editing compared to previous methods. Additionally, our method avoids the storage requirements associated with large finetuned models. These advantages make our approach particularly well-suited for editing on mobile devices and applications demanding rapid response times. While simple and fast, our method achieves 62-71\\% preference in a user-study and significantly better model-based editing strength and image preservation scores.","sentences":["We present a novel, training-free approach for textual editing of real images using diffusion models.","Unlike prior methods that rely on computationally expensive finetuning, our approach leverages LAtent SPatial Alignment (LASPA) to efficiently preserve image details.","We demonstrate how the diffusion process is amenable to spatial guidance using a reference image, leading to semantically coherent edits.","This eliminates the need for complex optimization and costly model finetuning, resulting in significantly faster editing compared to previous methods.","Additionally, our method avoids the storage requirements associated with large finetuned models.","These advantages make our approach particularly well-suited for editing on mobile devices and applications demanding rapid response times.","While simple and fast, our method achieves 62-71\\% preference in a user-study and significantly better model-based editing strength and image preservation scores."],"url":"http://arxiv.org/abs/2403.12585v1","category":"cs.CV"}
{"created":"2024-03-19 09:46:17","title":"Robust Fuel-Optimal Landing Guidance for Hazardous Terrain using Multiple Sliding Surfaces","abstract":"In any spacecraft landing mission, precision soft landing in a fuel-efficient way while also avoiding nearby hazardous terrain is of utmost importance. Very few existing literature have attempted addressing both the problems of precision soft landing and terrain avoidance simultaneously. To this end, an optimal terrain avoidance landing guidance (OTALG) was recently developed, which showed promising performance in avoiding the terrain while consuming near-minimum fuel. However, its performance significantly degrades in the face of external disturbances, indicating lack of robustness. In order to mitigate this problem, in this paper, a novel near fuel-optimal guidance law is developed to avoid terrain and land precisely and softly at the desired landing site, under atmospheric perturbations and thrust deviations and constraints. Expanding the OTALG formulation by using sliding mode control with multiple sliding surfaces (MSS), the presented guidance law, named `MSS-OTALG', improves in terms of precision soft landing accuracy. Further, the sliding parameter is designed to allow the lander to avoid terrain by leaving the trajectory enforced by the sliding mode, and eventually returning to it when the terrain avoidance phase is completed. And finally, the robustness of the MSS-OTALG is established by proving practical fixed-time stability. Extensive numerical simulations are also presented to showcase its performance in terms of terrain avoidance, low fuel consumption and accuracy of precision soft landing. Comparative studies against existing relevant literature validates a balanced trade-off of all these performance measures achieved by the developed MSS-OTALG.","sentences":["In any spacecraft landing mission, precision soft landing in a fuel-efficient way while also avoiding nearby hazardous terrain is of utmost importance.","Very few existing literature have attempted addressing both the problems of precision soft landing and terrain avoidance simultaneously.","To this end, an optimal terrain avoidance landing guidance (OTALG) was recently developed, which showed promising performance in avoiding the terrain while consuming near-minimum fuel.","However, its performance significantly degrades in the face of external disturbances, indicating lack of robustness.","In order to mitigate this problem, in this paper, a novel near fuel-optimal guidance law is developed to avoid terrain and land precisely and softly at the desired landing site, under atmospheric perturbations and thrust deviations and constraints.","Expanding the OTALG formulation by using sliding mode control with multiple sliding surfaces (MSS), the presented guidance law, named `MSS-OTALG', improves in terms of precision soft landing accuracy.","Further, the sliding parameter is designed to allow the lander to avoid terrain by leaving the trajectory enforced by the sliding mode, and eventually returning to it when the terrain avoidance phase is completed.","And finally, the robustness of the MSS-OTALG is established by proving practical fixed-time stability.","Extensive numerical simulations are also presented to showcase its performance in terms of terrain avoidance, low fuel consumption and accuracy of precision soft landing.","Comparative studies against existing relevant literature validates a balanced trade-off of all these performance measures achieved by the developed MSS-OTALG."],"url":"http://arxiv.org/abs/2403.12584v1","category":"eess.SY"}
{"created":"2024-03-19 09:45:32","title":"An Upper Bound on the Weisfeiler-Leman Dimension","abstract":"The Weisfeiler-Leman (WL) dimension is a standard measure in descriptive complexity theory for the structural complexity of a graph. We prove that the WL-dimension of a graph on $n$ vertices is at most $3/20 \\cdot n + o(n)= 0.15 \\cdot n + o(n)$. The proof develops various techniques to analyze the structure of coherent configurations.   This includes sufficient conditions under which a fiber can be restored up to isomorphism if it is removed, a recursive proof exploiting a degree reduction and treewidth bounds, as well as an analysis of interspaces involving small fibers.   As a base case, we also analyze the dimension of coherent configurations with small fiber size and thereby graphs with small color class size.","sentences":["The Weisfeiler-Leman (WL) dimension is a standard measure in descriptive complexity theory for the structural complexity of a graph.","We prove that the WL-dimension of a graph on $n$ vertices is at most $3/20 \\cdot n + o(n)= 0.15 \\cdot n","+","o(n)$. The proof develops various techniques to analyze the structure of coherent configurations.   ","This includes sufficient conditions under which a fiber can be restored up to isomorphism if it is removed, a recursive proof exploiting a degree reduction and treewidth bounds, as well as an analysis of interspaces involving small fibers.   ","As a base case, we also analyze the dimension of coherent configurations with small fiber size and thereby graphs with small color class size."],"url":"http://arxiv.org/abs/2403.12581v1","category":"cs.DM"}
{"created":"2024-03-19 09:30:40","title":"Optimizing Reconfigurable Antenna MIMO Systems with Coherent Ising Machines","abstract":"Reconfigurable antenna multiple-input multiple-output (MIMO) is a promising technology for upcoming 6G communication systems. In this paper, we deal with the problem of configuration selection for reconfigurable antenna MIMO by leveraging Coherent Ising Machines (CIMs). By adopting the CIM as a heuristic solver for the Ising problem, the optimal antenna configuration that maximizes the received signal-to-noise ratio is investigated. A mathematical framework that converts the selection problem into a CIM-compatible unconstrained quadratic formulation is presented. Numerical studies show that the proposed CIM-based design outperforms classical counterparts and achieves near-optimal performance (similar to exponentially complex exhaustive searching) while ensuring polynomial complexity.","sentences":["Reconfigurable antenna multiple-input multiple-output (MIMO) is a promising technology for upcoming 6G communication systems.","In this paper, we deal with the problem of configuration selection for reconfigurable antenna MIMO by leveraging Coherent Ising Machines (CIMs).","By adopting the CIM as a heuristic solver for the Ising problem, the optimal antenna configuration that maximizes the received signal-to-noise ratio is investigated.","A mathematical framework that converts the selection problem into a CIM-compatible unconstrained quadratic formulation is presented.","Numerical studies show that the proposed CIM-based design outperforms classical counterparts and achieves near-optimal performance (similar to exponentially complex exhaustive searching) while ensuring polynomial complexity."],"url":"http://arxiv.org/abs/2403.12571v1","category":"cs.IT"}
{"created":"2024-03-19 09:15:20","title":"A Bayesian multilevel hidden Markov model with Poisson-lognormal emissions for intense longitudinal count data","abstract":"Hidden Markov models (HMMs) are probabilistic methods in which observations are seen as realizations of a latent Markov process with discrete states that switch over time. Moving beyond standard statistical tests, HMMs offer a statistical environment to optimally exploit the information present in multivariate time series, uncovering the latent dynamics that rule them. Here, we extend the Poisson HMM to the multilevel framework, accommodating variability between individuals with continuously distributed individual random effects following a lognormal distribution, and describe how to estimate the model in a fully parametric Bayesian framework. The proposed multilevel HMM enables probabilistic decoding of hidden state sequences from multivariate count time-series based on individual-specific parameters, and offers a framework to quantificate between-individual variability formally. Through a Monte Carlo study we show that the multilevel HMM outperforms the HMM for scenarios involving heterogeneity between individuals, demonstrating improved decoding accuracy and estimation performance of parameters of the emission distribution, and performs equally well when not between heterogeneity is present. Finally, we illustrate how to use our model to explore the latent dynamics governing complex multivariate count data in an empirical application concerning pilot whale diving behaviour in the wild, and how to identify neural states from multi-electrode recordings of motor neural cortex activity in a macaque monkey in an experimental set up. We make the multilevel HMM introduced in this study publicly available in the R-package mHMMbayes in CRAN.","sentences":["Hidden Markov models (HMMs) are probabilistic methods in which observations are seen as realizations of a latent Markov process with discrete states that switch over time.","Moving beyond standard statistical tests, HMMs offer a statistical environment to optimally exploit the information present in multivariate time series, uncovering the latent dynamics that rule them.","Here, we extend the Poisson HMM to the multilevel framework, accommodating variability between individuals with continuously distributed individual random effects following a lognormal distribution, and describe how to estimate the model in a fully parametric Bayesian framework.","The proposed multilevel HMM enables probabilistic decoding of hidden state sequences from multivariate count time-series based on individual-specific parameters, and offers a framework to quantificate between-individual variability formally.","Through a Monte Carlo study we show that the multilevel HMM outperforms the HMM for scenarios involving heterogeneity between individuals, demonstrating improved decoding accuracy and estimation performance of parameters of the emission distribution, and performs equally well when not between heterogeneity is present.","Finally, we illustrate how to use our model to explore the latent dynamics governing complex multivariate count data in an empirical application concerning pilot whale diving behaviour in the wild, and how to identify neural states from multi-electrode recordings of motor neural cortex activity in a macaque monkey in an experimental set up.","We make the multilevel HMM introduced in this study publicly available in the R-package mHMMbayes in CRAN."],"url":"http://arxiv.org/abs/2403.12561v1","category":"stat.ME"}
{"created":"2024-03-19 08:56:24","title":"Non-negative tensor factorization for vibration-based local damage detection","abstract":"In this study, a novel non-negative tensor factorization (NTF)-based method for vibration-based local damage detection in rolling element bearings is proposed. As the diagnostic signal registered from a faulty machine is non-stationary, the time-frequency method is frequently used as a primary decomposition technique. It is proposed here to extract multi-linear NTF-based components from a 3D array of time-frequency representations of an observed signal partitioned into blocks. As a result, frequency and temporal informative components can be efficiently separated from non-informative ones. The experiments performed on synthetic and real signals demonstrate the high efficiency of the proposed method with respect to the already known non-negative matrix factorization approach.","sentences":["In this study, a novel non-negative tensor factorization (NTF)-based method for vibration-based local damage detection in rolling element bearings is proposed.","As the diagnostic signal registered from a faulty machine is non-stationary, the time-frequency method is frequently used as a primary decomposition technique.","It is proposed here to extract multi-linear NTF-based components from a 3D array of time-frequency representations of an observed signal partitioned into blocks.","As a result, frequency and temporal informative components can be efficiently separated from non-informative ones.","The experiments performed on synthetic and real signals demonstrate the high efficiency of the proposed method with respect to the already known non-negative matrix factorization approach."],"url":"http://arxiv.org/abs/2403.12554v1","category":"eess.SP"}
{"created":"2024-03-19 08:56:20","title":"Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs","abstract":"Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data. To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to the function space. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-NO to outperform existing methods on the few-shot learning task by over $36\\%$. The code is available at https://github.com/ashiq24/CoDA-NO.","sentences":["Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs), due to complex geometries, interactions between physical variables, and the lack of large amounts of high-resolution training data.","To address these issues, we propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems.","Specifically, we extend positional encoding, self-attention, and normalization layers to the function space.","CoDA-NO can learn representations of different PDE systems with a single model.","We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings.","On complex downstream tasks with limited data, such as fluid flow simulations and fluid-structure interactions, we found CoDA-NO to outperform existing methods on the few-shot learning task by over $36\\%$. The code is available at https://github.com/ashiq24/CoDA-NO."],"url":"http://arxiv.org/abs/2403.12553v1","category":"cs.LG"}
{"created":"2024-03-19 08:48:09","title":"Kinetically constrained models constructed from dissipative quantum dynamics","abstract":"We propose a construction of kinetically constrained models using the Markovian quantum dynamics under strong dissipation. Using the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) formalism, we show that strong dissipation leads to the emergent decoherence-free subspaces, within which constrained quantum many-body unitary dynamics can take place. We argue that the unitary dynamics constructed by the GKSL dynamics is more tightly constrained than that constructed by the strongly interacting Hamiltonian, where the interactions have the same form with the GKSL jump operators. As an example, we demonstrate that a one-dimensional spin system with two-site dissipation leads to the kinetically constrained ``PXQ\" model, which exhibits the free domain-wall motion with an additional frozen-block structure. Under the uniform magnetic field, the PXQ model shows the domain-wall localization, similar to the Wannier-Stark localization. We then couple two PXQ chains with the magnetic field and inter-chain interaction. We discover that, while localization of the domain walls persists despite the interactions for typical parameter regimes, a non-trivial partial delocalization appears for a certain parameter line.","sentences":["We propose a construction of kinetically constrained models using the Markovian quantum dynamics under strong dissipation.","Using the Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) formalism, we show that strong dissipation leads to the emergent decoherence-free subspaces, within which constrained quantum many-body unitary dynamics can take place.","We argue that the unitary dynamics constructed by the GKSL dynamics is more tightly constrained than that constructed by the strongly interacting Hamiltonian, where the interactions have the same form with the GKSL jump operators.","As an example, we demonstrate that a one-dimensional spin system with two-site dissipation leads to the kinetically constrained ``PXQ\" model, which exhibits the free domain-wall motion with an additional frozen-block structure.","Under the uniform magnetic field, the PXQ model shows the domain-wall localization, similar to the Wannier-Stark localization.","We then couple two PXQ chains with the magnetic field and inter-chain interaction.","We discover that, while localization of the domain walls persists despite the interactions for typical parameter regimes, a non-trivial partial delocalization appears for a certain parameter line."],"url":"http://arxiv.org/abs/2403.12548v1","category":"quant-ph"}
{"created":"2024-03-19 08:46:41","title":"A two-scale effective model for defect-induced localization transitions in non-Hermitian systems","abstract":"We illuminate the fundamental mechanism responsible for the transition between the non-Hermitian skin effect and defect-induced localization in the bulk. We study a Hamiltonian with non-reciprocal couplings that exhibits the skin effect (the localization of all eigenvectors at one edge) and add an on-site defect in the center. Using a two-scale asymptotic method, we characterize the long-scale growth and decay of the eigenvectors and derive a simple and intuitive effective model for the transition that occurs when the defect is sufficiently large that one of the modes is localized at the defect site, rather than at the edge of the system.","sentences":["We illuminate the fundamental mechanism responsible for the transition between the non-Hermitian skin effect and defect-induced localization in the bulk.","We study a Hamiltonian with non-reciprocal couplings that exhibits the skin effect (the localization of all eigenvectors at one edge) and add an on-site defect in the center.","Using a two-scale asymptotic method, we characterize the long-scale growth and decay of the eigenvectors and derive a simple and intuitive effective model for the transition that occurs when the defect is sufficiently large that one of the modes is localized at the defect site, rather than at the edge of the system."],"url":"http://arxiv.org/abs/2403.12546v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-19 08:37:57","title":"Attitude Tracking of Uncertain Flexible Spacecraft Systems Subject to Unknown External Disturbances","abstract":"In this paper, we investigate the attitude tracking problem of uncertain flexible spacecraft systems subject to external disturbances. In sharp contrast to existing results, the dynamics of flexible spacecraft systems and external disturbances are allowed to be unknown. To deal with the challenges by these unknown factors, we develop a class of nonlinear internal models which converts the attitude tracking problem of uncertain flexible spacecraft systems into a regulation problem of an augmented system. Furthermore, to overcome the difficulties caused by the unmeasurable modal variable, the uncertainty introduced by the internal model, and the cross-coupling of the uncertainties with the system state, we design an auxiliary dynamic system for auxiliary stabilization, a dynamic compensator for dynamic compensation, and a linearly parameterized transformation for adaptive regulation in sequence. By introducing a series of coordinate and input transformations, we propose an adaptive dynamic control law to achieve regulation of the augmented system and thus leading to the solution to the attitude tracking problem. In addition, we analyze the convergence issue of the estimated parameter to its true value by the persistently exciting condition. Finally, the effec tiveness of the developed approach is verified by its application to the attitude manoeuvre of a flexible spacecraft system in the presence of external disturbances.","sentences":["In this paper, we investigate the attitude tracking problem of uncertain flexible spacecraft systems subject to external disturbances.","In sharp contrast to existing results, the dynamics of flexible spacecraft systems and external disturbances are allowed to be unknown.","To deal with the challenges by these unknown factors, we develop a class of nonlinear internal models which converts the attitude tracking problem of uncertain flexible spacecraft systems into a regulation problem of an augmented system.","Furthermore, to overcome the difficulties caused by the unmeasurable modal variable, the uncertainty introduced by the internal model, and the cross-coupling of the uncertainties with the system state, we design an auxiliary dynamic system for auxiliary stabilization, a dynamic compensator for dynamic compensation, and a linearly parameterized transformation for adaptive regulation in sequence.","By introducing a series of coordinate and input transformations, we propose an adaptive dynamic control law to achieve regulation of the augmented system and thus leading to the solution to the attitude tracking problem.","In addition, we analyze the convergence issue of the estimated parameter to its true value by the persistently exciting condition.","Finally, the effec tiveness of the developed approach is verified by its application to the attitude manoeuvre of a flexible spacecraft system in the presence of external disturbances."],"url":"http://arxiv.org/abs/2403.12542v1","category":"math.OC"}
{"created":"2024-03-19 08:37:13","title":"TAGS: Real-time Intrusion Detection with Tag-Propagation-based Provenance Graph Alignment on Streaming Events","abstract":"The evolution and advancement of cyberattacks pose challenges to existing security products. Recent concentrated research on provenance graph-based detection has proved its effectiveness in attack detection and investigation. However, implementing these approaches in practice encounters challenges such as high overhead, slow responsiveness, and low interpretability and extensibility.   Towards practical attack detection and investigation with provenance graphs, we propose TAGS, a tag-propagation-based streaming provenance graph alignment system. Utilizing the tag-based intermediate result caching mechanism alongside carefully crafted propagation rules, we eliminate the need to store and duplicate raw data processing. This approach effectively mitigates in-memory storage requirements and minimizes data processing overhead, facilitating rapid on-stream graph alignment while significantly conserving CPU and memory resources. As a result, TAGS can detect and investigate various cyber-attacks in real-time. Moreover, TAGS allows analysts to customize attack query graphs flexibly to identify extended attacks in data streams.   We conduct experimental evaluations on two large-scale public datasets containing 257.42 GB of audit logs with 12 relevant query graphs of varying sizes, covering multiple attack techniques and scenarios. The results demonstrate that TAGS is sufficiently efficient to process 176K events per second while sufficiently accurately identifying all 29 alignments in massive data. Moreover, it can effectively handle the dependency explosion problem with steady, low-level memory consumption (less than 300MB), producing only 3 false positives. Overall, the performance of TAGS significantly outperforms the state-of-the-art methods.","sentences":["The evolution and advancement of cyberattacks pose challenges to existing security products.","Recent concentrated research on provenance graph-based detection has proved its effectiveness in attack detection and investigation.","However, implementing these approaches in practice encounters challenges such as high overhead, slow responsiveness, and low interpretability and extensibility.   ","Towards practical attack detection and investigation with provenance graphs, we propose TAGS, a tag-propagation-based streaming provenance graph alignment system.","Utilizing the tag-based intermediate result caching mechanism alongside carefully crafted propagation rules, we eliminate the need to store and duplicate raw data processing.","This approach effectively mitigates in-memory storage requirements and minimizes data processing overhead, facilitating rapid on-stream graph alignment while significantly conserving CPU and memory resources.","As a result, TAGS can detect and investigate various cyber-attacks in real-time.","Moreover, TAGS allows analysts to customize attack query graphs flexibly to identify extended attacks in data streams.   ","We conduct experimental evaluations on two large-scale public datasets containing 257.42 GB of audit logs with 12 relevant query graphs of varying sizes, covering multiple attack techniques and scenarios.","The results demonstrate that TAGS is sufficiently efficient to process 176K events per second while sufficiently accurately identifying all 29 alignments in massive data.","Moreover, it can effectively handle the dependency explosion problem with steady, low-level memory consumption (less than 300MB), producing only 3 false positives.","Overall, the performance of TAGS significantly outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.12541v1","category":"cs.CR"}
{"created":"2024-03-19 08:25:42","title":"Multi-View Active Sensing for Human-Robot Interaction via Hierarchically Connected Tree","abstract":"Comprehensive perception of human beings is the prerequisite to ensure the safety of human-robot interaction. Currently, prevailing visual sensing approach typically involves a single static camera, resulting in a restricted and occluded field of view. In our work, we develop an active vision system using multiple cameras to dynamically capture multi-source RGB-D data. An integrated human sensing strategy based on a hierarchically connected tree structure is proposed to fuse localized visual information. Constituting the tree model are the nodes representing keypoints and the edges representing keyparts, which are consistently interconnected to preserve the structural constraints during multi-source fusion. Utilizing RGB-D data and HRNet, the 3D positions of keypoints are analytically estimated, and their presence is inferred through a sliding widow of confidence scores. Subsequently, the point clouds of reliable keyparts are extracted by drawing occlusion-resistant masks, enabling fine registration between data clouds and cylindrical model following the hierarchical order. Experimental results demonstrate that our method enhances keypart recognition recall from 69.20% to 90.10%, compared to employing a single static camera. Furthermore, in overcoming challenges related to localized and occluded perception, the robotic arm's obstacle avoidance capabilities are effectively improved.","sentences":["Comprehensive perception of human beings is the prerequisite to ensure the safety of human-robot interaction.","Currently, prevailing visual sensing approach typically involves a single static camera, resulting in a restricted and occluded field of view.","In our work, we develop an active vision system using multiple cameras to dynamically capture multi-source RGB-D data.","An integrated human sensing strategy based on a hierarchically connected tree structure is proposed to fuse localized visual information.","Constituting the tree model are the nodes representing keypoints and the edges representing keyparts, which are consistently interconnected to preserve the structural constraints during multi-source fusion.","Utilizing RGB-D data and HRNet, the 3D positions of keypoints are analytically estimated, and their presence is inferred through a sliding widow of confidence scores.","Subsequently, the point clouds of reliable keyparts are extracted by drawing occlusion-resistant masks, enabling fine registration between data clouds and cylindrical model following the hierarchical order.","Experimental results demonstrate that our method enhances keypart recognition recall from 69.20% to 90.10%, compared to employing a single static camera.","Furthermore, in overcoming challenges related to localized and occluded perception, the robotic arm's obstacle avoidance capabilities are effectively improved."],"url":"http://arxiv.org/abs/2403.12538v1","category":"cs.RO"}
{"created":"2024-03-19 08:21:54","title":"Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping with Multi-maps","abstract":"In this paper, we introduce Vox-Fusion++, a multi-maps-based robust dense tracking and mapping system that seamlessly fuses neural implicit representations with traditional volumetric fusion techniques. Building upon the concept of implicit mapping and positioning systems, our approach extends its applicability to real-world scenarios. Our system employs a voxel-based neural implicit surface representation, enabling efficient encoding and optimization of the scene within each voxel. To handle diverse environments without prior knowledge, we incorporate an octree-based structure for scene division and dynamic expansion. To achieve real-time performance, we propose a high-performance multi-process framework. This ensures the system's suitability for applications with stringent time constraints. Additionally, we adopt the idea of multi-maps to handle large-scale scenes, and leverage loop detection and hierarchical pose optimization strategies to reduce long-term pose drift and remove duplicate geometry. Through comprehensive evaluations, we demonstrate that our method outperforms previous methods in terms of reconstruction quality and accuracy across various scenarios. We also show that our Vox-Fusion++ can be used in augmented reality and collaborative mapping applications. Our source code will be publicly available at \\url{https://github.com/zju3dv/Vox-Fusion_Plus_Plus}","sentences":["In this paper, we introduce Vox-Fusion++, a multi-maps-based robust dense tracking and mapping system that seamlessly fuses neural implicit representations with traditional volumetric fusion techniques.","Building upon the concept of implicit mapping and positioning systems, our approach extends its applicability to real-world scenarios.","Our system employs a voxel-based neural implicit surface representation, enabling efficient encoding and optimization of the scene within each voxel.","To handle diverse environments without prior knowledge, we incorporate an octree-based structure for scene division and dynamic expansion.","To achieve real-time performance, we propose a high-performance multi-process framework.","This ensures the system's suitability for applications with stringent time constraints.","Additionally, we adopt the idea of multi-maps to handle large-scale scenes, and leverage loop detection and hierarchical pose optimization strategies to reduce long-term pose drift and remove duplicate geometry.","Through comprehensive evaluations, we demonstrate that our method outperforms previous methods in terms of reconstruction quality and accuracy across various scenarios.","We also show that our Vox-Fusion++ can be used in augmented reality and collaborative mapping applications.","Our source code will be publicly available at \\url{https://github.com/zju3dv/Vox-Fusion_Plus_Plus}"],"url":"http://arxiv.org/abs/2403.12536v1","category":"cs.CV"}
{"created":"2024-03-19 08:19:53","title":"High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization","abstract":"We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction. To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas. Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames. Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way. Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM.","sentences":["We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that provides metrically accurate pose tracking and visually realistic reconstruction.","To this end, we first propose a Gaussian densification strategy based on the rendering loss to map unobserved areas and refine reobserved areas.","Second, we introduce extra regularization parameters to alleviate the forgetting problem in the continuous mapping problem, where parameters tend to overfit the latest frame and result in decreasing rendering quality for previous frames.","Both mapping and tracking are performed with Gaussian parameters by minimizing re-rendering loss in a differentiable way.","Compared to recent neural and concurrently developed gaussian splatting RGBD SLAM baselines, our method achieves state-of-the-art results on the synthetic dataset Replica and competitive results on the real-world dataset TUM."],"url":"http://arxiv.org/abs/2403.12535v1","category":"cs.RO"}
{"created":"2024-03-19 08:15:53","title":"ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More","abstract":"Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns. However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks. We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty. In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective. Our ExACT brings two technical contributions. Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones. This subtly enhances the performance of ExACT without extra computational cost. Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation. In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation. Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.","sentences":["Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns.","However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks.","We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty.","In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective.","Our ExACT brings two technical contributions.","Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones.","This subtly enhances the performance of ExACT without extra computational cost.","Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation.","In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation.","Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively."],"url":"http://arxiv.org/abs/2403.12534v1","category":"cs.CV"}
{"created":"2024-03-19 07:56:42","title":"Prompt-based Graph Model for Joint Liberal Event Extraction and Event Schema Induction","abstract":"Events are essential components of speech and texts, describing the changes in the state of entities. The event extraction task aims to identify and classify events and find their participants according to event schemas. Manually predefined event schemas have limited coverage and are hard to migrate across domains. Therefore, the researchers propose Liberal Event Extraction (LEE), which aims to extract events and discover event schemas simultaneously. However, existing LEE models rely heavily on external language knowledge bases and require the manual development of numerous rules for noise removal and knowledge alignment, which is complex and laborious. To this end, we propose a Prompt-based Graph Model for Liberal Event Extraction (PGLEE). Specifically, we use a prompt-based model to obtain candidate triggers and arguments, and then build heterogeneous event graphs to encode the structures within and between events. Experimental results prove that our approach achieves excellent performance with or without predefined event schemas, while the automatically detected event schemas are proven high quality.","sentences":["Events are essential components of speech and texts, describing the changes in the state of entities.","The event extraction task aims to identify and classify events and find their participants according to event schemas.","Manually predefined event schemas have limited coverage and are hard to migrate across domains.","Therefore, the researchers propose Liberal Event Extraction (LEE), which aims to extract events and discover event schemas simultaneously.","However, existing LEE models rely heavily on external language knowledge bases and require the manual development of numerous rules for noise removal and knowledge alignment, which is complex and laborious.","To this end, we propose a Prompt-based Graph Model for Liberal Event Extraction (PGLEE).","Specifically, we use a prompt-based model to obtain candidate triggers and arguments, and then build heterogeneous event graphs to encode the structures within and between events.","Experimental results prove that our approach achieves excellent performance with or without predefined event schemas, while the automatically detected event schemas are proven high quality."],"url":"http://arxiv.org/abs/2403.12526v1","category":"cs.CL"}
{"created":"2024-03-19 07:42:57","title":"Dynamic Spatial-Temporal Aggregation for Skeleton-Aware Sign Language Recognition","abstract":"Skeleton-aware sign language recognition (SLR) has gained popularity due to its ability to remain unaffected by background information and its lower computational requirements. Current methods utilize spatial graph modules and temporal modules to capture spatial and temporal features, respectively. However, their spatial graph modules are typically built on fixed graph structures such as graph convolutional networks or a single learnable graph, which only partially explore joint relationships. Additionally, a simple temporal convolution kernel is used to capture temporal information, which may not fully capture the complex movement patterns of different signers. To overcome these limitations, we propose a new spatial architecture consisting of two concurrent branches, which build input-sensitive joint relationships and incorporates specific domain knowledge for recognition, respectively. These two branches are followed by an aggregation process to distinguishe important joint connections. We then propose a new temporal module to model multi-scale temporal information to capture complex human dynamics. Our method achieves state-of-the-art accuracy compared to previous skeleton-aware methods on four large-scale SLR benchmarks. Moreover, our method demonstrates superior accuracy compared to RGB-based methods in most cases while requiring much fewer computational resources, bringing better accuracy-computation trade-off. Code is available at https://github.com/hulianyuyy/DSTA-SLR.","sentences":["Skeleton-aware sign language recognition (SLR) has gained popularity due to its ability to remain unaffected by background information and its lower computational requirements.","Current methods utilize spatial graph modules and temporal modules to capture spatial and temporal features, respectively.","However, their spatial graph modules are typically built on fixed graph structures such as graph convolutional networks or a single learnable graph, which only partially explore joint relationships.","Additionally, a simple temporal convolution kernel is used to capture temporal information, which may not fully capture the complex movement patterns of different signers.","To overcome these limitations, we propose a new spatial architecture consisting of two concurrent branches, which build input-sensitive joint relationships and incorporates specific domain knowledge for recognition, respectively.","These two branches are followed by an aggregation process to distinguishe important joint connections.","We then propose a new temporal module to model multi-scale temporal information to capture complex human dynamics.","Our method achieves state-of-the-art accuracy compared to previous skeleton-aware methods on four large-scale SLR benchmarks.","Moreover, our method demonstrates superior accuracy compared to RGB-based methods in most cases while requiring much fewer computational resources, bringing better accuracy-computation trade-off.","Code is available at https://github.com/hulianyuyy/DSTA-SLR."],"url":"http://arxiv.org/abs/2403.12519v1","category":"cs.CV"}
{"created":"2024-03-19 07:41:37","title":"Solving vibrational Hamiltonians with neural canonical transformations","abstract":"The behavior of polyatomic molecules around their equilibrium positions can be regarded as quantum coupled anharmonic oscillators. Solving the corresponding Schr\\\"odinger equations can interpret or predict experimental spectra of molecules. In this study, we develop a novel approach to solve excited states of anharmonic vibrational systems. The normal coordinates of molecules are transformed into new coordinates through a normalizing flow parameterized by a neural network, facilitating the construction of a set of orthogonal many-body variational wavefunctions. Our methodology has been validated on an exactly solvable $64$-dimensional coupled harmonic oscillator, yielding numerical results with a relative error on the order of $10^{-6}$. Furthermore, the neural canonical transformations are also applied to calculate the energy levels of two specific molecules, acetonitrile ($\\text{C}\\text{H}_3\\text{CN}$) and ethylene oxide ($\\text{C}_2\\text{H}_4\\text{O}$) involving $12$ and $15$ vibrational modes respectively, which are in agreement with experimental findings. One of the key advantages of our approach is its flexibility concerning the potential energy surface, requiring no specific form for its application. Moreover, this method can be easily implemented on large-scale distributed computing platforms, making it particularly suitable for investigating more complex vibrational structures.","sentences":["The behavior of polyatomic molecules around their equilibrium positions can be regarded as quantum coupled anharmonic oscillators.","Solving the corresponding Schr\\\"odinger equations can interpret or predict experimental spectra of molecules.","In this study, we develop a novel approach to solve excited states of anharmonic vibrational systems.","The normal coordinates of molecules are transformed into new coordinates through a normalizing flow parameterized by a neural network, facilitating the construction of a set of orthogonal many-body variational wavefunctions.","Our methodology has been validated on an exactly solvable $64$-dimensional coupled harmonic oscillator, yielding numerical results with a relative error on the order of $10^{-6}$. Furthermore, the neural canonical transformations are also applied to calculate the energy levels of two specific molecules, acetonitrile ($\\text{C}\\text{H}_3\\text{CN}$) and ethylene oxide ($\\text{C}_2\\text{H}_4\\text{O}$) involving $12$ and $15$ vibrational modes respectively, which are in agreement with experimental findings.","One of the key advantages of our approach is its flexibility concerning the potential energy surface, requiring no specific form for its application.","Moreover, this method can be easily implemented on large-scale distributed computing platforms, making it particularly suitable for investigating more complex vibrational structures."],"url":"http://arxiv.org/abs/2403.12518v1","category":"physics.chem-ph"}
{"created":"2024-03-19 07:32:43","title":"Recent advances in the theory of the BCS-BEC crossover for fermionic superfluidity","abstract":"The BCS-BEC crossover realized experimentally with ultra-cold Fermi gases may be considered as one of the important scientific achievements occurred during the last several years. The flexibility for operating on these systems on the experimental side and the full control of the relevant system degrees of freedom on the theoretical side make quite stringent at a fundamental level the comparison between the experimental data and the corresponding theoretical calculations. Here, we briefly survey recent theoretical advances resting on a diagrammatic approach at equilibrium that improves in a systematic way on the widely used t-matrix approach, yielding a quite good comparison between theory and experiments for several physical quantities of interest. It is proposed that the physical phenomena underlying this theoretical approach may also be relevant to the superconducting phase of condensed-matter materials which cannot be described by the standard BCS theory.","sentences":["The BCS-BEC crossover realized experimentally with ultra-cold Fermi gases may be considered as one of the important scientific achievements occurred during the last several years.","The flexibility for operating on these systems on the experimental side and the full control of the relevant system degrees of freedom on the theoretical side make quite stringent at a fundamental level the comparison between the experimental data and the corresponding theoretical calculations.","Here, we briefly survey recent theoretical advances resting on a diagrammatic approach at equilibrium that improves in a systematic way on the widely used t-matrix approach, yielding a quite good comparison between theory and experiments for several physical quantities of interest.","It is proposed that the physical phenomena underlying this theoretical approach may also be relevant to the superconducting phase of condensed-matter materials which cannot be described by the standard BCS theory."],"url":"http://arxiv.org/abs/2403.12515v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-19 07:30:00","title":"One family of dark-bright solitons with striking width differences","abstract":"Most of previously reported dark-bright solitons admit identical width for the two components in both theoretical and experimental studies. We report dark-bright solitons can admit strikingly different widths, and derive a family of analytical solutions for them by Lagrangian variational method. The existence regimes for these solitons become much more widespread in the space of nonlinear parameters, than the ones for the previously known dark-bright solitons with identical width. Our analysis indicates that the effective quantum wells are quite different in the two components, in sharp contrast to the ones for all previously known vector solitons. Especially, the particle number of bright soliton can be used to control the generation of dark-bright solitons with varied ratios of solitons' widths. Based on the current experimental technologies, we propose an experimental scheme for observing these novel dark-bright solitons. The results suggest that abundant vector solitons with difference widths exist in multi-components coupled systems, and would inspire experiments to observe them in nonlinear optical fibers, Bose-Einstein condensates, and other nonlinear coupled systems.","sentences":["Most of previously reported dark-bright solitons admit identical width for the two components in both theoretical and experimental studies.","We report dark-bright solitons can admit strikingly different widths, and derive a family of analytical solutions for them by Lagrangian variational method.","The existence regimes for these solitons become much more widespread in the space of nonlinear parameters, than the ones for the previously known dark-bright solitons with identical width.","Our analysis indicates that the effective quantum wells are quite different in the two components, in sharp contrast to the ones for all previously known vector solitons.","Especially, the particle number of bright soliton can be used to control the generation of dark-bright solitons with varied ratios of solitons' widths.","Based on the current experimental technologies, we propose an experimental scheme for observing these novel dark-bright solitons.","The results suggest that abundant vector solitons with difference widths exist in multi-components coupled systems, and would inspire experiments to observe them in nonlinear optical fibers, Bose-Einstein condensates, and other nonlinear coupled systems."],"url":"http://arxiv.org/abs/2403.12514v1","category":"nlin.PS"}
{"created":"2024-03-19 07:28:48","title":"Uranus and Neptune as methane planets: producing icy giants from refractory planetesimals","abstract":"Uranus and Neptune are commonly considered ice giants, and it is often assumed that, in addition to a solar mix of hydrogen and helium, they contain roughly twice as much water as rock. This classical picture has led to successful models of their internal structure and has been understood to be compatible with the composition of the solar nebula during their formation (Reynolds and Summers 1965; Podolak and Cameron 1974; Podolak and Reynolds 1984; Podolak et al. 1995; Nettelmann et al. 2013). However, the dominance of water has been recently questioned (Teanby et al. 2020; Helled and Fortney 2020; Podolak et al. 2022). Planetesimals in the outer solar system are composed mainly of refractory materials, leading to an inconsistency between the icy composition of Uranus and Neptune and the ice-poor planetesimals they accreted during formation (Podolak et al. 2022). Here we elaborate on this problem, and propose a new potential solution. We show that chemical reactions between planetesimals dominated by organic-rich refractory materials and the hydrogen in gaseous atmospheres of protoplanets can form large amounts of methane 'ice'. Uranus and Neptune could thus be compatible with having accreted refractory-dominated planetesimals, while still remaining icy. Using random statistical computer models for a wide parameter space, we show that the resulting methane-rich internal composition could be a natural solution, giving a good match to the size, mass and moment of inertia of Uranus and Neptune, whereas rock-rich models appear to only work if a rocky interior is heavily mixed with hydrogen. Our model predicts a lower than solar hydrogen to helium ratio, which can be tested. We conclude that Uranus, Neptune and similar exoplanets could be methane-rich, and discuss why Jupiter and Saturn cannot.","sentences":["Uranus and Neptune are commonly considered ice giants, and it is often assumed that, in addition to a solar mix of hydrogen and helium, they contain roughly twice as much water as rock.","This classical picture has led to successful models of their internal structure and has been understood to be compatible with the composition of the solar nebula during their formation (Reynolds and Summers 1965; Podolak and Cameron 1974; Podolak and Reynolds 1984; Podolak et al. 1995; Nettelmann et al. 2013).","However, the dominance of water has been recently questioned (Teanby et al. 2020; Helled and Fortney 2020; Podolak et al. 2022).","Planetesimals in the outer solar system are composed mainly of refractory materials, leading to an inconsistency between the icy composition of Uranus and Neptune and the ice-poor planetesimals they accreted during formation (Podolak et al. 2022).","Here we elaborate on this problem, and propose a new potential solution.","We show that chemical reactions between planetesimals dominated by organic-rich refractory materials and the hydrogen in gaseous atmospheres of protoplanets can form large amounts of methane 'ice'.","Uranus and Neptune could thus be compatible with having accreted refractory-dominated planetesimals, while still remaining icy.","Using random statistical computer models for a wide parameter space, we show that the resulting methane-rich internal composition could be a natural solution, giving a good match to the size, mass and moment of inertia of Uranus and Neptune, whereas rock-rich models appear to only work if a rocky interior is heavily mixed with hydrogen.","Our model predicts a lower than solar hydrogen to helium ratio, which can be tested.","We conclude that Uranus, Neptune and similar exoplanets could be methane-rich, and discuss why Jupiter and Saturn cannot."],"url":"http://arxiv.org/abs/2403.12512v1","category":"astro-ph.EP"}
{"created":"2024-03-19 07:18:59","title":"Fractional regularity, global persistence, and asymptotic properties of the Boussinesq equations on bounded domains","abstract":"We address the long-time behavior of the 2D Boussinesq system, which consists of the incompressible Navier-Stokes equations driven by a non-diffusive density. We construct globally persistent solutions on a smooth bounded domain, when the initial data belongs to $(H^k\\cap V)\\times H^k$ for $k\\in\\mathbb{N}$ and $H^s\\times H^s$ for $0<s<2$. The proofs use parabolic maximal regularity and specific compatibility conditions at the initial time. Additionally, we also deduce various asymptotic properties of the velocity and density in the long-time limit and present a necessary and sufficient condition for the convergence to a steady state.","sentences":["We address the long-time behavior of the 2D Boussinesq system, which consists of the incompressible Navier-Stokes equations driven by a non-diffusive density.","We construct globally persistent solutions on a smooth bounded domain, when the initial data belongs to $(H^k\\cap V)\\times H^k$ for $k\\in\\mathbb{N}$ and $H^s\\times H^s$ for $0<s<2$.","The proofs use parabolic maximal regularity and specific compatibility conditions at the initial time.","Additionally, we also deduce various asymptotic properties of the velocity and density in the long-time limit and present a necessary and sufficient condition for the convergence to a steady state."],"url":"http://arxiv.org/abs/2403.12509v1","category":"math.AP"}
{"created":"2024-03-19 07:13:17","title":"Sparse Estimation for XL-MIMO with Unified LoS/NLoS Representation","abstract":"Extremely large-scale antenna array (ELAA) is promising as one of the key ingredients for the sixth generation (6G) of wireless communications. The electromagnetic propagation of spherical wavefronts introduces an additional distance-dependent dimension beyond conventional beamspace. In this paper, we first present one concise closed-form channel formulation for extremely large-scale multiple-input multiple-output (XL-MIMO). All line-of-sight (LoS) and non-line-of-sight (NLoS) paths, far-field and near-field scenarios, and XL-MIMO and XL-MISO channels are unified under the framework, where additional Vandermonde windowing matrix is exclusively considered for LoS path. Under this framework, we further propose one low-complexity unified LoS/NLoS orthogonal matching pursuit (XL-UOMP) algorithm for XL-MIMO channel estimation. The simulation results demonstrate the superiority of the proposed algorithm on both estimation accuracy and pilot consumption.","sentences":["Extremely large-scale antenna array (ELAA) is promising as one of the key ingredients for the sixth generation (6G) of wireless communications.","The electromagnetic propagation of spherical wavefronts introduces an additional distance-dependent dimension beyond conventional beamspace.","In this paper, we first present one concise closed-form channel formulation for extremely large-scale multiple-input multiple-output (XL-MIMO).","All line-of-sight (LoS) and non-line-of-sight (NLoS) paths, far-field and near-field scenarios, and XL-MIMO and XL-MISO channels are unified under the framework, where additional Vandermonde windowing matrix is exclusively considered for LoS path.","Under this framework, we further propose one low-complexity unified LoS/NLoS orthogonal matching pursuit (XL-UOMP) algorithm for XL-MIMO channel estimation.","The simulation results demonstrate the superiority of the proposed algorithm on both estimation accuracy and pilot consumption."],"url":"http://arxiv.org/abs/2403.12506v1","category":"cs.IT"}
{"created":"2024-03-19 07:11:00","title":"TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO","abstract":"Temporal misalignment (time offset) between sensors is common in low cost visual-inertial odometry (VIO) systems. Such temporal misalignment introduces inconsistent constraints for state estimation, leading to a significant positioning drift especially in high dynamic motion scenarios. In this article, we focus on online temporal calibration to reduce the positioning drift caused by the time offset for high dynamic motion VIO. For the time offset observation model, most existing methods rely on accurate state estimation or stable visual tracking. For the prediction model, current methods oversimplify the time offset as a constant value with white Gaussian noise. However, these ideal conditions are seldom satisfied in real high dynamic scenarios, resulting in the poor performance. In this paper, we introduce online time offset modeling networks (TON) to enhance real-time temporal calibration. TON improves the accuracy of time offset observation and prediction modeling. Specifically, for observation modeling, we propose feature velocity observation networks to enhance velocity computation for features in unstable visual tracking conditions. For prediction modeling, we present time offset prediction networks to learn its evolution pattern. To highlight the effectiveness of our method, we integrate the proposed TON into both optimization-based and filter-based VIO systems. Simulation and real-world experiments are conducted to demonstrate the enhanced performance of our approach. Additionally, to contribute to the VIO community, we will open-source the code of our method on: https://github.com/Franky-X/FVON-TPN.","sentences":["Temporal misalignment (time offset) between sensors is common in low cost visual-inertial odometry (VIO) systems.","Such temporal misalignment introduces inconsistent constraints for state estimation, leading to a significant positioning drift especially in high dynamic motion scenarios.","In this article, we focus on online temporal calibration to reduce the positioning drift caused by the time offset for high dynamic motion VIO.","For the time offset observation model, most existing methods rely on accurate state estimation or stable visual tracking.","For the prediction model, current methods oversimplify the time offset as a constant value with white Gaussian noise.","However, these ideal conditions are seldom satisfied in real high dynamic scenarios, resulting in the poor performance.","In this paper, we introduce online time offset modeling networks (TON) to enhance real-time temporal calibration.","TON improves the accuracy of time offset observation and prediction modeling.","Specifically, for observation modeling, we propose feature velocity observation networks to enhance velocity computation for features in unstable visual tracking conditions.","For prediction modeling, we present time offset prediction networks to learn its evolution pattern.","To highlight the effectiveness of our method, we integrate the proposed TON into both optimization-based and filter-based VIO systems.","Simulation and real-world experiments are conducted to demonstrate the enhanced performance of our approach.","Additionally, to contribute to the VIO community, we will open-source the code of our method on: https://github.com/Franky-X/FVON-TPN."],"url":"http://arxiv.org/abs/2403.12504v1","category":"cs.RO"}
{"created":"2024-03-19 07:07:05","title":"Listwise Generative Retrieval Models via a Sequential Learning Process","abstract":"Recently, a novel generative retrieval (GR) paradigm has been proposed, where a single sequence-to-sequence model is learned to directly generate a list of relevant document identifiers (docids) given a query. Existing GR models commonly employ maximum likelihood estimation (MLE) for optimization: this involves maximizing the likelihood of a single relevant docid given an input query, with the assumption that the likelihood for each docid is independent of the other docids in the list. We refer to these models as the pointwise approach in this paper. While the pointwise approach has been shown to be effective in the context of GR, it is considered sub-optimal due to its disregard for the fundamental principle that ranking involves making predictions about lists. In this paper, we address this limitation by introducing an alternative listwise approach, which empowers the GR model to optimize the relevance at the docid list level. Specifically, we view the generation of a ranked docid list as a sequence learning process: at each step we learn a subset of parameters that maximizes the corresponding generation likelihood of the $i$-th docid given the (preceding) top $i-1$ docids. To formalize the sequence learning process, we design a positional conditional probability for GR. To alleviate the potential impact of beam search on the generation quality during inference, we perform relevance calibration on the generation likelihood of model-generated docids according to relevance grades. We conduct extensive experiments on representative binary and multi-graded relevance datasets. Our empirical results demonstrate that our method outperforms state-of-the-art GR baselines in terms of retrieval performance.","sentences":["Recently, a novel generative retrieval (GR) paradigm has been proposed, where a single sequence-to-sequence model is learned to directly generate a list of relevant document identifiers (docids) given a query.","Existing GR models commonly employ maximum likelihood estimation (MLE) for optimization: this involves maximizing the likelihood of a single relevant docid given an input query, with the assumption that the likelihood for each docid is independent of the other docids in the list.","We refer to these models as the pointwise approach in this paper.","While the pointwise approach has been shown to be effective in the context of GR, it is considered sub-optimal due to its disregard for the fundamental principle that ranking involves making predictions about lists.","In this paper, we address this limitation by introducing an alternative listwise approach, which empowers the GR model to optimize the relevance at the docid list level.","Specifically, we view the generation of a ranked docid list as a sequence learning process: at each step we learn a subset of parameters that maximizes the corresponding generation likelihood of the $i$-th docid given the (preceding) top $i-1$ docids.","To formalize the sequence learning process, we design a positional conditional probability for GR.","To alleviate the potential impact of beam search on the generation quality during inference, we perform relevance calibration on the generation likelihood of model-generated docids according to relevance grades.","We conduct extensive experiments on representative binary and multi-graded relevance datasets.","Our empirical results demonstrate that our method outperforms state-of-the-art GR baselines in terms of retrieval performance."],"url":"http://arxiv.org/abs/2403.12499v1","category":"cs.IR"}
{"created":"2024-03-19 07:02:13","title":"Impact of the Russia-Ukraine conflict on the international staple agrifood trade networks","abstract":"The Russia-Ukraine conflict is a growing concern worldwide and poses serious threats to regional and global food security. Using monthly trade data for maize, rice, and wheat from 2016/1 to 2022/12, this paper constructs three international crop trade networks (iCTNs) and an aggregate international food trade network (iFTN). We aim to examine the structural changes following the occurrence of the Russia-Ukraine conflict. We find significant shifts in the number of edges, average degree, density, efficiency, and natural connectivity in the third quarter of 2022, particularly in the international wheat trade network. Additionally, we have shown that political reasons have caused more pronounced changes in the trade connections between the economies of the North Atlantic Treaty Organization and Russia than with Ukraine. This paper could provide insights into the negative impact of geopolitical conflicts on the global food system and encourage a series of effective strategies to mitigate the negative impact of the conflict on global food trade.","sentences":["The Russia-Ukraine conflict is a growing concern worldwide and poses serious threats to regional and global food security.","Using monthly trade data for maize, rice, and wheat from 2016/1 to 2022/12, this paper constructs three international crop trade networks (iCTNs) and an aggregate international food trade network (iFTN).","We aim to examine the structural changes following the occurrence of the Russia-Ukraine conflict.","We find significant shifts in the number of edges, average degree, density, efficiency, and natural connectivity in the third quarter of 2022, particularly in the international wheat trade network.","Additionally, we have shown that political reasons have caused more pronounced changes in the trade connections between the economies of the North Atlantic Treaty Organization and Russia than with Ukraine.","This paper could provide insights into the negative impact of geopolitical conflicts on the global food system and encourage a series of effective strategies to mitigate the negative impact of the conflict on global food trade."],"url":"http://arxiv.org/abs/2403.12496v1","category":"physics.soc-ph"}
{"created":"2024-03-19 07:02:09","title":"NSGAN: A Non-Dominant Sorting Optimisation-Based Generative Adversarial Design Framework for Alloy Discovery","abstract":"The design and discovery of new materials is fundamental to advancing scientific and technological innovation. The recent emergence of the materials genome concept holds great promise in revolutionising materials science by enabling the systematic utilisation of data for efficient prediction and optimisation of superior materials. However, the materials genome approach can be stymied by the vast complexity of design spaces, which often demand substantial computational resources and sophisticated data processing capabilities. To address these challenges, this work introduces a novel generative design framework called the non-dominant sorting optimisation-based generative adversarial networks (NSGAN). Capitalising on the synergies of genetic algorithms (GA) and generative adversarial networks (GANs), NSGAN provides a robust and efficient approach for tackling high-dimensional multi-objective optimisation design problems. To validate the efficacy of the proposed framework, we applied the model to a comprehensive dataset of aluminium alloys. Additionally, an online tool was created as a supplementary resource, offering a brief introduction to this innovative method for the wider scientific community. This study explores the potential of a predictive and data-driven approach in material design, indicating a promising pathway for widespread applications in the field of materials science.","sentences":["The design and discovery of new materials is fundamental to advancing scientific and technological innovation.","The recent emergence of the materials genome concept holds great promise in revolutionising materials science by enabling the systematic utilisation of data for efficient prediction and optimisation of superior materials.","However, the materials genome approach can be stymied by the vast complexity of design spaces, which often demand substantial computational resources and sophisticated data processing capabilities.","To address these challenges, this work introduces a novel generative design framework called the non-dominant sorting optimisation-based generative adversarial networks (NSGAN).","Capitalising on the synergies of genetic algorithms (GA) and generative adversarial networks (GANs), NSGAN provides a robust and efficient approach for tackling high-dimensional multi-objective optimisation design problems.","To validate the efficacy of the proposed framework, we applied the model to a comprehensive dataset of aluminium alloys.","Additionally, an online tool was created as a supplementary resource, offering a brief introduction to this innovative method for the wider scientific community.","This study explores the potential of a predictive and data-driven approach in material design, indicating a promising pathway for widespread applications in the field of materials science."],"url":"http://arxiv.org/abs/2403.12495v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 07:02:06","title":"A Trainable Feature Extractor Module for Deep Neural Networks and Scanpath Classification","abstract":"Scanpath classification is an area in eye tracking research with possible applications in medicine, manufacturing as well as training systems for students in various domains. In this paper we propose a trainable feature extraction module for deep neural networks. The purpose of this module is to transform a scanpath into a feature vector which is directly useable for the deep neural network architecture. Based on the backpropagated error of the deep neural network, the feature extraction module adapts its parameters to improve the classification performance. Therefore, our feature extraction module is jointly trainable with the deep neural network. The motivation to this feature extraction module is based on classical histogram-based approaches which usually compute distributions over a scanpath. We evaluated our module on three public datasets and compared it to the state of the art approaches.","sentences":["Scanpath classification is an area in eye tracking research with possible applications in medicine, manufacturing as well as training systems for students in various domains.","In this paper we propose a trainable feature extraction module for deep neural networks.","The purpose of this module is to transform a scanpath into a feature vector which is directly useable for the deep neural network architecture.","Based on the backpropagated error of the deep neural network, the feature extraction module adapts its parameters to improve the classification performance.","Therefore, our feature extraction module is jointly trainable with the deep neural network.","The motivation to this feature extraction module is based on classical histogram-based approaches which usually compute distributions over a scanpath.","We evaluated our module on three public datasets and compared it to the state of the art approaches."],"url":"http://arxiv.org/abs/2403.12493v1","category":"cs.CV"}
{"created":"2024-03-19 06:55:59","title":"Genetically programmable optical random neural networks","abstract":"Today, machine learning tools, particularly artificial neural networks, have become crucial for diverse applications. However, current digital computing tools to train and deploy artificial neural networks often struggle with massive data sizes and high power consumptions. Optical computing provides inherent parallelism and perform fundamental operations with passive optical components. However, most of the optical computing platforms suffer from relatively low accuracies for machine learning tasks due to fixed connections while avoiding complex and sensitive techniques. Here, we demonstrate a genetically programmable yet simple optical neural network to achieve high performances with optical random projection. By genetically programming the orientation of the scattering medium which acts as a random projection kernel and only using 1% of the search space, our novel technique finds an optimum kernel and improves its initial test accuracies 7-22% for various machine learning tasks. Our optical computing method presents a promising approach to achieve high performance in optical neural networks with a simple and scalable design.","sentences":["Today, machine learning tools, particularly artificial neural networks, have become crucial for diverse applications.","However, current digital computing tools to train and deploy artificial neural networks often struggle with massive data sizes and high power consumptions.","Optical computing provides inherent parallelism and perform fundamental operations with passive optical components.","However, most of the optical computing platforms suffer from relatively low accuracies for machine learning tasks due to fixed connections while avoiding complex and sensitive techniques.","Here, we demonstrate a genetically programmable yet simple optical neural network to achieve high performances with optical random projection.","By genetically programming the orientation of the scattering medium which acts as a random projection kernel and only using 1% of the search space, our novel technique finds an optimum kernel and improves its initial test accuracies 7-22% for various machine learning tasks.","Our optical computing method presents a promising approach to achieve high performance in optical neural networks with a simple and scalable design."],"url":"http://arxiv.org/abs/2403.12490v1","category":"cs.ET"}
{"created":"2024-03-19 06:50:15","title":"Unveiling Four Key Factors for Tire Force Control Allocation in 4WID-4WIS Electric Vehicles at Handling Limits","abstract":"The four-wheel independent drive and four-wheel independent steering (4WID-4WIS) configurations enhance control flexibility and dynamic performance potential for more integrated electric vehicles. This paper comprehensively analyzes the impacts of four key factors on tire force control allocation: vertical load estimation, actuator dynamic characteristics, tire force constraints, and wheel steering precision at handling limits. The study demonstrates that precise vertical load estimation enhances lateral force allocation accuracy. Additionally, the self-compensating effect of lateral tire forces minimizes the impact of small deviations in vertical load estimation on tire force control allocation. A novel control allocation method considering actuator dynamics is introduced, effectively improving yaw rate response and reducing tracking errors. Considering tire-road adhesion and actuator rate constraints, an innovative method to calculate the real-time attainable tire force volume is proposed based on the tire slip ratio and slip angle. Feedforward control with bump steer compensation is implemented to improve wheel steering precision and lateral tire force control accuracy. Matlab/Simulink and Carsim co-simulation results emphasize the importance of these key factors' individual impacts and combined effects. This analysis offers valuable insights for developing advanced tire force control allocation strategies in 4WID-4WIS electric vehicles.","sentences":["The four-wheel independent drive and four-wheel independent steering (4WID-4WIS) configurations enhance control flexibility and dynamic performance potential for more integrated electric vehicles.","This paper comprehensively analyzes the impacts of four key factors on tire force control allocation: vertical load estimation, actuator dynamic characteristics, tire force constraints, and wheel steering precision at handling limits.","The study demonstrates that precise vertical load estimation enhances lateral force allocation accuracy.","Additionally, the self-compensating effect of lateral tire forces minimizes the impact of small deviations in vertical load estimation on tire force control allocation.","A novel control allocation method considering actuator dynamics is introduced, effectively improving yaw rate response and reducing tracking errors.","Considering tire-road adhesion and actuator rate constraints, an innovative method to calculate the real-time attainable tire force volume is proposed based on the tire slip ratio and slip angle.","Feedforward control with bump steer compensation is implemented to improve wheel steering precision and lateral tire force control accuracy.","Matlab/Simulink and Carsim co-simulation results emphasize the importance of these key factors' individual impacts and combined effects.","This analysis offers valuable insights for developing advanced tire force control allocation strategies in 4WID-4WIS electric vehicles."],"url":"http://arxiv.org/abs/2403.12487v1","category":"eess.SY"}
{"created":"2024-03-19 06:35:23","title":"Chiral-symmetric Topological Origin of Nonlinear Fixed Points","abstract":"Particle-hole symmetry and chiral symmetry play a pivotal role in multiple areas of physics, yet they remain un-studied in systems with nonlinear interactions that are beyond Kerr-type. Here, we establish these two non-spatial symmetries in systems with strong and general nonlinear interactions. Chiral symmetry ensures the quantization of the Berry phase of nonlinear normal modes and categorizes the topological phases of nonlinear dynamics. We show edge modes that serve as topologically protected fixed points of chiral-symmetric nonlinear dynamics. Our theoretical framework paves the way towards the topological classification of general nonlinear dynamics.","sentences":["Particle-hole symmetry and chiral symmetry play a pivotal role in multiple areas of physics, yet they remain un-studied in systems with nonlinear interactions that are beyond Kerr-type.","Here, we establish these two non-spatial symmetries in systems with strong and general nonlinear interactions.","Chiral symmetry ensures the quantization of the Berry phase of nonlinear normal modes and categorizes the topological phases of nonlinear dynamics.","We show edge modes that serve as topologically protected fixed points of chiral-symmetric nonlinear dynamics.","Our theoretical framework paves the way towards the topological classification of general nonlinear dynamics."],"url":"http://arxiv.org/abs/2403.12480v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-19 06:25:41","title":"Dielectric response in twisted MoS2 bilayer facilitated by spin-orbit coupling effect","abstract":"Twisted van der Waals bilayers provide ideal two-dimensional (2D) platforms to study the interplay between spin and charge degree of freedom of electron. An exotic dielectric response behavior in such system is what we find here through the investigation of twisted bilayer MoS2 with two different stackings but same size of the commensurate supercell, which forms H-type like and R-type like stacked bilayer system. Our first-principles calculations show that applying an out-of-plane electric field gives rise to different response of the electric polarization, whose susceptibility is suppressed in H-type like case compared with that of R-type like one. Further analysis shows the dielectric response to perpendicular electric field is linked to the moire ferroelectric dipole texture. Their underlying link comes from that, through Rashba spin-orbit coupling (SOC) effect, the external electric field tends to change the internal pseudo-spin texture, which further depends on the twist induced dipole texture. Consequently, the suppression of dielectric response can be attributed to the topology protection of such ferroelectric dipole structure.","sentences":["Twisted van der Waals bilayers provide ideal two-dimensional (2D) platforms to study the interplay between spin and charge degree of freedom of electron.","An exotic dielectric response behavior in such system is what we find here through the investigation of twisted bilayer MoS2 with two different stackings but same size of the commensurate supercell, which forms H-type like and R-type like stacked bilayer system.","Our first-principles calculations show that applying an out-of-plane electric field gives rise to different response of the electric polarization, whose susceptibility is suppressed in H-type like case compared with that of R-type like one.","Further analysis shows the dielectric response to perpendicular electric field is linked to the moire ferroelectric dipole texture.","Their underlying link comes from that, through Rashba spin-orbit coupling (SOC) effect, the external electric field tends to change the internal pseudo-spin texture, which further depends on the twist induced dipole texture.","Consequently, the suppression of dielectric response can be attributed to the topology protection of such ferroelectric dipole structure."],"url":"http://arxiv.org/abs/2403.12475v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 05:52:21","title":"Digital Twin Channel for 6G: Concepts, Architectures and Potential Applications","abstract":"Digital twin channel (DTC) is the real-time mapping of a wireless channel for the physical wireless communication in the digital world, which is expected to be one of the essential enablers for the sixth-generation (6G) communication systems and provides significant performance enhancements for air-interface design. In this work, we first define the evolution of five levels of channel twins with the progression of wireless communication. The autonomous DTC for 6G is elaborated from multi-dimensional factors such as methodology, characterization precision, and data category. Then, we provide detailed insights into the requirements and architecture of a complete DTC for 6G. Subsequently, a sensing-enhanced real-time channel prediction platform and experimental validations are exhibited. Finally, drawing from the vision of the 6G network, we explore potential applications and address open issues in DTC research.","sentences":["Digital twin channel (DTC) is the real-time mapping of a wireless channel for the physical wireless communication in the digital world, which is expected to be one of the essential enablers for the sixth-generation (6G) communication systems and provides significant performance enhancements for air-interface design.","In this work, we first define the evolution of five levels of channel twins with the progression of wireless communication.","The autonomous DTC for 6G is elaborated from multi-dimensional factors such as methodology, characterization precision, and data category.","Then, we provide detailed insights into the requirements and architecture of a complete DTC for 6G. Subsequently, a sensing-enhanced real-time channel prediction platform and experimental validations are exhibited.","Finally, drawing from the vision of the 6G network, we explore potential applications and address open issues in DTC research."],"url":"http://arxiv.org/abs/2403.12467v1","category":"eess.SP"}
{"created":"2024-03-19 05:31:38","title":"Stochastic variance reduced gradient method for linear ill-posed inverse problems","abstract":"In this paper we apply the stochastic variance reduced gradient (SVRG) method, which is a popular variance reduction method in optimization for accelerating the stochastic gradient method, to solve large scale linear ill-posed systems in Hilbert spaces. Under {\\it a priori} choices of stopping indices, we derive a convergence rate result when the sought solution satisfies a benchmark source condition and establish a convergence result without using any source condition. To terminate the method in an {\\it a posteriori} manner, we consider the discrepancy principle and show that it terminates the method in finite many iteration steps almost surely. Various numerical results are reported to test the performance of the method.","sentences":["In this paper we apply the stochastic variance reduced gradient (SVRG) method, which is a popular variance reduction method in optimization for accelerating the stochastic gradient method, to solve large scale linear ill-posed systems in Hilbert spaces.","Under {\\it a priori} choices of stopping indices, we derive a convergence rate result when the sought solution satisfies a benchmark source condition and establish a convergence result without using any source condition.","To terminate the method in an {\\it a posteriori} manner, we consider the discrepancy principle and show that it terminates the method in finite many iteration steps almost surely.","Various numerical results are reported to test the performance of the method."],"url":"http://arxiv.org/abs/2403.12460v1","category":"math.NA"}
{"created":"2024-03-19 05:29:34","title":"The mapping cone of an Eisenbud operator and applications to exact zero divisors","abstract":"Let $(Q,\\mathfrak{m},{\\sf k})$ be a local ring that admits an exact pair of zero divisors $(f,g)$, $M$ a $Q$-module with $fM = 0$ and $U$ a free resolution of $M$ over $Q$. We construct a degree $-2$ chain map, which we call an Einsenbud operator, on the complex $U \\otimes_Q Q/(f,g)$ and use the mapping cone of the operator to study two exact sequences that relate homology over $Q$ to homology over $Q/(f)$. Several applications are given.","sentences":["Let $(Q,\\mathfrak{m},{\\sf k})$ be a local ring that admits an exact pair of zero divisors $(f,g)$, $M$ a $Q$-module with $fM = 0$ and $U$ a free resolution of $M$ over $Q$. We construct a degree $-2$ chain map, which we call an Einsenbud operator, on the complex $U \\otimes_Q Q/(f,g)$ and use the mapping cone of the operator to study two exact sequences that relate homology over $Q$ to homology over $Q/(f)$.","Several applications are given."],"url":"http://arxiv.org/abs/2403.12458v1","category":"math.AC"}
{"created":"2024-03-19 05:16:13","title":"Bosonic KG-oscillators in Eddington-inspired Born-Infeld gravity: Wu-Yang magnetic monopole and Ricci scalar curvature effects","abstract":"We investigate the bosonic Klein-Gordon (KG) oscillators in a global monopole (GM) spacetime in Eddington-inspired Born-Infeld (EiBI) gravity and a Wu-Yang magnetic monopole (WYMM). We discuss the gravitational effects in the presence of Ricci scalar curvature $R=R_{\\upsilon }^{\\upsilon }$. It is observed that the presence of the Ricci scalar curvature, effectively and manifestly, introduces a force field that makes the corresponding quantum mechanical repulsive core more repulsive. Similar effect is also observed for the EiBI-gravitational field. We reiterate and report that the corresponding bosonic KG-oscillator quantum mechanical system admits a solution in the form of confluent Heun functions, the truncation of which into a physically admissible polynomial is shown to be associated with some parametric correlations/conditions. The use of such conditions/correlations is mandatory and yields a set of allowed/restricted quantum mechanical orbital $\\ell $-excitations, for all radial quantum numbers $n_{r}\\geq 0$. Our procedure is shown to be quite handy, in the since that it allows one to retrieve results for KG-oscillators in GM-spacetime in different EiBI-gravity and Ricci scalar curvature settings.","sentences":["We investigate the bosonic Klein-Gordon (KG) oscillators in a global monopole (GM) spacetime in Eddington-inspired Born-Infeld (EiBI) gravity and a Wu-Yang magnetic monopole (WYMM).","We discuss the gravitational effects in the presence of Ricci scalar curvature $R=R_{\\upsilon }^{\\upsilon }$.","It is observed that the presence of the Ricci scalar curvature, effectively and manifestly, introduces a force field that makes the corresponding quantum mechanical repulsive core more repulsive.","Similar effect is also observed for the EiBI-gravitational field.","We reiterate and report that the corresponding bosonic KG-oscillator quantum mechanical system admits a solution in the form of confluent Heun functions, the truncation of which into a physically admissible polynomial is shown to be associated with some parametric correlations/conditions.","The use of such conditions/correlations is mandatory and yields a set of allowed/restricted quantum mechanical orbital $\\ell $-excitations, for all radial quantum numbers $n_{r}\\geq 0$.","Our procedure is shown to be quite handy, in the since that it allows one to retrieve results for KG-oscillators in GM-spacetime in different EiBI-gravity and Ricci scalar curvature settings."],"url":"http://arxiv.org/abs/2403.12447v1","category":"gr-qc"}
{"created":"2024-03-19 04:55:30","title":"Oscillatory and chaotic pattern dynamics driven by surface curvature","abstract":"Patterns on curved surfaces are ubiquitous, yet the influence of surface geometry on pattern dynamics remains elusive. We recently reported a new mechanism of pattern propagation in which a static pattern on a flat plane becomes a propagating pattern on a curved surface [Nishide and Ishihara, Phys. Rev. Lett. 2022]. Here, we address whether surface curvature can drive more complex pattern dynamics beyond propagation. By employing a combination of weakly nonlinear analysis and numerical simulation, we show that oscillatory and chaotic pattern dynamics can emerge by controlling the surface shapes. These findings highlight a new role of surface topography in pattern formation and dynamics.","sentences":["Patterns on curved surfaces are ubiquitous, yet the influence of surface geometry on pattern dynamics remains elusive.","We recently reported a new mechanism of pattern propagation in which a static pattern on a flat plane becomes a propagating pattern on a curved surface","[Nishide and Ishihara, Phys. Rev. Lett. 2022].","Here, we address whether surface curvature can drive more complex pattern dynamics beyond propagation.","By employing a combination of weakly nonlinear analysis and numerical simulation, we show that oscillatory and chaotic pattern dynamics can emerge by controlling the surface shapes.","These findings highlight a new role of surface topography in pattern formation and dynamics."],"url":"http://arxiv.org/abs/2403.12442v1","category":"nlin.CD"}
{"created":"2024-03-19 04:49:17","title":"Evaluating Datalog over Semirings: A Grounding-based Approach","abstract":"Datalog is a powerful yet elegant language that allows expressing recursive computation. Although Datalog evaluation has been extensively studied in the literature, so far, only loose upper bounds are known on how fast a Datalog program can be evaluated. In this work, we ask the following question: given a Datalog program over a naturally-ordered semiring $\\sigma$, what is the tightest possible runtime? To this end, our main contribution is a general two-phase framework for analyzing the data complexity of Datalog over $\\sigma$: first ground the program into an equivalent system of polynomial equations (i.e. grounding) and then find the least fixpoint of the grounding over $\\sigma$. We present algorithms that use structure-aware query evaluation techniques to obtain the smallest possible groundings. Next, efficient algorithms for fixpoint evaluation are introduced over two classes of semirings: (1) finite-rank semirings and (2) absorptive semirings of total order. Combining both phases, we obtain state-of-the-art and new algorithmic results. Finally, we complement our results with a matching fine-grained lower bound.","sentences":["Datalog is a powerful yet elegant language that allows expressing recursive computation.","Although Datalog evaluation has been extensively studied in the literature, so far, only loose upper bounds are known on how fast a Datalog program can be evaluated.","In this work, we ask the following question: given a Datalog program over a naturally-ordered semiring $\\sigma$, what is the tightest possible runtime?","To this end, our main contribution is a general two-phase framework for analyzing the data complexity of Datalog over $\\sigma$: first ground the program into an equivalent system of polynomial equations (i.e. grounding) and then find the least fixpoint of the grounding over $\\sigma$. We present algorithms that use structure-aware query evaluation techniques to obtain the smallest possible groundings.","Next, efficient algorithms for fixpoint evaluation are introduced over two classes of semirings: (1) finite-rank semirings and (2) absorptive semirings of total order.","Combining both phases, we obtain state-of-the-art and new algorithmic results.","Finally, we complement our results with a matching fine-grained lower bound."],"url":"http://arxiv.org/abs/2403.12436v1","category":"cs.DB"}
{"created":"2024-03-19 04:48:07","title":"Astronomical Test with CMOS on the 60-cm Telescope at the Xinglong Observatory, NAOC","abstract":"This work shows details of an evaluation of an observational system comprising a CMOS detector, 60-cm telescope, and filter complement. The system's photometric precision and differential photometric precision, and extinction coefficients were assessed through observations of Supersky flat fields, open clusters, standard stars, and exoplanets. Photometry was precision achieved at the 0.02 mag level, while differential photometry of 0.004 mag precision. Extinction was found to be agreed with previous studies conducted at Xinglong Observatory. Ultimately, the results demonstrate this observing system is capable of precision scientific observations with CCD across the optical wavelengths.","sentences":["This work shows details of an evaluation of an observational system comprising a CMOS detector, 60-cm telescope, and filter complement.","The system's photometric precision and differential photometric precision, and extinction coefficients were assessed through observations of Supersky flat fields, open clusters, standard stars, and exoplanets.","Photometry was precision achieved at the 0.02 mag level, while differential photometry of 0.004 mag precision.","Extinction was found to be agreed with previous studies conducted at Xinglong Observatory.","Ultimately, the results demonstrate this observing system is capable of precision scientific observations with CCD across the optical wavelengths."],"url":"http://arxiv.org/abs/2403.12435v1","category":"astro-ph.IM"}
{"created":"2024-03-19 04:46:56","title":"Algorithmic Complexity Attacks on Dynamic Learned Indexes","abstract":"Learned Index Structures (LIS) view a sorted index as a model that learns the data distribution, takes a data element key as input, and outputs the predicted position of the key. The original LIS can only handle lookup operations with no support for updates, rendering it impractical to use for typical workloads. To address this limitation, recent studies have focused on designing efficient dynamic learned indexes. ALEX, as the pioneering dynamic learned index structures, enables dynamism by incorporating a series of design choices, including adaptive key space partitioning, dynamic model retraining, and sophisticated engineering and policies that prioritize read/write performance. While these design choices offer improved average-case performance, the emphasis on flexibility and performance increases the attack surface by allowing adversarial behaviors that maximize ALEX's memory space and time complexity in worst-case scenarios. In this work, we present the first systematic investigation of algorithmic complexity attacks (ACAs) targeting the worst-case scenarios of ALEX. We introduce new ACAs that fall into two categories, space ACAs and time ACAs, which target the memory space and time complexity, respectively. First, our space ACA on data nodes exploits ALEX's gapped array layout and uses Multiple-Choice Knapsack (MCK) to generate an optimal adversarial insertion plan for maximizing the memory consumption at the data node level. Second, our space ACA on internal nodes exploits ALEX's catastrophic cost mitigation mechanism, causing an out-of-memory error with only a few hundred adversarial insertions. Third, our time ACA generates pathological insertions to increase the disparity between the actual key distribution and the linear models of data nodes, deteriorating the runtime performance by up to 1,641X compared to ALEX operating under legitimate workloads.","sentences":["Learned Index Structures (LIS) view a sorted index as a model that learns the data distribution, takes a data element key as input, and outputs the predicted position of the key.","The original LIS can only handle lookup operations with no support for updates, rendering it impractical to use for typical workloads.","To address this limitation, recent studies have focused on designing efficient dynamic learned indexes.","ALEX, as the pioneering dynamic learned index structures, enables dynamism by incorporating a series of design choices, including adaptive key space partitioning, dynamic model retraining, and sophisticated engineering and policies that prioritize read/write performance.","While these design choices offer improved average-case performance, the emphasis on flexibility and performance increases the attack surface by allowing adversarial behaviors that maximize ALEX's memory space and time complexity in worst-case scenarios.","In this work, we present the first systematic investigation of algorithmic complexity attacks (ACAs) targeting the worst-case scenarios of ALEX.","We introduce new ACAs that fall into two categories, space ACAs and time ACAs, which target the memory space and time complexity, respectively.","First, our space ACA on data nodes exploits ALEX's gapped array layout and uses Multiple-Choice Knapsack (MCK) to generate an optimal adversarial insertion plan for maximizing the memory consumption at the data node level.","Second, our space ACA on internal nodes exploits ALEX's catastrophic cost mitigation mechanism, causing an out-of-memory error with only a few hundred adversarial insertions.","Third, our time ACA generates pathological insertions to increase the disparity between the actual key distribution and the linear models of data nodes, deteriorating the runtime performance by up to 1,641X compared to ALEX operating under legitimate workloads."],"url":"http://arxiv.org/abs/2403.12433v1","category":"cs.DB"}
{"created":"2024-03-19 04:08:18","title":"UniDexFPM: Universal Dexterous Functional Pre-grasp Manipulation Via Diffusion Policy","abstract":"Objects in the real world are often not naturally positioned for functional grasping, which usually requires repositioning and reorientation before they can be grasped, a process known as pre-grasp manipulation. However, effective learning of universal dexterous functional pre-grasp manipulation necessitates precise control over relative position, relative orientation, and contact between the hand and object, while generalizing to diverse dynamic scenarios with varying objects and goal poses. We address the challenge by using teacher-student learning. We propose a novel mutual reward that incentivizes agents to jointly optimize three key criteria. Furthermore, we introduce a pipeline that leverages a mixture-of-experts strategy to learn diverse manipulation policies, followed by a diffusion policy to capture complex action distributions from these experts. Our method achieves a success rate of 72.6% across 30+ object categories encompassing 1400+ objects and 10k+ goal poses. Notably, our method relies solely on object pose information for universal dexterous functional pre-grasp manipulation by using extrinsic dexterity and adjusting from feedback. Additional experiments under noisy object pose observation showcase the robustness of our method and its potential for real-world applications. The demonstrations can be viewed at https://unidexfpm.github.io.","sentences":["Objects in the real world are often not naturally positioned for functional grasping, which usually requires repositioning and reorientation before they can be grasped, a process known as pre-grasp manipulation.","However, effective learning of universal dexterous functional pre-grasp manipulation necessitates precise control over relative position, relative orientation, and contact between the hand and object, while generalizing to diverse dynamic scenarios with varying objects and goal poses.","We address the challenge by using teacher-student learning.","We propose a novel mutual reward that incentivizes agents to jointly optimize three key criteria.","Furthermore, we introduce a pipeline that leverages a mixture-of-experts strategy to learn diverse manipulation policies, followed by a diffusion policy to capture complex action distributions from these experts.","Our method achieves a success rate of 72.6% across 30+ object categories encompassing 1400+ objects and 10k+ goal poses.","Notably, our method relies solely on object pose information for universal dexterous functional pre-grasp manipulation by using extrinsic dexterity and adjusting from feedback.","Additional experiments under noisy object pose observation showcase the robustness of our method and its potential for real-world applications.","The demonstrations can be viewed at https://unidexfpm.github.io."],"url":"http://arxiv.org/abs/2403.12421v1","category":"cs.RO"}
{"created":"2024-03-19 03:55:39","title":"VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation","abstract":"This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.","sentences":["This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation.","With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances.","Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation.","Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding."],"url":"http://arxiv.org/abs/2403.12415v1","category":"cs.CV"}
{"created":"2024-03-19 03:54:46","title":"Development of low-radon ultra-pure water for the Jiangmen Underground Neutrino Observatory","abstract":"The Jiangmen Underground Neutrino Observatory(JUNO) is a state-of-the-art liquid scintillator-based neutrino physics experiment under construction in South China. To reduce the background from external radioactivities, a water Cherenkov detector composed of 35~kton ultra-pure water and 2,400 20-inch photomultiplier tubes is developed. Even after specialized treatment, ultra-pure water still contains trace levels of radioactive elements that can contribute to the detector background. Among which $^{222}$Rn is particularly significant. To address this, an online radon removal system based on the JUNO prototype has been developed. By integrating micro-bubble generators to enhance degasser's radon removal efficiency, the radon concentration in water can be reduced to 1~mBq/m$^{3}$ level, meeting the stringent requirements of JUNO. Additionally, a highly sensitive online radon concentration measurement system capable of detecting concentrations $\\sim$1~mBq/m$^3$ has been developed to monitor the radon concentration in water. In this paper, the details regarding both systems will be presented.","sentences":["The Jiangmen Underground Neutrino Observatory(JUNO) is a state-of-the-art liquid scintillator-based neutrino physics experiment under construction in South China.","To reduce the background from external radioactivities, a water Cherenkov detector composed of 35~kton ultra-pure water and 2,400 20-inch photomultiplier tubes is developed.","Even after specialized treatment, ultra-pure water still contains trace levels of radioactive elements that can contribute to the detector background.","Among which $^{222}$Rn is particularly significant.","To address this, an online radon removal system based on the JUNO prototype has been developed.","By integrating micro-bubble generators to enhance degasser's radon removal efficiency, the radon concentration in water can be reduced to 1~mBq/m$^{3}$ level, meeting the stringent requirements of JUNO.","Additionally, a highly sensitive online radon concentration measurement system capable of detecting concentrations $\\sim$1~mBq/m$^3$ has been developed to monitor the radon concentration in water.","In this paper, the details regarding both systems will be presented."],"url":"http://arxiv.org/abs/2403.12414v1","category":"physics.ins-det"}
{"created":"2024-03-19 03:53:47","title":"Third-Party Language Model Performance Prediction from Instruction","abstract":"Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions. However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task. We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time. We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, and prompt format. Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations of modern instruction-following natural language processing systems.","sentences":["Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions.","However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task.","We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time.","We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, and prompt format.","Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations of modern instruction-following natural language processing systems."],"url":"http://arxiv.org/abs/2403.12413v1","category":"cs.CL"}
{"created":"2024-03-19 17:55:22","title":"FutureDepth: Learning to Predict the Future Improves Video Depth Estimation","abstract":"In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training. More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively. In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process. Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes. At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network. Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models","sentences":["In this paper, we propose a novel video depth estimation approach, FutureDepth, which enables the model to implicitly leverage multi-frame and motion cues to improve depth estimation by making it learn to predict the future at training.","More specifically, we propose a future prediction network, F-Net, which takes the features of multiple consecutive frames and is trained to predict multi-frame features one time step ahead iteratively.","In this way, F-Net learns the underlying motion and correspondence information, and we incorporate its features into the depth decoding process.","Additionally, to enrich the learning of multiframe correspondence cues, we further leverage a reconstruction network, R-Net, which is trained via adaptively masked auto-encoding of multiframe feature volumes.","At inference time, both F-Net and R-Net are used to produce queries to work with the depth decoder, as well as a final refinement network.","Through extensive experiments on several benchmarks, i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and open-domain scenarios, we show that FutureDepth significantly improves upon baseline models, outperforms existing video depth estimation methods, and sets new state-of-the-art (SOTA) accuracy.","Furthermore, FutureDepth is more efficient than existing SOTA video depth estimation models and has similar latencies when comparing to monocular models"],"url":"http://arxiv.org/abs/2403.12953v1","category":"cs.CV"}
{"created":"2024-03-19 17:50:32","title":"On Safety in Safe Bayesian Optimization","abstract":"Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this. Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world. In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms. First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees. We provide a detailed analysis of this problem and introduce Real-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees. Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage. To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes. Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems. To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety.","sentences":["Optimizing an unknown function under safety constraints is a central task in robotics, biomedical engineering, and many other disciplines, and increasingly safe Bayesian Optimization (BO) is used for this.","Due to the safety critical nature of these applications, it is of utmost importance that theoretical safety guarantees for these algorithms translate into the real world.","In this work, we investigate three safety-related issues of the popular class of SafeOpt-type algorithms.","First, these algorithms critically rely on frequentist uncertainty bounds for Gaussian Process (GP) regression, but concrete implementations typically utilize heuristics that invalidate all safety guarantees.","We provide a detailed analysis of this problem and introduce Real-\\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent GP bounds and thus retains all theoretical guarantees.","Second, we identify assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of the target function, a key technical assumption in SafeOpt-like algorithms, as a central obstacle to real-world usage.","To overcome this challenge, we introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm, which guarantees safety without an assumption on the RKHS bound, and empirically show that this algorithm is not only safe, but also exhibits superior performance compared to the state-of-the-art on several function classes.","Third, SafeOpt and derived algorithms rely on a discrete search space, making them difficult to apply to higher-dimensional problems.","To widen the applicability of these algorithms, we introduce Lipschitz-only GP-UCB (LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional problems, while retaining safety."],"url":"http://arxiv.org/abs/2403.12948v1","category":"cs.LG"}
{"created":"2024-03-19 17:50:24","title":"Fundamental limitations on the recoverability of quantum processes","abstract":"Quantum information processing and computing tasks can be understood as quantum networks, comprising quantum states and channels and possible physical transformations on them. It is hence pertinent to estimate the change in informational content of quantum processes due to physical transformations they undergo. The physical transformations of quantum states are described by quantum channels, while the transformations of quantum channels are described by quantum superchannels. In this work, we determine fundamental limitations on how well can the physical transformation on quantum channels be undone or reversed, which are of crucial interest to design and benchmark quantum information and computation devices. In particular, we refine the quantum data processing inequality for quantum channels by strengthening the monotonicity inequality of quantum relative entropy of quantum channels under the action of an arbitrary quantum superchannel. We identify a class of quantum superchannels, which appears to be quantum superchannel analogue of quantum subunital channels, under the action of which the entropy of an arbitrary quantum channel is nondecreasing.","sentences":["Quantum information processing and computing tasks can be understood as quantum networks, comprising quantum states and channels and possible physical transformations on them.","It is hence pertinent to estimate the change in informational content of quantum processes due to physical transformations they undergo.","The physical transformations of quantum states are described by quantum channels, while the transformations of quantum channels are described by quantum superchannels.","In this work, we determine fundamental limitations on how well can the physical transformation on quantum channels be undone or reversed, which are of crucial interest to design and benchmark quantum information and computation devices.","In particular, we refine the quantum data processing inequality for quantum channels by strengthening the monotonicity inequality of quantum relative entropy of quantum channels under the action of an arbitrary quantum superchannel.","We identify a class of quantum superchannels, which appears to be quantum superchannel analogue of quantum subunital channels, under the action of which the entropy of an arbitrary quantum channel is nondecreasing."],"url":"http://arxiv.org/abs/2403.12947v1","category":"quant-ph"}
{"created":"2024-03-19 17:28:51","title":"Supporting Energy Policy Research with Large Language Models","abstract":"The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances. These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures. In this context, efficient access to and management of siting ordinance data becomes imperative. The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need. This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape. A novel contribution of this research is the integration of a decision tree framework with LLMs. Our results show that this approach is 85 to 90% accurate with outputs that can be used directly in downstream quantitative modeling. We discuss opportunities to use this work to support similar large-scale policy research in the energy sector. By unlocking new efficiencies in the extraction and analysis of legal documents using LLMs, this study enables a path forward for automated large-scale energy policy research.","sentences":["The recent growth in renewable energy development in the United States has been accompanied by a simultaneous surge in renewable energy siting ordinances.","These zoning laws play a critical role in dictating the placement of wind and solar resources that are critical for achieving low-carbon energy futures.","In this context, efficient access to and management of siting ordinance data becomes imperative.","The National Renewable Energy Laboratory (NREL) recently introduced a public wind and solar siting database to fill this need.","This paper presents a method for harnessing Large Language Models (LLMs) to automate the extraction of these siting ordinances from legal documents, enabling this database to maintain accurate up-to-date information in the rapidly changing energy policy landscape.","A novel contribution of this research is the integration of a decision tree framework with LLMs.","Our results show that this approach is 85 to 90% accurate with outputs that can be used directly in downstream quantitative modeling.","We discuss opportunities to use this work to support similar large-scale policy research in the energy sector.","By unlocking new efficiencies in the extraction and analysis of legal documents using LLMs, this study enables a path forward for automated large-scale energy policy research."],"url":"http://arxiv.org/abs/2403.12924v1","category":"cs.CL"}
{"created":"2024-03-19 17:28:48","title":"Solving Combinatorial Pricing Problems using Embedded Dynamic Programming Models","abstract":"The combinatorial pricing problem (CPP) is a bilevel problem in which the leader maximizes their revenue by imposing tolls on certain items that they can control. Based on the tolls set by the leader, the follower selects a subset of items corresponding to an optimal solution of a combinatorial optimization problem. To accomplish the leader's goal, the tolls need to be sufficiently low to discourage the follower from choosing the items offered by the competitors. In this paper, we derive a single-level reformulation for the CPP by rewriting the follower's problem as a longest path problem using a dynamic programming model, and then taking its dual and applying strong duality. We proceed to solve the reformulation in a dynamic fashion with a cutting plane method. We apply this methodology to 2 distinct dynamic programming models, namely, a novel formulation designated as selection diagram and the well-known decision diagram. We also produce numerical results to evaluate their performances across 3 different specializations of the CPP and a closely related problem that is the knapsack interdiction problem. Our results showcase the potential of the 2 proposed reformulations over the natural value function approach, expanding the set of tools to solve combinatorial bilevel programs.","sentences":["The combinatorial pricing problem (CPP) is a bilevel problem in which the leader maximizes their revenue by imposing tolls on certain items that they can control.","Based on the tolls set by the leader, the follower selects a subset of items corresponding to an optimal solution of a combinatorial optimization problem.","To accomplish the leader's goal, the tolls need to be sufficiently low to discourage the follower from choosing the items offered by the competitors.","In this paper, we derive a single-level reformulation for the CPP by rewriting the follower's problem as a longest path problem using a dynamic programming model, and then taking its dual and applying strong duality.","We proceed to solve the reformulation in a dynamic fashion with a cutting plane method.","We apply this methodology to 2 distinct dynamic programming models, namely, a novel formulation designated as selection diagram and the well-known decision diagram.","We also produce numerical results to evaluate their performances across 3 different specializations of the CPP and a closely related problem that is the knapsack interdiction problem.","Our results showcase the potential of the 2 proposed reformulations over the natural value function approach, expanding the set of tools to solve combinatorial bilevel programs."],"url":"http://arxiv.org/abs/2403.12923v1","category":"math.OC"}
{"created":"2024-03-19 16:29:59","title":"Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments","abstract":"In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation. In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance. Previous methods employed prototype methods for domain adaptation on robust feature spaces. However, these approaches struggle to effectively classify classes with similar features under noisy environments. To address this issue, we propose a new method to detect and correct confusing class pair. We first divide classes into easy and hard classes based on the small loss criterion. We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes. We apply label correction to the noisy samples within the confusing pair. With the proposed label correction method, we can train our model with more accurate labels. Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods. Our codes are publicly available at https://github.com/Hehxcf/CPC/.","sentences":["In this paper, we address unsupervised domain adaptation under noisy environments, which is more challenging and practical than traditional domain adaptation.","In this scenario, the model is prone to overfitting noisy labels, resulting in a more pronounced domain shift and a notable decline in the overall model performance.","Previous methods employed prototype methods for domain adaptation on robust feature spaces.","However, these approaches struggle to effectively classify classes with similar features under noisy environments.","To address this issue, we propose a new method to detect and correct confusing class pair.","We first divide classes into easy and hard classes based on the small loss criterion.","We then leverage the top-2 predictions for each sample after aligning the source and target domain to find the confusing pair in the hard classes.","We apply label correction to the noisy samples within the confusing pair.","With the proposed label correction method, we can train our model with more accurate labels.","Extensive experiments confirm the effectiveness of our method and demonstrate its favorable performance compared with existing state-of-the-art methods.","Our codes are publicly available at https://github.com/Hehxcf/CPC/."],"url":"http://arxiv.org/abs/2403.12883v1","category":"cs.CV"}
{"created":"2024-03-19 16:18:12","title":"Markovian lifting and optimal control for integral stochastic Volterra equations with completely monotone kernels","abstract":"In this paper, we focus on solving the optimal control problem for integral stochastic Volterra equations in a finite dimensional setting. In our setting, the noise term is driven by a pure jump L\\'evy noise and the control acts on the intensity of the jumps.   We use recent techniques proposed by Hamaguchi, where a crucial requirement is that the convolution kernel should be a completely monotone function. This allows us to use Bernstein's representation and the machinery of Laplace transform to obtain a Markovian lift.   It is natural that the Markovian lift, in whatever form constructed, transforms the state equation into a stochastic differential equation in an infinite-dimensional space. This space should be large enough to contain all the information about the history of the process. Hence, although the original equation is taken in a finite dimensional space, the resulting lift is always infinite dimensional.   We solve the problem by using the forward-backward approach in the infinite-dimensional setting and prove the existence of the optimal control for the original problem. Under additional assumptions on the coefficients, we see that a control in closed-loop form can be achieved.","sentences":["In this paper, we focus on solving the optimal control problem for integral stochastic Volterra equations in a finite dimensional setting.","In our setting, the noise term is driven by a pure jump L\\'evy noise and the control acts on the intensity of the jumps.   ","We use recent techniques proposed by Hamaguchi, where a crucial requirement is that the convolution kernel should be a completely monotone function.","This allows us to use Bernstein's representation and the machinery of Laplace transform to obtain a Markovian lift.   ","It is natural that the Markovian lift, in whatever form constructed, transforms the state equation into a stochastic differential equation in an infinite-dimensional space.","This space should be large enough to contain all the information about the history of the process.","Hence, although the original equation is taken in a finite dimensional space, the resulting lift is always infinite dimensional.   ","We solve the problem by using the forward-backward approach in the infinite-dimensional setting and prove the existence of the optimal control for the original problem.","Under additional assumptions on the coefficients, we see that a control in closed-loop form can be achieved."],"url":"http://arxiv.org/abs/2403.12875v1","category":"math.OC"}
{"created":"2024-03-19 16:17:21","title":"Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints","abstract":"We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance. The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints. The output irradiance values are transformed to focus on unknown short-term dynamics. Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably. Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features). For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model.","sentences":["We report a data-parsimonious machine learning model for short-term forecasting of solar irradiance.","The model inputs include sky camera images that are reduced to scalar features to meet data transmission constraints.","The output irradiance values are transformed to focus on unknown short-term dynamics.","Inspired by control theory, a noise input is used to reflect unmeasured variables and is shown to improve model predictions, often considerably.","Five years of data from the NREL Solar Radiation Research Laboratory were used to create three rolling train-validate sets and determine the best representations for time, the optimal span of input measurements, and the most impactful model input data (features).","For the chosen test data, the model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline 134.35 $W/m^2$ using the persistence of cloudiness model."],"url":"http://arxiv.org/abs/2403.12873v1","category":"cs.LG"}
{"created":"2024-03-19 16:08:27","title":"A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection","abstract":"Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry. The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations. Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated. While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources. In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints.","sentences":["Spacecraft operations are highly critical, demanding impeccable reliability and safety.","Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures.","With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations.","This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data.","The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures.","Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types.","Initial results indicate that while CNNs excel in identifying spatial patterns and may be effective for some classes of spacecraft data, LSTMs and RNNs show a marked proficiency in capturing temporal anomalies seen in time-series spacecraft telemetry.","The Transformer-based architectures, given their ability to focus on both local and global contexts, have showcased promising results, especially in scenarios where anomalies are subtle and span over longer durations.","Additionally, considerations such as computational efficiency, ease of deployment, and real-time processing capabilities were evaluated.","While CNNs and LSTMs demonstrated a balance between accuracy and computational demands, Transformer architectures, though highly accurate, require significant computational resources.","In conclusion, the choice of deep learning architecture for spacecraft anomaly detection is highly contingent on the nature of the data, the type of anomalies, and operational constraints."],"url":"http://arxiv.org/abs/2403.12864v1","category":"cs.LG"}
{"created":"2024-03-19 15:59:47","title":"An inhomogeneous porous medium equation with non-integrable data: asymptotics","abstract":"We investigate the asymptotic behavior as $t\\to+\\infty$ of solutions to a weighted porous medium equation in $ \\mathbb{R}^N $, whose weight $\\rho(x)$ behaves at spatial infinity like $ |x|^{-\\gamma} $ with subcritical power, namely $ \\gamma \\in [0,2) $. Inspired by some results by Alikakos-Rostamian and Kamin-Ughi from the 1980s on the unweighted problem, we focus on solutions whose initial data $u_0(x)$ are not globally integrable with respect to the weight and behave at infinity like $ |x|^{-\\alpha} $, for $\\alpha\\in(0,N-\\gamma)$. In the special case $ \\rho(x)=|x|^{-\\gamma} $ and $ u_0(x)=|x|^{-\\alpha} $ we show that self-similar solutions of Barenblatt type, i.e. reminiscent of the usual source-type solutions, still exist, although they are no longer compactly supported. Moreover, they exhibit a transition phenomenon which is new even for the unweighted equation. We prove that such self-similar solutions are attractors for the original problem, and convergence takes place globally in suitable weighted $ L^p $ spaces for $p\\in[1,\\infty)$ and even globally in $L^\\infty$ under some mild additional regularity assumptions on the weight. Among the fundamental tools that we exploit, it is worth mentioning a global smoothing effect for non-integrable data.","sentences":["We investigate the asymptotic behavior as $t\\to+\\infty$ of solutions to a weighted porous medium equation in $ \\mathbb{R}^N $, whose weight $\\rho(x)$ behaves at spatial infinity like $ |x|^{-\\gamma} $ with subcritical power, namely $ \\gamma \\in [0,2) $.","Inspired by some results by Alikakos-Rostamian and Kamin-Ughi from the 1980s on the unweighted problem, we focus on solutions whose initial data $u_0(x)$ are not globally integrable with respect to the weight and behave at infinity like $ |x|^{-\\alpha} $, for $\\alpha\\in(0,N-\\gamma)$. In the special case $ \\rho(x)=|x|^{-\\gamma} $ and $ u_0(x)=|x|^{-\\alpha} $ we show that self-similar solutions of Barenblatt type, i.e. reminiscent of the usual source-type solutions, still exist, although they are no longer compactly supported.","Moreover, they exhibit a transition phenomenon which is new even for the unweighted equation.","We prove that such self-similar solutions are attractors for the original problem, and convergence takes place globally in suitable weighted $ L^p $ spaces for $p\\in[1,\\infty)$ and even globally in $L^\\infty$ under some mild additional regularity assumptions on the weight.","Among the fundamental tools that we exploit, it is worth mentioning a global smoothing effect for non-integrable data."],"url":"http://arxiv.org/abs/2403.12854v1","category":"math.AP"}
{"created":"2024-03-19 15:55:37","title":"3d Quantum Trace Map","abstract":"We construct the 3d quantum trace map, a homomorphism from the Kauffman bracket skein module of an ideally triangulated 3-manifold to its (square root) quantum gluing module, thereby giving a precise relationship between the two quantizations of the character variety of ideally triangulated 3-manifolds. This map, whose existence was conjectured earlier by Agarwal, Gang, Lee, and Romo, is a natural 3-dimensional analog of the 2d quantum trace map of Bonahon and Wong. Our construction is based on the study of stated skein modules and their behavior under splitting, especially into face suspensions.","sentences":["We construct the 3d quantum trace map, a homomorphism from the Kauffman bracket skein module of an ideally triangulated 3-manifold to its (square root) quantum gluing module, thereby giving a precise relationship between the two quantizations of the character variety of ideally triangulated 3-manifolds.","This map, whose existence was conjectured earlier by Agarwal, Gang, Lee, and Romo, is a natural 3-dimensional analog of the 2d quantum trace map of Bonahon and Wong.","Our construction is based on the study of stated skein modules and their behavior under splitting, especially into face suspensions."],"url":"http://arxiv.org/abs/2403.12850v1","category":"math.GT"}
{"created":"2024-03-19 13:53:37","title":"Towards Multimodal In-Context Learning for Vision & Language Models","abstract":"Inspired by the emergence of Large Language Models (LLMs) that can truly understand human language, significant progress has been made in aligning other, non-language, modalities to be `understandable' by an LLM, primarily via converting their samples into a sequence of embedded language-like tokens directly fed into the LLM (decoder) input stream. However, so far limited attention has been given to transferring (and evaluating) one of the core LLM capabilities to the emerging VLMs, namely the In-Context Learning (ICL) ability, or in other words to guide VLMs to desired target downstream tasks or output structure using in-context image+text demonstrations. In this work, we dive deeper into analyzing the capabilities of some of the state-of-the-art VLMs to follow ICL instructions, discovering them to be somewhat lacking. We discover that even models that underwent large-scale mixed modality pre-training and were implicitly guided to make use of interleaved image and text information (intended to consume helpful context from multiple images) under-perform when prompted with few-shot (ICL) demonstrations, likely due to their lack of `direct' ICL instruction tuning. To test this conjecture, we propose a simple, yet surprisingly effective, strategy of extending a common VLM alignment framework with ICL support, methodology, and curriculum. We explore, analyze, and provide insights into effective data mixes, leading up to a significant 21.03% (and 11.3% on average) ICL performance boost over the strongest VLM baselines and a variety of ICL benchmarks. We also contribute new benchmarks for ICL evaluation in VLMs and discuss their advantages over the prior art.","sentences":["Inspired by the emergence of Large Language Models (LLMs) that can truly understand human language, significant progress has been made in aligning other, non-language, modalities to be `understandable' by an LLM, primarily via converting their samples into a sequence of embedded language-like tokens directly fed into the LLM (decoder) input stream.","However, so far limited attention has been given to transferring (and evaluating) one of the core LLM capabilities to the emerging VLMs, namely the In-Context Learning (ICL) ability, or in other words to guide VLMs to desired target downstream tasks or output structure using in-context image+text demonstrations.","In this work, we dive deeper into analyzing the capabilities of some of the state-of-the-art VLMs to follow ICL instructions, discovering them to be somewhat lacking.","We discover that even models that underwent large-scale mixed modality pre-training and were implicitly guided to make use of interleaved image and text information (intended to consume helpful context from multiple images) under-perform when prompted with few-shot (ICL) demonstrations, likely due to their lack of `direct' ICL instruction tuning.","To test this conjecture, we propose a simple, yet surprisingly effective, strategy of extending a common VLM alignment framework with ICL support, methodology, and curriculum.","We explore, analyze, and provide insights into effective data mixes, leading up to a significant 21.03% (and 11.3% on average) ICL performance boost over the strongest VLM baselines and a variety of ICL benchmarks.","We also contribute new benchmarks for ICL evaluation in VLMs and discuss their advantages over the prior art."],"url":"http://arxiv.org/abs/2403.12736v1","category":"cs.CV"}
{"created":"2024-03-19 13:43:00","title":"Optimizing Leapover Lengths of L\u00e9vy Flights with Resetting","abstract":"We consider a one-dimensional search process under stochastic resetting conditions. A target is located at $b\\geq0$ and a searcher, starting from the origin, performs a discrete-time random walk with independent jumps drawn from a heavy-tailed distribution. Before each jump, there is a given probability $r$ of restarting the walk from the initial position. The efficiency of a \"myopic search\" - in which the search stops upon crossing the target for the first time - is usually characterized in terms of the first-passage time $\\tau$. On the other hand, great relevance is encapsulated by the leapover length $l = x_{\\tau} - b$, which measures how far from the target the search ends. For symmetric heavy-tailed jump distributions, in the absence of resetting the average leapover is always infinite. Here we show instead that resetting induces a finite average leapover $\\ell_b(r)$ if the mean jump length is finite. We compute exactly $\\ell_b(r)$ and determine the condition under which resetting allows for nontrivial optimization, i.e., for the existence of $r^*$ such that $\\ell_b(r^*)$ is minimal and smaller than the average leapover of the single jump.","sentences":["We consider a one-dimensional search process under stochastic resetting conditions.","A target is located at $b\\geq0$ and a searcher, starting from the origin, performs a discrete-time random walk with independent jumps drawn from a heavy-tailed distribution.","Before each jump, there is a given probability $r$ of restarting the walk from the initial position.","The efficiency of a \"myopic search\" - in which the search stops upon crossing the target for the first time - is usually characterized in terms of the first-passage time $\\tau$. On the other hand, great relevance is encapsulated by the leapover length $l = x_{\\tau} - b$, which measures how far from the target the search ends.","For symmetric heavy-tailed jump distributions, in the absence of resetting the average leapover is always infinite.","Here we show instead that resetting induces a finite average leapover $\\ell_b(r)$ if the mean jump length is finite.","We compute exactly $\\ell_b(r)$ and determine the condition under which resetting allows for nontrivial optimization, i.e., for the existence of $r^*$ such that $\\ell_b(r^*)$ is minimal and smaller than the average leapover of the single jump."],"url":"http://arxiv.org/abs/2403.12727v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-19 13:05:22","title":"$J/\u03c8$ production within a jet in high-energy proton-proton and nucleus-nucleus collisions","abstract":"Within the framework of leading power factorization formalism of nonrelativistic quantum chromodynamics, we calculate the jet fragmentation function for $J/\\psi$ production in proton-proton (pp) collisions ranging from $\\sqrt{s}=500$ GeV to $13$ TeV. The reasonable agreements between theory and experimental data indicate that $J/\\psi$ production within a jet is mainly dominated by gluon fragmentation. Such a mechanism can be further tested by the predicted jet transverse momentum and radius dependence of jet fragmentation function. Based on the satisfying description of pp baseline, we carry out the first theoretical investigation on medium modification on $J/\\psi$ production within jet in heavy-ion collisions at the Large Hadron Collider, using a linear Boltzmann transport model combined with hydrodynamics for the simulation of jet-medium interaction. The consistency with the experimental measurement on nuclear modification factor $R_{\\text{AA}}$ by CMS collaboration reveals that the gluon jet quenching is the driving force for the suppression of $J/\\psi$ production in jet. Furthermore, we make predictions for the dependence of $R_{\\text{AA}}$ on the jet transverse momentum and jet radius $R$, which can be tested in future measurements to further constrain the flavor dependence of jet quenching.","sentences":["Within the framework of leading power factorization formalism of nonrelativistic quantum chromodynamics, we calculate the jet fragmentation function for $J/\\psi$ production in proton-proton (pp) collisions ranging from $\\sqrt{s}=500$ GeV to $13$ TeV. The reasonable agreements between theory and experimental data indicate that $J/\\psi$ production within a jet is mainly dominated by gluon fragmentation.","Such a mechanism can be further tested by the predicted jet transverse momentum and radius dependence of jet fragmentation function.","Based on the satisfying description of pp baseline, we carry out the first theoretical investigation on medium modification on $J/\\psi$ production within jet in heavy-ion collisions at the Large Hadron Collider, using a linear Boltzmann transport model combined with hydrodynamics for the simulation of jet-medium interaction.","The consistency with the experimental measurement on nuclear modification factor $R_{\\text{AA}}$ by CMS collaboration reveals that the gluon jet quenching is the driving force for the suppression of $J/\\psi$ production in jet.","Furthermore, we make predictions for the dependence of $R_{\\text{AA}}$ on the jet transverse momentum and jet radius $R$, which can be tested in future measurements to further constrain the flavor dependence of jet quenching."],"url":"http://arxiv.org/abs/2403.12704v1","category":"hep-ph"}
{"created":"2024-03-19 12:21:10","title":"$n$-cotorsion pairs and recollements of extriangulated categories","abstract":"In this article, we prove that if $(\\mathcal A ,\\mathcal B,\\mathcal C)$ is a recollement of extriangulated categories, then $n$-cotorsion pairs in $\\mathcal A$ and $\\mathcal C$ can induce $n$-cotorsion pairs in $\\mathcal B$. Conversely, this holds true under natural assumptions. Besides, we give mild conditions on a pseudo cluster tilting subcategory on the middle category of a recollement of extriangulated categories, for the corresponding additive quotients to form a recollement of semi-abelian categories.","sentences":["In this article, we prove that if $(\\mathcal A ,\\mathcal B,\\mathcal C)$ is a recollement of extriangulated categories, then $n$-cotorsion pairs in $\\mathcal A$ and $\\mathcal C$ can induce $n$-cotorsion pairs in $\\mathcal B$. Conversely, this holds true under natural assumptions.","Besides, we give mild conditions on a pseudo cluster tilting subcategory on the middle category of a recollement of extriangulated categories, for the corresponding additive quotients to form a recollement of semi-abelian categories."],"url":"http://arxiv.org/abs/2403.12673v1","category":"math.RT"}
{"created":"2024-03-19 11:36:13","title":"Well-posedness of the stochastic thin-film equation with an interface potential","abstract":"We consider strictly positive solutions to a class of fourth-order conservative quasilinear SPDEs on the $d$-dimensional torus modeled after the stochastic thin-film equation. Using recent results on quasilinear stochastic evolution equations, we show local well-posedness, blow-up criteria and instantaneous regularization in suitable function spaces under corresponding smoothness conditions on the noise. As a key ingredient, we prove stochastic maximal $L^p$-regularity estimates for thin film-type operators with measurable in time coefficients. With the aid of the above-mentioned results, we obtain global well-posedness of the stochastic thin-film equation with an interface potential by closing $\\alpha$-entropy estimates and subsequently an energy estimate in dimension one. In particular, we can treat a wide range of mobility functions including the power laws $u^n$ for $n\\in [0,6)$ as long as the interface potential is sufficiently repulsive.","sentences":["We consider strictly positive solutions to a class of fourth-order conservative quasilinear SPDEs on the $d$-dimensional torus modeled after the stochastic thin-film equation.","Using recent results on quasilinear stochastic evolution equations, we show local well-posedness, blow-up criteria and instantaneous regularization in suitable function spaces under corresponding smoothness conditions on the noise.","As a key ingredient, we prove stochastic maximal $L^p$-regularity estimates for thin film-type operators with measurable in time coefficients.","With the aid of the above-mentioned results, we obtain global well-posedness of the stochastic thin-film equation with an interface potential by closing $\\alpha$-entropy estimates and subsequently an energy estimate in dimension one.","In particular, we can treat a wide range of mobility functions including the power laws $u^n$ for $n\\in [0,6)$ as long as the interface potential is sufficiently repulsive."],"url":"http://arxiv.org/abs/2403.12652v1","category":"math.AP"}
{"created":"2024-03-19 11:25:43","title":"Interlayer Dzyaloshinskii-Moriya interaction in synthetic ferrimagnets","abstract":"The antisymmetric interlayer exchange interaction, i.e., interlayer Dzyaloshinskii-Moriya interaction (IL-DMI) has attracted significant interest since this long-range chiral spin interaction provides a new dimension for controlling spin textures and dynamics. However, the role of IL-DMI in the field induced and spin-orbit torque (SOT) induced switching of synthetic ferrimagnets (SFi) has not been uncovered. Here, we exploit interlayer chiral exchange bias fields in SFi to address both the sign and magnitude of the IL-DMI. Depending on the degree of imbalance between the two magnetic moments of the SFi, the amount of asymmetry, addressed via loop shifts of the hysteresis loops under an in-plane field reveals a unidirectional and chiral nature of the IL-DMI. The devices are then tested with SOT switching experiments and the process is examined via both transient state and steady state detection. In addition to field-free SOT switching, we find that the combination of IL-DMI and SOT give rise to multi-resistance states, which provides a possible direction for the future design of neuromorphic computing devices based on SOT. This work is a step towards characterizing and understanding the IL-DMI for spintronic applications.","sentences":["The antisymmetric interlayer exchange interaction, i.e., interlayer Dzyaloshinskii-Moriya interaction (IL-DMI) has attracted significant interest since this long-range chiral spin interaction provides a new dimension for controlling spin textures and dynamics.","However, the role of IL-DMI in the field induced and spin-orbit torque (SOT) induced switching of synthetic ferrimagnets (SFi) has not been uncovered.","Here, we exploit interlayer chiral exchange bias fields in SFi to address both the sign and magnitude of the IL-DMI.","Depending on the degree of imbalance between the two magnetic moments of the SFi, the amount of asymmetry, addressed via loop shifts of the hysteresis loops under an in-plane field reveals a unidirectional and chiral nature of the IL-DMI.","The devices are then tested with SOT switching experiments and the process is examined via both transient state and steady state detection.","In addition to field-free SOT switching, we find that the combination of IL-DMI and SOT give rise to multi-resistance states, which provides a possible direction for the future design of neuromorphic computing devices based on SOT.","This work is a step towards characterizing and understanding the IL-DMI for spintronic applications."],"url":"http://arxiv.org/abs/2403.12642v1","category":"physics.app-ph"}
{"created":"2024-03-19 10:59:22","title":"High Precision Inertial Sensors on a One Inch Diameter Optic","abstract":"Compact, high-precision inertial sensors are needed to isolate many modern physics experiments from disturbances caused by seismic motion. We present a novel inertial sensor whose mechanical oscillator fits on a standard one-inch diameter optic. The oscillators achieve a Quality factor of over 600,000 and a resonance frequency of 50\\,Hz, giving them a suspension thermal noise floor lower than all commercially available inertial sensors. The oscillator is combined with a Pound-Drever-Hall based readout scheme that achieves a displacement noise of 100\\,f\\msqrthz above 0.2\\,Hz. We integrate the oscillator and readout to make two inertial sensors. Of order n$g$ performance is achieved in a broad band from 0.1\\,Hz to 200\\,Hz. Below 20\\,Hz, the sensor presented here offers comparable performance to the best inertial sensors available today while being a fraction of the size. Above 20\\,Hz, the sensor is, to the author's knowledge, the best demonstrated in the literature to date for a device of this style, with a self-noise floor of 0.1\\,n$g$\\sqrthz. The excellent performance of the sensors across the relevant seismic frequencies, vacuum compatibility, and compact size make it a prime candidate for integration into sophisticated seismic isolation schemes, such as those used by gravitational wave detectors.","sentences":["Compact, high-precision inertial sensors are needed to isolate many modern physics experiments from disturbances caused by seismic motion.","We present a novel inertial sensor whose mechanical oscillator fits on a standard one-inch diameter optic.","The oscillators achieve a Quality factor of over 600,000 and a resonance frequency of 50\\,Hz, giving them a suspension thermal noise floor lower than all commercially available inertial sensors.","The oscillator is combined with a Pound-Drever-Hall based readout scheme that achieves a displacement noise of 100\\,f\\msqrthz above 0.2\\,Hz.","We integrate the oscillator and readout to make two inertial sensors.","Of order n$g$ performance is achieved in a broad band from 0.1\\,Hz to 200\\,Hz.","Below 20\\,Hz, the sensor presented here offers comparable performance to the best inertial sensors available today while being a fraction of the size.","Above 20\\,Hz, the sensor is, to the author's knowledge, the best demonstrated in the literature to date for a device of this style, with a self-noise floor of 0.1\\,n$g$\\sqrthz.","The excellent performance of the sensors across the relevant seismic frequencies, vacuum compatibility, and compact size make it a prime candidate for integration into sophisticated seismic isolation schemes, such as those used by gravitational wave detectors."],"url":"http://arxiv.org/abs/2403.12632v1","category":"physics.ins-det"}
{"created":"2024-03-19 09:38:57","title":"The Smoothed Duality Gap as a Stopping Criterion","abstract":"We optimize the running time of the primal-dual algorithms by optimizing their stopping criteria for solving convex optimization problems under affine equality constraints, which means terminating the algorithm earlier with fewer iterations. We study the relations between four stopping criteria and show under which conditions they are accurate to detect optimal solutions. The uncomputable one: ''Optimality gap and Feasibility error'', and the computable ones: the ''Karush-Kuhn-Tucker error'', the ''Projected Duality Gap'', and the ''Smoothed Duality Gap''. Assuming metric sub-regularity or quadratic error bound, we establish that all of the computable criteria provide practical upper bounds for the optimality gap, and approximate it effectively. Furthermore, we establish comparability between some of the computable criteria under certain conditions. Numerical experiments on basis pursuit, and quadratic programs with(out) non-negative weights corroborate these findings and show the superior stability of the smoothed duality gap over the rest.","sentences":["We optimize the running time of the primal-dual algorithms by optimizing their stopping criteria for solving convex optimization problems under affine equality constraints, which means terminating the algorithm earlier with fewer iterations.","We study the relations between four stopping criteria and show under which conditions they are accurate to detect optimal solutions.","The uncomputable one: ''Optimality gap and Feasibility error'', and the computable ones: the ''Karush-Kuhn-Tucker error'', the ''Projected Duality Gap'', and the ''Smoothed Duality Gap''.","Assuming metric sub-regularity or quadratic error bound, we establish that all of the computable criteria provide practical upper bounds for the optimality gap, and approximate it effectively.","Furthermore, we establish comparability between some of the computable criteria under certain conditions.","Numerical experiments on basis pursuit, and quadratic programs with(out) non-negative weights corroborate these findings and show the superior stability of the smoothed duality gap over the rest."],"url":"http://arxiv.org/abs/2403.12579v1","category":"math.OC"}
{"created":"2024-03-19 09:36:08","title":"Rate-optimal higher-order adaptive conforming FEM for biharmonic eigenvalue problems on polygonal domains","abstract":"The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021. It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical. This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof. This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes. The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory. Five computational benchmarks display accurate reference eigenvalues up to 30 digits.","sentences":["The a posteriori error analysis of the classical Argyris finite element methods dates back to 1996, while the optimal convergence rates of associated adaptive finite element schemes are established only very recently in 2021.","It took a long time to realise the necessity of an extension of the classical finite element spaces to make them hierarchical.","This paper establishes the novel adaptive schemes for the biharmonic eigenvalue problems and provides a mathematical proof of optimal convergence rates towards a simple eigenvalue and numerical evidence thereof.","This makes the suggested algorithm highly competitive and clearly justifies the higher computational and implementational costs compared to low-order nonconforming schemes.","The numerical experiments provide overwhelming evidence that higher polynomial degrees pay off with higher convergence rates and underline that adaptive mesh-refining is mandatory.","Five computational benchmarks display accurate reference eigenvalues up to 30 digits."],"url":"http://arxiv.org/abs/2403.12577v1","category":"math.NA"}
{"created":"2024-03-19 09:00:23","title":"Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation","abstract":"Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations. However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT. Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM). Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve. To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the training process into two stages. In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained LLM to inspire the LLM's translation potential. This factorized training strategy proves to be highly effective as evidenced by significant improvements achieved across three SLT datasets which are all conducted under the gloss-free setting.","sentences":["Previous Sign Language Translation (SLT) methods achieve superior performance by relying on gloss annotations.","However, labeling high-quality glosses is a labor-intensive task, which limits the further development of SLT.","Although some approaches work towards gloss-free SLT through jointly training the visual encoder and translation network, these efforts still suffer from poor performance and inefficient use of the powerful Large Language Model (LLM).","Most seriously, we find that directly introducing LLM into SLT will lead to insufficient learning of visual representations as LLM dominates the learning curve.","To address these problems, we propose Factorized Learning assisted with Large Language Model (FLa-LLM) for gloss-free SLT.","Concretely, we factorize the training process into two stages.","In the visual initialing stage, we employ a lightweight translation model after the visual encoder to pre-train the visual encoder.","In the LLM fine-tuning stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained LLM to inspire the LLM's translation potential.","This factorized training strategy proves to be highly effective as evidenced by significant improvements achieved across three SLT datasets which are all conducted under the gloss-free setting."],"url":"http://arxiv.org/abs/2403.12556v1","category":"cs.CL"}
{"created":"2024-03-19 08:47:26","title":"Non-negative matrix underapproximation as optimal frequency band selector","abstract":"Time-frequency representation (TFR) is often used for non-stationary signal analysis. The most intuitive and interpretable TFR is the spectrogram. Recently, a concept of non-negative matrix factorization (NMF) has been successfully applied to local damage detection in rolling elements of bearings via spectrogram factorization. NMF applied to the spectrogram allows one to find an informative frequency band, which could be further used as a filter characteristic. However, the obtained filter characteristics mostly detect the informative frequency band, which also encompasses a lot of noise. In the case where noise is more problematic, as is the case for acoustic signals from industrial machines, the NMF hardly detects the damage. To solve this problem and obtain more selective filters, which are more robust to noise, we propose the non-negative matrix under-approximation (NMU) as an informative frequency band selector. Due to the more sparse parts-based representation of the NMU compared to NMF, NMU provides more selective filter characteristics, which neglect the non-informative frequency bands related to the noise. In practice, it means that NMU gives a better signal-to-noise ratio for the filtered signal. The efficiency of the proposed approach has been validated on the vibration signal from the test rig and the acoustic signal from an idler.","sentences":["Time-frequency representation (TFR) is often used for non-stationary signal analysis.","The most intuitive and interpretable TFR is the spectrogram.","Recently, a concept of non-negative matrix factorization (NMF) has been successfully applied to local damage detection in rolling elements of bearings via spectrogram factorization.","NMF applied to the spectrogram allows one to find an informative frequency band, which could be further used as a filter characteristic.","However, the obtained filter characteristics mostly detect the informative frequency band, which also encompasses a lot of noise.","In the case where noise is more problematic, as is the case for acoustic signals from industrial machines, the NMF hardly detects the damage.","To solve this problem and obtain more selective filters, which are more robust to noise, we propose the non-negative matrix under-approximation (NMU) as an informative frequency band selector.","Due to the more sparse parts-based representation of the NMU compared to NMF, NMU provides more selective filter characteristics, which neglect the non-informative frequency bands related to the noise.","In practice, it means that NMU gives a better signal-to-noise ratio for the filtered signal.","The efficiency of the proposed approach has been validated on the vibration signal from the test rig and the acoustic signal from an idler."],"url":"http://arxiv.org/abs/2403.12547v1","category":"eess.SP"}
{"created":"2024-03-19 08:09:27","title":"UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All","abstract":"We present UniBind, a flexible and efficient approach that learns a unified representation space for seven diverse modalities -- images, text, audio, point cloud, thermal, video, and event data. Existing works, eg., ImageBind, treat the image as the central modality and build an image-centered representation space; however, the space may be sub-optimal as it leads to an unbalanced representation space among all modalities. Moreover, the category names are directly used to extract text embeddings for the downstream tasks, making it hardly possible to represent the semantics of multi-modal data. The 'out-of-the-box' insight of our UniBind is to make the alignment center modality-agnostic and further learn a unified and balanced representation space, empowered by the large language models (LLMs). UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts. To make this possible, we 1) construct a knowledge base of text embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build LLM-augmented class-wise embedding center on top of the knowledge base and encoded visual embeddings; 3) align all the embeddings to the LLM-augmented embedding center via contrastive learning to achieve a unified and balanced representation space. UniBind shows strong zero-shot recognition performance gains over prior arts by an average of 6.36%. Finally, we achieve new state-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal fine-tuning setting while reducing 90% of the learnable parameters.","sentences":["We present UniBind, a flexible and efficient approach that learns a unified representation space for seven diverse modalities -- images, text, audio, point cloud, thermal, video, and event data.","Existing works, eg., ImageBind, treat the image as the central modality and build an image-centered representation space; however, the space may be sub-optimal as it leads to an unbalanced representation space among all modalities.","Moreover, the category names are directly used to extract text embeddings for the downstream tasks, making it hardly possible to represent the semantics of multi-modal data.","The 'out-of-the-box' insight of our UniBind is to make the alignment center modality-agnostic and further learn a unified and balanced representation space, empowered by the large language models (LLMs).","UniBind is superior in its flexible application to all CLIP-style models and delivers remarkable performance boosts.","To make this possible, we 1) construct a knowledge base of text embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build LLM-augmented class-wise embedding center on top of the knowledge base and encoded visual embeddings; 3) align all the embeddings to the LLM-augmented embedding center via contrastive learning to achieve a unified and balanced representation space.","UniBind shows strong zero-shot recognition performance gains over prior arts by an average of 6.36%.","Finally, we achieve new state-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal fine-tuning setting while reducing 90% of the learnable parameters."],"url":"http://arxiv.org/abs/2403.12532v1","category":"cs.CV"}
{"created":"2024-03-19 07:10:04","title":"Under-actuated Robotic Gripper with Multiple Grasping Modes Inspired by Human Finger","abstract":"Under-actuated robot grippers as a pervasive tool of robots have become a considerable research focus. Despite their simplicity of mechanical design and control strategy, they suffer from poor versatility and weak adaptability, making widespread applications limited. To better relieve relevant research gaps, we present a novel 3-finger linkage-based gripper that realizes retractable and reconfigurable multi-mode grasps driven by a single motor. Firstly, inspired by the changes that occurred in the contact surface with a human finger moving, we artfully design a slider-slide rail mechanism as the phalanx to achieve retraction of each finger, allowing for better performance in the enveloping grasping mode. Secondly, a reconfigurable structure is constructed to broaden the grasping range of objects' dimensions for the proposed gripper. By adjusting the configuration and gesture of each finger, the gripper can achieve five grasping modes. Thirdly, the proposed gripper is just actuated by a single motor, yet it can be capable of grasping and reconfiguring simultaneously. Finally, various experiments on grasps of slender, thin, and large-volume objects are implemented to evaluate the performance of the proposed gripper in practical scenarios, which demonstrates the excellent grasping capabilities of the gripper.","sentences":["Under-actuated robot grippers as a pervasive tool of robots have become a considerable research focus.","Despite their simplicity of mechanical design and control strategy, they suffer from poor versatility and weak adaptability, making widespread applications limited.","To better relieve relevant research gaps, we present a novel 3-finger linkage-based gripper that realizes retractable and reconfigurable multi-mode grasps driven by a single motor.","Firstly, inspired by the changes that occurred in the contact surface with a human finger moving, we artfully design a slider-slide rail mechanism as the phalanx to achieve retraction of each finger, allowing for better performance in the enveloping grasping mode.","Secondly, a reconfigurable structure is constructed to broaden the grasping range of objects' dimensions for the proposed gripper.","By adjusting the configuration and gesture of each finger, the gripper can achieve five grasping modes.","Thirdly, the proposed gripper is just actuated by a single motor, yet it can be capable of grasping and reconfiguring simultaneously.","Finally, various experiments on grasps of slender, thin, and large-volume objects are implemented to evaluate the performance of the proposed gripper in practical scenarios, which demonstrates the excellent grasping capabilities of the gripper."],"url":"http://arxiv.org/abs/2403.12502v1","category":"cs.RO"}
{"created":"2024-03-19 06:42:11","title":"Scalar dark matter explanation of the excess in the Belle II $B^+\\to K^+ + \\mbox{invisible}$ measurement","abstract":"Recently Belle II reported the first measurement of $B^+\\to K^++\\mbox{invisible (inv)}$, which is $2.7\\sigma$ above the standard model (SM) prediction. If confirmed, this calls for new physics beyond SM. In the SM, the invisible particles are neutrino-anti-neutrino pairs. There are more possibilities when going beyond the SM. In this work, we focus on decays to dark matter (DM) and show that the $B\\to K +\\mathrm{inv}$ excess from Belle II and DM relic density can be simultaneously explained in a simple extension of the SM. The model introduces a real scalar singlet $\\phi$ acting as a DM candidate, and two heavy vector-like quarks $Q,D$ with the same quantum numbers as the SM left-handed quark doublet and right-handed down-type quark singlet, respectively. All these new particles are odd under a $\\mathbb{Z}_2$ symmetry while the SM particles are even. The model can successfully explain the Belle II anomaly and DM relic density for TeV-scale heavy quarks with hierarchical Yukawa couplings involving $b$ and $s$ quarks. At the same time, it can easily satisfy other flavour physics and DM detection constraints.","sentences":["Recently Belle II reported the first measurement of $B^+\\to K^++\\mbox{invisible (inv)}$, which is $2.7\\sigma$ above the standard model (SM) prediction.","If confirmed, this calls for new physics beyond SM.","In the SM, the invisible particles are neutrino-anti-neutrino pairs.","There are more possibilities when going beyond the SM.","In this work, we focus on decays to dark matter (DM) and show that the $B\\to K +\\mathrm{inv}$ excess from Belle II and DM relic density can be simultaneously explained in a simple extension of the SM.","The model introduces a real scalar singlet $\\phi$ acting as a DM candidate, and two heavy vector-like quarks $Q,D$ with the same quantum numbers as the SM left-handed quark doublet and right-handed down-type quark singlet, respectively.","All these new particles are odd under a $\\mathbb{Z}_2$ symmetry while the SM particles are even.","The model can successfully explain the Belle II anomaly and DM relic density for TeV-scale heavy quarks with hierarchical Yukawa couplings involving $b$ and $s$ quarks.","At the same time, it can easily satisfy other flavour physics and DM detection constraints."],"url":"http://arxiv.org/abs/2403.12485v1","category":"hep-ph"}
{"created":"2024-03-19 06:33:07","title":"Con-CDVAE: A method for the conditional generation of crystal structures","abstract":"In recent years, progress has been made in generating new crystalline materials using generative machine learning models, though gaps remain in efficiently generating crystals based on target properties. This paper proposes the Con-CDVAE model, an extension of the Crystal Diffusion Variational Autoencoder (CDVAE), for conditional crystal generation. We introduce innovative components, design a two-step training method, and develop three unique generation strategies to enhance model performance. The effectiveness of Con-CDVAE is demonstrated through extensive testing under various conditions, including both single and combined property targets. Ablation studies further underscore the critical role of the new components in achieving our model's performance. Additionally, we validate the physical credibility of the generated crystals through Density Functional Theory (DFT) calculations, confirming Con-CDVAE's potential in material science research.","sentences":["In recent years, progress has been made in generating new crystalline materials using generative machine learning models, though gaps remain in efficiently generating crystals based on target properties.","This paper proposes the Con-CDVAE model, an extension of the Crystal Diffusion Variational Autoencoder (CDVAE), for conditional crystal generation.","We introduce innovative components, design a two-step training method, and develop three unique generation strategies to enhance model performance.","The effectiveness of Con-CDVAE is demonstrated through extensive testing under various conditions, including both single and combined property targets.","Ablation studies further underscore the critical role of the new components in achieving our model's performance.","Additionally, we validate the physical credibility of the generated crystals through Density Functional Theory (DFT) calculations, confirming Con-CDVAE's potential in material science research."],"url":"http://arxiv.org/abs/2403.12478v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 06:27:46","title":"On the Siegel series in terms of lattice counting","abstract":"In this paper we describe each coefficient of the Siegel series associated to a quadratic $\\mathfrak{o}$-lattice $L$ in terms of lattice counting problems, where $\\mathfrak{o}$ is the ring of integers of a non-Archimedean local field of characteristic $0$. Under the restriction that $p$ is odd and that the dimension of the radical of the quadratic space $L\\otimes\\kappa$ on the residue field $\\kappa$ is at most $2$, we provide explicit values of coefficients and reprove the functional equation of the Siegel series.","sentences":["In this paper we describe each coefficient of the Siegel series associated to a quadratic $\\mathfrak{o}$-lattice $L$ in terms of lattice counting problems, where $\\mathfrak{o}$ is the ring of integers of a non-Archimedean local field of characteristic $0$. Under the restriction that $p$ is odd and that the dimension of the radical of the quadratic space $L\\otimes\\kappa$ on the residue field $\\kappa$ is at most $2$, we provide explicit values of coefficients and reprove the functional equation of the Siegel series."],"url":"http://arxiv.org/abs/2403.12476v1","category":"math.NT"}
{"created":"2024-03-19 06:22:58","title":"FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization","abstract":"Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nonsensitive information. We also provide theoretical explanations for our rationale, concluding that F3 can be realized by emphasizing the features of each node's heterogeneous neighbors (neighbors with different sensitive attributes). We name our method as FairSIN, and present three implementation variants from both data-centric and model-centric perspectives. Experimental results on five benchmark datasets with three different GNN backbones show that FairSIN significantly improves fairness metrics while maintaining high prediction accuracies.","sentences":["Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender.","For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking.","However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness.","To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing.","The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nonsensitive information.","We also provide theoretical explanations for our rationale, concluding that F3 can be realized by emphasizing the features of each node's heterogeneous neighbors (neighbors with different sensitive attributes).","We name our method as FairSIN, and present three implementation variants from both data-centric and model-centric perspectives.","Experimental results on five benchmark datasets with three different GNN backbones show that FairSIN significantly improves fairness metrics while maintaining high prediction accuracies."],"url":"http://arxiv.org/abs/2403.12474v1","category":"cs.LG"}
{"created":"2024-03-19 06:18:25","title":"PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery","abstract":"With the recent advancements in single-image-based human mesh recovery, there is a growing interest in enhancing its performance in certain extreme scenarios, such as occlusion, while maintaining overall model accuracy. Although obtaining accurately annotated 3D human poses under occlusion is challenging, there is still a wealth of rich and precise 2D pose annotations that can be leveraged. However, existing works mostly focus on directly leveraging 2D pose coordinates to estimate 3D pose and mesh. In this paper, we present PostoMETRO($\\textbf{Pos}$e $\\textbf{to}$ken enhanced $\\textbf{ME}$sh $\\textbf{TR}$ansf$\\textbf{O}$rmer), which integrates occlusion-resilient 2D pose representation into transformers in a token-wise manner. Utilizing a specialized pose tokenizer, we efficiently condense 2D pose data to a compact sequence of pose tokens and feed them to the transformer together with the image tokens. This process not only ensures a rich depiction of texture from the image but also fosters a robust integration of pose and image information. Subsequently, these combined tokens are queried by vertex and joint tokens to decode 3D coordinates of mesh vertices and human joints. Facilitated by the robust pose token representation and the effective combination, we are able to produce more precise 3D coordinates, even under extreme scenarios like occlusion. Experiments on both standard and occlusion-specific benchmarks demonstrate the effectiveness of PostoMETRO. Qualitative results further illustrate the clarity of how 2D pose can help 3D reconstruction. Code will be made available.","sentences":["With the recent advancements in single-image-based human mesh recovery, there is a growing interest in enhancing its performance in certain extreme scenarios, such as occlusion, while maintaining overall model accuracy.","Although obtaining accurately annotated 3D human poses under occlusion is challenging, there is still a wealth of rich and precise 2D pose annotations that can be leveraged.","However, existing works mostly focus on directly leveraging 2D pose coordinates to estimate 3D pose and mesh.","In this paper, we present PostoMETRO($\\textbf{Pos}$e $\\textbf{to}$ken enhanced $\\textbf{ME}$sh $\\textbf{TR}$ansf$\\textbf{O}$rmer), which integrates occlusion-resilient 2D pose representation into transformers in a token-wise manner.","Utilizing a specialized pose tokenizer, we efficiently condense 2D pose data to a compact sequence of pose tokens and feed them to the transformer together with the image tokens.","This process not only ensures a rich depiction of texture from the image but also fosters a robust integration of pose and image information.","Subsequently, these combined tokens are queried by vertex and joint tokens to decode 3D coordinates of mesh vertices and human joints.","Facilitated by the robust pose token representation and the effective combination, we are able to produce more precise 3D coordinates, even under extreme scenarios like occlusion.","Experiments on both standard and occlusion-specific benchmarks demonstrate the effectiveness of PostoMETRO.","Qualitative results further illustrate the clarity of how 2D pose can help 3D reconstruction.","Code will be made available."],"url":"http://arxiv.org/abs/2403.12473v1","category":"cs.CV"}
{"created":"2024-03-19 05:32:53","title":"Revisiting the Dirac Nature of Neutrinos","abstract":"Amid the uncertainty regarding the fundamental nature of neutrinos, we adhere to the Dirac one, and construct a model in the light of $\\Delta(27)$ symmetry. The model is enriched by an additional $Z_{10}$ symmetry, to eliminate the possibility of the Majorana mass terms. The neutrino mass matrix exhibits four texture zeroes, and the associated mixing scheme aligns with the experimental data, notably controlled by a single parameter. In addition, the model emphasizes on the \\emph{standard parametrization} of the lepton mixing matrix, which is crucial in the light of model building and neutrino physics experiments","sentences":["Amid the uncertainty regarding the fundamental nature of neutrinos, we adhere to the Dirac one, and construct a model in the light of $\\Delta(27)$ symmetry.","The model is enriched by an additional $Z_{10}$ symmetry, to eliminate the possibility of the Majorana mass terms.","The neutrino mass matrix exhibits four texture zeroes, and the associated mixing scheme aligns with the experimental data, notably controlled by a single parameter.","In addition, the model emphasizes on the \\emph{standard parametrization} of the lepton mixing matrix, which is crucial in the light of model building and neutrino physics experiments"],"url":"http://arxiv.org/abs/2403.12461v1","category":"hep-ph"}
{"created":"2024-03-19 05:26:56","title":"Unveiling the Reactivity of Oxygen and Ozone on C2N Monolayer: A First-Principles Study","abstract":"The process of environmental oxidation is pivotal in determining the physical and chemical properties of two-dimensional (2D) materials. Its impact holds great significance for the practical application of these materials in nanoscale devices functioning under ambient conditions. This study delves into the influence of O2 and O3 exposure on the structural and electronic characteristics of the C2N monolayer, focusing on the kinetics of adsorption and dissociation reactions. Employing first-principles density functional theory calculations alongside climbing image nudged elastic band calculations, we observe that the C2N monolayer exhibits resistance to oxidation and ozonation, evidenced by energy barriers of 0.05 eV and 0.56 eV, respectively. These processes are accompanied by the formation of epoxide (C-O-C) groups. Furthermore, the dissociation mechanism involves charge transfers from the monolayer to the molecules. Notably, the dissociated configurations demonstrate higher bandgaps compared to the pristine C2N monolayer, attributed to robust C-O hybridization. These findings suggest the robustness of C2N monolayers against oxygen/ozone exposures, ensuring stability for devices incorporating these materials.","sentences":["The process of environmental oxidation is pivotal in determining the physical and chemical properties of two-dimensional (2D) materials.","Its impact holds great significance for the practical application of these materials in nanoscale devices functioning under ambient conditions.","This study delves into the influence of O2 and O3 exposure on the structural and electronic characteristics of the C2N monolayer, focusing on the kinetics of adsorption and dissociation reactions.","Employing first-principles density functional theory calculations alongside climbing image nudged elastic band calculations, we observe that the C2N monolayer exhibits resistance to oxidation and ozonation, evidenced by energy barriers of 0.05 eV and 0.56 eV, respectively.","These processes are accompanied by the formation of epoxide (C-O-C) groups.","Furthermore, the dissociation mechanism involves charge transfers from the monolayer to the molecules.","Notably, the dissociated configurations demonstrate higher bandgaps compared to the pristine C2N monolayer, attributed to robust C-O hybridization.","These findings suggest the robustness of C2N monolayers against oxygen/ozone exposures, ensuring stability for devices incorporating these materials."],"url":"http://arxiv.org/abs/2403.12454v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 05:21:12","title":"Intention Action Anticipation Model with Guide-Feedback Loop Mechanism","abstract":"Anticipating human intention from videos has broad applications, such as automatic driving, robot assistive technology, and virtual reality. This study addresses the problem of intention action anticipation using egocentric video sequences to estimate actions that indicate human intention. We propose a Hierarchical Complete-Recent (HCR) information fusion model that makes full use of the features of the entire video sequence (i.e., complete features) and the features of the video tail sequence (i.e., recent features). The HCR model has two primary mechanisms. The Guide-Feedback Loop (GFL) mechanism is proposed to model the relation between one recent feature and one complete feature. Based on GFL, the MultiComplete-Recent Feature Aggregation (MCRFA) module is proposed to model the relation of one recent feature with multiscale complete features. Based on GFL and MCRFA, the HCR model can hierarchically explore the rich interrelationships between multiscale complete features and multiscale recent features. Through comparative and ablation experiments, we validate the effectiveness of our model on two well-known public datasets: EPIC-Kitchens and EGTEA Gaze+.","sentences":["Anticipating human intention from videos has broad applications, such as automatic driving, robot assistive technology, and virtual reality.","This study addresses the problem of intention action anticipation using egocentric video sequences to estimate actions that indicate human intention.","We propose a Hierarchical Complete-Recent (HCR) information fusion model that makes full use of the features of the entire video sequence (i.e., complete features) and the features of the video tail sequence (i.e., recent features).","The HCR model has two primary mechanisms.","The Guide-Feedback Loop (GFL) mechanism is proposed to model the relation between one recent feature and one complete feature.","Based on GFL, the MultiComplete-Recent Feature Aggregation (MCRFA) module is proposed to model the relation of one recent feature with multiscale complete features.","Based on GFL and MCRFA, the HCR model can hierarchically explore the rich interrelationships between multiscale complete features and multiscale recent features.","Through comparative and ablation experiments, we validate the effectiveness of our model on two well-known public datasets: EPIC-Kitchens and EGTEA Gaze+."],"url":"http://arxiv.org/abs/2403.12450v1","category":"cs.CV"}
{"created":"2024-03-19 05:18:47","title":"Multi-Object RANSAC: Efficient Plane Clustering Method in a Clutter","abstract":"In this paper, we propose a novel method for plane clustering specialized in cluttered scenes using an RGB-D camera and validate its effectiveness through robot grasping experiments. Unlike existing methods, which focus on large-scale indoor structures, our approach -- Multi-Object RANSAC emphasizes cluttered environments that contain a wide range of objects with different scales. It enhances plane segmentation by generating subplanes in Deep Plane Clustering (DPC) module, which are then merged with the final planes by post-processing. DPC rearranges the point cloud by voting layers to make subplane clusters, trained in a self-supervised manner using pseudo-labels generated from RANSAC. Multi-Object RANSAC demonstrates superior plane instance segmentation performances over other recent RANSAC applications. We conducted an experiment on robot suction-based grasping, comparing our method with vision-based grasping network and RANSAC applications. The results from this real-world scenario showed its remarkable performance surpassing the baseline methods, highlighting its potential for advanced scene understanding and manipulation.","sentences":["In this paper, we propose a novel method for plane clustering specialized in cluttered scenes using an RGB-D camera and validate its effectiveness through robot grasping experiments.","Unlike existing methods, which focus on large-scale indoor structures, our approach -- Multi-Object RANSAC emphasizes cluttered environments that contain a wide range of objects with different scales.","It enhances plane segmentation by generating subplanes in Deep Plane Clustering (DPC) module, which are then merged with the final planes by post-processing.","DPC rearranges the point cloud by voting layers to make subplane clusters, trained in a self-supervised manner using pseudo-labels generated from RANSAC.","Multi-Object RANSAC demonstrates superior plane instance segmentation performances over other recent RANSAC applications.","We conducted an experiment on robot suction-based grasping, comparing our method with vision-based grasping network and RANSAC applications.","The results from this real-world scenario showed its remarkable performance surpassing the baseline methods, highlighting its potential for advanced scene understanding and manipulation."],"url":"http://arxiv.org/abs/2403.12449v1","category":"cs.RO"}
{"created":"2024-03-19 04:54:59","title":"Self-learning Canonical Space for Multi-view 3D Human Pose Estimation","abstract":"Multi-view 3D human pose estimation is naturally superior to single view one, benefiting from more comprehensive information provided by images of multiple views. The information includes camera poses, 2D/3D human poses, and 3D geometry. However, the accurate annotation of these information is hard to obtain, making it challenging to predict accurate 3D human pose from multi-view images. To deal with this issue, we propose a fully self-supervised framework, named cascaded multi-view aggregating network (CMANet), to construct a canonical parameter space to holistically integrate and exploit multi-view information. In our framework, the multi-view information is grouped into two categories: 1) intra-view information , 2) inter-view information. Accordingly, CMANet consists of two components: intra-view module (IRV) and inter-view module (IEV). IRV is used for extracting initial camera pose and 3D human pose of each view; IEV is to fuse complementary pose information and cross-view 3D geometry for a final 3D human pose. To facilitate the aggregation of the intra- and inter-view, we define a canonical parameter space, depicted by per-view camera pose and human pose and shape parameters ($\\theta$ and $\\beta$) of SMPL model, and propose a two-stage learning procedure. At first stage, IRV learns to estimate camera pose and view-dependent 3D human pose supervised by confident output of an off-the-shelf 2D keypoint detector. At second stage, IRV is frozen and IEV further refines the camera pose and optimizes the 3D human pose by implicitly encoding the cross-view complement and 3D geometry constraint, achieved by jointly fitting predicted multi-view 2D keypoints. The proposed framework, modules, and learning strategy are demonstrated to be effective by comprehensive experiments and CMANet is superior to state-of-the-art methods in extensive quantitative and qualitative analysis.","sentences":["Multi-view 3D human pose estimation is naturally superior to single view one, benefiting from more comprehensive information provided by images of multiple views.","The information includes camera poses, 2D/3D human poses, and 3D geometry.","However, the accurate annotation of these information is hard to obtain, making it challenging to predict accurate 3D human pose from multi-view images.","To deal with this issue, we propose a fully self-supervised framework, named cascaded multi-view aggregating network (CMANet), to construct a canonical parameter space to holistically integrate and exploit multi-view information.","In our framework, the multi-view information is grouped into two categories: 1) intra-view information , 2) inter-view information.","Accordingly, CMANet consists of two components: intra-view module (IRV) and inter-view module (IEV).","IRV is used for extracting initial camera pose and 3D human pose of each view; IEV is to fuse complementary pose information and cross-view 3D geometry for a final 3D human pose.","To facilitate the aggregation of the intra- and inter-view, we define a canonical parameter space, depicted by per-view camera pose and human pose and shape parameters ($\\theta$ and $\\beta$) of SMPL model, and propose a two-stage learning procedure.","At first stage, IRV learns to estimate camera pose and view-dependent 3D human pose supervised by confident output of an off-the-shelf 2D keypoint detector.","At second stage, IRV is frozen and IEV further refines the camera pose and optimizes the 3D human pose by implicitly encoding the cross-view complement and 3D geometry constraint, achieved by jointly fitting predicted multi-view 2D keypoints.","The proposed framework, modules, and learning strategy are demonstrated to be effective by comprehensive experiments and CMANet is superior to state-of-the-art methods in extensive quantitative and qualitative analysis."],"url":"http://arxiv.org/abs/2403.12440v1","category":"cs.CV"}
{"created":"2024-03-19 04:47:56","title":"Human Mesh Recovery from Arbitrary Multi-view Images","abstract":"Human mesh recovery from arbitrary multi-view images involves two characteristics: the arbitrary camera poses and arbitrary number of camera views. Because of the variability, designing a unified framework to tackle this task is challenging. The challenges can be summarized as the dilemma of being able to simultaneously estimate arbitrary camera poses and recover human mesh from arbitrary multi-view images while maintaining flexibility. To solve this dilemma, we propose a divide and conquer framework for Unified Human Mesh Recovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR consists of a decoupled structure and two main components: camera and body decoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion (AVF). As camera poses and human body mesh are independent of each other, CBD splits the estimation of them into two sub-tasks for two individual sub-networks (\\ie, CPE and AVF) to handle respectively, thus the two sub-tasks are disentangled. In CPE, since each camera pose is unrelated to the others, we adopt a shared MLP to process all views in a parallel way. In AVF, in order to fuse multi-view information and make the fusion operation independent of the number of views, we introduce a transformer decoder with a SMPL parameters query token to extract cross-view features for mesh recovery. To demonstrate the efficacy and flexibility of the proposed framework and effect of each component, we conduct extensive experiments on three public datasets: Human3.6M, MPI-INF-3DHP, and TotalCapture.","sentences":["Human mesh recovery from arbitrary multi-view images involves two characteristics: the arbitrary camera poses and arbitrary number of camera views.","Because of the variability, designing a unified framework to tackle this task is challenging.","The challenges can be summarized as the dilemma of being able to simultaneously estimate arbitrary camera poses and recover human mesh from arbitrary multi-view images while maintaining flexibility.","To solve this dilemma, we propose a divide and conquer framework for Unified Human Mesh Recovery (U-HMR) from arbitrary multi-view images.","In particular, U-HMR consists of a decoupled structure and two main components: camera and body decoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion (AVF).","As camera poses and human body mesh are independent of each other, CBD splits the estimation of them into two sub-tasks for two individual sub-networks (\\ie, CPE and AVF) to handle respectively, thus the two sub-tasks are disentangled.","In CPE, since each camera pose is unrelated to the others, we adopt a shared MLP to process all views in a parallel way.","In AVF, in order to fuse multi-view information and make the fusion operation independent of the number of views, we introduce a transformer decoder with a SMPL parameters query token to extract cross-view features for mesh recovery.","To demonstrate the efficacy and flexibility of the proposed framework and effect of each component, we conduct extensive experiments on three public datasets: Human3.6M, MPI-INF-3DHP, and TotalCapture."],"url":"http://arxiv.org/abs/2403.12434v2","category":"cs.CV"}
{"created":"2024-03-19 04:40:11","title":"Electrical transport crossover and large magnetoresistance in selenium deficient van der Waals HfSe2-x","abstract":"Transition metal dichalcogenides have received much attention in the past decade not only due to the new fundamental physics, but also due to the emergent applications in these materials. Currently chalcogenide deficiencies in TMDs are commonly believed either during the high temperature growth procedure or in the nanofabrication process resulting significant changes of their reported physical properties in the literature. Here we perform a systematic study involving pristine stochiometric HfSe2, Se deficient HfSe1.9 and HfSe1.8. Stochiometric HfSe2 transport results show semiconducting behavior with a gap of 1.1eV. Annealing HfSe2 under high vacuum at room temperature causes the Se loss resulting in HfSe1.9, which shows unconventionally large magnetoresistivity following the extended Kohler's rule at low temperatures below 50 K. Moreover, a clear electrical resistivity crossover, mimicking the metal-insulator transition, is observed in the HfSe1.9 single crystal. Further increasing the degree of deficiency in HfSe1.8 results in complete metallic electrical transport at all temperatures down to 2K. Such a drastic difference in the transport behaviors of stoichiometric and Se-deficient HfSe2 further emphasizes that defect control and engineering could be an effective method that could be used to tailor the electronic structure of 2D materials, potentially unlock new states of matter, or even discover new materials.","sentences":["Transition metal dichalcogenides have received much attention in the past decade not only due to the new fundamental physics, but also due to the emergent applications in these materials.","Currently chalcogenide deficiencies in TMDs are commonly believed either during the high temperature growth procedure or in the nanofabrication process resulting significant changes of their reported physical properties in the literature.","Here we perform a systematic study involving pristine stochiometric HfSe2, Se deficient HfSe1.9 and HfSe1.8.","Stochiometric HfSe2 transport results show semiconducting behavior with a gap of 1.1eV. Annealing HfSe2 under high vacuum at room temperature causes the Se loss resulting in HfSe1.9, which shows unconventionally large magnetoresistivity following the extended Kohler's rule at low temperatures below 50 K. Moreover, a clear electrical resistivity crossover, mimicking the metal-insulator transition, is observed in the HfSe1.9 single crystal.","Further increasing the degree of deficiency in HfSe1.8 results in complete metallic electrical transport at all temperatures down to 2K. Such a drastic difference in the transport behaviors of stoichiometric and Se-deficient HfSe2 further emphasizes that defect control and engineering could be an effective method that could be used to tailor the electronic structure of 2D materials, potentially unlock new states of matter, or even discover new materials."],"url":"http://arxiv.org/abs/2403.12430v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 04:19:07","title":"On affine multi-color urns grown under multiple drawing","abstract":"Early investigation of P\\'{o}lya urns considered drawing balls one at a time. In the last two decades, several authors considered multiple drawing in each step, but mostly for schemes on two colors. In this manuscript, we consider multiple drawing from urns of balls of multiple colors, formulating asymptotic theory for specific urn classes and addressing more applications.   The class we consider is affine and tenable, built around a \"core\" square matrix. An index for the drawing schema is derived from the eigenvalues of the core. We identify three regimes: small-, critical-, and large-index. In the small-index regime, we find an asymptotic Gaussian law. In the critical-index regime, we also find an asymptotic Gaussian law, albeit a difference in the scale factor, which involves logarithmic terms.   In both of these regimes, we have explicit forms for the structure of the mean and the covariance matrix of the composition vector (both exact and asymptotic). In all three regimes we have strong laws.","sentences":["Early investigation of P\\'{o}lya urns considered drawing balls one at a time.","In the last two decades, several authors considered multiple drawing in each step, but mostly for schemes on two colors.","In this manuscript, we consider multiple drawing from urns of balls of multiple colors, formulating asymptotic theory for specific urn classes and addressing more applications.   ","The class we consider is affine and tenable, built around a \"core\" square matrix.","An index for the drawing schema is derived from the eigenvalues of the core.","We identify three regimes: small-, critical-, and large-index.","In the small-index regime, we find an asymptotic Gaussian law.","In the critical-index regime, we also find an asymptotic Gaussian law, albeit a difference in the scale factor, which involves logarithmic terms.   ","In both of these regimes, we have explicit forms for the structure of the mean and the covariance matrix of the composition vector (both exact and asymptotic).","In all three regimes we have strong laws."],"url":"http://arxiv.org/abs/2403.12423v1","category":"math.PR"}
{"created":"2024-03-19 04:07:45","title":"Bin Packing Optimization via Deep Reinforcement Learning","abstract":"The Bin Packing Problem (BPP) has attracted enthusiastic research interest recently, owing to widespread applications in logistics and warehousing environments. It is truly essential to optimize the bin packing to enable more objects to be packed into boxes. Object packing order and placement strategy are the two crucial optimization objectives of the BPP. However, existing optimization methods for BPP, such as the genetic algorithm (GA), emerge as the main issues in highly computational cost and relatively low accuracy, making it difficult to implement in realistic scenarios. To well relieve the research gaps, we present a novel optimization methodology of two-dimensional (2D)-BPP and three-dimensional (3D)-BPP for objects with regular shapes via deep reinforcement learning (DRL), maximizing the space utilization and minimizing the usage number of boxes. First, an end-to-end DRL neural network constructed by a modified Pointer Network consisting of an encoder, a decoder and an attention module is proposed to achieve the optimal object packing order. Second, conforming to the top-down operation mode, the placement strategy based on a height map is used to arrange the ordered objects in the boxes, preventing the objects from colliding with boxes and other objects in boxes. Third, the reward and loss functions are defined as the indicators of the compactness, pyramid, and usage number of boxes to conduct the training of the DRL neural network based on an on-policy actor-critic framework. Finally, a series of experiments are implemented to compare our method with conventional packing methods, from which we conclude that our method outperforms these packing methods in both packing accuracy and efficiency.","sentences":["The Bin Packing Problem (BPP) has attracted enthusiastic research interest recently, owing to widespread applications in logistics and warehousing environments.","It is truly essential to optimize the bin packing to enable more objects to be packed into boxes.","Object packing order and placement strategy are the two crucial optimization objectives of the BPP.","However, existing optimization methods for BPP, such as the genetic algorithm (GA), emerge as the main issues in highly computational cost and relatively low accuracy, making it difficult to implement in realistic scenarios.","To well relieve the research gaps, we present a novel optimization methodology of two-dimensional (2D)-BPP and three-dimensional (3D)-BPP for objects with regular shapes via deep reinforcement learning (DRL), maximizing the space utilization and minimizing the usage number of boxes.","First, an end-to-end DRL neural network constructed by a modified Pointer Network consisting of an encoder, a decoder and an attention module is proposed to achieve the optimal object packing order.","Second, conforming to the top-down operation mode, the placement strategy based on a height map is used to arrange the ordered objects in the boxes, preventing the objects from colliding with boxes and other objects in boxes.","Third, the reward and loss functions are defined as the indicators of the compactness, pyramid, and usage number of boxes to conduct the training of the DRL neural network based on an on-policy actor-critic framework.","Finally, a series of experiments are implemented to compare our method with conventional packing methods, from which we conclude that our method outperforms these packing methods in both packing accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.12420v1","category":"cs.RO"}
{"created":"2024-03-19 03:44:33","title":"Vibrational Polaritons with Broken In-Plane Translational Symmetry","abstract":"For the calculation of polariton dispersion relations in planar Fabry-P\\'erot microcavities, the single-mode approximation is usually applied. This approximation becomes invalid when the molecular distribution along the cavity mirror plane breaks the in-plane translational symmetry. Herein, both perturbative theory and numerical calculations have been performed to study polariton dispersion relations with various in-plane molecular distribution patterns under vibrational strong coupling conditions. If a homogeneous in-plane molecular distribution is modulated by sinusoidal fluctuations, in addition to a pair of upper and lower polariton branches, a discrete number of side polariton branches may emerge in the polariton dispersion relation. Moreover, for a Gaussian molecular density distribution, only two, yet significantly broadened polariton branches exist in the spectra. This polariton linewidth broadening is due to the breakdown of the single-mode approximation and the scattering between cavity modes at different in-plane frequencies, which is distinguished from known causes of polariton broadening such as the homogeneous broadening of molecules and the cavity loss. Associated with the broadened polariton branches, under the Gaussian in-plane inhomogeneity, a significant amount of the VSC eigenstates contain a non-zero contribution from the cavity photon mode at zero in-plane frequency, blurring the distinction between the bright and the dark modes. Looking forward, our theoretical investigation should facilitate the experimental exploration of vibrational polaritons with patterned in-plane molecular density distributions.","sentences":["For the calculation of polariton dispersion relations in planar Fabry-P\\'erot microcavities, the single-mode approximation is usually applied.","This approximation becomes invalid when the molecular distribution along the cavity mirror plane breaks the in-plane translational symmetry.","Herein, both perturbative theory and numerical calculations have been performed to study polariton dispersion relations with various in-plane molecular distribution patterns under vibrational strong coupling conditions.","If a homogeneous in-plane molecular distribution is modulated by sinusoidal fluctuations, in addition to a pair of upper and lower polariton branches, a discrete number of side polariton branches may emerge in the polariton dispersion relation.","Moreover, for a Gaussian molecular density distribution, only two, yet significantly broadened polariton branches exist in the spectra.","This polariton linewidth broadening is due to the breakdown of the single-mode approximation and the scattering between cavity modes at different in-plane frequencies, which is distinguished from known causes of polariton broadening such as the homogeneous broadening of molecules and the cavity loss.","Associated with the broadened polariton branches, under the Gaussian in-plane inhomogeneity, a significant amount of the VSC eigenstates contain a non-zero contribution from the cavity photon mode at zero in-plane frequency, blurring the distinction between the bright and the dark modes.","Looking forward, our theoretical investigation should facilitate the experimental exploration of vibrational polaritons with patterned in-plane molecular density distributions."],"url":"http://arxiv.org/abs/2403.12411v1","category":"physics.chem-ph"}
{"created":"2024-03-19 03:19:07","title":"VQ-NeRV: A Vector Quantized Neural Representation for Videos","abstract":"Implicit neural representations (INR) excel in encoding videos within neural networks, showcasing promise in computer vision tasks like video compression and denoising. INR-based approaches reconstruct video frames from content-agnostic embeddings, which hampers their efficacy in video frame regression and restricts their generalization ability for video interpolation. To address these deficiencies, Hybrid Neural Representation for Videos (HNeRV) was introduced with content-adaptive embeddings. Nevertheless, HNeRV's compression ratios remain relatively low, attributable to an oversight in leveraging the network's shallow features and inter-frame residual information. In this work, we introduce an advanced U-shaped architecture, Vector Quantized-NeRV (VQ-NeRV), which integrates a novel component--the VQ-NeRV Block. This block incorporates a codebook mechanism to discretize the network's shallow residual features and inter-frame residual information effectively. This approach proves particularly advantageous in video compression, as it results in smaller size compared to quantized features. Furthermore, we introduce an original codebook optimization technique, termed shallow codebook optimization, designed to refine the utility and efficiency of the codebook. The experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video regression tasks, delivering superior reconstruction quality (with an increase of 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp) efficiency, and improved video inpainting outcomes.","sentences":["Implicit neural representations (INR) excel in encoding videos within neural networks, showcasing promise in computer vision tasks like video compression and denoising.","INR-based approaches reconstruct video frames from content-agnostic embeddings, which hampers their efficacy in video frame regression and restricts their generalization ability for video interpolation.","To address these deficiencies, Hybrid Neural Representation for Videos (HNeRV) was introduced with content-adaptive embeddings.","Nevertheless, HNeRV's compression ratios remain relatively low, attributable to an oversight in leveraging the network's shallow features and inter-frame residual information.","In this work, we introduce an advanced U-shaped architecture, Vector Quantized-NeRV (VQ-NeRV), which integrates a novel component--the VQ-NeRV Block.","This block incorporates a codebook mechanism to discretize the network's shallow residual features and inter-frame residual information effectively.","This approach proves particularly advantageous in video compression, as it results in smaller size compared to quantized features.","Furthermore, we introduce an original codebook optimization technique, termed shallow codebook optimization, designed to refine the utility and efficiency of the codebook.","The experimental evaluations indicate that VQ-NeRV outperforms HNeRV on video regression tasks, delivering superior reconstruction quality (with an increase of 1-2 dB in Peak Signal-to-Noise Ratio (PSNR)), better bit per pixel (bpp) efficiency, and improved video inpainting outcomes."],"url":"http://arxiv.org/abs/2403.12401v1","category":"cs.CV"}
{"created":"2024-03-19 03:14:24","title":"Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing","abstract":"The problem of online social network manipulation for community canvassing is of real concern in today's world. Motivated by the study of voter models, opinion and polarization dynamics on networks, we model community canvassing as a dynamic process over a network enabled via gradient-based attacks on GNNs. Existing attacks on GNNs are all single-step and do not account for the dynamic cascading nature of information diffusion in networks. We consider the realistic scenario where an adversary uses a GNN as a proxy to predict and manipulate voter preferences, especially uncertain voters. Gradient-based attacks on the GNN inform the adversary of strategic manipulations that can be made to proselytize targeted voters. In particular, we explore $\\textit{minimum budget attacks for community canvassing}$ (MBACC). We show that the MBACC problem is NP-Hard and propose Dynamic Multi-Step Adversarial Community Canvassing (MAC) to address it. MAC makes dynamic local decisions based on the heuristic of low budget and high second-order influence to convert and perturb target voters. MAC is a dynamic multi-step attack that discovers low-budget and high-influence targets from which efficient cascading attacks can happen. We evaluate MAC against single-step baselines on the MBACC problem with multiple underlying networks and GNN models. Our experiments show the superiority of MAC which is able to discover efficient multi-hop attacks for adversarial community canvassing. Our code implementation and data is available at https://github.com/saurabhsharma1993/mac.","sentences":["The problem of online social network manipulation for community canvassing is of real concern in today's world.","Motivated by the study of voter models, opinion and polarization dynamics on networks, we model community canvassing as a dynamic process over a network enabled via gradient-based attacks on GNNs.","Existing attacks on GNNs are all single-step and do not account for the dynamic cascading nature of information diffusion in networks.","We consider the realistic scenario where an adversary uses a GNN as a proxy to predict and manipulate voter preferences, especially uncertain voters.","Gradient-based attacks on the GNN inform the adversary of strategic manipulations that can be made to proselytize targeted voters.","In particular, we explore $\\textit{minimum budget attacks for community canvassing}$ (MBACC).","We show that the MBACC problem is NP-Hard and propose Dynamic Multi-Step Adversarial Community Canvassing (MAC) to address it.","MAC makes dynamic local decisions based on the heuristic of low budget and high second-order influence to convert and perturb target voters.","MAC is a dynamic multi-step attack that discovers low-budget and high-influence targets from which efficient cascading attacks can happen.","We evaluate MAC against single-step baselines on the MBACC problem with multiple underlying networks and GNN models.","Our experiments show the superiority of MAC which is able to discover efficient multi-hop attacks for adversarial community canvassing.","Our code implementation and data is available at https://github.com/saurabhsharma1993/mac."],"url":"http://arxiv.org/abs/2403.12399v1","category":"cs.LG"}
{"created":"2024-03-19 02:56:18","title":"ProgrammableGrass: A Shape-Changing Artificial Grass Display Adapted for Dynamic and Interactive Display Features","abstract":"There are various proposals for employing grass materials as a green landscape-friendly display. However, it is difficult for current techniques to display smooth animations using 8-bit images and to adjust display resolution, similar to conventional displays. We present ProgrammableGrass, an artificial grass display with scalable resolution, capable of swiftly controlling grass color at 8-bit levels. This grass display can control grass colors linearly at the 8-bit level, similar to an LCD display, and can also display not only 8-bit-based images but also videos. This display enables pixel-by-pixel color transitions from yellow to green using fixed-length yellow and adjustable-length green grass. We designed a grass module that can be connected to other modules. Utilizing a proportional derivative control, the grass colors are manipulated to display animations at approximately 10 [fps]. Since the relationship between grass lengths and colors is nonlinear, we developed a calibration system for ProgrammableGrass. We revealed that this calibration system allows ProgrammableGrass to linearly control grass colors at 8-bit levels through experiments under multiple conditions. Lastly, we demonstrate ProgrammableGrass to show smooth animations with 8-bit grayscale images. Moreover, we show several application examples to illustrate the potential of ProgrammableGrass. With the advancement of this technology, users will be able to treat grass as a green-based interactive display device.","sentences":["There are various proposals for employing grass materials as a green landscape-friendly display.","However, it is difficult for current techniques to display smooth animations using 8-bit images and to adjust display resolution, similar to conventional displays.","We present ProgrammableGrass, an artificial grass display with scalable resolution, capable of swiftly controlling grass color at 8-bit levels.","This grass display can control grass colors linearly at the 8-bit level, similar to an LCD display, and can also display not only 8-bit-based images but also videos.","This display enables pixel-by-pixel color transitions from yellow to green using fixed-length yellow and adjustable-length green grass.","We designed a grass module that can be connected to other modules.","Utilizing a proportional derivative control, the grass colors are manipulated to display animations at approximately 10 [fps].","Since the relationship between grass lengths and colors is nonlinear, we developed a calibration system for ProgrammableGrass.","We revealed that this calibration system allows ProgrammableGrass to linearly control grass colors at 8-bit levels through experiments under multiple conditions.","Lastly, we demonstrate ProgrammableGrass to show smooth animations with 8-bit grayscale images.","Moreover, we show several application examples to illustrate the potential of ProgrammableGrass.","With the advancement of this technology, users will be able to treat grass as a green-based interactive display device."],"url":"http://arxiv.org/abs/2403.12387v1","category":"cs.GR"}
{"created":"2024-03-19 02:46:33","title":"Probabilistic reachable sets of stochastic nonlinear systems with contextual uncertainties","abstract":"Validating and controlling safety-critical systems in uncertain environments necessitates probabilistic reachable sets of future state evolutions. The existing methods of computing probabilistic reachable sets normally assume that the uncertainties are independent of the state. However, this assumption falls short in many real-world applications, where uncertainties are state-dependent, referred to as contextual uncertainties. This paper formulates the problem of computing probabilistic reachable sets of stochastic nonlinear states with contextual uncertainties by seeking minimum-volume polynomial sublevel sets with contextual chance constraints. The formulated problem cannot be solved by the existing sample-based approximation method since the existing methods do not consider the conditional probability densities. To address this, we propose a consistent sample approximation of the original problem by leveraging the conditional density estimation and resampling. The obtained approximate problem is a tractable optimization problem. Additionally, we prove the almost uniform convergence of the proposed sample-based approximation, showing that it gives the optimal solution almost consistently with the original ones. Through a numerical example, we evaluate the effectiveness of the proposed method against existing approaches, highlighting its capability to significantly reduce the bias inherent in sample-based approximation without considering a conditional probability density.","sentences":["Validating and controlling safety-critical systems in uncertain environments necessitates probabilistic reachable sets of future state evolutions.","The existing methods of computing probabilistic reachable sets normally assume that the uncertainties are independent of the state.","However, this assumption falls short in many real-world applications, where uncertainties are state-dependent, referred to as contextual uncertainties.","This paper formulates the problem of computing probabilistic reachable sets of stochastic nonlinear states with contextual uncertainties by seeking minimum-volume polynomial sublevel sets with contextual chance constraints.","The formulated problem cannot be solved by the existing sample-based approximation method since the existing methods do not consider the conditional probability densities.","To address this, we propose a consistent sample approximation of the original problem by leveraging the conditional density estimation and resampling.","The obtained approximate problem is a tractable optimization problem.","Additionally, we prove the almost uniform convergence of the proposed sample-based approximation, showing that it gives the optimal solution almost consistently with the original ones.","Through a numerical example, we evaluate the effectiveness of the proposed method against existing approaches, highlighting its capability to significantly reduce the bias inherent in sample-based approximation without considering a conditional probability density."],"url":"http://arxiv.org/abs/2403.12379v1","category":"eess.SY"}
{"created":"2024-03-19 02:45:13","title":"Distributionally Robust Density Control with Wasserstein Ambiguity Sets","abstract":"Precise control under uncertainty requires a good understanding and characterization of the noise affecting the system. This paper studies the problem of steering state distributions of dynamical systems subject to partially known uncertainties. We model the distributional uncertainty of the noise process in terms of Wasserstein ambiguity sets, which, based on recent results, have been shown to be an effective means of capturing and propagating uncertainty through stochastic LTI systems. To this end, we propagate the distributional uncertainty of the state through the dynamical system, and, using an affine feedback control law, we steer the ambiguity set of the state to a prescribed, terminal ambiguity set. We also enforce distributionally robust CVaR constraints for the transient motion of the state so as to reside within a prescribed constraint space. The resulting optimization problem is formulated as a semi-definite program, which can be solved efficiently using standard off-the-shelf solvers. We illustrate the proposed distributionally-robust framework on a quadrotor landing problem subject to wind turbulence.","sentences":["Precise control under uncertainty requires a good understanding and characterization of the noise affecting the system.","This paper studies the problem of steering state distributions of dynamical systems subject to partially known uncertainties.","We model the distributional uncertainty of the noise process in terms of Wasserstein ambiguity sets, which, based on recent results, have been shown to be an effective means of capturing and propagating uncertainty through stochastic LTI systems.","To this end, we propagate the distributional uncertainty of the state through the dynamical system, and, using an affine feedback control law, we steer the ambiguity set of the state to a prescribed, terminal ambiguity set.","We also enforce distributionally robust CVaR constraints for the transient motion of the state so as to reside within a prescribed constraint space.","The resulting optimization problem is formulated as a semi-definite program, which can be solved efficiently using standard off-the-shelf solvers.","We illustrate the proposed distributionally-robust framework on a quadrotor landing problem subject to wind turbulence."],"url":"http://arxiv.org/abs/2403.12378v1","category":"math.OC"}
{"created":"2024-03-19 02:39:41","title":"MARPF: Multi-Agent and Multi-Rack Path Finding","abstract":"In environments where many automated guided vehicles (AGVs) operate, planning efficient, collision-free paths is essential. Related research has mainly focused on environments with static passages, resulting in space inefficiency. We define multi-agent and multi-rack path finding (MARPF) as the problem of planning paths for AGVs to convey target racks to their designated locations in environments without passages. In such environments, an AGV without a rack can pass under racks, whereas an AGV with a rack cannot pass under racks to avoid collisions. MARPF entails conveying the target racks without collisions, while the other obstacle racks are positioned without a specific arrangement. AGVs are essential for relocating other racks to prevent any interference with the target racks. We formulated MARPF as an integer linear programming problem in a network flow. To distinguish situations in which an AGV is or is not loading a rack, the proposed method introduces two virtual layers into the network. We optimized the AGVs' movements to move obstacle racks and convey the target racks. The formulation and applicability of the algorithm were validated through numerical experiments. The results indicated that the proposed algorithm addressed issues in environments with dense racks.","sentences":["In environments where many automated guided vehicles (AGVs) operate, planning efficient, collision-free paths is essential.","Related research has mainly focused on environments with static passages, resulting in space inefficiency.","We define multi-agent and multi-rack path finding (MARPF) as the problem of planning paths for AGVs to convey target racks to their designated locations in environments without passages.","In such environments, an AGV without a rack can pass under racks, whereas an AGV with a rack cannot pass under racks to avoid collisions.","MARPF entails conveying the target racks without collisions, while the other obstacle racks are positioned without a specific arrangement.","AGVs are essential for relocating other racks to prevent any interference with the target racks.","We formulated MARPF as an integer linear programming problem in a network flow.","To distinguish situations in which an AGV is or is not loading a rack, the proposed method introduces two virtual layers into the network.","We optimized the AGVs' movements to move obstacle racks and convey the target racks.","The formulation and applicability of the algorithm were validated through numerical experiments.","The results indicated that the proposed algorithm addressed issues in environments with dense racks."],"url":"http://arxiv.org/abs/2403.12376v1","category":"cs.MA"}
{"created":"2024-03-19 02:34:18","title":"RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners","abstract":"Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of up to 13\\%. RankPrompt also excels in LLM-based automatic evaluations for open-ended generation, aligning with human preferences 74\\% of the time in the AlpacaEval set. Moreover, RankPrompt demonstrates robustness against variations in the orderings and consistencies of responses.","sentences":["Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks.","However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes.","Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses.","To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources.","RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars.","Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of up to 13\\%.","RankPrompt also excels in LLM-based automatic evaluations for open-ended generation, aligning with human preferences 74\\% of the time in the AlpacaEval set.","Moreover, RankPrompt demonstrates robustness against variations in the orderings and consistencies of responses."],"url":"http://arxiv.org/abs/2403.12373v1","category":"cs.CL"}
{"created":"2024-03-19 02:29:34","title":"XPose: eXplainable Human Pose Estimation","abstract":"Current approaches in pose estimation primarily concentrate on enhancing model architectures, often overlooking the importance of comprehensively understanding the rationale behind model decisions. In this paper, we propose XPose, a novel framework that incorporates Explainable AI (XAI) principles into pose estimation. This integration aims to elucidate the individual contribution of each keypoint to final prediction, thereby elevating the model's transparency and interpretability. Conventional XAI techniques have predominantly addressed tasks with single-target tasks like classification. Additionally, the application of Shapley value, a common measure in XAI, to pose estimation has been hindered by prohibitive computational demands.   To address these challenges, this work introduces an innovative concept called Group Shapley Value (GSV). This approach strategically organizes keypoints into clusters based on their interdependencies. Within these clusters, GSV meticulously calculates Shapley value for keypoints, while for inter-cluster keypoints, it opts for a more holistic group-level valuation. This dual-level computation framework meticulously assesses keypoint contributions to the final outcome, optimizing computational efficiency. Building on the insights into keypoint interactions, we devise a novel data augmentation technique known as Group-based Keypoint Removal (GKR). This method ingeniously removes individual keypoints during training phases, deliberately preserving those with strong mutual connections, thereby refining the model's predictive prowess for non-visible keypoints. The empirical validation of GKR across a spectrum of standard approaches attests to its efficacy. GKR's success demonstrates how using Explainable AI (XAI) can directly enhance pose estimation models.","sentences":["Current approaches in pose estimation primarily concentrate on enhancing model architectures, often overlooking the importance of comprehensively understanding the rationale behind model decisions.","In this paper, we propose XPose, a novel framework that incorporates Explainable AI (XAI) principles into pose estimation.","This integration aims to elucidate the individual contribution of each keypoint to final prediction, thereby elevating the model's transparency and interpretability.","Conventional XAI techniques have predominantly addressed tasks with single-target tasks like classification.","Additionally, the application of Shapley value, a common measure in XAI, to pose estimation has been hindered by prohibitive computational demands.   ","To address these challenges, this work introduces an innovative concept called Group Shapley Value (GSV).","This approach strategically organizes keypoints into clusters based on their interdependencies.","Within these clusters, GSV meticulously calculates Shapley value for keypoints, while for inter-cluster keypoints, it opts for a more holistic group-level valuation.","This dual-level computation framework meticulously assesses keypoint contributions to the final outcome, optimizing computational efficiency.","Building on the insights into keypoint interactions, we devise a novel data augmentation technique known as Group-based Keypoint Removal (GKR).","This method ingeniously removes individual keypoints during training phases, deliberately preserving those with strong mutual connections, thereby refining the model's predictive prowess for non-visible keypoints.","The empirical validation of GKR across a spectrum of standard approaches attests to its efficacy.","GKR's success demonstrates how using Explainable AI (XAI) can directly enhance pose estimation models."],"url":"http://arxiv.org/abs/2403.12370v1","category":"cs.CV"}
{"created":"2024-03-19 02:24:16","title":"Semisupervised score based matching algorithm to evaluate the effect of public health interventions","abstract":"Multivariate matching algorithms \"pair\" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of \"pairs\" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a \"training\" set of paired units by domain experts, is practically intriguing.   We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\\beta}(x_i,x_j)= \\beta^T (x_i-x_j)(x_i-x_j)^T \\beta$. The weights $\\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units. Further, in the typical but intricate case where the training set is much smaller than the unpaired set, we propose a \\underline{s}emisupervised \\underline{c}ompanion \\underline{o}ne-\\underline{t}o-\\underline{o}ne \\underline{m}atching \\underline{a}lgorithm (SCOTOMA) that makes the best use of the unpaired units. The proposed weight estimator is proved to be consistent when the truth matching criterion is indeed the quadratic score function. When the model assumptions are violated, we demonstrate that the proposed algorithm still outperforms some popular competing matching algorithms through a series of simulations. We applied the proposed algorithm to a real-world study to investigate the effect of in-person schooling on community Covid-19 transmission rate for policy making purpose.","sentences":["Multivariate matching algorithms \"pair\" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations.","In one-to-one multivariate matching algorithms, a large number of \"pairs\" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a \"training\" set of paired units by domain experts, is practically intriguing.   ","We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\\beta}(x_i,x_j)= \\beta^T (x_i-x_j)(x_i-x_j)^T \\beta$.","The weights $\\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units.","Further, in the typical but intricate case where the training set is much smaller than the unpaired set, we propose a \\underline{s}emisupervised \\underline{c}ompanion \\underline{o}ne-\\underline{t}o-\\underline{o}ne \\underline{m}atching \\underline{a}lgorithm (SCOTOMA) that makes the best use of the unpaired units.","The proposed weight estimator is proved to be consistent when the truth matching criterion is indeed the quadratic score function.","When the model assumptions are violated, we demonstrate that the proposed algorithm still outperforms some popular competing matching algorithms through a series of simulations.","We applied the proposed algorithm to a real-world study to investigate the effect of in-person schooling on community Covid-19 transmission rate for policy making purpose."],"url":"http://arxiv.org/abs/2403.12367v1","category":"stat.ML"}
{"created":"2024-03-19 02:22:21","title":"GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation","abstract":"Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method's effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: https://zerg-overmind.github.io/GaussianFlow.github.io/","sentences":["Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature.","While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored.","In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames.","The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space.","This differentiable process enables direct dynamic supervision from optical flow.","Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods.","The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics.","Superior visual quality on extensive experiments demonstrates our method's effectiveness.","Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis.","Project page: https://zerg-overmind.github.io/GaussianFlow.github.io/"],"url":"http://arxiv.org/abs/2403.12365v1","category":"cs.CV"}
{"created":"2024-03-19 02:19:57","title":"Class and Region-Adaptive Constraints for Network Calibration","abstract":"In this work, we present a novel approach to calibrate segmentation networks that considers the inherent challenges posed by different categories and object regions. In particular, we present a formulation that integrates class and region-wise constraints into the learning objective, with multiple penalty weights to account for class and region differences. Finding the optimal penalty weights manually, however, might be unfeasible, and potentially hinder the optimization process. To overcome this limitation, we propose an approach based on Class and Region-Adaptive constraints (CRaC), which allows to learn the class and region-wise penalty weights during training. CRaC is based on a general Augmented Lagrangian method, a well-established technique in constrained optimization. Experimental results on two popular segmentation benchmarks, and two well-known segmentation networks, demonstrate the superiority of CRaC compared to existing approaches. The code is available at: https://github.com/Bala93/CRac/","sentences":["In this work, we present a novel approach to calibrate segmentation networks that considers the inherent challenges posed by different categories and object regions.","In particular, we present a formulation that integrates class and region-wise constraints into the learning objective, with multiple penalty weights to account for class and region differences.","Finding the optimal penalty weights manually, however, might be unfeasible, and potentially hinder the optimization process.","To overcome this limitation, we propose an approach based on Class and Region-Adaptive constraints (CRaC), which allows to learn the class and region-wise penalty weights during training.","CRaC is based on a general Augmented Lagrangian method, a well-established technique in constrained optimization.","Experimental results on two popular segmentation benchmarks, and two well-known segmentation networks, demonstrate the superiority of CRaC compared to existing approaches.","The code is available at: https://github.com/Bala93/CRac/"],"url":"http://arxiv.org/abs/2403.12364v1","category":"cs.CV"}
{"created":"2024-03-19 02:07:22","title":"The Lyapunov exponent as a signature of dissipative many-body quantum chaos","abstract":"A distinct feature of Hermitian quantum chaotic dynamics is the exponential increase of certain out-of-time-order-correlation (OTOC) functions around the Ehrenfest time with a rate given by a Lyapunov exponent. Physically, the OTOCs describe the growth of quantum uncertainty that crucially depends on the nature of the quantum motion. Here, we employ the OTOC in order to provide a precise definition of dissipative quantum chaos. For this purpose, we compute analytically the Lyapunov exponent for the vectorized formulation of the large $q$-limit of a $q$-body Sachdev-Ye-Kitaev model coupled to a Markovian bath. These analytic results are confirmed by an explicit numerical calculation of the Lyapunov exponent for several values of $q \\geq 4$ based on the solutions of the Schwinger-Dyson and Bethe-Salpeter equations. We show that the Lyapunov exponent decreases monotonically as the coupling to the bath increases and eventually becomes negative at a critical value of the coupling signaling a transition to a dynamics which is no longer quantum chaotic. Therefore, a positive Lyapunov exponent is a defining feature of dissipative many-body quantum chaos. The observation of the breaking of the exponential growth for sufficiently strong coupling suggests that dissipative quantum chaos may require in certain cases a sufficiently weak coupling to the environment.","sentences":["A distinct feature of Hermitian quantum chaotic dynamics is the exponential increase of certain out-of-time-order-correlation (OTOC) functions around the Ehrenfest time with a rate given by a Lyapunov exponent.","Physically, the OTOCs describe the growth of quantum uncertainty that crucially depends on the nature of the quantum motion.","Here, we employ the OTOC in order to provide a precise definition of dissipative quantum chaos.","For this purpose, we compute analytically the Lyapunov exponent for the vectorized formulation of the large $q$-limit of a $q$-body Sachdev-Ye-Kitaev model coupled to a Markovian bath.","These analytic results are confirmed by an explicit numerical calculation of the Lyapunov exponent for several values of $q \\geq 4$ based on the solutions of the Schwinger-Dyson and Bethe-Salpeter equations.","We show that the Lyapunov exponent decreases monotonically as the coupling to the bath increases and eventually becomes negative at a critical value of the coupling signaling a transition to a dynamics which is no longer quantum chaotic.","Therefore, a positive Lyapunov exponent is a defining feature of dissipative many-body quantum chaos.","The observation of the breaking of the exponential growth for sufficiently strong coupling suggests that dissipative quantum chaos may require in certain cases a sufficiently weak coupling to the environment."],"url":"http://arxiv.org/abs/2403.12359v1","category":"hep-th"}
{"created":"2024-03-19 01:07:35","title":"Stochastic Halpern iteration in normed spaces and applications to reinforcement learning","abstract":"We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\\tilde{O}(\\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\\Omega(\\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\\varepsilon^{-2}(1-\\gamma)^{-3})$ complexity bound in the case in which the operator is a $\\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, for the average reward, our method improves on the best-known sample complexity.","sentences":["We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space.","We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\\tilde{O}(\\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration.","Also, we establish a lower bound of $\\Omega(\\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching.","Using a suitable modification of our approach, we derive a $O(\\varepsilon^{-2}(1-\\gamma)^{-3})$ complexity bound in the case in which the operator is a $\\gamma$-contraction.","As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes.","In particular, for the average reward, our method improves on the best-known sample complexity."],"url":"http://arxiv.org/abs/2403.12338v1","category":"math.OC"}
{"created":"2024-03-19 00:07:48","title":"Deep Few-view High-resolution Photon-counting Extremity CT at Halved Dose for a Clinical Trial","abstract":"The latest X-ray photon-counting computed tomography (PCCT) for extremity allows multi-energy high-resolution (HR) imaging for tissue characterization and material decomposition. However, both radiation dose and imaging speed need improvement for contrast-enhanced and other studies. Despite the success of deep learning methods for 2D few-view reconstruction, applying them to HR volumetric reconstruction of extremity scans for clinical diagnosis has been limited due to GPU memory constraints, training data scarcity, and domain gap issues. In this paper, we propose a deep learning-based approach for PCCT image reconstruction at halved dose and doubled speed in a New Zealand clinical trial. Particularly, we present a patch-based volumetric refinement network to alleviate the GPU memory limitation, train network with synthetic data, and use model-based iterative refinement to bridge the gap between synthetic and real-world data. The simulation and phantom experiments demonstrate consistently improved results under different acquisition conditions on both in- and off-domain structures using a fixed network. The image quality of 8 patients from the clinical trial are evaluated by three radiologists in comparison with the standard image reconstruction with a full-view dataset. It is shown that our proposed approach is essentially identical to or better than the clinical benchmark in terms of diagnostic image quality scores. Our approach has a great potential to improve the safety and efficiency of PCCT without compromising image quality.","sentences":["The latest X-ray photon-counting computed tomography (PCCT) for extremity allows multi-energy high-resolution (HR) imaging for tissue characterization and material decomposition.","However, both radiation dose and imaging speed need improvement for contrast-enhanced and other studies.","Despite the success of deep learning methods for 2D few-view reconstruction, applying them to HR volumetric reconstruction of extremity scans for clinical diagnosis has been limited due to GPU memory constraints, training data scarcity, and domain gap issues.","In this paper, we propose a deep learning-based approach for PCCT image reconstruction at halved dose and doubled speed in a New Zealand clinical trial.","Particularly, we present a patch-based volumetric refinement network to alleviate the GPU memory limitation, train network with synthetic data, and use model-based iterative refinement to bridge the gap between synthetic and real-world data.","The simulation and phantom experiments demonstrate consistently improved results under different acquisition conditions on both in- and off-domain structures using a fixed network.","The image quality of 8 patients from the clinical trial are evaluated by three radiologists in comparison with the standard image reconstruction with a full-view dataset.","It is shown that our proposed approach is essentially identical to or better than the clinical benchmark in terms of diagnostic image quality scores.","Our approach has a great potential to improve the safety and efficiency of PCCT without compromising image quality."],"url":"http://arxiv.org/abs/2403.12331v1","category":"physics.med-ph"}
{"created":"2024-03-19 00:03:40","title":"FedFisher: Leveraging Fisher Information for One-Shot Federated Learning","abstract":"Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks. One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication. In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL. First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases. Next, we propose practical variants of FedFisher using the diagonal Fisher and K-FAC approximation for the full Fisher and highlight their communication and compute efficiency for FL. Finally, we conduct extensive experiments on various datasets, which show that these variants of FedFisher consistently improve over competing baselines.","sentences":["Standard federated learning (FL) algorithms typically require multiple rounds of communication between the server and the clients, which has several drawbacks, including requiring constant network connectivity, repeated investment of computational resources, and susceptibility to privacy attacks.","One-Shot FL is a new paradigm that aims to address this challenge by enabling the server to train a global model in a single round of communication.","In this work, we present FedFisher, a novel algorithm for one-shot FL that makes use of Fisher information matrices computed on local client models, motivated by a Bayesian perspective of FL.","First, we theoretically analyze FedFisher for two-layer over-parameterized ReLU neural networks and show that the error of our one-shot FedFisher global model becomes vanishingly small as the width of the neural networks and amount of local training at clients increases.","Next, we propose practical variants of FedFisher using the diagonal Fisher and K-FAC approximation for the full Fisher and highlight their communication and compute efficiency for FL.","Finally, we conduct extensive experiments on various datasets, which show that these variants of FedFisher consistently improve over competing baselines."],"url":"http://arxiv.org/abs/2403.12329v1","category":"cs.LG"}
{"created":"2024-03-18 23:33:58","title":"Towards a Theory of Pragmatic Information","abstract":"The subject generally known as ``information theory'' has nothing to say about how much meaning is conveyed by the information. Accordingly, we fill this gap with the first rigorously justifiable, quantitative definition of ``pragmatic information'' as the amount of information that becomes meaningful because it is used in making a decision. We posit that such information updates a ``state of the world'' random variable, $\\omega$, that informs the decision. The pragmatic information of a single message is then defined as the Kulbach-Leibler divergence between the a priori and updated probability distributions of $\\omega$, and the pragmatic information of a message ensemble is defined as the expected value of the pragmatic information values of the ensemble's component messages. We justify these definitions by showing, first, that the pragmatic information of a single message is the expected difference between the shortest binary encoding of $\\omega$ under the {\\it a priori} and updated probability distributions, and, second, that the average of the pragmatic values of individual messages, when sampled a large number of times from the ensemble, approaches its expected value.   The resulting pragmatic information formulas have many hoped-for properties, such as non-negativity and additivity for independent decisions and ``pragmatically independent'' messages. We also sketch two applications of these formulas: The first is the single play of a slot machine, a.k.a. a ``one armed bandit'', with an unknown probability of payout; the second being the reformulation of the efficient market hypothesis of financial economics as the claim that the pragmatic information content of all available data about a given security is zero.","sentences":["The subject generally known as ``information theory'' has nothing to say about how much meaning is conveyed by the information.","Accordingly, we fill this gap with the first rigorously justifiable, quantitative definition of ``pragmatic information'' as the amount of information that becomes meaningful because it is used in making a decision.","We posit that such information updates a ``state of the world'' random variable, $\\omega$, that informs the decision.","The pragmatic information of a single message is then defined as the Kulbach-Leibler divergence between the a priori and updated probability distributions of $\\omega$, and the pragmatic information of a message ensemble is defined as the expected value of the pragmatic information values of the ensemble's component messages.","We justify these definitions by showing, first, that the pragmatic information of a single message is the expected difference between the shortest binary encoding of $\\omega$ under the {\\it a priori} and updated probability distributions, and, second, that the average of the pragmatic values of individual messages, when sampled a large number of times from the ensemble, approaches its expected value.   ","The resulting pragmatic information formulas have many hoped-for properties, such as non-negativity and additivity for independent decisions and ``pragmatically independent'' messages.","We also sketch two applications of these formulas: The first is the single play of a slot machine, a.k.a.","a ``one armed bandit'', with an unknown probability of payout; the second being the reformulation of the efficient market hypothesis of financial economics as the claim that the pragmatic information content of all available data about a given security is zero."],"url":"http://arxiv.org/abs/2403.12324v1","category":"cs.IT"}
{"created":"2024-03-18 23:25:46","title":"Explainable agency: human preferences for simple or complex explanations","abstract":"Research in cognitive psychology has established that whether people prefer simpler explanations to complex ones is context dependent, but the question of `simple vs. complex' becomes critical when an artificial agent seeks to explain its decisions or predictions to humans. We present a model for abstracting causal reasoning chains for the purpose of explanation. This model uses a set of rules to progressively abstract different types of causal information in causal proof traces. We perform online studies using 123 Amazon MTurk participants and with five industry experts over two domains: maritime patrol and weather prediction. We found participants' satisfaction with generated explanations was based on the consistency of relationships among the causes (coherence) that explain an event; and that the important question is not whether people prefer simple or complex explanations, but what types of causal information are relevant to individuals in specific contexts.","sentences":["Research in cognitive psychology has established that whether people prefer simpler explanations to complex ones is context dependent, but the question of `simple vs. complex' becomes critical when an artificial agent seeks to explain its decisions or predictions to humans.","We present a model for abstracting causal reasoning chains for the purpose of explanation.","This model uses a set of rules to progressively abstract different types of causal information in causal proof traces.","We perform online studies using 123 Amazon MTurk participants and with five industry experts over two domains: maritime patrol and weather prediction.","We found participants' satisfaction with generated explanations was based on the consistency of relationships among the causes (coherence) that explain an event; and that the important question is not whether people prefer simple or complex explanations, but what types of causal information are relevant to individuals in specific contexts."],"url":"http://arxiv.org/abs/2403.12321v1","category":"cs.HC"}
{"created":"2024-03-18 23:22:37","title":"EffiPerception: an Efficient Framework for Various Perception Tasks","abstract":"The accuracy-speed-memory trade-off is always the priority to consider for several computer vision perception tasks.   Previous methods mainly focus on a single or small couple of these tasks, such as creating effective data augmentation, feature extractor, learning strategies, etc. These approaches, however, could be inherently task-specific: their proposed model's performance may depend on a specific perception task or a dataset.   Targeting to explore common learning patterns and increasing the module robustness, we propose the EffiPerception framework.   It could achieve great accuracy-speed performance with relatively low memory cost under several perception tasks: 2D Object Detection, 3D Object Detection, 2D Instance Segmentation, and 3D Point Cloud Segmentation.   Overall, the framework consists of three parts:   (1) Efficient Feature Extractors, which extract the input features for each modality. (2) Efficient Layers, plug-in plug-out layers that further process the feature representation, aggregating core learned information while pruning noisy proposals. (3) The EffiOptim, an 8-bit optimizer to further cut down the computational cost and facilitate performance stability.   Extensive experiments on the KITTI, semantic-KITTI, and COCO datasets revealed that EffiPerception could show great accuracy-speed-memory overall performance increase within the four detection and segmentation tasks, in comparison to earlier, well-respected methods.","sentences":["The accuracy-speed-memory trade-off is always the priority to consider for several computer vision perception tasks.   ","Previous methods mainly focus on a single or small couple of these tasks, such as creating effective data augmentation, feature extractor, learning strategies, etc.","These approaches, however, could be inherently task-specific: their proposed model's performance may depend on a specific perception task or a dataset.   ","Targeting to explore common learning patterns and increasing the module robustness, we propose the EffiPerception framework.   ","It could achieve great accuracy-speed performance with relatively low memory cost under several perception tasks: 2D Object Detection, 3D Object Detection, 2D Instance Segmentation, and 3D Point Cloud Segmentation.   ","Overall, the framework consists of three parts:   (1) Efficient Feature Extractors, which extract the input features for each modality.","(2) Efficient Layers, plug-in plug-out layers that further process the feature representation, aggregating core learned information while pruning noisy proposals.","(3) The EffiOptim, an 8-bit optimizer to further cut down the computational cost and facilitate performance stability.   ","Extensive experiments on the KITTI, semantic-KITTI, and COCO datasets revealed that EffiPerception could show great accuracy-speed-memory overall performance increase within the four detection and segmentation tasks, in comparison to earlier, well-respected methods."],"url":"http://arxiv.org/abs/2403.12317v1","category":"cs.CV"}
{"created":"2024-03-18 22:26:19","title":"Estimation and Analysis of Slice Propagation Uncertainty in 3D Anatomy Segmentation","abstract":"Supervised methods for 3D anatomy segmentation demonstrate superior performance but are often limited by the availability of annotated data. This limitation has led to a growing interest in self-supervised approaches in tandem with the abundance of available un-annotated data. Slice propagation has emerged as an self-supervised approach that leverages slice registration as a self-supervised task to achieve full anatomy segmentation with minimal supervision. This approach significantly reduces the need for domain expertise, time, and the cost associated with building fully annotated datasets required for training segmentation networks. However, this shift toward reduced supervision via deterministic networks raises concerns about the trustworthiness and reliability of predictions, especially when compared with more accurate supervised approaches. To address this concern, we propose the integration of calibrated uncertainty quantification (UQ) into slice propagation methods, providing insights into the model's predictive reliability and confidence levels. Incorporating uncertainty measures enhances user confidence in self-supervised approaches, thereby improving their practical applicability. We conducted experiments on three datasets for 3D abdominal segmentation using five UQ methods. The results illustrate that incorporating UQ improves not only model trustworthiness, but also segmentation accuracy. Furthermore, our analysis reveals various failure modes of slice propagation methods that might not be immediately apparent to end-users. This study opens up new research avenues to improve the accuracy and trustworthiness of slice propagation methods.","sentences":["Supervised methods for 3D anatomy segmentation demonstrate superior performance but are often limited by the availability of annotated data.","This limitation has led to a growing interest in self-supervised approaches in tandem with the abundance of available un-annotated data.","Slice propagation has emerged as an self-supervised approach that leverages slice registration as a self-supervised task to achieve full anatomy segmentation with minimal supervision.","This approach significantly reduces the need for domain expertise, time, and the cost associated with building fully annotated datasets required for training segmentation networks.","However, this shift toward reduced supervision via deterministic networks raises concerns about the trustworthiness and reliability of predictions, especially when compared with more accurate supervised approaches.","To address this concern, we propose the integration of calibrated uncertainty quantification (UQ) into slice propagation methods, providing insights into the model's predictive reliability and confidence levels.","Incorporating uncertainty measures enhances user confidence in self-supervised approaches, thereby improving their practical applicability.","We conducted experiments on three datasets for 3D abdominal segmentation using five UQ methods.","The results illustrate that incorporating UQ improves not only model trustworthiness, but also segmentation accuracy.","Furthermore, our analysis reveals various failure modes of slice propagation methods that might not be immediately apparent to end-users.","This study opens up new research avenues to improve the accuracy and trustworthiness of slice propagation methods."],"url":"http://arxiv.org/abs/2403.12290v1","category":"eess.IV"}
{"created":"2024-03-18 22:13:27","title":"Interfacing Quantum Spin Hall and Quantum Anomalous Hall insulators: Bi bilayer on MnBi$_2$Te$_4$-family materials","abstract":"Meeting of non-trivial topology with magnetism results in novel phases of matter, such as Quantum Anomalous Hall (QAH) or axion insulator phases. Even more exotic states with high and tunable Chern numbers are expected at the contact of intrinsic magnetic topological insulators (IMTIs) and 2D topological insulators (TIs).Here we synthesize a heterostructures composed of 2D TI and 3D IMTIs, specifically of bismuth bilayer on top of MnBi$_2$Te$_4$-family of compounds and study their electronic properties by means of angle-resolved photoelectron spectroscopy (ARPES) and density functional theory (DFT). The epitaxial interface is characterized by hybridized Bi and IMTI electronic states. The Bi bilayer-derived states on different members of MnBi$_2$Te$_4$-family of materials are similar, except in the region of mixing with the topological surface states of the substrate. In that region, the new, substrate dependent interface Dirac state is observed. Our \\emph{ab initio} calculations show rich interface phases with emergence of exchange split 1D edge states, making the Bi/IMTI heterostructures promising playground for observation of novel members in the family of quantum Hall effects.","sentences":["Meeting of non-trivial topology with magnetism results in novel phases of matter, such as Quantum Anomalous Hall (QAH) or axion insulator phases.","Even more exotic states with high and tunable Chern numbers are expected at the contact of intrinsic magnetic topological insulators (IMTIs) and 2D topological insulators (TIs).Here we synthesize a heterostructures composed of 2D TI and 3D IMTIs, specifically of bismuth bilayer on top of MnBi$_2$Te$_4$-family of compounds and study their electronic properties by means of angle-resolved photoelectron spectroscopy (ARPES) and density functional theory (DFT).","The epitaxial interface is characterized by hybridized Bi and IMTI electronic states.","The Bi bilayer-derived states on different members of MnBi$_2$Te$_4$-family of materials are similar, except in the region of mixing with the topological surface states of the substrate.","In that region, the new, substrate dependent interface Dirac state is observed.","Our \\emph{ab initio} calculations show rich interface phases with emergence of exchange split 1D edge states, making the Bi/IMTI heterostructures promising playground for observation of novel members in the family of quantum Hall effects."],"url":"http://arxiv.org/abs/2403.12287v1","category":"cond-mat.other"}
{"created":"2024-03-18 22:11:00","title":"FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications","abstract":"There are multiple sources of financial news online which influence market movements and trader's decisions. This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment analysis data, so as to jointly handle the complexities of financial lexicon and context, and further equipping it with a neural network based decision mechanism. Such a generator-classifier scheme, referred to as FinLlama, is trained not only to classify the sentiment valence but also quantify its strength, thus offering traders a nuanced insight into financial news articles. Complementing this, the implementation of parameter-efficient fine-tuning through LoRA optimises trainable parameters, thus minimising computational and memory requirements, without sacrificing accuracy. Simulation results demonstrate the ability of the proposed FinLlama to provide a framework for enhanced portfolio management decisions and increased market returns. These results underpin the ability of FinLlama to construct high-return portfolios which exhibit enhanced resilience, even during volatile periods and unpredictable market events.","sentences":["There are multiple sources of financial news online which influence market movements and trader's decisions.","This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions.","Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions.","However, they are known to suffer from issues related to context sensitivity and word ordering.","Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources.","To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation.","This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment analysis data, so as to jointly handle the complexities of financial lexicon and context, and further equipping it with a neural network based decision mechanism.","Such a generator-classifier scheme, referred to as FinLlama, is trained not only to classify the sentiment valence but also quantify its strength, thus offering traders a nuanced insight into financial news articles.","Complementing this, the implementation of parameter-efficient fine-tuning through LoRA optimises trainable parameters, thus minimising computational and memory requirements, without sacrificing accuracy.","Simulation results demonstrate the ability of the proposed FinLlama to provide a framework for enhanced portfolio management decisions and increased market returns.","These results underpin the ability of FinLlama to construct high-return portfolios which exhibit enhanced resilience, even during volatile periods and unpredictable market events."],"url":"http://arxiv.org/abs/2403.12285v1","category":"cs.CL"}
{"created":"2024-03-18 22:09:48","title":"The Wreaths of KHAN: Uniform Graph Feature Selection with False Discovery Rate Control","abstract":"Graphical models find numerous applications in biology, chemistry, sociology, neuroscience, etc. While substantial progress has been made in graph estimation, it remains largely unexplored how to select significant graph signals with uncertainty assessment, especially those graph features related to topological structures including cycles (i.e., wreaths), cliques, hubs, etc. These features play a vital role in protein substructure analysis, drug molecular design, and brain network connectivity analysis. To fill the gap, we propose a novel inferential framework for general high dimensional graphical models to select graph features with false discovery rate controlled. Our method is based on the maximum of $p$-values from single edges that comprise the topological feature of interest, thus is able to detect weak signals. Moreover, we introduce the $K$-dimensional persistent Homology Adaptive selectioN (KHAN) algorithm to select all the homological features within $K$ dimensions with the uniform control of the false discovery rate over continuous filtration levels. The KHAN method applies a novel discrete Gram-Schmidt algorithm to select statistically significant generators from the homology group. We apply the structural screening method to identify the important residues of the SARS-CoV-2 spike protein during the binding process to the ACE2 receptors. We score the residues for all domains in the spike protein by the $p$-value weighted filtration level in the network persistent homology for the closed, partially open, and open states and identify the residues crucial for protein conformational changes and thus being potential targets for inhibition.","sentences":["Graphical models find numerous applications in biology, chemistry, sociology, neuroscience, etc.","While substantial progress has been made in graph estimation, it remains largely unexplored how to select significant graph signals with uncertainty assessment, especially those graph features related to topological structures including cycles (i.e., wreaths), cliques, hubs, etc.","These features play a vital role in protein substructure analysis, drug molecular design, and brain network connectivity analysis.","To fill the gap, we propose a novel inferential framework for general high dimensional graphical models to select graph features with false discovery rate controlled.","Our method is based on the maximum of $p$-values from single edges that comprise the topological feature of interest, thus is able to detect weak signals.","Moreover, we introduce the $K$-dimensional persistent Homology Adaptive selectioN (KHAN) algorithm to select all the homological features within $K$ dimensions with the uniform control of the false discovery rate over continuous filtration levels.","The KHAN method applies a novel discrete Gram-Schmidt algorithm to select statistically significant generators from the homology group.","We apply the structural screening method to identify the important residues of the SARS-CoV-2 spike protein during the binding process to the ACE2 receptors.","We score the residues for all domains in the spike protein by the $p$-value weighted filtration level in the network persistent homology for the closed, partially open, and open states and identify the residues crucial for protein conformational changes and thus being potential targets for inhibition."],"url":"http://arxiv.org/abs/2403.12284v1","category":"math.ST"}
{"created":"2024-03-18 21:23:38","title":"Quasi-van der Waals Epitaxial Growth of \u03b3'-GaSe Thin Films on GaAs(111)B Substrates","abstract":"GaSe is an important member of the post-transition metal chalcogenide family and is an emerging two-dimensional (2D) semiconductor material. Because it is a van der Waals (vdW) material, it can be fabricated into atomic-scale ultrathin films, making it suitable for the preparation of compact, heterostructure devices. In addition, GaSe possesses unusual optical and electronic properties, such as a shift from an indirect-bandgap single-layer film to a direct-bandgap bulk material, rare intrinsic p-type conduction, and nonlinear optical behaviors. These properties make GaSe an appealing candidate for the fabrication of field-effect transistors, photodetectors, and photovoltaics. However, the wafer-scale production of pure GaSe single crystal thin films remains challenging. This study develops an approach for the direct growth of GaSe thin films on GaAs substrates using molecular beam epitaxy. It yields smooth thin GaSe films with a {\\gamma}'-configuration, a recently-proposed novel polymorph. We analyze the formation mechanism of {\\gamma}'-GaSe using density functional theory, finding that this polymorph is stabilized by Ga vacancies. Finally, we investigate the growth conditions of GaSe, providing valuable insights for exploring 2D/3D quasi-vdW epitaxial growth.","sentences":["GaSe is an important member of the post-transition metal chalcogenide family and is an emerging two-dimensional (2D) semiconductor material.","Because it is a van der Waals (vdW) material, it can be fabricated into atomic-scale ultrathin films, making it suitable for the preparation of compact, heterostructure devices.","In addition, GaSe possesses unusual optical and electronic properties, such as a shift from an indirect-bandgap single-layer film to a direct-bandgap bulk material, rare intrinsic p-type conduction, and nonlinear optical behaviors.","These properties make GaSe an appealing candidate for the fabrication of field-effect transistors, photodetectors, and photovoltaics.","However, the wafer-scale production of pure GaSe single crystal thin films remains challenging.","This study develops an approach for the direct growth of GaSe thin films on GaAs substrates using molecular beam epitaxy.","It yields smooth thin GaSe films with a {\\gamma}'-configuration, a recently-proposed novel polymorph.","We analyze the formation mechanism of {\\gamma}'-GaSe using density functional theory, finding that this polymorph is stabilized by Ga vacancies.","Finally, we investigate the growth conditions of GaSe, providing valuable insights for exploring 2D/3D quasi-vdW epitaxial growth."],"url":"http://arxiv.org/abs/2403.12265v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 21:16:53","title":"Grain Boundary Defect Production during Successive Displacement Cascades on a Tungsten Surface","abstract":"This study delves into the complex mechanisms of defect production and accumulation in nanocrystalline tungsten under irradiation, with a particular focus on the interplay between grain boundaries and free surfaces. Through molecular dynamics simulations, the research explores how grain boundaries act as sinks for irradiation-induced defects, a critical aspect in designing nanomaterials with enhanced radiation tolerance. The investigation leverages a novel Modified Wigner-Seitz Analysis to accurately quantify defect trends amidst dynamic surface reconstruction, providing a nuanced understanding of defect distribution in response to irradiation. This methodology underscores the intricate relationship between defect dynamics and the nano-scale structure of materials, specifically highlighting the role of interfaces in mediating these dynamics. The findings reveal a complex balance between defect production, surface interactions, and the influence of pre-existing defects and temperature on the primary defect production process. Surfaces are shown to amplify defect production due to biased accumulation of interstitials, alongside suppressed defect recombination, emphasizing the nuanced nature of defect dynamics in irradiated materials. This research contributes significantly to the fundamental understanding of defect formation and evolution in irradiated tungsten, offering insights that are instrumental in the development of nanomaterials poised for applications in extreme irradiation environments, such as fusion reactors, thereby advancing the field of materials science and engineering.","sentences":["This study delves into the complex mechanisms of defect production and accumulation in nanocrystalline tungsten under irradiation, with a particular focus on the interplay between grain boundaries and free surfaces.","Through molecular dynamics simulations, the research explores how grain boundaries act as sinks for irradiation-induced defects, a critical aspect in designing nanomaterials with enhanced radiation tolerance.","The investigation leverages a novel Modified Wigner-Seitz Analysis to accurately quantify defect trends amidst dynamic surface reconstruction, providing a nuanced understanding of defect distribution in response to irradiation.","This methodology underscores the intricate relationship between defect dynamics and the nano-scale structure of materials, specifically highlighting the role of interfaces in mediating these dynamics.","The findings reveal a complex balance between defect production, surface interactions, and the influence of pre-existing defects and temperature on the primary defect production process.","Surfaces are shown to amplify defect production due to biased accumulation of interstitials, alongside suppressed defect recombination, emphasizing the nuanced nature of defect dynamics in irradiated materials.","This research contributes significantly to the fundamental understanding of defect formation and evolution in irradiated tungsten, offering insights that are instrumental in the development of nanomaterials poised for applications in extreme irradiation environments, such as fusion reactors, thereby advancing the field of materials science and engineering."],"url":"http://arxiv.org/abs/2403.12261v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 21:16:38","title":"The Best of Many Robustness Criteria in Decision Making: Formulation and Application to Robust Pricing","abstract":"In robust decision-making under non-Bayesian uncertainty, different robust optimization criteria, such as maximin performance, minimax regret, and maximin ratio, have been proposed. In many problems, all three criteria are well-motivated and well-grounded from a decision-theoretic perspective, yet different criteria give different prescriptions. This paper initiates a systematic study of overfitting to robustness criteria. How good is a prescription derived from one criterion when evaluated against another criterion? Does there exist a prescription that performs well against all criteria of interest? We formalize and study these questions through the prototypical problem of robust pricing under various information structures, including support, moments, and percentiles of the distribution of values. We provide a unified analysis of three focal robust criteria across various information structures and evaluate the relative performance of mechanisms optimized for each criterion against the others. We find that mechanisms optimized for one criterion often perform poorly against other criteria, highlighting the risk of overfitting to a particular robustness criterion. Remarkably, we show it is possible to design mechanisms that achieve good performance across all three criteria simultaneously, suggesting that decision-makers need not compromise among criteria.","sentences":["In robust decision-making under non-Bayesian uncertainty, different robust optimization criteria, such as maximin performance, minimax regret, and maximin ratio, have been proposed.","In many problems, all three criteria are well-motivated and well-grounded from a decision-theoretic perspective, yet different criteria give different prescriptions.","This paper initiates a systematic study of overfitting to robustness criteria.","How good is a prescription derived from one criterion when evaluated against another criterion?","Does there exist a prescription that performs well against all criteria of interest?","We formalize and study these questions through the prototypical problem of robust pricing under various information structures, including support, moments, and percentiles of the distribution of values.","We provide a unified analysis of three focal robust criteria across various information structures and evaluate the relative performance of mechanisms optimized for each criterion against the others.","We find that mechanisms optimized for one criterion often perform poorly against other criteria, highlighting the risk of overfitting to a particular robustness criterion.","Remarkably, we show it is possible to design mechanisms that achieve good performance across all three criteria simultaneously, suggesting that decision-makers need not compromise among criteria."],"url":"http://arxiv.org/abs/2403.12260v1","category":"math.OC"}
{"created":"2024-03-18 21:15:27","title":"The Role of Temperature on Irradiation Defect Evolution near Surfaces and Grain Boundaries in Tungsten","abstract":"This study explores the impact of temperature on defect dynamics in tungsten, emphasizing its application in nuclear fusion reactors as Plasma Facing Components (PFCs). Through atomistic simulations, the research elucidates the intricate interplay of defect production, annihilation, and redistribution under irradiation at room (300K) and elevated temperatures (1000K). It demonstrates that higher temperatures significantly increase the number and mobility of defects, leading to a substantial rise in the total number of surviving Frenkel Pairs (FPs), with a notable preference for surface distribution. This redistribution is attributed to energy gradient-driven relocation processes, enhanced by the defects' increased mobility at elevated temperatures. Moreover, the study reveals that elevated temperatures promote biased accumulation of interstitial defects in grain boundaries, especially in configurations that facilitate efficient interstitial migration, indicating a strategy for minimizing lattice interstitial accumulation under irradiation. These findings underscore the critical role of temperature in modulating irradiation-induced defect dynamics in tungsten, providing valuable insights for designing and selecting materials with optimized irradiation resistance for use in extreme conditions of nuclear fusion reactors. The research suggests a material design approach that accounts for temperature effects to enhance the durability and performance of nuclear fusion materials.","sentences":["This study explores the impact of temperature on defect dynamics in tungsten, emphasizing its application in nuclear fusion reactors as Plasma Facing Components (PFCs).","Through atomistic simulations, the research elucidates the intricate interplay of defect production, annihilation, and redistribution under irradiation at room (300K) and elevated temperatures (1000K).","It demonstrates that higher temperatures significantly increase the number and mobility of defects, leading to a substantial rise in the total number of surviving Frenkel Pairs (FPs), with a notable preference for surface distribution.","This redistribution is attributed to energy gradient-driven relocation processes, enhanced by the defects' increased mobility at elevated temperatures.","Moreover, the study reveals that elevated temperatures promote biased accumulation of interstitial defects in grain boundaries, especially in configurations that facilitate efficient interstitial migration, indicating a strategy for minimizing lattice interstitial accumulation under irradiation.","These findings underscore the critical role of temperature in modulating irradiation-induced defect dynamics in tungsten, providing valuable insights for designing and selecting materials with optimized irradiation resistance for use in extreme conditions of nuclear fusion reactors.","The research suggests a material design approach that accounts for temperature effects to enhance the durability and performance of nuclear fusion materials."],"url":"http://arxiv.org/abs/2403.12259v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 21:10:26","title":"Role of Helium Bubbles and Voids on the Property of Nanocrystalline Tungsten","abstract":"This manuscript embarks on an inquiry into the influence of helium implantation on nanocrystalline tungsten, a contender for plasma-facing components (PFCs) in nuclear fusion reactors. The study underscores the inevitability of helium retention in tungsten due to the anticipated high flux of helium atoms from D-T fusion reactions in future reactors such as ITER and DEMO. This retention, potentially culminating in surface blistering and grain boundary embrittlement, propels an investigation into the helium's preferential substitutional configuration over interstitial within tungsten's lattice, leading to cavity and bubble formation. Atomistic tensile testing simulations, exploiting a helium-specific interatomic potential, dissect the ramifications of helium concentration and voids on tungsten's mechanical properties across grain sizes of 10nm and 15nm. The presence of helium at grain boundaries instigates detachment, influencing structural behaviors under strain. Notably, the study reveals a dichotomy in helium's effect: benign at concentrations up to the critical point, beyond which significant embrittlement and elastic softening occur, corroborating with theoretical predictions on helium-induced grain boundary embrittlement. This nuanced exploration delineates the intricate dance between helium implantation levels, grain size, and the ensuing mechanical property alterations in nanocrystalline tungsten, casting light on its viability and endurance as a PFC material under the rigorous conditions forecasted in fusion reactors.","sentences":["This manuscript embarks on an inquiry into the influence of helium implantation on nanocrystalline tungsten, a contender for plasma-facing components (PFCs) in nuclear fusion reactors.","The study underscores the inevitability of helium retention in tungsten due to the anticipated high flux of helium atoms from D-T fusion reactions in future reactors such as ITER and DEMO.","This retention, potentially culminating in surface blistering and grain boundary embrittlement, propels an investigation into the helium's preferential substitutional configuration over interstitial within tungsten's lattice, leading to cavity and bubble formation.","Atomistic tensile testing simulations, exploiting a helium-specific interatomic potential, dissect the ramifications of helium concentration and voids on tungsten's mechanical properties across grain sizes of 10nm and 15nm.","The presence of helium at grain boundaries instigates detachment, influencing structural behaviors under strain.","Notably, the study reveals a dichotomy in helium's effect: benign at concentrations up to the critical point, beyond which significant embrittlement and elastic softening occur, corroborating with theoretical predictions on helium-induced grain boundary embrittlement.","This nuanced exploration delineates the intricate dance between helium implantation levels, grain size, and the ensuing mechanical property alterations in nanocrystalline tungsten, casting light on its viability and endurance as a PFC material under the rigorous conditions forecasted in fusion reactors."],"url":"http://arxiv.org/abs/2403.12255v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 21:01:00","title":"Bayesian Optimization Sequential Surrogate (BOSS) Algorithm: Fast Bayesian Inference for a Broad Class of Bayesian Hierarchical Models","abstract":"Approximate Bayesian inference based on Laplace approximation and quadrature methods have become increasingly popular for their efficiency at fitting latent Gaussian models (LGM), which encompass popular models such as Bayesian generalized linear models, survival models, and spatio-temporal models. However, many useful models fall under the LGM framework only if some conditioning parameters are fixed, as the design matrix would vary with these parameters otherwise. Such models are termed the conditional LGMs with examples in change-point detection, non-linear regression, etc. Existing methods for fitting conditional LGMs rely on grid search or Markov-chain Monte Carlo (MCMC); both require a large number of evaluations of the unnormalized posterior density of the conditioning parameters. As each evaluation of the density requires fitting a separate LGM, these methods become computationally prohibitive beyond simple scenarios. In this work, we introduce the Bayesian optimization sequential surrogate (BOSS) algorithm, which combines Bayesian optimization with approximate Bayesian inference methods to significantly reduce the computational resources required for fitting conditional LGMs. With orders of magnitude fewer evaluations compared to grid or MCMC methods, Bayesian optimization provides us with sequential design points that capture the majority of the posterior mass of the conditioning parameters, which subsequently yields an accurate surrogate posterior distribution that can be easily normalized. We illustrate the efficiency, accuracy, and practical utility of the proposed method through extensive simulation studies and real-world applications in epidemiology, environmental sciences, and astrophysics.","sentences":["Approximate Bayesian inference based on Laplace approximation and quadrature methods have become increasingly popular for their efficiency at fitting latent Gaussian models (LGM), which encompass popular models such as Bayesian generalized linear models, survival models, and spatio-temporal models.","However, many useful models fall under the LGM framework only if some conditioning parameters are fixed, as the design matrix would vary with these parameters otherwise.","Such models are termed the conditional LGMs with examples in change-point detection, non-linear regression, etc.","Existing methods for fitting conditional LGMs rely on grid search or Markov-chain Monte Carlo (MCMC); both require a large number of evaluations of the unnormalized posterior density of the conditioning parameters.","As each evaluation of the density requires fitting a separate LGM, these methods become computationally prohibitive beyond simple scenarios.","In this work, we introduce the Bayesian optimization sequential surrogate (BOSS) algorithm, which combines Bayesian optimization with approximate Bayesian inference methods to significantly reduce the computational resources required for fitting conditional LGMs.","With orders of magnitude fewer evaluations compared to grid or MCMC methods, Bayesian optimization provides us with sequential design points that capture the majority of the posterior mass of the conditioning parameters, which subsequently yields an accurate surrogate posterior distribution that can be easily normalized.","We illustrate the efficiency, accuracy, and practical utility of the proposed method through extensive simulation studies and real-world applications in epidemiology, environmental sciences, and astrophysics."],"url":"http://arxiv.org/abs/2403.12250v1","category":"stat.ME"}
{"created":"2024-03-18 20:51:26","title":"Phase field modelling of the detachment of bubbles from a solid substrate","abstract":"We develop and implement numerically a phase field model for the evolution and detachment of a gas bubble resting on a solid substrate and surrounded by a viscous liquid. The bubble has a static contact angle $\\theta $ and will be subject to gravitational forces. We compute, as a function of the static contact angle, the cricital Bond number over which bubbles detach from the substrate. Then, we perform similar studies for bubble resting on inclined substrates and bubbles under the action of an external flow. We provide approximate formulas for the critical Bond number under all these circumstances. Our method is also able to resolve the pinchoff of the bubble and the possible appearence of satellites.","sentences":["We develop and implement numerically a phase field model for the evolution and detachment of a gas bubble resting on a solid substrate and surrounded by a viscous liquid.","The bubble has a static contact angle $\\theta $ and will be subject to gravitational forces.","We compute, as a function of the static contact angle, the cricital Bond number over which bubbles detach from the substrate.","Then, we perform similar studies for bubble resting on inclined substrates and bubbles under the action of an external flow.","We provide approximate formulas for the critical Bond number under all these circumstances.","Our method is also able to resolve the pinchoff of the bubble and the possible appearence of satellites."],"url":"http://arxiv.org/abs/2403.12246v1","category":"physics.flu-dyn"}
{"created":"2024-03-18 20:50:10","title":"Time-Since-Infection Model for Hospitalization and Incidence Data","abstract":"The Time Since Infection (TSI) models, which use disease surveillance data to model infectious diseases, have become increasingly popular recently due to their flexibility and capacity to address complex disease control questions. However, a notable limitation of TSI models is their primary reliance on incidence data. Even when hospitalization data are available, existing TSI models have not been crafted to estimate disease transmission or predict disease-related hospitalizations - metrics crucial for understanding a pandemic and planning hospital resources. Moreover, their dependence on reported infection data makes them vulnerable to variations in data quality. In this study, we advance TSI models by integrating hospitalization data, marking a significant step forward in modeling with TSI models. Our improvements enable the estimation of key infectious disease parameters without relying on contact tracing data, reduce bias in incidence data, and provide a foundation to connect TSI models with other infectious disease models. We introduce hospitalization propensity parameters to jointly model incidence and hospitalization data. We use a composite likelihood function to accommodate complex data structure and an MCEM algorithm to estimate model parameters. We apply our method to COVID-19 data to estimate disease transmission, assess risk factor impacts, and calculate hospitalization propensity.","sentences":["The Time Since Infection (TSI) models, which use disease surveillance data to model infectious diseases, have become increasingly popular recently due to their flexibility and capacity to address complex disease control questions.","However, a notable limitation of TSI models is their primary reliance on incidence data.","Even when hospitalization data are available, existing TSI models have not been crafted to estimate disease transmission or predict disease-related hospitalizations - metrics crucial for understanding a pandemic and planning hospital resources.","Moreover, their dependence on reported infection data makes them vulnerable to variations in data quality.","In this study, we advance TSI models by integrating hospitalization data, marking a significant step forward in modeling with TSI models.","Our improvements enable the estimation of key infectious disease parameters without relying on contact tracing data, reduce bias in incidence data, and provide a foundation to connect TSI models with other infectious disease models.","We introduce hospitalization propensity parameters to jointly model incidence and hospitalization data.","We use a composite likelihood function to accommodate complex data structure and an MCEM algorithm to estimate model parameters.","We apply our method to COVID-19 data to estimate disease transmission, assess risk factor impacts, and calculate hospitalization propensity."],"url":"http://arxiv.org/abs/2403.12243v1","category":"stat.ME"}
{"created":"2024-03-18 20:37:04","title":"Shape and size measurements of nonequilibrium Bose-Einstein condensates using image processing","abstract":"Bose-Einstein condensates have been the subject of intense research in recent years due to their potential applications in quantum computing and many other areas. However, measuring the shape and size of out-of-equilibrium Bose-Einstein condensates is a challenging task that requires sophisticated image processing techniques. We propose to study perturbed BEC based on general concepts of analysis, which are widely used in the image processing community. The mathematical basis underlying the algorithms is quite general and independent of the type of image studied. The morphological changes observed in the perturbed atomic clouds as a result of excitation amplitude were observed in a consistent manner. And the spatial expansion of the atomic clouds under free fall shows some symmetry, but it was only observed under certain conditions","sentences":["Bose-Einstein condensates have been the subject of intense research in recent years due to their potential applications in quantum computing and many other areas.","However, measuring the shape and size of out-of-equilibrium Bose-Einstein condensates is a challenging task that requires sophisticated image processing techniques.","We propose to study perturbed BEC based on general concepts of analysis, which are widely used in the image processing community.","The mathematical basis underlying the algorithms is quite general and independent of the type of image studied.","The morphological changes observed in the perturbed atomic clouds as a result of excitation amplitude were observed in a consistent manner.","And the spatial expansion of the atomic clouds under free fall shows some symmetry, but it was only observed under certain conditions"],"url":"http://arxiv.org/abs/2403.12238v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-18 20:33:06","title":"IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and Rank Minimization","abstract":"Inverse kinematics (IK) is a fundamental problem frequently occurred in robot control and motion planning. However, the problem is nonconvex because the kinematic map between the configuration and task spaces is generally nonlinear, which makes it challenging for fast and accurate solutions. The problem can be more complicated with the existence of different physical constraints imposed by the robot structure. In this paper, we develop an inverse kinematics solver named IKSPARK (Inverse Kinematics using Semidefinite Programming And RanK minimization) that can find solutions for robots with various structures, including open/closed kinematic chains, spherical, revolute, and/or prismatic joints. The solver works in the space of rotation matrices of the link reference frames and involves solving only convex semidefinite problems (SDPs). Specifically, the IK problem is formulated as an SDP with an additional rank-1 constraint on symmetric matrices with constant traces. The solver first solves this SDP disregarding the rank constraint to get a start point and then finds the rank-1 solution iteratively via a rank minimization algorithm with proven local convergence. Compared to other work that performs SDP relaxation for IK problems, our formulation is simpler, and uses variables with smaller sizes. We validate our approach via simulations on different robots, comparing against a standard IK method.","sentences":["Inverse kinematics (IK) is a fundamental problem frequently occurred in robot control and motion planning.","However, the problem is nonconvex because the kinematic map between the configuration and task spaces is generally nonlinear, which makes it challenging for fast and accurate solutions.","The problem can be more complicated with the existence of different physical constraints imposed by the robot structure.","In this paper, we develop an inverse kinematics solver named IKSPARK (Inverse Kinematics using Semidefinite Programming And RanK minimization) that can find solutions for robots with various structures, including open/closed kinematic chains, spherical, revolute, and/or prismatic joints.","The solver works in the space of rotation matrices of the link reference frames and involves solving only convex semidefinite problems (SDPs).","Specifically, the IK problem is formulated as an SDP with an additional rank-1 constraint on symmetric matrices with constant traces.","The solver first solves this SDP disregarding the rank constraint to get a start point and then finds the rank-1 solution iteratively via a rank minimization algorithm with proven local convergence.","Compared to other work that performs SDP relaxation for IK problems, our formulation is simpler, and uses variables with smaller sizes.","We validate our approach via simulations on different robots, comparing against a standard IK method."],"url":"http://arxiv.org/abs/2403.12235v1","category":"cs.RO"}
{"created":"2024-03-18 20:18:32","title":"Large-scale flood modeling and forecasting with FloodCast","abstract":"Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators. GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling. Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods. Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling. Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors.","sentences":["Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost.","This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings.","In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast.","The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling.","In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction.","In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators.","GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains.","Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling.","Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods.","Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling.","Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors."],"url":"http://arxiv.org/abs/2403.12226v1","category":"cs.LG"}
{"created":"2024-03-18 20:13:38","title":"High-throughput measurement of elastic moduli of microfibers by rope coiling","abstract":"There are many fields where it is of interest to measure the elastic moduli of tiny fragile fibers, such as filamentous bacteria, actin filaments, DNA, carbon nanotubes, and functional microfibers. The elastic modulus is typically deduced from a sophisticated tensile test under a microscope, but the throughput is low and limited by the time-consuming and skill-intensive sample loading/unloading. Here, we demonstrate a simple microfluidic method enabling the high-throughput measurement of the elastic moduli of microfibers by rope coiling using a localized compression, where sample loading/unloading are not needed between consecutive measurements. The rope coiling phenomenon occurs spontaneously when a microfiber flows from a small channel into a wide channel. The elastic modulus is determined by measuring either the buckling length or the coiling radius. The throughput of this method, currently 3,300 fibers per hour, is a thousand times higher than that of a tensile tester. We demonstrate the feasibility of the method by testing a nonuniform fiber with axially varying elastic modulus. We also demonstrate its capability for in situ inline measurement in a microfluidic production line. We envisage that high-throughput measurements may facilitate potential applications such as screening or sorting by mechanical properties and real-time control during production of microfibers.","sentences":["There are many fields where it is of interest to measure the elastic moduli of tiny fragile fibers, such as filamentous bacteria, actin filaments, DNA, carbon nanotubes, and functional microfibers.","The elastic modulus is typically deduced from a sophisticated tensile test under a microscope, but the throughput is low and limited by the time-consuming and skill-intensive sample loading/unloading.","Here, we demonstrate a simple microfluidic method enabling the high-throughput measurement of the elastic moduli of microfibers by rope coiling using a localized compression, where sample loading/unloading are not needed between consecutive measurements.","The rope coiling phenomenon occurs spontaneously when a microfiber flows from a small channel into a wide channel.","The elastic modulus is determined by measuring either the buckling length or the coiling radius.","The throughput of this method, currently 3,300 fibers per hour, is a thousand times higher than that of a tensile tester.","We demonstrate the feasibility of the method by testing a nonuniform fiber with axially varying elastic modulus.","We also demonstrate its capability for in situ inline measurement in a microfluidic production line.","We envisage that high-throughput measurements may facilitate potential applications such as screening or sorting by mechanical properties and real-time control during production of microfibers."],"url":"http://arxiv.org/abs/2403.12225v1","category":"cond-mat.soft"}
{"created":"2024-03-18 20:11:54","title":"On the (Local) Lifting Property","abstract":"The (Local) Lifting Property ((L)LP) is introduced by Kirchberg and deals with lifting completely positive maps. We give a characterization of the (L)LP in terms of lifting $\\ast$-homomorphisms. We use it to prove that if $A$ and $B$ have the LP and $F$ is their finite-dimensional C*-subalgebra, then $A\\ast_F B$ has the LP. This answers a question of Ozawa.   We prove that Exel's soft tori have the LP. As a consequence we obtain that $C^*(F_n\\times F_n)$ is inductive limit of RFD C*-algebras with the LP.   We prove that for a class of C*-algebras including $C^*(F_n\\times F_n)$ and all contractible C*-algebras, the LLP is equivalent to Ext being a group.   As byproduct of methods developed in the paper we generalize Kirchebrg's theorem about extensions with the WEP, give short proofs of several, old and new, facts about soft tori, new unified proofs of Li and Shen characterization of RFD property of free products amalgamated over a finite-dimensional subalgebra and Blackadar's characterization of semiprojectivity of them, and we show that the class of C*-algebras with the LP is closed under cross products with amenable groups.","sentences":["The (Local) Lifting Property ((L)LP) is introduced by Kirchberg and deals with lifting completely positive maps.","We give a characterization of the (L)LP in terms of lifting $\\ast$-homomorphisms.","We use it to prove that if $A$ and $B$ have the LP and $F$ is their finite-dimensional C*-subalgebra, then $A\\ast_F B$ has the LP.","This answers a question of Ozawa.   ","We prove that Exel's soft tori have the LP.","As a consequence we obtain that $C^*(F_n\\times F_n)$ is inductive limit of RFD C*-algebras with the LP.   ","We prove that for a class of C*-algebras including $C^*(F_n\\times F_n)$ and all contractible C*-algebras, the LLP is equivalent to Ext being a group.   ","As byproduct of methods developed in the paper we generalize Kirchebrg's theorem about extensions with the WEP, give short proofs of several, old and new, facts about soft tori, new unified proofs of Li and Shen characterization of RFD property of free products amalgamated over a finite-dimensional subalgebra and Blackadar's characterization of semiprojectivity of them, and we show that the class of C*-algebras with the LP is closed under cross products with amenable groups."],"url":"http://arxiv.org/abs/2403.12224v1","category":"math.OA"}
{"created":"2024-03-18 20:10:29","title":"Multiplexed quantum state transfer in waveguides","abstract":"In this article, we consider a realistic waveguide implementation of a quantum network that serves as a testbed to show how to maximize the storage and manipulation of quantum information in QED setups. We analyze two approaches using wavepacket engineering and quantum state transfer protocols. First, we propose and design a family of orthogonal photons in the time domain. These photons allow for a selective interaction with distinct targeted qubits. Yet, mode multiplexing employing resonant nodes is largely spoiled by cross-talk effects. This motivates the second approach, namely, frequency multiplexing. Here we explore the limits of frequency multiplexing through the waveguide, analyzing its capabilities to host and faithfully transmit photons of different frequencies within a given bandwidth. We perform detailed one- and two-photon simulations and provide theoretical bounds for the fidelity of coherent quantum state transfer protocols under realistic conditions. Our results show that state-of-the-art experiments can employ dozens of multiplexed photons with global fidelities fulfilling the requirements imposed by fault-tolerant quantum computing. This is with the caveat that the conditions for single-photon fidelity are met.","sentences":["In this article, we consider a realistic waveguide implementation of a quantum network that serves as a testbed to show how to maximize the storage and manipulation of quantum information in QED setups.","We analyze two approaches using wavepacket engineering and quantum state transfer protocols.","First, we propose and design a family of orthogonal photons in the time domain.","These photons allow for a selective interaction with distinct targeted qubits.","Yet, mode multiplexing employing resonant nodes is largely spoiled by cross-talk effects.","This motivates the second approach, namely, frequency multiplexing.","Here we explore the limits of frequency multiplexing through the waveguide, analyzing its capabilities to host and faithfully transmit photons of different frequencies within a given bandwidth.","We perform detailed one-","and two-photon simulations and provide theoretical bounds for the fidelity of coherent quantum state transfer protocols under realistic conditions.","Our results show that state-of-the-art experiments can employ dozens of multiplexed photons with global fidelities fulfilling the requirements imposed by fault-tolerant quantum computing.","This is with the caveat that the conditions for single-photon fidelity are met."],"url":"http://arxiv.org/abs/2403.12222v1","category":"quant-ph"}
{"created":"2024-03-18 20:06:20","title":"High-precision astrometry with VVV -- II. A near-infrared extension of Gaia into the Galactic plane","abstract":"Aims. We use near-infrared, ground-based data from the VISTA Variables in the Via Lactea (VVV) survey to indirectly extend the astrometry provided by the Gaia catalog to objects in heavily-extincted regions towards the Galactic bulge and plane that are beyond Gaia's reach. Methods. We make use of the state-of-the-art techniques developed for high-precision astrometry and photometry with the Hubble Space Telescope to process the VVV data. We employ empirical, spatially-variable, effective point-spread functions and local transformations to mitigate the effects of systematic errors, like residual geometric distortion and image motion, and to improve measurements in crowded fields and for faint stars. We also anchor our astrometry to the absolute reference frame of the Gaia Data Release 3. Results. We measure between 20 and 60 times more sources than Gaia in the region surrounding the Galactic center, obtaining an single-exposure precision of about 12 mas and a proper-motion precision of better than 1 mas yr$^{-1}$ for bright, unsaturated sources. Our astrometry provides an extension of Gaia into the Galactic center. We publicly release the astro-photometric catalogs of the two VVV fields considered in this work, which contain a total of $\\sim$ 3.5 million sources. Our catalogs cover $\\sim$ 3 sq. degrees, about 0.5% of the entire VVV survey area.","sentences":["Aims.","We use near-infrared, ground-based data from the VISTA Variables in the Via Lactea (VVV) survey to indirectly extend the astrometry provided by the Gaia catalog to objects in heavily-extincted regions towards the Galactic bulge and plane that are beyond Gaia's reach.","Methods.","We make use of the state-of-the-art techniques developed for high-precision astrometry and photometry with the Hubble Space Telescope to process the VVV data.","We employ empirical, spatially-variable, effective point-spread functions and local transformations to mitigate the effects of systematic errors, like residual geometric distortion and image motion, and to improve measurements in crowded fields and for faint stars.","We also anchor our astrometry to the absolute reference frame of the Gaia Data Release 3. Results.","We measure between 20 and 60 times more sources than Gaia in the region surrounding the Galactic center, obtaining an single-exposure precision of about 12 mas and a proper-motion precision of better than 1 mas yr$^{-1}$ for bright, unsaturated sources.","Our astrometry provides an extension of Gaia into the Galactic center.","We publicly release the astro-photometric catalogs of the two VVV fields considered in this work, which contain a total of $\\sim$ 3.5 million sources.","Our catalogs cover $\\sim$ 3 sq. degrees, about 0.5% of the entire VVV survey area."],"url":"http://arxiv.org/abs/2403.12219v1","category":"astro-ph.GA"}
{"created":"2024-03-18 20:02:31","title":"Secure Synchronization of Heterogeneous Pulse-Coupled Oscillators","abstract":"In this paper, we consider the synchronization of heterogeneous pulse-coupled oscillators (PCOs), where some of the oscillators might be faulty or malicious. The oscillators interact through identical pulses at discrete instants and evolve continuously with different frequencies otherwise. Despite the presence of misbehaviors, benign oscillators aim to reach synchronization. To achieve this objective, two resilient synchronization protocols are developed in this paper by adapting the real-valued mean-subsequence reduced (MSR) algorithm to pulse-based interactions. The first protocol relies on packet-based communication to transmit absolute frequencies, while the second protocol operates purely with pulses to calculate relative frequencies. In both protocols, each normal oscillator periodically counts the received pulses to detect possible malicious behaviors. By disregarding suspicious pulses from its neighbors, the oscillator updates both its phases and frequencies. The paper establishes sufficient conditions on the initial states and graph structure under which resilient synchronization is achieved in the PCO network. Specifically, the normal oscillators can either detect the presence of malicious nodes or synchronize in both phases and frequencies. Additionally, a comparison between the two algorithms reveals a trade-off between relaxed initial conditions and reduced communication burden.","sentences":["In this paper, we consider the synchronization of heterogeneous pulse-coupled oscillators (PCOs), where some of the oscillators might be faulty or malicious.","The oscillators interact through identical pulses at discrete instants and evolve continuously with different frequencies otherwise.","Despite the presence of misbehaviors, benign oscillators aim to reach synchronization.","To achieve this objective, two resilient synchronization protocols are developed in this paper by adapting the real-valued mean-subsequence reduced (MSR) algorithm to pulse-based interactions.","The first protocol relies on packet-based communication to transmit absolute frequencies, while the second protocol operates purely with pulses to calculate relative frequencies.","In both protocols, each normal oscillator periodically counts the received pulses to detect possible malicious behaviors.","By disregarding suspicious pulses from its neighbors, the oscillator updates both its phases and frequencies.","The paper establishes sufficient conditions on the initial states and graph structure under which resilient synchronization is achieved in the PCO network.","Specifically, the normal oscillators can either detect the presence of malicious nodes or synchronize in both phases and frequencies.","Additionally, a comparison between the two algorithms reveals a trade-off between relaxed initial conditions and reduced communication burden."],"url":"http://arxiv.org/abs/2403.12218v1","category":"eess.SY"}
{"created":"2024-03-18 19:59:05","title":"Aggregate Peak EV Charging Demand: The Impact of Segmented Network Tariffs","abstract":"Aggregate peak Electric Vehicle (EV) charging demand has become an increasingly concerning issue for network operators, impacting their reliable operations. Various network tariff schemes have been proposed to limit peak demand by incentivizing flexible asset users to shift their demand from peak periods. However, fewer studies quantify the overall effect of these tariff schemes, which operate at the level of individual connections, on the aggregate load. In this paper, we investigate the effect of a multi-level segmented network tariff on aggregate EV charging demand. The interaction between peak-reducing network tariffs and dynamic energy prices is studied, shedding light on the competition between peak-reducing (network tariffs) and peak-forming (wholesale prices) charging behaviour. Results based on real charging transactions from over 650 public charging stations (CS) in the Netherlands show that the segmented network tariff with flat energy prices results in more diverse load profiles with increasing aggregation compared to cost-optimized dispatch based on dynamic day-ahead prices. The average peak demand for charging with segmented network tariffs and flat energy prices is reduced by 49% and 29% compared to optimized charging under dynamic day-ahead energy prices and unoptimized (dumb) charging, respectively.","sentences":["Aggregate peak Electric Vehicle (EV) charging demand has become an increasingly concerning issue for network operators, impacting their reliable operations.","Various network tariff schemes have been proposed to limit peak demand by incentivizing flexible asset users to shift their demand from peak periods.","However, fewer studies quantify the overall effect of these tariff schemes, which operate at the level of individual connections, on the aggregate load.","In this paper, we investigate the effect of a multi-level segmented network tariff on aggregate EV charging demand.","The interaction between peak-reducing network tariffs and dynamic energy prices is studied, shedding light on the competition between peak-reducing (network tariffs) and peak-forming (wholesale prices) charging behaviour.","Results based on real charging transactions from over 650 public charging stations (CS) in the Netherlands show that the segmented network tariff with flat energy prices results in more diverse load profiles with increasing aggregation compared to cost-optimized dispatch based on dynamic day-ahead prices.","The average peak demand for charging with segmented network tariffs and flat energy prices is reduced by 49% and 29% compared to optimized charging under dynamic day-ahead energy prices and unoptimized (dumb) charging, respectively."],"url":"http://arxiv.org/abs/2403.12215v1","category":"eess.SY"}
{"created":"2024-03-18 19:07:42","title":"The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating Traverses across Lunar Polar Terrain under Extreme Lighting Conditions","abstract":"We present the POLAR Traverse Dataset: a dataset of high-fidelity stereo pair images of lunar-like terrain under polar lighting conditions designed to simulate a straight-line traverse. Images from individual traverses with different camera heights and pitches were recorded at 1 m intervals by moving a suspended stereo bar across a test bed filled with regolith simulant and shaped to mimic lunar south polar terrain. Ground truth geometry and camera position information was also recorded. This dataset is intended for developing and testing software algorithms that rely on stereo or monocular camera images, such as visual odometry, for use in the lunar polar environment, as well as to provide insight into the expected lighting conditions in lunar polar regions.","sentences":["We present the POLAR Traverse Dataset: a dataset of high-fidelity stereo pair images of lunar-like terrain under polar lighting conditions designed to simulate a straight-line traverse.","Images from individual traverses with different camera heights and pitches were recorded at 1 m intervals by moving a suspended stereo bar across a test bed filled with regolith simulant and shaped to mimic lunar south polar terrain.","Ground truth geometry and camera position information was also recorded.","This dataset is intended for developing and testing software algorithms that rely on stereo or monocular camera images, such as visual odometry, for use in the lunar polar environment, as well as to provide insight into the expected lighting conditions in lunar polar regions."],"url":"http://arxiv.org/abs/2403.12194v1","category":"cs.CV"}
{"created":"2024-03-18 19:04:56","title":"Continual Domain Randomization","abstract":"Domain Randomization (DR) is commonly used for sim2real transfer of reinforcement learning (RL) policies in robotics. Most DR approaches require a simulator with a fixed set of tunable parameters from the start of the training, from which the parameters are randomized simultaneously to train a robust model for use in the real world. However, the combined randomization of many parameters increases the task difficulty and might result in sub-optimal policies. To address this problem and to provide a more flexible training process, we propose Continual Domain Randomization (CDR) for RL that combines domain randomization with continual learning to enable sequential training in simulation on a subset of randomization parameters at a time. Starting from a model trained in a non-randomized simulation where the task is easier to solve, the model is trained on a sequence of randomizations, and continual learning is employed to remember the effects of previous randomizations. Our robotic reaching and grasping tasks experiments show that the model trained in this fashion learns effectively in simulation and performs robustly on the real robot while matching or outperforming baselines that employ combined randomization or sequential randomization without continual learning. Our code and videos are available at https://continual-dr.github.io/.","sentences":["Domain Randomization (DR) is commonly used for sim2real transfer of reinforcement learning (RL) policies in robotics.","Most DR approaches require a simulator with a fixed set of tunable parameters from the start of the training, from which the parameters are randomized simultaneously to train a robust model for use in the real world.","However, the combined randomization of many parameters increases the task difficulty and might result in sub-optimal policies.","To address this problem and to provide a more flexible training process, we propose Continual Domain Randomization (CDR) for RL that combines domain randomization with continual learning to enable sequential training in simulation on a subset of randomization parameters at a time.","Starting from a model trained in a non-randomized simulation where the task is easier to solve, the model is trained on a sequence of randomizations, and continual learning is employed to remember the effects of previous randomizations.","Our robotic reaching and grasping tasks experiments show that the model trained in this fashion learns effectively in simulation and performs robustly on the real robot while matching or outperforming baselines that employ combined randomization or sequential randomization without continual learning.","Our code and videos are available at https://continual-dr.github.io/."],"url":"http://arxiv.org/abs/2403.12193v1","category":"cs.RO"}
{"created":"2024-03-18 18:59:42","title":"PETScML: Second-order solvers for training regression problems in Scientific Machine Learning","abstract":"In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications. At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods. However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization. We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization. We empirically demonstrate the superior efficacy of a trust region method based on the Gauss-Newton approximation of the Hessian in improving the generalization errors arising from regression tasks when learning surrogate models for a wide range of scientific machine-learning techniques and test cases. All the conventional second-order solvers tested, including L-BFGS and inexact Newton with line-search, compare favorably, either in terms of cost or accuracy, with the adaptive first-order methods used to validate the surrogate models.","sentences":["In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications.","At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods.","However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization.","We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization.","We empirically demonstrate the superior efficacy of a trust region method based on the Gauss-Newton approximation of the Hessian in improving the generalization errors arising from regression tasks when learning surrogate models for a wide range of scientific machine-learning techniques and test cases.","All the conventional second-order solvers tested, including L-BFGS and inexact Newton with line-search, compare favorably, either in terms of cost or accuracy, with the adaptive first-order methods used to validate the surrogate models."],"url":"http://arxiv.org/abs/2403.12188v1","category":"cs.LG"}
{"created":"2024-03-18 18:55:46","title":"Fragile Stable Matchings","abstract":"We show how fragile stable matchings are in a decentralized one-to-one matching setting. The classical work of Roth and Vande Vate (1990) suggests simple decentralized dynamics in which randomly-chosen blocking pairs match successively. Such decentralized interactions guarantee convergence to a stable matching. Our first theorem shows that, under mild conditions, any unstable matching -- including a small perturbation of a stable matching -- can culminate in any stable matching through these dynamics. Our second theorem highlights another aspect of fragility: stabilization may take a long time. Even in markets with a unique stable matching, where the dynamics always converge to the same matching, decentralized interactions can require an exponentially long duration to converge. A small perturbation of a stable matching may lead the market away from stability and involve a sizable proportion of mismatched participants for extended periods. Our results hold for a broad class of dynamics.","sentences":["We show how fragile stable matchings are in a decentralized one-to-one matching setting.","The classical work of Roth and Vande Vate (1990) suggests simple decentralized dynamics in which randomly-chosen blocking pairs match successively.","Such decentralized interactions guarantee convergence to a stable matching.","Our first theorem shows that, under mild conditions, any unstable matching -- including a small perturbation of a stable matching -- can culminate in any stable matching through these dynamics.","Our second theorem highlights another aspect of fragility: stabilization may take a long time.","Even in markets with a unique stable matching, where the dynamics always converge to the same matching, decentralized interactions can require an exponentially long duration to converge.","A small perturbation of a stable matching may lead the market away from stability and involve a sizable proportion of mismatched participants for extended periods.","Our results hold for a broad class of dynamics."],"url":"http://arxiv.org/abs/2403.12183v1","category":"econ.TH"}
{"created":"2024-03-18 18:16:19","title":"Star formation exists in all early-type galaxies -- evidence from ubiquitous structure in UV images","abstract":"Recent surveys have demonstrated the widespread presence of UV emission in early-type (elliptical/S0) galaxies, suggesting the presence of star formation in many of these systems. However, potential UV contributions from old and young stars, together with model uncertainties, makes it challenging to confirm the presence of young stars using integrated photometry alone. This is particularly true in ETGs that are fainter in the UV and have red UV-optical colours. An unambiguous way of disentangling the source of the UV is to look for structure in UV images. Optical images of ETGs, which are dominated by old stars, are smooth and devoid of structure. If the UV is also produced by these old stars, then the UV images will share this smoothness, while, if driven by young stars, they will exhibit significant structure. We compare the UV and optical morphologies of 32 ETGs (93 percent of which are at $z<0.03$) using quantitative parameters (concentration, asymmetry, clumpiness and the S\\'ersic index), calculated via deep UV and optical images with similar resolution. Regardless of stellar mass, UV-optical colour or the presence of interactions, the asymmetry and clumpiness of ETGs is significantly (often several orders of magnitudes) larger in the UV than in the optical, while the UV S\\'ersic indices are typically lower than their optical counterparts. The ubiquitous presence of structure demonstrates that the UV flux across our entire ETG sample is dominated by young stars and indicates that star formation exists in all ETGs in the nearby Universe.","sentences":["Recent surveys have demonstrated the widespread presence of UV emission in early-type (elliptical/S0) galaxies, suggesting the presence of star formation in many of these systems.","However, potential UV contributions from old and young stars, together with model uncertainties, makes it challenging to confirm the presence of young stars using integrated photometry alone.","This is particularly true in ETGs that are fainter in the UV and have red UV-optical colours.","An unambiguous way of disentangling the source of the UV is to look for structure in UV images.","Optical images of ETGs, which are dominated by old stars, are smooth and devoid of structure.","If the UV is also produced by these old stars, then the UV images will share this smoothness, while, if driven by young stars, they will exhibit significant structure.","We compare the UV and optical morphologies of 32 ETGs (93 percent of which are at $z<0.03$) using quantitative parameters (concentration, asymmetry, clumpiness and the S\\'ersic index), calculated via deep UV and optical images with similar resolution.","Regardless of stellar mass, UV-optical colour or the presence of interactions, the asymmetry and clumpiness of ETGs is significantly (often several orders of magnitudes) larger in the UV than in the optical, while the UV S\\'ersic indices are typically lower than their optical counterparts.","The ubiquitous presence of structure demonstrates that the UV flux across our entire ETG sample is dominated by young stars and indicates that star formation exists in all ETGs in the nearby Universe."],"url":"http://arxiv.org/abs/2403.12160v1","category":"astro-ph.GA"}
{"created":"2024-03-18 18:12:17","title":"An Imperative study of the angular observables in $\u039b_b^0 \\to \u039b_c^{+}(\\to \u039b\u03c0^{+})\u03c4\\bar{\u03bd_\u03c4}$ decay and probing the footprint of new physics","abstract":"We study the 4-body angular distribution for $\\Lambda_b \\to \\Lambda_c^{+}(\\to \\Lambda \\pi^{+})\\ell^- \\bar{\\nu}$ decays and find out the analytical expressions for various asymmetric and angular observables in the Standard Model and the new physics scenarios and compared them with the literature. Using the available inputs from the lattice, we have predicted the values with uncertainties of all these observables in the Standard Model. Considering the new physics effects in $b\\to c\\tau^-\\bar{\\nu}_{\\tau}$ transitions, we have constrained the new Wilson coefficients of the one operator and two operator scenarios from the available data on these decays. The two-operator scenario with scalar-pseudoscalar and tensor quark current provides the most plausible solution to the current data. Also, we have tested the new physics sensitivities (one or two-operator scenarios) of the different angular observables in $\\Lambda_b \\to \\Lambda_c (\\to \\Lambda \\pi^{+}) \\tau^- \\bar{\\nu}_{\\tau}$ decays and found correlations among them. We have noted a few one or two-operator scenarios to which some of these observables are sensitive. By measuring these observables, it will be possible to distinguish these effects from one another.","sentences":["We study the 4-body angular distribution for $\\Lambda_b \\to \\Lambda_c^{+}(\\to \\Lambda \\pi^{+})\\ell^- \\bar{\\nu}$ decays and find out the analytical expressions for various asymmetric and angular observables in the Standard Model and the new physics scenarios and compared them with the literature.","Using the available inputs from the lattice, we have predicted the values with uncertainties of all these observables in the Standard Model.","Considering the new physics effects in $b\\to c\\tau^-\\bar{\\nu}_{\\tau}$ transitions, we have constrained the new Wilson coefficients of the one operator and two operator scenarios from the available data on these decays.","The two-operator scenario with scalar-pseudoscalar and tensor quark current provides the most plausible solution to the current data.","Also, we have tested the new physics sensitivities (one or two-operator scenarios) of the different angular observables in $\\Lambda_b \\to \\Lambda_c (\\to \\Lambda \\pi^{+}) \\tau^- \\bar{\\nu}_{\\tau}$ decays and found correlations among them.","We have noted a few one or two-operator scenarios to which some of these observables are sensitive.","By measuring these observables, it will be possible to distinguish these effects from one another."],"url":"http://arxiv.org/abs/2403.12155v1","category":"hep-ph"}
{"created":"2024-03-18 18:09:22","title":"Development of Automated Neural Network Prediction for Echocardiographic Left ventricular Ejection Fraction","abstract":"The echocardiographic measurement of left ventricular ejection fraction (LVEF) is fundamental to the diagnosis and classification of patients with heart failure (HF). In order to quantify LVEF automatically and accurately, this paper proposes a new pipeline method based on deep neural networks and ensemble learning. Within the pipeline, an Atrous Convolutional Neural Network (ACNN) was first trained to segment the left ventricle (LV), before employing the area-length formulation based on the ellipsoid single-plane model to calculate LVEF values. This formulation required inputs of LV area, derived from segmentation using an improved Jeffrey's method, as well as LV length, derived from a novel ensemble learning model. To further improve the pipeline's accuracy, an automated peak detection algorithm was used to identify end-diastolic and end-systolic frames, avoiding issues with human error. Subsequently, single-beat LVEF values were averaged across all cardiac cycles to obtain the final LVEF. This method was developed and internally validated in an open-source dataset containing 10,030 echocardiograms. The Pearson's correlation coefficient was 0.83 for LVEF prediction compared to expert human analysis (p<0.001), with a subsequent area under the receiver operator curve (AUROC) of 0.98 (95% confidence interval 0.97 to 0.99) for categorisation of HF with reduced ejection (HFrEF; LVEF<40%). In an external dataset with 200 echocardiograms, this method achieved an AUC of 0.90 (95% confidence interval 0.88 to 0.91) for HFrEF assessment. This study demonstrates that an automated neural network-based calculation of LVEF is comparable to expert clinicians performing time-consuming, frame-by-frame manual evaluation of cardiac systolic function.","sentences":["The echocardiographic measurement of left ventricular ejection fraction (LVEF) is fundamental to the diagnosis and classification of patients with heart failure (HF).","In order to quantify LVEF automatically and accurately, this paper proposes a new pipeline method based on deep neural networks and ensemble learning.","Within the pipeline, an Atrous Convolutional Neural Network (ACNN) was first trained to segment the left ventricle (LV), before employing the area-length formulation based on the ellipsoid single-plane model to calculate LVEF values.","This formulation required inputs of LV area, derived from segmentation using an improved Jeffrey's method, as well as LV length, derived from a novel ensemble learning model.","To further improve the pipeline's accuracy, an automated peak detection algorithm was used to identify end-diastolic and end-systolic frames, avoiding issues with human error.","Subsequently, single-beat LVEF values were averaged across all cardiac cycles to obtain the final LVEF.","This method was developed and internally validated in an open-source dataset containing 10,030 echocardiograms.","The Pearson's correlation coefficient was 0.83 for LVEF prediction compared to expert human analysis (p<0.001), with a subsequent area under the receiver operator curve (AUROC) of 0.98 (95% confidence interval 0.97 to 0.99) for categorisation of HF with reduced ejection (HFrEF; LVEF<40%).","In an external dataset with 200 echocardiograms, this method achieved an AUC of 0.90 (95% confidence interval 0.88 to 0.91) for HFrEF assessment.","This study demonstrates that an automated neural network-based calculation of LVEF is comparable to expert clinicians performing time-consuming, frame-by-frame manual evaluation of cardiac systolic function."],"url":"http://arxiv.org/abs/2403.12152v1","category":"cs.CV"}
{"created":"2024-03-18 18:02:18","title":"Feynman-Kac formulas for semigroups generated by multi-polaron Hamiltonians in magnetic fields and on general domains","abstract":"We prove Feynman-Kac formulas for the semigroups generated by selfadjoint operators in a class containing Fr\\\"ohlich Hamiltonians known from solid state physics. The latter model multi-polarons, i.e., a fixed number of quantum mechanical electrons moving in a polarizable crystal and interacting with the quantized phonon field generated by the crystal's vibrational modes. Both the electrons and phonons can be confined to suitable open subsets of Euclidean space. We also include possibly very singular magnetic vector potentials and electrostatic potentials. Our Feynman-Kac formulas comprise Fock space operator-valued multiplicative functionals and can be applied to every vector in the underlying Hilbert space. In comparison to the renormalized Nelson model, for which analogous Feynman-Kac formulas are known, the analysis of the creation and annihilation terms in the multiplicative functionals requires novel ideas to overcome difficulties caused by the phonon dispersion relation being constant. Getting these terms under control and generalizing other construction steps so as to cover confined systems are the main achievements of this article.","sentences":["We prove Feynman-Kac formulas for the semigroups generated by selfadjoint operators in a class containing Fr\\\"ohlich Hamiltonians known from solid state physics.","The latter model multi-polarons, i.e., a fixed number of quantum mechanical electrons moving in a polarizable crystal and interacting with the quantized phonon field generated by the crystal's vibrational modes.","Both the electrons and phonons can be confined to suitable open subsets of Euclidean space.","We also include possibly very singular magnetic vector potentials and electrostatic potentials.","Our Feynman-Kac formulas comprise Fock space operator-valued multiplicative functionals and can be applied to every vector in the underlying Hilbert space.","In comparison to the renormalized Nelson model, for which analogous Feynman-Kac formulas are known, the analysis of the creation and annihilation terms in the multiplicative functionals requires novel ideas to overcome difficulties caused by the phonon dispersion relation being constant.","Getting these terms under control and generalizing other construction steps so as to cover confined systems are the main achievements of this article."],"url":"http://arxiv.org/abs/2403.12147v1","category":"math-ph"}
{"created":"2024-03-18 18:00:20","title":"Fractionalization Signatures in the Dynamics of Quantum Spin Liquids","abstract":"We investigate the signatures of fractionalization in quantum spin liquids by studying different phases of the Kitaev honeycomb model in the presence of an out-of-plane magnetic field through which the model becomes non-integrable. Using the infinite Projected Entangled Pair States (iPEPS) ansatz, along with analytical calculations and exact diagonalization, we calculate dynamical signatures of fractionalized particles through spin-spin and dimer-dimer correlations. Our analysis demonstrates the ability of these correlations to discern distinct fractionalized quantum sectors, namely Majorana fermions and the emergent $Z_2$ fluxes, in both the chiral spin liquid (CSL) phase under weak field and the emergent intermediate gapless phase (IGP) under moderate field. Importantly, our calculation reveals the nature of IGP observed at moderate fields, a region of ongoing debate, indicating that this phase is a Majorana metal induced by strong flux fluctuations.","sentences":["We investigate the signatures of fractionalization in quantum spin liquids by studying different phases of the Kitaev honeycomb model in the presence of an out-of-plane magnetic field through which the model becomes non-integrable.","Using the infinite Projected Entangled Pair States (iPEPS) ansatz, along with analytical calculations and exact diagonalization, we calculate dynamical signatures of fractionalized particles through spin-spin and dimer-dimer correlations.","Our analysis demonstrates the ability of these correlations to discern distinct fractionalized quantum sectors, namely Majorana fermions and the emergent $Z_2$ fluxes, in both the chiral spin liquid (CSL) phase under weak field and the emergent intermediate gapless phase (IGP) under moderate field.","Importantly, our calculation reveals the nature of IGP observed at moderate fields, a region of ongoing debate, indicating that this phase is a Majorana metal induced by strong flux fluctuations."],"url":"http://arxiv.org/abs/2403.12141v1","category":"cond-mat.str-el"}
{"created":"2024-03-19 17:10:52","title":"Strangers in a foreign land: 'Yeastizing' plant enzymes","abstract":"Expressing plant metabolic pathways in microbial platforms is an efficient, cost-effective solution for producing many desired plant compounds. As eukaryotic organisms, yeasts are often the preferred platform. However, expression of plant enzymes in a yeast frequently leads to failure because the enzymes are poorly adapted to the foreign yeast cellular environment. Here we first summarize current engineering approaches for optimizing performance of plant enzymes in yeast. A critical limitation of these approaches is that they are labor-intensive and must be customized for each individual enzyme, which significantly hinders the establishment of plant pathways in cellular factories. In response to this challenge, we propose the development of a cost-effective computational pipeline to redesign plant enzymes for better adaptation to the yeast cellular milieu. This proposition is underpinned by compelling evidence that plant and yeast enzymes exhibit distinct sequence features that are generalizable across enzyme families. Consequently, we introduce a data-driven machine learning framework designed to extract 'yeastizing' rules from natural protein sequence variations, which can be broadly applied to all enzymes. Additionally, we discuss the potential to integrate the machine learning model into a full design-build-test-cycle.","sentences":["Expressing plant metabolic pathways in microbial platforms is an efficient, cost-effective solution for producing many desired plant compounds.","As eukaryotic organisms, yeasts are often the preferred platform.","However, expression of plant enzymes in a yeast frequently leads to failure because the enzymes are poorly adapted to the foreign yeast cellular environment.","Here we first summarize current engineering approaches for optimizing performance of plant enzymes in yeast.","A critical limitation of these approaches is that they are labor-intensive and must be customized for each individual enzyme, which significantly hinders the establishment of plant pathways in cellular factories.","In response to this challenge, we propose the development of a cost-effective computational pipeline to redesign plant enzymes for better adaptation to the yeast cellular milieu.","This proposition is underpinned by compelling evidence that plant and yeast enzymes exhibit distinct sequence features that are generalizable across enzyme families.","Consequently, we introduce a data-driven machine learning framework designed to extract 'yeastizing' rules from natural protein sequence variations, which can be broadly applied to all enzymes.","Additionally, we discuss the potential to integrate the machine learning model into a full design-build-test-cycle."],"url":"http://arxiv.org/abs/2403.12912v2","category":"q-bio.BM"}
{"created":"2024-03-19 15:17:23","title":"Dynamic Survival Analysis for Early Event Prediction","abstract":"This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics. By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference). This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management.","sentences":["This study advances Early Event Prediction (EEP) in healthcare through Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk localization into alarm policies to enhance clinical event metrics.","By adapting and evaluating DSA models against traditional EEP benchmarks, our research demonstrates their ability to match EEP models on a time-step level and significantly improve event-level metrics through a new alarm prioritization scheme (up to 11% AuPRC difference).","This approach represents a significant step forward in predictive healthcare, providing a more nuanced and actionable framework for early event prediction and management."],"url":"http://arxiv.org/abs/2403.12818v1","category":"cs.LG"}
{"created":"2024-03-19 14:30:56","title":"Neural Parameter Regression for Explicit Representations of PDE Solution Operators","abstract":"We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs). Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters. By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces. Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability. The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples.","sentences":["We introduce Neural Parameter Regression (NPR), a novel framework specifically developed for learning solution operators in Partial Differential Equations (PDEs).","Tailored for operator learning, this approach surpasses traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN) parameters.","By parametrizing each solution based on specific initial conditions, it effectively approximates a mapping between function spaces.","Our method enhances parameter efficiency by incorporating low-rank matrices, thereby boosting computational efficiency and scalability.","The framework shows remarkable adaptability to new initial and boundary conditions, allowing for rapid fine-tuning and inference, even in cases of out-of-distribution examples."],"url":"http://arxiv.org/abs/2403.12764v1","category":"cs.LG"}
{"created":"2024-03-19 14:12:54","title":"Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian Dialectal Data","abstract":"Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki. We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.","sentences":["Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects.","This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval.","The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information.","We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian.","Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet.","Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus.","Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki.","We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance."],"url":"http://arxiv.org/abs/2403.12749v1","category":"cs.CL"}
{"created":"2024-03-19 13:52:33","title":"To blow-up or not to blow-up for a granular kinetic equation","abstract":"A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a convolution integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics. While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial. The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport. We present a preliminary study through a meticulous numerical investigation and heuristic arguments. We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations. We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario.","sentences":["A simplified kinetic description of rapid granular media leads to a nonlocal Vlasov-type equation with a convolution integral operator that is of the same form as the continuity equations for aggregation-diffusion macroscopic dynamics.","While the singular behavior of these nonlinear continuity equations is well studied in the literature, the extension to the corresponding granular kinetic equation is highly nontrivial.","The main question is whether the singularity formed in velocity direction will be enhanced or mitigated by the shear in phase space due to free transport.","We present a preliminary study through a meticulous numerical investigation and heuristic arguments.","We have numerically developed a structure-preserving method with adaptive mesh refinement that can effectively capture potential blow-up behavior in the solution for granular kinetic equations.","We have analytically constructed a finite-time blow-up infinite mass solution and discussed how this can provide insights into the finite mass scenario."],"url":"http://arxiv.org/abs/2403.12735v1","category":"math.NA"}
{"created":"2024-03-19 13:19:41","title":"Addressing Source Scale Bias via Image Warping for Domain Adaptation","abstract":"In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture. Highlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC. Our approach adds minimal memory during training and has no additional latency at inference time. Please see Appendix for more results and analysis.","sentences":["In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets.","Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference.","While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited.","Besides, they increase computational load during training and latency during inference.","In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training.","Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation.","Our approach improves adaptation across geographies, lighting and weather conditions, is agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture.","Highlights include +6.1 mAP50 for BDD100K Clear $\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\\rightarrow$ ACDC.","Our approach adds minimal memory during training and has no additional latency at inference time.","Please see Appendix for more results and analysis."],"url":"http://arxiv.org/abs/2403.12712v1","category":"cs.CV"}
{"created":"2024-03-19 13:01:57","title":"Learning Cross-view Visual Geo-localization without Ground Truth","abstract":"Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image. Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens. In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels. We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references. To address these challenges, we propose a self-supervised learning framework to train a learnable adapter for a frozen Foundation Model (FM). This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively. To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter. To maintain the robustness of the FM's representation, we incorporate an information consistency module with a reconstruction loss, ensuring that adapted features retain strong discriminative ability across views. Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to supervised methods, while necessitating fewer training parameters and relying solely on unlabeled data. Evaluation of our adaptation for task-specific models further highlights its broad applicability.","sentences":["Cross-View Geo-Localization (CVGL) involves determining the geographical location of a query image by matching it with a corresponding GPS-tagged reference image.","Current state-of-the-art methods predominantly rely on training models with labeled paired images, incurring substantial annotation costs and training burdens.","In this study, we investigate the adaptation of frozen models for CVGL without requiring ground truth pair labels.","We observe that training on unlabeled cross-view images presents significant challenges, including the need to establish relationships within unlabeled data and reconcile view discrepancies between uncertain queries and references.","To address these challenges, we propose a self-supervised learning framework to train a learnable adapter for a frozen Foundation Model (FM).","This adapter is designed to map feature distributions from diverse views into a uniform space using unlabeled data exclusively.","To establish relationships within unlabeled data, we introduce an Expectation-Maximization-based Pseudo-labeling module, which iteratively estimates associations between cross-view features and optimizes the adapter.","To maintain the robustness of the FM's representation, we incorporate an information consistency module with a reconstruction loss, ensuring that adapted features retain strong discriminative ability across views.","Experimental results demonstrate that our proposed method achieves significant improvements over vanilla FMs and competitive accuracy compared to supervised methods, while necessitating fewer training parameters and relying solely on unlabeled data.","Evaluation of our adaptation for task-specific models further highlights its broad applicability."],"url":"http://arxiv.org/abs/2403.12702v1","category":"cs.CV"}
{"created":"2024-03-19 12:45:18","title":"WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar","abstract":"The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments. Inspired by visual grounding, in this paper, we introduce WaterVG, the first visual grounding dataset designed for USV-based waterway perception based on human intention prompts. WaterVG encompasses prompts describing multiple targets, with annotations at the instance level including bounding boxes and masks. Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text prompts. Furthermore, we propose a novel multi-modal visual grounding model, Potamoi, which is a multi-modal and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained prompts. Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts.","sentences":["The perception of waterways based on human intent holds significant importance for autonomous navigation and operations of Unmanned Surface Vehicles (USVs) in water environments.","Inspired by visual grounding, in this paper, we introduce WaterVG, the first visual grounding dataset designed for USV-based waterway perception based on human intention prompts.","WaterVG encompasses prompts describing multiple targets, with annotations at the instance level including bounding boxes and masks.","Notably, WaterVG includes 11,568 samples with 34,950 referred targets, which integrates both visual and radar characteristics captured by monocular camera and millimeter-wave (mmWave) radar, enabling a finer granularity of text prompts.","Furthermore, we propose a novel multi-modal visual grounding model, Potamoi, which is a multi-modal and multi-task model based on the one-stage paradigm with a designed Phased Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA).","In specific, MHSCA is a low-cost and efficient fusion module with a remarkably small parameter count and FLOPs, elegantly aligning and fusing scenario context information captured by two sensors with linguistic features, which can effectively address tasks of referring expression comprehension and segmentation based on fine-grained prompts.","Comprehensive experiments and evaluations have been conducted on WaterVG, where our Potamoi archives state-of-the-art performances compared with counterparts."],"url":"http://arxiv.org/abs/2403.12686v1","category":"cs.CV"}
{"created":"2024-03-19 10:31:12","title":"Surfactant-laden liquid thread breakup driven by thermal fluctuations","abstract":"The breakup of liquid threads into droplets is crucial in various applications, such as nanoprinting, nanomanufacturing, and inkjet printing, where a detailed understanding of the thinning neck dynamics allows for a precise droplet control. Here, the role of surfactant in the breakup process is studied by many-body dissipative particle dynamics, in particular, the various regime transitions and thread profiles, shedding light on molecular-level intricacies of this process hitherto inaccessible to continuum theory and experiments. Moreover, the role of surfactant in the most unstable perturbation, the formed droplet size, and surfactant distributions have been unraveled. As surfactant concentration rises, both the wavelength and time to breakup steadily increase due to the lowering of surface tension below the critical micelle concentration (CMC) and viscous effects introduced by micelles above the CMC. These changes prior to the breakup lead to larger droplets being formed in cases with higher surfactant concentration. We also compared the thinning dynamics to existing theoretical predictions, revealing that the surfactant-laden breakup starts at the inertial regime and transitions into the thermal fluctuation regime when the concentration is increased. Thus, we illuminate the hitherto poorly investigated and intricate breakup process of surfactant-laden liquid threads driven by thermal fluctuations, contributing to a deeper understanding of this process at molecular scales.","sentences":["The breakup of liquid threads into droplets is crucial in various applications, such as nanoprinting, nanomanufacturing, and inkjet printing, where a detailed understanding of the thinning neck dynamics allows for a precise droplet control.","Here, the role of surfactant in the breakup process is studied by many-body dissipative particle dynamics, in particular, the various regime transitions and thread profiles, shedding light on molecular-level intricacies of this process hitherto inaccessible to continuum theory and experiments.","Moreover, the role of surfactant in the most unstable perturbation, the formed droplet size, and surfactant distributions have been unraveled.","As surfactant concentration rises, both the wavelength and time to breakup steadily increase due to the lowering of surface tension below the critical micelle concentration (CMC) and viscous effects introduced by micelles above the CMC.","These changes prior to the breakup lead to larger droplets being formed in cases with higher surfactant concentration.","We also compared the thinning dynamics to existing theoretical predictions, revealing that the surfactant-laden breakup starts at the inertial regime and transitions into the thermal fluctuation regime when the concentration is increased.","Thus, we illuminate the hitherto poorly investigated and intricate breakup process of surfactant-laden liquid threads driven by thermal fluctuations, contributing to a deeper understanding of this process at molecular scales."],"url":"http://arxiv.org/abs/2403.12614v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 08:23:12","title":"Prompt-Guided Adaptive Model Transformation for Whole Slide Image Classification","abstract":"Multiple instance learning (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs). Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images. To address this issue, we propose PAMT, a novel Prompt-guided Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data. To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual Prompt (PVP) to reform the input data, building a compact while informative representation. Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features. We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models. Our findings affirm the potential of PAMT to set a new benchmark in WSI classification, underscoring the value of a targeted reprogramming approach.","sentences":["Multiple instance learning (MIL) has emerged as a popular method for classifying histopathology whole slide images (WSIs).","Existing approaches typically rely on frozen pre-trained models to extract instance features, neglecting the substantial domain shift between pre-training natural and histopathological images.","To address this issue, we propose PAMT, a novel Prompt-guided Adaptive Model Transformation framework that enhances MIL classification performance by seamlessly adapting pre-trained models to the specific characteristics of histopathology data.","To capture the intricate histopathology distribution, we introduce Representative Patch Sampling (RPS) and Prototypical Visual Prompt (PVP) to reform the input data, building a compact while informative representation.","Furthermore, to narrow the domain gap, we introduce Adaptive Model Transformation (AMT) that integrates adapter blocks within the feature extraction pipeline, enabling the pre-trained models to learn domain-specific features.","We rigorously evaluate our approach on two publicly available datasets, Camelyon16 and TCGA-NSCLC, showcasing substantial improvements across various MIL models.","Our findings affirm the potential of PAMT to set a new benchmark in WSI classification, underscoring the value of a targeted reprogramming approach."],"url":"http://arxiv.org/abs/2403.12537v1","category":"cs.CV"}
{"created":"2024-03-19 07:11:53","title":"Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation","abstract":"This paper addresses an interesting yet challenging problem -- source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation -- given only a pinhole image-trained model (i.e., source) and unlabeled panoramic images (i.e., target). Tackling this problem is nontrivial due to the semantic mismatches, style discrepancies, and inevitable distortion of panoramic images. To this end, we propose a novel method that utilizes Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images. Both projections are shown effective in extracting knowledge from the source model. However, the distinct projection discrepancies between source and target domains impede the direct knowledge transfer; thus, we propose a panoramic prototype adaptation module (PPAM) to integrate panoramic prototypes from the extracted knowledge for adaptation. We then impose the loss constraints on both predictions and prototypes and propose a cross-dual attention module (CDAM) at the feature level to better align the spatial and channel characteristics across the domains and projections. Both knowledge extraction and transfer processes are synchronously updated to reach the best performance. Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our method achieves significantly better performance than prior SFUDA methods for pinhole-to-panoramic adaptation.","sentences":["This paper addresses an interesting yet challenging problem -- source-free unsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic segmentation -- given only a pinhole image-trained model (i.e., source) and unlabeled panoramic images (i.e., target).","Tackling this problem is nontrivial due to the semantic mismatches, style discrepancies, and inevitable distortion of panoramic images.","To this end, we propose a novel method that utilizes Tangent Projection (TP) as it has less distortion and meanwhile slits the equirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.","Both projections are shown effective in extracting knowledge from the source model.","However, the distinct projection discrepancies between source and target domains impede the direct knowledge transfer; thus, we propose a panoramic prototype adaptation module (PPAM) to integrate panoramic prototypes from the extracted knowledge for adaptation.","We then impose the loss constraints on both predictions and prototypes and propose a cross-dual attention module (CDAM) at the feature level to better align the spatial and channel characteristics across the domains and projections.","Both knowledge extraction and transfer processes are synchronously updated to reach the best performance.","Extensive experiments on the synthetic and real-world benchmarks, including outdoor and indoor scenarios, demonstrate that our method achieves significantly better performance than prior SFUDA methods for pinhole-to-panoramic adaptation."],"url":"http://arxiv.org/abs/2403.12505v1","category":"cs.CV"}
{"created":"2024-03-19 07:02:08","title":"Task-Customized Mixture of Adapters for General Image Fusion","abstract":"General image fusion aims at integrating important information from multi-source images. However, due to the significant cross-task gap, the respective fusion mechanism varies considerably in practice, resulting in limited performance across subtasks. To handle this problem, we propose a novel task-customized mixture of adapters (TC-MoA) for general image fusion, adaptively prompting various fusion tasks in a unified model. We borrow the insight from the mixture of experts (MoE), taking the experts as efficient tuning adapters to prompt a pre-trained foundation model. These adapters are shared across different tasks and constrained by mutual information regularization, ensuring compatibility with different tasks while complementarity for multi-source images. The task-specific routing networks customize these adapters to extract task-specific information from different sources with dynamic dominant intensity, performing adaptive visual feature prompt fusion. Notably, our TC-MoA controls the dominant intensity bias for different fusion tasks, successfully unifying multiple fusion tasks in a single model. Extensive experiments show that TC-MoA outperforms the competing approaches in learning commonalities while retaining compatibility for general image fusion (multi-modal, multi-exposure, and multi-focus), and also demonstrating striking controllability on more generalization experiments. The code is available at https://github.com/YangSun22/TC-MoA .","sentences":["General image fusion aims at integrating important information from multi-source images.","However, due to the significant cross-task gap, the respective fusion mechanism varies considerably in practice, resulting in limited performance across subtasks.","To handle this problem, we propose a novel task-customized mixture of adapters (TC-MoA) for general image fusion, adaptively prompting various fusion tasks in a unified model.","We borrow the insight from the mixture of experts (MoE), taking the experts as efficient tuning adapters to prompt a pre-trained foundation model.","These adapters are shared across different tasks and constrained by mutual information regularization, ensuring compatibility with different tasks while complementarity for multi-source images.","The task-specific routing networks customize these adapters to extract task-specific information from different sources with dynamic dominant intensity, performing adaptive visual feature prompt fusion.","Notably, our TC-MoA controls the dominant intensity bias for different fusion tasks, successfully unifying multiple fusion tasks in a single model.","Extensive experiments show that TC-MoA outperforms the competing approaches in learning commonalities while retaining compatibility for general image fusion (multi-modal, multi-exposure, and multi-focus), and also demonstrating striking controllability on more generalization experiments.","The code is available at https://github.com/YangSun22/TC-MoA ."],"url":"http://arxiv.org/abs/2403.12494v1","category":"cs.CV"}
{"created":"2024-03-19 06:07:01","title":"Theoretical Modeling and Bio-inspired Trajectory Optimization of A Multiple-locomotion Origami Robot","abstract":"Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures. However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation. To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming. Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces' frictional coefficients on displacements in different motion phases. Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained. The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with multiple joints. Simulation experiments have been conducted to illustrate the effectiveness of the proposed modeling.","sentences":["Recent research on mobile robots has focused on increasing their adaptability to unpredictable and unstructured environments using soft materials and structures.","However, the determination of key design parameters and control over these compliant robots are predominantly iterated through experiments, lacking a solid theoretical foundation.","To improve their efficiency, this paper aims to provide mathematics modeling over two locomotion, crawling and swimming.","Specifically, a dynamic model is first devised to reveal the influence of the contact surfaces' frictional coefficients on displacements in different motion phases.","Besides, a swimming kinematics model is provided using coordinate transformation, based on which, we further develop an algorithm that systematically plans human-like swimming gaits, with maximum thrust obtained.","The proposed algorithm is highly generalizable and has the potential to be applied in other soft robots with multiple joints.","Simulation experiments have been conducted to illustrate the effectiveness of the proposed modeling."],"url":"http://arxiv.org/abs/2403.12471v1","category":"cs.RO"}
{"created":"2024-03-19 05:52:56","title":"CrossTune: Black-Box Few-Shot Classification with Label Enhancement","abstract":"Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning. A switch mechanism is implemented to exclude low-quality ChatGPT-generated data. Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average. Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach.","sentences":["Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating recent efforts to explore parameter-efficient adaptation to downstream tasks.","One approach is to treat these models as black boxes and use forward passes (Inference APIs) to interact with them.","Current research focuses on adapting these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an expensive process of searching task-specific prompts.","Therefore, we are motivated to study black-box language model adaptation without prompt search.","Specifically, we introduce a label-enhanced cross-attention network called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label descriptions.","Its effectiveness is examined in the context of few-shot text classification.","To improve the generalization of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning.","A switch mechanism is implemented to exclude low-quality ChatGPT-generated data.","Through extensive experiments on seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous state-of-the-art gradient-free black-box tuning method by 5.7% on average.","Even without using ChatGPT-augmented data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.12468v1","category":"cs.CL"}
{"created":"2024-03-19 05:27:04","title":"CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation","abstract":"Open-vocabulary video instance segmentation strives to segment and track instances belonging to an open set of categories in a video. The vision-language model Contrastive Language-Image Pre-training (CLIP) has shown strong zero-shot classification ability in image-level open-vocabulary task. In this paper, we propose a simple encoder-decoder network, called CLIP-VIS, to adapt CLIP for open-vocabulary video instance segmentation. Our CLIP-VIS adopts frozen CLIP image encoder and introduces three modules, including class-agnostic mask generation, temporal topK-enhanced matching, and weighted open-vocabulary classification. Given a set of initial queries, class-agnostic mask generation employs a transformer decoder to predict query masks and corresponding object scores and mask IoU scores. Then, temporal topK-enhanced matching performs query matching across frames by using K mostly matched frames. Finally, weighted open-vocabulary classification first generates query visual features with mask pooling, and second performs weighted classification using object scores and mask IoU scores. Our CLIP-VIS does not require the annotations of instance categories and identities. The experiments are performed on various video instance segmentation datasets, which demonstrate the effectiveness of our proposed method, especially on novel categories. When using ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and APn scores of 32.1% and 40.3% on validation set of LV-VIS dataset, which outperforms OV2Seg by 11.0% and 24.0% respectively. We will release the source code and models at https://github.com/zwq456/CLIP-VIS.git.","sentences":["Open-vocabulary video instance segmentation strives to segment and track instances belonging to an open set of categories in a video.","The vision-language model Contrastive Language-Image Pre-training (CLIP) has shown strong zero-shot classification ability in image-level open-vocabulary task.","In this paper, we propose a simple encoder-decoder network, called CLIP-VIS, to adapt CLIP for open-vocabulary video instance segmentation.","Our CLIP-VIS adopts frozen CLIP image encoder and introduces three modules, including class-agnostic mask generation, temporal topK-enhanced matching, and weighted open-vocabulary classification.","Given a set of initial queries, class-agnostic mask generation employs a transformer decoder to predict query masks and corresponding object scores and mask IoU scores.","Then, temporal topK-enhanced matching performs query matching across frames by using K mostly matched frames.","Finally, weighted open-vocabulary classification first generates query visual features with mask pooling, and second performs weighted classification using object scores and mask IoU scores.","Our CLIP-VIS does not require the annotations of instance categories and identities.","The experiments are performed on various video instance segmentation datasets, which demonstrate the effectiveness of our proposed method, especially on novel categories.","When using ConvNeXt-B as backbone, our CLIP-VIS achieves the AP and APn scores of 32.1% and 40.3% on validation set of LV-VIS dataset, which outperforms OV2Seg by 11.0% and 24.0% respectively.","We will release the source code and models at https://github.com/zwq456/CLIP-VIS.git."],"url":"http://arxiv.org/abs/2403.12455v1","category":"cs.CV"}
{"created":"2024-03-19 04:36:41","title":"TransformMix: Learning Transformation and Mixing Strategies from Data","abstract":"Data augmentation improves the generalization power of deep learning models by synthesizing more training samples. Sample-mixing is a popular data augmentation approach that creates additional data by combining existing samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs. Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically. A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets. If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations. In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data. In particular, TransformMix applies learned transformations and mixing masks to create compelling mixed images that contain correct and important information for the target tasks. We demonstrate the effectiveness of TransformMix on multiple datasets in transfer learning, classification, object detection, and knowledge distillation settings. Experimental results show that our method achieves better performance as well as efficiency when compared with strong sample-mixing baselines.","sentences":["Data augmentation improves the generalization power of deep learning models by synthesizing more training samples.","Sample-mixing is a popular data augmentation approach that creates additional data by combining existing samples.","Recent sample-mixing methods, like Mixup and Cutmix, adopt simple mixing operations to blend multiple inputs.","Although such a heuristic approach shows certain performance gains in some computer vision tasks, it mixes the images blindly and does not adapt to different datasets automatically.","A mixing strategy that is effective for a particular dataset does not often generalize well to other datasets.","If not properly configured, the methods may create misleading mixed images, which jeopardize the effectiveness of sample-mixing augmentations.","In this work, we propose an automated approach, TransformMix, to learn better transformation and mixing augmentation strategies from data.","In particular, TransformMix applies learned transformations and mixing masks to create compelling mixed images that contain correct and important information for the target tasks.","We demonstrate the effectiveness of TransformMix on multiple datasets in transfer learning, classification, object detection, and knowledge distillation settings.","Experimental results show that our method achieves better performance as well as efficiency when compared with strong sample-mixing baselines."],"url":"http://arxiv.org/abs/2403.12429v1","category":"cs.CV"}
{"created":"2024-03-19 03:08:08","title":"A kinetic-magnetohydrodynamic model with adaptive mesh refinement for modeling heliosphere neutral-plasma interaction","abstract":"The charge exchange between the interstellar medium (ISM) and the solar wind plasma is crucial for determining the structures of the heliosphere. Since both the neutral-ion and neutral-neutral collision mean free paths are either comparable to or larger than the size of the heliosphere, the neutral phase space distribution can deviate far away from the Maxwellian distribution. A kinetic description for the neutrals is crucial for accurately modeling the heliosphere. It is computationally challenging to run three-dimensional (3D) time-dependent kinetic simulations due to the large number of macro-particles. In this paper, we present the new highly efficient SHIELD-2 model with a kinetic model of neutrals and a magnetohydrodynamic (MHD) model for the ions and electrons. To improve the simulation efficiency, we implement adaptive mesh refinement (AMR) and particle splitting and merging algorithms for the neutral particles to reduce the particle number that is required for an accurate simulation. We present several tests to verify and demonstrate the capabilities of the model.","sentences":["The charge exchange between the interstellar medium (ISM) and the solar wind plasma is crucial for determining the structures of the heliosphere.","Since both the neutral-ion and neutral-neutral collision mean free paths are either comparable to or larger than the size of the heliosphere, the neutral phase space distribution can deviate far away from the Maxwellian distribution.","A kinetic description for the neutrals is crucial for accurately modeling the heliosphere.","It is computationally challenging to run three-dimensional (3D) time-dependent kinetic simulations due to the large number of macro-particles.","In this paper, we present the new highly efficient SHIELD-2 model with a kinetic model of neutrals and a magnetohydrodynamic (MHD) model for the ions and electrons.","To improve the simulation efficiency, we implement adaptive mesh refinement (AMR) and particle splitting and merging algorithms for the neutral particles to reduce the particle number that is required for an accurate simulation.","We present several tests to verify and demonstrate the capabilities of the model."],"url":"http://arxiv.org/abs/2403.12395v1","category":"physics.space-ph"}
{"created":"2024-03-19 02:47:33","title":"Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising","abstract":"Deep learning-based denoiser has been the focus of recent development on image denoising. In the past few years, there has been increasing interest in developing self-supervised denoising networks that only require noisy images, without the need for clean ground truth for training. However, a performance gap remains between current self-supervised methods and their supervised counterparts. Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios. Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between self-supervised and supervised methods, thereby enhancing the performance of self-supervised learning. To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges the gap between self-supervised and supervised learning. Furthermore, we have discovered that several existing self-supervised denoising frameworks naturally fall within the proposed trace-constraint loss as subcases. Extensive experiments conducted on natural and confocal image datasets indicate that our method achieves state-of-the-art performance within the realm of zero-shot self-supervised image denoising approaches, without relying on any assumptions regarding the noise.","sentences":["Deep learning-based denoiser has been the focus of recent development on image denoising.","In the past few years, there has been increasing interest in developing self-supervised denoising networks that only require noisy images, without the need for clean ground truth for training.","However, a performance gap remains between current self-supervised methods and their supervised counterparts.","Additionally, these methods commonly depend on assumptions about noise characteristics, thereby constraining their applicability in real-world scenarios.","Inspired by the properties of the Frobenius norm expansion, we discover that incorporating a trace term reduces the optimization goal disparity between self-supervised and supervised methods, thereby enhancing the performance of self-supervised learning.","To exploit this insight, we propose a trace-constraint loss function and design the low-trace adaptation Noise2Noise (LoTA-N2N) model that bridges the gap between self-supervised and supervised learning.","Furthermore, we have discovered that several existing self-supervised denoising frameworks naturally fall within the proposed trace-constraint loss as subcases.","Extensive experiments conducted on natural and confocal image datasets indicate that our method achieves state-of-the-art performance within the realm of zero-shot self-supervised image denoising approaches, without relying on any assumptions regarding the noise."],"url":"http://arxiv.org/abs/2403.12382v1","category":"eess.IV"}
{"created":"2024-03-19 02:47:10","title":"Explainable AutoML (xAutoML) with adaptive modeling for yield enhancement in semiconductor smart manufacturing","abstract":"Enhancing yield is recognized as a paramount driver to reducing production costs in semiconductor smart manufacturing. However, optimizing and ensuring high yield rates is a highly complex and technical challenge, especially while maintaining reliable yield diagnosis and prognosis, and this shall require understanding all the confounding factors in a complex condition. This study proposes a domain-specific explainable automated machine learning technique (termed xAutoML), which autonomously self-learns the optimal models for yield prediction, with an extent of explainability, and also provides insights on key diagnosis factors. The xAutoML incorporates tailored problem-solving functionalities in an auto-optimization pipeline to address the intricacies of semiconductor yield enhancement. Firstly, to capture the key diagnosis factors, knowledge-informed feature extraction coupled with model-agnostic key feature selection is designed. Secondly, combined algorithm selection and hyperparameter tuning with adaptive loss are developed to generate optimized classifiers for better defect prediction, and adaptively evolve in response to shifting data patterns. Moreover, a suite of explainability tools is provided throughout the AutoML pipeline, enhancing user understanding and fostering trust in the automated processes. The proposed xAutoML exhibits superior performance, with domain-specific refined countermeasures, adaptive optimization capabilities, and embedded explainability. Findings exhibit that the proposed xAutoML is a compelling solution for semiconductor yield improvement, defect diagnosis, and related applications.","sentences":["Enhancing yield is recognized as a paramount driver to reducing production costs in semiconductor smart manufacturing.","However, optimizing and ensuring high yield rates is a highly complex and technical challenge, especially while maintaining reliable yield diagnosis and prognosis, and this shall require understanding all the confounding factors in a complex condition.","This study proposes a domain-specific explainable automated machine learning technique (termed xAutoML), which autonomously self-learns the optimal models for yield prediction, with an extent of explainability, and also provides insights on key diagnosis factors.","The xAutoML incorporates tailored problem-solving functionalities in an auto-optimization pipeline to address the intricacies of semiconductor yield enhancement.","Firstly, to capture the key diagnosis factors, knowledge-informed feature extraction coupled with model-agnostic key feature selection is designed.","Secondly, combined algorithm selection and hyperparameter tuning with adaptive loss are developed to generate optimized classifiers for better defect prediction, and adaptively evolve in response to shifting data patterns.","Moreover, a suite of explainability tools is provided throughout the AutoML pipeline, enhancing user understanding and fostering trust in the automated processes.","The proposed xAutoML exhibits superior performance, with domain-specific refined countermeasures, adaptive optimization capabilities, and embedded explainability.","Findings exhibit that the proposed xAutoML is a compelling solution for semiconductor yield improvement, defect diagnosis, and related applications."],"url":"http://arxiv.org/abs/2403.12381v1","category":"cs.CE"}
{"created":"2024-03-19 01:49:40","title":"Local well-posedness for dispersion generalized Benjamin-Ono equations in Fourier-Lebesgue spaces","abstract":"We prove that the Cauchy problem for the dispersion generalized Benjamin-Ono equation where $0<\\alpha \\leq 1$ \\begin{eqnarray*} \\left\\{ \\begin{array}{l} \\partial_t u+|\\partial_x|^{1+\\alpha}\\partial_x u+uu_x=0,\\\\ u(x,0)=u_0(x), \\end{array} \\right. \\end{eqnarray*} is $C^2$ locally well-posed in the Fourier-Lebesgue space $\\widehat{H}^{s}_{r}(\\mathbb{R})$. This is proved via Picard iteration arguments using $X^{s,b}$-type space adapted to the Fourier-Lebesgue space, inspired by the work of Gr\\\"unrock and Vega. We also proved the ill-posedness result that shows the range is sharp. Note that, previously, Molinet, Saut and Tzvetkov \\cite{MST2001} proved that the solution map is not $C^2$ in $H^s$ for any $s$ if $0\\leq \\alpha<1$. However, in the Fourier-Lebesgue space, we have a stronger smoothing effect to handle the $high\\times low$ interactions.","sentences":["We prove that the Cauchy problem for the dispersion generalized Benjamin-Ono equation where $0<\\alpha \\leq 1$ \\begin{eqnarray*} \\left\\{ \\begin{array}{l} \\partial_t u+|\\partial_x|^{1+\\alpha}\\partial_x u+uu_x=0,\\\\ u(x,0)=u_0(x), \\end{array} \\right.","\\end{eqnarray*} is $C^2$ locally well-posed in the Fourier-Lebesgue space $\\widehat{H}^{s}_{r}(\\mathbb{R})$.","This is proved via Picard iteration arguments using $X^{s,b}$-type space adapted to the Fourier-Lebesgue space, inspired by the work of Gr\\\"unrock and Vega.","We also proved the ill-posedness result that shows the range is sharp.","Note that, previously, Molinet, Saut and Tzvetkov \\cite{MST2001} proved that the solution map is not $C^2$ in $H^s$ for any $s$ if $0\\leq \\alpha<1$. However, in the Fourier-Lebesgue space, we have a stronger smoothing effect to handle the $high\\times low$ interactions."],"url":"http://arxiv.org/abs/2403.12353v1","category":"math.AP"}
{"created":"2024-03-19 01:27:15","title":"Human Factors in Space Exploration: Opportunities for International and Interdisciplinary Collaboration","abstract":"As humanity pushes the boundaries of space exploration, human factors research becomes more important. Human factors encompass a broad spectrum of psychological, physiological, and ergonomic factors that affect human performance, well-being, and safety in the unique and challenging space environment. This panel explores the multifaceted field of human factors in space exploration and highlights the opportunities that lie in fostering international and interdisciplinary cooperation. This exploration delves into the current state of research on human factors in space missions, addressing the physiological and psychological challenges astronauts face during long space flights. It emphasizes the importance of interdisciplinary collaboration, combining knowledge from fields such as psychology, medicine, engineering, and design to address the complex interaction of factors affecting human performance and adaptation to the space environment","sentences":["As humanity pushes the boundaries of space exploration, human factors research becomes more important.","Human factors encompass a broad spectrum of psychological, physiological, and ergonomic factors that affect human performance, well-being, and safety in the unique and challenging space environment.","This panel explores the multifaceted field of human factors in space exploration and highlights the opportunities that lie in fostering international and interdisciplinary cooperation.","This exploration delves into the current state of research on human factors in space missions, addressing the physiological and psychological challenges astronauts face during long space flights.","It emphasizes the importance of interdisciplinary collaboration, combining knowledge from fields such as psychology, medicine, engineering, and design to address the complex interaction of factors affecting human performance and adaptation to the space environment"],"url":"http://arxiv.org/abs/2403.12344v1","category":"cs.HC"}
{"created":"2024-03-19 00:09:50","title":"A maximum penalised likelihood approach for semiparametric accelerated failure time models with time-varying covariates and partly interval censoring","abstract":"Accelerated failure time (AFT) models are frequently used for modelling survival data. This approach is attractive as it quantifies the direct relationship between the time until an event occurs and various covariates. It asserts that the failure times experience either acceleration or deceleration through a multiplicative factor when these covariates are present. While existing literature provides numerous methods for fitting AFT models with time-fixed covariates, adapting these approaches to scenarios involving both time-varying covariates and partly interval-censored data remains challenging. In this paper, we introduce a maximum penalised likelihood approach to fit a semiparametric AFT model. This method, designed for survival data with partly interval-censored failure times, accommodates both time-fixed and time-varying covariates. We utilise Gaussian basis functions to construct a smooth approximation of the nonparametric baseline hazard and fit the model via a constrained optimisation approach. To illustrate the effectiveness of our proposed method, we conduct a comprehensive simulation study. We also present an implementation of our approach on a randomised clinical trial dataset on advanced melanoma patients.","sentences":["Accelerated failure time (AFT) models are frequently used for modelling survival data.","This approach is attractive as it quantifies the direct relationship between the time until an event occurs and various covariates.","It asserts that the failure times experience either acceleration or deceleration through a multiplicative factor when these covariates are present.","While existing literature provides numerous methods for fitting AFT models with time-fixed covariates, adapting these approaches to scenarios involving both time-varying covariates and partly interval-censored data remains challenging.","In this paper, we introduce a maximum penalised likelihood approach to fit a semiparametric AFT model.","This method, designed for survival data with partly interval-censored failure times, accommodates both time-fixed and time-varying covariates.","We utilise Gaussian basis functions to construct a smooth approximation of the nonparametric baseline hazard and fit the model via a constrained optimisation approach.","To illustrate the effectiveness of our proposed method, we conduct a comprehensive simulation study.","We also present an implementation of our approach on a randomised clinical trial dataset on advanced melanoma patients."],"url":"http://arxiv.org/abs/2403.12332v1","category":"stat.ME"}
{"created":"2024-03-18 23:20:08","title":"Improving LoRA in Privacy-preserving Federated Learning","abstract":"Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs. The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.","sentences":["Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency.","LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module.","However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters.","A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server.","Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs.","The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices.","Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL.","Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks."],"url":"http://arxiv.org/abs/2403.12313v1","category":"cs.LG"}
{"created":"2024-03-18 21:07:57","title":"Adaptive LPD Radar Waveform Design with Generative Deep Learning","abstract":"We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing. To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can generate LPD waveforms that reduce detectability by up to 90% while simultaneously offering improved ambiguity function (sensing) characteristics. Our framework also provides a mechanism to trade-off detectability and sensing performance.","sentences":["We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment.","Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing.","To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background.","To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms.","We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network.","We find that our method can generate LPD waveforms that reduce detectability by up to 90% while simultaneously offering improved ambiguity function (sensing) characteristics.","Our framework also provides a mechanism to trade-off detectability and sensing performance."],"url":"http://arxiv.org/abs/2403.12254v1","category":"eess.SP"}
{"created":"2024-03-18 20:20:13","title":"Fusion Transformer with Object Mask Guidance for Image Forgery Analysis","abstract":"In this work, we introduce OMG-Fuser, a fusion transformer-based network designed to extract information from various forensic signals to enable robust image forgery detection and localization. Our approach can operate with an arbitrary number of forensic signals and leverages object information for their analysis -- unlike previous methods that rely on fusion schemes with few signals and often disregard image semantics. To this end, we design a forensic signal stream composed of a transformer guided by an object attention mechanism, associating patches that depict the same objects. In that way, we incorporate object-level information from the image. Each forensic signal is processed by a different stream that adapts to its peculiarities. Subsequently, a token fusion transformer efficiently aggregates the outputs of an arbitrary number of network streams and generates a fused representation for each image patch. These representations are finally processed by a long-range dependencies transformer that captures the intrinsic relations between the image patches. We assess two fusion variants on top of the proposed approach: (i) score-level fusion that fuses the outputs of multiple image forensics algorithms and (ii) feature-level fusion that fuses low-level forensic traces directly. Both variants exceed state-of-the-art performance on seven datasets for image forgery detection and localization, with a relative average improvement of 12.1% and 20.4% in terms of F1. Our network demonstrates robustness against traditional and novel forgery attacks and can be expanded with new signals without training from scratch.","sentences":["In this work, we introduce OMG-Fuser, a fusion transformer-based network designed to extract information from various forensic signals to enable robust image forgery detection and localization.","Our approach can operate with an arbitrary number of forensic signals and leverages object information for their analysis -- unlike previous methods that rely on fusion schemes with few signals and often disregard image semantics.","To this end, we design a forensic signal stream composed of a transformer guided by an object attention mechanism, associating patches that depict the same objects.","In that way, we incorporate object-level information from the image.","Each forensic signal is processed by a different stream that adapts to its peculiarities.","Subsequently, a token fusion transformer efficiently aggregates the outputs of an arbitrary number of network streams and generates a fused representation for each image patch.","These representations are finally processed by a long-range dependencies transformer that captures the intrinsic relations between the image patches.","We assess two fusion variants on top of the proposed approach: (i) score-level fusion that fuses the outputs of multiple image forensics algorithms and (ii) feature-level fusion that fuses low-level forensic traces directly.","Both variants exceed state-of-the-art performance on seven datasets for image forgery detection and localization, with a relative average improvement of 12.1% and 20.4% in terms of F1.","Our network demonstrates robustness against traditional and novel forgery attacks and can be expanded with new signals without training from scratch."],"url":"http://arxiv.org/abs/2403.12229v1","category":"cs.CV"}
{"created":"2024-03-18 19:25:57","title":"Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight","abstract":"We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL's advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tuning. Our experiments in both simulated and real-world environments demonstrate that our approach achieves superior performance and robustness than IL or RL alone in navigating a quadrotor through a racing course using only visual information without explicit state estimation.","sentences":["We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing.","We focus on directly processing visual input without explicit state estimation.","While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs.","Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift.","To overcome these limitations, we propose a novel training framework combining RL and IL's advantages.","Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tuning.","Our experiments in both simulated and real-world environments demonstrate that our approach achieves superior performance and robustness than IL or RL alone in navigating a quadrotor through a racing course using only visual information without explicit state estimation."],"url":"http://arxiv.org/abs/2403.12203v1","category":"cs.RO"}
{"created":"2024-03-18 18:50:59","title":"AMReX and pyAMReX: Looking Beyond ECP","abstract":"AMReX is a software framework for the development of block-structured mesh applications with adaptive mesh refinement (AMR). AMReX was initially developed and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale Computing Project, and is continuing to grow post-ECP. In addition to adding new functionality and performance improvements to the core AMReX framework, we have also developed a Python binding, pyAMReX, that provides a bridge between AMReX-based application codes and the data science ecosystem. pyAMReX provides zero-copy application GPU data access for AI/ML, in situ analysis and application coupling, and enables rapid, massively parallel prototyping. In this paper we review the overall functionality of AMReX and pyAMReX, focusing on new developments, new functionality, and optimizations of key operations. We also summarize capabilities of ECP projects that used AMReX and provide an overview of new, non-ECP applications.","sentences":["AMReX is a software framework for the development of block-structured mesh applications with adaptive mesh refinement (AMR).","AMReX was initially developed and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale Computing Project, and is continuing to grow post-ECP.","In addition to adding new functionality and performance improvements to the core AMReX framework, we have also developed a Python binding, pyAMReX, that provides a bridge between AMReX-based application codes and the data science ecosystem.","pyAMReX provides zero-copy application GPU data access for AI/ML, in situ analysis and application coupling, and enables rapid, massively parallel prototyping.","In this paper we review the overall functionality of AMReX and pyAMReX, focusing on new developments, new functionality, and optimizations of key operations.","We also summarize capabilities of ECP projects that used AMReX and provide an overview of new, non-ECP applications."],"url":"http://arxiv.org/abs/2403.12179v1","category":"cs.DC"}
{"created":"2024-03-18 18:00:00","title":"Unveiling MOA-2007-BLG-192: An M Dwarf Hosting a Likely Super-Earth","abstract":"We present an analysis of high angular resolution images of the microlensing target MOA-2007-BLG-192 using Keck adaptive optics and the Hubble Space Telescope. The planetary host star is robustly detected as it separates from the background source star in nearly all of the Keck and Hubble data. The amplitude and direction of the lens-source separation allows us to break a degeneracy related to the microlensing parallax and source radius crossing time. Thus, we are able to reduce the number of possible solutions by a factor of ${\\sim}2$, demonstrating the power of high angular resolution follow-up imaging for events with sparse light curve coverage. Following Bennett et al. 2023, we apply constraints from the high resolution imaging on the light curve modeling to find host star and planet masses of $M_{\\textrm{host}} = 0.28 \\pm 0.04M_{\\odot}$ and $m_p = 12.49^{+65.47}_{-8.03}M_{\\oplus}$ at a distance from Earth of $D_L = 2.16 \\pm 0.30\\,$kpc. This work illustrates the necessity for the Nancy Grace Roman Galactic Exoplanet Survey (RGES) to use its own high resolution imaging to inform light curve modeling for microlensing planets that the mission discovers.","sentences":["We present an analysis of high angular resolution images of the microlensing target MOA-2007-BLG-192 using Keck adaptive optics and the Hubble Space Telescope.","The planetary host star is robustly detected as it separates from the background source star in nearly all of the Keck and Hubble data.","The amplitude and direction of the lens-source separation allows us to break a degeneracy related to the microlensing parallax and source radius crossing time.","Thus, we are able to reduce the number of possible solutions by a factor of ${\\sim}2$, demonstrating the power of high angular resolution follow-up imaging for events with sparse light curve coverage.","Following Bennett et al. 2023, we apply constraints from the high resolution imaging on the light curve modeling to find host star and planet masses of $M_{\\textrm{host}} = 0.28 \\pm 0.04M_{\\odot}$ and $m_p = 12.49^{+65.47}_{-8.03}M_{\\oplus}$ at a distance from Earth of $D_L = 2.16 \\pm 0.30\\,$kpc.","This work illustrates the necessity for the Nancy Grace Roman Galactic Exoplanet Survey (RGES) to use its own high resolution imaging to inform light curve modeling for microlensing planets that the mission discovers."],"url":"http://arxiv.org/abs/2403.12118v1","category":"astro-ph.EP"}
{"created":"2024-03-18 18:00:00","title":"Low-overhead non-Clifford topological fault-tolerant circuits for all non-chiral abelian topological phases","abstract":"We propose a family of explicit geometrically local circuits realizing any abelian non-chiral topological phase as an actively error-corrected fault-tolerant memory. These circuits are constructed from measuring 1-form symmetries in discrete fixed-point path integrals, which we express through cellular cohomology and higher-order cup products. The specific path integral we use is the abelian Dijkgraaf-Witten state sum on a 3-dimensional cellulation, which is a spacetime representation of the twisted quantum double model. The resulting circuits are based on a syndrome extraction circuit of the (qudit) stabilizer toric code, into which we insert non-Clifford phase gates that implement the ``twist''. The overhead compared to the toric code is moderate, in contrast to known constructions for twisted abelian phases. We also show that other architectures for the (qudit) toric code phase, like measurement-based topological quantum computation or Floquet codes, can be enriched with phase gates to implement twisted quantum doubles instead of their untwisted versions. As a further result, we prove fault tolerance under arbitrary local (including non-Pauli) noise for a very general class of topological circuits that we call 1-form symmetric fixed-point circuits. This notion unifies the circuits in this paper as well as the stabilizer toric code, subsystem toric code, measurement-based topological quantum computation, or the (CSS) honeycomb Floquet code. We also demonstrate how our method can be adapted to construct fault-tolerant circuits for specific non-Abelian phases. In the appendix we present an explicit combinatorial procedure to define formulas for higher cup products on arbitrary cellulations, which might be interesting in its own right to the TQFT and topological-phases community.","sentences":["We propose a family of explicit geometrically local circuits realizing any abelian non-chiral topological phase as an actively error-corrected fault-tolerant memory.","These circuits are constructed from measuring 1-form symmetries in discrete fixed-point path integrals, which we express through cellular cohomology and higher-order cup products.","The specific path integral we use is the abelian Dijkgraaf-Witten state sum on a 3-dimensional cellulation, which is a spacetime representation of the twisted quantum double model.","The resulting circuits are based on a syndrome extraction circuit of the (qudit) stabilizer toric code, into which we insert non-Clifford phase gates that implement the ``twist''.","The overhead compared to the toric code is moderate, in contrast to known constructions for twisted abelian phases.","We also show that other architectures for the (qudit) toric code phase, like measurement-based topological quantum computation or Floquet codes, can be enriched with phase gates to implement twisted quantum doubles instead of their untwisted versions.","As a further result, we prove fault tolerance under arbitrary local (including non-Pauli) noise for a very general class of topological circuits that we call 1-form symmetric fixed-point circuits.","This notion unifies the circuits in this paper as well as the stabilizer toric code, subsystem toric code, measurement-based topological quantum computation, or the (CSS) honeycomb Floquet code.","We also demonstrate how our method can be adapted to construct fault-tolerant circuits for specific non-Abelian phases.","In the appendix we present an explicit combinatorial procedure to define formulas for higher cup products on arbitrary cellulations, which might be interesting in its own right to the TQFT and topological-phases community."],"url":"http://arxiv.org/abs/2403.12119v1","category":"quant-ph"}
{"created":"2024-03-18 17:59:40","title":"One-Step Image Translation with Text-to-Image Models","abstract":"In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning. To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives. Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference. This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives. Our code and models are available at https://github.com/GaParmar/img2img-turbo.","sentences":["In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning.","To tackle these issues, we introduce a general method for adapting a single-step diffusion model to new tasks and domains through adversarial learning objectives.","Specifically, we consolidate various modules of the vanilla latent diffusion model into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input image structure while reducing overfitting.","We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain.","We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference.","This work suggests that single-step diffusion models can serve as strong backbones for a range of GAN learning objectives.","Our code and models are available at https://github.com/GaParmar/img2img-turbo."],"url":"http://arxiv.org/abs/2403.12036v1","category":"cs.CV"}
{"created":"2024-03-18 17:59:09","title":"Generic 3D Diffusion Adapter Using Controlled Multi-View Editing","abstract":"Open-domain 3D object synthesis has been lagging behind image synthesis due to limited data and higher computational complexity. To bridge this gap, recent works have investigated multi-view diffusion but often fall short in either 3D consistency, visual quality, or efficiency. This paper proposes MVEdit, which functions as a 3D counterpart of SDEdit, employing ancestral sampling to jointly denoise multi-view images and output high-quality textured meshes. Built on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency through a training-free 3D Adapter, which lifts the 2D views of the last timestep into a coherent 3D representation, then conditions the 2D views of the next timestep using rendered views, without uncompromising visual quality. With an inference time of only 2-5 minutes, this framework achieves better trade-off between quality and speed than score distillation. MVEdit is highly versatile and extendable, with a wide range of applications including text/image-to-3D generation, 3D-to-3D editing, and high-quality texture synthesis. In particular, evaluations demonstrate state-of-the-art performance in both image-to-3D and text-guided texture generation tasks. Additionally, we introduce a method for fine-tuning 2D latent diffusion models on small 3D datasets with limited resources, enabling fast low-resolution text-to-3D initialization.","sentences":["Open-domain 3D object synthesis has been lagging behind image synthesis due to limited data and higher computational complexity.","To bridge this gap, recent works have investigated multi-view diffusion but often fall short in either 3D consistency, visual quality, or efficiency.","This paper proposes MVEdit, which functions as a 3D counterpart of SDEdit, employing ancestral sampling to jointly denoise multi-view images and output high-quality textured meshes.","Built on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency through a training-free 3D Adapter, which lifts the 2D views of the last timestep into a coherent 3D representation, then conditions the 2D views of the next timestep using rendered views, without uncompromising visual quality.","With an inference time of only 2-5 minutes, this framework achieves better trade-off between quality and speed than score distillation.","MVEdit is highly versatile and extendable, with a wide range of applications including text/image-to-3D generation, 3D-to-3D editing, and high-quality texture synthesis.","In particular, evaluations demonstrate state-of-the-art performance in both image-to-3D and text-guided texture generation tasks.","Additionally, we introduce a method for fine-tuning 2D latent diffusion models on small 3D datasets with limited resources, enabling fast low-resolution text-to-3D initialization."],"url":"http://arxiv.org/abs/2403.12032v2","category":"cs.CV"}
{"created":"2024-03-18 17:58:13","title":"Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning","abstract":"Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new-stage spaces. Correspondingly, we design a semantic-guided prototype complement strategy that synthesizes old classes' new features without using any old class instance. Extensive experiments on seven benchmark datasets verify EASE's state-of-the-art performance. Code is available at: https://github.com/sun-hailong/CVPR24-Ease","sentences":["Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting.","Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones.","Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes.","As a result, it is desired to figure out a way of efficient model updating without harming former knowledge.","In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL.","To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces.","These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces.","As data evolves, the expanding subspaces render the old class classifiers incompatible with new-stage spaces.","Correspondingly, we design a semantic-guided prototype complement strategy that synthesizes old classes' new features without using any old class instance.","Extensive experiments on seven benchmark datasets verify EASE's state-of-the-art performance.","Code is available at: https://github.com/sun-hailong/CVPR24-Ease"],"url":"http://arxiv.org/abs/2403.12030v1","category":"cs.CV"}
{"created":"2024-03-18 17:46:06","title":"SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion","abstract":"We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.","sentences":["We present Stable Video 3D (SV3D) -- a latent video diffusion model for high-resolution, image-to-multi-view generation of orbital videos around a 3D object.","Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization.","However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation.","In this work, we propose SV3D that adapts image-to-video diffusion model for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS.","We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation.","Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D's state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works."],"url":"http://arxiv.org/abs/2403.12008v1","category":"cs.CV"}
{"created":"2024-03-18 17:41:26","title":"GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning","abstract":"Self-supervised learning has achieved remarkable success in acquiring high-quality representations from unlabeled data. The widely adopted contrastive learning framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image. However, existing techniques to construct positive views highly rely on manual transformations, resulting in limited diversity and potentially false positive pairs. To tackle these challenges, we present GenView, a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics. We develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability. Additionally, we introduce a quality-driven contrastive loss, which assesses the quality of positive pairs by considering both foreground similarity and background diversity. This loss prioritizes the high-quality positive pairs we construct while reducing the influence of low-quality pairs, thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive data augmentation. Thanks to the improved positive view quality and the quality-driven contrastive loss, GenView significantly improves self-supervised learning across various tasks. For instance, GenView improves MoCov2 performance by 2.5%/2.2% on ImageNet linear/semi-supervised classification. Moreover, GenView even performs much better than naively augmenting the ImageNet dataset with Laion400M or ImageNet21K. Code is available at https://github.com/xiaojieli0903/genview.","sentences":["Self-supervised learning has achieved remarkable success in acquiring high-quality representations from unlabeled data.","The widely adopted contrastive learning framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image.","However, existing techniques to construct positive views highly rely on manual transformations, resulting in limited diversity and potentially false positive pairs.","To tackle these challenges, we present GenView, a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics.","We develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability.","Additionally, we introduce a quality-driven contrastive loss, which assesses the quality of positive pairs by considering both foreground similarity and background diversity.","This loss prioritizes the high-quality positive pairs we construct while reducing the influence of low-quality pairs, thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive data augmentation.","Thanks to the improved positive view quality and the quality-driven contrastive loss, GenView significantly improves self-supervised learning across various tasks.","For instance, GenView improves MoCov2 performance by 2.5%/2.2% on ImageNet linear/semi-supervised classification.","Moreover, GenView even performs much better than naively augmenting the ImageNet dataset with Laion400M or ImageNet21K. Code is available at https://github.com/xiaojieli0903/genview."],"url":"http://arxiv.org/abs/2403.12003v1","category":"cs.CV"}
{"created":"2024-03-18 17:32:23","title":"Learning Useful Representations of Recurrent Neural Network Weight Matrices","abstract":"Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority.","sentences":["Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers.","The program of an RNN is its weight matrix.","How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks?","While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping.","We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs.","Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs.","We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior.","We create and release the first two 'model zoo' datasets for RNN weight representation learning.","One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits.","With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications.","On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority."],"url":"http://arxiv.org/abs/2403.11998v1","category":"cs.LG"}
{"created":"2024-03-18 17:27:00","title":"Adaptive stepsize algorithms for Langevin dynamics","abstract":"We discuss the design of an invariant measure-preserving transformed dynamics for the numerical treatment of Langevin dynamics based on rescaling of time, with the goal of sampling from an invariant measure. Given an appropriate monitor function which characterizes the numerical difficulty of the problem as a function of the state of the system, this method allows the stepsizes to be reduced only when necessary, facilitating efficient recovery of long-time behavior. We study both the overdamped and underdamped Langevin dynamics. We investigate how an appropriate correction term that ensures preservation of the invariant measure should be incorporated into a numerical splitting scheme. Finally, we demonstrate the use of the technique in several model systems, including a Bayesian sampling problem with a steep prior.","sentences":["We discuss the design of an invariant measure-preserving transformed dynamics for the numerical treatment of Langevin dynamics based on rescaling of time, with the goal of sampling from an invariant measure.","Given an appropriate monitor function which characterizes the numerical difficulty of the problem as a function of the state of the system, this method allows the stepsizes to be reduced only when necessary, facilitating efficient recovery of long-time behavior.","We study both the overdamped and underdamped Langevin dynamics.","We investigate how an appropriate correction term that ensures preservation of the invariant measure should be incorporated into a numerical splitting scheme.","Finally, we demonstrate the use of the technique in several model systems, including a Bayesian sampling problem with a steep prior."],"url":"http://arxiv.org/abs/2403.11993v2","category":"math.NA"}
{"created":"2024-03-18 17:23:50","title":"Suppression of the superfluid Kelvin-Helmholtz instability due to massive vortex cores, friction and confinement","abstract":"We characterize the dynamical instability responsible for the breakdown of regular rows and necklaces of quantized vortices that appear at the interface between two superfluids in relative motion. Making use of a generalized point-vortex model, we identify several mechanisms leading to the suppression of this instability. They include a non-zero mass of the vortex cores, dissipative processes resulting from the interaction between the vortices and the excitations of the superfluid, and the proximity of the vortex array to the sample boundaries. We show that massive vortex cores not only have a mitigating effect on the dynamical instability, but also change the associated scaling law and affect the direction along which it develops. The predictions of our massive and dissipative point-vortex model are eventually compared against recent experimental measurements of the maximum instability growth rate relevant to vortex necklaces in a cold-atom platform.","sentences":["We characterize the dynamical instability responsible for the breakdown of regular rows and necklaces of quantized vortices that appear at the interface between two superfluids in relative motion.","Making use of a generalized point-vortex model, we identify several mechanisms leading to the suppression of this instability.","They include a non-zero mass of the vortex cores, dissipative processes resulting from the interaction between the vortices and the excitations of the superfluid, and the proximity of the vortex array to the sample boundaries.","We show that massive vortex cores not only have a mitigating effect on the dynamical instability, but also change the associated scaling law and affect the direction along which it develops.","The predictions of our massive and dissipative point-vortex model are eventually compared against recent experimental measurements of the maximum instability growth rate relevant to vortex necklaces in a cold-atom platform."],"url":"http://arxiv.org/abs/2403.11987v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-18 17:12:00","title":"OUCopula: Bi-Channel Multi-Label Copula-Enhanced Adapter-Based CNN for Myopia Screening Based on OU-UWF Images","abstract":"Myopia screening using cutting-edge ultra-widefield (UWF) fundus imaging is potentially significant for ophthalmic outcomes. Current multidisciplinary research between ophthalmology and deep learning (DL) concentrates primarily on disease classification and diagnosis using single-eye images, largely ignoring joint modeling and prediction for Oculus Uterque (OU, both eyes). Inspired by the complex relationships between OU and the high correlation between the (continuous) outcome labels (Spherical Equivalent and Axial Length), we propose a framework of copula-enhanced adapter convolutional neural network (CNN) learning with OU UWF fundus images (OUCopula) for joint prediction of multiple clinical scores. We design a novel bi-channel multi-label CNN that can (1) take bi-channel image inputs subject to both high correlation and heterogeneity (by sharing the same backbone network and employing adapters to parameterize the channel-wise discrepancy), and (2) incorporate correlation information between continuous output labels (using a copula). Solid experiments show that OUCopula achieves satisfactory performance in myopia score prediction compared to backbone models. Moreover, OUCopula can far exceed the performance of models constructed for single-eye inputs. Importantly, our study also hints at the potential extension of the bi-channel model to a multi-channel paradigm and the generalizability of OUCopula across various backbone CNNs.","sentences":["Myopia screening using cutting-edge ultra-widefield (UWF) fundus imaging is potentially significant for ophthalmic outcomes.","Current multidisciplinary research between ophthalmology and deep learning (DL) concentrates primarily on disease classification and diagnosis using single-eye images, largely ignoring joint modeling and prediction for Oculus Uterque (OU, both eyes).","Inspired by the complex relationships between OU and the high correlation between the (continuous) outcome labels (Spherical Equivalent and Axial Length), we propose a framework of copula-enhanced adapter convolutional neural network (CNN) learning with OU UWF fundus images (OUCopula) for joint prediction of multiple clinical scores.","We design a novel bi-channel multi-label CNN that can (1) take bi-channel image inputs subject to both high correlation and heterogeneity (by sharing the same backbone network and employing adapters to parameterize the channel-wise discrepancy), and (2) incorporate correlation information between continuous output labels (using a copula).","Solid experiments show that OUCopula achieves satisfactory performance in myopia score prediction compared to backbone models.","Moreover, OUCopula can far exceed the performance of models constructed for single-eye inputs.","Importantly, our study also hints at the potential extension of the bi-channel model to a multi-channel paradigm and the generalizability of OUCopula across various backbone CNNs."],"url":"http://arxiv.org/abs/2403.11974v1","category":"eess.IV"}
{"created":"2024-03-18 17:08:24","title":"Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory","abstract":"Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation.","sentences":["Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning.","In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties.","Despite the empirical success, theory of conditional diffusion models is largely missing.","This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models.","Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound.","The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique.","Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation."],"url":"http://arxiv.org/abs/2403.11968v1","category":"cs.LG"}
{"created":"2024-03-18 17:08:04","title":"Probing Site-Resolved Current in Strongly Interacting Superconducting Circuit Lattices","abstract":"Transport measurements are fundamental for understanding condensed matter phenomena, from superconductivity to the fractional quantum Hall effect. Analogously, they can be powerful tools for probing synthetic quantum matter in quantum simulators. Here we demonstrate the measurement of in-situ particle current in a superconducting circuit lattice and apply it to study transport in both coherent and bath-coupled lattices. Our method utilizes controlled tunneling in a double-well potential to map current to on-site density, revealing site-resolved current and current statistics. We prepare a strongly interacting Bose-Hubbard lattice at different lattice fillings, and observe the change in current statistics as the many-body states transition from superfluid to Mott insulator. Furthermore, we explore non-equilibrium current dynamics by coupling the lattice to engineered driven-dissipative baths that serve as tunable particle source and drain. We observe steady-state current in discrete conduction channels and interaction-assisted transport. These results establish a versatile platform to investigate microscopic quantum transport in superconducting circuits.","sentences":["Transport measurements are fundamental for understanding condensed matter phenomena, from superconductivity to the fractional quantum Hall effect.","Analogously, they can be powerful tools for probing synthetic quantum matter in quantum simulators.","Here we demonstrate the measurement of in-situ particle current in a superconducting circuit lattice and apply it to study transport in both coherent and bath-coupled lattices.","Our method utilizes controlled tunneling in a double-well potential to map current to on-site density, revealing site-resolved current and current statistics.","We prepare a strongly interacting Bose-Hubbard lattice at different lattice fillings, and observe the change in current statistics as the many-body states transition from superfluid to Mott insulator.","Furthermore, we explore non-equilibrium current dynamics by coupling the lattice to engineered driven-dissipative baths that serve as tunable particle source and drain.","We observe steady-state current in discrete conduction channels and interaction-assisted transport.","These results establish a versatile platform to investigate microscopic quantum transport in superconducting circuits."],"url":"http://arxiv.org/abs/2403.11967v1","category":"quant-ph"}
{"created":"2024-03-18 16:42:39","title":"Learning Dynamical Systems Encoding Non-Linearity within Space Curvature","abstract":"Dynamical Systems (DS) are an effective and powerful means of shaping high-level policies for robotics control. They provide robust and reactive control while ensuring the stability of the driving vector field. The increasing complexity of real-world scenarios necessitates DS with a higher degree of non-linearity, along with the ability to adapt to potential changes in environmental conditions, such as obstacles. Current learning strategies for DSs often involve a trade-off, sacrificing either stability guarantees or offline computational efficiency in order to enhance the capabilities of the learned DS. Online local adaptation to environmental changes is either not taken into consideration or treated as a separate problem. In this paper, our objective is to introduce a method that enhances the complexity of the learned DS without compromising efficiency during training or stability guarantees. Furthermore, we aim to provide a unified approach for seamlessly integrating the initially learned DS's non-linearity with any local non-linearities that may arise due to changes in the environment. We propose a geometrical approach to learn asymptotically stable non-linear DS for robotics control. Each DS is modeled as a harmonic damped oscillator on a latent manifold. By learning the manifold's Euclidean embedded representation, our approach encodes the non-linearity of the DS within the curvature of the space. Having an explicit embedded representation of the manifold allows us to showcase obstacle avoidance by directly inducing local deformations of the space. We demonstrate the effectiveness of our methodology through two scenarios: first, the 2D learning of synthetic vector fields, and second, the learning of 3D robotic end-effector motions in real-world settings.","sentences":["Dynamical Systems (DS) are an effective and powerful means of shaping high-level policies for robotics control.","They provide robust and reactive control while ensuring the stability of the driving vector field.","The increasing complexity of real-world scenarios necessitates DS with a higher degree of non-linearity, along with the ability to adapt to potential changes in environmental conditions, such as obstacles.","Current learning strategies for DSs often involve a trade-off, sacrificing either stability guarantees or offline computational efficiency in order to enhance the capabilities of the learned DS.","Online local adaptation to environmental changes is either not taken into consideration or treated as a separate problem.","In this paper, our objective is to introduce a method that enhances the complexity of the learned DS without compromising efficiency during training or stability guarantees.","Furthermore, we aim to provide a unified approach for seamlessly integrating the initially learned DS's non-linearity with any local non-linearities that may arise due to changes in the environment.","We propose a geometrical approach to learn asymptotically stable non-linear DS for robotics control.","Each DS is modeled as a harmonic damped oscillator on a latent manifold.","By learning the manifold's Euclidean embedded representation, our approach encodes the non-linearity of the DS within the curvature of the space.","Having an explicit embedded representation of the manifold allows us to showcase obstacle avoidance by directly inducing local deformations of the space.","We demonstrate the effectiveness of our methodology through two scenarios: first, the 2D learning of synthetic vector fields, and second, the learning of 3D robotic end-effector motions in real-world settings."],"url":"http://arxiv.org/abs/2403.11948v1","category":"cs.RO"}
{"created":"2024-03-18 16:40:41","title":"Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees","abstract":"With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources. Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs. However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses. We aim to address this challenging problem and introduce a reinforcement learning-based approach using differentiable decision trees. This approach integrates the scalability of data-driven reinforcement learning with the explainability of (differentiable) decision trees. This leads to a controller that can be easily adapted across different houses and provides a simple control policy that can be explained to end-users, further improving user acceptance. As a proof-of-concept, we analyze our method using a home energy management problem, comparing its performance with commercially available rule-based baseline and standard neural network-based RL controllers. Through this preliminary study, we show that the performance of our proposed method is comparable to standard RL-based controllers, outperforming baseline controllers by ~20% in terms of daily cost savings while being straightforward to explain.","sentences":["With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources.","Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs.","However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses.","We aim to address this challenging problem and introduce a reinforcement learning-based approach using differentiable decision trees.","This approach integrates the scalability of data-driven reinforcement learning with the explainability of (differentiable) decision trees.","This leads to a controller that can be easily adapted across different houses and provides a simple control policy that can be explained to end-users, further improving user acceptance.","As a proof-of-concept, we analyze our method using a home energy management problem, comparing its performance with commercially available rule-based baseline and standard neural network-based RL controllers.","Through this preliminary study, we show that the performance of our proposed method is comparable to standard RL-based controllers, outperforming baseline controllers by ~20% in terms of daily cost savings while being straightforward to explain."],"url":"http://arxiv.org/abs/2403.11947v1","category":"eess.SY"}
{"created":"2024-03-18 16:36:09","title":"Perfect Zero-Knowledge PCPs for #P","abstract":"We construct perfect zero-knowledge probabilistically checkable proofs (PZK-PCPs) for every language in #P. This is the first construction of a PZK-PCP for any language outside BPP. Furthermore, unlike previous constructions of (statistical) zero-knowledge PCPs, our construction simultaneously achieves non-adaptivity and zero knowledge against arbitrary (adaptive) polynomial-time malicious verifiers.   Our construction consists of a novel masked sumcheck PCP, which uses the combinatorial nullstellensatz to obtain antisymmetric structure within the hypercube and randomness outside of it. To prove zero knowledge, we introduce the notion of locally simulatable encodings: randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message. We show that the code arising from the sumcheck protocol (the Reed-Muller code augmented with subcube sums) admits a locally simulatable encoding. This reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions.","sentences":["We construct perfect zero-knowledge probabilistically checkable proofs (PZK-PCPs) for every language in #P. This is the first construction of a PZK-PCP for any language outside BPP.","Furthermore, unlike previous constructions of (statistical) zero-knowledge PCPs, our construction simultaneously achieves non-adaptivity and zero knowledge against arbitrary (adaptive) polynomial-time malicious verifiers.   ","Our construction consists of a novel masked sumcheck PCP, which uses the combinatorial nullstellensatz to obtain antisymmetric structure within the hypercube and randomness outside of it.","To prove zero knowledge, we introduce the notion of locally simulatable encodings: randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message.","We show that the code arising from the sumcheck protocol (the Reed-Muller code augmented with subcube sums) admits a locally simulatable encoding.","This reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions."],"url":"http://arxiv.org/abs/2403.11941v2","category":"cs.CC"}
{"created":"2024-03-18 16:19:41","title":"Adaptative Bilingual Aligning Using Multilingual Sentence Embedding","abstract":"In this paper, we present an adaptive bitextual alignment system called AIlign. This aligner relies on sentence embeddings to extract reliable anchor points that can guide the alignment path, even for texts whose parallelism is fragmentary and not strictly monotonic. In an experiment on several datasets, we show that AIlign achieves results equivalent to the state of the art, with quasi-linear complexity. In addition, AIlign is able to handle texts whose parallelism and monotonicity properties are only satisfied locally, unlike recent systems such as Vecalign or Bertalign.","sentences":["In this paper, we present an adaptive bitextual alignment system called AIlign.","This aligner relies on sentence embeddings to extract reliable anchor points that can guide the alignment path, even for texts whose parallelism is fragmentary and not strictly monotonic.","In an experiment on several datasets, we show that AIlign achieves results equivalent to the state of the art, with quasi-linear complexity.","In addition, AIlign is able to handle texts whose parallelism and monotonicity properties are only satisfied locally, unlike recent systems such as Vecalign or Bertalign."],"url":"http://arxiv.org/abs/2403.11921v1","category":"cs.CL"}
{"created":"2024-03-18 16:12:23","title":"Bacterial Communications and Computing in Internet of Everything (IoE)","abstract":"Concurrent with advancements in molecular communication (MC), bacterial communication is emerging as a key area of interest. Given the frequent use of bacteria in various MC models, it is essential to have a thorough grasp of their intrinsic communication, signaling, and engineering techniques. Although it is crucial to have a strong understanding of the communication background, the inherent biological variability of bacteria may introduce complexity. Thus, an in-depth understanding of bacteria and their communication is a must for improving and extending the models in which they are utilized. Furthermore, the emerging and evolving domain of bacterial computing provides an exciting opportunity for advancing applications in areas such as environmental monitoring and biological computing networks. By integrating the communication and sensing capabilities, bacterial computing offers a promising framework for enhancing the adaptability and responsiveness of bacteria. This paper provides a comprehensive review of bacterial communication and computing, illustrating their application and the link with the concept of the Internet of Everything (IoE). Through the analysis of these biological systems, we reach a deeper insight on how the small-scale interactions may contribute to the major concept of universal interconnectedness; thus, we make the knowledge to flow and communication stronger between different fields. The discussion include the identification of the different bacterial mechanisms that could revolutionize the traditional communication systems. Thus, this paper offers valuable insights into previously unaddressed aspects of bacterial behavior, suggesting novel avenues for future research and aiming to advance understanding and application of bacterial sensing, communication and computing in MC models.","sentences":["Concurrent with advancements in molecular communication (MC), bacterial communication is emerging as a key area of interest.","Given the frequent use of bacteria in various MC models, it is essential to have a thorough grasp of their intrinsic communication, signaling, and engineering techniques.","Although it is crucial to have a strong understanding of the communication background, the inherent biological variability of bacteria may introduce complexity.","Thus, an in-depth understanding of bacteria and their communication is a must for improving and extending the models in which they are utilized.","Furthermore, the emerging and evolving domain of bacterial computing provides an exciting opportunity for advancing applications in areas such as environmental monitoring and biological computing networks.","By integrating the communication and sensing capabilities, bacterial computing offers a promising framework for enhancing the adaptability and responsiveness of bacteria.","This paper provides a comprehensive review of bacterial communication and computing, illustrating their application and the link with the concept of the Internet of Everything (IoE).","Through the analysis of these biological systems, we reach a deeper insight on how the small-scale interactions may contribute to the major concept of universal interconnectedness; thus, we make the knowledge to flow and communication stronger between different fields.","The discussion include the identification of the different bacterial mechanisms that could revolutionize the traditional communication systems.","Thus, this paper offers valuable insights into previously unaddressed aspects of bacterial behavior, suggesting novel avenues for future research and aiming to advance understanding and application of bacterial sensing, communication and computing in MC models."],"url":"http://arxiv.org/abs/2403.11911v1","category":"eess.SP"}
{"created":"2024-03-18 16:03:45","title":"A Closer Look at Claim Decomposition","abstract":"As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources. Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference. We investigate how various methods of claim decomposition -- especially LLM-based methods -- affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used. This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric's decomposition step. To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore. We then propose an LLM-based approach to generating decompositions inspired by Bertrand Russell's theory of logical atomism and neo-Davidsonian semantics and demonstrate its improved decomposition quality over previous methods.","sentences":["As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources.","Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference.","We investigate how various methods of claim decomposition -- especially LLM-based methods -- affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used.","This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric's decomposition step.","To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore.","We then propose an LLM-based approach to generating decompositions inspired by Bertrand Russell's theory of logical atomism and neo-Davidsonian semantics and demonstrate its improved decomposition quality over previous methods."],"url":"http://arxiv.org/abs/2403.11903v1","category":"cs.CL"}
{"created":"2024-03-18 15:47:14","title":"Dual-Energy Cone-Beam CT Using Two Complementary Limited-Angle Scans with A Projection-Consistent Diffusion Model","abstract":"Background: Dual-energy imaging on cone-beam CT (CBCT) scanners has great potential in different clinical applications, including image-guided surgery and adaptive proton therapy. However, the clinical practice of dual-energy CBCT (DE-CBCT) has been hindered by the requirement of sophisticated hardware components. Purpose: In this work, we aim to propose a practical solution for single-scan dual-energy imaging on current CBCT scanners without hardware modifications, using two complementary limited-angle scans with a projection-consistent diffusion model. Methods: Our approach has two major components: data acquisition using two complementary limited-angle scans, and dual-energy projections restoration with subsequent FDK reconstruction. Two complementary scans at different kVps are performed in a single rotation by switching the tube voltage at the middle of the source trajectory, acquiring the mixed-spectra projection in a single CBCT scan. Full-sampled dual-energy projections are then restored by a projection-consistent diffusion model in a slice-by-slice manner, followed by the DE-CBCT reconstruction using the FDK algorithm. Results: The proposed method was evaluated in a simulation study of digital abdomen phantoms and a study of real rat data. In the simulation study, the proposed method produced DE-CBCT images at a mean absolute error (MAE) of 20 HU. In the small-animal study, reconstructed DE-CBCT images using the proposed method gave an MAE of 25 HU. Conclusion: This study demonstrates the feasibility of DE-CBCT imaging using two complementary limited-angle scans with a projection-consistent diffusion model in both half-fan and short scans. The proposed method may allow quantitative applications of DE-CBCT and enable DE-CBCT-based adaptive proton therapy.","sentences":["Background: Dual-energy imaging on cone-beam CT (CBCT) scanners has great potential in different clinical applications, including image-guided surgery and adaptive proton therapy.","However, the clinical practice of dual-energy CBCT (DE-CBCT) has been hindered by the requirement of sophisticated hardware components.","Purpose:","In this work, we aim to propose a practical solution for single-scan dual-energy imaging on current CBCT scanners without hardware modifications, using two complementary limited-angle scans with a projection-consistent diffusion model.","Methods: Our approach has two major components: data acquisition using two complementary limited-angle scans, and dual-energy projections restoration with subsequent FDK reconstruction.","Two complementary scans at different kVps are performed in a single rotation by switching the tube voltage at the middle of the source trajectory, acquiring the mixed-spectra projection in a single CBCT scan.","Full-sampled dual-energy projections are then restored by a projection-consistent diffusion model in a slice-by-slice manner, followed by the DE-CBCT reconstruction using the FDK algorithm.","Results:","The proposed method was evaluated in a simulation study of digital abdomen phantoms and a study of real rat data.","In the simulation study, the proposed method produced DE-CBCT images at a mean absolute error (MAE) of 20 HU.","In the small-animal study, reconstructed DE-CBCT images using the proposed method gave an MAE of 25 HU.","Conclusion: This study demonstrates the feasibility of DE-CBCT imaging using two complementary limited-angle scans with a projection-consistent diffusion model in both half-fan and short scans.","The proposed method may allow quantitative applications of DE-CBCT and enable DE-CBCT-based adaptive proton therapy."],"url":"http://arxiv.org/abs/2403.11890v1","category":"physics.med-ph"}
{"created":"2024-03-18 15:27:58","title":"Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic Vision Sensors","abstract":"Unmanned Aerial Vehicles (UAVs) are gaining popularity in civil and military applications. However, uncontrolled access to restricted areas threatens privacy and security. Thus, prevention and detection of UAVs are pivotal to guarantee confidentiality and safety. Although active scanning, mainly based on radars, is one of the most accurate technologies, it can be expensive and less versatile than passive inspections, e.g., object recognition. Dynamic vision sensors (DVS) are bio-inspired event-based vision models that leverage timestamped pixel-level brightness changes in fast-moving scenes that adapt well to low-latency object detection. This paper presents F-UAV-D (Fast Unmanned Aerial Vehicle Detector), an embedded system that enables fast-moving drone detection. In particular, we propose a setup to exploit DVS as an alternative to RGB cameras in a real-time and low-power configuration. Our approach leverages the high-dynamic range (HDR) and background suppression of DVS and, when trained with various fast-moving drones, outperforms RGB input in suboptimal ambient conditions such as low illumination and fast-moving scenes. Our results show that F-UAV-D can (i) detect drones by using less than <15 W on average and (ii) perform real-time inference (i.e., <50 ms) by leveraging the CPU and GPU nodes of our edge computer.","sentences":["Unmanned Aerial Vehicles (UAVs) are gaining popularity in civil and military applications.","However, uncontrolled access to restricted areas threatens privacy and security.","Thus, prevention and detection of UAVs are pivotal to guarantee confidentiality and safety.","Although active scanning, mainly based on radars, is one of the most accurate technologies, it can be expensive and less versatile than passive inspections, e.g., object recognition.","Dynamic vision sensors (DVS) are bio-inspired event-based vision models that leverage timestamped pixel-level brightness changes in fast-moving scenes that adapt well to low-latency object detection.","This paper presents F-UAV-D (Fast Unmanned Aerial Vehicle Detector), an embedded system that enables fast-moving drone detection.","In particular, we propose a setup to exploit DVS as an alternative to RGB cameras in a real-time and low-power configuration.","Our approach leverages the high-dynamic range (HDR) and background suppression of DVS and, when trained with various fast-moving drones, outperforms RGB input in suboptimal ambient conditions such as low illumination and fast-moving scenes.","Our results show that F-UAV-D can (i) detect drones by using less than <15 W on average and (ii) perform real-time inference (i.e., <50 ms) by leveraging the CPU and GPU nodes of our edge computer."],"url":"http://arxiv.org/abs/2403.11875v1","category":"cs.CV"}
{"created":"2024-03-18 14:50:48","title":"Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine Learning","abstract":"This paper describes a generalizable model evaluation method that can be adapted to evaluate AI/ML models across multiple criteria including core scientific principles and more practical outcomes. Emerging from prediction competitions in Psychology and Decision Science, the method evaluates a group of candidate models of varying type and structure across multiple scientific, theoretic, and practical criteria. Ordinal ranking of criteria scores are evaluated using voting rules from the field of computational social choice and allow the comparison of divergent measures and types of models in a holistic evaluation. Additional advantages and applications are discussed.","sentences":["This paper describes a generalizable model evaluation method that can be adapted to evaluate AI/ML models across multiple criteria including core scientific principles and more practical outcomes.","Emerging from prediction competitions in Psychology and Decision Science, the method evaluates a group of candidate models of varying type and structure across multiple scientific, theoretic, and practical criteria.","Ordinal ranking of criteria scores are evaluated using voting rules from the field of computational social choice and allow the comparison of divergent measures and types of models in a holistic evaluation.","Additional advantages and applications are discussed."],"url":"http://arxiv.org/abs/2403.11840v1","category":"cs.LG"}
{"created":"2024-03-18 14:15:39","title":"Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery","abstract":"We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness.","sentences":["We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D.","This is a challenging problem due to two primary reasons.","Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation.","Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene.","To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF.","We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels.","Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results.","Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness."],"url":"http://arxiv.org/abs/2403.11812v1","category":"cs.CV"}
{"created":"2024-03-18 14:05:52","title":"Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation","abstract":"Existing parameter-efficient fine-tuning (PEFT) methods have achieved significant success on vision transformers (ViTs) adaptation by improving parameter efficiency. However, the exploration of enhancing inference efficiency during adaptation remains underexplored. This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive. In this paper, we propose Dynamic Tuning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation. Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference. Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance. We validate DyT across various tasks, including image/video recognition and semantic segmentation. For instance, DyT achieves comparable or even superior performance compared to existing PEFT methods while evoking only 71%-85% of their FLOPs on the VTAB-1K benchmark.","sentences":["Existing parameter-efficient fine-tuning (PEFT) methods have achieved significant success on vision transformers (ViTs) adaptation by improving parameter efficiency.","However, the exploration of enhancing inference efficiency during adaptation remains underexplored.","This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive.","In this paper, we propose Dynamic Tuning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation.","Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference.","Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance.","We validate DyT across various tasks, including image/video recognition and semantic segmentation.","For instance, DyT achieves comparable or even superior performance compared to existing PEFT methods while evoking only 71%-85% of their FLOPs on the VTAB-1K benchmark."],"url":"http://arxiv.org/abs/2403.11808v1","category":"cs.CV"}
{"created":"2024-03-18 14:02:53","title":"Federated Modality-specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation","abstract":"Most existing federated learning (FL) methods for medical image analysis only considered intramodal heterogeneity, limiting their applicability to multimodal imaging applications. In practice, it is not uncommon that some FL participants only possess a subset of the complete imaging modalities, posing inter-modal heterogeneity as a challenge to effectively training a global model on all participants' data. In addition, each participant would expect to obtain a personalized model tailored for its local data characteristics from the FL in such a scenario. In this work, we propose a new FL framework with federated modality-specific encoders and multimodal anchors (FedMEMA) to simultaneously address the two concurrent issues. Above all, FedMEMA employs an exclusive encoder for each modality to account for the inter-modal heterogeneity in the first place. In the meantime, while the encoders are shared by the participants, the decoders are personalized to meet individual needs. Specifically, a server with full-modal data employs a fusion decoder to aggregate and fuse representations from all modality-specific encoders, thus bridging the modalities to optimize the encoders via backpropagation reversely. Meanwhile, multiple anchors are extracted from the fused multimodal representations and distributed to the clients in addition to the encoder parameters. On the other end, the clients with incomplete modalities calibrate their missing-modal representations toward the global full-modal anchors via scaled dot-product cross-attention, making up the information loss due to absent modalities while adapting the representations of present ones. FedMEMA is validated on the BraTS 2020 benchmark for multimodal brain tumor segmentation. Results show that it outperforms various up-to-date methods for multimodal and personalized FL and that its novel designs are effective. Our code is available.","sentences":["Most existing federated learning (FL) methods for medical image analysis only considered intramodal heterogeneity, limiting their applicability to multimodal imaging applications.","In practice, it is not uncommon that some FL participants only possess a subset of the complete imaging modalities, posing inter-modal heterogeneity as a challenge to effectively training a global model on all participants' data.","In addition, each participant would expect to obtain a personalized model tailored for its local data characteristics from the FL in such a scenario.","In this work, we propose a new FL framework with federated modality-specific encoders and multimodal anchors (FedMEMA) to simultaneously address the two concurrent issues.","Above all, FedMEMA employs an exclusive encoder for each modality to account for the inter-modal heterogeneity in the first place.","In the meantime, while the encoders are shared by the participants, the decoders are personalized to meet individual needs.","Specifically, a server with full-modal data employs a fusion decoder to aggregate and fuse representations from all modality-specific encoders, thus bridging the modalities to optimize the encoders via backpropagation reversely.","Meanwhile, multiple anchors are extracted from the fused multimodal representations and distributed to the clients in addition to the encoder parameters.","On the other end, the clients with incomplete modalities calibrate their missing-modal representations toward the global full-modal anchors via scaled dot-product cross-attention, making up the information loss due to absent modalities while adapting the representations of present ones.","FedMEMA is validated on the BraTS 2020 benchmark for multimodal brain tumor segmentation.","Results show that it outperforms various up-to-date methods for multimodal and personalized FL and that its novel designs are effective.","Our code is available."],"url":"http://arxiv.org/abs/2403.11803v1","category":"cs.CV"}
{"created":"2024-03-18 13:46:32","title":"Locomotion Generation for a Rat Robot based on Environmental Changes via Reinforcement Learning","abstract":"This research focuses on developing reinforcement learning approaches for the locomotion generation of small-size quadruped robots. The rat robot NeRmo is employed as the experimental platform. Due to the constrained volume, small-size quadruped robots typically possess fewer and weaker sensors, resulting in difficulty in accurately perceiving and responding to environmental changes. In this context, insufficient and imprecise feedback data from sensors makes it difficult to generate adaptive locomotion based on reinforcement learning. To overcome these challenges, this paper proposes a novel reinforcement learning approach that focuses on extracting effective perceptual information to enhance the environmental adaptability of small-size quadruped robots. According to the frequency of a robot's gait stride, key information of sensor data is analyzed utilizing sinusoidal functions derived from Fourier transform results. Additionally, a multifunctional reward mechanism is proposed to generate adaptive locomotion in different tasks. Extensive simulations are conducted to assess the effectiveness of the proposed reinforcement learning approach in generating rat robot locomotion in various environments. The experiment results illustrate the capability of the proposed approach to maintain stable locomotion of a rat robot across different terrains, including ramps, stairs, and spiral stairs.","sentences":["This research focuses on developing reinforcement learning approaches for the locomotion generation of small-size quadruped robots.","The rat robot NeRmo is employed as the experimental platform.","Due to the constrained volume, small-size quadruped robots typically possess fewer and weaker sensors, resulting in difficulty in accurately perceiving and responding to environmental changes.","In this context, insufficient and imprecise feedback data from sensors makes it difficult to generate adaptive locomotion based on reinforcement learning.","To overcome these challenges, this paper proposes a novel reinforcement learning approach that focuses on extracting effective perceptual information to enhance the environmental adaptability of small-size quadruped robots.","According to the frequency of a robot's gait stride, key information of sensor data is analyzed utilizing sinusoidal functions derived from Fourier transform results.","Additionally, a multifunctional reward mechanism is proposed to generate adaptive locomotion in different tasks.","Extensive simulations are conducted to assess the effectiveness of the proposed reinforcement learning approach in generating rat robot locomotion in various environments.","The experiment results illustrate the capability of the proposed approach to maintain stable locomotion of a rat robot across different terrains, including ramps, stairs, and spiral stairs."],"url":"http://arxiv.org/abs/2403.11788v2","category":"cs.RO"}
{"created":"2024-03-18 13:42:06","title":"ForzaETH Race Stack -- Scaled Autonomous Head-to-Head Racing on Fully Commercial off-the-Shelf Hardware","abstract":"Autonomous racing in robotics combines high-speed dynamics with the necessity for reliability and real-time decision-making. While such racing pushes software and hardware to their limits, many existing full-system solutions necessitate complex, custom hardware and software, and usually focus on Time-Trials rather than full unrestricted Head-to-Head racing, due to financial and safety constraints. This limits their reproducibility, making advancements and replication feasible mostly for well-resourced laboratories with comprehensive expertise in mechanical, electrical, and robotics fields. Researchers interested in the autonomy domain but with only partial experience in one of these fields, need to spend significant time with familiarization and integration. The ForzaETH Race Stack addresses this gap by providing an autonomous racing software platform designed for F1TENTH, a 1:10 scaled Head-to-Head autonomous racing competition, which simplifies replication by using commercial off-the-shelf hardware. This approach enhances the competitive aspect of autonomous racing and provides an accessible platform for research and development in the field. The ForzaETH Race Stack is designed with modularity and operational ease of use in mind, allowing customization and adaptability to various environmental conditions, such as track friction and layout. Capable of handling both Time-Trials and Head-to-Head racing, the stack has demonstrated its effectiveness, robustness, and adaptability in the field by winning the official F1TENTH international competition multiple times.","sentences":["Autonomous racing in robotics combines high-speed dynamics with the necessity for reliability and real-time decision-making.","While such racing pushes software and hardware to their limits, many existing full-system solutions necessitate complex, custom hardware and software, and usually focus on Time-Trials rather than full unrestricted Head-to-Head racing, due to financial and safety constraints.","This limits their reproducibility, making advancements and replication feasible mostly for well-resourced laboratories with comprehensive expertise in mechanical, electrical, and robotics fields.","Researchers interested in the autonomy domain but with only partial experience in one of these fields, need to spend significant time with familiarization and integration.","The ForzaETH Race Stack addresses this gap by providing an autonomous racing software platform designed for F1TENTH, a 1:10 scaled Head-to-Head autonomous racing competition, which simplifies replication by using commercial off-the-shelf hardware.","This approach enhances the competitive aspect of autonomous racing and provides an accessible platform for research and development in the field.","The ForzaETH Race Stack is designed with modularity and operational ease of use in mind, allowing customization and adaptability to various environmental conditions, such as track friction and layout.","Capable of handling both Time-Trials and Head-to-Head racing, the stack has demonstrated its effectiveness, robustness, and adaptability in the field by winning the official F1TENTH international competition multiple times."],"url":"http://arxiv.org/abs/2403.11784v1","category":"cs.RO"}
{"created":"2024-03-18 12:57:24","title":"Revisiting Tensor Basis Neural Networks for Reynolds stress modeling: application to plane channel and square duct flows","abstract":"Several Tensor Basis Neural Network (TBNN) frameworks aimed at enhancing turbulence RANS modeling have recently been proposed in the literature as data-driven constitutive models for systems with known invariance properties. However, persistent ambiguities remain regarding the physical adequacy of applying the General Eddy Viscosity Model (GEVM). This work aims at investigating this aspect in an a priori stage for better predictions of the Reynolds stress anisotropy tensor, while preserving the Galilean and rotational invariances. In particular, we propose a general framework providing optimal tensor basis models for two types of canonical flows: Plane Channel Flow (PCF) and Square Duct Flow (SDF). Subsequently, deep neural networks based on these optimal models are trained using state-of-the-art strategies to achieve a balanced and physically sound prediction of the full anisotropy tensor. A priori results obtained by the proposed framework are in very good agreement with the reference DNS data. Notably, our shallow network with three layers provides accurate predictions of the anisotropy tensor for PCF at unobserved friction Reynolds numbers, both in interpolation and extrapolation scenarios. Learning the SDF case is more challenging because of its physical nature and a lack of training data at various regimes. We propose to alleviate this problem based on Transfer Learning (TL). To more efficiently generalize to an unseen intermediate $\\mathrm{Re}_\\tau$ regime, we take advantage of our prior knowledge acquired from a training with a larger and wider dataset. Our results indicate the potential of the developed network model, and demonstrate the feasibility and efficiency of the TL process in terms of training data size and training time. Based on these results, we believe there is a promising future by integrating these neural networks into an adapted in-house RANS solver.","sentences":["Several Tensor Basis Neural Network (TBNN) frameworks aimed at enhancing turbulence RANS modeling have recently been proposed in the literature as data-driven constitutive models for systems with known invariance properties.","However, persistent ambiguities remain regarding the physical adequacy of applying the General Eddy Viscosity Model (GEVM).","This work aims at investigating this aspect in an a priori stage for better predictions of the Reynolds stress anisotropy tensor, while preserving the Galilean and rotational invariances.","In particular, we propose a general framework providing optimal tensor basis models for two types of canonical flows: Plane Channel Flow (PCF) and Square Duct Flow (SDF).","Subsequently, deep neural networks based on these optimal models are trained using state-of-the-art strategies to achieve a balanced and physically sound prediction of the full anisotropy tensor.","A priori results obtained by the proposed framework are in very good agreement with the reference DNS data.","Notably, our shallow network with three layers provides accurate predictions of the anisotropy tensor for PCF at unobserved friction Reynolds numbers, both in interpolation and extrapolation scenarios.","Learning the SDF case is more challenging because of its physical nature and a lack of training data at various regimes.","We propose to alleviate this problem based on Transfer Learning (TL).","To more efficiently generalize to an unseen intermediate $\\mathrm{Re}_\\tau$ regime, we take advantage of our prior knowledge acquired from a training with a larger and wider dataset.","Our results indicate the potential of the developed network model, and demonstrate the feasibility and efficiency of the TL process in terms of training data size and training time.","Based on these results, we believe there is a promising future by integrating these neural networks into an adapted in-house RANS solver."],"url":"http://arxiv.org/abs/2403.11746v1","category":"physics.flu-dyn"}
{"created":"2024-03-18 12:55:40","title":"PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks","abstract":"In this work we address flexibility in deep learning by means of transductive reasoning. For adaptation to new tasks or new data, existing methods typically involve tuning of learnable parameters or even complete re-training from scratch, rendering such approaches unflexible in practice. We argue that the notion of separating computation from memory by the means of transduction can act as a stepping stone for solving these issues. We therefore propose PARMESAN (parameter-free memory search and transduction), a scalable transduction method which leverages a memory module for solving dense prediction tasks. At inference, hidden representations in memory are being searched to find corresponding examples. In contrast to other methods, PARMESAN learns without the requirement for any continuous training or fine-tuning of learnable parameters simply by modifying the memory content. Our method is compatible with commonly used neural architectures and canonically transfers to 1D, 2D, and 3D grid-based data. We demonstrate the capabilities of our approach at complex tasks such as continual and few-shot learning. PARMESAN learns up to 370 times faster than common baselines while being on par in terms of predictive performance, knowledge retention, and data-efficiency.","sentences":["In this work we address flexibility in deep learning by means of transductive reasoning.","For adaptation to new tasks or new data, existing methods typically involve tuning of learnable parameters or even complete re-training from scratch, rendering such approaches unflexible in practice.","We argue that the notion of separating computation from memory by the means of transduction can act as a stepping stone for solving these issues.","We therefore propose PARMESAN (parameter-free memory search and transduction), a scalable transduction method which leverages a memory module for solving dense prediction tasks.","At inference, hidden representations in memory are being searched to find corresponding examples.","In contrast to other methods, PARMESAN learns without the requirement for any continuous training or fine-tuning of learnable parameters simply by modifying the memory content.","Our method is compatible with commonly used neural architectures and canonically transfers to 1D, 2D, and 3D grid-based data.","We demonstrate the capabilities of our approach at complex tasks such as continual and few-shot learning.","PARMESAN learns up to 370 times faster than common baselines while being on par in terms of predictive performance, knowledge retention, and data-efficiency."],"url":"http://arxiv.org/abs/2403.11743v1","category":"cs.LG"}
{"created":"2024-03-18 12:40:06","title":"Controller to eliminate the first harmonic in delayed periodic signal with application to rotational scanning atomic force microscopy","abstract":"The plant (system to be controlled) produces a periodic signal containing broad spectrum of the Fourier harmonics. The first Fourier harmonic (sine-type signal) assumed to be undesirable and should be removed by an external force, while other harmonics should be preserved. Because the measured plant data has an unknown amount of time delay and a sensitivity of the plant to external force is unknown, thus the amplitude and the phase of an anti-sine control force are unknown as well. We developed an adaptive controller described as a linear time-invariant system which is able to remove the first harmonic from the plant's output by constantly adjusting it's parameters of control force even if time delay of the plant output signal and plant sensitivity to the external force have slow variations over time. That type of controller was requested to further extend capabilities of newly developed high-speed large-area rotational scanning atomic force microscopy technique where the sample is rotated and a tilt angle between the normal of the sample surface and the axis of rotation produces the parasitic first Fourier harmonic which significantly limits the scanning area.","sentences":["The plant (system to be controlled) produces a periodic signal containing broad spectrum of the Fourier harmonics.","The first Fourier harmonic (sine-type signal) assumed to be undesirable and should be removed by an external force, while other harmonics should be preserved.","Because the measured plant data has an unknown amount of time delay and a sensitivity of the plant to external force is unknown, thus the amplitude and the phase of an anti-sine control force are unknown as well.","We developed an adaptive controller described as a linear time-invariant system which is able to remove the first harmonic from the plant's output by constantly adjusting it's parameters of control force even if time delay of the plant output signal and plant sensitivity to the external force have slow variations over time.","That type of controller was requested to further extend capabilities of newly developed high-speed large-area rotational scanning atomic force microscopy technique where the sample is rotated and a tilt angle between the normal of the sample surface and the axis of rotation produces the parasitic first Fourier harmonic which significantly limits the scanning area."],"url":"http://arxiv.org/abs/2403.11731v1","category":"nlin.AO"}
{"created":"2024-03-18 05:53:20","title":"SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction","abstract":"Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction. To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement. However, they either incur a large amount of computation or bring limited improvement, if not both. In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation. Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario. SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models. Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models. Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission. Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement. Codes are available at https://github.com/opendilab/SmartRefine/","sentences":["Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments.","Context information, such as road maps and surrounding agents' states, provides crucial geometric and semantic information for motion behavior prediction.","To this end, recent works explore two-stage prediction frameworks where coarse trajectories are first proposed, and then used to select critical context information for trajectory refinement.","However, they either incur a large amount of computation or bring limited improvement, if not both.","In this paper, we introduce a novel scenario-adaptive refinement strategy, named SmartRefine, to refine prediction with minimal additional computation.","Specifically, SmartRefine can comprehensively adapt refinement configurations based on each scenario's properties, and smartly chooses the number of refinement iterations by introducing a quality score to measure the prediction quality and remaining refinement potential of each scenario.","SmartRefine is designed as a generic and flexible approach that can be seamlessly integrated into most state-of-the-art motion prediction models.","Experiments on Argoverse (1 & 2) show that our method consistently improves the prediction accuracy of multiple state-of-the-art prediction models.","Specifically, by adding SmartRefine to QCNet, we outperform all published ensemble-free works on the Argoverse 2 leaderboard (single agent track) at submission.","Comprehensive studies are also conducted to ablate design choices and explore the mechanism behind multi-iteration refinement.","Codes are available at https://github.com/opendilab/SmartRefine/"],"url":"http://arxiv.org/abs/2403.11492v2","category":"cs.CV"}
{"created":"2024-03-19 17:55:52","title":"Damped energy-norm a posteriori error estimates for fully discrete approximations of the wave equation using C2-reconstructions","abstract":"We derive a posteriori error estimates for the the scalar wave equation discretized in space by continuous finite elements and in time by the explicit leapfrog scheme. Our analysis combines the idea of invoking extra time-regularity for the right-hand side, as previously introduced in the space semi-discrete setting, with a novel, piecewise quartic, globally twice-differentiable time-reconstruction of the fully discrete solution. Our main results show that the proposed estimator is reliable and efficient in a damped energy norm. These properties are illustrated in a series of numerical examples.","sentences":["We derive a posteriori error estimates for the the scalar wave equation discretized in space by continuous finite elements and in time by the explicit leapfrog scheme.","Our analysis combines the idea of invoking extra time-regularity for the right-hand side, as previously introduced in the space semi-discrete setting, with a novel, piecewise quartic, globally twice-differentiable time-reconstruction of the fully discrete solution.","Our main results show that the proposed estimator is reliable and efficient in a damped energy norm.","These properties are illustrated in a series of numerical examples."],"url":"http://arxiv.org/abs/2403.12954v1","category":"math.NA"}
{"created":"2024-03-19 17:36:34","title":"Chemical differentiation and gas kinematics around massive young stellar objects in RCW 120","abstract":"We present results of a spectral survey towards a dense molecular condensation and young stellar objects (YSOs) projected on the border of the HII region RCW 120 and discuss emission of 20 molecules which produce the brightest lines. The survey was performed with the APEX telescope in the frequency range 200 -- 260 GHz. We provide evidences for two outflows in the dense gas. The first one is powered by the RCW 120 S2 YSO and oriented along the line of sight. The second outflow around RCW 120 S1 is aligned almost perpendicular to the line of sight. We show that area with bright emission of CH$_3$OH, CH$_3$CCH and CH$_3$CN are organised into an onion-like structure where CH$_3$CN traces warmer regions around the YSOs than the other molecules. Methanol seems to be released to the gas phase by shock waves in the vicinity of the outflows while thermal evaporation still does not work towards the YSOs. We find only a single manifestation of the UV radiation to the molecules, namely, enhanced abundances of small hydrocarbons CCH and c-C$_3$H$_2$ in the photo-dissociation region.","sentences":["We present results of a spectral survey towards a dense molecular condensation and young stellar objects (YSOs) projected on the border of the HII region RCW 120 and discuss emission of 20 molecules which produce the brightest lines.","The survey was performed with the APEX telescope in the frequency range 200 -- 260 GHz.","We provide evidences for two outflows in the dense gas.","The first one is powered by the RCW 120 S2 YSO and oriented along the line of sight.","The second outflow around RCW 120 S1 is aligned almost perpendicular to the line of sight.","We show that area with bright emission of CH$_3$OH, CH$_3$CCH and CH$_3$CN are organised into an onion-like structure where CH$_3$CN traces warmer regions around the YSOs than the other molecules.","Methanol seems to be released to the gas phase by shock waves in the vicinity of the outflows while thermal evaporation still does not work towards the YSOs.","We find only a single manifestation of the UV radiation to the molecules, namely, enhanced abundances of small hydrocarbons CCH and c-C$_3$H$_2$ in the photo-dissociation region."],"url":"http://arxiv.org/abs/2403.12934v1","category":"astro-ph.GA"}
{"created":"2024-03-19 17:06:04","title":"Regularised Spectral Estimation for High Dimensional Point Processes","abstract":"Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process. We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix. In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons. However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics. In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting. We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes. Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer connectivity in the brain network.","sentences":["Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process.","We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix.","In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons.","However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics.","In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting.","We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes.","Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer connectivity in the brain network."],"url":"http://arxiv.org/abs/2403.12908v1","category":"stat.ME"}
{"created":"2024-03-19 16:58:09","title":"Geodesic vector fields, induced contact structures and tightness in dimension three","abstract":"In this paper, we provide new and simpler proofs of two theorems of Gluck and Harrison on contact structures induced by great circle or line fibrations. Furthermore, we prove that a geodesic vector field whose Jacobi tensor is parallel along flow lines (e.g. if the underlying manifold is locally symmetric) induces a contact structure if the 'mixed' sectional curvatures are nonnegative, and if a certain nondegeneracy condition holds. Additionally, we prove that in dimension three, contact structures admitting a Reeb flow which is either periodic, isometric, or free and proper, must be universally tight. In particular, we generalise an earlier result of Geiges and the author, by showing that every contact form on $\\mathbb{R}^3$ whose Reeb vector field spans a line fibration is necessarily tight. Furthermore, we provide a characterisation of isometric Reeb vector fields. As an application, we recover a result of Kegel and Lange on Seifert fibrations spanned by Reeb vector fields, and we classify closed contact $3$-manifolds with isometric Reeb flows (also known as $R$-contact manifolds) up to diffeomorphism.","sentences":["In this paper, we provide new and simpler proofs of two theorems of Gluck and Harrison on contact structures induced by great circle or line fibrations.","Furthermore, we prove that a geodesic vector field whose Jacobi tensor is parallel along flow lines (e.g. if the underlying manifold is locally symmetric) induces a contact structure if the 'mixed' sectional curvatures are nonnegative, and if a certain nondegeneracy condition holds.","Additionally, we prove that in dimension three, contact structures admitting a Reeb flow which is either periodic, isometric, or free and proper, must be universally tight.","In particular, we generalise an earlier result of Geiges and the author, by showing that every contact form on $\\mathbb{R}^3$ whose Reeb vector field spans a line fibration is necessarily tight.","Furthermore, we provide a characterisation of isometric Reeb vector fields.","As an application, we recover a result of Kegel and Lange on Seifert fibrations spanned by Reeb vector fields, and we classify closed contact $3$-manifolds with isometric Reeb flows (also known as $R$-contact manifolds) up to diffeomorphism."],"url":"http://arxiv.org/abs/2403.12903v1","category":"math.SG"}
{"created":"2024-03-19 16:34:31","title":"Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport","abstract":"We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, we propose to train our model with gradient flow w.r.t. the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition. Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width. Performing a local Polyak-\\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer. This is the first result of this type for infinitely deep and arbitrarily wide ResNets.","sentences":["We study the convergence of gradient flow for the training of deep neural networks.","If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective.","Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent.","To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers.","Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures.","Motivated by this approach, we propose to train our model with gradient flow w.r.t.","the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition.","Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width.","Performing a local Polyak-\\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer.","This is the first result of this type for infinitely deep and arbitrarily wide ResNets."],"url":"http://arxiv.org/abs/2403.12887v1","category":"cs.LG"}
{"created":"2024-03-19 16:33:03","title":"Improved decay results for micropolar flows with nonlinear damping","abstract":"We examine the long-time behavior of solutions (and their derivatives) to the micropolar equations with nonlinear velocity damping. Additionally, we get a speed-up gain of $ t^{1/2} $ for the angular velocity, consistent with established findings for classic micropolar flows lacking nonlinear damping. Consequently, we also obtain a sharper result regarding the asymptotic stability of the micro-rotational velocity $\\ww(\\cdot,t)$. Related results of independent interest are also included.","sentences":["We examine the long-time behavior of solutions (and their derivatives) to the micropolar equations with nonlinear velocity damping.","Additionally, we get a speed-up gain of $ t^{1/2} $ for the angular velocity, consistent with established findings for classic micropolar flows lacking nonlinear damping.","Consequently, we also obtain a sharper result regarding the asymptotic stability of the micro-rotational velocity $\\ww(\\cdot,t)$. Related results of independent interest are also included."],"url":"http://arxiv.org/abs/2403.12885v1","category":"math.AP"}
{"created":"2024-03-19 16:15:44","title":"Wildfire danger prediction optimization with transfer learning","abstract":"Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection. This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas. Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions. The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns. Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\\% in identifying burnt areas. This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires. By combining transfer learning and CNNs, this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations.","sentences":["Convolutional Neural Networks (CNNs) have proven instrumental across various computer science domains, enabling advancements in object detection, classification, and anomaly detection.","This paper explores the application of CNNs to analyze geospatial data specifically for identifying wildfire-affected areas.","Leveraging transfer learning techniques, we fine-tuned CNN hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess moisture conditions.","The study establishes a methodology for computing wildfire risk levels on a scale of 0 to 5, dynamically linked to weather patterns.","Notably, through the integration of transfer learning, the CNN model achieved an impressive accuracy of 95\\% in identifying burnt areas.","This research sheds light on the inner workings of CNNs and their practical, real-time utility in predicting and mitigating wildfires.","By combining transfer learning and CNNs, this study contributes a robust approach to assess burnt areas, facilitating timely interventions and preventative measures against conflagrations."],"url":"http://arxiv.org/abs/2403.12871v1","category":"cs.LG"}
{"created":"2024-03-19 16:15:08","title":"PoNQ: a Neural QEM-based Mesh Representation","abstract":"Although polygon meshes have been a standard representation in geometry processing, their irregular and combinatorial nature hinders their suitability for learning-based applications. In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape, which we denote PoNQ. A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors. Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume. Notably, our representation does not rely on a regular grid, is supervised directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics.","sentences":["Although polygon meshes have been a standard representation in geometry processing, their irregular and combinatorial nature hinders their suitability for learning-based applications.","In this work, we introduce a novel learnable mesh representation through a set of local 3D sample Points and their associated Normals and Quadric error metrics (QEM) w.r.t.","the underlying shape, which we denote PoNQ.","A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the local quadric errors.","Besides marking the first use of QEM within a neural shape representation, our contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh does not self-intersect and is always the boundary of a volume.","Notably, our representation does not rely on a regular grid, is supervised directly by the target surface alone, and also handles open surfaces with boundaries and/or sharp features.","We demonstrate the efficacy of PoNQ through a learning-based mesh prediction from SDF grids and show that our method surpasses recent state-of-the-art techniques in terms of both surface and edge-based metrics."],"url":"http://arxiv.org/abs/2403.12870v1","category":"cs.CV"}
{"created":"2024-03-19 16:01:25","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning","abstract":"In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.","sentences":["In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance.","However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge.","Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks.","This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles.","We further add a regularization term for adding inductive bias during training.","In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance."],"url":"http://arxiv.org/abs/2403.12856v1","category":"cs.LG"}
{"created":"2024-03-19 15:32:23","title":"Semilinear elliptic degenerate equations with critical exponent","abstract":"In this paper we are mainly concerned with nontrivial positive solutions to the Dirichlet problem for the degenerate elliptic equation   \\begin{gather}   -\\frac{\\partial^2 u}{\\partial x^2} -\\left|x\\right|^{2k}\\frac{\\partial^2 u}{\\partial y^2}=|x|^{2k}u^p+f(x,y,u) \\quad\\text{ in }\\Omega, \\ u=0 \\quad\\text{ on }\\partial\\Omega,\\label{equ0}   \\end{gather} where $\\Omega$ is a bounded domain with smooth boundary in $\\mathbb{R}^2, \\Omega \\cap \\{x=0\\}\\ne \\emptyset,$ $k\\in\\mathbb N,$ $f(x,y,0)=0,$ and $p=(4+5k)/k$ is the critical exponent. Recently, the equation (1) was investigated in [12] for the subcritical case based on a new result obtained in [17] on embedding theorem of weighted Sobolev spaces. In the critical case considered in this paper we will essentially use the optimal functions and constants found in [17]","sentences":["In this paper we are mainly concerned with nontrivial positive solutions to the Dirichlet problem for the degenerate elliptic equation   \\begin{gather}   -\\frac{\\partial^2 u}{\\partial x^2} -\\left|x\\right|^{2k}\\frac{\\partial^2 u}{\\partial y^2}=|x|^{2k}u^p+f(x,y,u) \\quad\\text{ in }\\Omega, \\ u=0 \\quad\\text{ on }\\partial\\Omega,\\label{equ0}   \\end{gather} where $\\Omega$ is a bounded domain with smooth boundary in $\\mathbb{R}^2, \\Omega \\cap \\{x=0\\}\\ne \\emptyset,$","$k\\in\\mathbb N,$ $f(x,y,0)=0,$ and $p=(4+5k)/k$ is the critical exponent.","Recently, the equation (1) was investigated in [12] for the subcritical case based on a new result obtained in [17] on embedding theorem of weighted Sobolev spaces.","In the critical case considered in this paper we will essentially use the optimal functions and constants found in [17]"],"url":"http://arxiv.org/abs/2403.12828v1","category":"math.AP"}
{"created":"2024-03-19 15:31:24","title":"Cross Algorithms for Cost-Effective Time Integration of Nonlinear Tensor Differential Equations on Low-Rank Tucker Tensor and Tensor Train Manifolds","abstract":"Dynamical low-rank approximation (DLRA) provides a rigorous, cost-effective mathematical framework for solving high-dimensional tensor differential equations (TDEs) on low-rank tensor manifolds. Despite their effectiveness, DLRA-based low-rank approximations lose their computational efficiency when applied to nonlinear TDEs, particularly those exhibiting non-polynomial nonlinearity. In this paper, we present a novel algorithm for the time integration of TDEs on the tensor train and Tucker tensor low-rank manifolds, which are the building blocks of many tensor network decompositions. This paper builds on our previous work (Donello et al., Proceedings of the Royal Society A, Vol. 479, 2023) on solving nonlinear matrix differential equations on low-rank matrix manifolds using CUR decompositions. The methodology we present offers multiple advantages: (i) it leverages cross algorithms based on the discrete empirical interpolation method to strategically sample sparse entries of the time-discrete TDEs to advance the solution in low-rank form. As a result, it offers near-optimal computational savings both in terms of memory and floating-point operations. (ii) The time integration is robust in the presence of small or zero singular values. (iii) The algorithm is remarkably easy to implement, as it requires the evaluation of the full-order model TDE at strategically selected entries and it does not use tangent space projections, whose efficient implementation is intrusive and time-consuming. (iv) We develop high-order explicit Runge-Kutta schemes for the time integration of TDEs on low-rank manifolds. We demonstrate the efficiency of the presented algorithm for several test cases, including a 100-dimensional TDE with non-polynomial nonlinearity.","sentences":["Dynamical low-rank approximation (DLRA) provides a rigorous, cost-effective mathematical framework for solving high-dimensional tensor differential equations (TDEs) on low-rank tensor manifolds.","Despite their effectiveness, DLRA-based low-rank approximations lose their computational efficiency when applied to nonlinear TDEs, particularly those exhibiting non-polynomial nonlinearity.","In this paper, we present a novel algorithm for the time integration of TDEs on the tensor train and Tucker tensor low-rank manifolds, which are the building blocks of many tensor network decompositions.","This paper builds on our previous work (Donello et al., Proceedings of the Royal Society A, Vol. 479, 2023) on solving nonlinear matrix differential equations on low-rank matrix manifolds using CUR decompositions.","The methodology we present offers multiple advantages: (i) it leverages cross algorithms based on the discrete empirical interpolation method to strategically sample sparse entries of the time-discrete TDEs to advance the solution in low-rank form.","As a result, it offers near-optimal computational savings both in terms of memory and floating-point operations.","(ii) The time integration is robust in the presence of small or zero singular values.","(iii) The algorithm is remarkably easy to implement, as it requires the evaluation of the full-order model TDE at strategically selected entries and it does not use tangent space projections, whose efficient implementation is intrusive and time-consuming.","(iv) We develop high-order explicit Runge-Kutta schemes for the time integration of TDEs on low-rank manifolds.","We demonstrate the efficiency of the presented algorithm for several test cases, including a 100-dimensional TDE with non-polynomial nonlinearity."],"url":"http://arxiv.org/abs/2403.12826v1","category":"math.NA"}
{"created":"2024-03-19 15:28:27","title":"Well-posedness and no-uniform dependence for the Euler-Poincar\u00e9 equations in Triebel-Lizorkin spaces","abstract":"In this paper, we study the Cauchy problem of the Euler-Poincar\\'{e} equations in $\\R^d$ with initial data belonging to the Triebel-Lizorkin spaces. We prove the local-in-time unique existence of solutions to the Euler-Poincar\\'{e} equations in $F^s_{p,r}(\\R^d)$. Furthermore, we obtain that the data-to-solution of this equation is continuous but not uniformly continuous in these spaces.","sentences":["In this paper, we study the Cauchy problem of the Euler-Poincar\\'{e} equations in $\\R^d$ with initial data belonging to the Triebel-Lizorkin spaces.","We prove the local-in-time unique existence of solutions to the Euler-Poincar\\'{e} equations in $F^s_{p,r}(\\R^d)$.","Furthermore, we obtain that the data-to-solution of this equation is continuous but not uniformly continuous in these spaces."],"url":"http://arxiv.org/abs/2403.12824v1","category":"math.AP"}
{"created":"2024-03-19 14:30:13","title":"A smoothing effect for the fractional Schr\u00f6dinger equations on the circle and observability","abstract":"We show that, after a renormalisation, one can define the square of the modulus of the solution of the fractional Schr\\\"odinger equations on the circle with data in Sobolev spaces of arbitrary negative index. As an application, we obtain observability estimates with rough controls.","sentences":["We show that, after a renormalisation, one can define the square of the modulus of the solution of the fractional Schr\\\"odinger equations on the circle with data in Sobolev spaces of arbitrary negative index.","As an application, we obtain observability estimates with rough controls."],"url":"http://arxiv.org/abs/2403.12763v1","category":"math.AP"}
{"created":"2024-03-19 12:57:29","title":"A second-order iterative time integration scheme for linear poroelasticity","abstract":"We propose a novel time stepping method for linear poroelasticity by extending a recent iterative decoupling approach to the second-order case. This results in a two-step scheme with an inner iteration and a relaxation step. We prove second-order convergence for a prescribed number of inner iteration steps, only depending on the coupling strength of the elastic and the flow equation. The efficiency of the scheme is illustrated by a number of numerical experiments, including a simulation of three-dimensional brain tissue.","sentences":["We propose a novel time stepping method for linear poroelasticity by extending a recent iterative decoupling approach to the second-order case.","This results in a two-step scheme with an inner iteration and a relaxation step.","We prove second-order convergence for a prescribed number of inner iteration steps, only depending on the coupling strength of the elastic and the flow equation.","The efficiency of the scheme is illustrated by a number of numerical experiments, including a simulation of three-dimensional brain tissue."],"url":"http://arxiv.org/abs/2403.12699v1","category":"math.NA"}
{"created":"2024-03-19 12:36:51","title":"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single image and a NeRF model","abstract":"We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation. IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model. From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis. The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle. We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image. Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess.","sentences":["We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera pose of a given image, building on the Neural Radiance Fields (NeRF) formulation.","IFFNeRF is specifically designed to operate in real-time and eliminates the need for an initial pose guess that is proximate to the sought solution.","IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface points from within the NeRF model.","From these sampled points, we cast rays and deduce the color for each ray through pixel-level view synthesis.","The camera pose can then be estimated as the solution to a Least Squares problem by selecting correspondences between the query image and the resulting bundle.","We facilitate this process through a learned attention mechanism, bridging the query image embedding with the embedding of parameterized rays, thereby matching rays pertinent to the image.","Through synthetic and real evaluation settings, we show that our method can improve the angular and translation error accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing at 34fps on consumer hardware and not requiring the initial pose guess."],"url":"http://arxiv.org/abs/2403.12682v1","category":"cs.CV"}
{"created":"2024-03-19 11:50:36","title":"Reflected Brownian Motion in a wedge: sum-of-exponential absorption probability at the vertex and differential properties","abstract":"We study a Brownian motion with drift in a wedge of angle $\\beta$ which is obliquely reflected on each edge along angles $\\varepsilon$ and $\\delta$. We assume that the classical parameter $\\alpha=\\frac{\\delta+\\varepsilon - \\pi}{\\beta}$ is greater than $1$ and we focus on transient cases where the process can either be absorbed at the vertex or escape to infinity. We show that $\\alpha\\in\\mathbb{N}^*$ is a necessary and sufficient condition for the absorption probability, seen as a function of the starting point, to be written as a finite sum of terms of exponential product form. In such cases, we give expressions for the absorption probability and its Laplace transform. When $\\alpha\\in\\mathbb{Z}+\\frac{\\pi}{\\beta}\\mathbb{Z}$ we find explicit D-algebraic expression for the Laplace transform. Our results rely on Tutte's invariant method and on a recursive compensation approach.","sentences":["We study a Brownian motion with drift in a wedge of angle $\\beta$ which is obliquely reflected on each edge along angles $\\varepsilon$ and $\\delta$.","We assume that the classical parameter $\\alpha=\\frac{\\delta+\\varepsilon - \\pi}{\\beta}$ is greater than $1$ and we focus on transient cases where the process can either be absorbed at the vertex or escape to infinity.","We show that $\\alpha\\in\\mathbb{N}^*$ is a necessary and sufficient condition for the absorption probability, seen as a function of the starting point, to be written as a finite sum of terms of exponential product form.","In such cases, we give expressions for the absorption probability and its Laplace transform.","When $\\alpha\\in\\mathbb{Z}+\\frac{\\pi}{\\beta}\\mathbb{Z}$ we find explicit D-algebraic expression for the Laplace transform.","Our results rely on Tutte's invariant method and on a recursive compensation approach."],"url":"http://arxiv.org/abs/2403.12661v1","category":"math.PR"}
{"created":"2024-03-19 11:35:59","title":"Mean-field derivation of Landau-like equations","abstract":"We derive a class of space homogeneous Landau-like equations from stochastic interacting particles. Through the use of relative entropy, we obtain quantitative bounds on the distance between the solution of the N-particle Liouville equation and the tensorised solution of the limiting Landau-like equation.","sentences":["We derive a class of space homogeneous Landau-like equations from stochastic interacting particles.","Through the use of relative entropy, we obtain quantitative bounds on the distance between the solution of the N-particle Liouville equation and the tensorised solution of the limiting Landau-like equation."],"url":"http://arxiv.org/abs/2403.12651v1","category":"math.AP"}
{"created":"2024-03-19 10:26:01","title":"Fixing a Minor Mistake in the Theory of Stochastic Integration and Differential Equations","abstract":"When considering stochastic integration and the theory of stochastic differential equations, P. Protter's textbook \\cite{protter} undoubtedly is a main piece of standard literature. Not only is it well-written, but it also contains various profound results regarding these fields. Unfortunately, Theorem 12 of Chapter V, which presents an equivalence to uniform convergence on compacts in probability, is found to be incorrect. Given that numerous important results rely on this theorem, this paper aims to present a corrected version of it.","sentences":["When considering stochastic integration and the theory of stochastic differential equations, P. Protter's textbook \\cite{protter} undoubtedly is a main piece of standard literature.","Not only is it well-written, but it also contains various profound results regarding these fields.","Unfortunately, Theorem 12 of Chapter V, which presents an equivalence to uniform convergence on compacts in probability, is found to be incorrect.","Given that numerous important results rely on this theorem, this paper aims to present a corrected version of it."],"url":"http://arxiv.org/abs/2403.12613v1","category":"math.PR"}
{"created":"2024-03-19 08:52:10","title":"Non-coercive Neumann boundary control problems","abstract":"The article examines a linear-quadratic Neumann control problem that is governed by a non-coercive elliptic equation. Due to the non-self-adjoint nature of the linear control-to-state operator, it is necessary to independently study both the state and adjoint state equations. The article establishes the existence and uniqueness of solutions for both equations, with minimal assumptions made about the problem's data. Next, the regularity of these solutions is studied in three frameworks: Hilbert-Sobolev spaces, Sobolev-Slobodecki\\u\\i{} spaces, and weighted Sobolev spaces. These regularity results enable a numerical analysis of the finite element approximation of both the state and adjoint state equations. The results cover both convex and non-convex domains and quasi-uniform and graded meshes. Finally, the optimal control problem is analyzed and discretized. Existence and uniqueness of the solution, first-order optimality conditions, and error estimates for the finite element approximation of the control are obtained. Numerical experiments confirming these results are included. A significant highlight is that the discretization error estimates known from the literature, are improved even for the coercive case.","sentences":["The article examines a linear-quadratic Neumann control problem that is governed by a non-coercive elliptic equation.","Due to the non-self-adjoint nature of the linear control-to-state operator, it is necessary to independently study both the state and adjoint state equations.","The article establishes the existence and uniqueness of solutions for both equations, with minimal assumptions made about the problem's data.","Next, the regularity of these solutions is studied in three frameworks: Hilbert-Sobolev spaces, Sobolev-Slobodecki\\u\\i{} spaces, and weighted Sobolev spaces.","These regularity results enable a numerical analysis of the finite element approximation of both the state and adjoint state equations.","The results cover both convex and non-convex domains and quasi-uniform and graded meshes.","Finally, the optimal control problem is analyzed and discretized.","Existence and uniqueness of the solution, first-order optimality conditions, and error estimates for the finite element approximation of the control are obtained.","Numerical experiments confirming these results are included.","A significant highlight is that the discretization error estimates known from the literature, are improved even for the coercive case."],"url":"http://arxiv.org/abs/2403.12551v1","category":"math.OC"}
{"created":"2024-03-19 07:46:03","title":"Hybrid star within $f(\\mathcal{G})$ gravity","abstract":"The purpose of this work is to investigate some interesting features of a static anisotropic relativistic stellar object composed of two different types of fluid distributions typically termed as quark matter (QM) and ordinary baryonic matter (OBM) together with Krori-Barua type (KB) {\\em ansatz} in the regime of modified $f(\\mathcal{G})$ gravity, where $\\mathcal{G}$ being the Gauss-Bonnet invariant term. In order to explain the correlation between pressure and matter density for the quark matter distribution within the compact object, we have taken into consideration the well-known MIT bag equation of state (EoS) whereas there is a simple linear correlation between pressure and matter density for ordinary baryonic matter. Furthermore, using graphical representations for varying parameters, the physical credibility of our obtained solutions has been intensively examined by regularity checking of the metric coefficients and matter variables, energy conditions, mass function, and causality conditions. For these analyses, we consider a particular compact stellar candidate 4U 1538-52. Finally, we found that the resulting outcome depicts the viability of the considered hybrid stellar model.","sentences":["The purpose of this work is to investigate some interesting features of a static anisotropic relativistic stellar object composed of two different types of fluid distributions typically termed as quark matter (QM) and ordinary baryonic matter (OBM) together with Krori-Barua type (KB) {\\em ansatz} in the regime of modified $f(\\mathcal{G})$ gravity, where $\\mathcal{G}$ being the Gauss-Bonnet invariant term.","In order to explain the correlation between pressure and matter density for the quark matter distribution within the compact object, we have taken into consideration the well-known MIT bag equation of state (EoS) whereas there is a simple linear correlation between pressure and matter density for ordinary baryonic matter.","Furthermore, using graphical representations for varying parameters, the physical credibility of our obtained solutions has been intensively examined by regularity checking of the metric coefficients and matter variables, energy conditions, mass function, and causality conditions.","For these analyses, we consider a particular compact stellar candidate 4U 1538-52.","Finally, we found that the resulting outcome depicts the viability of the considered hybrid stellar model."],"url":"http://arxiv.org/abs/2403.12522v1","category":"gr-qc"}
{"created":"2024-03-19 07:25:36","title":"Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training","abstract":"Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forward Gradient, fails to converge to the optimal solution. We demonstrate the convergence attributes of our proposed algorithms using a numerical example.","sentences":["Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level.","However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients.","This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients.","We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient.","In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forward Gradient, fails to converge to the optimal solution.","We demonstrate the convergence attributes of our proposed algorithms using a numerical example."],"url":"http://arxiv.org/abs/2403.12511v1","category":"cs.LG"}
{"created":"2024-03-19 07:08:45","title":"Multilevel Markov Chain Monte Carlo for Bayesian inverse problems for Navier Stokes equation with Lagrangian Observations","abstract":"In this paper, we extend our work to the Bayesian inverse problems for inferring unknown forcing and initial condition of the forward Navier-Stokes equation coupled with tracer equation with noisy Lagrangian observation on the positions of the tracers. We consider the Navier-Stokes equations in the two dimensional periodic torus with a tracer equation which is a simple ordinary differential equation. We developed rigorously the theory for the case of the uniform prior where the forcing and the initial condition depend linearly on a countable set of random variables which are uniformly distributed in a compact interval. Numerical experiment using the MLMCMC method produces approximations for posterior expectation of quantities of interest which are in agreement with the theoretical optimal convergence rate established.","sentences":["In this paper, we extend our work to the Bayesian inverse problems for inferring unknown forcing and initial condition of the forward Navier-Stokes equation coupled with tracer equation with noisy Lagrangian observation on the positions of the tracers.","We consider the Navier-Stokes equations in the two dimensional periodic torus with a tracer equation which is a simple ordinary differential equation.","We developed rigorously the theory for the case of the uniform prior where the forcing and the initial condition depend linearly on a countable set of random variables which are uniformly distributed in a compact interval.","Numerical experiment using the MLMCMC method produces approximations for posterior expectation of quantities of interest which are in agreement with the theoretical optimal convergence rate established."],"url":"http://arxiv.org/abs/2403.12501v1","category":"math.NA"}
{"created":"2024-03-19 06:41:33","title":"Thermoelectric properties of marcasite-type compounds MSb$_2$ (M = Ta, Nb): A combined experimental and computational study","abstract":"Here, we investigate the thermoelectric properties of the marcasite-type compounds MSb$_2$ (M = Ta, Nb) in the temperature range of 310-730 K. These compounds were synthesized by a solid-state reaction followed by the spark plasma sintering process. The Rietveld refinement method confirms the monoclinic phase with space group C2/m for both compounds. The observed values of Seebeck coefficients exhibit non-monotonic behaviour in the studied temperature range, with the maximum magnitude of -14.4 and -22.7 $\\mu$V K$^{-1}$ for TaSb$_2$ and NbSb$_2$, respectively at ~444 K. The negative sign of S in the full temperature window signifies the n-type behaviour of these compounds. Both electrical and thermal conductivities show an increasing trend with temperature. The experimentally observed thermoelectric properties are understood through the first-principles DFT and Boltzmann transport equation. A pseudogap in the density of states around the Fermi level characterizes the semimetallic behaviour of these compounds. The multi-band electron and hole pockets were found to be mainly responsible for the temperature dependence of transport properties. The experimental power factors are found to be ~0.09 and ~0.42 mW m$^{-1}$ K$^{-2}$ at 310 K for TaSb2 and NbSb2, respectively. From the DFT-based calculations, the maximum possible power factors for p-type conduction are predicted as ~1.14 and ~1.74 mW m$^{-1}$ K$^{-2}$, while these values are found to be ~1.16 and ~1.80 mW m$^{-1}$ K$^{-2}$ for n-type TaSb$_2$ and NbSb$_2$, respectively at 300 K with the corresponding doping concentrations. The present study suggests that the combined DFT and Boltzmann transport theory are found to be reasonably good at explaining the experimental transport properties, and moderate power factors are predicted.","sentences":["Here, we investigate the thermoelectric properties of the marcasite-type compounds MSb$_2$ (M = Ta, Nb) in the temperature range of 310-730 K. These compounds were synthesized by a solid-state reaction followed by the spark plasma sintering process.","The Rietveld refinement method confirms the monoclinic phase with space group C2/m for both compounds.","The observed values of Seebeck coefficients exhibit non-monotonic behaviour in the studied temperature range, with the maximum magnitude of -14.4 and -22.7 $\\mu$V K$^{-1}$ for TaSb$_2$ and NbSb$_2$, respectively at ~444 K. The negative sign of S in the full temperature window signifies the n-type behaviour of these compounds.","Both electrical and thermal conductivities show an increasing trend with temperature.","The experimentally observed thermoelectric properties are understood through the first-principles DFT and Boltzmann transport equation.","A pseudogap in the density of states around the Fermi level characterizes the semimetallic behaviour of these compounds.","The multi-band electron and hole pockets were found to be mainly responsible for the temperature dependence of transport properties.","The experimental power factors are found to be ~0.09 and ~0.42 mW m$^{-1}$ K$^{-2}$ at 310 K for TaSb2 and NbSb2, respectively.","From the DFT-based calculations, the maximum possible power factors for p-type conduction are predicted as ~1.14 and ~1.74 mW m$^{-1}$ K$^{-2}$, while these values are found to be ~1.16 and ~1.80 mW m$^{-1}$ K$^{-2}$ for n-type TaSb$_2$ and NbSb$_2$, respectively at 300 K with the corresponding doping concentrations.","The present study suggests that the combined DFT and Boltzmann transport theory are found to be reasonably good at explaining the experimental transport properties, and moderate power factors are predicted."],"url":"http://arxiv.org/abs/2403.12484v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 06:40:06","title":"A Hybrid Transformer-Sequencer approach for Age and Gender classification from in-wild facial images","abstract":"The advancements in computer vision and image processing techniques have led to emergence of new application in the domain of visual surveillance, targeted advertisement, content-based searching, and human-computer interaction etc. Out of the various techniques in computer vision, face analysis, in particular, has gained much attention. Several previous studies have tried to explore different applications of facial feature processing for a variety of tasks, including age and gender classification. However, despite several previous studies having explored the problem, the age and gender classification of in-wild human faces is still far from the achieving the desired levels of accuracy required for real-world applications. This paper, therefore, attempts to bridge this gap by proposing a hybrid model that combines self-attention and BiLSTM approaches for age and gender classification problems. The proposed models performance is compared with several state-of-the-art model proposed so far. An improvement of approximately 10percent and 6percent over the state-of-the-art implementations for age and gender classification, respectively, are noted for the proposed model. The proposed model is thus found to achieve superior performance and is found to provide a more generalized learning. The model can, therefore, be applied as a core classification component in various image processing and computer vision problems.","sentences":["The advancements in computer vision and image processing techniques have led to emergence of new application in the domain of visual surveillance, targeted advertisement, content-based searching, and human-computer interaction etc.","Out of the various techniques in computer vision, face analysis, in particular, has gained much attention.","Several previous studies have tried to explore different applications of facial feature processing for a variety of tasks, including age and gender classification.","However, despite several previous studies having explored the problem, the age and gender classification of in-wild human faces is still far from the achieving the desired levels of accuracy required for real-world applications.","This paper, therefore, attempts to bridge this gap by proposing a hybrid model that combines self-attention and BiLSTM approaches for age and gender classification problems.","The proposed models performance is compared with several state-of-the-art model proposed so far.","An improvement of approximately 10percent and 6percent over the state-of-the-art implementations for age and gender classification, respectively, are noted for the proposed model.","The proposed model is thus found to achieve superior performance and is found to provide a more generalized learning.","The model can, therefore, be applied as a core classification component in various image processing and computer vision problems."],"url":"http://arxiv.org/abs/2403.12483v1","category":"cs.CV"}
{"created":"2024-03-19 06:33:08","title":"Contact structures of type $G_2$ associated to solutions of Noth's equation","abstract":"We establish a correspondence between solutions of Noth's equation, a non-linear ordinary differential equation that shows up in the theory of $(2,3,5)$-distributions, and diffeomorphisms of any contact structure of type $G_{2}$ to the standard one.","sentences":["We establish a correspondence between solutions of Noth's equation, a non-linear ordinary differential equation that shows up in the theory of $(2,3,5)$-distributions, and diffeomorphisms of any contact structure of type $G_{2}$ to the standard one."],"url":"http://arxiv.org/abs/2403.12479v1","category":"math.DG"}
{"created":"2024-03-19 06:09:55","title":"Determinants of pseudo-laplacians and $\u03b6^{({\\rm reg})}(1)$ for spinor bundles over Riemann surfaces","abstract":"Let $P$ be a point of a Riemann surface $X$. We study self-adjoint extensions of Dolbeault Laplacians in hermitian line bundles $L$ over $X$ initially defined on sections with compact supports in $X\\backslash\\{P\\}$. We define the $\\zeta$-regularized determinants for these operators and derive the comparison formulas for them. We introduce the notion of the Robin mass of $L$. This quantity enters the comparison formulas for determinants and is related to the regularized $\\zeta(1)$ for the Dolbeault Laplacian. For spinor bundles of even characteristic, we find the explicit expressions for the Robin mass.","sentences":["Let $P$ be a point of a Riemann surface $X$. We study self-adjoint extensions of Dolbeault Laplacians in hermitian line bundles $L$ over $X$ initially defined on sections with compact supports in $X\\backslash\\{P\\}$.","We define the $\\zeta$-regularized determinants for these operators and derive the comparison formulas for them.","We introduce the notion of the Robin mass of $L$. This quantity enters the comparison formulas for determinants and is related to the regularized $\\zeta(1)$ for the Dolbeault Laplacian.","For spinor bundles of even characteristic, we find the explicit expressions for the Robin mass."],"url":"http://arxiv.org/abs/2403.12472v1","category":"math.SP"}
{"created":"2024-03-19 05:07:54","title":"Weakly nonlinear analysis of Turing pattern dynamics on curved surfaces","abstract":"Pattern dynamics on curved surfaces are ubiquitous. Although the effect of surface topography on pattern dynamics has gained much interest, there is a limited understanding of the roles of surface geometry and topology in pattern dynamics. Recently, we reported that a static pattern on a flat plane can become a propagating pattern on a curved surface [Nishide and Ishihara, Phys. Rev. Lett. 2022]. By examining reaction-diffusion equations on axisymmetric surfaces, certain conditions for the onset of pattern propagation were determined. However, this analysis was limited by the assumption that the pattern propagates at a constant speed. Here, we investigate the pattern propagation driven by surface curvature using weakly nonlinear analysis, which enables a more comprehensive approach to the aforementioned problem. The analysis reveals consistent conditions of the pattern propagation similar to our previous results, and further predicts that rich dynamics other than pattern propagation, such as periodic and chaotic behaviors, can arise depending on the surface geometry. This study provides a new perspective on the relationship between surfaces and pattern dynamics and a basis for controlling pattern dynamics on surfaces.","sentences":["Pattern dynamics on curved surfaces are ubiquitous.","Although the effect of surface topography on pattern dynamics has gained much interest, there is a limited understanding of the roles of surface geometry and topology in pattern dynamics.","Recently, we reported that a static pattern on a flat plane can become a propagating pattern on a curved surface","[Nishide and Ishihara, Phys. Rev. Lett. 2022].","By examining reaction-diffusion equations on axisymmetric surfaces, certain conditions for the onset of pattern propagation were determined.","However, this analysis was limited by the assumption that the pattern propagates at a constant speed.","Here, we investigate the pattern propagation driven by surface curvature using weakly nonlinear analysis, which enables a more comprehensive approach to the aforementioned problem.","The analysis reveals consistent conditions of the pattern propagation similar to our previous results, and further predicts that rich dynamics other than pattern propagation, such as periodic and chaotic behaviors, can arise depending on the surface geometry.","This study provides a new perspective on the relationship between surfaces and pattern dynamics and a basis for controlling pattern dynamics on surfaces."],"url":"http://arxiv.org/abs/2403.12444v1","category":"nlin.PS"}
{"created":"2024-03-19 04:51:38","title":"Precise-Physics Driven Text-to-3D Generation","abstract":"Text-to-3D generation has shown great promise in generating novel 3D content based on given text prompts. However, existing generative methods mostly focus on geometric or visual plausibility while ignoring precise physics perception for the generated 3D shapes. This greatly hinders the practicality of generated 3D shapes in real-world applications. In this work, we propose Phy3DGen, a precise-physics-driven text-to-3D generation method. By analyzing the solid mechanics of generated 3D shapes, we reveal that the 3D shapes generated by existing text-to-3D generation methods are impractical for real-world applications as the generated 3D shapes do not conform to the laws of physics. To this end, we leverage 3D diffusion models to provide 3D shape priors and design a data-driven differentiable physics layer to optimize 3D shape priors with solid mechanics. This allows us to optimize geometry efficiently and learn precise physics information about 3D shapes at the same time. Experimental results demonstrate that our method can consider both geometric plausibility and precise physics perception, further bridging 3D virtual modeling and precise physical worlds.","sentences":["Text-to-3D generation has shown great promise in generating novel 3D content based on given text prompts.","However, existing generative methods mostly focus on geometric or visual plausibility while ignoring precise physics perception for the generated 3D shapes.","This greatly hinders the practicality of generated 3D shapes in real-world applications.","In this work, we propose Phy3DGen, a precise-physics-driven text-to-3D generation method.","By analyzing the solid mechanics of generated 3D shapes, we reveal that the 3D shapes generated by existing text-to-3D generation methods are impractical for real-world applications as the generated 3D shapes do not conform to the laws of physics.","To this end, we leverage 3D diffusion models to provide 3D shape priors and design a data-driven differentiable physics layer to optimize 3D shape priors with solid mechanics.","This allows us to optimize geometry efficiently and learn precise physics information about 3D shapes at the same time.","Experimental results demonstrate that our method can consider both geometric plausibility and precise physics perception, further bridging 3D virtual modeling and precise physical worlds."],"url":"http://arxiv.org/abs/2403.12438v1","category":"cs.CV"}
{"created":"2024-03-19 04:29:50","title":"Usefulness of signed eigenvalue/vector distributions of random tensors","abstract":"Quantum field theories can be applied to compute various statistical properties of random tensors. In particular signed distributions of tensor eigenvalues/vectors are the easiest, which can be computed as partition functions of four-fermi theories. Though signed distributions are different from genuine ones because of extra signs of weights, they are expected to coincide in vicinities of ends of distributions. In this paper, we perform a case study of the signed eigenvalue/vector distribution of the real symmetric order-three random tensor. The correct critical point and the correct end in the large $N$ limit have been obtained from the four-fermi theory, for which a method using the Schwinger-Dyson equation is very efficient. Since locations of ends are particularly important in applications, such as the largest eigenvalues and the best rank-one tensor approximations, signed distributions are the easiest and highly useful through the Schwinger-Dyson method.","sentences":["Quantum field theories can be applied to compute various statistical properties of random tensors.","In particular signed distributions of tensor eigenvalues/vectors are the easiest, which can be computed as partition functions of four-fermi theories.","Though signed distributions are different from genuine ones because of extra signs of weights, they are expected to coincide in vicinities of ends of distributions.","In this paper, we perform a case study of the signed eigenvalue/vector distribution of the real symmetric order-three random tensor.","The correct critical point and the correct end in the large $N$ limit have been obtained from the four-fermi theory, for which a method using the Schwinger-Dyson equation is very efficient.","Since locations of ends are particularly important in applications, such as the largest eigenvalues and the best rank-one tensor approximations, signed distributions are the easiest and highly useful through the Schwinger-Dyson method."],"url":"http://arxiv.org/abs/2403.12427v1","category":"hep-th"}
{"created":"2024-03-19 03:06:57","title":"Calculus of variations on hypergraphs","abstract":"We have established a coherent framework for applying variational methods to partial differential equations on hypergraphs, which includes the propositions of calculus and function spaces on hypergraphs. Several results related to the maximum principle on hypergraphs have also been proven. As applications, we demonstrated how these can be used to study partial differential equations on hypergraphs.","sentences":["We have established a coherent framework for applying variational methods to partial differential equations on hypergraphs, which includes the propositions of calculus and function spaces on hypergraphs.","Several results related to the maximum principle on hypergraphs have also been proven.","As applications, we demonstrated how these can be used to study partial differential equations on hypergraphs."],"url":"http://arxiv.org/abs/2403.12394v1","category":"math.AP"}
{"created":"2024-03-19 02:57:10","title":"Learning-guided iterated local search for the minmax multiple traveling salesman problem","abstract":"The minmax multiple traveling salesman problem involves minimizing the longest tour among a set of tours. The problem is of great practical interest because it can be used to formulate several real-life applications. To solve this computationally challenging problem, we propose a leaning-driven iterated local search approach that combines an aggressive local search procedure with a probabilistic acceptance criterion to find high-quality local optimal solutions and a multi-armed bandit algorithm to select various removal and insertion operators to escape local optimal traps. Extensive experiments on 77 commonly used benchmark instances show that our algorithm achieves excellent results in terms of solution quality and running time. In particular, it achieves 32 new best-known results and matches the best-known results for 35 other instances. Additional experiments shed light on the understanding of the composing elements of the algorithm.","sentences":["The minmax multiple traveling salesman problem involves minimizing the longest tour among a set of tours.","The problem is of great practical interest because it can be used to formulate several real-life applications.","To solve this computationally challenging problem, we propose a leaning-driven iterated local search approach that combines an aggressive local search procedure with a probabilistic acceptance criterion to find high-quality local optimal solutions and a multi-armed bandit algorithm to select various removal and insertion operators to escape local optimal traps.","Extensive experiments on 77 commonly used benchmark instances show that our algorithm achieves excellent results in terms of solution quality and running time.","In particular, it achieves 32 new best-known results and matches the best-known results for 35 other instances.","Additional experiments shed light on the understanding of the composing elements of the algorithm."],"url":"http://arxiv.org/abs/2403.12389v1","category":"cs.NE"}
{"created":"2024-03-19 02:52:06","title":"VideoBadminton: A Video Dataset for Badminton Action Recognition","abstract":"In the dynamic and evolving field of computer vision, action recognition has become a key focus, especially with the advent of sophisticated methodologies like Convolutional Neural Networks (CNNs), Convolutional 3D, Transformer, and spatial-temporal feature fusion. These technologies have shown promising results on well-established benchmarks but face unique challenges in real-world applications, particularly in sports analysis, where the precise decomposition of activities and the distinction of subtly different actions are crucial. Existing datasets like UCF101, HMDB51, and Kinetics have offered a diverse range of video data for various scenarios. However, there's an increasing need for fine-grained video datasets that capture detailed categorizations and nuances within broader action categories. In this paper, we introduce the VideoBadminton dataset derived from high-quality badminton footage. Through an exhaustive evaluation of leading methodologies on this dataset, this study aims to advance the field of action recognition, particularly in badminton sports. The introduction of VideoBadminton could not only serve for badminton action recognition but also provide a dataset for recognizing fine-grained actions. The insights gained from these evaluations are expected to catalyze further research in action comprehension, especially within sports contexts.","sentences":["In the dynamic and evolving field of computer vision, action recognition has become a key focus, especially with the advent of sophisticated methodologies like Convolutional Neural Networks (CNNs), Convolutional 3D, Transformer, and spatial-temporal feature fusion.","These technologies have shown promising results on well-established benchmarks but face unique challenges in real-world applications, particularly in sports analysis, where the precise decomposition of activities and the distinction of subtly different actions are crucial.","Existing datasets like UCF101, HMDB51, and Kinetics have offered a diverse range of video data for various scenarios.","However, there's an increasing need for fine-grained video datasets that capture detailed categorizations and nuances within broader action categories.","In this paper, we introduce the VideoBadminton dataset derived from high-quality badminton footage.","Through an exhaustive evaluation of leading methodologies on this dataset, this study aims to advance the field of action recognition, particularly in badminton sports.","The introduction of VideoBadminton could not only serve for badminton action recognition but also provide a dataset for recognizing fine-grained actions.","The insights gained from these evaluations are expected to catalyze further research in action comprehension, especially within sports contexts."],"url":"http://arxiv.org/abs/2403.12385v1","category":"cs.CV"}
{"created":"2024-03-19 02:47:58","title":"New regularity criteria for NSE and SQG in critical spaces","abstract":"In this paper, we investigate some priori estimates to provide the critical regularity criteria for incompressible   Navier-Stokes equations on $\\mathbb{R}^3$ and super critical surface quasi-geostrophic equations on $\\mathbb{R}^2$. Concerning the Navier-Stokes equation, we demonstrate that a Leray-Hopf solution $u$ is regular if $u\\in L_T^{\\frac{2}{1-\\alpha}} \\dot{B}^{-\\alpha}_{\\infty,\\infty}(\\mathbb{R}^3)$, or $u$ in Lorentz space $ L_T^{p,r} \\dot{B}^{-1+\\frac{2}{p}}_{\\infty,\\infty}(\\mathbb{R}^3)$, with $4\\leq p\\leq r<\\infty$. Additionally, an alternative regularity condition is expressed as $u\\in L_{T}^{\\frac{2}{1-\\alpha}}   \\dot{B}^{-\\alpha}_{\\infty,\\infty}(\\mathbb{R}^3)+{L_T^\\infty\\dot{B}^{-1}_{\\infty,\\infty}}(\\mathbb{R}^3)$($\\alpha\\in(0,1)$), contingent upon a smallness assumption on the norm $L_T^\\infty\\dot{B}^{-1}_{\\infty,\\infty}$. For the SQG equation, we derive that a Leray-Hopf weak solution $\\theta\\in L_T^{\\frac{\\alpha}{\\varepsilon}} \\dot{C}^{1-\\alpha+\\epsilon}(\\mathbb{R}^2)$ is smooth for any $\\varepsilon$ small enough. Similar to the case of Navier-Stokes equation, we derive regularity criterion in more refined spaces, i.e. Lorentz spaces $L_T^{\\frac{\\alpha}{\\epsilon},r}\\dot{C}^{1-\\alpha+\\epsilon}(\\mathbb{R}^2)$ and addition of two critical spaces $L_{T}^{\\frac{\\alpha}{\\epsilon}}\\dot{C}^{1-\\alpha+\\epsilon}(\\mathbb{R}^2)+{L_T^\\infty\\dot{C}^{1-\\alpha}(\\mathbb{R}^2)}$, with smallness assumption on $L_T^\\infty\\dot{C}^{1-\\alpha}(\\mathbb{R}^2)$.","sentences":["In this paper, we investigate some priori estimates to provide the critical regularity criteria for incompressible   Navier-Stokes equations on $\\mathbb{R}^3$ and super critical surface quasi-geostrophic equations on $\\mathbb{R}^2$. Concerning the Navier-Stokes equation, we demonstrate that a Leray-Hopf solution $u$ is regular if $u\\in L_T^{\\frac{2}{1-\\alpha}} \\dot{B}^{-\\alpha}_{\\infty,\\infty}(\\mathbb{R}^3)$, or $u$ in Lorentz space $ L_T^{p,r} \\dot{B}^{-1+\\frac{2}{p}}_{\\infty,\\infty}(\\mathbb{R}^3)$, with $4\\leq p\\leq r<\\infty$. Additionally, an alternative regularity condition is expressed as $u\\in L_{T}^{\\frac{2}{1-\\alpha}}   \\dot{B}^{-\\alpha}_{\\infty,\\infty}(\\mathbb{R}^3)+{L_T^\\infty\\dot{B}^{-1}_{\\infty,\\infty}}(\\mathbb{R}^3)$($\\alpha\\in(0,1)$), contingent upon a smallness assumption on the norm $L_T^\\infty\\dot{B}^{-1}_{\\infty,\\infty}$. For the SQG equation, we derive that a Leray-Hopf weak solution $\\theta\\in L_T^{\\frac{\\alpha}{\\varepsilon}} \\dot{C}^{1-\\alpha+\\epsilon}(\\mathbb{R}^2)$ is smooth for any $\\varepsilon$ small enough.","Similar to the case of Navier-Stokes equation, we derive regularity criterion in more refined spaces, i.e. Lorentz spaces $L_T^{\\frac{\\alpha}{\\epsilon},r}\\dot{C}^{1-\\alpha+\\epsilon}(\\mathbb{R}^2)$ and addition of two critical spaces $L_{T}^{\\frac{\\alpha}{\\epsilon}}\\dot{C}^{1-\\alpha+\\epsilon}(\\mathbb{R}^2)+{L_T^\\infty\\dot{C}^{1-\\alpha}(\\mathbb{R}^2)}$, with smallness assumption on $L_T^\\infty\\dot{C}^{1-\\alpha}(\\mathbb{R}^2)$."],"url":"http://arxiv.org/abs/2403.12383v1","category":"math.AP"}
{"created":"2024-03-19 02:37:15","title":"Gauge Theoretical Method in Solving Zero-curvature Equations I. -- Application to the Static Einstein-Maxwell Equations with Magnetic Charge","abstract":"The inverse scattering problem is applied to 2-dimensional partial differential equations called soliton equations such as the KdV equation and so on. It is also used to integrate the Einstein equations with axial symmetry. These inverse scattering problems look different. We show that they can be understood in a unified way. As an application to the Einstein equation, we find solutions of the Einstein-Maxwell equations with magnetic charge.","sentences":["The inverse scattering problem is applied to 2-dimensional partial differential equations called soliton equations such as the KdV equation and so on.","It is also used to integrate the Einstein equations with axial symmetry.","These inverse scattering problems look different.","We show that they can be understood in a unified way.","As an application to the Einstein equation, we find solutions of the Einstein-Maxwell equations with magnetic charge."],"url":"http://arxiv.org/abs/2403.12375v1","category":"hep-th"}
{"created":"2024-03-19 01:39:33","title":"Friendly Sharpness-Aware Minimization","abstract":"Sharpness-Aware Minimization (SAM) has been instrumental in improving deep neural network training by minimizing both training loss and loss sharpness. Despite the practical success, the mechanisms behind SAM's generalization enhancements remain elusive, limiting its progress in deep learning optimization. In this work, we investigate SAM's core components for generalization improvement and introduce \"Friendly-SAM\" (F-SAM) to further enhance SAM's generalization. Our investigation reveals the key role of batch-specific stochastic gradient noise within the adversarial perturbation, i.e., the current minibatch gradient, which significantly influences SAM's generalization performance. By decomposing the adversarial perturbation in SAM into full gradient and stochastic gradient noise components, we discover that relying solely on the full gradient component degrades generalization while excluding it leads to improved performance. The possible reason lies in the full gradient component's increase in sharpness loss for the entire dataset, creating inconsistencies with the subsequent sharpness minimization step solely on the current minibatch data. Inspired by these insights, F-SAM aims to mitigate the negative effects of the full gradient component. It removes the full gradient estimated by an exponentially moving average (EMA) of historical stochastic gradients, and then leverages stochastic gradient noise for improved generalization. Moreover, we provide theoretical validation for the EMA approximation and prove the convergence of F-SAM on non-convex problems. Extensive experiments demonstrate the superior generalization performance and robustness of F-SAM over vanilla SAM. Code is available at https://github.com/nblt/F-SAM.","sentences":["Sharpness-Aware Minimization (SAM) has been instrumental in improving deep neural network training by minimizing both training loss and loss sharpness.","Despite the practical success, the mechanisms behind SAM's generalization enhancements remain elusive, limiting its progress in deep learning optimization.","In this work, we investigate SAM's core components for generalization improvement and introduce \"Friendly-SAM\" (F-SAM) to further enhance SAM's generalization.","Our investigation reveals the key role of batch-specific stochastic gradient noise within the adversarial perturbation, i.e., the current minibatch gradient, which significantly influences SAM's generalization performance.","By decomposing the adversarial perturbation in SAM into full gradient and stochastic gradient noise components, we discover that relying solely on the full gradient component degrades generalization while excluding it leads to improved performance.","The possible reason lies in the full gradient component's increase in sharpness loss for the entire dataset, creating inconsistencies with the subsequent sharpness minimization step solely on the current minibatch data.","Inspired by these insights, F-SAM aims to mitigate the negative effects of the full gradient component.","It removes the full gradient estimated by an exponentially moving average (EMA) of historical stochastic gradients, and then leverages stochastic gradient noise for improved generalization.","Moreover, we provide theoretical validation for the EMA approximation and prove the convergence of F-SAM on non-convex problems.","Extensive experiments demonstrate the superior generalization performance and robustness of F-SAM over vanilla SAM.","Code is available at https://github.com/nblt/F-SAM."],"url":"http://arxiv.org/abs/2403.12350v1","category":"cs.LG"}
{"created":"2024-03-19 01:15:26","title":"Investigating Millimeter-Wave Thin-film Superconducting Resonators: A Study Using Tunnel Junction Detectors","abstract":"Investigations into the propagation characteristics, specifically loss and wave velocity, of superconducting coplanar waveguides and microstrip lines were conducted at a 2 mm wavelength. This was achieved through the measurement of on-chip half-wavelength resonators, employing superconductor-insulator-superconductor tunnel junctions as detectors. A continuous wave millimeter wave probe signal was introduced to the chip via a silicon membrane-based orthomode transducer. This setup not only facilitated the injection of the probe signal but also provided a reference path essential for differential measurements. The observed resonance frequencies aligned closely with theoretical predictions, exhibiting a discrepancy of only several percent. However, the measured losses significantly exceeded those anticipated from quasi-particle loss mechanisms, suggesting the presence of additional loss factors. Notably, the measurement results revealed that the tangential loss attributable to the dielectric layer, specifically silicon dioxide, was approximately $\\rm{7\\pm 2 \\times 10^{-3}}$. This factor emerged as the dominant contributor to overall loss at temperatures around 4 K.","sentences":["Investigations into the propagation characteristics, specifically loss and wave velocity, of superconducting coplanar waveguides and microstrip lines were conducted at a 2 mm wavelength.","This was achieved through the measurement of on-chip half-wavelength resonators, employing superconductor-insulator-superconductor tunnel junctions as detectors.","A continuous wave millimeter wave probe signal was introduced to the chip via a silicon membrane-based orthomode transducer.","This setup not only facilitated the injection of the probe signal but also provided a reference path essential for differential measurements.","The observed resonance frequencies aligned closely with theoretical predictions, exhibiting a discrepancy of only several percent.","However, the measured losses significantly exceeded those anticipated from quasi-particle loss mechanisms, suggesting the presence of additional loss factors.","Notably, the measurement results revealed that the tangential loss attributable to the dielectric layer, specifically silicon dioxide, was approximately $\\rm{7\\pm 2 \\times 10^{-3}}$.","This factor emerged as the dominant contributor to overall loss at temperatures around 4 K."],"url":"http://arxiv.org/abs/2403.12342v1","category":"astro-ph.IM"}
{"created":"2024-03-19 00:53:51","title":"Attention-Based Neural Network Emulators for Multi-Probe Data Vectors Part II: Assessing Tension Metrics","abstract":"The next generation of cosmological surveys is expected to generate unprecedented high-quality data, consequently increasing the already substantial computational costs of Bayesian statistical methods. This will pose a significant challenge to analyzing theoretical models of cosmology. Additionally, new mitigation techniques of baryonic effects, intrinsic alignment, and other systematic effects will inevitably introduce more parameters, slowing down the convergence of Bayesian analyses. In this scenario, machine-learning-based accelerators are a promising solution, capable of reducing the computational costs and execution time of such tools by order of thousands. Yet, they have not been able to provide accurate predictions over the wide prior ranges in parameter space adopted by Stage III/IV collaborations in studies employing real-space two-point correlation functions. This paper offers a leap in this direction by carefully investigating the modern transformer-based neural network (NN) architectures in realistic simulated Rubin Observatory year one cosmic shear $\\Lambda$CDM inferences. Building on the framework introduced in Part I, we generalize the transformer block and incorporate additional layer types to develop a more versatile architecture. We present a scalable method to efficiently generate an extensive training dataset that significantly exceeds the scope of prior volumes considered in Part I, while still meeting strict accuracy standards. Through our meticulous architecture comparison and comprehensive hyperparameter optimization, we establish that the attention-based architecture performs an order of magnitude better in accuracy than widely adopted NN designs. Finally, we test and apply our emulators to calibrate tension metrics.","sentences":["The next generation of cosmological surveys is expected to generate unprecedented high-quality data, consequently increasing the already substantial computational costs of Bayesian statistical methods.","This will pose a significant challenge to analyzing theoretical models of cosmology.","Additionally, new mitigation techniques of baryonic effects, intrinsic alignment, and other systematic effects will inevitably introduce more parameters, slowing down the convergence of Bayesian analyses.","In this scenario, machine-learning-based accelerators are a promising solution, capable of reducing the computational costs and execution time of such tools by order of thousands.","Yet, they have not been able to provide accurate predictions over the wide prior ranges in parameter space adopted by Stage III/IV collaborations in studies employing real-space two-point correlation functions.","This paper offers a leap in this direction by carefully investigating the modern transformer-based neural network (NN) architectures in realistic simulated Rubin Observatory year one cosmic shear $\\Lambda$CDM inferences.","Building on the framework introduced in Part I, we generalize the transformer block and incorporate additional layer types to develop a more versatile architecture.","We present a scalable method to efficiently generate an extensive training dataset that significantly exceeds the scope of prior volumes considered in Part I, while still meeting strict accuracy standards.","Through our meticulous architecture comparison and comprehensive hyperparameter optimization, we establish that the attention-based architecture performs an order of magnitude better in accuracy than widely adopted NN designs.","Finally, we test and apply our emulators to calibrate tension metrics."],"url":"http://arxiv.org/abs/2403.12337v1","category":"astro-ph.IM"}
{"created":"2024-03-19 00:52:49","title":"Collision of two solitons for $1d$ Nonlinear Schrodinger Equation with same mass","abstract":"We study the global dynamics of the collision of two solitons having the same mass for one-dimensional Nonlinear Schr\\\"odinger models with multi-power nonlinearity. For any natural number k, it is verified that if the incoming speed v between the two solitary waves is small enough, then, after the collision, the two solitons will move away with an outcoming speed v_{f}=v+O(v^{k}) and the remainder of the solution will also have energy and weighted norms of order O(v^{k}). This is applied to several one-dimensional models such as the cubic NLS and the cubic-quintic NLS.","sentences":["We study the global dynamics of the collision of two solitons having the same mass for one-dimensional Nonlinear Schr\\\"odinger models with multi-power nonlinearity.","For any natural number k, it is verified that if the incoming speed v between the two solitary waves is small enough, then, after the collision, the two solitons will move away with an outcoming speed v_{f}=v+O(v^{k}) and the remainder of the solution will also have energy and weighted norms of order O(v^{k}).","This is applied to several one-dimensional models such as the cubic NLS and the cubic-quintic NLS."],"url":"http://arxiv.org/abs/2403.12336v1","category":"math.AP"}
{"created":"2024-03-19 00:48:25","title":"Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems","abstract":"Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justification for this approach based on Koopman spectral theory and empirically demonstrate tcKAE's superior performance over state-of-the-art KAE models across a variety of test cases, including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data.","sentences":["Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems.","Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics.","However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability.","To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data.","This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models.","We provide analytical justification for this approach based on Koopman spectral theory and empirically demonstrate tcKAE's superior performance over state-of-the-art KAE models across a variety of test cases, including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data."],"url":"http://arxiv.org/abs/2403.12335v1","category":"cs.LG"}
{"created":"2024-03-19 00:28:27","title":"Structural Validation Of Synthetic Power Distribution Networks Using The Multiscale Flat Norm","abstract":"We study the problem of comparing a pair of geometric networks that may not be similarly defined, i.e., when they do not have one-to-one correspondences between their nodes and edges. Our motivating application is to compare power distribution networks of a region. Due to the lack of openly available power network datasets, researchers synthesize realistic networks resembling their actual counterparts. But the synthetic digital twins may vary significantly from one another and from actual networks due to varying underlying assumptions and approaches. Hence the user wants to evaluate the quality of networks in terms of their structural similarity to actual power networks. But the lack of correspondence between the networks renders most standard approaches, e.g., subgraph isomorphism and edit distance, unsuitable.   We propose an approach based on the multiscale flat norm, a notion of distance between objects defined in the field of geometric measure theory, to compute the distance between a pair of planar geometric networks. Using a triangulation of the domain containing the input networks, the flat norm distance between two networks at a given scale can be computed by solving a linear program. In addition, this computation automatically identifies the 2D regions (patches) that capture where the two networks are different. We demonstrate through 2D examples that the flat norm distance can capture the variations of inputs more accurately than the commonly used Hausdorff distance. As a notion of stability, we also derive upper bounds on the flat norm distance between a simple 1D curve and its perturbed version as a function of the radius of perturbation for a restricted class of perturbations. We demonstrate our approach on a set of actual power networks from a county in the USA.","sentences":["We study the problem of comparing a pair of geometric networks that may not be similarly defined, i.e., when they do not have one-to-one correspondences between their nodes and edges.","Our motivating application is to compare power distribution networks of a region.","Due to the lack of openly available power network datasets, researchers synthesize realistic networks resembling their actual counterparts.","But the synthetic digital twins may vary significantly from one another and from actual networks due to varying underlying assumptions and approaches.","Hence the user wants to evaluate the quality of networks in terms of their structural similarity to actual power networks.","But the lack of correspondence between the networks renders most standard approaches, e.g., subgraph isomorphism and edit distance, unsuitable.   ","We propose an approach based on the multiscale flat norm, a notion of distance between objects defined in the field of geometric measure theory, to compute the distance between a pair of planar geometric networks.","Using a triangulation of the domain containing the input networks, the flat norm distance between two networks at a given scale can be computed by solving a linear program.","In addition, this computation automatically identifies the 2D regions (patches) that capture where the two networks are different.","We demonstrate through 2D examples that the flat norm distance can capture the variations of inputs more accurately than the commonly used Hausdorff distance.","As a notion of stability, we also derive upper bounds on the flat norm distance between a simple 1D curve and its perturbed version as a function of the radius of perturbation for a restricted class of perturbations.","We demonstrate our approach on a set of actual power networks from a county in the USA."],"url":"http://arxiv.org/abs/2403.12334v1","category":"cs.CG"}
{"created":"2024-03-18 23:23:40","title":"Molecular dynamics simulation with finite electric fields using Perturbed Neural Network Potentials","abstract":"The interaction of condensed phase systems with external electric fields is crucial in myriad processes in nature and technology ranging from the field-directed motion of cells (galvanotaxis), to energy storage and conversion systems including supercapacitors, batteries and solar cells. Molecular simulation in the presence of electric fields would give important atomistic insight into these processes but applications of the most accurate methods such as ab-initio molecular dynamics are limited in scope by their computational expense. Here we introduce Perturbed Neural Network Potential Molecular Dynamics (PNNP MD) to push back the accessible time and length scales of such simulations at virtually no loss in accuracy. The total forces on the atoms are expressed in terms of the unperturbed potential energy surface represented by a standard neural network potential and a field-induced perturbation obtained from a series expansion of the field interaction truncated at first order. The latter is represented in terms of an equivariant graph neural network, trained on the atomic polar tensor. PNNP MD is shown to give excellent results for the dielectric relaxation dynamics, the dielectric constant and the field-dependent IR spectrum of liquid water when compared to ab-initio molecular dynamics or experiment, up to surprisingly high field strengths of about 0.2 V/A. This is remarkable because, in contrast to most previous approaches, the two neural networks on which PNNL MD is based are exclusively trained on zero-field molecular configurations demonstrating that the networks not only interpolate but also reliably extrapolate the field response. PNNP MD is based on rigorous theory yet it is simple, general, modular, and systematically improvable allowing us to obtain atomistic insight into the interaction of a wide range of condensed phase systems with external electric fields.","sentences":["The interaction of condensed phase systems with external electric fields is crucial in myriad processes in nature and technology ranging from the field-directed motion of cells (galvanotaxis), to energy storage and conversion systems including supercapacitors, batteries and solar cells.","Molecular simulation in the presence of electric fields would give important atomistic insight into these processes but applications of the most accurate methods such as ab-initio molecular dynamics are limited in scope by their computational expense.","Here we introduce Perturbed Neural Network Potential Molecular Dynamics (PNNP MD) to push back the accessible time and length scales of such simulations at virtually no loss in accuracy.","The total forces on the atoms are expressed in terms of the unperturbed potential energy surface represented by a standard neural network potential and a field-induced perturbation obtained from a series expansion of the field interaction truncated at first order.","The latter is represented in terms of an equivariant graph neural network, trained on the atomic polar tensor.","PNNP MD is shown to give excellent results for the dielectric relaxation dynamics, the dielectric constant and the field-dependent IR spectrum of liquid water when compared to ab-initio molecular dynamics or experiment, up to surprisingly high field strengths of about 0.2 V/A. This is remarkable because, in contrast to most previous approaches, the two neural networks on which PNNL MD is based are exclusively trained on zero-field molecular configurations demonstrating that the networks not only interpolate but also reliably extrapolate the field response.","PNNP MD is based on rigorous theory yet","it is simple, general, modular, and systematically improvable allowing us to obtain atomistic insight into the interaction of a wide range of condensed phase systems with external electric fields."],"url":"http://arxiv.org/abs/2403.12319v1","category":"physics.chem-ph"}
{"created":"2024-03-18 23:20:27","title":"Homotopy BV-algebras in Hermitian geometry","abstract":"We show that the de Rham complex of any almost Hermitian manifold carries a natural commutative $BV_\\infty$-algebra structure satisfying the degeneration property. In the almost K\\\"ahler case, this recovers Koszul's BV-algebra, defined for any Poisson manifold. As a consequence, both the Dolbeault and the de Rham cohomologies of any compact Hermitian manifold are canonically endowed with homotopy hypercommutative algebra structures, also known as formal homotopy Frobenius manifolds. Similar results are developed for (almost) symplectic manifolds with Lagrangian subbundles.","sentences":["We show that the de Rham complex of any almost Hermitian manifold carries a natural commutative $BV_\\infty$-algebra structure satisfying the degeneration property.","In the almost K\\\"ahler case, this recovers Koszul's BV-algebra, defined for any Poisson manifold.","As a consequence, both the Dolbeault and the de Rham cohomologies of any compact Hermitian manifold are canonically endowed with homotopy hypercommutative algebra structures, also known as formal homotopy Frobenius manifolds.","Similar results are developed for (almost) symplectic manifolds with Lagrangian subbundles."],"url":"http://arxiv.org/abs/2403.12314v1","category":"math.AT"}
{"created":"2024-03-18 23:03:31","title":"Conformally symplectic structures and the Lefschetz condition","abstract":"This short note provides a symplectic analogue of Vaisman's theorem in complex geometry. Namely, for any compact symplectic manifold satisfying the hard Lefschetz condition in degree 1, every locally conformally symplectic structure is in fact globally conformally symplectic, whenever there is a mutually compatible almost complex structure.","sentences":["This short note provides a symplectic analogue of Vaisman's theorem in complex geometry.","Namely, for any compact symplectic manifold satisfying the hard Lefschetz condition in degree 1, every locally conformally symplectic structure is in fact globally conformally symplectic, whenever there is a mutually compatible almost complex structure."],"url":"http://arxiv.org/abs/2403.12304v1","category":"math.SG"}
{"created":"2024-03-18 22:32:39","title":"Long time regularity of the $p$-Gauss curvature flow with flat side","abstract":"In this paper, we prove the long time regularity of the interface in the $p$-Gauss curvature flow with flat side in all dimensions for $p>\\frac1n$. Here the interface is the boundary of the flat part in the flow. In dimension $2$, this problem was solved in \\cite{DL2004} for $p=1$ and in \\cite{KimLeeRhee2013} for $p\\in(1/2,1)$. We utilize the duality method to transform the Gauss curvature flow to a singular parabolic Monge-Amp\\`ere equation, and prove the regularity of the interface by studying the asymptotic cone of the parabolic Monge-Amp\\`ere equation in the polar coordinates.","sentences":["In this paper, we prove the long time regularity of the interface in the $p$-Gauss curvature flow with flat side in all dimensions for $p>\\frac1n$. Here the interface is the boundary of the flat part in the flow.","In dimension $2$, this problem was solved in \\cite{DL2004} for $p=1$ and in \\cite{KimLeeRhee2013} for $p\\in(1/2,1)$. We utilize the duality method to transform the Gauss curvature flow to a singular parabolic Monge-Amp\\`ere equation, and prove the regularity of the interface by studying the asymptotic cone of the parabolic Monge-Amp\\`ere equation in the polar coordinates."],"url":"http://arxiv.org/abs/2403.12292v1","category":"math.AP"}
{"created":"2024-03-18 21:55:25","title":"Reachability-based Trajectory Design via Exact Formulation of Implicit Neural Signed Distance Functions","abstract":"Generating receding-horizon motion trajectories for autonomous vehicles in real-time while also providing safety guarantees is challenging. This is because a future trajectory needs to be planned before the previously computed trajectory is completely executed. This becomes even more difficult if the trajectory is required to satisfy continuous-time collision-avoidance constraints while accounting for a large number of obstacles. To address these challenges, this paper proposes a novel real-time, receding-horizon motion planning algorithm named REachability-based trajectory Design via Exact Formulation of Implicit NEural signed Distance functions (REDEFINED). REDEFINED first applies offline reachability analysis to compute zonotope-based reachable sets that overapproximate the motion of the ego vehicle. During online planning, REDEFINED leverages zonotope arithmetic to construct a neural implicit representation that computes the exact signed distance between a parameterized swept volume of the ego vehicle and obstacle vehicles. REDEFINED then implements a novel, real-time optimization framework that utilizes the neural network to construct a collision avoidance constraint. REDEFINED is compared to a variety of state-of-the-art techniques and is demonstrated to successfully enable the vehicle to safely navigate through complex environments. Code, data, and video demonstrations can be found at https://roahmlab.github.io/redefined/.","sentences":["Generating receding-horizon motion trajectories for autonomous vehicles in real-time while also providing safety guarantees is challenging.","This is because a future trajectory needs to be planned before the previously computed trajectory is completely executed.","This becomes even more difficult if the trajectory is required to satisfy continuous-time collision-avoidance constraints while accounting for a large number of obstacles.","To address these challenges, this paper proposes a novel real-time, receding-horizon motion planning algorithm named REachability-based trajectory Design via Exact Formulation of Implicit NEural signed Distance functions (REDEFINED).","REDEFINED first applies offline reachability analysis to compute zonotope-based reachable sets that overapproximate the motion of the ego vehicle.","During online planning, REDEFINED leverages zonotope arithmetic to construct a neural implicit representation that computes the exact signed distance between a parameterized swept volume of the ego vehicle and obstacle vehicles.","REDEFINED then implements a novel, real-time optimization framework that utilizes the neural network to construct a collision avoidance constraint.","REDEFINED is compared to a variety of state-of-the-art techniques and is demonstrated to successfully enable the vehicle to safely navigate through complex environments.","Code, data, and video demonstrations can be found at https://roahmlab.github.io/redefined/."],"url":"http://arxiv.org/abs/2403.12280v1","category":"cs.RO"}
{"created":"2024-03-18 21:53:56","title":"Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices","abstract":"Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\\mathbf{A}$ is to being rank deficient and even if $\\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \\textit{implicitly regularizes} tall and skinny matrices $\\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.","sentences":["Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\\mathbf{A}$ with many more rows than columns.","We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\\mathbf{A}$ is to being rank deficient and even if $\\mathbf{A}$ is rank-deficient.","In other words, stochastic rounding \\textit{implicitly regularizes} tall and skinny matrices $\\mathbf{A}$ so that the rounded version has full column rank.","Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces."],"url":"http://arxiv.org/abs/2403.12278v1","category":"cs.LG"}
{"created":"2024-03-18 21:35:48","title":"Spacetime symmetries and and geometric diffusion","abstract":"We examine relativistic diffusion through the frame and observer bundles associated with a Lorentzian manifold $(M,g)$. Our focus is on spacetimes with a non-trivial isometry group, and we detail the conditions required to find symmetric solutions of the relativistic diffusion equation. Additionally, we analyze the conservation laws associated with the presence of Killing vector fields on $(M,g)$ and their implications for the expressions of the geodesic spray and the vertical Laplacian on both the frame and the observer bundles. Finally, we present several relevant examples of symmetric spacetimes.","sentences":["We examine relativistic diffusion through the frame and observer bundles associated with a Lorentzian manifold $(M,g)$.","Our focus is on spacetimes with a non-trivial isometry group, and we detail the conditions required to find symmetric solutions of the relativistic diffusion equation.","Additionally, we analyze the conservation laws associated with the presence of Killing vector fields on $(M,g)$ and their implications for the expressions of the geodesic spray and the vertical Laplacian on both the frame and the observer bundles.","Finally, we present several relevant examples of symmetric spacetimes."],"url":"http://arxiv.org/abs/2403.12270v1","category":"gr-qc"}
{"created":"2024-03-18 21:06:00","title":"Microscopic derivation of the thin film equation using the Mori-Zwanzig formalism","abstract":"The hydrodynamics of thin films is typically described using phenomenological models whose connection to the microscopic particle dynamics is a subject of ongoing research. Existing methods based on density functional theory provide a good description of static thin films, but are not sufficient for understanding nonequilibrium dynamics. In this work, we present a microscopic derivation of the thin film equation using the Mori-Zwanzig projection operator formalism. This method allows to directly obtain the correct gradient dynamics structure along with microscopic expressions for the mobility and the free energy. Our results are verified against molecular dynamics simulations for both simple fluids and polymers.","sentences":["The hydrodynamics of thin films is typically described using phenomenological models whose connection to the microscopic particle dynamics is a subject of ongoing research.","Existing methods based on density functional theory provide a good description of static thin films, but are not sufficient for understanding nonequilibrium dynamics.","In this work, we present a microscopic derivation of the thin film equation using the Mori-Zwanzig projection operator formalism.","This method allows to directly obtain the correct gradient dynamics structure along with microscopic expressions for the mobility and the free energy.","Our results are verified against molecular dynamics simulations for both simple fluids and polymers."],"url":"http://arxiv.org/abs/2403.12253v1","category":"cond-mat.soft"}
{"created":"2024-03-18 20:54:42","title":"Converging/diverging self-similar shock waves: from collapse to reflection","abstract":"We solve the continuation problem for the non-isentropic Euler equations following the collapse of an imploding shock wave. More precisely, we prove that the self-similar G\\\"uderley imploding shock solutions for a perfect gas with adiabatic exponent $\\gamma\\in(1,3]$ admit a self-similar extension consisting of two regions of smooth flow separated by an outgoing spherically symmetric shock wave of finite strength. In addition, for $\\gamma\\in(1,\\frac53]$, we show that there is a unique choice of shock wave that gives rise to a globally defined self-similar flow with physical state at the spatial origin.","sentences":["We solve the continuation problem for the non-isentropic Euler equations following the collapse of an imploding shock wave.","More precisely, we prove that the self-similar G\\\"uderley imploding shock solutions for a perfect gas with adiabatic exponent $\\gamma\\in(1,3]$ admit a self-similar extension consisting of two regions of smooth flow separated by an outgoing spherically symmetric shock wave of finite strength.","In addition, for $\\gamma\\in(1,\\frac53]$, we show that there is a unique choice of shock wave that gives rise to a globally defined self-similar flow with physical state at the spatial origin."],"url":"http://arxiv.org/abs/2403.12247v1","category":"math.AP"}
{"created":"2024-03-18 20:51:08","title":"Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds","abstract":"We propose a method for improving the prediction accuracy of learned robot dynamics models on out-of-distribution (OOD) states. We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints. Crucially, we do not assume this structure is known a priori, and instead learn it from data. We use contrastive learning to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics. We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a Gaussian process (GP) representation of the constraint manifold. We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines.","sentences":["We propose a method for improving the prediction accuracy of learned robot dynamics models on out-of-distribution (OOD) states.","We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints.","Crucially, we do not assume this structure is known a priori, and instead learn it from data.","We use contrastive learning to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics.","We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a Gaussian process (GP) representation of the constraint manifold.","We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines."],"url":"http://arxiv.org/abs/2403.12245v2","category":"cs.RO"}
{"created":"2024-03-18 20:23:15","title":"The advancing wave front on a sloping channel covered by a rod canopy following an instantaneous dam break","abstract":"The drag coefficient $C_d$ for a rigid and uniformly distributed rod canopy covering a sloping channel following the instantaneous collapse of a dam was examined using flume experiments. The measurements included space $x$ and time $t$ high resolution images of the water surface $h(x,t)$ for multiple channel bed slopes $S_o$ and water depths behind the dam $H_o$ along with drag estimates provided by sequential load cells. Analysis of the Saint-Venant Equation (SVE) for the front speed using the diffusive wave approximation lead to a front velocity $U_f=\\sqrt{\\Gamma_h 2 g \\phi_v'/(C_d m D)}$, where $\\Gamma_h=-\\partial h/\\partial x$, $g$ is the gravitational acceleration, $\\phi_v'=1-\\phi_v$ is fluid volume fraction per ground area, $\\phi_v=m \\pi D^2/4$ is the solid volume fraction per ground area, $m$ is the number of rods per ground area, and $D$ is the rod diameter. An inferred $C_d=0.4$ from the $h(x,t)$ data near the advancing front region, also confirmed by load cell measurements, is much reduced relative to its independently measured steady-uniform flow case. This finding suggests that drag reduction mechanisms associated with transients and flow disturbances are more likely to play a dominant role when compared to conventional sheltering or blocking effects on $C_d$ examined in uniform flow. The increased air volume entrained into the advancing wave front region as determined from an inflow-outflow volume balance partly explains the $C_d$ reduction from unity.","sentences":["The drag coefficient $C_d$ for a rigid and uniformly distributed rod canopy covering a sloping channel following the instantaneous collapse of a dam was examined using flume experiments.","The measurements included space $x$ and time $t$ high resolution images of the water surface $h(x,t)$ for multiple channel bed slopes $S_o$ and water depths behind the dam $H_o$ along with drag estimates provided by sequential load cells.","Analysis of the Saint-Venant Equation (SVE) for the front speed using the diffusive wave approximation lead to a front velocity $U_f=\\sqrt{\\Gamma_h 2 g \\phi_v'/(C_d m D)}$, where $\\Gamma_h=-\\partial h/\\partial x$, $g$ is the gravitational acceleration, $\\phi_v'=1-\\phi_v$ is fluid volume fraction per ground area, $\\phi_v=m \\pi D^2/4$ is the solid volume fraction per ground area, $m$ is the number of rods per ground area, and $D$ is the rod diameter.","An inferred $C_d=0.4$ from the $h(x,t)$ data near the advancing front region, also confirmed by load cell measurements, is much reduced relative to its independently measured steady-uniform flow case.","This finding suggests that drag reduction mechanisms associated with transients and flow disturbances are more likely to play a dominant role when compared to conventional sheltering or blocking effects on $C_d$ examined in uniform flow.","The increased air volume entrained into the advancing wave front region as determined from an inflow-outflow volume balance partly explains the $C_d$ reduction from unity."],"url":"http://arxiv.org/abs/2403.12232v1","category":"physics.flu-dyn"}
{"created":"2024-03-18 20:06:31","title":"Assessing the dark degeneracy through the gas mass fraction data","abstract":"It is well-known that Einstein's equations constrain only the total energy-momentum tensor of the cosmic substratum, without specifying the characteristics of its individual constituents. Consequently, cosmological models featuring distinct decompositions within the dark sector, while sharing identical values for the sum of dark components' energy-momentum tensor, remain indistinguishable when assessed through observables based on distance measurements. Notably, it has been already demonstrated that cosmological models with dynamical descriptions of dark energy, characterized by a time-dependent equation of state (EoS), can always be mapped into a model featuring a decaying vacuum ($w=-1$) coupled with dark matter. We explore the possibility of breaking this degeneracy by using measurements of the gas mass fraction observed in massive and relaxed galaxy clusters. This data is particularly interesting for this purpose because it isolates the matter contribution, possibly allowing the degeneracy breaking. We study the particular case of the $w$CDM model with its interactive counterpart. We compare the results obtained from both descriptions with a non-parametric analysis obtained through Gaussian Process. Even though the degeneracy may be broken from the theoretical point of view, we find that current gas mass fraction data seems to be insufficient for a final conclusion about which approach is favored, even when combined with SNIa, BAO and CMB.","sentences":["It is well-known that Einstein's equations constrain only the total energy-momentum tensor of the cosmic substratum, without specifying the characteristics of its individual constituents.","Consequently, cosmological models featuring distinct decompositions within the dark sector, while sharing identical values for the sum of dark components' energy-momentum tensor, remain indistinguishable when assessed through observables based on distance measurements.","Notably, it has been already demonstrated that cosmological models with dynamical descriptions of dark energy, characterized by a time-dependent equation of state (EoS), can always be mapped into a model featuring a decaying vacuum ($w=-1$) coupled with dark matter.","We explore the possibility of breaking this degeneracy by using measurements of the gas mass fraction observed in massive and relaxed galaxy clusters.","This data is particularly interesting for this purpose because it isolates the matter contribution, possibly allowing the degeneracy breaking.","We study the particular case of the $w$CDM model with its interactive counterpart.","We compare the results obtained from both descriptions with a non-parametric analysis obtained through Gaussian Process.","Even though the degeneracy may be broken from the theoretical point of view, we find that current gas mass fraction data seems to be insufficient for a final conclusion about which approach is favored, even when combined with SNIa, BAO and CMB."],"url":"http://arxiv.org/abs/2403.12220v1","category":"astro-ph.CO"}
{"created":"2024-03-18 19:54:59","title":"Private graphon estimation via sum-of-squares","abstract":"We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks. The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems. The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks. The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm.","sentences":["We develop the first pure node-differentially-private algorithms for learning stochastic block models and for graphon estimation with polynomial running time for any constant number of blocks.","The statistical utility guarantees match those of the previous best information-theoretic (exponential-time) node-private mechanisms for these problems.","The algorithm is based on an exponential mechanism for a score function defined in terms of a sum-of-squares relaxation whose level depends on the number of blocks.","The key ingredients of our results are (1) a characterization of the distance between the block graphons in terms of a quadratic optimization over the polytope of doubly stochastic matrices, (2) a general sum-of-squares convergence result for polynomial optimization over arbitrary polytopes, and (3) a general approach to perform Lipschitz extensions of score functions as part of the sum-of-squares algorithmic paradigm."],"url":"http://arxiv.org/abs/2403.12213v1","category":"cs.DS"}
{"created":"2024-03-18 19:13:02","title":"FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos","abstract":"Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information. Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy.","sentences":["Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training.","Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue.","However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera.","With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue.","We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch.","This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information.","Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy."],"url":"http://arxiv.org/abs/2403.12198v1","category":"cs.CV"}
{"created":"2024-03-18 18:58:23","title":"Approximation of RKHS Functionals by Neural Networks","abstract":"Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals). In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks. We establish the universality of the approximation of functionals on the RKHS's. Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels. Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models. Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions. By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is much simpler in that we use point evaluations to replace basis function expansions.","sentences":["Motivated by the abundance of functional data such as time series and images, there has been a growing interest in integrating such data into neural networks and learning maps from function spaces to R (i.e., functionals).","In this paper, we study the approximation of functionals on reproducing kernel Hilbert spaces (RKHS's) using neural networks.","We establish the universality of the approximation of functionals on the RKHS's.","Specifically, we derive explicit error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev kernels.","Moreover, we apply our findings to functional regression, proving that neural networks can accurately approximate the regression maps in generalized functional linear models.","Existing works on functional learning require integration-type basis function expansions with a set of pre-specified basis functions.","By leveraging the interpolating orthogonal projections in RKHS's, our proposed network is much simpler in that we use point evaluations to replace basis function expansions."],"url":"http://arxiv.org/abs/2403.12187v1","category":"stat.ML"}
{"created":"2024-03-18 18:10:34","title":"ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis","abstract":"Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method.","sentences":["Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing.","However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction.","This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene.","To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly.","To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information.","Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction.","Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method."],"url":"http://arxiv.org/abs/2403.12154v1","category":"cs.CV"}
{"created":"2024-03-18 18:00:01","title":"Modularity of Schur index, modular differential equations, and high-temperature asymptotics","abstract":"In this paper we analytically explore the modularity of the flavored Schur index of 4d $\\mathcal{N} = 2$ SCFTs. We focus on the $A_1$ theories of class-$\\mathcal{S}$ and $\\mathcal{N} = 4$ theories with $SU(N)$ gauge group. We work out the modular orbit of the flavored index and defect index, compute the dimension of the space spanned by the orbit, and provide complete basis for computing modular transformation matrices. The dimension obtained from the flavored analysis predicts the minimal order of the unflavored modular differential equation satisfied by the unflavored Schur index. With the help of modularity, we also study analytically the high-temperature asymptotics of the Schur index. In the high-temperature limit $\\tau \\to +i0$, we identified the (defect) Schur index of the genus-zero $A_1$ theories of class-$\\mathcal{S}$ with the $S^3$-partition function of the $SU(2) \\times U(1)^n$ star-shape quiver (with Wilson line insertion). In the identification, we observe an interesting relation between the linear-independence of defect indices and the convergence of the Wilson line partition functions.","sentences":["In this paper we analytically explore the modularity of the flavored Schur index of 4d $\\mathcal{N} = 2$ SCFTs.","We focus on the $A_1$ theories of class-$\\mathcal{S}$ and $\\mathcal{N} = 4$ theories with $SU(N)$ gauge group.","We work out the modular orbit of the flavored index and defect index, compute the dimension of the space spanned by the orbit, and provide complete basis for computing modular transformation matrices.","The dimension obtained from the flavored analysis predicts the minimal order of the unflavored modular differential equation satisfied by the unflavored Schur index.","With the help of modularity, we also study analytically the high-temperature asymptotics of the Schur index.","In the high-temperature limit $\\tau \\to +i0$, we identified the (defect) Schur index of the genus-zero $A_1$ theories of class-$\\mathcal{S}$ with the $S^3$-partition function of the $SU(2) \\times U(1)^n$ star-shape quiver (with Wilson line insertion).","In the identification, we observe an interesting relation between the linear-independence of defect indices and the convergence of the Wilson line partition functions."],"url":"http://arxiv.org/abs/2403.12127v1","category":"hep-th"}
{"created":"2024-03-18 18:00:00","title":"Holographic Energy Correlators for Confining Theories","abstract":"We present a holographic calculation of energy correlators in a simple model of confinement based on a warped extra dimension with an IR brane. For small distances we reproduce the constant correlators of a strongly-coupled conformal field theory, while for large distances the effects of confinement dominate and the correlators decay exponentially. We find exact shockwave solutions to the Einstein equations in the presence of the IR brane, hence avoiding the need for a perturbative expansion in terms of Witten diagrams. While some of the expected qualitative features of energy correlators in quantum chromodynamics (QCD) are reproduced, our crude model of confinement does not capture the effects of asymptotic freedom nor exhibit jetty behavior. We expect that our method can also be applied to more realistic models of confinement incorporating asymptotic freedom, which should fix some of the deviations from QCD.","sentences":["We present a holographic calculation of energy correlators in a simple model of confinement based on a warped extra dimension with an IR brane.","For small distances we reproduce the constant correlators of a strongly-coupled conformal field theory, while for large distances the effects of confinement dominate and the correlators decay exponentially.","We find exact shockwave solutions to the Einstein equations in the presence of the IR brane, hence avoiding the need for a perturbative expansion in terms of Witten diagrams.","While some of the expected qualitative features of energy correlators in quantum chromodynamics (QCD) are reproduced, our crude model of confinement does not capture the effects of asymptotic freedom nor exhibit jetty behavior.","We expect that our method can also be applied to more realistic models of confinement incorporating asymptotic freedom, which should fix some of the deviations from QCD."],"url":"http://arxiv.org/abs/2403.12123v1","category":"hep-ph"}
{"created":"2024-03-18 17:55:20","title":"Simplifications of Lax pairs for differential-difference equations by gauge transformations and (doubly) modified integrable equations","abstract":"Matrix differential-difference Lax pairs play an essential role in the theory of integrable nonlinear differential-difference equations. We present sufficient conditions for the possibility to simplify such a Lax pair by matrix gauge transformations. Furthermore, we describe a procedure for such a simplification and present applications of it to constructing new integrable equations connected by (non-invertible) discrete substitutions to known equations with Lax pairs.   Suppose that one has three (possibly multicomponent) equations $E$, $E_1$, $E_2$, a discrete substitution from $E_1$ to $E$, and a discrete substitution from $E_2$ to $E_1$. Then $E_1$ and $E_2$ can be called a modified version of $E$ and a doubly modified version of $E$, respectively. We demonstrate how the above-mentioned procedure helps (in the considered examples) to construct modified and doubly modified versions of a given equation possessing a Lax pair satisfying certain conditions.   The considered examples include scalar equations of Itoh-Narita-Bogoyavlensky type and $2$-component equations related to the Toda lattice. Several new integrable equations and discrete substitutions are presented.","sentences":["Matrix differential-difference Lax pairs play an essential role in the theory of integrable nonlinear differential-difference equations.","We present sufficient conditions for the possibility to simplify such a Lax pair by matrix gauge transformations.","Furthermore, we describe a procedure for such a simplification and present applications of it to constructing new integrable equations connected by (non-invertible) discrete substitutions to known equations with Lax pairs.   ","Suppose that one has three (possibly multicomponent) equations $E$, $E_1$, $E_2$, a discrete substitution from $E_1$ to $E$, and a discrete substitution from $E_2$ to $E_1$. Then $E_1$ and $E_2$ can be called a modified version of $E$ and a doubly modified version of $E$, respectively.","We demonstrate how the above-mentioned procedure helps (in the considered examples) to construct modified and doubly modified versions of a given equation possessing a Lax pair satisfying certain conditions.   ","The considered examples include scalar equations of Itoh-Narita-Bogoyavlensky type and $2$-component equations related to the Toda lattice.","Several new integrable equations and discrete substitutions are presented."],"url":"http://arxiv.org/abs/2403.12022v1","category":"nlin.SI"}
{"created":"2024-03-18 17:54:57","title":"A classification result for eternal mean convex flows of finite total curvature type","abstract":"In this article we partially classify the space of eternal mean convex flows in $\\mathbb{R}^3$ of finite total curvature type, a condition implied by finite total curvature. In particular we show that topologically nonplanar ones must flow out of a catenoid in a natural sense.","sentences":["In this article we partially classify the space of eternal mean convex flows in $\\mathbb{R}^3$ of finite total curvature type, a condition implied by finite total curvature.","In particular we show that topologically nonplanar ones must flow out of a catenoid in a natural sense."],"url":"http://arxiv.org/abs/2403.12020v1","category":"math.DG"}
{"created":"2024-03-18 17:54:34","title":"LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation","abstract":"The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called LN3Diff to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Our proposed LN3Diff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks.","sentences":["The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques.","Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled.","This paper introduces a novel framework called LN3Diff to address this gap and enable fast, high-quality, and generic conditional 3D generation.","Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent space.","The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field.","Through training a diffusion model on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets.","Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization.","Our proposed LN3Diff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks."],"url":"http://arxiv.org/abs/2403.12019v1","category":"cs.CV"}
{"created":"2024-03-18 17:46:07","title":"Posterior Uncertainty Quantification in Neural Networks using Data Augmentation","abstract":"In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data. Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly defined Bayesian posterior. Our empirical analysis showcases that MixupMP achieves superior predictive performance and uncertainty quantification on various image classification datasets, when compared with existing Bayesian and non-Bayesian approaches.","sentences":["In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future data.","Under this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future data are supported on existing observations only -- a situation rarely encountered in practice.","To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular data augmentation techniques.","MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random simulation from this predictive distribution.","Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly defined Bayesian posterior.","Our empirical analysis showcases that MixupMP achieves superior predictive performance and uncertainty quantification on various image classification datasets, when compared with existing Bayesian and non-Bayesian approaches."],"url":"http://arxiv.org/abs/2403.12729v1","category":"stat.ML"}
{"created":"2024-03-18 17:26:27","title":"Sub-photon accuracy noise reduction of single shot coherent diffraction pattern with atomic model trained autoencoder","abstract":"Single-shot imaging with femtosecond X-ray lasers is a powerful measurement technique that can achieve both high spatial and temporal resolution. However, its accuracy has been severely limited by the difficulty of applying conventional noise-reduction processing. This study uses deep learning to validate noise reduction techniques, with autoencoders serving as the learning model. Focusing on the diffraction patterns of nanoparticles, we simulated a large dataset treating the nanoparticles as composed of many independent atoms. Three neural network architectures are investigated: neural network, convolutional neural network and U-net, with U-net showing superior performance in noise reduction and subphoton reproduction. We also extended our models to apply to diffraction patterns of particle shapes different from those in the simulated data. We then applied the U-net model to a coherent diffractive imaging study, wherein a nanoparticle in a microfluidic device is exposed to a single X-ray free-electron laser pulse. After noise reduction, the reconstructed nanoparticle image improved significantly even though the nanoparticle shape was different from the training data, highlighting the importance of transfer learning.","sentences":["Single-shot imaging with femtosecond X-ray lasers is a powerful measurement technique that can achieve both high spatial and temporal resolution.","However, its accuracy has been severely limited by the difficulty of applying conventional noise-reduction processing.","This study uses deep learning to validate noise reduction techniques, with autoencoders serving as the learning model.","Focusing on the diffraction patterns of nanoparticles, we simulated a large dataset treating the nanoparticles as composed of many independent atoms.","Three neural network architectures are investigated: neural network, convolutional neural network and U-net, with U-net showing superior performance in noise reduction and subphoton reproduction.","We also extended our models to apply to diffraction patterns of particle shapes different from those in the simulated data.","We then applied the U-net model to a coherent diffractive imaging study, wherein a nanoparticle in a microfluidic device is exposed to a single X-ray free-electron laser pulse.","After noise reduction, the reconstructed nanoparticle image improved significantly even though the nanoparticle shape was different from the training data, highlighting the importance of transfer learning."],"url":"http://arxiv.org/abs/2403.11992v1","category":"physics.optics"}
{"created":"2024-03-18 17:25:32","title":"Encoding of linear kinetic plasma problems in quantum circuits via data compression","abstract":"We propose an algorithm for encoding of linear kinetic plasma problems in quantum circuits. The focus is made on modeling electrostatic linear waves in one-dimensional Maxwellian electron plasma. The waves are described by the linearized Vlasov-Amp\\`ere system with a spatially localized external current that drives plasma oscillations. This system is formulated as a boundary-value problem and cast in the form of a linear vector equation $A\\psi = b$ to be solved by using the quantum signal processing algorithm. The latter requires encoding of the matrix $A$ in a quantum circuit as a subblock of a unitary matrix. We propose how to encode $A$ in a circuit in a compressed form and discuss how the resulting circuit scales with the problem size and the desired precision.","sentences":["We propose an algorithm for encoding of linear kinetic plasma problems in quantum circuits.","The focus is made on modeling electrostatic linear waves in one-dimensional Maxwellian electron plasma.","The waves are described by the linearized Vlasov-Amp\\`ere system with a spatially localized external current that drives plasma oscillations.","This system is formulated as a boundary-value problem and cast in the form of a linear vector equation $A\\psi = b$ to be solved by using the quantum signal processing algorithm.","The latter requires encoding of the matrix $A$ in a quantum circuit as a subblock of a unitary matrix.","We propose how to encode $A$ in a circuit in a compressed form and discuss how the resulting circuit scales with the problem size and the desired precision."],"url":"http://arxiv.org/abs/2403.11989v1","category":"physics.plasm-ph"}
{"created":"2024-03-18 17:24:42","title":"Measurement-driven Langevin modeling of superparamagnetic tunnel junctions","abstract":"Superparamagnetic tunnel junctions are important devices for a range of emerging technologies, but most existing compact models capture only their mean switching rates. Capturing qualitatively accurate analog dynamics of these devices will be important as the technology scales up. Here we present results using a one-dimensional overdamped Langevin equation that captures statistical properties of measured time traces, including voltage histograms, drift and diffusion characteristics as measured with Kramers-Moyal coefficients, and dwell times distributions. While common macrospin models are more physically-motivated magnetic models than the Langevin model, we show that for the device measured here, they capture even fewer of the measured experimental behaviors.","sentences":["Superparamagnetic tunnel junctions are important devices for a range of emerging technologies, but most existing compact models capture only their mean switching rates.","Capturing qualitatively accurate analog dynamics of these devices will be important as the technology scales up.","Here we present results using a one-dimensional overdamped Langevin equation that captures statistical properties of measured time traces, including voltage histograms, drift and diffusion characteristics as measured with Kramers-Moyal coefficients, and dwell times distributions.","While common macrospin models are more physically-motivated magnetic models than the Langevin model, we show that for the device measured here, they capture even fewer of the measured experimental behaviors."],"url":"http://arxiv.org/abs/2403.11988v1","category":"physics.app-ph"}
{"created":"2024-03-18 17:00:40","title":"Extrinsically homogeneous Lagrangian submanifolds of the pseudo-nearly K\u00e4hler $\\mathrm{SL}(2,\\mathbb{R})\\times\\mathrm{SL}(2,\\mathbb{R})$","abstract":"We consider the pseudo-nearly K\\\"ahler $\\mathrm{SL}(2,\\mathbb{R})\\times\\mathrm{SL}(2,\\mathbb{R})$ and we study its Lagrangian submanifolds. We provide examples of Lagrangian submanifolds which do not have an analogue in $\\mathbb{S}^3\\times\\mathbb{S}^3$. We also provide an expression for the isometry group of $\\mathrm{SL}(2,\\mathbb{R})\\times\\mathrm{SL}(2,\\mathbb{R})$ with the pseudo-Riemannian nearly K\\\"ahler metric. The main result is a complete classification of extrinsically homogeneous Lagrangian submanifolds in this space.","sentences":["We consider the pseudo-nearly K\\\"ahler $\\mathrm{SL}(2,\\mathbb{R})\\times\\mathrm{SL}(2,\\mathbb{R})$ and we study its Lagrangian submanifolds.","We provide examples of Lagrangian submanifolds which do not have an analogue in $\\mathbb{S}^3\\times\\mathbb{S}^3$. We also provide an expression for the isometry group of $\\mathrm{SL}(2,\\mathbb{R})\\times\\mathrm{SL}(2,\\mathbb{R})$ with the pseudo-Riemannian nearly K\\\"ahler metric.","The main result is a complete classification of extrinsically homogeneous Lagrangian submanifolds in this space."],"url":"http://arxiv.org/abs/2403.11962v1","category":"math.DG"}
{"created":"2024-03-18 16:52:50","title":"Relative aspherical conjecture and higher codimensional obstruction to positive scalar curvature","abstract":"Motivated by the solution of the aspherical conjecture up to dimension 5 [CL20][Gro20], we want to study a relative version of the aspherical conjecture. We present a natural condition generalizing the model $X\\times\\mathbb{T}^k$ to the relative aspherical setting. Such model is closely related to submanifold obstruction of positive scalar curvature (PSC), and would be in similar spirit as [HPS15][CRZ23] in codim 2 case. In codim 3 and 4, we prove results on how 3-manifold obstructs the existence of PSC under our relative aspherical condition. This could be regarded as a relative version extension of the aspherical conjecture up to dim 5.","sentences":["Motivated by the solution of the aspherical conjecture up to dimension 5","[CL20][Gro20], we want to study a relative version of the aspherical conjecture.","We present a natural condition generalizing the model $X\\times\\mathbb{T}^k$ to the relative aspherical setting.","Such model is closely related to submanifold obstruction of positive scalar curvature (PSC), and would be in similar spirit as [HPS15][CRZ23] in codim 2 case.","In codim 3 and 4, we prove results on how 3-manifold obstructs the existence of PSC under our relative aspherical condition.","This could be regarded as a relative version extension of the aspherical conjecture up to dim 5."],"url":"http://arxiv.org/abs/2403.11957v1","category":"math.DG"}
{"created":"2024-03-18 16:51:56","title":"Robust Estimation and Inference in Categorical Data","abstract":"In empirical science, many variables of interest are categorical. Like any model, models for categorical responses can be misspecified, leading to possibly large biases in estimation. One particularly troublesome source of misspecification is inattentive responding in questionnaires, which is well-known to jeopardize the validity of structural equation models (SEMs) and other survey-based analyses. I propose a general estimator that is designed to be robust to misspecification of models for categorical responses. Unlike hitherto approaches, the estimator makes no assumption whatsoever on the degree, magnitude, or type of misspecification. The proposed estimator generalizes maximum likelihood estimation, is strongly consistent, asymptotically Gaussian, has the same time complexity as maximum likelihood, and can be applied to any model for categorical responses. In addition, I develop a novel test that tests whether a given response can be fitted well by the assumed model, which allows one to trace back possible sources of misspecification. I verify the attractive theoretical properties of the proposed methodology in Monte Carlo experiments, and demonstrate its practical usefulness in an empirical application on a SEM of personality traits, where I find compelling evidence for the presence of inattentive responding whose adverse effects the proposed estimator can withstand, unlike maximum likelihood.","sentences":["In empirical science, many variables of interest are categorical.","Like any model, models for categorical responses can be misspecified, leading to possibly large biases in estimation.","One particularly troublesome source of misspecification is inattentive responding in questionnaires, which is well-known to jeopardize the validity of structural equation models (SEMs) and other survey-based analyses.","I propose a general estimator that is designed to be robust to misspecification of models for categorical responses.","Unlike hitherto approaches, the estimator makes no assumption whatsoever on the degree, magnitude, or type of misspecification.","The proposed estimator generalizes maximum likelihood estimation, is strongly consistent, asymptotically Gaussian, has the same time complexity as maximum likelihood, and can be applied to any model for categorical responses.","In addition, I develop a novel test that tests whether a given response can be fitted well by the assumed model, which allows one to trace back possible sources of misspecification.","I verify the attractive theoretical properties of the proposed methodology in Monte Carlo experiments, and demonstrate its practical usefulness in an empirical application on a SEM of personality traits, where I find compelling evidence for the presence of inattentive responding whose adverse effects the proposed estimator can withstand, unlike maximum likelihood."],"url":"http://arxiv.org/abs/2403.11954v1","category":"stat.ME"}
{"created":"2024-03-18 16:44:16","title":"Higher time-derivative theories from space-time interchanged integrable field theories","abstract":"We compare a relativistic and a nonrelativistic version of Ostrogradsky's method for higher-time derivative theories extended to scalar field theories and consider as an alternative a multi-field variant. We apply the schemes to space-time rotated modified Korteweg-de Vries systems and, exploiting their integrability, to Hamiltonian systems built from space-time rotated inverse Legendre transformed higher-order charges of these systems. We derive the equal-time Poisson bracket structures of these theories, establish the integrability of the latter theories by means of the Painlev\\'e test and construct exact analytical period benign solutions in terms of Jacobi elliptic functions to the classical equations of motion. The classical energies of these partially complex solutions are real when they respect a certain modified CPT-symmetry and complex when this symmetry is broken. The higher order Cauchy and initial-boundary value problem are addressed analytically and numerically. Finally, we provide the explicit quantization of the simplest mKdV system, exhibiting the usual conundrum of having the choice between either having to deal with a theory that includes non-normalizable states or spectra that are unbounded from below. In our non-Hermitian system the choice is dictated by the correct sign in the decay width.","sentences":["We compare a relativistic and a nonrelativistic version of Ostrogradsky's method for higher-time derivative theories extended to scalar field theories and consider as an alternative a multi-field variant.","We apply the schemes to space-time rotated modified Korteweg-de Vries systems and, exploiting their integrability, to Hamiltonian systems built from space-time rotated inverse Legendre transformed higher-order charges of these systems.","We derive the equal-time Poisson bracket structures of these theories, establish the integrability of the latter theories by means of the Painlev\\'e test and construct exact analytical period benign solutions in terms of Jacobi elliptic functions to the classical equations of motion.","The classical energies of these partially complex solutions are real when they respect a certain modified CPT-symmetry and complex when this symmetry is broken.","The higher order Cauchy and initial-boundary value problem are addressed analytically and numerically.","Finally, we provide the explicit quantization of the simplest mKdV system, exhibiting the usual conundrum of having the choice between either having to deal with a theory that includes non-normalizable states or spectra that are unbounded from below.","In our non-Hermitian system the choice is dictated by the correct sign in the decay width."],"url":"http://arxiv.org/abs/2403.11949v1","category":"hep-th"}
{"created":"2024-03-18 16:36:01","title":"Multistep Inverse Is Not All You Need","abstract":"In real-world control settings, the observation space is often unnecessarily high-dimensional and subject to time-correlated noise. However, the controllable dynamics of the system are often far simpler than the dynamics of the raw observations. It is therefore desirable to learn an encoder to map the observation space to a simpler space of control-relevant variables. In this work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022), which formalizes control problems where observations can be factorized into an action-dependent latent state which evolves deterministically, and action-independent time-correlated noise. Lamb et al. (2022) proposes the \"AC-State\" method for learning an encoder to extract a complete action-dependent latent state representation from the observations in such problems. AC-State is a multistep-inverse method, in that it uses the encoding of the the first and last state in a path to predict the first action in the path. However, we identify cases where AC-State will fail to learn a correct latent representation of the agent-controllable factor of the state. We therefore propose a new algorithm, ACDF, which combines multistep-inverse prediction with a latent forward model. ACDF is guaranteed to correctly infer an action-dependent latent state encoder for a large class of Ex-BMDP models. We demonstrate the effectiveness of ACDF on tabular Ex-BMDPs through numerical simulations; as well as high-dimensional environments using neural-network-based encoders. Code is available at https://github.com/midi-lab/acdf.","sentences":["In real-world control settings, the observation space is often unnecessarily high-dimensional and subject to time-correlated noise.","However, the controllable dynamics of the system are often far simpler than the dynamics of the raw observations.","It is therefore desirable to learn an encoder to map the observation space to a simpler space of control-relevant variables.","In this work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022), which formalizes control problems where observations can be factorized into an action-dependent latent state which evolves deterministically, and action-independent time-correlated noise.","Lamb et al. (2022) proposes the \"AC-State\" method for learning an encoder to extract a complete action-dependent latent state representation from the observations in such problems.","AC-State is a multistep-inverse method, in that it uses the encoding of the the first and last state in a path to predict the first action in the path.","However, we identify cases where AC-State will fail to learn a correct latent representation of the agent-controllable factor of the state.","We therefore propose a new algorithm, ACDF, which combines multistep-inverse prediction with a latent forward model.","ACDF is guaranteed to correctly infer an action-dependent latent state encoder for a large class of Ex-BMDP models.","We demonstrate the effectiveness of ACDF on tabular Ex-BMDPs through numerical simulations; as well as high-dimensional environments using neural-network-based encoders.","Code is available at https://github.com/midi-lab/acdf."],"url":"http://arxiv.org/abs/2403.11940v1","category":"cs.LG"}
{"created":"2024-03-18 16:35:13","title":"State space representations of the Roesser type for convolutional layers","abstract":"From the perspective of control theory, convolutional layers (of neural networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual representation of convolutional layers by the convolution kernel corresponds to the representation of a dynamical system by its impulse response. However, many analysis tools from control theory, e.g., involving linear matrix inequalities, require a state space representation. For this reason, we explicitly provide a state space representation of the Roesser type for 2-D convolutional layers with $c_\\mathrm{in}r_1 + c_\\mathrm{out}r_2$ states, where $c_\\mathrm{in}$/$c_\\mathrm{out}$ is the number of input/output channels of the layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel. This representation is shown to be minimal for $c_\\mathrm{in} = c_\\mathrm{out}$. We further construct state space representations for dilated, strided, and N-D convolutions.","sentences":["From the perspective of control theory, convolutional layers (of neural networks) are 2-D (or N-D) linear time-invariant dynamical systems.","The usual representation of convolutional layers by the convolution kernel corresponds to the representation of a dynamical system by its impulse response.","However, many analysis tools from control theory, e.g., involving linear matrix inequalities, require a state space representation.","For this reason, we explicitly provide a state space representation of the Roesser type for 2-D convolutional layers with $c_\\mathrm{in}r_1 + c_\\mathrm{out}r_2$ states, where $c_\\mathrm{in}$/$c_\\mathrm{out}$ is the number of input/output channels of the layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel.","This representation is shown to be minimal for $c_\\mathrm{in} = c_\\mathrm{out}$. We further construct state space representations for dilated, strided, and N-D convolutions."],"url":"http://arxiv.org/abs/2403.11938v1","category":"eess.SY"}
{"created":"2024-03-18 16:34:38","title":"Regularity and nondegeneracy for nonlocal Bernoulli problems with variable kernels","abstract":"We consider a generalization of the Bernoulli free boundary problem where the underlying differential operator is a nonlocal, non-translation-invariant elliptic operator of order $2s\\in (0,2)$. Because of the lack of translation invariance, the Caffarelli-Silvestre extension is unavailable, and we must work with the nonlocal problem directly instead of transforming to a thin free boundary problem. We prove global H\\\"older continuity of minimizers for both the one- and two-phase problems. Next, for the one-phase problem, we show H\\\"older continuity at the free boundary with the optimal exponent $s$. We also prove matching nondegeneracy estimates. A key novelty of our work is that all our findings hold without requiring any regularity assumptions on the kernel of the nonlocal operator. This characteristic makes them crucial in the development of a universal regularity theory for nonlocal free boundary problems.","sentences":["We consider a generalization of the Bernoulli free boundary problem where the underlying differential operator is a nonlocal, non-translation-invariant elliptic operator of order $2s\\in (0,2)$. Because of the lack of translation invariance, the Caffarelli-Silvestre extension is unavailable, and we must work with the nonlocal problem directly instead of transforming to a thin free boundary problem.","We prove global H\\\"older continuity of minimizers for both the one- and two-phase problems.","Next, for the one-phase problem, we show H\\\"older continuity at the free boundary with the optimal exponent $s$. We also prove matching nondegeneracy estimates.","A key novelty of our work is that all our findings hold without requiring any regularity assumptions on the kernel of the nonlocal operator.","This characteristic makes them crucial in the development of a universal regularity theory for nonlocal free boundary problems."],"url":"http://arxiv.org/abs/2403.11937v1","category":"math.AP"}
{"created":"2024-03-18 16:30:38","title":"Quantitative phase microscopies: accuracy comparison","abstract":"This article presents a thorough comparison of themain QPM techniques, focusing on their accuracy in terms of measurement precision and trueness. We focus on 8 techniques, namely digital holographic microscopy (DHM), cross-grating wavefront microscopy (CGM), which is based on QLSI (quadriwave lateral shearing interferometry), diffraction phase microscopy (DPM), differential phase-contrast (DPC) microscopy, phase-shifting interferometry (PSI) imaging, Fourier phase microscopy (FPM), spatial light interference microscopy (SLIM), and transport-of-intensity equation (TIE) imaging. For this purpose, we used a home-made numerical toolbox based on discrete dipole approximation (IF-DDA). This toolbox is designed to compute the electromagnetic field at the sample plane of a microscope, irrespective of the object's complexity or the illumination conditions. We upgraded this toolbox to enable it to model any type of QPM, and to take into account shot noise. In a nutshell, the results show that DHM and PSI are inherently free from artefacts and rather suffer from coherent noise; In CGM, DPC, DPM and TIE, there is a trade off between precision and trueness, which can be balanced by varying one experimental parameter; FPM and SLIM suffer from inherent artefacts that cannot be discarded experimentally in most cases, making the techniques not quantitative especially for large objects covering a large part of the field of view, such as eukaryotic cells.","sentences":["This article presents a thorough comparison of themain QPM techniques, focusing on their accuracy in terms of measurement precision and trueness.","We focus on 8 techniques, namely digital holographic microscopy (DHM), cross-grating wavefront microscopy (CGM), which is based on QLSI (quadriwave lateral shearing interferometry), diffraction phase microscopy (DPM), differential phase-contrast (DPC) microscopy, phase-shifting interferometry (PSI) imaging, Fourier phase microscopy (FPM), spatial light interference microscopy (SLIM), and transport-of-intensity equation (TIE) imaging.","For this purpose, we used a home-made numerical toolbox based on discrete dipole approximation (IF-DDA).","This toolbox is designed to compute the electromagnetic field at the sample plane of a microscope, irrespective of the object's complexity or the illumination conditions.","We upgraded this toolbox to enable it to model any type of QPM, and to take into account shot noise.","In a nutshell, the results show that DHM and PSI are inherently free from artefacts and rather suffer from coherent noise; In CGM, DPC, DPM and TIE, there is a trade off between precision and trueness, which can be balanced by varying one experimental parameter; FPM and SLIM suffer from inherent artefacts that cannot be discarded experimentally in most cases, making the techniques not quantitative especially for large objects covering a large part of the field of view, such as eukaryotic cells."],"url":"http://arxiv.org/abs/2403.11930v1","category":"physics.optics"}
{"created":"2024-03-18 16:28:26","title":"Langevin equation in heterogeneous landscapes: how to choose the interpretation","abstract":"The Langevin equation is a common tool to model diffusion at a single-particle level. In non-homogeneous environments, such as aqueous two-phase systems or biological condensates with different diffusion coefficients in different phases, the solution to a Langevin equation is not unique unless the interpretation of stochastic integrals involved is selected. We analyze the diffusion of particles in such systems and evaluate the mean, the mean square displacement, and the distribution of particles, as well as the variance of the time-averaged mean-squared displacements. Our analytical results provide a method to choose the interpretation parameter from single particle tracking experiments.","sentences":["The Langevin equation is a common tool to model diffusion at a single-particle level.","In non-homogeneous environments, such as aqueous two-phase systems or biological condensates with different diffusion coefficients in different phases, the solution to a Langevin equation is not unique unless the interpretation of stochastic integrals involved is selected.","We analyze the diffusion of particles in such systems and evaluate the mean, the mean square displacement, and the distribution of particles, as well as the variance of the time-averaged mean-squared displacements.","Our analytical results provide a method to choose the interpretation parameter from single particle tracking experiments."],"url":"http://arxiv.org/abs/2403.11928v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-18 16:23:46","title":"Exploring Dielectric Properties in Models of Amorphous Boron Nitride","abstract":"We report a theoretical study of dielectric properties of models of amorphous Boron Nitride, using interatomic potentials generated by machine learning. We first perform first-principles simulations on small (about $100$ atoms in the periodic cell) sample sizes to explore the emergence of mid-gap states and its correlation with structural features. Next, by using a simplified tight-binding electronic model, we analyse the dielectric functions for complex three dimensional models (containing about $10.000$ atoms) embedding varying concentrations of ${\\rm sp^{1}, sp^{2}}$ and ${\\rm sp^3}$ bonds between B and N atoms. Within the limits of these methodologies, the resulting value of the zero-frequency dielectric constant is shown to be influenced by the population density of such mid-gap states and their localization characteristics. We observe nontrivial correlations between the structure-induced electronic fluctuations and the resulting dielectric constant values. Our findings are however just a first step in the quest of accessing fully accurate dielectric properties of as-grown amorphous BN of relevance for interconnect technologies and beyond.","sentences":["We report a theoretical study of dielectric properties of models of amorphous Boron Nitride, using interatomic potentials generated by machine learning.","We first perform first-principles simulations on small (about $100$ atoms in the periodic cell) sample sizes to explore the emergence of mid-gap states and its correlation with structural features.","Next, by using a simplified tight-binding electronic model, we analyse the dielectric functions for complex three dimensional models (containing about $10.000$ atoms) embedding varying concentrations of ${\\rm sp^{1}, sp^{2}}$ and ${\\rm sp^3}$ bonds between B and N atoms.","Within the limits of these methodologies, the resulting value of the zero-frequency dielectric constant is shown to be influenced by the population density of such mid-gap states and their localization characteristics.","We observe nontrivial correlations between the structure-induced electronic fluctuations and the resulting dielectric constant values.","Our findings are however just a first step in the quest of accessing fully accurate dielectric properties of as-grown amorphous BN of relevance for interconnect technologies and beyond."],"url":"http://arxiv.org/abs/2403.11924v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 16:18:57","title":"The hybrid anti-symmetrized coupled channels method (haCC) for the tRecX code","abstract":"We present a new implementation of the hybrid antisymmetrized Coupled Channels (haCC) method in the framework of the tRecX [A. Scrinzi, Comp. Phys. Comm., 270:108146, 2022.]. The method represents atomic and molecular multi-electron functions by combining CI functions, Gaussian molecular orbitals, and a numerical single-electron basis. It is suitable for describing high harmonic generation and the strong-field dynamics of ionization. Fully differential photo\\-emission spectra are computed by the tSurff method. The theoretical background of haCC is outlined and key improvements compared to its original formulation are highlighted. We discuss control of over-completeness resulting from the joint use of the numerical basis and Gaussian molecular orbitals by pseudo-inverses based on the Woodbury formula. Further new features of this tRecX release are the iSurff method, new input features, and the AMOS gateway interface. The mapping of haCC into the tRecX framework for solving the time-dependent Schr\\\"odinger equation is shown. Use, performance, and accuracy of haCC are discussed on the examples of high-harmonic generation and strong-field photo-emission by short laser pulses impinging on the Helium atom and on the linear molecules $N_2$ and $CO$.","sentences":["We present a new implementation of the hybrid antisymmetrized Coupled Channels (haCC) method in the framework of the tRecX [A. Scrinzi, Comp. Phys.","Comm., 270:108146, 2022.].","The method represents atomic and molecular multi-electron functions by combining CI functions, Gaussian molecular orbitals, and a numerical single-electron basis.","It is suitable for describing high harmonic generation and the strong-field dynamics of ionization.","Fully differential photo\\-emission spectra are computed by the tSurff method.","The theoretical background of haCC is outlined and key improvements compared to its original formulation are highlighted.","We discuss control of over-completeness resulting from the joint use of the numerical basis and Gaussian molecular orbitals by pseudo-inverses based on the Woodbury formula.","Further new features of this tRecX release are the iSurff method, new input features, and the AMOS gateway interface.","The mapping of haCC into the tRecX framework for solving the time-dependent Schr\\\"odinger equation is shown.","Use, performance, and accuracy of haCC are discussed on the examples of high-harmonic generation and strong-field photo-emission by short laser pulses impinging on the Helium atom and on the linear molecules $N_2$ and $CO$."],"url":"http://arxiv.org/abs/2403.11918v1","category":"physics.comp-ph"}
{"created":"2024-03-18 16:18:36","title":"Well-posedness of stochastic evolution equations with H\u00f6lder continuous noise","abstract":"We show existence and pathwise uniqueness of probabilistically strong solutions to a pseudomonotone stochastic evolution problem on a bounded domain $D\\subseteq\\mathbb{R}^d$, $d\\in\\mathbb{N}$, with homogeneous Dirichlet boundary conditions and random initial data $u_0\\in L^2(\\Omega;L^2(D))$. The main novelty is the presence of a merely H\\\"older continuous multiplicative noise term. In order to show the well-posedness, we simultaneously regularize the H\\\"older noise term by inf-convolution and add a perturbation by a higher order operator to the equation. Using a stochastic compactness argument we may pass to the limit and we obtain first a martingale solution. Then by a pathwise uniqueness argument we get existence of a probabilistically strong solution.","sentences":["We show existence and pathwise uniqueness of probabilistically strong solutions to a pseudomonotone stochastic evolution problem on a bounded domain $D\\subseteq\\mathbb{R}^d$, $d\\in\\mathbb{N}$, with homogeneous Dirichlet boundary conditions and random initial data $u_0\\in L^2(\\Omega;L^2(D))$. The main novelty is the presence of a merely H\\\"older continuous multiplicative noise term.","In order to show the well-posedness, we simultaneously regularize the H\\\"older noise term by inf-convolution and add a perturbation by a higher order operator to the equation.","Using a stochastic compactness argument we may pass to the limit and we obtain first a martingale solution.","Then by a pathwise uniqueness argument we get existence of a probabilistically strong solution."],"url":"http://arxiv.org/abs/2403.11917v1","category":"math.PR"}
{"created":"2024-03-18 16:14:28","title":"Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target","abstract":"Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and local (Equilibrium propagation) learning rules, achieves a 97.6% test accuracy on the MNIST dataset. Furthermore, we demonstrate that incorporating a hidden layer enhances classification accuracy and the quality of learned features across all training methods, showcasing the advantages of end-to-end unsupervised training. Extending to semi-supervised learning, our method dynamically adjusts the target according to data availability, reaching a 96.6% accuracy with just 600 labeled MNIST samples. This result highlights our 'unsupervised target' strategy's efficacy and flexibility in scenarios ranging from abundant to no labeled data availability.","sentences":["Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning.","Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets.","To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism.","This approach, framework-agnostic and compatible with both global (Backpropagation) and local (Equilibrium propagation) learning rules, achieves a 97.6% test accuracy on the MNIST dataset.","Furthermore, we demonstrate that incorporating a hidden layer enhances classification accuracy and the quality of learned features across all training methods, showcasing the advantages of end-to-end unsupervised training.","Extending to semi-supervised learning, our method dynamically adjusts the target according to data availability, reaching a 96.6% accuracy with just 600 labeled MNIST samples.","This result highlights our 'unsupervised target' strategy's efficacy and flexibility in scenarios ranging from abundant to no labeled data availability."],"url":"http://arxiv.org/abs/2403.12116v1","category":"cs.NE"}
{"created":"2024-03-18 16:13:02","title":"Single-Agent Actor Critic for Decentralized Cooperative Driving","abstract":"Active traffic management incorporating autonomous vehicles (AVs) promises a future with diminished congestion and enhanced traffic flow. However, developing algorithms for real-world application requires addressing the challenges posed by continuous traffic flow and partial observability. To bridge this gap and advance the field of active traffic management towards greater decentralization, we introduce a novel asymmetric actor-critic model aimed at learning decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. Our approach employs attention neural networks with masking to handle the dynamic nature of real-world traffic flow and partial observability. Through extensive evaluations against baseline controllers across various traffic scenarios, our model shows great potential for improving traffic flow at diverse bottleneck locations within the road system. Additionally, we explore the challenge associated with the conservative driving behaviors of autonomous vehicles that adhere strictly to traffic regulations. The experiment results illustrate that our proposed cooperative policy can mitigate potential traffic slowdowns without compromising safety.","sentences":["Active traffic management incorporating autonomous vehicles (AVs) promises a future with diminished congestion and enhanced traffic flow.","However, developing algorithms for real-world application requires addressing the challenges posed by continuous traffic flow and partial observability.","To bridge this gap and advance the field of active traffic management towards greater decentralization, we introduce a novel asymmetric actor-critic model aimed at learning decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning.","Our approach employs attention neural networks with masking to handle the dynamic nature of real-world traffic flow and partial observability.","Through extensive evaluations against baseline controllers across various traffic scenarios, our model shows great potential for improving traffic flow at diverse bottleneck locations within the road system.","Additionally, we explore the challenge associated with the conservative driving behaviors of autonomous vehicles that adhere strictly to traffic regulations.","The experiment results illustrate that our proposed cooperative policy can mitigate potential traffic slowdowns without compromising safety."],"url":"http://arxiv.org/abs/2403.11914v1","category":"cs.LG"}
{"created":"2024-03-18 16:11:43","title":"Numerical method for nonlinear Kolmogorov PDEs via sensitivity analysis","abstract":"We examine nonlinear Kolmogorov partial differential equations (PDEs). Here the nonlinear part of the PDE comes from its Hamiltonian where one maximizes over all possible drift and diffusion coefficients which fall within a $\\varepsilon$-neighborhood of pre-specified baseline coefficients. Our goal is to quantify and compute how sensitive those PDEs are to such a small nonlinearity, and then use the results to develop an efficient numerical method for their approximation. We show that as $\\varepsilon\\downarrow 0$, the nonlinear Kolmogorov PDE equals the linear Kolmogorov PDE defined with respect to the corresponding baseline coefficients plus $\\varepsilon$ times a correction term which can be also characterized by the solution of another linear Kolmogorov PDE involving the baseline coefficients. As these linear Kolmogorov PDEs can be efficiently solved in high-dimensions by exploiting their Feynman-Kac representation, our derived sensitivity analysis then provides a Monte Carlo based numerical method which can efficiently solve these nonlinear Kolmogorov equations. We provide numerical examples in up to 100 dimensions to empirically demonstrate the applicability of our numerical method.","sentences":["We examine nonlinear Kolmogorov partial differential equations (PDEs).","Here the nonlinear part of the PDE comes from its Hamiltonian where one maximizes over all possible drift and diffusion coefficients which fall within a $\\varepsilon$-neighborhood of pre-specified baseline coefficients.","Our goal is to quantify and compute how sensitive those PDEs are to such a small nonlinearity, and then use the results to develop an efficient numerical method for their approximation.","We show that as $\\varepsilon\\downarrow 0$, the nonlinear Kolmogorov PDE equals the linear Kolmogorov PDE defined with respect to the corresponding baseline coefficients plus $\\varepsilon$ times a correction term which can be also characterized by the solution of another linear Kolmogorov PDE involving the baseline coefficients.","As these linear Kolmogorov PDEs can be efficiently solved in high-dimensions by exploiting their Feynman-Kac representation, our derived sensitivity analysis then provides a Monte Carlo based numerical method which can efficiently solve these nonlinear Kolmogorov equations.","We provide numerical examples in up to 100 dimensions to empirically demonstrate the applicability of our numerical method."],"url":"http://arxiv.org/abs/2403.11910v1","category":"math.NA"}
{"created":"2024-03-18 16:11:42","title":"RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF","abstract":"Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset.","sentences":["Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis.","Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration.","One approach to mitigate this issue is to enhance images post-rendering.","2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation.","Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images.","We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms.","Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion.","Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration.","We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset."],"url":"http://arxiv.org/abs/2403.11909v1","category":"cs.CV"}
{"created":"2024-03-18 16:11:25","title":"Isotropization of a longitudinally expanding system of scalar fields in the 2PI formalism","abstract":"Motivated by isotropization of QCD matter in the initial stages of heavy-ion collisions, we consider a system of scalar fields that undergoes a boost invariant longitudinal expansion. We use the framework of the two-particle irreducible (2PI) effective action, which is close to the underlying quantum field theory, and resum self-energy corrections up to three loops. The resulting 2PI equations of motion are expressed in terms of the Milne coordinates to account for longitudinal expansion. By solving numerically these equations of motion, we can extract the occupation density and the effective mass generated by in-medium interactions. At the largest values of the coupling considered in this study, we observe the onset of isotropization both in the occupation number and in the momentum dependence of the effective mass.","sentences":["Motivated by isotropization of QCD matter in the initial stages of heavy-ion collisions, we consider a system of scalar fields that undergoes a boost invariant longitudinal expansion.","We use the framework of the two-particle irreducible (2PI) effective action, which is close to the underlying quantum field theory, and resum self-energy corrections up to three loops.","The resulting 2PI equations of motion are expressed in terms of the Milne coordinates to account for longitudinal expansion.","By solving numerically these equations of motion, we can extract the occupation density and the effective mass generated by in-medium interactions.","At the largest values of the coupling considered in this study, we observe the onset of isotropization both in the occupation number and in the momentum dependence of the effective mass."],"url":"http://arxiv.org/abs/2403.11908v1","category":"hep-ph"}
{"created":"2024-03-18 16:09:49","title":"Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers","abstract":"Demand-side flexibility is gaining importance as a crucial element in the energy transition process. Accounting for about 25% of final energy consumption globally, the residential sector is an important (potential) source of energy flexibility. However, unlocking this flexibility requires developing a control framework that (1) easily scales across different houses, (2) is easy to maintain, and (3) is simple to understand for end-users. A potential control framework for such a task is data-driven control, specifically model-free reinforcement learning (RL). Such RL-based controllers learn a good control policy by interacting with their environment, learning purely based on data and with minimal human intervention. Yet, they lack explainability, which hampers user acceptance. Moreover, limited hardware capabilities of residential assets forms a hurdle (e.g., using deep neural networks). To overcome both those challenges, we propose a novel method to obtain explainable RL policies by using differentiable decision trees. Using a policy distillation approach, we train these differentiable decision trees to mimic standard RL-based controllers, leading to a decision tree-based control policy that is data-driven and easy to explain. As a proof-of-concept, we examine the performance and explainability of our proposed approach in a battery-based home energy management system to reduce energy costs. For this use case, we show that our proposed approach can outperform baseline rule-based policies by about 20-25%, while providing simple, explainable control policies. We further compare these explainable policies with standard RL policies and examine the performance trade-offs associated with this increased explainability.","sentences":["Demand-side flexibility is gaining importance as a crucial element in the energy transition process.","Accounting for about 25% of final energy consumption globally, the residential sector is an important (potential) source of energy flexibility.","However, unlocking this flexibility requires developing a control framework that (1) easily scales across different houses, (2) is easy to maintain, and (3) is simple to understand for end-users.","A potential control framework for such a task is data-driven control, specifically model-free reinforcement learning (RL).","Such RL-based controllers learn a good control policy by interacting with their environment, learning purely based on data and with minimal human intervention.","Yet, they lack explainability, which hampers user acceptance.","Moreover, limited hardware capabilities of residential assets forms a hurdle (e.g., using deep neural networks).","To overcome both those challenges, we propose a novel method to obtain explainable RL policies by using differentiable decision trees.","Using a policy distillation approach, we train these differentiable decision trees to mimic standard RL-based controllers, leading to a decision tree-based control policy that is data-driven and easy to explain.","As a proof-of-concept, we examine the performance and explainability of our proposed approach in a battery-based home energy management system to reduce energy costs.","For this use case, we show that our proposed approach can outperform baseline rule-based policies by about 20-25%, while providing simple, explainable control policies.","We further compare these explainable policies with standard RL policies and examine the performance trade-offs associated with this increased explainability."],"url":"http://arxiv.org/abs/2403.11907v1","category":"eess.SY"}
{"created":"2024-03-18 16:00:21","title":"Radiative loss and ion-neutral collisional effects in astrophysical plasmas","abstract":"In this paper we study the role of radiative cooling in a two-fluid model consisting of coupled neutrals and charged particles. We first analyze the linearized two-fluid equations where we include radiative losses in the energy equation for the charged particles. In a 1D geometry for parallel propagation and in the limiting cases of weak and strong coupling, it can be shown analytically that the instability conditions for the thermal mode and the sound waves, the isobaric and isentropic criteria, respectively, remain unchanged with respect to one-fluid radiative plasmas. For the parameters considered in this paper, representative for the solar corona, the radiative cooling produces growth of the thermal mode and damping of the sound waves. When neutrals are included and are sufficiently coupled to the charges, the thermal mode growth rate and the wave damping both reduce by the same factor, which depends on the ionization fraction only. For a heating function which is constant in time, we find that the growth of the thermal mode and the damping of the sound waves are slightly larger. The numerical calculation of the eigenvalues of the general system of equations in a 3D geometry confirm the analytic results. We then run 2D fully nonlinear simulations which give consistent results: a higher ionization fraction or lower coupling will increase the growth rate. The magnetic field contribution is negligible in the linear phase. Ionization-recombination effects might play an important role because the radiative cooling produces a large range of temperatures in the system. In the numerical simulation, after the first condensation phase, when the minimum temperature is reached, the fraction of neutrals increases four orders of magnitude because of the recombination.","sentences":["In this paper we study the role of radiative cooling in a two-fluid model consisting of coupled neutrals and charged particles.","We first analyze the linearized two-fluid equations where we include radiative losses in the energy equation for the charged particles.","In a 1D geometry for parallel propagation and in the limiting cases of weak and strong coupling, it can be shown analytically that the instability conditions for the thermal mode and the sound waves, the isobaric and isentropic criteria, respectively, remain unchanged with respect to one-fluid radiative plasmas.","For the parameters considered in this paper, representative for the solar corona, the radiative cooling produces growth of the thermal mode and damping of the sound waves.","When neutrals are included and are sufficiently coupled to the charges, the thermal mode growth rate and the wave damping both reduce by the same factor, which depends on the ionization fraction only.","For a heating function which is constant in time, we find that the growth of the thermal mode and the damping of the sound waves are slightly larger.","The numerical calculation of the eigenvalues of the general system of equations in a 3D geometry confirm the analytic results.","We then run 2D fully nonlinear simulations which give consistent results: a higher ionization fraction or lower coupling will increase the growth rate.","The magnetic field contribution is negligible in the linear phase.","Ionization-recombination effects might play an important role because the radiative cooling produces a large range of temperatures in the system.","In the numerical simulation, after the first condensation phase, when the minimum temperature is reached, the fraction of neutrals increases four orders of magnitude because of the recombination."],"url":"http://arxiv.org/abs/2403.11900v1","category":"astro-ph.SR"}
{"created":"2024-03-18 15:58:03","title":"GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors","abstract":"Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.","sentences":["Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS).","Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes.","However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry.","To address the challenges, we propose a Gaussian-based representation of normals in SDF fields.","Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods.","Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors.","To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry.","We also evaluated our framework on the PANDORA dataset.","Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin."],"url":"http://arxiv.org/abs/2403.11899v1","category":"cs.CV"}
{"created":"2024-03-18 15:45:18","title":"Kelvin equation for bridging transitions","abstract":"We study bridging transitions between a pair of non-planar surfaces. We show that the transition can be described using a generalized Kelvin equation by mapping the system to a slit of finite length. The proposed equation is applied to analyze the asymptotic behaviour of the growth of the bridging film, which occurs when the confining walls are gradually flattened. This phenomenon is characterized by a power-law divergence with geometry-dependent critical exponents that we determine for a wide class of walls' geometries. In particular, for a linear-wedge model, a covariance law revealing a relation between a geometric and Young's contact angle is presented. These predictions are shown to be fully in line with the numerical results obtained from a microscopic (classical) density functional theory.","sentences":["We study bridging transitions between a pair of non-planar surfaces.","We show that the transition can be described using a generalized Kelvin equation by mapping the system to a slit of finite length.","The proposed equation is applied to analyze the asymptotic behaviour of the growth of the bridging film, which occurs when the confining walls are gradually flattened.","This phenomenon is characterized by a power-law divergence with geometry-dependent critical exponents that we determine for a wide class of walls' geometries.","In particular, for a linear-wedge model, a covariance law revealing a relation between a geometric and Young's contact angle is presented.","These predictions are shown to be fully in line with the numerical results obtained from a microscopic (classical) density functional theory."],"url":"http://arxiv.org/abs/2403.11889v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-18 15:43:45","title":"Deep learning automates Cobb angle measurement compared with multi-expert observers","abstract":"Scoliosis, a prevalent condition characterized by abnormal spinal curvature leading to deformity, requires precise assessment methods for effective diagnosis and management. The Cobb angle is a widely used scoliosis quantification method that measures the degree of curvature between the tilted vertebrae. Yet, manual measuring of Cobb angles is time-consuming and labor-intensive, fraught with significant interobserver and intraobserver variability. To address these challenges and the lack of interpretability found in certain existing automated methods, we have created fully automated software that not only precisely measures the Cobb angle but also provides clear visualizations of these measurements. This software integrates deep neural network-based spine region detection and segmentation, spine centerline identification, pinpointing the most significantly tilted vertebrae, and direct visualization of Cobb angles on the original images. Upon comparison with the assessments of 7 expert readers, our algorithm exhibited a mean deviation in Cobb angle measurements of 4.17 degrees, notably surpassing the manual approach's average intra-reader discrepancy of 5.16 degrees. The algorithm also achieved intra-class correlation coefficients (ICC) exceeding 0.96 and Pearson correlation coefficients above 0.944, reflecting robust agreement with expert assessments and superior measurement reliability. Through the comprehensive reader study and statistical analysis, we believe this algorithm not only ensures a higher consensus with expert readers but also enhances interpretability and reproducibility during assessments. It holds significant promise for clinical application, potentially aiding physicians in more accurate scoliosis assessment and diagnosis, thereby improving patient care.","sentences":["Scoliosis, a prevalent condition characterized by abnormal spinal curvature leading to deformity, requires precise assessment methods for effective diagnosis and management.","The Cobb angle is a widely used scoliosis quantification method that measures the degree of curvature between the tilted vertebrae.","Yet, manual measuring of Cobb angles is time-consuming and labor-intensive, fraught with significant interobserver and intraobserver variability.","To address these challenges and the lack of interpretability found in certain existing automated methods, we have created fully automated software that not only precisely measures the Cobb angle but also provides clear visualizations of these measurements.","This software integrates deep neural network-based spine region detection and segmentation, spine centerline identification, pinpointing the most significantly tilted vertebrae, and direct visualization of Cobb angles on the original images.","Upon comparison with the assessments of 7 expert readers, our algorithm exhibited a mean deviation in Cobb angle measurements of 4.17 degrees, notably surpassing the manual approach's average intra-reader discrepancy of 5.16 degrees.","The algorithm also achieved intra-class correlation coefficients (ICC) exceeding 0.96","and Pearson correlation coefficients above 0.944, reflecting robust agreement with expert assessments and superior measurement reliability.","Through the comprehensive reader study and statistical analysis, we believe this algorithm not only ensures a higher consensus with expert readers but also enhances interpretability and reproducibility during assessments.","It holds significant promise for clinical application, potentially aiding physicians in more accurate scoliosis assessment and diagnosis, thereby improving patient care."],"url":"http://arxiv.org/abs/2403.12115v1","category":"eess.IV"}
{"created":"2024-03-18 15:31:09","title":"Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids","abstract":"Thermal power flow (TPF) is an important task for various control purposes in 4 Th generation district heating grids with multiple decentral heat sources and meshed grid structures. Computing the TPF, i.e., determining the grid state consisting of temperatures, pressures, and mass flows for given supply and demand values, is classically done by solving the nonlinear heat grid equations, but can be sped up by orders of magnitude using learned models such as neural networks. We propose a novel, efficient scheme to generate a sufficiently large training data set covering relevant supply and demand values. Instead of sampling supply and demand values, our approach generates training examples from a proxy distribution over generator and consumer mass flows, omitting the iterations needed for solving the heat grid equations. The exact, but slightly different, training examples can be weighted to represent the original training distribution. We show with simulations for typical grid structures that the new approach can reduce training set generation times by two orders of magnitude compared to sampling supply and demand values directly, without loss of relevance for the training samples. Moreover, learning TPF with a training data set is shown to outperform sample-free, physics-aware training approaches significantly.","sentences":["Thermal power flow (TPF) is an important task for various control purposes in 4 Th generation district heating grids with multiple decentral heat sources and meshed grid structures.","Computing the TPF, i.e., determining the grid state consisting of temperatures, pressures, and mass flows for given supply and demand values, is classically done by solving the nonlinear heat grid equations, but can be sped up by orders of magnitude using learned models such as neural networks.","We propose a novel, efficient scheme to generate a sufficiently large training data set covering relevant supply and demand values.","Instead of sampling supply and demand values, our approach generates training examples from a proxy distribution over generator and consumer mass flows, omitting the iterations needed for solving the heat grid equations.","The exact, but slightly different, training examples can be weighted to represent the original training distribution.","We show with simulations for typical grid structures that the new approach can reduce training set generation times by two orders of magnitude compared to sampling supply and demand values directly, without loss of relevance for the training samples.","Moreover, learning TPF with a training data set is shown to outperform sample-free, physics-aware training approaches significantly."],"url":"http://arxiv.org/abs/2403.11877v1","category":"cs.LG"}
{"created":"2024-03-18 15:26:05","title":"NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction","abstract":"Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\\% efficiency and labelling them according to particle type with 94.9\\% efficiency. The network operates directly on detector observables across multiple 2D representations, but utilizes a 3D-context-aware mechanism to encourage consistency between these representations. Model inference takes 0.12 s/event on a CPU, and 0.005 s/event batched on a GPU. This architecture is designed to be a general-purpose solution for particle reconstruction in neutrino physics, with the potential for deployment across a broad range of detector technologies, and offers a core convolution engine that can be leveraged for a variety of tasks beyond the two described in this article.","sentences":["Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques.","This article describes NuGraph2, a Graph Neural Network (GNN) for low-level reconstruction of simulated neutrino interactions in a LArTPC detector.","Simulated neutrino interactions in the MicroBooNE detector geometry are described as heterogeneous graphs, with energy depositions on each detector plane forming nodes on planar subgraphs.","The network utilizes a multi-head attention message-passing mechanism to perform background filtering and semantic labelling on these graph nodes, identifying those associated with the primary physics interaction with 98.0\\% efficiency and labelling them according to particle type with 94.9\\% efficiency.","The network operates directly on detector observables across multiple 2D representations, but utilizes a 3D-context-aware mechanism to encourage consistency between these representations.","Model inference takes 0.12 s/event on a CPU, and 0.005 s/event batched on a GPU.","This architecture is designed to be a general-purpose solution for particle reconstruction in neutrino physics, with the potential for deployment across a broad range of detector technologies, and offers a core convolution engine that can be leveraged for a variety of tasks beyond the two described in this article."],"url":"http://arxiv.org/abs/2403.11872v1","category":"physics.data-an"}
{"created":"2024-03-18 15:24:47","title":"The Real Tropical Geometry of Neural Networks","abstract":"We consider a binary classifier defined as the sign of a tropical rational function, that is, as the difference of two convex piecewise linear functions. The parameter space of ReLU neural networks is contained as a semialgebraic set inside the parameter space of tropical rational functions. We initiate the study of two different subdivisions of this parameter space: a subdivision into semialgebraic sets, on which the combinatorial type of the decision boundary is fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of the partitions of the dataset. The sublevel sets of the 0/1-loss function arise as subfans of this classification fan, and we show that the level-sets are not necessarily connected. We describe the classification fan i) geometrically, as normal fan of the activation polytope, and ii) combinatorially through a list of properties of associated bipartite graphs, in analogy to covector axioms of oriented matroids and tropical oriented matroids. Our findings extend and refine the connection between neural networks and tropical geometry by observing structures established in real tropical geometry, such as positive tropicalizations of hypersurfaces and tropical semialgebraic sets.","sentences":["We consider a binary classifier defined as the sign of a tropical rational function, that is, as the difference of two convex piecewise linear functions.","The parameter space of ReLU neural networks is contained as a semialgebraic set inside the parameter space of tropical rational functions.","We initiate the study of two different subdivisions of this parameter space: a subdivision into semialgebraic sets, on which the combinatorial type of the decision boundary is fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of the partitions of the dataset.","The sublevel sets of the 0/1-loss function arise as subfans of this classification fan, and we show that the level-sets are not necessarily connected.","We describe the classification fan i) geometrically, as normal fan of the activation polytope, and ii) combinatorially through a list of properties of associated bipartite graphs, in analogy to covector axioms of oriented matroids and tropical oriented matroids.","Our findings extend and refine the connection between neural networks and tropical geometry by observing structures established in real tropical geometry, such as positive tropicalizations of hypersurfaces and tropical semialgebraic sets."],"url":"http://arxiv.org/abs/2403.11871v1","category":"math.CO"}
{"created":"2024-03-18 15:23:48","title":"IDF-CR: Iterative Diffusion Process for Divide-and-Conquer Cloud Removal in Remote-sensing Images","abstract":"Deep learning technologies have demonstrated their effectiveness in removing cloud cover from optical remote-sensing images. Convolutional Neural Networks (CNNs) exert dominance in the cloud removal tasks. However, constrained by the inherent limitations of convolutional operations, CNNs can address only a modest fraction of cloud occlusion. In recent years, diffusion models have achieved state-of-the-art (SOTA) proficiency in image generation and reconstruction due to their formidable generative capabilities. Inspired by the rapid development of diffusion models, we first present an iterative diffusion process for cloud removal (IDF-CR), which exhibits a strong generative capabilities to achieve component divide-and-conquer cloud removal. IDF-CR consists of a pixel space cloud removal module (Pixel-CR) and a latent space iterative noise diffusion network (IND). Specifically, IDF-CR is divided into two-stage models that address pixel space and latent space. The two-stage model facilitates a strategic transition from preliminary cloud reduction to meticulous detail refinement. In the pixel space stage, Pixel-CR initiates the processing of cloudy images, yielding a suboptimal cloud removal prior to providing the diffusion model with prior cloud removal knowledge. In the latent space stage, the diffusion model transforms low-quality cloud removal into high-quality clean output. We refine the Stable Diffusion by implementing ControlNet. In addition, an unsupervised iterative noise refinement (INR) module is introduced for diffusion model to optimize the distribution of the predicted noise, thereby enhancing advanced detail recovery. Our model performs best with other SOTA methods, including image reconstruction and optical remote-sensing cloud removal on the optical remote-sensing datasets.","sentences":["Deep learning technologies have demonstrated their effectiveness in removing cloud cover from optical remote-sensing images.","Convolutional Neural Networks (CNNs) exert dominance in the cloud removal tasks.","However, constrained by the inherent limitations of convolutional operations, CNNs can address only a modest fraction of cloud occlusion.","In recent years, diffusion models have achieved state-of-the-art (SOTA) proficiency in image generation and reconstruction due to their formidable generative capabilities.","Inspired by the rapid development of diffusion models, we first present an iterative diffusion process for cloud removal (IDF-CR), which exhibits a strong generative capabilities to achieve component divide-and-conquer cloud removal.","IDF-CR consists of a pixel space cloud removal module (Pixel-CR) and a latent space iterative noise diffusion network (IND).","Specifically, IDF-CR is divided into two-stage models that address pixel space and latent space.","The two-stage model facilitates a strategic transition from preliminary cloud reduction to meticulous detail refinement.","In the pixel space stage, Pixel-CR initiates the processing of cloudy images, yielding a suboptimal cloud removal prior to providing the diffusion model with prior cloud removal knowledge.","In the latent space stage, the diffusion model transforms low-quality cloud removal into high-quality clean output.","We refine the Stable Diffusion by implementing ControlNet.","In addition, an unsupervised iterative noise refinement (INR) module is introduced for diffusion model to optimize the distribution of the predicted noise, thereby enhancing advanced detail recovery.","Our model performs best with other SOTA methods, including image reconstruction and optical remote-sensing cloud removal on the optical remote-sensing datasets."],"url":"http://arxiv.org/abs/2403.11870v1","category":"cs.CV"}
{"created":"2024-03-18 15:17:44","title":"Symmetry-reduced Loop Quantum Gravity: Plane Waves, Flat Space and the Hamiltonian Constraint","abstract":"Loop quantum gravity methods are applied to a symmetry-reduced model with homogeneity in two dimensions, derived from a Gowdy model [5,6]. The conditions for propagation of unidirectional plane gravitational waves at exactly the speed of light are set up in form of null Killing equations in terms of Ashtekar variables and imposed as operators on quantum states of the system. Due to the effective one-dimensionality, holonomies and holonomy operators appear as simple phase factors. In correspondence, state functions might be considered as U(1) elements with the usual inner product. Under the assumption of equal spacing of the eigenvalues of geometrical quantities the solutions are not normalizable in this sense. With decreasing spacing for growing eigenvalues, as introduced for example in [11], the situation becomes worse. Taking over the inner product from the genuine gauge group SU(2) of LQG renders the obtained states normalizable, nevertheless fluctuations of geometrical quantities remain divergent. In consequence, the solutions of the Killing conditions are modified, which means allowing for small fluctuations of the propagation speed, i. e. dispersion of gravitational waves. Vacuum fluctuations of Minkowski space are sketched. Finally the same methods are applied to the Hamiltonian constraint with the same result concerning normalizability. With such a modification also the constraint is not exactly satisfied any more, which indicates the necessary presence of some kind of interacting matter.","sentences":["Loop quantum gravity methods are applied to a symmetry-reduced model with homogeneity in two dimensions, derived from a Gowdy model [5,6].","The conditions for propagation of unidirectional plane gravitational waves at exactly the speed of light are set up in form of null Killing equations in terms of Ashtekar variables and imposed as operators on quantum states of the system.","Due to the effective one-dimensionality, holonomies and holonomy operators appear as simple phase factors.","In correspondence, state functions might be considered as U(1) elements with the usual inner product.","Under the assumption of equal spacing of the eigenvalues of geometrical quantities the solutions are not normalizable in this sense.","With decreasing spacing for growing eigenvalues, as introduced for example in [11], the situation becomes worse.","Taking over the inner product from the genuine gauge group SU(2) of LQG renders the obtained states normalizable, nevertheless fluctuations of geometrical quantities remain divergent.","In consequence, the solutions of the Killing conditions are modified, which means allowing for small fluctuations of the propagation speed, i. e. dispersion of gravitational waves.","Vacuum fluctuations of Minkowski space are sketched.","Finally the same methods are applied to the Hamiltonian constraint with the same result concerning normalizability.","With such a modification also the constraint is not exactly satisfied any more, which indicates the necessary presence of some kind of interacting matter."],"url":"http://arxiv.org/abs/2403.11864v1","category":"gr-qc"}
{"created":"2024-03-18 14:59:10","title":"On the solution existence for collocation discretizations of time-fractional subdiffusion equations","abstract":"Time-fractional parabolic equations with a Caputo time derivative of order $\\alpha\\in(0,1)$ are discretized in time using continuous collocation methods. For such discretizations, we give sufficient conditions for existence and uniqueness of their solutions. Two approaches are explored: the Lax-Milgram Theorem and the eigenfunction expansion. The resulting sufficient conditions, which involve certain $m\\times m$ matrices (where $m$ is the order of the collocation scheme), are verified both analytically, for all $m\\ge 1$ and all sets of collocation points, and computationally, for all $ m\\le 20$. The semilinear case is also addressed.","sentences":["Time-fractional parabolic equations with a Caputo time derivative of order $\\alpha\\in(0,1)$ are discretized in time using continuous collocation methods.","For such discretizations, we give sufficient conditions for existence and uniqueness of their solutions.","Two approaches are explored: the Lax-Milgram Theorem and the eigenfunction expansion.","The resulting sufficient conditions, which involve certain $m\\times m$ matrices (where $m$ is the order of the collocation scheme), are verified both analytically, for all $m\\ge 1$ and all sets of collocation points, and computationally, for all $ m\\le 20$.","The semilinear case is also addressed."],"url":"http://arxiv.org/abs/2403.11847v1","category":"math.NA"}
{"created":"2024-03-18 14:45:52","title":"Towards Understanding the Relationship between In-context Learning and Compositional Generalization","abstract":"According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data. However, many neural network models, including Transformers, have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal Transformer in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible few-shot learning problems attainable from the dataset. The model can solve the task, however, by utilizing earlier examples to generalize to later ones (i.e. in-context learning). In evaluations on the datasets, SCAN, COGS, and GeoQuery, models trained in this manner indeed show improved compositional generalization. This indicates the usefulness of in-context learning problems as an inductive bias for generalization.","sentences":["According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined.","This principle is crucial for human language processing and also, arguably, for NLP models in the face of out-of-distribution data.","However, many neural network models, including Transformers, have been shown to struggle with compositional generalization.","In this paper, we hypothesize that forcing models to in-context learn can provide an inductive bias to promote compositional generalization.","To test this hypothesis, we train a causal Transformer in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels.","This corresponds to training the model on all possible few-shot learning problems attainable from the dataset.","The model can solve the task, however, by utilizing earlier examples to generalize to later ones (i.e. in-context learning).","In evaluations on the datasets, SCAN, COGS, and GeoQuery, models trained in this manner indeed show improved compositional generalization.","This indicates the usefulness of in-context learning problems as an inductive bias for generalization."],"url":"http://arxiv.org/abs/2403.11834v1","category":"cs.CL"}
{"created":"2024-03-18 14:43:04","title":"BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting","abstract":"While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at https://lingzhezhao.github.io/BAD-Gaussians/","sentences":["While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses.","Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions.","However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering.","In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   ","In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction.","Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   ","In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   ","Our project page and source code is available at https://lingzhezhao.github.io/BAD-Gaussians/"],"url":"http://arxiv.org/abs/2403.11831v2","category":"cs.CV"}
{"created":"2024-03-18 14:36:54","title":"Grothendieck-Katz conjecture","abstract":"In this article we prove that linear differential equations with only algebraic solutions have zero $m$-curvature modulo $p^k$ for all except a finite number of primes $p$ and all $k,m\\in\\mathbb N$ with ${\\rm ord}_pm!\\geq k$. This provides us with a reformulation of Grothendieck-Katz conjecture with stronger hypothesis.","sentences":["In this article we prove that linear differential equations with only algebraic solutions have zero $m$-curvature modulo $p^k$ for all except a finite number of primes $p$ and all $k,m\\in\\mathbb N$ with ${\\rm ord}_pm!\\geq k$.","This provides us with a reformulation of Grothendieck-Katz conjecture with stronger hypothesis."],"url":"http://arxiv.org/abs/2403.11829v1","category":"math.NT"}
{"created":"2024-03-18 14:31:09","title":"CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution","abstract":"With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress. In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of Graph Neural Network (GNN) architectures. Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard GNNs. These capsules are a group of neurons with vector activations. The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule. Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis. We have studied the usefulness of our architecture with the LorentzNet architecture for quark-gluon tagging. Here, we have replaced the decoding block of LorentzNet with a capsulated decoding block and have called the resulting architecture CapsLorentzNet. Our new architecture can enhance the performance of LorentzNet by 20 \\% for the quark-gluon tagging task.","sentences":["With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress.","In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of Graph Neural Network (GNN) architectures.","Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard GNNs.","These capsules are a group of neurons with vector activations.","The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule.","Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis.","We have studied the usefulness of our architecture with the LorentzNet architecture for quark-gluon tagging.","Here, we have replaced the decoding block of LorentzNet with a capsulated decoding block and have called the resulting architecture CapsLorentzNet.","Our new architecture can enhance the performance of LorentzNet by 20 \\% for the quark-gluon tagging task."],"url":"http://arxiv.org/abs/2403.11826v1","category":"hep-ph"}
{"created":"2024-03-18 14:22:05","title":"Detecting immersed obstacle in Stokes fluid flow using the coupled complex boundary method","abstract":"A non-conventional shape optimization approach is introduced to address the identification of an obstacle immersed in a fluid described by the Stokes equation within a larger bounded domain, relying on boundary measurements on the accessible surface. The approach employs tools from shape optimization, utilizing the coupled complex boundary method to transform the over-specified problem into a complex boundary value problem by incorporating a complex Robin boundary condition. This condition is derived by coupling the Dirichlet and Neumann boundary conditions along the accessible boundary. The identification of the obstacle involves optimizing a cost function constructed based on the imaginary part of the solution across the entire domain. The subsequent calculation of the shape gradient of this cost function, rigorously performed via the rearrangement method, enables the iterative solution of the optimization problem using a Sobolev gradient descent algorithm. The feasibility of the method is illustrated through numerical experiments in both two and three spatial dimensions, demonstrating its effectiveness in reconstructing obstacles with pronounced concavities under high-level noise-contaminated data, all without perimeter or volume functional penalization.","sentences":["A non-conventional shape optimization approach is introduced to address the identification of an obstacle immersed in a fluid described by the Stokes equation within a larger bounded domain, relying on boundary measurements on the accessible surface.","The approach employs tools from shape optimization, utilizing the coupled complex boundary method to transform the over-specified problem into a complex boundary value problem by incorporating a complex Robin boundary condition.","This condition is derived by coupling the Dirichlet and Neumann boundary conditions along the accessible boundary.","The identification of the obstacle involves optimizing a cost function constructed based on the imaginary part of the solution across the entire domain.","The subsequent calculation of the shape gradient of this cost function, rigorously performed via the rearrangement method, enables the iterative solution of the optimization problem using a Sobolev gradient descent algorithm.","The feasibility of the method is illustrated through numerical experiments in both two and three spatial dimensions, demonstrating its effectiveness in reconstructing obstacles with pronounced concavities under high-level noise-contaminated data, all without perimeter or volume functional penalization."],"url":"http://arxiv.org/abs/2403.11819v1","category":"math.OC"}
{"created":"2024-03-18 14:18:08","title":"HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation","abstract":"We present a hybrid-view-based knowledge distillation framework, termed HVDistill, to guide the feature learning of a point cloud neural network with a pre-trained image network in an unsupervised manner. By exploiting the geometric relationship between RGB cameras and LiDAR sensors, the correspondence between the two modalities based on both image-plane view and bird-eye view can be established, which facilitates representation learning. Specifically, the image-plane correspondences can be simply obtained by projecting the point clouds, while the bird-eye-view correspondences can be achieved by lifting pixels to the 3D space with the predicted depths under the supervision of projected point clouds. The image teacher networks provide rich semantics from the image-plane view and meanwhile acquire geometric information from the bird-eye view. Indeed, image features from the two views naturally complement each other and together can ameliorate the learned feature representation of the point cloud student networks. Moreover, with a self-supervised pre-trained 2D network, HVDistill requires neither 2D nor 3D annotations. We pre-train our model on nuScenes dataset and transfer it to several downstream tasks on nuScenes, SemanticKITTI, and KITTI datasets for evaluation. Extensive experimental results show that our method achieves consistent improvements over the baseline trained from scratch and significantly outperforms the existing schemes. Codes are available at git@github.com:zhangsha1024/HVDistill.git.","sentences":["We present a hybrid-view-based knowledge distillation framework, termed HVDistill, to guide the feature learning of a point cloud neural network with a pre-trained image network in an unsupervised manner.","By exploiting the geometric relationship between RGB cameras and LiDAR sensors, the correspondence between the two modalities based on both image-plane view and bird-eye view can be established, which facilitates representation learning.","Specifically, the image-plane correspondences can be simply obtained by projecting the point clouds, while the bird-eye-view correspondences can be achieved by lifting pixels to the 3D space with the predicted depths under the supervision of projected point clouds.","The image teacher networks provide rich semantics from the image-plane view and meanwhile acquire geometric information from the bird-eye view.","Indeed, image features from the two views naturally complement each other and together can ameliorate the learned feature representation of the point cloud student networks.","Moreover, with a self-supervised pre-trained 2D network, HVDistill requires neither 2D nor 3D annotations.","We pre-train our model on nuScenes dataset and transfer it to several downstream tasks on nuScenes, SemanticKITTI, and KITTI datasets for evaluation.","Extensive experimental results show that our method achieves consistent improvements over the baseline trained from scratch and significantly outperforms the existing schemes.","Codes are available at git@github.com:zhangsha1024/HVDistill.git."],"url":"http://arxiv.org/abs/2403.11817v1","category":"cs.CV"}
{"created":"2024-03-18 13:54:07","title":"Machine Learning LSST 3x2pt analyses -- forecasting the impact of systematics on cosmological constraints using neural networks","abstract":"Validating modeling choices through simulated analyses and quantifying the impact of different systematic effects will form a major computational bottleneck in the preparation for 3$\\times$2 analysis with Stage-IV surveys such as Vera Rubin Observatory's Legacy Survey of Space and Time (LSST). We can significantly reduce the computational requirements by using machine learning based emulators, which allow us to run fast inference while maintaining the full realism of the data analysis pipeline. In this paper, we use such an emulator to run simulated 3$\\times$2 (cosmic shear, galaxy-galaxy lensing, and galaxy clustering) analyses for mock LSST-Y1/Y3/Y6/Y10 surveys and study the impact of various systematic effects (galaxy bias, intrinsic alignment, baryonic physics, shear calibration and photo-$z$ uncertainties). Closely following the DESC Science Requirement Document (with several updates) our main findings are: {\\it a)} The largest contribution to the `systematic error budget' of LSST 3$\\times$2 analysis comes from galaxy bias uncertainties, while the contribution of baryonic and shear calibration uncertainties are significantly less important. {\\it b)} Tighter constraints on intrinsic alignment and photo-$z$ parameters can improve cosmological constraints noticeably, which illustrates synergies of LSST and spectroscopic surveys. {\\it c)} The scale cuts adopted in the DESC SRD may be too conservative and pushing to smaller scales can increase cosmological information significantly. {\\it d)} We investigate the impact of photo-$z$ outliers on 3$\\times$2 pt analysis and find that we need to determine the outlier fraction to within $5-10\\%$ accuracy to ensure robust cosmological analysis. We caution that these findings depend on analysis choices (parameterizations, priors, scale cuts) and can change for different settings.","sentences":["Validating modeling choices through simulated analyses and quantifying the impact of different systematic effects will form a major computational bottleneck in the preparation for 3$\\times$2 analysis with Stage-IV surveys such as Vera Rubin Observatory's Legacy Survey of Space and Time (LSST).","We can significantly reduce the computational requirements by using machine learning based emulators, which allow us to run fast inference while maintaining the full realism of the data analysis pipeline.","In this paper, we use such an emulator to run simulated 3$\\times$2 (cosmic shear, galaxy-galaxy lensing, and galaxy clustering) analyses for mock LSST-Y1/Y3/Y6/Y10 surveys and study the impact of various systematic effects (galaxy bias, intrinsic alignment, baryonic physics, shear calibration and photo-$z$ uncertainties).","Closely following the DESC Science Requirement Document (with several updates) our main findings are: {\\it a)} The largest contribution to the `systematic error budget' of LSST 3$\\times$2 analysis comes from galaxy bias uncertainties, while the contribution of baryonic and shear calibration uncertainties are significantly less important.","{\\it b)} Tighter constraints on intrinsic alignment and photo-$z$ parameters can improve cosmological constraints noticeably, which illustrates synergies of LSST and spectroscopic surveys.","{\\it c)}","The scale cuts adopted in the DESC SRD may be too conservative and pushing to smaller scales can increase cosmological information significantly.","{\\it d)} We investigate the impact of photo-$z$ outliers on 3$\\times$2 pt analysis and find that we need to determine the outlier fraction to within $5-10\\%$ accuracy to ensure robust cosmological analysis.","We caution that these findings depend on analysis choices (parameterizations, priors, scale cuts) and can change for different settings."],"url":"http://arxiv.org/abs/2403.11797v1","category":"astro-ph.CO"}
{"created":"2024-03-18 13:53:48","title":"OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation","abstract":"3D reconstruction has been widely used in autonomous navigation fields of mobile robotics. However, the former research can only provide the basic geometry structure without the capability of open-world scene understanding, limiting advanced tasks like human interaction and visual navigation. Moreover, traditional 3D scene understanding approaches rely on expensive labeled 3D datasets to train a model for a single task with supervision. Thus, geometric reconstruction with zero-shot scene understanding i.e. Open vocabulary 3D Understanding and Reconstruction, is crucial for the future development of mobile robots. In this paper, we propose OpenOcc, a novel framework unifying the 3D scene reconstruction and open vocabulary understanding with neural radiance fields. We model the geometric structure of the scene with occupancy representation and distill the pre-trained open vocabulary model into a 3D language field via volume rendering for zero-shot inference. Furthermore, a novel semantic-aware confidence propagation (SCP) method has been proposed to relieve the issue of language field representation degeneracy caused by inconsistent measurements in distilled features. Experimental results show that our approach achieves competitive performance in 3D scene understanding tasks, especially for small and long-tail objects.","sentences":["3D reconstruction has been widely used in autonomous navigation fields of mobile robotics.","However, the former research can only provide the basic geometry structure without the capability of open-world scene understanding, limiting advanced tasks like human interaction and visual navigation.","Moreover, traditional 3D scene understanding approaches rely on expensive labeled 3D datasets to train a model for a single task with supervision.","Thus, geometric reconstruction with zero-shot scene understanding i.e. Open vocabulary 3D Understanding and Reconstruction, is crucial for the future development of mobile robots.","In this paper, we propose OpenOcc, a novel framework unifying the 3D scene reconstruction and open vocabulary understanding with neural radiance fields.","We model the geometric structure of the scene with occupancy representation and distill the pre-trained open vocabulary model into a 3D language field via volume rendering for zero-shot inference.","Furthermore, a novel semantic-aware confidence propagation (SCP) method has been proposed to relieve the issue of language field representation degeneracy caused by inconsistent measurements in distilled features.","Experimental results show that our approach achieves competitive performance in 3D scene understanding tasks, especially for small and long-tail objects."],"url":"http://arxiv.org/abs/2403.11796v1","category":"cs.CV"}
{"created":"2024-03-18 13:50:35","title":"SETA: Semantic-Aware Token Augmentation for Domain Generalization","abstract":"Domain generalization (DG) aims to enhance the model robustness against domain shifts without accessing target domains. A prevalent category of methods for DG is data augmentation, which focuses on generating virtual samples to simulate domain shifts. However, existing augmentation techniques in DG are mainly tailored for convolutional neural networks (CNNs), with limited exploration in token-based architectures, i.e., vision transformer (ViT) and multi-layer perceptrons (MLP) models. In this paper, we study the impact of prior CNN-based augmentation methods on token-based models, revealing their performance is suboptimal due to the lack of incentivizing the model to learn holistic shape information. To tackle the issue, we propose the SEmantic-aware Token Augmentation (SETA) method. SETA transforms token features by perturbing local edge cues while preserving global shape features, thereby enhancing the model learning of shape information. To further enhance the generalization ability of the model, we introduce two stylized variants of our method combined with two state-of-the-art style augmentation methods in DG. We provide a theoretical insight into our method, demonstrating its effectiveness in reducing the generalization risk bound. Comprehensive experiments on five benchmarks prove that our method achieves SOTA performances across various ViT and MLP architectures. Our code is available at https://github.com/lingeringlight/SETA.","sentences":["Domain generalization (DG) aims to enhance the model robustness against domain shifts without accessing target domains.","A prevalent category of methods for DG is data augmentation, which focuses on generating virtual samples to simulate domain shifts.","However, existing augmentation techniques in DG are mainly tailored for convolutional neural networks (CNNs), with limited exploration in token-based architectures, i.e., vision transformer (ViT) and multi-layer perceptrons (MLP) models.","In this paper, we study the impact of prior CNN-based augmentation methods on token-based models, revealing their performance is suboptimal due to the lack of incentivizing the model to learn holistic shape information.","To tackle the issue, we propose the SEmantic-aware Token Augmentation (SETA) method.","SETA transforms token features by perturbing local edge cues while preserving global shape features, thereby enhancing the model learning of shape information.","To further enhance the generalization ability of the model, we introduce two stylized variants of our method combined with two state-of-the-art style augmentation methods in DG.","We provide a theoretical insight into our method, demonstrating its effectiveness in reducing the generalization risk bound.","Comprehensive experiments on five benchmarks prove that our method achieves SOTA performances across various ViT and MLP architectures.","Our code is available at https://github.com/lingeringlight/SETA."],"url":"http://arxiv.org/abs/2403.11792v1","category":"cs.CV"}
{"created":"2024-03-19 16:46:29","title":"MEDBind: Unifying Language and Multimodal Medical Data Embeddings","abstract":"Medical vision-language pretraining models (VLPM) have achieved remarkable progress in fusing chest X-rays (CXR) with clinical texts, introducing image-text data binding approaches that enable zero-shot learning and downstream clinical tasks. However, the current landscape lacks the holistic integration of additional medical modalities, such as electrocardiograms (ECG). We present MEDBind (Medical Electronic patient recorD), which learns joint embeddings across CXR, ECG, and medical text. Using text data as the central anchor, MEDBind features tri-modality binding, delivering competitive performance in top-K retrieval, zero-shot, and few-shot benchmarks against established VLPM, and the ability for CXR-to-ECG zero-shot classification and retrieval. This seamless integration is achieved through combination of contrastive loss on modality-text pairs with our proposed contrastive loss function, Edge-Modality Contrastive Loss, fostering a cohesive embedding space for CXR, ECG, and text. Finally, we demonstrate that MEDBind can improve downstream tasks by directly integrating CXR and ECG embeddings into a large-language model for multimodal prompt tuning.","sentences":["Medical vision-language pretraining models (VLPM) have achieved remarkable progress in fusing chest X-rays (CXR) with clinical texts, introducing image-text data binding approaches that enable zero-shot learning and downstream clinical tasks.","However, the current landscape lacks the holistic integration of additional medical modalities, such as electrocardiograms (ECG).","We present MEDBind (Medical Electronic patient recorD), which learns joint embeddings across CXR, ECG, and medical text.","Using text data as the central anchor, MEDBind features tri-modality binding, delivering competitive performance in top-K retrieval, zero-shot, and few-shot benchmarks against established VLPM, and the ability for CXR-to-ECG zero-shot classification and retrieval.","This seamless integration is achieved through combination of contrastive loss on modality-text pairs with our proposed contrastive loss function, Edge-Modality Contrastive Loss, fostering a cohesive embedding space for CXR, ECG, and text.","Finally, we demonstrate that MEDBind can improve downstream tasks by directly integrating CXR and ECG embeddings into a large-language model for multimodal prompt tuning."],"url":"http://arxiv.org/abs/2403.12894v1","category":"cs.CV"}
{"created":"2024-03-19 16:05:51","title":"D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation","abstract":"Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications. Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation. Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process. Through empirical evaluation on a public benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin. We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task.","sentences":["Mastering dexterous robotic manipulation of deformable objects is vital for overcoming the limitations of parallel grippers in real-world applications.","Current trajectory optimisation approaches often struggle to solve such tasks due to the large search space and the limited task information available from a cost function.","In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks.","D-Cubed learns a skill-latent space that encodes short-horizon actions in the play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory in the dataset.","To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process.","In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation.","Then, D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process.","This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process.","Through empirical evaluation on a public benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin.","We further demonstrate that trajectories found by D-Cubed readily transfer to a real-world LEAP hand on a folding task."],"url":"http://arxiv.org/abs/2403.12861v1","category":"cs.RO"}
{"created":"2024-03-19 15:37:27","title":"Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects","abstract":"The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models. MLaaS providers expect this to be their ultimate safeguard for regulatory compliance. Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus. This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks. We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing. By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation of unlearning at the individual data point level. Utilizing these metrics, we conduct an in-depth analysis of current approximate machine unlearning algorithms, identifying three key directions where these approaches fall short: utility, resilience, and equity. Our aim is that this work will greatly improve our understanding of approximate machine unlearning methods, taking a significant stride towards converting the theoretical right to data erasure into a auditable reality.","sentences":["The growing concerns surrounding data privacy and security have underscored the critical necessity for machine unlearning--aimed at fully removing data lineage from machine learning models.","MLaaS providers expect this to be their ultimate safeguard for regulatory compliance.","Despite its critical importance, the pace at which privacy communities have been developing and implementing strong methods to verify the effectiveness of machine unlearning has been disappointingly slow, with this vital area often receiving insufficient focus.","This paper seeks to address this shortfall by introducing well-defined and effective metrics for black-box unlearning auditing tasks.","We transform the auditing challenge into a question of non-membership inference and develop efficient metrics for auditing.","By relying exclusively on the original and unlearned models--eliminating the need to train additional shadow models--our approach simplifies the evaluation of unlearning at the individual data point level.","Utilizing these metrics, we conduct an in-depth analysis of current approximate machine unlearning algorithms, identifying three key directions where these approaches fall short: utility, resilience, and equity.","Our aim is that this work will greatly improve our understanding of approximate machine unlearning methods, taking a significant stride towards converting the theoretical right to data erasure into a auditable reality."],"url":"http://arxiv.org/abs/2403.12830v1","category":"cs.LG"}
{"created":"2024-03-19 15:31:02","title":"Oriented and Non-oriented Cubical Surfaces in The Penteract","abstract":"Which surfaces can be realized with two-dimensional faces of the five-dimensional cube (the penteract)? How can we visualize them? In recent work, Aveni, Govc, and Roldan, show that there exist 2690 connected closed cubical surfaces up to isomorphism in the 5-cube. They give a classification in terms of their genus $g$ for closed orientable cubical surfaces and their demigenus $k$ for a closed non-orientable cubical surface. In this paper, we explain the main idea behind the exhaustive search and we visualize the projection to $\\mathbb{R}^3$ of a torus, a genus two torus, the projective plane, and the Klein bottle. We use reinforcement learning techniques to obtain configurations optimized for 3D printing.","sentences":["Which surfaces can be realized with two-dimensional faces of the five-dimensional cube (the penteract)?","How can we visualize them?","In recent work, Aveni, Govc, and Roldan, show that there exist 2690 connected closed cubical surfaces up to isomorphism in the 5-cube.","They give a classification in terms of their genus $g$ for closed orientable cubical surfaces and their demigenus $k$ for a closed non-orientable cubical surface.","In this paper, we explain the main idea behind the exhaustive search and we visualize the projection to $\\mathbb{R}^3$ of a torus, a genus two torus, the projective plane, and the Klein bottle.","We use reinforcement learning techniques to obtain configurations optimized for 3D printing."],"url":"http://arxiv.org/abs/2403.12825v1","category":"math.GT"}
{"created":"2024-03-19 14:51:54","title":"Brain volume is a better biomarker of outcomes in ischemic stroke compared to brain atrophy","abstract":"Brain parenchymal fraction (BPF) has been used as a surrogate measure of global brain atrophy, and as a biomarker of brain reserve in studies evaluating clinical outcomes after brain injury. Total brain volume at the time of injury has recently been shown to influence functional outcomes, where larger brain volumes are associated with better outcomes. Here, we assess if brain volume at the time of ischemic stroke injury is a better biomarker of functional outcome than BPF. Acute ischemic stroke cases at a single center between 2003 and 2011, with MR neuroimaging obtained within 48 hours from presentation were eligible. Functional outcomes represented by the modified Rankin Score (mRS) at 90 days post admission (mRS<3 deemed a favorable outcome) were obtained via patient interview or per chart review. Deep learning enabled automated segmentation pipelines were used to calculate brain volume, intracranial volume (ICV), and BPF on the acute neuroimaging data. Patient outcomes were modeled through logistic regressions, and model comparison was conducted using the Bayes Information Criterion (BIC). 467 patients with arterial ischemic stroke were included in the analysis. Median age was 65.8 years, and 65.3% were male. In both models, age and a larger stroke lesion volume were associated with worse functional outcomes. Higher BPF and a larger brain volume were both associated with favorable functional outcomes, however, comparison of both models suggested that the brain volume model (BIC=501) explains the data better compared to the BPF model (BIC=511). The extent of global brain atrophy has been regarded as an important biomarker of post-stroke functional outcomes and resilience to acute injury. Here, we demonstrate that a higher global brain volume at the time of injury better explains favorable functional outcomes, which can be directly clinically assessed.","sentences":["Brain parenchymal fraction (BPF) has been used as a surrogate measure of global brain atrophy, and as a biomarker of brain reserve in studies evaluating clinical outcomes after brain injury.","Total brain volume at the time of injury has recently been shown to influence functional outcomes, where larger brain volumes are associated with better outcomes.","Here, we assess if brain volume at the time of ischemic stroke injury is a better biomarker of functional outcome than BPF.","Acute ischemic stroke cases at a single center between 2003 and 2011, with MR neuroimaging obtained within 48 hours from presentation were eligible.","Functional outcomes represented by the modified Rankin Score (mRS) at 90 days post admission (mRS<3 deemed a favorable outcome) were obtained via patient interview or per chart review.","Deep learning enabled automated segmentation pipelines were used to calculate brain volume, intracranial volume (ICV), and BPF on the acute neuroimaging data.","Patient outcomes were modeled through logistic regressions, and model comparison was conducted using the Bayes Information Criterion (BIC).","467 patients with arterial ischemic stroke were included in the analysis.","Median age was 65.8 years, and 65.3% were male.","In both models, age and a larger stroke lesion volume were associated with worse functional outcomes.","Higher BPF and a larger brain volume were both associated with favorable functional outcomes, however, comparison of both models suggested that the brain volume model (BIC=501) explains the data better compared to the BPF model (BIC=511).","The extent of global brain atrophy has been regarded as an important biomarker of post-stroke functional outcomes and resilience to acute injury.","Here, we demonstrate that a higher global brain volume at the time of injury better explains favorable functional outcomes, which can be directly clinically assessed."],"url":"http://arxiv.org/abs/2403.12788v1","category":"q-bio.NC"}
{"created":"2024-03-19 14:51:01","title":"DDSB: An Unsupervised and Training-free Method for Phase Detection in Echocardiography","abstract":"Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is key for cardiac function assessment through echocardiography. However, traditional methods face several limitations: they require extensive amounts of data, extensive annotations by medical experts, significant training resources, and often lack robustness. Addressing these challenges, we proposed an unsupervised and training-free method, our novel approach leverages unsupervised segmentation to enhance fault tolerance against segmentation inaccuracies. By identifying anchor points and analyzing directional deformation, we effectively reduce dependence on the accuracy of initial segmentation images and enhance fault tolerance, all while improving robustness. Tested on Echo-dynamic and CAMUS datasets, our method achieves comparable accuracy to learning-based models without their associated drawbacks. The code is available at https://github.com/MRUIL/DDSB","sentences":["Accurate identification of End-Diastolic (ED) and End-Systolic (ES) frames is key for cardiac function assessment through echocardiography.","However, traditional methods face several limitations: they require extensive amounts of data, extensive annotations by medical experts, significant training resources, and often lack robustness.","Addressing these challenges, we proposed an unsupervised and training-free method, our novel approach leverages unsupervised segmentation to enhance fault tolerance against segmentation inaccuracies.","By identifying anchor points and analyzing directional deformation, we effectively reduce dependence on the accuracy of initial segmentation images and enhance fault tolerance, all while improving robustness.","Tested on Echo-dynamic and CAMUS datasets, our method achieves comparable accuracy to learning-based models without their associated drawbacks.","The code is available at https://github.com/MRUIL/DDSB"],"url":"http://arxiv.org/abs/2403.12787v1","category":"cs.CV"}
{"created":"2024-03-19 13:43:27","title":"Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation","abstract":"Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive mannual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model for self-supervised training, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer in our network. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation, respectively. Furthermore, we introduce a pretrain-to-refine self-supervised training paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.","sentences":["Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive mannual labeling costs.","Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets.","However, most methods rely on synthetic data or 3D CAD model for self-supervised training, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction.","To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors.","Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer in our network.","This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation, respectively.","Furthermore, we introduce a pretrain-to-refine self-supervised training paradigm to train our network.","It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism.","Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods."],"url":"http://arxiv.org/abs/2403.12728v1","category":"cs.CV"}
{"created":"2024-03-19 11:41:04","title":"Water Aging Effects on Graphene Nanoplatelets and Multi-walled Carbon Nanotube Reinforced Epoxy Glass Fiber Nanocomposites","abstract":"Nanocomposites reinforced with hybrid fillers of multi-walled carbon nanotubes (MWCNTs) and graphene nanoplatelets (GNPs) were developed, aimed at improving electrical and morphological properties of the hybrid nanocomposites while reducing the cost of the final product. GNPs and MWCNTs nanofillers have shown improved electrical and morphological properties with most polymers. In this work, the effect of short-term water aging for 1440 h on MCWNTs and GNPs reinforced epoxy glass fiber nanocomposites was studied. Epoxy nanocomposites were prepared with varying combinations of MCWNTs and GNPs (2:1, 2:2, and 2:3wt. %) as conducting fillers and their electrical conductivity was evaluated after short-term water aging. It was shown that the addition of MCWNTs and GNPs enhanced the electrical conductivity of composites: A low percolation threshold was achieved with 2 wt. % MCWNTs and 3 wt. % GNPs. The addition of MCWNTs enhanced the electrical conductivity and dielectric constant, confirming the synergistic effect of CNTs as multifunctional filler. Microstructural investigations and morphology of nanocomposites were investigated using Fourier transform-infrared spectroscopy and X-ray diffraction. The novelty of this work arises from the combination of two conducting fillers with different geometry and aspect ratios as well as different dispersion characteristics. The results obtained from the electrical measurements after water aging on hybrid nanocomposites indicates slight increases in electrical conductivity.","sentences":["Nanocomposites reinforced with hybrid fillers of multi-walled carbon nanotubes (MWCNTs) and graphene nanoplatelets (GNPs) were developed, aimed at improving electrical and morphological properties of the hybrid nanocomposites while reducing the cost of the final product.","GNPs and MWCNTs nanofillers have shown improved electrical and morphological properties with most polymers.","In this work, the effect of short-term water aging for 1440 h on MCWNTs and GNPs reinforced epoxy glass fiber nanocomposites was studied.","Epoxy nanocomposites were prepared with varying combinations of MCWNTs and GNPs (2:1, 2:2, and 2:3wt.","%) as conducting fillers and their electrical conductivity was evaluated after short-term water aging.","It was shown that the addition of MCWNTs and GNPs enhanced the electrical conductivity of composites: A low percolation threshold was achieved with 2 wt.","% MCWNTs and 3 wt. % GNPs.","The addition of MCWNTs enhanced the electrical conductivity and dielectric constant, confirming the synergistic effect of CNTs as multifunctional filler.","Microstructural investigations and morphology of nanocomposites were investigated using Fourier transform-infrared spectroscopy and X-ray diffraction.","The novelty of this work arises from the combination of two conducting fillers with different geometry and aspect ratios as well as different dispersion characteristics.","The results obtained from the electrical measurements after water aging on hybrid nanocomposites indicates slight increases in electrical conductivity."],"url":"http://arxiv.org/abs/2403.12654v1","category":"physics.app-ph"}
{"created":"2024-03-19 09:14:52","title":"Confidence Self-Calibration for Multi-Label Class-Incremental Learning","abstract":"The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable. This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space. In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for label relationship calibration, we introduce a class-incremental graph convolutional network that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship graph. Then, for confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions. Our approach attains new state-of-the-art results in MLCIL tasks on both MS-COCO and PASCAL VOC datasets, with the calibration of label confidences confirmed through our methodology.","sentences":["The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable.","This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space.","In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach.","Firstly, for label relationship calibration, we introduce a class-incremental graph convolutional network that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship graph.","Then, for confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions.","Our approach attains new state-of-the-art results in MLCIL tasks on both MS-COCO and PASCAL VOC datasets, with the calibration of label confidences confirmed through our methodology."],"url":"http://arxiv.org/abs/2403.12559v1","category":"cs.CV"}
{"created":"2024-03-19 08:40:19","title":"HCPM: Hierarchical Candidates Pruning for Efficient Detector-Free Matching","abstract":"Deep learning-based image matching methods play a crucial role in computer vision, yet they often suffer from substantial computational demands. To tackle this challenge, we present HCPM, an efficient and detector-free local feature-matching method that employs hierarchical pruning to optimize the matching pipeline. In contrast to recent detector-free methods that depend on an exhaustive set of coarse-level candidates for matching, HCPM selectively concentrates on a concise subset of informative candidates, resulting in fewer computational candidates and enhanced matching efficiency. The method comprises a self-pruning stage for selecting reliable candidates and an interactive-pruning stage that identifies correlated patches at the coarse level. Our results reveal that HCPM significantly surpasses existing methods in terms of speed while maintaining high accuracy. The source code will be made available upon publication.","sentences":["Deep learning-based image matching methods play a crucial role in computer vision, yet they often suffer from substantial computational demands.","To tackle this challenge, we present HCPM, an efficient and detector-free local feature-matching method that employs hierarchical pruning to optimize the matching pipeline.","In contrast to recent detector-free methods that depend on an exhaustive set of coarse-level candidates for matching, HCPM selectively concentrates on a concise subset of informative candidates, resulting in fewer computational candidates and enhanced matching efficiency.","The method comprises a self-pruning stage for selecting reliable candidates and an interactive-pruning stage that identifies correlated patches at the coarse level.","Our results reveal that HCPM significantly surpasses existing methods in terms of speed while maintaining high accuracy.","The source code will be made available upon publication."],"url":"http://arxiv.org/abs/2403.12543v1","category":"cs.CV"}
{"created":"2024-03-19 06:36:42","title":"TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer","abstract":"Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\\textsubscript{Txt} for text, ResNet and BLIP\\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results indicate TT-BLIP outperforms the state-of-the-art models.","sentences":["Detecting fake news has received a lot of attention.","Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information.","Also, the absence of specialized feature extraction for text and images further limits these methods.","This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\\textsubscript{Txt} for text, ResNet and BLIP\\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information.","The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis.","The experiments are performed using two fake news datasets, Weibo and Gossipcop.","The results indicate TT-BLIP outperforms the state-of-the-art models."],"url":"http://arxiv.org/abs/2403.12481v1","category":"cs.LG"}
{"created":"2024-03-19 05:46:20","title":"Diagrammatic Instructions to Specify Spatial Objectives and Constraints with Applications to Mobile Base Placement","abstract":"This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach for human operators to specify objectives and constraints that are related to spatial regions in the working environment. Human operators are enabled to sketch out regions directly on camera images that correspond to the objectives and constraints. These sketches are projected to 3D spatial coordinates, and continuous Spatial Instruction Maps (SIMs) are learned upon them. These maps can then be integrated into optimization problems for tasks of robots. In particular, we demonstrate how Spatial Diagrammatic Instructions can be applied to solve the Base Placement Problem of mobile manipulators, which concerns the best place to put the manipulator to facilitate a certain task. Human operators can specify, via sketch, spatial regions of interest for a manipulation task and permissible regions for the mobile manipulator to be at. Then, an optimization problem that maximizes the manipulator's reachability, or coverage, over the designated regions of interest while remaining in the permissible regions is solved. We provide extensive empirical evaluations, and show that our formulation of Spatial Instruction Maps provides accurate representations of user-specified diagrammatic instructions. Furthermore, we demonstrate that our diagrammatic approach to the Mobile Base Placement Problem enables higher quality solutions and faster run-time.","sentences":["This paper introduces Spatial Diagrammatic Instructions (SDIs), an approach for human operators to specify objectives and constraints that are related to spatial regions in the working environment.","Human operators are enabled to sketch out regions directly on camera images that correspond to the objectives and constraints.","These sketches are projected to 3D spatial coordinates, and continuous Spatial Instruction Maps (SIMs) are learned upon them.","These maps can then be integrated into optimization problems for tasks of robots.","In particular, we demonstrate how Spatial Diagrammatic Instructions can be applied to solve the Base Placement Problem of mobile manipulators, which concerns the best place to put the manipulator to facilitate a certain task.","Human operators can specify, via sketch, spatial regions of interest for a manipulation task and permissible regions for the mobile manipulator to be at.","Then, an optimization problem that maximizes the manipulator's reachability, or coverage, over the designated regions of interest while remaining in the permissible regions is solved.","We provide extensive empirical evaluations, and show that our formulation of Spatial Instruction Maps provides accurate representations of user-specified diagrammatic instructions.","Furthermore, we demonstrate that our diagrammatic approach to the Mobile Base Placement Problem enables higher quality solutions and faster run-time."],"url":"http://arxiv.org/abs/2403.12465v1","category":"cs.RO"}
{"created":"2024-03-19 04:35:59","title":"Transfer in Sequential Multi-armed Bandits via Reward Samples","abstract":"We consider a sequential stochastic multi-armed bandit problem where the agent interacts with bandit over multiple episodes. The reward distribution of the arms remain constant throughout an episode but can change over different episodes. We propose an algorithm based on UCB to transfer the reward samples from the previous episodes and improve the cumulative regret performance over all the episodes. We provide regret analysis and empirical results for our algorithm, which show significant improvement over the standard UCB algorithm without transfer.","sentences":["We consider a sequential stochastic multi-armed bandit problem where the agent interacts with bandit over multiple episodes.","The reward distribution of the arms remain constant throughout an episode but can change over different episodes.","We propose an algorithm based on UCB to transfer the reward samples from the previous episodes and improve the cumulative regret performance over all the episodes.","We provide regret analysis and empirical results for our algorithm, which show significant improvement over the standard UCB algorithm without transfer."],"url":"http://arxiv.org/abs/2403.12428v1","category":"cs.LG"}
{"created":"2024-03-19 04:25:54","title":"Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation","abstract":"This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition. We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio features. Through the utilization of Temporal Convolutional Network (TCN) modules, we effectively captured the temporal and spatial correlations between these features. Subsequently, we employed a Transformer encoder structure to learn long-range dependencies, thereby enhancing the model's performance and generalization ability. Our method leverages a multimodal data fusion approach, integrating pre-trained audio and video backbones for feature extraction, followed by TCN-based spatiotemporal encoding and Transformer-based temporal information capture. Experimental results demonstrate the effectiveness of our approach, achieving competitive performance in VA estimation on the AffWild2 dataset.","sentences":["This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition.","We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio features.","Through the utilization of Temporal Convolutional Network (TCN) modules, we effectively captured the temporal and spatial correlations between these features.","Subsequently, we employed a Transformer encoder structure to learn long-range dependencies, thereby enhancing the model's performance and generalization ability.","Our method leverages a multimodal data fusion approach, integrating pre-trained audio and video backbones for feature extraction, followed by TCN-based spatiotemporal encoding and Transformer-based temporal information capture.","Experimental results demonstrate the effectiveness of our approach, achieving competitive performance in VA estimation on the AffWild2 dataset."],"url":"http://arxiv.org/abs/2403.12425v1","category":"cs.CV"}
{"created":"2024-03-19 04:09:11","title":"Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization","abstract":"Pretraining transformers are generally time-consuming. Fully quantized training (FQT) is a promising approach to speed up pretraining. However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations. In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers. Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers. Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers. Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline.","sentences":["Pretraining transformers are generally time-consuming.","Fully quantized training (FQT) is a promising approach to speed up pretraining.","However, most FQT methods adopt a quantize-compute-dequantize procedure, which often leads to suboptimal speedup and significant performance degradation when used in transformers due to the high memory access overheads and low-precision computations.","In this work, we propose Jetfire, an efficient and accurate INT8 training method specific to transformers.","Our method features an INT8 data flow to optimize memory access and a per-block quantization method to maintain the accuracy of pretrained transformers.","Extensive experiments demonstrate that our INT8 FQT method achieves comparable accuracy to the FP16 training baseline and outperforms the existing INT8 training works for transformers.","Moreover, for a standard transformer block, our method offers an end-to-end training speedup of 1.42x and a 1.49x memory reduction compared to the FP16 baseline."],"url":"http://arxiv.org/abs/2403.12422v1","category":"cs.LG"}
{"created":"2024-03-19 03:39:43","title":"ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance","abstract":"Generating high-quality 3D assets from a given image is highly desirable in various applications such as AR/VR. Recent advances in single-image 3D generation explore feed-forward models that learn to infer the 3D model of an object without optimization. Though promising results have been achieved in single object generation, these methods often struggle to model complex 3D assets that inherently contain multiple objects. In this work, we present ComboVerse, a 3D generation framework that produces high-quality 3D assets with complex compositions by learning to combine multiple models. 1) We first perform an in-depth analysis of this ``multi-object gap'' from both model and data perspectives. 2) Next, with reconstructed 3D models of different objects, we seek to adjust their sizes, rotation angles, and locations to create a 3D asset that matches the given image. 3) To automate this process, we apply spatially-aware score distillation sampling (SSDS) from pretrained diffusion models to guide the positioning of objects. Our proposed framework emphasizes spatial alignment of objects, compared with standard score distillation sampling, and thus achieves more accurate results. Extensive experiments validate ComboVerse achieves clear improvements over existing methods in generating compositional 3D assets.","sentences":["Generating high-quality 3D assets from a given image is highly desirable in various applications such as AR/VR.","Recent advances in single-image 3D generation explore feed-forward models that learn to infer the 3D model of an object without optimization.","Though promising results have been achieved in single object generation, these methods often struggle to model complex 3D assets that inherently contain multiple objects.","In this work, we present ComboVerse, a 3D generation framework that produces high-quality 3D assets with complex compositions by learning to combine multiple models.","1) We first perform an in-depth analysis of this ``multi-object gap'' from both model and data perspectives.","2) Next, with reconstructed 3D models of different objects, we seek to adjust their sizes, rotation angles, and locations to create a 3D asset that matches the given image.","3) To automate this process, we apply spatially-aware score distillation sampling (SSDS) from pretrained diffusion models to guide the positioning of objects.","Our proposed framework emphasizes spatial alignment of objects, compared with standard score distillation sampling, and thus achieves more accurate results.","Extensive experiments validate ComboVerse achieves clear improvements over existing methods in generating compositional 3D assets."],"url":"http://arxiv.org/abs/2403.12409v1","category":"cs.CV"}
{"created":"2024-03-19 03:35:18","title":"Cross-Lingual Transfer for Natural Language Inference via Multilingual Prompt Translator","abstract":"Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario. To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge. Concretely, we first train prompt in source language and employ translator to translate it into target prompt. Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge. In few-shot settings on XNLI, MPT demonstrates superiority over baselines by remarkable improvements. MPT is more prominent compared with vanilla prompting when transferring to languages quite distinct from source language.","sentences":["Based on multilingual pre-trained models, cross-lingual transfer with prompt learning has shown promising effectiveness, where soft prompt learned in a source language is transferred to target languages for downstream tasks, particularly in the low-resource scenario.","To efficiently transfer soft prompt, we propose a novel framework, Multilingual Prompt Translator (MPT), where a multilingual prompt translator is introduced to properly process crucial knowledge embedded in prompt by changing language knowledge while retaining task knowledge.","Concretely, we first train prompt in source language and employ translator to translate it into target prompt.","Besides, we extend an external corpus as auxiliary data, on which an alignment task for predicted answer probability is designed to convert language knowledge, thereby equipping target prompt with multilingual knowledge.","In few-shot settings on XNLI, MPT demonstrates superiority over baselines by remarkable improvements.","MPT is more prominent compared with vanilla prompting when transferring to languages quite distinct from source language."],"url":"http://arxiv.org/abs/2403.12407v1","category":"cs.CL"}
{"created":"2024-03-19 03:22:28","title":"An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis","abstract":"Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves the content's semantics but mimics the prompt's style. However, there is no systematic understanding on how the synthesized audio is controlled by the prompt and content. In this work, we conduct an empirical study of the widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and provide insights into the prompt design and content semantic units. Our analysis reveals that heterogeneous and nonstationary prompts hurt the audio quality in contrast to the previous finding that longer prompts always lead to better synthesis. Moreover, we find that the speaker style of the synthesized audio is also affected by the content in addition to the prompt. We further show that semantic units carry rich acoustic information such as pitch, tempo, volume and speech emphasis, which might be leaked from the content to the synthesized audio.","sentences":["Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning.","A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves the content's semantics but mimics the prompt's style.","However, there is no systematic understanding on how the synthesized audio is controlled by the prompt and content.","In this work, we conduct an empirical study of the widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and provide insights into the prompt design and content semantic units.","Our analysis reveals that heterogeneous and nonstationary prompts hurt the audio quality in contrast to the previous finding that longer prompts always lead to better synthesis.","Moreover, we find that the speaker style of the synthesized audio is also affected by the content in addition to the prompt.","We further show that semantic units carry rich acoustic information such as pitch, tempo, volume and speech emphasis, which might be leaked from the content to the synthesized audio."],"url":"http://arxiv.org/abs/2403.12402v1","category":"cs.CL"}
{"created":"2024-03-19 02:59:12","title":"Cepheids with giant companions. II. Spectroscopic confirmation of nine new double-lined binary systems composed of two Cepheids","abstract":"Binary Cepheids with giant companions are crucial for studying the physical properties of Cepheid variables, providing the best means to measure their masses. Systems composed of two Cepheids are even more important but to date, only one such system in the Large Magellanic Cloud (LMC) was known. Our current aim is to increase the number of these systems tenfold and provide their basic characteristics. The final goal is to obtain the physical properties of the component Cepheids, including their masses and radii, and to learn about their evolution in the multiple systems, also revealing their origin. We started a spectroscopic monitoring of nine unresolved pairs of Cepheids from the OGLE catalog, to check if they are gravitationally bound. Two of these so-called double Cepheids are located in the LMC, five in the Small Magellanic Cloud (SMC), and two in the Milky Way (MW). We report the spectroscopic detection of binarity of all 9 of these double Cepheids with orbital periods from 2 to 18 years. This increases the number of known binary double (BIND) Cepheids from 1 to 10 and triples the number of all confirmed double-lined binary (SB2) Cepheids. For five BIND Cepheids disentangled pulsational light curves of the components show anti-correlated phase shifts due to orbital motion. We show the first empirical evidence that typical period-luminosity relations (PLRs) are rather binary Cepheid PLRs that include the companion's light. The statistics of pulsation period ratios of BIND Cepheids do not agree with those expected for pairs of the same-age Cepheids. These ratios together with the mass ratios far from unity suggest merger-origin of at least one component for about half of the systems. The SMC and MW objects are the first found in SB2 systems composed of giants in their host galaxies. The Milky Way BIND Cepheids are also the closest such systems, being located at about 11 and 26 kpc.","sentences":["Binary Cepheids with giant companions are crucial for studying the physical properties of Cepheid variables, providing the best means to measure their masses.","Systems composed of two Cepheids are even more important but to date, only one such system in the Large Magellanic Cloud (LMC) was known.","Our current aim is to increase the number of these systems tenfold and provide their basic characteristics.","The final goal is to obtain the physical properties of the component Cepheids, including their masses and radii, and to learn about their evolution in the multiple systems, also revealing their origin.","We started a spectroscopic monitoring of nine unresolved pairs of Cepheids from the OGLE catalog, to check if they are gravitationally bound.","Two of these so-called double Cepheids are located in the LMC, five in the Small Magellanic Cloud (SMC), and two in the Milky Way (MW).","We report the spectroscopic detection of binarity of all 9 of these double Cepheids with orbital periods from 2 to 18 years.","This increases the number of known binary double (BIND) Cepheids from 1 to 10 and triples the number of all confirmed double-lined binary (SB2) Cepheids.","For five BIND Cepheids disentangled pulsational light curves of the components show anti-correlated phase shifts due to orbital motion.","We show the first empirical evidence that typical period-luminosity relations (PLRs) are rather binary Cepheid PLRs that include the companion's light.","The statistics of pulsation period ratios of BIND Cepheids do not agree with those expected for pairs of the same-age Cepheids.","These ratios together with the mass ratios far from unity suggest merger-origin of at least one component for about half of the systems.","The SMC and MW objects are the first found in SB2 systems composed of giants in their host galaxies.","The Milky Way BIND Cepheids are also the closest such systems, being located at about 11 and 26 kpc."],"url":"http://arxiv.org/abs/2403.12390v1","category":"astro-ph.SR"}
{"created":"2024-03-19 02:49:32","title":"An Aligning and Training Framework for Multimodal Recommendations","abstract":"With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendation framework. To effectively train our AlignRec, we propose starting from pre-training the first alignment to obtain unified multimodal features and subsequently training the following two alignments together with these features as input. As it is essential to analyze whether each multimodal feature helps in training, we design three new classes of metrics to evaluate intermediate performance. Our extensive experiments on three real-world datasets consistently verify the superiority of AlignRec compared to nine baselines. We also find that the multimodal features generated by AlignRec are better than currently used ones, which are to be open-sourced.","sentences":["With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions.","Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items.","In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec.","In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items.","Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendation framework.","To effectively train our AlignRec, we propose starting from pre-training the first alignment to obtain unified multimodal features and subsequently training the following two alignments together with these features as input.","As it is essential to analyze whether each multimodal feature helps in training, we design three new classes of metrics to evaluate intermediate performance.","Our extensive experiments on three real-world datasets consistently verify the superiority of AlignRec compared to nine baselines.","We also find that the multimodal features generated by AlignRec are better than currently used ones, which are to be open-sourced."],"url":"http://arxiv.org/abs/2403.12384v2","category":"cs.IR"}
{"created":"2024-03-19 02:34:33","title":"Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning","abstract":"The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives. However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications. This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs. We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications. GatorTronGPT achieved the best F1 scores for both datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.","sentences":["The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives.","However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications.","This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs.","We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health.","The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications.","GatorTronGPT achieved the best F1 scores for both datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a cross-institution setting, and 5.5% and 14.5% in a cross-disease setting."],"url":"http://arxiv.org/abs/2403.12374v1","category":"cs.CL"}
{"created":"2024-03-19 02:32:47","title":"Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model","abstract":"Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a sequence of discrete tokens based on a reconstruction optimization process. Besides, we highlight that predicting a high proportion of corrupted tokens can be very helpful for extracting informative patterns across different domains during SSL pre-training, which has been largely overlooked in past years. Furthermore, unlike previous works, our work treats the pre-training language model (PLM) as the initialization of the encoder network, investigating the feasibility of transferring the knowledge learned by the PLM to the time series area. Through these efforts, the path to cross-domain pre-training of a generic time series model can be effectively paved. We conduct extensive experiments in a real-world scenario across various time series classification domains. The experimental results clearly confirm CrossTimeNet's superior performance.","sentences":["Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task.","Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains.","The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales.","To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task.","One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a sequence of discrete tokens based on a reconstruction optimization process.","Besides, we highlight that predicting a high proportion of corrupted tokens can be very helpful for extracting informative patterns across different domains during SSL pre-training, which has been largely overlooked in past years.","Furthermore, unlike previous works, our work treats the pre-training language model (PLM) as the initialization of the encoder network, investigating the feasibility of transferring the knowledge learned by the PLM to the time series area.","Through these efforts, the path to cross-domain pre-training of a generic time series model can be effectively paved.","We conduct extensive experiments in a real-world scenario across various time series classification domains.","The experimental results clearly confirm CrossTimeNet's superior performance."],"url":"http://arxiv.org/abs/2403.12372v1","category":"cs.LG"}
{"created":"2024-03-19 02:32:24","title":"Advancing Time Series Classification with Multimodal Language Modeling","abstract":"For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specific instructions and raw time series are treated as multimodal inputs while the label information is represented by texts. To accomplish this goal, three distinct designs are developed in the InstructTime. Firstly, a time series discretization module is designed to convert continuous time series into a sequence of hard tokens to solve the inconsistency issue across modal inputs. To solve the modality representation gap issue, for one thing, we introduce an alignment projected layer before feeding the transformed token of time series into language models. For another, we highlight the necessity of auto-regressive pre-training across domains, which can facilitate the transferability of the language model and boost the generalization performance. Extensive experiments are conducted over benchmark datasets, whose results uncover the superior performance of InstructTime and the potential for a universal foundation model in time series classification.","sentences":["For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution.","Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm.","In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm.","Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specific instructions and raw time series are treated as multimodal inputs while the label information is represented by texts.","To accomplish this goal, three distinct designs are developed in the InstructTime.","Firstly, a time series discretization module is designed to convert continuous time series into a sequence of hard tokens to solve the inconsistency issue across modal inputs.","To solve the modality representation gap issue, for one thing, we introduce an alignment projected layer before feeding the transformed token of time series into language models.","For another, we highlight the necessity of auto-regressive pre-training across domains, which can facilitate the transferability of the language model and boost the generalization performance.","Extensive experiments are conducted over benchmark datasets, whose results uncover the superior performance of InstructTime and the potential for a universal foundation model in time series classification."],"url":"http://arxiv.org/abs/2403.12371v1","category":"cs.LG"}
{"created":"2024-03-19 02:16:32","title":"DMAD: Dual Memory Bank for Real-World Anomaly Detection","abstract":"Training a unified model is considered to be more suitable for practical industrial anomaly detection scenarios due to its generalization ability and storage efficiency. However, this multi-class setting, which exclusively uses normal data, overlooks the few but important accessible annotated anomalies in the real world. To address the challenge of real-world anomaly detection, we propose a new framework named Dual Memory bank enhanced representation learning for Anomaly Detection (DMAD). This framework handles both unsupervised and semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a dual memory bank to calculate feature distance and feature attention between normal and abnormal patterns, thereby encapsulating knowledge about normal and abnormal instances. This knowledge is then used to construct an enhanced representation for anomaly score learning. We evaluated DMAD on the MVTec-AD and VisA datasets. The results show that DMAD surpasses current state-of-the-art methods, highlighting DMAD's capability in handling the complexities of real-world anomaly detection scenarios.","sentences":["Training a unified model is considered to be more suitable for practical industrial anomaly detection scenarios due to its generalization ability and storage efficiency.","However, this multi-class setting, which exclusively uses normal data, overlooks the few but important accessible annotated anomalies in the real world.","To address the challenge of real-world anomaly detection, we propose a new framework named Dual Memory bank enhanced representation learning for Anomaly Detection (DMAD).","This framework handles both unsupervised and semi-supervised scenarios in a unified (multi-class) setting.","DMAD employs a dual memory bank to calculate feature distance and feature attention between normal and abnormal patterns, thereby encapsulating knowledge about normal and abnormal instances.","This knowledge is then used to construct an enhanced representation for anomaly score learning.","We evaluated DMAD on the MVTec-AD and VisA datasets.","The results show that DMAD surpasses current state-of-the-art methods, highlighting DMAD's capability in handling the complexities of real-world anomaly detection scenarios."],"url":"http://arxiv.org/abs/2403.12362v1","category":"cs.CV"}
{"created":"2024-03-19 02:01:01","title":"An Alternative Graphical Lasso Algorithm for Precision Matrices","abstract":"The Graphical Lasso (GLasso) algorithm is fast and widely used for estimating sparse precision matrices (Friedman et al., 2008). Its central role in the literature of high-dimensional covariance estimation rivals that of Lasso regression for sparse estimation of the mean vector. Some mysteries regarding its optimization target, convergence, positive-definiteness and performance have been unearthed, resolved and presented in Mazumder and Hastie (2011), leading to a new/improved (dual-primal) DP-GLasso. Using a new and slightly different reparametriztion of the last column of a precision matrix we show that the regularized normal log-likelihood naturally decouples into a sum of two easy to minimize convex functions one of which is a Lasso regression problem. This decomposition is the key in developing a transparent, simple iterative block coordinate descent algorithm for computing the GLasso updates with performance comparable to DP-GLasso. In particular, our algorithm has the precision matrix as its optimization target right at the outset, and retains all the favorable properties of the DP-GLasso algorithm.","sentences":["The Graphical Lasso (GLasso) algorithm is fast and widely used for estimating sparse precision matrices (Friedman et al., 2008).","Its central role in the literature of high-dimensional covariance estimation rivals that of Lasso regression for sparse estimation of the mean vector.","Some mysteries regarding its optimization target, convergence, positive-definiteness and performance have been unearthed, resolved and presented in Mazumder and Hastie (2011), leading to a new/improved (dual-primal) DP-GLasso.","Using a new and slightly different reparametriztion of the last column of a precision matrix we show that the regularized normal log-likelihood naturally decouples into a sum of two easy to minimize convex functions one of which is a Lasso regression problem.","This decomposition is the key in developing a transparent, simple iterative block coordinate descent algorithm for computing the GLasso updates with performance comparable to DP-GLasso.","In particular, our algorithm has the precision matrix as its optimization target right at the outset, and retains all the favorable properties of the DP-GLasso algorithm."],"url":"http://arxiv.org/abs/2403.12357v1","category":"stat.CO"}
{"created":"2024-03-18 23:48:33","title":"Methods for Generating Drift in Text Streams","abstract":"Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Yelp and Airbnb datasets and tested using incremental classifiers respecting the stream mining paradigm to evaluate their ability to recover from the drifts. Results show that all methods have their performance degraded right after the drifts, and the incremental SVM is the fastest to run and recover the previous performance levels regarding accuracy and Macro F1-Score.","sentences":["Systems and individuals produce data continuously.","On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on.","Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example.","To learn from textual data over time, the machine learning system must account for concept drift.","Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time.","For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time.","Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature.","To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts.","These methods were applied to Yelp and Airbnb datasets and tested using incremental classifiers respecting the stream mining paradigm to evaluate their ability to recover from the drifts.","Results show that all methods have their performance degraded right after the drifts, and the incremental SVM is the fastest to run and recover the previous performance levels regarding accuracy and Macro F1-Score."],"url":"http://arxiv.org/abs/2403.12328v1","category":"cs.LG"}
{"created":"2024-03-18 23:45:18","title":"GT-Rain Single Image Deraining Challenge Report","abstract":"This report reviews the results of the GT-Rain challenge on single image deraining at the UG2+ workshop at CVPR 2023. The aim of this competition is to study the rainy weather phenomenon in real world scenarios, provide a novel real world rainy image dataset, and to spark innovative ideas that will further the development of single image deraining methods on real images. Submissions were trained on the GT-Rain dataset and evaluated on an extension of the dataset consisting of 15 additional scenes. Scenes in GT-Rain are comprised of real rainy image and ground truth image captured moments after the rain had stopped. 275 participants were registered in the challenge and 55 competed in the final testing phase.","sentences":["This report reviews the results of the GT-Rain challenge on single image deraining at the UG2+ workshop at CVPR 2023.","The aim of this competition is to study the rainy weather phenomenon in real world scenarios, provide a novel real world rainy image dataset, and to spark innovative ideas that will further the development of single image deraining methods on real images.","Submissions were trained on the GT-Rain dataset and evaluated on an extension of the dataset consisting of 15 additional scenes.","Scenes in GT-Rain are comprised of real rainy image and ground truth image captured moments after the rain had stopped.","275 participants were registered in the challenge and 55 competed in the final testing phase."],"url":"http://arxiv.org/abs/2403.12327v1","category":"cs.CV"}
{"created":"2024-03-18 23:42:04","title":"Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts","abstract":"Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority over state-of-the-art erasure methods in terms of removing undesirable content while preserving other unrelated elements.","sentences":["Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions.","However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content.","In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module.","This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs.","Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts.","We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority over state-of-the-art erasure methods in terms of removing undesirable content while preserving other unrelated elements."],"url":"http://arxiv.org/abs/2403.12326v1","category":"cs.LG"}
{"created":"2024-03-18 22:35:43","title":"Selecting informative conformal prediction sets with false coverage rate control","abstract":"In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictors. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be `informative' in a well defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction label sets or prediction intervals small enough, excluding null values, or obeying other appropriate `monotone' constraints. While this covers many settings of possible interest in various applications, we develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP, are to our knowledge the first ones providing FCR control for informative prediction sets. We show the usefulness of our resulting procedures on real and simulated data.","sentences":["In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictors.","We consider here the case where such prediction sets come after a selection process.","The selection process requires that the selected prediction sets be `informative' in a well defined sense.","We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction label sets or prediction intervals small enough, excluding null values, or obeying other appropriate `monotone' constraints.","While this covers many settings of possible interest in various applications, we develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample.","While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP, are to our knowledge the first ones providing FCR control for informative prediction sets.","We show the usefulness of our resulting procedures on real and simulated data."],"url":"http://arxiv.org/abs/2403.12295v1","category":"math.ST"}
{"created":"2024-03-18 21:44:46","title":"Fast and accurate nonadiabatic molecular dynamics enabled through variational interpolation of correlated electron wavefunctions","abstract":"We build on the concept of eigenvector continuation to develop an efficient multi-state method for the rigorous and smooth interpolation of a small training set of many-body wavefunctions through chemical space at mean-field cost. The inferred states are represented as variationally optimal linear combinations of the training states transferred between the many-body basis of different nuclear geometries. We show that analytic multi-state forces and nonadiabatic couplings from the model enable application to nonadiabatic molecular dynamics, developing an active learning scheme to ensure a compact and systematically improvable training set. This culminates in application to the nonadiabatic molecular dynamics of a photoexcited 28-atom hydrogen chain, with surprising complexity in the resulting nuclear motion. With just 22 DMRG calculations of training states from the low-energy correlated electronic structure at different geometries, we infer the multi-state energies, forces and nonadiabatic coupling vectors at 12,000 geometries with provable convergence to high accuracy along an ensemble of molecular trajectories, which would not be feasible with a brute force approach. This opens up a route to bridge the timescales between accurate single-point correlated electronic structure methods and timescales of relevance for photo-induced molecular dynamics.","sentences":["We build on the concept of eigenvector continuation to develop an efficient multi-state method for the rigorous and smooth interpolation of a small training set of many-body wavefunctions through chemical space at mean-field cost.","The inferred states are represented as variationally optimal linear combinations of the training states transferred between the many-body basis of different nuclear geometries.","We show that analytic multi-state forces and nonadiabatic couplings from the model enable application to nonadiabatic molecular dynamics, developing an active learning scheme to ensure a compact and systematically improvable training set.","This culminates in application to the nonadiabatic molecular dynamics of a photoexcited 28-atom hydrogen chain, with surprising complexity in the resulting nuclear motion.","With just 22 DMRG calculations of training states from the low-energy correlated electronic structure at different geometries, we infer the multi-state energies, forces and nonadiabatic coupling vectors at 12,000 geometries with provable convergence to high accuracy along an ensemble of molecular trajectories, which would not be feasible with a brute force approach.","This opens up a route to bridge the timescales between accurate single-point correlated electronic structure methods and timescales of relevance for photo-induced molecular dynamics."],"url":"http://arxiv.org/abs/2403.12275v1","category":"physics.chem-ph"}
{"created":"2024-03-18 21:32:58","title":"Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity","abstract":"Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \\method\\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet and its shifted versions. Moreover, we show that our subsets obtain 1.5x the average accuracy across 11 downstream datasets, of the next best baseline. The code is available at: https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip.","sentences":["Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization.","However, such models require a massive amount of pre-training data.","Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume.","Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question.","In this work, we propose the first theoretically rigorous data selection method for CLIP.","We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance.","Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \\method\\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet and its shifted versions.","Moreover, we show that our subsets obtain 1.5x the average accuracy across 11 downstream datasets, of the next best baseline.","The code is available at: https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip."],"url":"http://arxiv.org/abs/2403.12267v2","category":"cs.CV"}
{"created":"2024-03-18 20:33:44","title":"Improving Generalization via Meta-Learning on Hard Samples","abstract":"Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M, CAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. We also show that using naturally hard examples for validation (Imagenet-R / Imagenet-A) in LRW training for Imagenet improves performance on both clean and naturally hard test instances by 1-2%. Secondary analyses show that using hard validation data in an LRW framework improves margins on test data, hinting at the mechanism underlying our empirical gains. We believe this work opens up new research directions for the meta-optimization of meta-learning in a supervised learning context.","sentences":["Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset.","We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization.","In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization.","We provide an efficient algorithm for training this meta-optimized model, as well as a simple train-twice heuristic for careful comparative study.","We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem.","Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M, CAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet.","We also show that using naturally hard examples for validation (Imagenet-R / Imagenet-A) in LRW training for Imagenet improves performance on both clean and naturally hard test instances by 1-2%.","Secondary analyses show that using hard validation data in an LRW framework improves margins on test data, hinting at the mechanism underlying our empirical gains.","We believe this work opens up new research directions for the meta-optimization of meta-learning in a supervised learning context."],"url":"http://arxiv.org/abs/2403.12236v1","category":"cs.LG"}
{"created":"2024-03-19 17:10:17","title":"Boundary Layer Estimates in Stochastic Homogenization","abstract":"We prove quantitative decay estimates for the boundary layer corrector in stochastic homogenization in the case of a half-space boundary. Our estimates are of optimal order and show that the gradient of the boundary layer corrector features nearly fluctuation-order decay; its expected value decays even one order faster. As a corollary, we deduce estimates on the accuracy of the representative volume element method for the computation of effective coefficients: our understanding of the decay of boundary layers enables us to improve the order of convergence of the RVE method for $d\\geq 3$.","sentences":["We prove quantitative decay estimates for the boundary layer corrector in stochastic homogenization in the case of a half-space boundary.","Our estimates are of optimal order and show that the gradient of the boundary layer corrector features nearly fluctuation-order decay; its expected value decays even one order faster.","As a corollary, we deduce estimates on the accuracy of the representative volume element method for the computation of effective coefficients: our understanding of the decay of boundary layers enables us to improve the order of convergence of the RVE method for $d\\geq 3$."],"url":"http://arxiv.org/abs/2403.12911v1","category":"math.AP"}
{"created":"2024-03-19 13:39:05","title":"HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting","abstract":"Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.","sentences":["Holistic understanding of urban scenes based on RGB images is a challenging yet important problem.","It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects.","Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes.","In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding.","Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints.","Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy.","Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.12722v1","category":"cs.CV"}
{"created":"2024-03-19 09:38:39","title":"Self-Orthogonal Codes from Vectorial Dual-Bent Functions","abstract":"Self-orthogonal codes are a significant class of linear codes in coding theory and have attracted a lot of attention. In \\cite{HLL2023Te,LH2023Se}, $p$-ary self-orthogonal codes were constructed by using $p$-ary weakly regular bent functions, where $p$ is an odd prime. In \\cite{WH2023Se}, two classes of non-degenerate quadratic forms were used to construct $q$-ary self-orthogonal codes, where $q$ is a power of a prime. In this paper, we construct new families of $q$-ary self-orthogonal codes using vectorial dual-bent functions. Some classes of at least almost optimal linear codes are obtained from the dual codes of the constructed self-orthogonal codes. In some cases, we completely determine the weight distributions of the constructed self-orthogonal codes. From the view of vectorial dual-bent functions, we illustrate that the works on constructing self-orthogonal codes from $p$-ary weakly regular bent functions \\cite{HLL2023Te,LH2023Se} and non-degenerate quadratic forms with $q$ being odd \\cite{WH2023Se} can be obtained by our results. We partially answer an open problem on determining the weight distribution of a class of self-orthogonal codes given in \\cite{LH2023Se}. As applications, we construct new infinite families of at least almost optimal $q$-ary linear complementary dual codes (for short, LCD codes) and quantum codes.","sentences":["Self-orthogonal codes are a significant class of linear codes in coding theory and have attracted a lot of attention.","In \\cite{HLL2023Te,LH2023Se}, $p$-ary self-orthogonal codes were constructed by using $p$-ary weakly regular bent functions, where $p$ is an odd prime.","In \\cite{WH2023Se}, two classes of non-degenerate quadratic forms were used to construct $q$-ary self-orthogonal codes, where $q$ is a power of a prime.","In this paper, we construct new families of $q$-ary self-orthogonal codes using vectorial dual-bent functions.","Some classes of at least almost optimal linear codes are obtained from the dual codes of the constructed self-orthogonal codes.","In some cases, we completely determine the weight distributions of the constructed self-orthogonal codes.","From the view of vectorial dual-bent functions, we illustrate that the works on constructing self-orthogonal codes from $p$-ary weakly regular bent functions \\cite{HLL2023Te,LH2023Se} and non-degenerate quadratic forms with $q$ being odd \\cite{WH2023Se} can be obtained by our results.","We partially answer an open problem on determining the weight distribution of a class of self-orthogonal codes given in \\cite{LH2023Se}.","As applications, we construct new infinite families of at least almost optimal $q$-ary linear complementary dual codes (for short, LCD codes) and quantum codes."],"url":"http://arxiv.org/abs/2403.12578v1","category":"cs.IT"}
{"created":"2024-03-19 09:20:21","title":"Tree-based conditional copula estimation","abstract":"This paper proposes a regression tree procedure to estimate conditional copulas. The associated algorithm determines classes of observations based on covariate values and fits a simple parametric copula model on each class. The association parameter changes from one class to another, allowing for non-linearity in the dependence structure modeling. It also allows the definition of classes of observations on which the so-called \"simplifying assumption\" [see Derumigny and Fermanian, 2017] holds reasonably well. When considering observations belonging to a given class separately, the association parameter no longer depends on the covariates according to our model. In this paper, we derive asymptotic consistency results for the regression tree procedure and show that the proposed pruning methodology, that is the model selection techniques selecting the appropriate number of classes, is optimal in some sense. Simulations provide finite sample results and an analysis of data of cases of human influenza presents the practical behavior of the procedure.","sentences":["This paper proposes a regression tree procedure to estimate conditional copulas.","The associated algorithm determines classes of observations based on covariate values and fits a simple parametric copula model on each class.","The association parameter changes from one class to another, allowing for non-linearity in the dependence structure modeling.","It also allows the definition of classes of observations on which the so-called \"simplifying assumption\" [see Derumigny and Fermanian, 2017] holds reasonably well.","When considering observations belonging to a given class separately, the association parameter no longer depends on the covariates according to our model.","In this paper, we derive asymptotic consistency results for the regression tree procedure and show that the proposed pruning methodology, that is the model selection techniques selecting the appropriate number of classes, is optimal in some sense.","Simulations provide finite sample results and an analysis of data of cases of human influenza presents the practical behavior of the procedure."],"url":"http://arxiv.org/abs/2403.12565v1","category":"math.ST"}
{"created":"2024-03-19 09:20:17","title":"Bearing damage detection with orthogonal and non-negative low-rank feature extraction","abstract":"Local damage of bearings can be detected as a weak cyclic and impulsive component in a highly noisy measured signal. A key problem is how to extract the signal of interest (SOI) from the raw signal, i.e., how to identify and design an optimal filter. To tackle this problem, we propose to use stochastic sampled orthogonal non-negative matrix factorization for extracting frequency-based features from a spectrogram of the measured signal. The proposed algorithm finds a selective filter that is tailored to the frequency band of the SOI. We show that our approach outperforms the other state-of-the-art selectors that were previously used in condition monitoring. The efficiency of the proposed method is illustrated using both a simulation study and the following real signals: (a) vibration signal from a test rig in the laboratory and (b) acoustic signal from a belt conveyor.","sentences":["Local damage of bearings can be detected as a weak cyclic and impulsive component in a highly noisy measured signal.","A key problem is how to extract the signal of interest (SOI) from the raw signal, i.e., how to identify and design an optimal filter.","To tackle this problem, we propose to use stochastic sampled orthogonal non-negative matrix factorization for extracting frequency-based features from a spectrogram of the measured signal.","The proposed algorithm finds a selective filter that is tailored to the frequency band of the SOI.","We show that our approach outperforms the other state-of-the-art selectors that were previously used in condition monitoring.","The efficiency of the proposed method is illustrated using both a simulation study and the following real signals: (a) vibration signal from a test rig in the laboratory and (b) acoustic signal from a belt conveyor."],"url":"http://arxiv.org/abs/2403.12564v1","category":"eess.SP"}
{"created":"2024-03-19 06:58:20","title":"A consistent test of spherical symmetry for multivariate and high-dimensional data via data augmentation","abstract":"We develop a test for spherical symmetry of a multivariate distribution $P$ that works even when the dimension of the data $d$ is larger than the sample size $n$. We propose a non-negative measure $\\zeta(P)$ such that $\\zeta(P)=0$ if and only if $P$ is spherically symmetric. We construct a consistent estimator of $\\zeta(P)$ using the data augmentation method and investigate its large sample properties. The proposed test based on this estimator is calibrated using a novel resampling algorithm. Our test controls the Type-I error, and it is consistent against general alternatives. We also study its behaviour for a sequence of alternatives $(1-\\delta_n) F+\\delta_n G$, where $\\zeta(G)=0$ but $\\zeta(F)>0$, and $\\delta_n \\in [0,1]$. When $\\lim\\sup\\delta_n<1$, for any $G$, the power of our test converges to unity as $n$ increases. However, if $\\lim\\sup\\delta_n=1$, the asymptotic power of our test depends on $\\lim n(1-\\delta_n)^2$. We establish this by proving the minimax rate optimality of our test over a suitable class of alternatives and showing that it is Pitman efficient when $\\lim n(1-\\delta_n)^2>0$. Moreover, our test is provably consistent for high-dimensional data even when $d$ is larger than $n$. Our numerical results amply demonstrate the superiority of the proposed test over some state-of-the-art methods.","sentences":["We develop a test for spherical symmetry of a multivariate distribution $P$ that works even when the dimension of the data $d$ is larger than the sample size $n$. We propose a non-negative measure $\\zeta(P)$ such that $\\zeta(P)=0$ if and only if $P$ is spherically symmetric.","We construct a consistent estimator of $\\zeta(P)$ using the data augmentation method and investigate its large sample properties.","The proposed test based on this estimator is calibrated using a novel resampling algorithm.","Our test controls the Type-I error, and it is consistent against general alternatives.","We also study its behaviour for a sequence of alternatives $(1-\\delta_n) F+\\delta_n G$, where $\\zeta(G)=0$ but $\\zeta(F)>0$, and $\\delta_n \\in [0,1]$. When $\\lim\\sup\\delta_n<1$, for any $G$, the power of our test converges to unity as $n$ increases.","However, if $\\lim\\sup\\delta_n=1$, the asymptotic power of our test depends on $\\lim n(1-\\delta_n)^2$. We establish this by proving the minimax rate optimality of our test over a suitable class of alternatives and showing that it is Pitman efficient when $\\lim n(1-\\delta_n)^2>0$.","Moreover, our test is provably consistent for high-dimensional data even when $d$ is larger than $n$. Our numerical results amply demonstrate the superiority of the proposed test over some state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.12491v1","category":"math.ST"}
{"created":"2024-03-19 06:01:11","title":"SC-Diff: 3D Shape Completion with Latent Diffusion Models","abstract":"This paper introduces a 3D shape completion approach using a 3D latent diffusion model optimized for completing shapes, represented as Truncated Signed Distance Functions (TSDFs), from partial 3D scans. Our method combines image-based conditioning through cross-attention and spatial conditioning through the integration of 3D features from captured partial scans. This dual guidance enables high-fidelity, realistic shape completions at superior resolutions. At the core of our approach is the compression of 3D data into a low-dimensional latent space using an auto-encoder inspired by 2D latent diffusion models. This compression facilitates the processing of higher-resolution shapes and allows us to apply our model across multiple object classes, a significant improvement over other existing diffusion-based shape completion methods, which often require a separate diffusion model for each class. We validated our approach against two common benchmarks in the field of shape completion, demonstrating competitive performance in terms of accuracy and realism and performing on par with state-of-the-art methods despite operating at a higher resolution with a single model for all object classes. We present a comprehensive evaluation of our model, showcasing its efficacy in handling diverse shape completion challenges, even on unseen object classes. The code will be released upon acceptance.","sentences":["This paper introduces a 3D shape completion approach using a 3D latent diffusion model optimized for completing shapes, represented as Truncated Signed Distance Functions (TSDFs), from partial 3D scans.","Our method combines image-based conditioning through cross-attention and spatial conditioning through the integration of 3D features from captured partial scans.","This dual guidance enables high-fidelity, realistic shape completions at superior resolutions.","At the core of our approach is the compression of 3D data into a low-dimensional latent space using an auto-encoder inspired by 2D latent diffusion models.","This compression facilitates the processing of higher-resolution shapes and allows us to apply our model across multiple object classes, a significant improvement over other existing diffusion-based shape completion methods, which often require a separate diffusion model for each class.","We validated our approach against two common benchmarks in the field of shape completion, demonstrating competitive performance in terms of accuracy and realism and performing on par with state-of-the-art methods despite operating at a higher resolution with a single model for all object classes.","We present a comprehensive evaluation of our model, showcasing its efficacy in handling diverse shape completion challenges, even on unseen object classes.","The code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.12470v1","category":"cs.CV"}
{"created":"2024-03-19 05:10:10","title":"Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory","abstract":"Vision-language pre-training (VLP) models exhibit remarkable capabilities in comprehending both images and text, yet they remain susceptible to multimodal adversarial examples (AEs). Strengthening adversarial attacks and uncovering vulnerabilities, especially common issues in VLP models (e.g., high transferable AEs), can stimulate further research on constructing reliable and practical VLP models. A recent work (i.e., Set-level guidance attack) indicates that augmenting image-text pairs to increase AE diversity along the optimization path enhances the transferability of adversarial examples significantly. However, this approach predominantly emphasizes diversity around the online adversarial examples (i.e., AEs in the optimization period), leading to the risk of overfitting the victim model and affecting the transferability. In this study, we posit that the diversity of adversarial examples towards the clean input and online AEs are both pivotal for enhancing transferability across VLP models. Consequently, we propose using diversification along the intersection region of adversarial trajectory to expand the diversity of AEs. To fully leverage the interaction between modalities, we introduce text-guided adversarial example selection during optimization. Furthermore, to further mitigate the potential overfitting, we direct the adversarial text deviating from the last intersection region along the optimization path, rather than adversarial images as in existing methods. Extensive experiments affirm the effectiveness of our method in improving transferability across various VLP models and downstream vision-and-language tasks (e.g., Image-Text Retrieval(ITR), Visual Grounding(VG), Image Captioning(IC)).","sentences":["Vision-language pre-training (VLP) models exhibit remarkable capabilities in comprehending both images and text, yet they remain susceptible to multimodal adversarial examples (AEs).","Strengthening adversarial attacks and uncovering vulnerabilities, especially common issues in VLP models (e.g., high transferable AEs), can stimulate further research on constructing reliable and practical VLP models.","A recent work (i.e., Set-level guidance attack) indicates that augmenting image-text pairs to increase AE diversity along the optimization path enhances the transferability of adversarial examples significantly.","However, this approach predominantly emphasizes diversity around the online adversarial examples (i.e., AEs in the optimization period), leading to the risk of overfitting the victim model and affecting the transferability.","In this study, we posit that the diversity of adversarial examples towards the clean input and online AEs are both pivotal for enhancing transferability across VLP models.","Consequently, we propose using diversification along the intersection region of adversarial trajectory to expand the diversity of AEs.","To fully leverage the interaction between modalities, we introduce text-guided adversarial example selection during optimization.","Furthermore, to further mitigate the potential overfitting, we direct the adversarial text deviating from the last intersection region along the optimization path, rather than adversarial images as in existing methods.","Extensive experiments affirm the effectiveness of our method in improving transferability across various VLP models and downstream vision-and-language tasks (e.g., Image-Text Retrieval(ITR), Visual Grounding(VG), Image Captioning(IC))."],"url":"http://arxiv.org/abs/2403.12445v1","category":"cs.CV"}
{"created":"2024-03-19 04:26:19","title":"Unveiling the Dirac feature in the Metallated Carbyne as a Potential Platform for Exploring Widely Separated Majorana Fermions","abstract":"The realization of next-generation quantum computing devices is hindered by the formidable challenge of detecting and manipulating Majorana Fermion in nanomaterials. In this study, we explore a new approach of detecting Majorana Fermion in a metallated carbyne nanowire array. Through comprehensive optimizations, we successfully achieved a local magnetic moment exceeding 3{\\mu}B, with the average magnetic moment of the entire metallated carbyne surpassing 1{\\mu}B. Surprisingly, in the absence of spin-orbit coupling, the ferromagnetic Ru metallated carbyne, when coupled with a superconducting Ru substrate, is already able to demonstrate the symmetric opening of a Dirac gap at the gamma point. We discovered that the kink structure of the metallated carbyne plays a crucial role in modulating its topological properties. Moreover, we identified the origin of magnetic hybridization which is intricately linked to the distinctive features found in one-dimensional carbon structures. Our findings not only uncover the unconventional ferromagnetism observed in metallated carbyne but also present an exciting opportunity to realize carbon-based materials capable of hosting Majorana Zero Modes (MZM). This discovery has the potential to further stabilize MZM by decoupling the orbital perturbation from the MZM itself.","sentences":["The realization of next-generation quantum computing devices is hindered by the formidable challenge of detecting and manipulating Majorana Fermion in nanomaterials.","In this study, we explore a new approach of detecting Majorana Fermion in a metallated carbyne nanowire array.","Through comprehensive optimizations, we successfully achieved a local magnetic moment exceeding 3{\\mu}B, with the average magnetic moment of the entire metallated carbyne surpassing 1{\\mu}B. Surprisingly, in the absence of spin-orbit coupling, the ferromagnetic Ru metallated carbyne, when coupled with a superconducting Ru substrate, is already able to demonstrate the symmetric opening of a Dirac gap at the gamma point.","We discovered that the kink structure of the metallated carbyne plays a crucial role in modulating its topological properties.","Moreover, we identified the origin of magnetic hybridization which is intricately linked to the distinctive features found in one-dimensional carbon structures.","Our findings not only uncover the unconventional ferromagnetism observed in metallated carbyne but also present an exciting opportunity to realize carbon-based materials capable of hosting Majorana Zero Modes (MZM).","This discovery has the potential to further stabilize MZM by decoupling the orbital perturbation from the MZM itself."],"url":"http://arxiv.org/abs/2403.12426v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-19 01:36:57","title":"Shortest Trajectory of a Dubins Vehicle with a Controllable Laser","abstract":"We formulate a novel planar motion planning problem for a Dubins-Laser system that consists of a Dubins vehicle with an attached controllable laser. The vehicle moves with unit speed and the laser, having a finite range, can rotate in a clockwise or anti-clockwise direction with a bounded angular rate. From an arbitrary initial position and orientation, the objective is to steer the system so that a given static target is within the range of the laser and the laser is oriented at it in minimum time. We characterize multiple properties of the optimal trajectory and establish that the optimal trajectory for the Dubins-laser system is one out of a total of 16 candidates. Finally, we provide numerical insights that illustrate the properties characterized in this work.","sentences":["We formulate a novel planar motion planning problem for a Dubins-Laser system that consists of a Dubins vehicle with an attached controllable laser.","The vehicle moves with unit speed and the laser, having a finite range, can rotate in a clockwise or anti-clockwise direction with a bounded angular rate.","From an arbitrary initial position and orientation, the objective is to steer the system so that a given static target is within the range of the laser and the laser is oriented at it in minimum time.","We characterize multiple properties of the optimal trajectory and establish that the optimal trajectory for the Dubins-laser system is one out of a total of 16 candidates.","Finally, we provide numerical insights that illustrate the properties characterized in this work."],"url":"http://arxiv.org/abs/2403.12346v1","category":"eess.SY"}
{"created":"2024-03-18 23:34:19","title":"Tangent space generators of matrix product states and exact Floquet quantum scars","abstract":"The advancement of quantum simulators motivates the development of a theoretical framework to assist with efficient state preparation in quantum many-body systems. Generally, preparing a target entangled state via unitary evolution with time-dependent couplings is a challenging task and very little is known about the existence of solutions and their properties. In this work we develop a constructive approach for preparing matrix product states (MPS) via continuous unitary evolution. We provide an explicit construction of the operator which exactly implements the evolution of a given MPS along a specified direction in its tangent space. This operator can be written as a sum of local terms of finite range, yet it is in general non-Hermitian. Relying on the explicit construction of the non-Hermitian generator of the dynamics, we demonstrate the existence of a Hermitian sequence of operators that implements the desired MPS evolution with the error which decreases exponentially with the operator range. The construction is benchmarked on an explicit periodic trajectory in a translationally invariant MPS manifold. We demonstrate that the Floquet unitary generating the dynamics over one period of the trajectory features an approximate MPS-like eigenstate embedded among a sea of thermalizing eigenstates. These results show that our construction is useful not only for state preparation and control of many-body systems, but also provides a generic route towards Floquet scars -- periodically driven models with quasi-local generators of dynamics that have exact MPS eigenstates in their spectrum.","sentences":["The advancement of quantum simulators motivates the development of a theoretical framework to assist with efficient state preparation in quantum many-body systems.","Generally, preparing a target entangled state via unitary evolution with time-dependent couplings is a challenging task and very little is known about the existence of solutions and their properties.","In this work we develop a constructive approach for preparing matrix product states (MPS) via continuous unitary evolution.","We provide an explicit construction of the operator which exactly implements the evolution of a given MPS along a specified direction in its tangent space.","This operator can be written as a sum of local terms of finite range, yet it is in general non-Hermitian.","Relying on the explicit construction of the non-Hermitian generator of the dynamics, we demonstrate the existence of a Hermitian sequence of operators that implements the desired MPS evolution with the error which decreases exponentially with the operator range.","The construction is benchmarked on an explicit periodic trajectory in a translationally invariant MPS manifold.","We demonstrate that the Floquet unitary generating the dynamics over one period of the trajectory features an approximate MPS-like eigenstate embedded among a sea of thermalizing eigenstates.","These results show that our construction is useful not only for state preparation and control of many-body systems, but also provides a generic route towards Floquet scars -- periodically driven models with quasi-local generators of dynamics that have exact MPS eigenstates in their spectrum."],"url":"http://arxiv.org/abs/2403.12325v1","category":"quant-ph"}
{"created":"2024-03-18 21:54:04","title":"Scalable Networked Feature Selection with Randomized Algorithm for Robot Navigation","abstract":"We address the problem of sparse selection of visual features for localizing a team of robots navigating an unknown environment, where robots can exchange relative position measurements with neighbors. We select a set of the most informative features by anticipating their importance in robots localization by simulating trajectories of robots over a prediction horizon. Through theoretical proofs, we establish a crucial connection between graph Laplacian and the importance of features. We show that strong network connectivity translates to uniformity in feature importance, which enables uniform random sampling of features and reduces the overall computational complexity. We leverage a scalable randomized algorithm for sparse sums of positive semidefinite matrices to efficiently select the set of the most informative features and significantly improve the probabilistic performance bounds. Finally, we support our findings with extensive simulations.","sentences":["We address the problem of sparse selection of visual features for localizing a team of robots navigating an unknown environment, where robots can exchange relative position measurements with neighbors.","We select a set of the most informative features by anticipating their importance in robots localization by simulating trajectories of robots over a prediction horizon.","Through theoretical proofs, we establish a crucial connection between graph Laplacian and the importance of features.","We show that strong network connectivity translates to uniformity in feature importance, which enables uniform random sampling of features and reduces the overall computational complexity.","We leverage a scalable randomized algorithm for sparse sums of positive semidefinite matrices to efficiently select the set of the most informative features and significantly improve the probabilistic performance bounds.","Finally, we support our findings with extensive simulations."],"url":"http://arxiv.org/abs/2403.12279v1","category":"cs.RO"}
{"created":"2024-03-18 21:11:25","title":"Primary Defect Production in Doped Iron Grain Boundaries during Low Energy Collision Cascades","abstract":"This study explores the intricate interactions between grain boundaries (GBs) and irradiation-induced defects in nanocrystalline iron, highlighting the role of dopants like copper. Utilizing molecular dynamics simulations, the research delineates how GB properties, such as GB energy and defect formation energies, influence the formation and evolution of defects in low energy collision cascades. It reveals that GBs not only augment defect production but also show a marked preference for interstitials over vacancies, a behavior significantly modulated by the cascade's proximity to the GB. The presence of dopants is shown to alter GB properties, affecting both the rate and type of defect production, thereby underscoring the complex interplay between GB characteristics, dopant elements, and defect dynamics. Moreover, the investigation uncovers that the structural characteristics of GBs play a crucial role in cascade evolution and defect generation, with certain GB configurations undergoing reconfiguration in response to cascades. For instance, the reconfiguration of one pure Fe twist GB suggests that GB geometry can significantly influence defect generation mechanisms. These findings point to the potential of GB engineering in developing materials with enhanced radiation tolerance, advocating for a nuanced approach to material design. By tailoring GB properties and selectively introducing dopant elements, materials can be optimized to exhibit superior resistance to radiation-induced damage, offering insights for applications in nuclear reactors and other radiation-prone environments.","sentences":["This study explores the intricate interactions between grain boundaries (GBs) and irradiation-induced defects in nanocrystalline iron, highlighting the role of dopants like copper.","Utilizing molecular dynamics simulations, the research delineates how GB properties, such as GB energy and defect formation energies, influence the formation and evolution of defects in low energy collision cascades.","It reveals that GBs not only augment defect production but also show a marked preference for interstitials over vacancies, a behavior significantly modulated by the cascade's proximity to the GB.","The presence of dopants is shown to alter GB properties, affecting both the rate and type of defect production, thereby underscoring the complex interplay between GB characteristics, dopant elements, and defect dynamics.","Moreover, the investigation uncovers that the structural characteristics of GBs play a crucial role in cascade evolution and defect generation, with certain GB configurations undergoing reconfiguration in response to cascades.","For instance, the reconfiguration of one pure Fe twist GB suggests that GB geometry can significantly influence defect generation mechanisms.","These findings point to the potential of GB engineering in developing materials with enhanced radiation tolerance, advocating for a nuanced approach to material design.","By tailoring GB properties and selectively introducing dopant elements, materials can be optimized to exhibit superior resistance to radiation-induced damage, offering insights for applications in nuclear reactors and other radiation-prone environments."],"url":"http://arxiv.org/abs/2403.12257v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-18 19:43:00","title":"Useful Compact Representations for Data-Fitting","abstract":"For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.","sentences":["For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective.","However, conventional techniques generate dense matrices that are prohibitive for large problems.","Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems.","We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices.","We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions."],"url":"http://arxiv.org/abs/2403.12206v1","category":"math.OC"}
{"created":"2024-03-18 19:40:16","title":"Information Compression in Dynamic Information Disclosure Games","abstract":"We consider a two-player dynamic information design problem between a principal and a receiver -- a game is played between the two agents on top of a Markovian system controlled by the receiver's actions, where the principal obtains and strategically shares some information about the underlying system with the receiver in order to influence their actions. In our setting, both players have long-term objectives, and the principal sequentially commits to their strategies instead of committing at the beginning. Further, the principal cannot directly observe the system state, but at every turn they can choose randomized experiments to observe the system partially. The principal can share details about the experiments to the receiver. For our analysis we impose the truthful disclosure rule: the principal is required to truthfully announce the details and the result of each experiment to the receiver immediately after the experiment result is revealed. Based on the received information, the receiver takes an action when its their turn, with the action influencing the state of the underlying system. We show that there exist Perfect Bayesian equilibria in this game where both agents play Canonical Belief Based (CBB) strategies using a compressed version of their information, rather than full information, to choose experiments (for the principal) or actions (for the receiver). We also provide a backward inductive procedure to solve for an equilibrium in CBB strategies.","sentences":["We consider a two-player dynamic information design problem between a principal and a receiver -- a game is played between the two agents on top of a Markovian system controlled by the receiver's actions, where the principal obtains and strategically shares some information about the underlying system with the receiver in order to influence their actions.","In our setting, both players have long-term objectives, and the principal sequentially commits to their strategies instead of committing at the beginning.","Further, the principal cannot directly observe the system state, but at every turn they can choose randomized experiments to observe the system partially.","The principal can share details about the experiments to the receiver.","For our analysis we impose the truthful disclosure rule: the principal is required to truthfully announce the details and the result of each experiment to the receiver immediately after the experiment result is revealed.","Based on the received information, the receiver takes an action when its their turn, with the action influencing the state of the underlying system.","We show that there exist Perfect Bayesian equilibria in this game where both agents play Canonical Belief Based (CBB) strategies using a compressed version of their information, rather than full information, to choose experiments (for the principal) or actions (for the receiver).","We also provide a backward inductive procedure to solve for an equilibrium in CBB strategies."],"url":"http://arxiv.org/abs/2403.12204v1","category":"cs.GT"}
{"created":"2024-03-18 18:51:43","title":"Advanced Statistical Arbitrage with Reinforcement Learning","abstract":"Statistical arbitrage is a prevalent trading strategy which takes advantage of mean reverse property of spread of paired stocks. Studies on this strategy often rely heavily on model assumption. In this study, we introduce an innovative model-free and reinforcement learning based framework for statistical arbitrage. For the construction of mean reversion spreads, we establish an empirical reversion time metric and optimize asset coefficients by minimizing this empirical mean reversion time. In the trading phase, we employ a reinforcement learning framework to identify the optimal mean reversion strategy. Diverging from traditional mean reversion strategies that primarily focus on price deviations from a long-term mean, our methodology creatively constructs the state space to encapsulate the recent trends in price movements. Additionally, the reward function is carefully tailored to reflect the unique characteristics of mean reversion trading.","sentences":["Statistical arbitrage is a prevalent trading strategy which takes advantage of mean reverse property of spread of paired stocks.","Studies on this strategy often rely heavily on model assumption.","In this study, we introduce an innovative model-free and reinforcement learning based framework for statistical arbitrage.","For the construction of mean reversion spreads, we establish an empirical reversion time metric and optimize asset coefficients by minimizing this empirical mean reversion time.","In the trading phase, we employ a reinforcement learning framework to identify the optimal mean reversion strategy.","Diverging from traditional mean reversion strategies that primarily focus on price deviations from a long-term mean, our methodology creatively constructs the state space to encapsulate the recent trends in price movements.","Additionally, the reward function is carefully tailored to reflect the unique characteristics of mean reversion trading."],"url":"http://arxiv.org/abs/2403.12180v1","category":"q-fin.ST"}
{"created":"2024-03-18 18:50:28","title":"Recovery of HADES drift chambers suffering from Malter-like effects","abstract":"The central tracking system of the HADES detector, installed at the SIS-18 synchrotron at GSI/Darmstadt (Germany), employs large-area, low-mass drift chambers, featuring Aluminum potential wires and small cell sizes. The chambers in front of the magnetic field, closest to the interaction point, have developed significant self-sustained currents and discharges during operation, most probably triggered by isobutane-based gas mixtures. Only both, (i) replacing isobutane by CO2 and (ii) adding 1000 to 3500 ppmv of water into the Ar/CO2 counting gas mixture, individually optimized for a given chamber, allowed to recover the chambers, enabling stable operation in several production runs since then, e.g. with high-intensity heavy-ion induced reactions. The origin of the instability was found to be deposits on the cathode wires, provoking the Malter-like effects, by visual inspection and energy-dispersive X-ray spectroscopy. The charge on the wires accumulated during their lifetime does not point to so-called classical aging, but presumably the interaction of isobutane with materials in the gas flow, residual impurities, and reaction products formed in plasma, e.g., built by discharges.","sentences":["The central tracking system of the HADES detector, installed at the SIS-18 synchrotron at GSI/Darmstadt (Germany), employs large-area, low-mass drift chambers, featuring Aluminum potential wires and small cell sizes.","The chambers in front of the magnetic field, closest to the interaction point, have developed significant self-sustained currents and discharges during operation, most probably triggered by isobutane-based gas mixtures.","Only both, (i) replacing isobutane by CO2 and (ii) adding 1000 to 3500 ppmv of water into the Ar/CO2 counting gas mixture, individually optimized for a given chamber, allowed to recover the chambers, enabling stable operation in several production runs since then, e.g. with high-intensity heavy-ion induced reactions.","The origin of the instability was found to be deposits on the cathode wires, provoking the Malter-like effects, by visual inspection and energy-dispersive X-ray spectroscopy.","The charge on the wires accumulated during their lifetime does not point to so-called classical aging, but presumably the interaction of isobutane with materials in the gas flow, residual impurities, and reaction products formed in plasma, e.g., built by discharges."],"url":"http://arxiv.org/abs/2403.12178v1","category":"physics.ins-det"}
{"created":"2024-03-18 18:30:22","title":"The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection","abstract":"As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.","sentences":["As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models.","While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels.","Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field.","We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance.","By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers.","The re-calibrated weights are then mapped back to and propagated across the entire dataset.","Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training."],"url":"http://arxiv.org/abs/2403.12166v1","category":"cs.LG"}
{"created":"2024-03-18 18:03:08","title":"Ergonomic Optimization in Worker-Robot Bimanual Object Handover: Implementing REBA Using Reinforcement Learning in Virtual Reality","abstract":"Robots can serve as safety catalysts on construction job sites by taking over hazardous and repetitive tasks while alleviating the risks associated with existing manual workflows. Research on the safety of physical human-robot interaction (pHRI) is traditionally focused on addressing the risks associated with potential collisions. However, it is equally important to ensure that the workflows involving a collaborative robot are inherently safe, even though they may not result in an accident. For example, pHRI may require the human counterpart to use non-ergonomic body postures to conform to the robot hardware and physical configurations. Frequent and long-term exposure to such situations may result in chronic health issues. Safety and ergonomics assessment measures can be understood by robots if they are presented in algorithmic fashions so optimization for body postures is attainable. While frameworks such as Rapid Entire Body Assessment (REBA) have been an industry standard for many decades, they lack a rigorous mathematical structure which poses challenges in using them immediately for pHRI safety optimization purposes. Furthermore, learnable approaches have limited robustness outside of their training data, reducing generalizability. In this paper, we propose a novel framework that approaches optimization through Reinforcement Learning, ensuring precise, online ergonomic scores as compared to approximations, while being able to generalize and tune the regiment to any human and any task. To ensure practicality, the training is done in virtual reality utilizing Inverse Kinematics to simulate human movement mechanics. Experimental findings are compared to ergonomically naive object handover heuristics and indicate promising results where the developed framework can find the optimal object handover coordinates in pHRI contexts for manual material handling exemplary situations.","sentences":["Robots can serve as safety catalysts on construction job sites by taking over hazardous and repetitive tasks while alleviating the risks associated with existing manual workflows.","Research on the safety of physical human-robot interaction (pHRI) is traditionally focused on addressing the risks associated with potential collisions.","However, it is equally important to ensure that the workflows involving a collaborative robot are inherently safe, even though they may not result in an accident.","For example, pHRI may require the human counterpart to use non-ergonomic body postures to conform to the robot hardware and physical configurations.","Frequent and long-term exposure to such situations may result in chronic health issues.","Safety and ergonomics assessment measures can be understood by robots if they are presented in algorithmic fashions so optimization for body postures is attainable.","While frameworks such as Rapid Entire Body Assessment (REBA) have been an industry standard for many decades, they lack a rigorous mathematical structure which poses challenges in using them immediately for pHRI safety optimization purposes.","Furthermore, learnable approaches have limited robustness outside of their training data, reducing generalizability.","In this paper, we propose a novel framework that approaches optimization through Reinforcement Learning, ensuring precise, online ergonomic scores as compared to approximations, while being able to generalize and tune the regiment to any human and any task.","To ensure practicality, the training is done in virtual reality utilizing Inverse Kinematics to simulate human movement mechanics.","Experimental findings are compared to ergonomically naive object handover heuristics and indicate promising results where the developed framework can find the optimal object handover coordinates in pHRI contexts for manual material handling exemplary situations."],"url":"http://arxiv.org/abs/2403.12149v1","category":"cs.RO"}
{"created":"2024-03-18 17:59:47","title":"Zero-Shot Image Feature Consensus with Deep Functional Maps","abstract":"Correspondences emerge from large-scale vision models trained for generative and discriminative tasks. This has been revealed and benchmarked by computing correspondence maps between pairs of images, using nearest neighbors on the feature grids. Existing work has attempted to improve the quality of these correspondence maps by carefully mixing features from different sources, such as by combining the features of different layers or networks. We point out that a better correspondence strategy is available, which directly imposes structure on the correspondence field: the functional map. Wielding this simple mathematical tool, we lift the correspondence problem from the pixel space to the function space and directly optimize for mappings that are globally coherent. We demonstrate that our technique yields correspondences that are not only smoother but also more accurate, with the possibility of better reflecting the knowledge embedded in the large-scale vision models that we are studying. Our approach sets a new state-of-the-art on various dense correspondence tasks. We also demonstrate our effectiveness in keypoint correspondence and affordance map transfer.","sentences":["Correspondences emerge from large-scale vision models trained for generative and discriminative tasks.","This has been revealed and benchmarked by computing correspondence maps between pairs of images, using nearest neighbors on the feature grids.","Existing work has attempted to improve the quality of these correspondence maps by carefully mixing features from different sources, such as by combining the features of different layers or networks.","We point out that a better correspondence strategy is available, which directly imposes structure on the correspondence field: the functional map.","Wielding this simple mathematical tool, we lift the correspondence problem from the pixel space to the function space and directly optimize for mappings that are globally coherent.","We demonstrate that our technique yields correspondences that are not only smoother but also more accurate, with the possibility of better reflecting the knowledge embedded in the large-scale vision models that we are studying.","Our approach sets a new state-of-the-art on various dense correspondence tasks.","We also demonstrate our effectiveness in keypoint correspondence and affordance map transfer."],"url":"http://arxiv.org/abs/2403.12038v1","category":"cs.CV"}
{"created":"2024-03-18 17:51:43","title":"Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation","abstract":"Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed. Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models. This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting.","sentences":["Diffusion models are the main driver of progress in image and video synthesis, but suffer from slow inference speed.","Distillation methods, like the recently introduced adversarial diffusion distillation (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator.","We introduce Latent Adversarial Diffusion Distillation (LADD), a novel distillation approach overcoming the limitations of ADD.","In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent diffusion models.","This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis.","We apply LADD to Stable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art text-to-image generators using only four unguided sampling steps.","Moreover, we systematically investigate its scaling behavior and demonstrate LADD's effectiveness in various applications such as image editing and inpainting."],"url":"http://arxiv.org/abs/2403.12015v1","category":"cs.CV"}
{"created":"2024-03-18 17:50:20","title":"Convergence of Kinetic Langevin Monte Carlo on Lie groups","abstract":"Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under W2 distance. Only compactness of the Lie group and geodesically L-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langevin on curved spaces, and also the first quantitative result that requires no convexity or, at least not explicitly, any common relaxation such as isoperimetry.","sentences":["Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization.","We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the momentum variable is Euclidean despite that the potential function lives on a manifold.","We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics.","The Lie group structure is exactly preserved by this discretization.","Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under W2 distance.","Only compactness of the Lie group and geodesically L-smoothness of the potential function are needed.","To the best of our knowledge, this is the first convergence result for kinetic Langevin on curved spaces, and also the first quantitative result that requires no convexity or, at least not explicitly, any common relaxation such as isoperimetry."],"url":"http://arxiv.org/abs/2403.12012v1","category":"math.ST"}
{"created":"2024-03-18 17:35:47","title":"No-gap second-order conditions for minimization problems in spaces of measures","abstract":"Over the last years, minimization problems over spaces of measures have received increased interest due to their relevance in the context of inverse problems, optimal control and machine learning. A fundamental role in their numerical analysis is played by the assumption that the optimal dual state admits finitely many global extrema and satisfies a second-order sufficient optimality condition in each one of them. In this work, we show the full equivalence of these structural assumptions to a no-gap second-order condition involving the second subderivative of the Radon norm as well as to a local quadratic growth property of the objective functional with respect to the bounded Lipschitz norm.","sentences":["Over the last years, minimization problems over spaces of measures have received increased interest due to their relevance in the context of inverse problems, optimal control and machine learning.","A fundamental role in their numerical analysis is played by the assumption that the optimal dual state admits finitely many global extrema and satisfies a second-order sufficient optimality condition in each one of them.","In this work, we show the full equivalence of these structural assumptions to a no-gap second-order condition involving the second subderivative of the Radon norm as well as to a local quadratic growth property of the objective functional with respect to the bounded Lipschitz norm."],"url":"http://arxiv.org/abs/2403.12001v1","category":"math.OC"}
{"created":"2024-03-18 17:22:43","title":"SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation","abstract":"When exploring new areas, robotic systems generally exclusively plan and execute controls over geometry that has been directly measured. When entering space that was previously obstructed from view such as turning corners in hallways or entering new rooms, robots often pause to plan over the newly observed space. To address this we present SceneScene, a real-time 3D diffusion model for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks. SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted geometry around the platform at runtime, even when the geometry is occluded or out of view. Our architecture ensures that SceneSense never overwrites observed free or occupied space. By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions. While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities. SceneSense operates as part of any system that generates a running occupancy map `out of the box', removing conditioning from the framework. Alternatively, for maximum performance in new modalities, the perception backbone can be replaced and the model retrained for inference in new applications. Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime. Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map.","sentences":["When exploring new areas, robotic systems generally exclusively plan and execute controls over geometry that has been directly measured.","When entering space that was previously obstructed from view such as turning corners in hallways or entering new rooms, robots often pause to plan over the newly observed space.","To address this we present SceneScene, a real-time 3D diffusion model for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks.","SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted geometry around the platform at runtime, even when the geometry is occluded or out of view.","Our architecture ensures that SceneSense never overwrites observed free or occupied space.","By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions.","While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities.","SceneSense operates as part of any system that generates a running occupancy map `out of the box', removing conditioning from the framework.","Alternatively, for maximum performance in new modalities, the perception backbone can be replaced and the model retrained for inference in new applications.","Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime.","Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map."],"url":"http://arxiv.org/abs/2403.11985v1","category":"cs.RO"}
{"created":"2024-03-18 17:19:51","title":"Proposal of a general framework to categorize continuous predictor variables","abstract":"The use of discretized variables in the development of prediction models is a common practice, in part because the decision-making process is more natural when it is based on rules created from segmented models. Although this practice is perhaps more common in medicine, it is extensible to any area of knowledge where a predictive model helps in decision-making. Therefore, providing researchers with a useful and valid categorization method could be a relevant issue when developing prediction models. In this paper, we propose a new general methodology that can be applied to categorize a predictor variable in any regression model where the response variable belongs to the exponential family distribution. Furthermore, it can be applied in any multivariate context, allowing to categorize more than one continuous covariate simultaneously. In addition, a computationally very efficient method is proposed to obtain the optimal number of categories, based on a pseudo-BIC proposal. Several simulation studies have been conducted in which the efficiency of the method with respect to both the location and the number of estimated cut-off points is shown. Finally, the categorization proposal has been applied to a real data set of 543 patients with chronic obstructive pulmonary disease from Galdakao Hospital's five outpatient respiratory clinics, who were followed up for 10 years. We applied the proposed methodology to jointly categorize the continuous variables six-minute walking test and forced expiratory volume in one second in a multiple Poisson generalized additive model for the response variable rate of the number of hospital admissions by years of follow-up. The location and number of cut-off points obtained were clinically validated as being in line with the categorizations used in the literature.","sentences":["The use of discretized variables in the development of prediction models is a common practice, in part because the decision-making process is more natural when it is based on rules created from segmented models.","Although this practice is perhaps more common in medicine, it is extensible to any area of knowledge where a predictive model helps in decision-making.","Therefore, providing researchers with a useful and valid categorization method could be a relevant issue when developing prediction models.","In this paper, we propose a new general methodology that can be applied to categorize a predictor variable in any regression model where the response variable belongs to the exponential family distribution.","Furthermore, it can be applied in any multivariate context, allowing to categorize more than one continuous covariate simultaneously.","In addition, a computationally very efficient method is proposed to obtain the optimal number of categories, based on a pseudo-BIC proposal.","Several simulation studies have been conducted in which the efficiency of the method with respect to both the location and the number of estimated cut-off points is shown.","Finally, the categorization proposal has been applied to a real data set of 543 patients with chronic obstructive pulmonary disease from Galdakao Hospital's five outpatient respiratory clinics, who were followed up for 10 years.","We applied the proposed methodology to jointly categorize the continuous variables six-minute walking test and forced expiratory volume in one second in a multiple Poisson generalized additive model for the response variable rate of the number of hospital admissions by years of follow-up.","The location and number of cut-off points obtained were clinically validated as being in line with the categorizations used in the literature."],"url":"http://arxiv.org/abs/2403.11983v1","category":"stat.ME"}
{"created":"2024-03-18 17:09:04","title":"A Maximum Entropy Principle in Deep Thermalization and in Hilbert-Space Ergodicity","abstract":"We report universal statistical properties displayed by ensembles of pure states that naturally emerge in quantum many-body systems. Specifically, two classes of state ensembles are considered: those formed by i) the temporal trajectory of a quantum state under unitary evolution or ii) the quantum states of small subsystems obtained by partial, local projective measurements performed on their complements. These cases respectively exemplify the phenomena of \"Hilbert-space ergodicity\" and \"deep thermalization.\" In both cases, the resultant ensembles are defined by a simple principle: the distributions of pure states have maximum entropy, subject to constraints such as energy conservation, and effective constraints imposed by thermalization. We present and numerically verify quantifiable signatures of this principle by deriving explicit formulae for all statistical moments of the ensembles; proving the necessary and sufficient conditions for such universality under widely-accepted assumptions; and describing their measurable consequences in experiments. We further discuss information-theoretic implications of the universality: our ensembles have maximal information content while being maximally difficult to interrogate, establishing that generic quantum state ensembles that occur in nature hide (scramble) information as strongly as possible. Our results generalize the notions of Hilbert-space ergodicity to time-independent Hamiltonian dynamics and deep thermalization from infinite to finite effective temperature. Our work presents new perspectives to characterize and understand universal behaviors of quantum dynamics using statistical and information theoretic tools.","sentences":["We report universal statistical properties displayed by ensembles of pure states that naturally emerge in quantum many-body systems.","Specifically, two classes of state ensembles are considered: those formed by i) the temporal trajectory of a quantum state under unitary evolution or ii) the quantum states of small subsystems obtained by partial, local projective measurements performed on their complements.","These cases respectively exemplify the phenomena of \"Hilbert-space ergodicity\" and \"deep thermalization.\"","In both cases, the resultant ensembles are defined by a simple principle: the distributions of pure states have maximum entropy, subject to constraints such as energy conservation, and effective constraints imposed by thermalization.","We present and numerically verify quantifiable signatures of this principle by deriving explicit formulae for all statistical moments of the ensembles; proving the necessary and sufficient conditions for such universality under widely-accepted assumptions; and describing their measurable consequences in experiments.","We further discuss information-theoretic implications of the universality: our ensembles have maximal information content while being maximally difficult to interrogate, establishing that generic quantum state ensembles that occur in nature hide (scramble) information as strongly as possible.","Our results generalize the notions of Hilbert-space ergodicity to time-independent Hamiltonian dynamics and deep thermalization from infinite to finite effective temperature.","Our work presents new perspectives to characterize and understand universal behaviors of quantum dynamics using statistical and information theoretic tools."],"url":"http://arxiv.org/abs/2403.11970v1","category":"quant-ph"}
{"created":"2024-03-18 16:49:52","title":"Displacement Field Analysis via Optimal Transport: Multi-Tracer Approach to Cosmological Reconstruction","abstract":"We demonstrate the effectiveness of one of the many multi-tracer analyses enabled by Optimal Transport (OT) reconstruction. Leveraging a semi-discrete OT algorithm, we determine the displacements between initial and observed positions of biased tracers and the remaining matter field. With only redshift-space distorted final positions of biased tracers and a simple premise for the remaining mass distribution as input, OT solves the displacement field. This extracted field, assuming asymptotically uniform density and a gradient flow displacement, enables reconstruction of the initial overdensity fluctuation field. We show that the divergence of the OT displacement field is a good proxy of the linear density field, even though the method never assumes the linear theory growth. Additionally, this divergence field can be combined with the reconstructed protohalos to provide a higher signal-to-noise measurement of the BAO standard ruler than was possible with either measurement individually.","sentences":["We demonstrate the effectiveness of one of the many multi-tracer analyses enabled by Optimal Transport (OT) reconstruction.","Leveraging a semi-discrete OT algorithm, we determine the displacements between initial and observed positions of biased tracers and the remaining matter field.","With only redshift-space distorted final positions of biased tracers and a simple premise for the remaining mass distribution as input, OT solves the displacement field.","This extracted field, assuming asymptotically uniform density and a gradient flow displacement, enables reconstruction of the initial overdensity fluctuation field.","We show that the divergence of the OT displacement field is a good proxy of the linear density field, even though the method never assumes the linear theory growth.","Additionally, this divergence field can be combined with the reconstructed protohalos to provide a higher signal-to-noise measurement of the BAO standard ruler than was possible with either measurement individually."],"url":"http://arxiv.org/abs/2403.11951v1","category":"astro-ph.CO"}
{"created":"2024-03-18 16:40:07","title":"Kernel Modelling of Fading Memory Systems","abstract":"The paper introduces a kernel-based framework to model and identify time-invariant systems with the fading memory property. The key departure from the previous literature is to bypass the state-space representation of the model. Instead, a kernel representation is used to directly model the memory functional that maps past inputs to the present output. We explore the versatility of this approach to encode important system properties in the hyperparameters of the kernel. The approach is illustrated on the Hodgkin and Huxley model of neuronal excitability.","sentences":["The paper introduces a kernel-based framework to model and identify time-invariant systems with the fading memory property.","The key departure from the previous literature is to bypass the state-space representation of the model.","Instead, a kernel representation is used to directly model the memory functional that maps past inputs to the present output.","We explore the versatility of this approach to encode important system properties in the hyperparameters of the kernel.","The approach is illustrated on the Hodgkin and Huxley model of neuronal excitability."],"url":"http://arxiv.org/abs/2403.11945v1","category":"eess.SY"}
{"created":"2024-03-18 16:39:05","title":"The Share-a-Ride Problem with mixed ride-hailing and logistic vehicles","abstract":"This study explores the potential of using ride-hailing vehicles (RVs) for integrated passenger and freight transport based on shared mobility. In this crowd-sourced mode, ride-hailing platforms can profit from parcel delivery services, and logistics companies can reduce operational costs by utilizing the capacities of RVs. The Share-a-Ride problem with ride-hailing and logistic vehicles (SARP-RL) determines the number of logistic vehicles (LVs) and the assignment of passenger/parcel requests to RVs and LVs, aiming at maximizing the total RV profits and minimizing logistic costs. An exact solution framework is proposed by (1) generating a feasible trip that serves a given set of requests at maximal profits; (2) generating all feasible trips for the entire set of passenger and parcel requests via an efficient enumeration method; and (3) finding all Pareto-optimal solutions of the bi-objective problem via an $\\varepsilon$-constraint method. Not only is the proposed method exact, it also converts the NP-hard problem to a simple vehicle-trip matching problem. More importantly, the total computational time can be compressed to an arbitrary degree via straightforward parallelization. A case study of the Manhattan network demonstrates the solution characteristics of SARP-RL. The results indicate that: (i) Coordinating RV and LV operations to serve passenger and parcel requests (SARP-RL) can simultaneously reduce logistic costs and increase RV profits. (ii) Key factors influencing the performance of SARP-RL include the RV fleet size, spatial distribution of parcel requests, passenger/parcel request ratio, and unit price of transport service, which are quantitatively analyzed to offer managerial insights for real-world implementation.","sentences":["This study explores the potential of using ride-hailing vehicles (RVs) for integrated passenger and freight transport based on shared mobility.","In this crowd-sourced mode, ride-hailing platforms can profit from parcel delivery services, and logistics companies can reduce operational costs by utilizing the capacities of RVs.","The Share-a-Ride problem with ride-hailing and logistic vehicles (SARP-RL) determines the number of logistic vehicles (LVs) and the assignment of passenger/parcel requests to RVs and LVs, aiming at maximizing the total RV profits and minimizing logistic costs.","An exact solution framework is proposed by (1) generating a feasible trip that serves a given set of requests at maximal profits; (2) generating all feasible trips for the entire set of passenger and parcel requests via an efficient enumeration method; and (3) finding all Pareto-optimal solutions of the bi-objective problem via an $\\varepsilon$-constraint method.","Not only is the proposed method exact, it also converts the NP-hard problem to a simple vehicle-trip matching problem.","More importantly, the total computational time can be compressed to an arbitrary degree via straightforward parallelization.","A case study of the Manhattan network demonstrates the solution characteristics of SARP-RL.","The results indicate that: (i) Coordinating RV and LV operations to serve passenger and parcel requests (SARP-RL) can simultaneously reduce logistic costs and increase RV profits.","(ii) Key factors influencing the performance of SARP-RL include the RV fleet size, spatial distribution of parcel requests, passenger/parcel request ratio, and unit price of transport service, which are quantitatively analyzed to offer managerial insights for real-world implementation."],"url":"http://arxiv.org/abs/2403.11944v1","category":"math.OC"}
{"created":"2024-03-18 16:31:21","title":"Consistency of Value of Information: Effects of Packet Loss and Time Delay in Networked Control Systems Tasks","abstract":"In this chapter, we study the consistency of the value of information$\\unicode{x2014}$a semantic metric that claims to determine the right piece of information in networked control systems tasks$\\unicode{x2014}$in a lossy and delayed communication regime. Our analysis begins with a focus on state estimation, and subsequently extends to feedback control. To that end, we make a causal tradeoff between the packet rate and the mean square error. Associated with this tradeoff, we demonstrate the existence of an optimal policy profile, comprising a symmetric threshold scheduling policy based on the value of information for the encoder and a non-Gaussian linear estimation policy for the decoder. Our structural results assert that the scheduling policy is expressible in terms of $3d-1$ variables related to the source and the channel, where $d$ is the time delay, and that the estimation policy incorporates no residual related to signaling. We then construct an optimal control policy by exploiting the separation principle.","sentences":["In this chapter, we study the consistency of the value of information$\\unicode{x2014}$a semantic metric that claims to determine the right piece of information in networked control systems tasks$\\unicode{x2014}$in a lossy and delayed communication regime.","Our analysis begins with a focus on state estimation, and subsequently extends to feedback control.","To that end, we make a causal tradeoff between the packet rate and the mean square error.","Associated with this tradeoff, we demonstrate the existence of an optimal policy profile, comprising a symmetric threshold scheduling policy based on the value of information for the encoder and a non-Gaussian linear estimation policy for the decoder.","Our structural results assert that the scheduling policy is expressible in terms of $3d-1$ variables related to the source and the channel, where $d$ is the time delay, and that the estimation policy incorporates no residual related to signaling.","We then construct an optimal control policy by exploiting the separation principle."],"url":"http://arxiv.org/abs/2403.11932v1","category":"cs.IT"}
{"created":"2024-03-18 16:30:42","title":"Graph Algorithms with Neutral Atom Quantum Processors","abstract":"Neutral atom technology has steadily demonstrated significant theoretical and experimental advancements, positioning itself as a front-runner platform for running quantum algorithms. One unique advantage of this technology lies in the ability to reconfigure the geometry of the qubit register, from shot to shot. This unique feature makes possible the native embedding of graph-structured problems at the hardware level, with profound consequences for the resolution of complex optimization and machine learning tasks. By driving qubits, one can generate processed quantum states which retain graph complex properties. These states can then be leveraged to offer direct solutions to problems or as resources in hybrid quantum-classical schemes. In this paper, we review the advancements in quantum algorithms for graph problems running on neutral atom Quantum Processing Units (QPUs), and discuss recently introduced embedding and problem-solving techniques. In addition, we clarify ongoing advancements in hardware, with an emphasis on enhancing the scalability, controllability and computation repetition rate of neutral atom QPUs.","sentences":["Neutral atom technology has steadily demonstrated significant theoretical and experimental advancements, positioning itself as a front-runner platform for running quantum algorithms.","One unique advantage of this technology lies in the ability to reconfigure the geometry of the qubit register, from shot to shot.","This unique feature makes possible the native embedding of graph-structured problems at the hardware level, with profound consequences for the resolution of complex optimization and machine learning tasks.","By driving qubits, one can generate processed quantum states which retain graph complex properties.","These states can then be leveraged to offer direct solutions to problems or as resources in hybrid quantum-classical schemes.","In this paper, we review the advancements in quantum algorithms for graph problems running on neutral atom Quantum Processing Units (QPUs), and discuss recently introduced embedding and problem-solving techniques.","In addition, we clarify ongoing advancements in hardware, with an emphasis on enhancing the scalability, controllability and computation repetition rate of neutral atom QPUs."],"url":"http://arxiv.org/abs/2403.11931v1","category":"quant-ph"}
{"created":"2024-03-18 16:28:19","title":"Foundations of Value of Information: A Semantic Metric for Networked Control Systems Tasks","abstract":"In this chapter, we present our recent invention, i.e., the notion of the value of information$\\unicode{x2014}$a semantic metric that is fundamental for networked control systems tasks. We begin our analysis by formulating a causal tradeoff between the packet rate and the regulation cost, with an encoder and a decoder as two distributed decision makers, and show that the valuation of information is conceivable and quantifiable grounded on this tradeoff. More precisely, we characterize an equilibrium, and quantify the value of information there as the variation in a value function with respect to a piece of sensory measurement that can be communicated from the encoder to the decoder at each time. We prove that, in feedback control of a dynamical process over a noiseless channel, the value of information is a function of the discrepancy between the state estimates at the encoder and the decoder, and that a data packet containing a sensory measurement at each time should be exchanged only if the value of information at that time is nonnegative. Finally, we prove that the characterized equilibrium is in fact globally optimal.","sentences":["In this chapter, we present our recent invention, i.e., the notion of the value of information$\\unicode{x2014}$a semantic metric that is fundamental for networked control systems tasks.","We begin our analysis by formulating a causal tradeoff between the packet rate and the regulation cost, with an encoder and a decoder as two distributed decision makers, and show that the valuation of information is conceivable and quantifiable grounded on this tradeoff.","More precisely, we characterize an equilibrium, and quantify the value of information there as the variation in a value function with respect to a piece of sensory measurement that can be communicated from the encoder to the decoder at each time.","We prove that, in feedback control of a dynamical process over a noiseless channel, the value of information is a function of the discrepancy between the state estimates at the encoder and the decoder, and that a data packet containing a sensory measurement at each time should be exchanged only if the value of information at that time is nonnegative.","Finally, we prove that the characterized equilibrium is in fact globally optimal."],"url":"http://arxiv.org/abs/2403.11927v1","category":"cs.IT"}
{"created":"2024-03-18 16:25:15","title":"Relation between Value and Age of Information in Feedback Control","abstract":"In this chapter, we investigate the value of information as a more comprehensive instrument than the age of information for optimally shaping the information flow in a networked control system. In particular, we quantify the value of information based on the variation in a value function, and discuss the structural properties of this metric. Through our analysis, we establish the mathematical relation between the value of information and the age of information. We prove that the value of information is in general a function of an estimation discrepancy that depends on the age of information and the primitive variables. In addition, we prove that there exists a condition under which the value of information becomes completely expressible in terms of the age of information. Nonetheless, we show that this condition is not achievable without a degradation in the performance of the system.","sentences":["In this chapter, we investigate the value of information as a more comprehensive instrument than the age of information for optimally shaping the information flow in a networked control system.","In particular, we quantify the value of information based on the variation in a value function, and discuss the structural properties of this metric.","Through our analysis, we establish the mathematical relation between the value of information and the age of information.","We prove that the value of information is in general a function of an estimation discrepancy that depends on the age of information and the primitive variables.","In addition, we prove that there exists a condition under which the value of information becomes completely expressible in terms of the age of information.","Nonetheless, we show that this condition is not achievable without a degradation in the performance of the system."],"url":"http://arxiv.org/abs/2403.11926v1","category":"cs.IT"}
{"created":"2024-03-18 16:23:47","title":"Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic","abstract":"In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\\mathcal{O}\\left( \\sqrt{\\tau_{mix}} \\right)$ relative to prior work. With a 2D gridworld goal-reaching navigation experiment, we demonstrate that MAC achieves higher reward than a previous PG-based method for average reward, Parameterized Policy Gradient with Advantage Estimation (PPGAE), especially in cases with relatively small training sample budget restricting trajectory length.","sentences":["In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods.","This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications.","To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator.","With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence.","Furthermore, our approach exhibits the tightest-available dependence of $\\mathcal{O}\\left( \\sqrt{\\tau_{mix}} \\right)$ relative to prior work.","With a 2D gridworld goal-reaching navigation experiment, we demonstrate that MAC achieves higher reward than a previous PG-based method for average reward, Parameterized Policy Gradient with Advantage Estimation (PPGAE), especially in cases with relatively small training sample budget restricting trajectory length."],"url":"http://arxiv.org/abs/2403.11925v1","category":"cs.LG"}
{"created":"2024-03-18 16:12:48","title":"An Optimal-Control Approach to Infinite-Horizon Restless Bandits: Achieving Asymptotic Optimality with Minimal Assumptions","abstract":"We adopt an optimal-control framework for addressing the undiscounted infinite-horizon discrete-time restless $N$-armed bandit problem. Unlike most studies that rely on constructing policies based on the relaxed single-armed Markov Decision Process (MDP), we propose relaxing the entire bandit MDP as an optimal-control problem through the certainty equivalence control principle. Our main contribution is demonstrating that the reachability of an optimal stationary state within the optimal-control problem is a sufficient condition for the existence of an asymptotically optimal policy. Such a policy can be devised using an \"align and steer\" strategy. This reachability assumption is less stringent than any prior assumptions imposed on the arm-level MDP, notably the unichain condition is no longer needed. Through numerical examples, we show that employing model predictive control for steering generally results in superior performance compared to other existing policies.","sentences":["We adopt an optimal-control framework for addressing the undiscounted infinite-horizon discrete-time restless $N$-armed bandit problem.","Unlike most studies that rely on constructing policies based on the relaxed single-armed Markov Decision Process (MDP), we propose relaxing the entire bandit MDP as an optimal-control problem through the certainty equivalence control principle.","Our main contribution is demonstrating that the reachability of an optimal stationary state within the optimal-control problem is a sufficient condition for the existence of an asymptotically optimal policy.","Such a policy can be devised using an \"align and steer\" strategy.","This reachability assumption is less stringent than any prior assumptions imposed on the arm-level MDP, notably the unichain condition is no longer needed.","Through numerical examples, we show that employing model predictive control for steering generally results in superior performance compared to other existing policies."],"url":"http://arxiv.org/abs/2403.11913v1","category":"math.OC"}
{"created":"2024-03-18 15:53:30","title":"Quantum Coordination Rates in Multi-Partite Networks","abstract":"The optimal coordination rates are determined in three primary settings of multi-partite quantum networks, thus characterizing the minimal resources required in order to simulate a joint quantum state among multiple parties. We study the following models: (1) a cascade network with limited entanglement, (2) a broadcast network, which consists of a single sender and two receivers, (3) a multiple-access network with two senders and a single receiver. We establish the necessary and sufficient conditions on the asymptotically-achievable communication and entanglement rates in each setting. At last, we show the implications of our results on nonlocal games with quantum strategies.","sentences":["The optimal coordination rates are determined in three primary settings of multi-partite quantum networks, thus characterizing the minimal resources required in order to simulate a joint quantum state among multiple parties.","We study the following models: (1) a cascade network with limited entanglement, (2) a broadcast network, which consists of a single sender and two receivers, (3) a multiple-access network with two senders and a single receiver.","We establish the necessary and sufficient conditions on the asymptotically-achievable communication and entanglement rates in each setting.","At last, we show the implications of our results on nonlocal games with quantum strategies."],"url":"http://arxiv.org/abs/2403.11893v1","category":"quant-ph"}
{"created":"2024-03-18 15:33:28","title":"Data-Enabled Predictive Repetitive Control","abstract":"This work introduces the Data-Enabled Predictive Repetitive Control (DeePRC) algorithm, a direct data-driven approach for repetitive LTI systems. The DeePRC learns from previous iterations to improve its performance and achieves the optimal cost. By utilizing a tube-based variation of the DeePRC scheme, we propose a two-stage approach that enables safe active exploration using a left-kernel-based input disturbance design. This method generates informative trajectories to enrich the historical data, which extends the maximum achievable prediction horizon and leads to faster iteration convergence. In addition, we present an end-to-end formulation of the two-stage approach, integrating the disturbance design procedure into the planning phase. We showcase the effectiveness of the proposed algorithms on a numerical experiment.","sentences":["This work introduces the Data-Enabled Predictive Repetitive Control (DeePRC) algorithm, a direct data-driven approach for repetitive LTI systems.","The DeePRC learns from previous iterations to improve its performance and achieves the optimal cost.","By utilizing a tube-based variation of the DeePRC scheme, we propose a two-stage approach that enables safe active exploration using a left-kernel-based input disturbance design.","This method generates informative trajectories to enrich the historical data, which extends the maximum achievable prediction horizon and leads to faster iteration convergence.","In addition, we present an end-to-end formulation of the two-stage approach, integrating the disturbance design procedure into the planning phase.","We showcase the effectiveness of the proposed algorithms on a numerical experiment."],"url":"http://arxiv.org/abs/2403.11883v1","category":"eess.SY"}
{"created":"2024-03-18 15:27:35","title":"Benchmarking Analytical Query Processing in Intel SGXv2","abstract":"The recently introduced second generation of Intel SGX (SGXv2) lifts memory size limitations of the first generation. Theoretically, this promises to enable secure and highly efficient analytical DBMSs in the cloud. To validate this promise, in this paper, we conduct the first in-depth evaluation study of running analytical query processing algorithms inside SGXv2. Our study reveals that state-of-the-art query operators like radix joins and SIMD-based scans can indeed achieve high performance inside SGXv2 enclaves. These operations are orders of magnitude faster than joins optimized for the discontinued SGXv1 hardware. However, substantial performance overheads are still caused by subtle hardware and software differences influencing code execution inside an SGX enclave. We investigate these differences and propose new optimizations to bring the performance inside the enclave on par with native code execution outside an enclave.","sentences":["The recently introduced second generation of Intel SGX (SGXv2) lifts memory size limitations of the first generation.","Theoretically, this promises to enable secure and highly efficient analytical DBMSs in the cloud.","To validate this promise, in this paper, we conduct the first in-depth evaluation study of running analytical query processing algorithms inside SGXv2.","Our study reveals that state-of-the-art query operators like radix joins and SIMD-based scans can indeed achieve high performance inside SGXv2 enclaves.","These operations are orders of magnitude faster than joins optimized for the discontinued SGXv1 hardware.","However, substantial performance overheads are still caused by subtle hardware and software differences influencing code execution inside an SGX enclave.","We investigate these differences and propose new optimizations to bring the performance inside the enclave on par with native code execution outside an enclave."],"url":"http://arxiv.org/abs/2403.11874v1","category":"cs.DB"}
{"created":"2024-03-18 15:17:15","title":"Context-aware LLM-based Safe Control Against Latent Risks","abstract":"It is challenging for autonomous control systems to perform complex tasks in the presence of latent risks. Motivated by this challenge, this paper proposes an integrated framework that involves Large Language Models (LLMs), stochastic gradient descent (SGD), and optimization-based control. In the first phrase, the proposed framework breaks down complex tasks into a sequence of smaller subtasks, whose specifications account for contextual information and latent risks. In the second phase, these subtasks and their parameters are refined through a dual process involving LLMs and SGD. LLMs are used to generate rough guesses and failure explanations, and SGD is used to fine-tune parameters. The proposed framework is tested using simulated case studies of robots and vehicles. The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently.","sentences":["It is challenging for autonomous control systems to perform complex tasks in the presence of latent risks.","Motivated by this challenge, this paper proposes an integrated framework that involves Large Language Models (LLMs), stochastic gradient descent (SGD), and optimization-based control.","In the first phrase, the proposed framework breaks down complex tasks into a sequence of smaller subtasks, whose specifications account for contextual information and latent risks.","In the second phase, these subtasks and their parameters are refined through a dual process involving LLMs and SGD.","LLMs are used to generate rough guesses and failure explanations, and SGD is used to fine-tune parameters.","The proposed framework is tested using simulated case studies of robots and vehicles.","The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently."],"url":"http://arxiv.org/abs/2403.11863v1","category":"eess.SY"}
{"created":"2024-03-18 14:55:45","title":"Near-Optimal Solutions of Constrained Learning Problems","abstract":"With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we leverage the fact that non-convex, finite-dimensional constrained learning problems can be seen as parametrizations of convex, functional problems. Our results show that rich parametrizations effectively mitigate the issue of feasibility in dual methods, shedding light on prior empirical successes of dual learning. We illustrate our findings in fair learning tasks.","sentences":["With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent.","This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements.","These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms.","Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible.","Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications.","Still, final iterates have been observed to perform well in practice.","In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity.","To do this, we leverage the fact that non-convex, finite-dimensional constrained learning problems can be seen as parametrizations of convex, functional problems.","Our results show that rich parametrizations effectively mitigate the issue of feasibility in dual methods, shedding light on prior empirical successes of dual learning.","We illustrate our findings in fair learning tasks."],"url":"http://arxiv.org/abs/2403.11844v1","category":"cs.LG"}
{"created":"2024-03-18 14:45:20","title":"SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator","abstract":"Machine learning models are vulnerable to maliciously crafted Adversarial Examples (AEs). Training a machine learning model with AEs improves its robustness and stability against adversarial attacks. It is essential to develop models that produce high-quality AEs. Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision. This paper introduces a practical and efficient adversarial attack model called SSCAE for \\textbf{S}emantic, \\textbf{S}yntactic, and \\textbf{C}ontext-aware natural language \\textbf{AE}s generator. SSCAE identifies important words and uses a masked language model to generate an early set of substitutions. Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics. We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-quality AEs. As a black-box method, SSCAE generates humanly imperceptible and context-aware AEs that preserve semantic consistency and the source language's syntactical and grammatical requirements. The effectiveness and superiority of the proposed SSCAE model are illustrated with fifteen comparative experiments and extensive sensitivity analysis for parameter optimization. SSCAE outperforms the existing models in all experiments while maintaining a higher semantic consistency with a lower query number and a comparable perturbation rate.","sentences":["Machine learning models are vulnerable to maliciously crafted Adversarial Examples (AEs).","Training a machine learning model with AEs improves its robustness and stability against adversarial attacks.","It is essential to develop models that produce high-quality AEs.","Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision.","This paper introduces a practical and efficient adversarial attack model called SSCAE for \\textbf{S}emantic, \\textbf{S}yntactic, and \\textbf{C}ontext-aware natural language \\textbf{AE}s generator.","SSCAE identifies important words and uses a masked language model to generate an early set of substitutions.","Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics.","We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-quality AEs.","As a black-box method, SSCAE generates humanly imperceptible and context-aware AEs that preserve semantic consistency and the source language's syntactical and grammatical requirements.","The effectiveness and superiority of the proposed SSCAE model are illustrated with fifteen comparative experiments and extensive sensitivity analysis for parameter optimization.","SSCAE outperforms the existing models in all experiments while maintaining a higher semantic consistency with a lower query number and a comparable perturbation rate."],"url":"http://arxiv.org/abs/2403.11833v1","category":"cs.CL"}
{"created":"2024-03-18 14:30:38","title":"Nonconcave Robust Utility Maximization under Projective Determinacy","abstract":"We study a robust utility maximization problem in a general discrete-time frictionless market. The investor is assumed to have a random, nonconcave and nondecreasing utility function, which may or may not be finite on the whole real-line. She also faces model ambiguity on her beliefs about the market, which is modeled through a set of priors. We prove, using only primal methods, the existence of an optimal investment strategy when the utility function is also upper-semicontinuous. For that, we introduce the new notion of projectively measurable functions. We show basic properties of these functions as stability under sums, differences, products, suprema, infima and compositions but also assuming the set-theoretical axiom of Projective Determinacy (PD) stability under integration and existence of $\\epsilon$-optimal selectors. We consider projectively measurable random utility function and price process and assume that the graphs of the sets of local priors are projective sets. Our other assumptions are stated on a prior-by-prior basis and correspond to generally accepted assumptions in the literature on markets without ambiguity.","sentences":["We study a robust utility maximization problem in a general discrete-time frictionless market.","The investor is assumed to have a random, nonconcave and nondecreasing utility function, which may or may not be finite on the whole real-line.","She also faces model ambiguity on her beliefs about the market, which is modeled through a set of priors.","We prove, using only primal methods, the existence of an optimal investment strategy when the utility function is also upper-semicontinuous.","For that, we introduce the new notion of projectively measurable functions.","We show basic properties of these functions as stability under sums, differences, products, suprema, infima and compositions but also assuming the set-theoretical axiom of Projective Determinacy (PD) stability under integration and existence of $\\epsilon$-optimal selectors.","We consider projectively measurable random utility function and price process and assume that the graphs of the sets of local priors are projective sets.","Our other assumptions are stated on a prior-by-prior basis and correspond to generally accepted assumptions in the literature on markets without ambiguity."],"url":"http://arxiv.org/abs/2403.11824v1","category":"q-fin.MF"}
{"created":"2024-03-18 14:26:21","title":"Discovery of self-assembled Ru/Si heterostructures with unique periodic nanostripe patterns boosting hydrogen evolution","abstract":"Two-dimensional (2D) heterostructuring is a versatile methodology for designing nanoarchitecture catalytic systems that allow for reconstruction and modulation of interfaces and electronic structures. However, catalysts with such structures are extremely scarce due to limited synthetic strategies. Here, we report a highly ordered 2D Ru/Si nano-heterostructures (RSHS) by acid etching of the LaRuSi electride. RSHS shows a superior electrocatalytic activity for hydrogen evolution with an overpotential of 14 mV at 10 mA/cm2 in alkaline media. Both experimental analysis and first-principles calculations demonstrate that the electronic states of Ru can be tuned by strong interactions of the interfacial Ru-Si, leading to an optimized hydrogen adsorption energy. Moreover, due to the synergistic effect of Ru and Si, the energy barrier of water dissociation is significantly reduced. The unique nanostripe structure with abundant interfaces in RSHS will provide a paradigm for construction of efficient catalysts with tunable electronic states and dual active sites.","sentences":["Two-dimensional (2D) heterostructuring is a versatile methodology for designing nanoarchitecture catalytic systems that allow for reconstruction and modulation of interfaces and electronic structures.","However, catalysts with such structures are extremely scarce due to limited synthetic strategies.","Here, we report a highly ordered 2D Ru/Si nano-heterostructures (RSHS) by acid etching of the LaRuSi electride.","RSHS shows a superior electrocatalytic activity for hydrogen evolution with an overpotential of 14 mV at 10 mA/cm2 in alkaline media.","Both experimental analysis and first-principles calculations demonstrate that the electronic states of Ru can be tuned by strong interactions of the interfacial Ru-Si, leading to an optimized hydrogen adsorption energy.","Moreover, due to the synergistic effect of Ru and Si, the energy barrier of water dissociation is significantly reduced.","The unique nanostripe structure with abundant interfaces in RSHS will provide a paradigm for construction of efficient catalysts with tunable electronic states and dual active sites."],"url":"http://arxiv.org/abs/2403.11822v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 17:59:55","title":"The FLAMINGO project: the coupling between baryonic feedback and cosmology in light of the $S_8$ tension","abstract":"Large-scale structure surveys have reported measurements of the density of matter, $\\Omega_\\mathrm{m}$, and the amplitude of clustering, $\\sigma_8$, that are in tension with the values inferred from observations of the cosmic microwave background. While this may be a sign of new physics that slows the growth of structure at late times, strong astrophysical feedback processes could also be responsible. In this work, we argue that astrophysical processes are not independent of cosmology and that their coupling naturally leads to stronger baryonic feedback in cosmological models with suppressed structure formation or when combined with a mechanism that removes dark matter from halos. We illustrate this with two well-motivated extensions of the Standard Model known to suppress structure formation: massive neutrinos and decaying dark matter. Our results, based on the FLAMINGO suite of hydrodynamical simulations, show that the combined effect of baryonic and non-baryonic suppression mechanisms is greater than the sum of its parts, particularly for decaying dark matter. We also show that the dependence of baryonic feedback on cosmology can be modelled as a function of the ratio $f_\\mathrm{b}/c^2_\\mathrm{v}\\sim f_\\mathrm{b}/(\\Omega_\\mathrm{m}\\sigma_8)^{1/4}$ of the universal baryon fraction, $f_\\mathrm{b}$, to a velocity-based definition of halo concentration, $c^2_\\mathrm{v}$, giving an accurate fitting formula for the baryonic suppression of the matter power spectrum. Although the combination of baryonic and non-baryonic suppression mechanisms can resolve the tension, the models with neutrinos and decaying dark matter are challenged by constraints on the expansion history.","sentences":["Large-scale structure surveys have reported measurements of the density of matter, $\\Omega_\\mathrm{m}$, and the amplitude of clustering, $\\sigma_8$, that are in tension with the values inferred from observations of the cosmic microwave background.","While this may be a sign of new physics that slows the growth of structure at late times, strong astrophysical feedback processes could also be responsible.","In this work, we argue that astrophysical processes are not independent of cosmology and that their coupling naturally leads to stronger baryonic feedback in cosmological models with suppressed structure formation or when combined with a mechanism that removes dark matter from halos.","We illustrate this with two well-motivated extensions of the Standard Model known to suppress structure formation: massive neutrinos and decaying dark matter.","Our results, based on the FLAMINGO suite of hydrodynamical simulations, show that the combined effect of baryonic and non-baryonic suppression mechanisms is greater than the sum of its parts, particularly for decaying dark matter.","We also show that the dependence of baryonic feedback on cosmology can be modelled as a function of the ratio $f_\\mathrm{b}/c^2_\\mathrm{v}\\sim f_\\mathrm{b}/(\\Omega_\\mathrm{m}\\sigma_8)^{1/4}$ of the universal baryon fraction, $f_\\mathrm{b}$, to a velocity-based definition of halo concentration, $c^2_\\mathrm{v}$, giving an accurate fitting formula for the baryonic suppression of the matter power spectrum.","Although the combination of baryonic and non-baryonic suppression mechanisms can resolve the tension, the models with neutrinos and decaying dark matter are challenged by constraints on the expansion history."],"url":"http://arxiv.org/abs/2403.12967v1","category":"astro-ph.CO"}
{"created":"2024-03-19 17:48:32","title":"Statistical analysis of IUVB values in Mexico City from 2000 to 2022","abstract":"The solar radiation are electromagnetic waves, composed of infrared, visible spectrum and ultraviolet. The infrared component is the cause of thermal energy, the visible spectrum allows to see and the ultraviolet component is the most energetic part and dangerous for the human body (skin and eyes). The ultraviolet rays are divided in a wavelength range, in three parts; called UVA (100-280 nm), UVB (280-315 nm) and UVC (315-400 nm). The UVC are the most energetic (followed by the UVB rays ), however are stopped in the ozone layer. In this work it shown a statistical analysis of the UVB index measured by five station in Mexico City, from the years 2000 to 2022. Through a Gaussian fit distribution, it was found that the range when the IUVB value exceeds the value of seven, which is from 11:00 to 16:00, having an average mean value of 13:09 hrs. $\\pm$ 4 min. i.e. the time at which the UVB index reaches its maximum value. This occurs in the months from February to October. More than 80% of radiation is for UVB values less than 7 and less than the remaining 20% is for IUVB values greater than 7. Finally, it was possible to observe that the mean value per year, reaches its maximum when the solar cycles occurs, which was in the years 2003 and 2013.","sentences":["The solar radiation are electromagnetic waves, composed of infrared, visible spectrum and ultraviolet.","The infrared component is the cause of thermal energy, the visible spectrum allows to see and the ultraviolet component is the most energetic part and dangerous for the human body (skin and eyes).","The ultraviolet rays are divided in a wavelength range, in three parts; called UVA (100-280 nm), UVB (280-315 nm) and UVC (315-400 nm).","The UVC are the most energetic (followed by the UVB rays ), however are stopped in the ozone layer.","In this work it shown a statistical analysis of the UVB index measured by five station in Mexico City, from the years 2000 to 2022.","Through a Gaussian fit distribution, it was found that the range when the IUVB value exceeds the value of seven, which is from 11:00 to 16:00, having an average mean value of 13:09 hrs.","$\\pm$ 4 min. i.e. the time at which the UVB index reaches its maximum value.","This occurs in the months from February to October.","More than 80% of radiation is for UVB values less than 7 and less than the remaining 20% is for IUVB values greater than 7.","Finally, it was possible to observe that the mean value per year, reaches its maximum when the solar cycles occurs, which was in the years 2003 and 2013."],"url":"http://arxiv.org/abs/2403.12944v1","category":"physics.space-ph"}
{"created":"2024-03-19 17:36:00","title":"Revisiting the abundance pattern and charge-exchange emission in the M82 centre","abstract":"The interstellar medium (ISM) in starburst galaxies contains plenty of chemical elements synthesised by core-collapse supernova explosions. By measuring the abundances of these metals, we can study the chemical enrichment within galaxies and the transportation of metals into circumgalactic environments through powerful outflows. We perform the spectral analysis of the X-ray emissions from the M82 core using the Reflection Grating Spectrometer (RGS) onboard XMM-Newton to accurately estimate the metal abundances in the ISM. We analyse over 300 ks of RGS data observed with fourteen position angles, covering an 80 arcsec cross-dispersion width. We employ multi-temperature thermal plasma components in collisional ionisation equilibrium (CIE) to reproduce the observed spectra, each exhibiting different spatial broadenings. The O vii band CCD image shows a broader distribution compared to those for O viii and Fe-L bands. The O viii line profiles have a prominent double-peaked structure, corresponding to the northward and southward outflows. The O vii triplet feature exhibits marginal peaks, and a single CIE component, convolved with the O vii band image, approximately reproduces the spectral shape. Combining a CIE model with a charge-exchange emission model also successfully reproduces the O vii line profiles. However, the ratio of these two components varies significantly with the observed position angles, which is physically implausible. Spectral fitting of the broadband spectra suggests a multi-temperature phase in the ISM, approximated by three components at 0.1, 0.4, and 0.7 keV. Notably, the 0.1 keV component exhibits a broader distribution than the 0.4 and 0.7 keV plasmas. The derived abundance pattern shows super-solar N/O, solar Ne/O and Mg/O, and half-solar Fe/O ratios. These results indicate the chemical enrichments by core-collapse supernovae in starburst galaxies.","sentences":["The interstellar medium (ISM) in starburst galaxies contains plenty of chemical elements synthesised by core-collapse supernova explosions.","By measuring the abundances of these metals, we can study the chemical enrichment within galaxies and the transportation of metals into circumgalactic environments through powerful outflows.","We perform the spectral analysis of the X-ray emissions from the M82 core using the Reflection Grating Spectrometer (RGS) onboard XMM-Newton to accurately estimate the metal abundances in the ISM.","We analyse over 300 ks of RGS data observed with fourteen position angles, covering an 80 arcsec cross-dispersion width.","We employ multi-temperature thermal plasma components in collisional ionisation equilibrium (CIE) to reproduce the observed spectra, each exhibiting different spatial broadenings.","The O vii band CCD image shows a broader distribution compared to those for O viii and Fe-L bands.","The O viii line profiles have a prominent double-peaked structure, corresponding to the northward and southward outflows.","The O vii triplet feature exhibits marginal peaks, and a single CIE component, convolved with the O vii band image, approximately reproduces the spectral shape.","Combining a CIE model with a charge-exchange emission model also successfully reproduces the O vii line profiles.","However, the ratio of these two components varies significantly with the observed position angles, which is physically implausible.","Spectral fitting of the broadband spectra suggests a multi-temperature phase in the ISM, approximated by three components at 0.1, 0.4, and 0.7 keV.","Notably, the 0.1 keV component exhibits a broader distribution than the 0.4 and 0.7 keV plasmas.","The derived abundance pattern shows super-solar N/O, solar Ne/O and Mg/O, and half-solar Fe/O ratios.","These results indicate the chemical enrichments by core-collapse supernovae in starburst galaxies."],"url":"http://arxiv.org/abs/2403.12932v1","category":"astro-ph.GA"}
{"created":"2024-03-19 17:12:43","title":"Unitarity constraints on large multiplets of arbitrary gauge groups","abstract":"We impose partial-wave unitarity on $2 \\to 2$ tree-level scattering processes to derive constraints on the dimensions of large scalar and fermionic multiplets of arbitrary gauge groups. We apply our results to scalar and fermionic extensions of the Standard Model, and also to the Grand Unified Theories (GUTs) based on the groups $SU(5)$, $SO(10)$, and $E_6$. We find scenarios within the latter two GUTs that violate the unitarity condition; this may require a reevaluation of the validity of perturbation theory in those scenarios.","sentences":["We impose partial-wave unitarity on $2 \\to 2$ tree-level scattering processes to derive constraints on the dimensions of large scalar and fermionic multiplets of arbitrary gauge groups.","We apply our results to scalar and fermionic extensions of the Standard Model, and also to the Grand Unified Theories (GUTs) based on the groups $SU(5)$, $SO(10)$, and $E_6$. We find scenarios within the latter two GUTs that violate the unitarity condition; this may require a reevaluation of the validity of perturbation theory in those scenarios."],"url":"http://arxiv.org/abs/2403.12914v1","category":"hep-ph"}
{"created":"2024-03-19 16:58:45","title":"Social bots sour activist sentiment without eroding engagement","abstract":"Social media platforms have witnessed a substantial increase in social bot activity, significantly affecting online discourse. Our study explores the dynamic nature of bot engagement related to Extinction Rebellion climate change protests from 18 November 2019 to 10 December 2019. We find that bots exert a greater influence on human behavior than vice versa during heated online periods. To assess the causal impact of human-bot communication, we compared communication histories between human users who directly interacted with bots and matched human users who did not. Our findings demonstrate a consistent negative impact of bot interactions on subsequent human sentiment, with exposed users displaying significantly more negative sentiment than their counterparts. Furthermore, the nature of bot interaction influences human tweeting activity and the sentiment towards protests. Political astroturfing bots increase activity, whereas other bots decrease it. Sentiment changes towards protests depend on the user's original support level, indicating targeted manipulation. However, bot interactions do not change activists' engagement towards protests. Despite the seemingly minor impact of individual bot encounters, the cumulative effect is profound due to the large volume of bot communication. Our findings underscore the importance of unrestricted access to social media data for studying the prevalence and influence of social bots, as with new technological advancements distinguishing between bots and humans becomes nearly impossible.","sentences":["Social media platforms have witnessed a substantial increase in social bot activity, significantly affecting online discourse.","Our study explores the dynamic nature of bot engagement related to Extinction Rebellion climate change protests from 18 November 2019 to 10 December 2019.","We find that bots exert a greater influence on human behavior than vice versa during heated online periods.","To assess the causal impact of human-bot communication, we compared communication histories between human users who directly interacted with bots and matched human users who did not.","Our findings demonstrate a consistent negative impact of bot interactions on subsequent human sentiment, with exposed users displaying significantly more negative sentiment than their counterparts.","Furthermore, the nature of bot interaction influences human tweeting activity and the sentiment towards protests.","Political astroturfing bots increase activity, whereas other bots decrease it.","Sentiment changes towards protests depend on the user's original support level, indicating targeted manipulation.","However, bot interactions do not change activists' engagement towards protests.","Despite the seemingly minor impact of individual bot encounters, the cumulative effect is profound due to the large volume of bot communication.","Our findings underscore the importance of unrestricted access to social media data for studying the prevalence and influence of social bots, as with new technological advancements distinguishing between bots and humans becomes nearly impossible."],"url":"http://arxiv.org/abs/2403.12904v1","category":"cs.CY"}
{"created":"2024-03-19 16:16:17","title":"H$\u03b1$/H$\u03b2$ a Galactic Low Energy Cosmic Rays tracer","abstract":"Context. Investigating the diagnostic power of H$\\alpha$/H$\\beta$ Charge-Exchange (CE) emission as Low-Energy Galactic Cosmic Rays(LECRs) tracer in diffuse regions. Aims. In this work, we define and test a spectroscopic indicator of CE reactions between LECRs protons and neutral hydrogen atoms of the diffuse medium. This indicator can be used for mapping LECRs density in diffuse clouds and can lead to the identification of new LECRs sources as we expect density variations caused by the distance between an observed cloud and the nearest site of particle acceleration. We also lay the foundations for the definition of a photometric indicator to be used in the next full-sky photometric surveys such as the Vera Rubin 10-year Legacy Survey of Space and Time (LSST). Methods. Based on literature cross-sections, we calculate H$\\alpha$/H$\\beta$ line profile ratio in the case of CE and compare it with the recombination ratio. We then test our results on the Balmer-dominated filaments of the SNR RCW 86 and we explore how the spectroscopic constraints can turn into a photometric indicator based on colour indices. Results. We find that, in shocked environments, CE between LECRS and neutral hydrogen become the dominant process for Balmer lines emission. The hydrogen spectroscopic emission is expected to be modified, with respect to the recombination Balmer decrement,to result in double the H$\\alpha$/H$\\beta$ with respect to a similar but quiescent region. The test on the known Balmer-dominated filaments of the SNR RCW 86 confirm the efficiency of our spectroscopic indicator. Therefore we explore possible conversions of the spectroscopic indicator into colour indices combinations. This is the first step toward the definition and test of a photometric indicator for tracing LECRs to be applied in the LSST pipelines to photometrically identify new LECRs accelerators in the whole Galaxy.","sentences":["Context.","Investigating the diagnostic power of H$\\alpha$/H$\\beta$ Charge-Exchange (CE) emission as Low-Energy Galactic Cosmic Rays(LECRs) tracer in diffuse regions.","Aims.","In this work, we define and test a spectroscopic indicator of CE reactions between LECRs protons and neutral hydrogen atoms of the diffuse medium.","This indicator can be used for mapping LECRs density in diffuse clouds and can lead to the identification of new LECRs sources as we expect density variations caused by the distance between an observed cloud and the nearest site of particle acceleration.","We also lay the foundations for the definition of a photometric indicator to be used in the next full-sky photometric surveys such as the Vera Rubin 10-year Legacy Survey of Space and Time (LSST).","Methods.","Based on literature cross-sections, we calculate H$\\alpha$/H$\\beta$ line profile ratio in the case of CE and compare it with the recombination ratio.","We then test our results on the Balmer-dominated filaments of the SNR RCW 86 and we explore how the spectroscopic constraints can turn into a photometric indicator based on colour indices.","Results.","We find that, in shocked environments, CE between LECRS and neutral hydrogen become the dominant process for Balmer lines emission.","The hydrogen spectroscopic emission is expected to be modified, with respect to the recombination Balmer decrement,to result in double the H$\\alpha$/H$\\beta$ with respect to a similar but quiescent region.","The test on the known Balmer-dominated filaments of the SNR RCW 86 confirm the efficiency of our spectroscopic indicator.","Therefore we explore possible conversions of the spectroscopic indicator into colour indices combinations.","This is the first step toward the definition and test of a photometric indicator for tracing LECRs to be applied in the LSST pipelines to photometrically identify new LECRs accelerators in the whole Galaxy."],"url":"http://arxiv.org/abs/2403.12872v1","category":"astro-ph.GA"}
{"created":"2024-03-19 16:11:30","title":"Model-independent predictions for decays of double-heavy hadrons into pairs of heavy hadrons","abstract":"Double-heavy hadrons can decay into pairs of heavy hadrons through transitions from confining Born-Oppenheimer potentials to hadron-pair potentials with the same Born-Oppenheimer quantum numbers. The transitions are also constrained by conservation of angular momentum and parity. From these constraints, we derive model-independent selection rules for decays of double-heavy hadrons into pairs of heavy hadrons. The coupling potentials are expressed as sums of products of Born-Oppenheimer transition amplitudes and angular-momentum coefficients. If there is a single dominant Born-Oppenheimer transition amplitude, it factors out of the coupling potentials between double-heavy hadrons in the same Born-Oppenheimer multiplet and pairs of heavy hadrons in specific heavy-quark-spin-symmetry doublets. If furthermore the kinetic energies of the heavy hadrons are much larger than their spin splittings, we obtain analytic expressions for the relative partial decay rates in terms of Wigner 6-j and 9-j symbols. We consider in detail the decays of quarkonia and quarkonium hybrids into the lightest heavy-meson pairs. For quarkonia, our model-independent selection rules and relative partial decay rates agree with previous results from quark-pair-creation models in simple cases and give stronger results in other cases. For quarkonium hybrids, we find disagreement even in simple cases.","sentences":["Double-heavy hadrons can decay into pairs of heavy hadrons through transitions from confining Born-Oppenheimer potentials to hadron-pair potentials with the same Born-Oppenheimer quantum numbers.","The transitions are also constrained by conservation of angular momentum and parity.","From these constraints, we derive model-independent selection rules for decays of double-heavy hadrons into pairs of heavy hadrons.","The coupling potentials are expressed as sums of products of Born-Oppenheimer transition amplitudes and angular-momentum coefficients.","If there is a single dominant Born-Oppenheimer transition amplitude, it factors out of the coupling potentials between double-heavy hadrons in the same Born-Oppenheimer multiplet and pairs of heavy hadrons in specific heavy-quark-spin-symmetry doublets.","If furthermore the kinetic energies of the heavy hadrons are much larger than their spin splittings, we obtain analytic expressions for the relative partial decay rates in terms of Wigner 6-j and 9-j symbols.","We consider in detail the decays of quarkonia and quarkonium hybrids into the lightest heavy-meson pairs.","For quarkonia, our model-independent selection rules and relative partial decay rates agree with previous results from quark-pair-creation models in simple cases and give stronger results in other cases.","For quarkonium hybrids, we find disagreement even in simple cases."],"url":"http://arxiv.org/abs/2403.12868v1","category":"hep-ph"}
{"created":"2024-03-19 15:56:37","title":"Observation of spectral lines in the exceptional GRB 221009A","abstract":"As the brightest gamma-ray burst ever observed, GRB 221009A provided a precious opportunity to explore spectral line features. In this paper, we performed a comprehensive spectroscopy analysis of GRB 221009A jointly with GECAM-C and Fermi/GBM data to search for emission and absorption lines. For the first time we investigated the line feature throughout this GRB including the most bright part where many instruments suffered problems, and identified prominent emission lines in multiple time intervals. The central energy of the Gaussian emission line evolves from about 37 MeV to 6 MeV, with a nearly constant ratio (about 10\\%) between the line width and central energy. Particularly, we find that both the central energy and the energy flux of the emission line evolve with time as a power law decay with power law index of -1 and -2 respectively. We suggest that the observed emission lines most likely originate from the blue-shifted electron positron pair annihilation 511 keV line. We find that a standard high latitude emission scenario cannot fully interpret the observation, thus we propose that the emission line comes from some dense clumps with electron positron pairs traveling together with the jet. In this scenario, we can use the emission line to directly, for the first time, measure the bulk Lorentz factor of the jet ($\\Gamma$) and reveal its time evolution (i.e. $\\Gamma \\sim t^{-1}$) during the prompt emission. Interestingly, we find that the flux of the annihilation line in the co-moving frame keeps constant. These discoveries of the spectral line features shed new and important lights on the physics of GRB and relativistic jet.","sentences":["As the brightest gamma-ray burst ever observed, GRB 221009A provided a precious opportunity to explore spectral line features.","In this paper, we performed a comprehensive spectroscopy analysis of GRB 221009A jointly with GECAM-C and Fermi/GBM data to search for emission and absorption lines.","For the first time we investigated the line feature throughout this GRB including the most bright part where many instruments suffered problems, and identified prominent emission lines in multiple time intervals.","The central energy of the Gaussian emission line evolves from about 37 MeV to 6 MeV, with a nearly constant ratio (about 10\\%) between the line width and central energy.","Particularly, we find that both the central energy and the energy flux of the emission line evolve with time as a power law decay with power law index of -1 and -2","respectively.","We suggest that the observed emission lines most likely originate from the blue-shifted electron positron pair annihilation 511 keV line.","We find that a standard high latitude emission scenario cannot fully interpret the observation, thus we propose that the emission line comes from some dense clumps with electron positron pairs traveling together with the jet.","In this scenario, we can use the emission line to directly, for the first time, measure the bulk Lorentz factor of the jet ($\\Gamma$) and reveal its time evolution (i.e. $\\Gamma \\sim t^{-1}$) during the prompt emission.","Interestingly, we find that the flux of the annihilation line in the co-moving frame keeps constant.","These discoveries of the spectral line features shed new and important lights on the physics of GRB and relativistic jet."],"url":"http://arxiv.org/abs/2403.12851v1","category":"astro-ph.HE"}
{"created":"2024-03-19 15:54:01","title":"Higher Derivative Muffin Tin Orbitals (HDMTO) and Higher Derivative Koringa Khon and Rostoker (HDKKR) methods","abstract":"In this work we introduce a Linearized version of the Koringa Khon and Rostoker method (LKKR) and show it to be equivalent to the Linearized Muffin Tin Orbitals method (LMTO). We then present higher derivative versions of both methods, e.g. HDKKR and HDMTO and show them to be partially distinct (not equivalent). In particular HDKKR basis set does not have an equivalent ground state for the Khon Sham (KS) Hamiltonian as the HDKKR basis set and has greater variational power than the HDMTO one. Because the KS method, for Density Functional Theory (DFT), is variational HDKKR will give better ground state energies than HDMTO. However HDKKR is much harder to work with then HDMTO requiring much greater computer resources so HDMTO can often be preferred.","sentences":["In this work we introduce a Linearized version of the Koringa Khon and Rostoker method (LKKR) and show it to be equivalent to the Linearized Muffin Tin Orbitals method (LMTO).","We then present higher derivative versions of both methods, e.g. HDKKR and HDMTO and show them to be partially distinct (not equivalent).","In particular HDKKR basis set does not have an equivalent ground state for the Khon Sham (KS) Hamiltonian as the HDKKR basis set and has greater variational power than the HDMTO one.","Because the KS method, for Density Functional Theory (DFT), is variational HDKKR will give better ground state energies than HDMTO.","However HDKKR is much harder to work with then HDMTO requiring much greater computer resources so HDMTO can often be preferred."],"url":"http://arxiv.org/abs/2403.12846v1","category":"physics.chem-ph"}
{"created":"2024-03-19 15:40:54","title":"Weak exactness and amalgamated free product of von Neumann algebras","abstract":"We show that the amalgamated free product of weakly exact von Neumann algebras is weakly exact. This is done by using a universal property of Toeplitz-Pimsner algebras and a locally convex topology on bimodules of von Neumann algebras, which is used to characterize weakly exact von Neumann algebras.","sentences":["We show that the amalgamated free product of weakly exact von Neumann algebras is weakly exact.","This is done by using a universal property of Toeplitz-Pimsner algebras and a locally convex topology on bimodules of von Neumann algebras, which is used to characterize weakly exact von Neumann algebras."],"url":"http://arxiv.org/abs/2403.12833v1","category":"math.OA"}
{"created":"2024-03-19 15:07:16","title":"Fermi gas formalism for D-type quiver Chern-Simons theory with non-uniform ranks","abstract":"We construct the Fermi gas formalism for the partition function of supersymmetric Chern-Simons theories with affine $D$-type quiver diagrams with non-uniform ranks of the gauge groups and Fayet-Illiopoulos parameters by two different approaches: the open string formalism and the closed string formalism. In the closed string formalism approach, we find a novel connection between the partition function of this theory and the partition function of a four-nodes circular quiver supersymmetric Chern-Simons theory. We also studied a symmetry of a density matrix appeared in the closed string formalism. We further calculate the exact values of the partition function for finite $N$, with which we identified the exponent of the leading non-perturbative effect in $1/N$ corresponding to the worldsheet instantons in the circular quiver supersymmetric Chern-Simons theories.","sentences":["We construct the Fermi gas formalism for the partition function of supersymmetric Chern-Simons theories with affine $D$-type quiver diagrams with non-uniform ranks of the gauge groups and Fayet-Illiopoulos parameters by two different approaches: the open string formalism and the closed string formalism.","In the closed string formalism approach, we find a novel connection between the partition function of this theory and the partition function of a four-nodes circular quiver supersymmetric Chern-Simons theory.","We also studied a symmetry of a density matrix appeared in the closed string formalism.","We further calculate the exact values of the partition function for finite $N$, with which we identified the exponent of the leading non-perturbative effect in $1/N$ corresponding to the worldsheet instantons in the circular quiver supersymmetric Chern-Simons theories."],"url":"http://arxiv.org/abs/2403.12808v1","category":"hep-th"}
{"created":"2024-03-19 15:05:38","title":"The Bayes Principle and Segal Axioms for $P(\u03c6)_2$, with application to Periodic Covers","abstract":"The goal of the present paper is to reconcile the so-called $P(\\phi)_2$ model from classical constructive quantum field theory (CQFT) with (metric version of) geometric and categorical axioms proposed by G. Segal \\cite{Segal} in the 90's, in showing that the $P(\\phi)_2$ model satisfies these axioms, appropriately adjusted. The approach is based on previous works of Dimock arXiv:math-ph/0305017 and Pickrell arXiv:math-ph/0702077, while we give a new proof to a key step using what we call ``the Bayes principle'' of conditional probabilities in the infinite dimensional setting. We also give a precise statement and full proof of the locality of the $P(\\phi)_2$ interaction. After that, we note that the so-called ``transfer operator'' associated to a cobordism has strictly positive kernel, this implies the operator has a spectral gap and allows to define a $P(\\phi)_2$ Gibbs state on non-compact periodic curved surfaces (namely, with discrete ``time translations'') by analogy with 1-dimensional spin chains. This also implies asymptotic properties of the $P(\\phi)_2$ partition function on periodic covers of large degrees.","sentences":["The goal of the present paper is to reconcile the so-called $P(\\phi)_2$ model from classical constructive quantum field theory (CQFT) with (metric version of) geometric and categorical axioms proposed by G. Segal \\cite{Segal} in the 90's, in showing that the $P(\\phi)_2$ model satisfies these axioms, appropriately adjusted.","The approach is based on previous works of Dimock arXiv:math-ph/0305017 and Pickrell arXiv:math-ph/0702077, while we give a new proof to a key step using what we call ``the Bayes principle'' of conditional probabilities in the infinite dimensional setting.","We also give a precise statement and full proof of the locality of the $P(\\phi)_2$ interaction.","After that, we note that the so-called ``transfer operator'' associated to a cobordism has strictly positive kernel, this implies the operator has a spectral gap and allows to define a $P(\\phi)_2$ Gibbs state on non-compact periodic curved surfaces (namely, with discrete ``time translations'') by analogy with 1-dimensional spin chains.","This also implies asymptotic properties of the $P(\\phi)_2$ partition function on periodic covers of large degrees."],"url":"http://arxiv.org/abs/2403.12804v1","category":"math-ph"}
{"created":"2024-03-19 15:02:51","title":"$B$ Physics: From Present to Future Colliders","abstract":"These proceedings provide a brief overview of the status of $B$ meson physics, putting particular emphasis on precision tests of the Standard Model with meson mixing data, and on the anomalies in charged- and neutral-current semileptonic $B$ decays. In addition to summarising the current status, some promising directions to be pursued at future collider experiments are highlighted.","sentences":["These proceedings provide a brief overview of the status of $B$ meson physics, putting particular emphasis on precision tests of the Standard Model with meson mixing data, and on the anomalies in charged- and neutral-current semileptonic $B$ decays.","In addition to summarising the current status, some promising directions to be pursued at future collider experiments are highlighted."],"url":"http://arxiv.org/abs/2403.12802v1","category":"hep-ph"}
{"created":"2024-03-19 14:57:36","title":"Extraction of trans-helicity worm-gear distributions and opportunities at the Electron-Ion Collider in China","abstract":"We present a global analysis of the trans-helicity worm-gear distribution function, $g_{1T}^\\perp$, by fitting the longitudinal-transverse double spin asymmetry data of the semi-inclusive deep inelastic scattering. The analysis is performed within the framework of transverse momentum dependent factorization and evolution. It is found that the $u$-quark favors a positive distribution and the $d$-quark favors a negative distribution, which is consistent with previous model calculations and phenomenological extractions. Based on the fit to existing world data, we also study the impact of the proposed electron-ion collider in China and conclude that it can significantly improve the precision of the worm-gear distribution function and hence enhance our understanding of nucleon spin structures.","sentences":["We present a global analysis of the trans-helicity worm-gear distribution function, $g_{1T}^\\perp$, by fitting the longitudinal-transverse double spin asymmetry data of the semi-inclusive deep inelastic scattering.","The analysis is performed within the framework of transverse momentum dependent factorization and evolution.","It is found that the $u$-quark favors a positive distribution and the $d$-quark favors a negative distribution, which is consistent with previous model calculations and phenomenological extractions.","Based on the fit to existing world data, we also study the impact of the proposed electron-ion collider in China and conclude that it can significantly improve the precision of the worm-gear distribution function and hence enhance our understanding of nucleon spin structures."],"url":"http://arxiv.org/abs/2403.12795v1","category":"hep-ph"}
{"created":"2024-03-19 14:00:00","title":"Refined sheaf counting on local K3 surfaces","abstract":"We compute all refined sheaf counting invariants -- Vafa-Witten, reduced DT, stable pairs and Gopakumar-Vafa -- for all classes on local $K3$ surfaces. Along the way we develop rank 0 Vafa-Witten theory on $K3$ surfaces.   An important feature of the calculation is that the ``instanton contribution\" -- of sheaves supported scheme theoretically on $S$ -- to any of the invariants depends only on the square of the class, not its divisibility.","sentences":["We compute all refined sheaf counting invariants -- Vafa-Witten, reduced DT, stable pairs and Gopakumar-Vafa -- for all classes on local $K3$ surfaces.","Along the way we develop rank 0","Vafa-Witten theory on $K3$ surfaces.   ","An important feature of the calculation is that the ``instanton contribution\" -- of sheaves supported scheme theoretically on $S$ -- to any of the invariants depends only on the square of the class, not its divisibility."],"url":"http://arxiv.org/abs/2403.12741v1","category":"math.AG"}
{"created":"2024-03-19 13:41:42","title":"Intricacies of CO2-Basalt Interactions, Reactive Flow and Carbon Mineralization: Bridging Numerical Forecasts to Empirical Realities","abstract":"Subsurface fluid flow and solute transport are pivotal in addressing pressing energy, environmental, and societal challenges, such as geological CO2 storage. Basaltic rocks have gained prominence as suitable geological substrates for injecting substantial CO2 volumes and carbon mineralization, driven by their widespread occurrence, high concentrations of cation-rich silicate minerals, reported fast mineralization rate, and favorable characteristics such as porosity, permeability, and injectivity. The mineralization process within basaltic rocks is intricately linked, involving the dissolution of silicate minerals and the subsequent precipitation of carbonate minerals. Columnar flow and batch surface growth experiments revealed the spontaneous formation of a limited number of large crystals at various locations, rationalized by the overarching influence of probabilistic mineral nucleation. Experiments with CO2-acidified brine versus freshwater prove to be more challenging regarding the sweet spots for heavy carbon mineralization due to clay formation on the surface, particularly smectites. Despite numerical predictions suggesting the formation of MgFeCa-carbonates in CO2-basalt interactions at higher temperatures, our laboratory findings primarily indicated the growth of calcium carbonates. The experimental and numerical outcomes highlight the necessity of a probabilistic approach for accurately modeling reaction kinetics, crystal growth distribution, and the dynamic interplay between reactive flow, geochemical reactions, mineral carbonation, and geometry alteration.","sentences":["Subsurface fluid flow and solute transport are pivotal in addressing pressing energy, environmental, and societal challenges, such as geological CO2 storage.","Basaltic rocks have gained prominence as suitable geological substrates for injecting substantial CO2 volumes and carbon mineralization, driven by their widespread occurrence, high concentrations of cation-rich silicate minerals, reported fast mineralization rate, and favorable characteristics such as porosity, permeability, and injectivity.","The mineralization process within basaltic rocks is intricately linked, involving the dissolution of silicate minerals and the subsequent precipitation of carbonate minerals.","Columnar flow and batch surface growth experiments revealed the spontaneous formation of a limited number of large crystals at various locations, rationalized by the overarching influence of probabilistic mineral nucleation.","Experiments with CO2-acidified brine versus freshwater prove to be more challenging regarding the sweet spots for heavy carbon mineralization due to clay formation on the surface, particularly smectites.","Despite numerical predictions suggesting the formation of MgFeCa-carbonates in CO2-basalt interactions at higher temperatures, our laboratory findings primarily indicated the growth of calcium carbonates.","The experimental and numerical outcomes highlight the necessity of a probabilistic approach for accurately modeling reaction kinetics, crystal growth distribution, and the dynamic interplay between reactive flow, geochemical reactions, mineral carbonation, and geometry alteration."],"url":"http://arxiv.org/abs/2403.12724v1","category":"physics.flu-dyn"}
{"created":"2024-03-19 12:51:26","title":"Mass spectra of heavy hybrid quarkonia and $\\overline{b}gc$ mesons","abstract":"Masses and current couplings of the charmonium and bottomonium hybrids $ \\overline{c}gc$ and $\\overline{b}gb$ with spin-parities $J^{\\mathrm{PC} }=0^{++},\\ 0^{+-},\\ 0^{-+},\\ 0^{--}$ and $1^{++},\\ 1^{+-},\\ 1^{-+},\\ 1^{--}$ are calculated using QCD two-point sum rule method. Computations are performed by taking into account gluon condensates up to dimension 12 including terms $\\sim \\langle g_{s}^{3}G^{3}\\rangle ^{2}$. The parameters of the bottom-charm hybrids $\\overline{b}gc$ with quantum numbers $J^{\\mathrm{PC }}=0^{+},\\ 0^{-},\\ 1^{+}$, and $1^{-}$ are calculated as well. In computations the dominance of the pole contribution to sum rule results is ensured. It is demonstrated that all charmonia hybrids decay strongly to two-meson final states. The bottomonium hybrids $0^{-+}$ and $1^{-+}$ as well as the bottom-charm hybrids $0^{-(+)}$ and $1^{-(+)}$ may be stable against strong two-meson decay modes. Results of the present work are compared with ones obtained using the sum rule and alternative approaches. Our predictions for parameters of the heavy hybrid mesons may be useful to study their various decay channels which are important for interpretation of ongoing and future experiments.","sentences":["Masses and current couplings of the charmonium and bottomonium hybrids $ \\overline{c}gc$ and $\\overline{b}gb$ with spin-parities $J^{\\mathrm{PC} }=0^{++},\\ 0^{+-},\\ 0^{-+},\\ 0^{--}$","and $1^{++},\\ 1^{+-},\\ 1^{-+},\\ 1^{--}$ are calculated using QCD two-point sum rule method.","Computations are performed by taking into account gluon condensates up to dimension 12 including terms $\\sim \\langle g_{s}^{3}G^{3}\\rangle ^{2}$.","The parameters of the bottom-charm hybrids $\\overline{b}gc$ with quantum numbers $J^{\\mathrm{PC }}=0^{+},\\ 0^{-},\\ 1^{+}$, and $1^{-}$ are calculated as well.","In computations the dominance of the pole contribution to sum rule results is ensured.","It is demonstrated that all charmonia hybrids decay strongly to two-meson final states.","The bottomonium hybrids $0^{-+}$ and $1^{-+}$ as well as the bottom-charm hybrids $0^{-(+)}$ and $1^{-(+)}$ may be stable against strong two-meson decay modes.","Results of the present work are compared with ones obtained using the sum rule and alternative approaches.","Our predictions for parameters of the heavy hybrid mesons may be useful to study their various decay channels which are important for interpretation of ongoing and future experiments."],"url":"http://arxiv.org/abs/2403.12692v1","category":"hep-ph"}
{"created":"2024-03-19 12:34:42","title":"Plasma motions in the solar corona and solar wind to 1 au, as inferred from radio wave scattering observations","abstract":"Radio signals propagating via the solar corona and solar wind are significantly affected by compressive waves, impacting solar burst properties as well as sources viewed through the turbulent atmosphere. While static fluctuations scatter radio waves elastically, moving, turbulent or oscillating density irregularities act to broaden the frequency of the scattered waves. Using a new anisotropic density fluctuation model based on solar radio bursts, we deduce the plasma velocities required to explain observations of spacecraft signal frequency broadening. The frequency broadening is consistent with motions that are dominated by the solar wind at distances $\\gtrsim 10$ $R_\\odot$, but the levels of frequency broadening for $\\lesssim 10$ $R_\\odot$ require additional radial speeds $\\sim (100-300)$ km s$^{-1}$ and/or transverse speeds $\\sim (20-70)$ km s$^{-1}$. The inferred radial velocities appear consistent with the sound or proton thermal speeds, while the speeds perpendicular to the radial direction are consistent with non-thermal motions measured via coronal Doppler-line broadening, interpreted as Alfv\\'enic fluctuations. Landau damping of parallel propagating ion-sound (slow MHD) waves allow an estimate of the proton heating rate. The energy deposition rates due to ion-sound wave damping peak at a heliocentric distance of $\\sim(1-3)$ $R_\\odot$ are comparable to the rates available from a turbulent cascade of Alfv\\'enic waves at large scales, suggesting a coherent picture of energy transfer, via the cascade or/and parametric decay of Alfv\\'en waves to the small scales where heating takes place.","sentences":["Radio signals propagating via the solar corona and solar wind are significantly affected by compressive waves, impacting solar burst properties as well as sources viewed through the turbulent atmosphere.","While static fluctuations scatter radio waves elastically, moving, turbulent or oscillating density irregularities act to broaden the frequency of the scattered waves.","Using a new anisotropic density fluctuation model based on solar radio bursts, we deduce the plasma velocities required to explain observations of spacecraft signal frequency broadening.","The frequency broadening is consistent with motions that are dominated by the solar wind at distances $\\gtrsim 10$ $R_\\odot$, but the levels of frequency broadening for $\\lesssim 10$ $R_\\odot$ require additional radial speeds $\\sim (100-300)$ km s$^{-1}$ and/or transverse speeds $\\sim (20-70)$ km","s$^{-1}$.","The inferred radial velocities appear consistent with the sound or proton thermal speeds, while the speeds perpendicular to the radial direction are consistent with non-thermal motions measured via coronal Doppler-line broadening, interpreted as Alfv\\'enic fluctuations.","Landau damping of parallel propagating ion-sound (slow MHD) waves allow an estimate of the proton heating rate.","The energy deposition rates due to ion-sound wave damping peak at a heliocentric distance of $\\sim(1-3)$ $R_\\odot$ are comparable to the rates available from a turbulent cascade of Alfv\\'enic waves at large scales, suggesting a coherent picture of energy transfer, via the cascade or/and parametric decay of Alfv\\'en waves to the small scales where heating takes place."],"url":"http://arxiv.org/abs/2403.12680v1","category":"astro-ph.SR"}
{"created":"2024-03-19 12:02:38","title":"Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean","abstract":"Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy. Overall, RemBERT emerges as the most promising model. Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner.","sentences":["Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number.","An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology).","Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources.","In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup.","We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy.","Overall, RemBERT emerges as the most promising model.","Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner."],"url":"http://arxiv.org/abs/2403.12666v1","category":"cs.CL"}
{"created":"2024-03-19 11:48:35","title":"Tuning-Free Image Customization with Image and Text Guidance","abstract":"Despite significant advancements in image customization with diffusion models, current methods still have several limitations: 1) unintended changes in non-target areas when regenerating the entire image; 2) guidance solely by a reference image or text descriptions; and 3) time-consuming fine-tuning, which limits their practical application. In response, we introduce a tuning-free framework for simultaneous text-image-guided image customization, enabling precise editing of specific image regions within seconds. Our approach preserves the semantic features of the reference image subject while allowing modification of detailed attributes based on text descriptions. To achieve this, we propose an innovative attention blending strategy that blends self-attention features in the UNet decoder during the denoising process. To our knowledge, this is the first tuning-free method that concurrently utilizes text and image guidance for image customization in specific regions. Our approach outperforms previous methods in both human and quantitative evaluations, providing an efficient solution for various practical applications, such as image synthesis, design, and creative photography.","sentences":["Despite significant advancements in image customization with diffusion models, current methods still have several limitations: 1) unintended changes in non-target areas when regenerating the entire image; 2) guidance solely by a reference image or text descriptions; and 3) time-consuming fine-tuning, which limits their practical application.","In response, we introduce a tuning-free framework for simultaneous text-image-guided image customization, enabling precise editing of specific image regions within seconds.","Our approach preserves the semantic features of the reference image subject while allowing modification of detailed attributes based on text descriptions.","To achieve this, we propose an innovative attention blending strategy that blends self-attention features in the UNet decoder during the denoising process.","To our knowledge, this is the first tuning-free method that concurrently utilizes text and image guidance for image customization in specific regions.","Our approach outperforms previous methods in both human and quantitative evaluations, providing an efficient solution for various practical applications, such as image synthesis, design, and creative photography."],"url":"http://arxiv.org/abs/2403.12658v1","category":"cs.CV"}
{"created":"2024-03-19 11:37:30","title":"Composite likelihood estimation of stationary Gaussian processes with a view toward stochastic volatility","abstract":"We develop a framework for composite likelihood inference of parametric continuous-time stationary Gaussian processes. We derive the asymptotic theory of the associated maximum composite likelihood estimator. We implement our approach on a pair of models that has been proposed to describe the random log-spot variance of financial asset returns. A simulation study shows that it delivers good performance in these settings and improves upon a method-of-moments estimation. In an application, we inspect the dynamic of an intraday measure of spot variance computed with high-frequency data from the cryptocurrency market. The empirical evidence supports a mechanism, where the short- and long-term correlation structure of stochastic volatility are decoupled in order to capture its properties at different time scales.","sentences":["We develop a framework for composite likelihood inference of parametric continuous-time stationary Gaussian processes.","We derive the asymptotic theory of the associated maximum composite likelihood estimator.","We implement our approach on a pair of models that has been proposed to describe the random log-spot variance of financial asset returns.","A simulation study shows that it delivers good performance in these settings and improves upon a method-of-moments estimation.","In an application, we inspect the dynamic of an intraday measure of spot variance computed with high-frequency data from the cryptocurrency market.","The empirical evidence supports a mechanism, where the short- and long-term correlation structure of stochastic volatility are decoupled in order to capture its properties at different time scales."],"url":"http://arxiv.org/abs/2403.12653v1","category":"econ.EM"}
{"created":"2024-03-19 11:30:15","title":"Calibration of the RED-100 detector at the Kalinin nuclear power plant","abstract":"RED-100 is a two-phase Xe detector designed and built for the study of coherent elastic neutrino-nucleus scattering CEvNS of reactor antineutrinos. A comprehensive calibration was performed in order to obtain important parameters of the detector during its exposition at the Kalinin Nuclear Power Plant (Tver, Russia). This paper describes the analysis of calibration data, position and energy reconstruction procedures, and evaluation of the efficiency of electron extraction from the liquid xenon to the gas phase.","sentences":["RED-100 is a two-phase Xe detector designed and built for the study of coherent elastic neutrino-nucleus scattering CEvNS of reactor antineutrinos.","A comprehensive calibration was performed in order to obtain important parameters of the detector during its exposition at the Kalinin Nuclear Power Plant (Tver, Russia).","This paper describes the analysis of calibration data, position and energy reconstruction procedures, and evaluation of the efficiency of electron extraction from the liquid xenon to the gas phase."],"url":"http://arxiv.org/abs/2403.12645v1","category":"physics.ins-det"}
{"created":"2024-03-19 11:27:26","title":"PH-NODE: A DFPT and finite displacement supercell based python code for searching nodes in topological phononic materials","abstract":"Exploring the topological physics of phonons is fundamentally important for understanding various practical applications. Here, we present a density-functional perturbation theory and finite displacement supercell based Python 3 software package called PH-NODE for efficiently computing phonon nodes present in real material through a first-principle approach. The present version of the code is interfaced with the WIEN2k, Elk, and ABINIT packages. In order to benchmark the code, four different types of materials are considered, which include (i) FeSi, a well-known double-Weyl point; (ii) LiCaAs, a half-Heusler single-type-I Weyl topological phonon (TP); and (iii) ScZn, coexisting nodal-line and nodal-ring TPs; (iv) TiS, six pairs of bulk Weyl nodes. In FeSi, the node points are found at ${\\Gamma}$(0, 0, 0) and R(0.5, 0.5, 0.5) high symmetric points. Also, there are 21 energy values at which the node points are situated, corresponding to the full Brillouin Zone. For LiCaAs, the previously reported literature claims that there is a node point along the W-X high symmetry direction between the highest longitudinal acoustic and the lowest transverse optical branch, while in our DFT calculations, a gap of 0.17 meV is found. Furthermore, ScZn hosts six nodal-ring TPs phonons at the boundary planes of the Brillouin Zone in the vicinity of the M high-symmetric point. In addition to this, straight-line TPs are also found along the ${\\Gamma}$-X and ${\\Gamma}$-R high symmetric directions. Moreover, for TiS, six weyl node points (WP1, WP2, WP3, WP4, WP5 and WP6) are found along H-K high-symmetric direction. The results obtained from the PH-NODE code are in good agreement with the experimentally and theoretically reported data for each material.","sentences":["Exploring the topological physics of phonons is fundamentally important for understanding various practical applications.","Here, we present a density-functional perturbation theory and finite displacement supercell based Python 3 software package called PH-NODE for efficiently computing phonon nodes present in real material through a first-principle approach.","The present version of the code is interfaced with the WIEN2k, Elk, and ABINIT packages.","In order to benchmark the code, four different types of materials are considered, which include (i) FeSi, a well-known double-Weyl point; (ii) LiCaAs, a half-Heusler single-type-I Weyl topological phonon (TP); and (iii) ScZn, coexisting nodal-line and nodal-ring TPs; (iv) TiS, six pairs of bulk Weyl nodes.","In FeSi, the node points are found at ${\\Gamma}$(0, 0, 0) and R(0.5, 0.5, 0.5) high symmetric points.","Also, there are 21 energy values at which the node points are situated, corresponding to the full Brillouin Zone.","For LiCaAs, the previously reported literature claims that there is a node point along the W-X high symmetry direction between the highest longitudinal acoustic and the lowest transverse optical branch, while in our DFT calculations, a gap of 0.17 meV is found.","Furthermore, ScZn hosts six nodal-ring TPs phonons at the boundary planes of the Brillouin Zone in the vicinity of the M high-symmetric point.","In addition to this, straight-line TPs are also found along the ${\\Gamma}$-X and ${\\Gamma}$-R high symmetric directions.","Moreover, for TiS, six weyl node points (WP1, WP2, WP3, WP4, WP5 and WP6) are found along H-K high-symmetric direction.","The results obtained from the PH-NODE code are in good agreement with the experimentally and theoretically reported data for each material."],"url":"http://arxiv.org/abs/2403.12643v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 11:20:16","title":"Imaging and spectroscopic observations of a confined solar filament eruption with two-stage evolution","abstract":"Solar filament eruptions are often characterized by stepwise evolution due to the involvement of multiple mechanisms, such as magnetohydrodynamic instabilities and magnetic reconnection. In this article, we investigated a confined filament eruption with a distinct two-stage evolution by using the imaging and spectroscopic observations from the Interface Region Imaging Spectrograph (IRIS) and the Solar Dynamics Observatory (SDO). The eruption originated from a kinked filament thread that separated from an active region filament. In the first stage, the filament thread rose slowly and was obstructed due to flux pile-up in its front. This obstruction brought the filament thread into reconnection with a nearby loop-like structure, which enlarged the flux rope and changed its connectivity through the foot-point migration. The newly formed flux rope became more kink unstable and drove the rapid eruption in the second stage. It ascended into the upper atmosphere and initiated the reconnection with the overlying field. Finally, the flux rope was totally disintegrated, producing several solar jets along the overlying field. These observations demonstrate that the external reconnection between the flux rope and overlying field can destroy the flux rope, thus playing a crucial role in confining the solar eruptions.","sentences":["Solar filament eruptions are often characterized by stepwise evolution due to the involvement of multiple mechanisms, such as magnetohydrodynamic instabilities and magnetic reconnection.","In this article, we investigated a confined filament eruption with a distinct two-stage evolution by using the imaging and spectroscopic observations from the Interface Region Imaging Spectrograph (IRIS) and the Solar Dynamics Observatory (SDO).","The eruption originated from a kinked filament thread that separated from an active region filament.","In the first stage, the filament thread rose slowly and was obstructed due to flux pile-up in its front.","This obstruction brought the filament thread into reconnection with a nearby loop-like structure, which enlarged the flux rope and changed its connectivity through the foot-point migration.","The newly formed flux rope became more kink unstable and drove the rapid eruption in the second stage.","It ascended into the upper atmosphere and initiated the reconnection with the overlying field.","Finally, the flux rope was totally disintegrated, producing several solar jets along the overlying field.","These observations demonstrate that the external reconnection between the flux rope and overlying field can destroy the flux rope, thus playing a crucial role in confining the solar eruptions."],"url":"http://arxiv.org/abs/2403.12639v1","category":"astro-ph.SR"}
{"created":"2024-03-19 10:35:20","title":"Do Temperate Rocky Planets Around M Dwarfs have an Atmosphere ?","abstract":"Detecting an atmosphere on nearby temperate planets is one of the most important scientific objectives of the Webb mission, an endeavour in practice limited to a handful of well-characterized planets: Trappist-1d, e, f, g, LHS1140b, and the mini-Neptune K2- 18b. The first 18 months of atmospheric characterization with JWST have confirmed both its power and versatility to probe exoplanet atmospheres, and have highlighted the challenge of stellar activity in studying those atmospheres through transmission spectroscopy. Assessing the prevalence of atmosphere in temperate planets with a minimal degree of confidence will require a multi-cycle program of order of a few 1000 hours involving both eclipse photometry and transmission spectroscopy, a program that would have to be executed over a significant fraction of JWST's lifetime. The forthcoming 500 hours of Cycle 3 Director Discretionary Time dedicated to exoplanet programs represent a unique opportunity to initiate a deep reconnaissance of habitability of the best keystone temperate planets.","sentences":["Detecting an atmosphere on nearby temperate planets is one of the most important scientific objectives of the Webb mission, an endeavour in practice limited to a handful of well-characterized planets: Trappist-1d, e, f, g, LHS1140b, and the mini-Neptune K2- 18b.","The first 18 months of atmospheric characterization with JWST have confirmed both its power and versatility to probe exoplanet atmospheres, and have highlighted the challenge of stellar activity in studying those atmospheres through transmission spectroscopy.","Assessing the prevalence of atmosphere in temperate planets with a minimal degree of confidence will require a multi-cycle program of order of a few 1000 hours involving both eclipse photometry and transmission spectroscopy, a program that would have to be executed over a significant fraction of JWST's lifetime.","The forthcoming 500 hours of Cycle 3 Director Discretionary Time dedicated to exoplanet programs represent a unique opportunity to initiate a deep reconnaissance of habitability of the best keystone temperate planets."],"url":"http://arxiv.org/abs/2403.12617v1","category":"astro-ph.EP"}
{"created":"2024-03-19 10:34:21","title":"Polarization Dynamics in Paramagnet of Charged Quark-Gluon Plasma","abstract":"It is commonly understood that the strong magnetic field produced in heavy ion collisions is short-lived. The electric conductivity of the quark-gluon plasma is unable to significantly extend the life time of magnetic field. We propose an alternative scenario to achieve this: with finite baryon density and spin polarization by the initial magnetic field, the quark-gluon plasma behaves as a paramagnet, which may continue to polarize quark after fading of initial magnetic field. We confirm this picture by calculations in both quantum electrodynamics and quantum chromodynamics. In the former case, we find a splitting in the damping rates of probe fermion with opposite spin component along the magnetic field with the splitting parametrically small than the average damping rate. In the latter case, we find a similar splitting in the damping rates of probe quark with opposite spin components along the magnetic field. The splitting is parametrically comparable to the average damping rate, providing an efficient way of polarizing strange quarks by the quark-gluon plasma paramagnet consisting of light quarks.","sentences":["It is commonly understood that the strong magnetic field produced in heavy ion collisions is short-lived.","The electric conductivity of the quark-gluon plasma is unable to significantly extend the life time of magnetic field.","We propose an alternative scenario to achieve this: with finite baryon density and spin polarization by the initial magnetic field, the quark-gluon plasma behaves as a paramagnet, which may continue to polarize quark after fading of initial magnetic field.","We confirm this picture by calculations in both quantum electrodynamics and quantum chromodynamics.","In the former case, we find a splitting in the damping rates of probe fermion with opposite spin component along the magnetic field with the splitting parametrically small than the average damping rate.","In the latter case, we find a similar splitting in the damping rates of probe quark with opposite spin components along the magnetic field.","The splitting is parametrically comparable to the average damping rate, providing an efficient way of polarizing strange quarks by the quark-gluon plasma paramagnet consisting of light quarks."],"url":"http://arxiv.org/abs/2403.12615v1","category":"hep-ph"}
{"created":"2024-03-19 10:25:53","title":"Large Rashba spin-orbit coupling in metallic SrTaO$_3$ thin films","abstract":"Epitaxial thin films of SrTaO$_3$ with thickness ($t$) smaller than 74 nm were successfully fabricated on an insulator (LaAlO$_3$)$_{0.3}$(Sr$_2$AlTaO$_6$)$_{0.7}$ substrate. Films with $t$ above 8.6 nm showed metallic conduction. Both conductivity and a mobility showed a decrease with increasing $t$ above 42 nm, suggesting the instability of thick SrTaO$_3$ films. This instability was also supported by TEM image and XRD intensity. For the metallic films with $t$ below 25 nm, energy band splitting due to spin-orbit coupling ($\\Delta$$_{so}$) and Rashba parameter ($\\alpha$$_R$) were deduced from an analysis of a magnetoresistance using two-dimensional weak antilocalization theory. The values of $\\Delta$$_{so}$ ranged from 26 to 120 meV, which were the largest among other metallic oxide films, such as SrNbO$_3$, SrIrO$_3$, and La$_{2/3}$Sr$_{1/3}$MnO$_3$ thin films, indicating that spin-orbit coupling in SrTaO$_3$ was the largest among the metallic perovskite oxides reported so far. The values of $\\alpha$$_R$ for our SrTaO$_3$ films ranged from $8.8 \\times$10$^{-13}$ to $1.7 \\times$10$^{-12}$ eV m, which were much larger than those reported for other metallic oxide thin films.","sentences":["Epitaxial thin films of SrTaO$_3$ with thickness ($t$) smaller than 74 nm were successfully fabricated on an insulator (LaAlO$_3$)$_{0.3}$(Sr$_2$AlTaO$_6$)$_{0.7}$ substrate.","Films with $t$ above 8.6 nm showed metallic conduction.","Both conductivity and a mobility showed a decrease with increasing $t$ above 42 nm, suggesting the instability of thick SrTaO$_3$ films.","This instability was also supported by TEM image and XRD intensity.","For the metallic films with $t$ below 25 nm, energy band splitting due to spin-orbit coupling ($\\Delta$$_{so}$) and Rashba parameter ($\\alpha$$_R$) were deduced from an analysis of a magnetoresistance using two-dimensional weak antilocalization theory.","The values of $\\Delta$$_{so}$ ranged from 26 to 120 meV, which were the largest among other metallic oxide films, such as SrNbO$_3$, SrIrO$_3$, and La$_{2/3}$Sr$_{1/3}$MnO$_3$ thin films, indicating that spin-orbit coupling in SrTaO$_3$ was the largest among the metallic perovskite oxides reported so far.","The values of $\\alpha$$_R$ for our SrTaO$_3$ films ranged from $8.8 \\times$10$^{-13}$ to $1.7 \\times$10$^{-12}$ eV m, which were much larger than those reported for other metallic oxide thin films."],"url":"http://arxiv.org/abs/2403.12612v1","category":"cond-mat.str-el"}
{"created":"2024-03-19 08:29:45","title":"A program for 3D nuclear static and time-dependent density-functional theory with full Skyrme energy density functional: HIT3D","abstract":"This work presents a computer program that performs symmetry-unrestricted 3D nuclear time-dependent density function theory (DFT) calculations. The program features the augmented Lagrangian constraint in the static calculation. This allows for the calculation of the potential energy surface. In addition, the code includes the full energy density functionals derived from the Skyrme interaction, meaning that the time-even and time-odd tensor parts are included for the time-dependent calculations. The results of the hit3d code are carefully compared with the Sky3D and Ev8 programs. The testing cases include unconstrained DFT calculations for doubly magic nuclei, the constrained DFT + BCS calculations for medium-heavy nucleus 110Mo, and the dynamic applications for harmonic vibration and nuclear reactions.","sentences":["This work presents a computer program that performs symmetry-unrestricted 3D nuclear time-dependent density function theory (DFT) calculations.","The program features the augmented Lagrangian constraint in the static calculation.","This allows for the calculation of the potential energy surface.","In addition, the code includes the full energy density functionals derived from the Skyrme interaction, meaning that the time-even and time-odd tensor parts are included for the time-dependent calculations.","The results of the hit3d code are carefully compared with the Sky3D and Ev8 programs.","The testing cases include unconstrained DFT calculations for doubly magic nuclei, the constrained DFT + BCS calculations for medium-heavy nucleus 110Mo, and the dynamic applications for harmonic vibration and nuclear reactions."],"url":"http://arxiv.org/abs/2403.12539v1","category":"nucl-th"}
{"created":"2024-03-19 07:52:32","title":"Search for GeV gamma-ray emission from the possible TeV-bright red dwarfs with Fermi-LAT","abstract":"Red dwarfs have been suggested to be among the possible astrophysical species accelerating particles and emitting TeV $\\gamma$-rays. As an effort to search for the GeV $\\gamma$-ray counterparts of the suggested TeV emission from eight red dwarfs, we analyse the 0.2--500 GeV $\\gamma$-ray emission of the regions covering them exploiting the $\\sim$13.6 yr Pass 8 data of the Fermi Large Area Telescope. A GeV $\\gamma$-ray emission excess with significance of 3.8$\\sigma$ is detected in the direction of the red dwarf V962 Tau. This emission contains V962 Tau in 1$\\sigma$ error radius and is independent of the catalog source. However, the stellar flare scenario can hardly explain the total energy and lightcurve derived from the $\\gamma$-ray emission in view of the spectral analysis. We also analyse the lightcurves in the positions of the eight red dwarfs and no time bin with significance $>$5$\\sigma$ is found. Therefore, no significant emission from the red dwarfs could be concluded to be detected by Fermi-LAT.","sentences":["Red dwarfs have been suggested to be among the possible astrophysical species accelerating particles and emitting TeV $\\gamma$-rays.","As an effort to search for the GeV $\\gamma$-ray counterparts of the suggested TeV emission from eight red dwarfs, we analyse the 0.2--500 GeV $\\gamma$-ray emission of the regions covering them exploiting the $\\sim$13.6 yr Pass 8 data of the Fermi Large Area Telescope.","A GeV $\\gamma$-ray emission excess with significance of 3.8$\\sigma$ is detected in the direction of the red dwarf V962 Tau.","This emission contains V962 Tau in 1$\\sigma$ error radius and is independent of the catalog source.","However, the stellar flare scenario can hardly explain the total energy and lightcurve derived from the $\\gamma$-ray emission in view of the spectral analysis.","We also analyse the lightcurves in the positions of the eight red dwarfs and no time bin with significance $>$5$\\sigma$ is found.","Therefore, no significant emission from the red dwarfs could be concluded to be detected by Fermi-LAT."],"url":"http://arxiv.org/abs/2403.12524v1","category":"astro-ph.HE"}
{"created":"2024-03-19 07:43:59","title":"First Measurement of the $\u03bd_e$ and $\u03bd_\u03bc$ Interaction Cross Sections at the LHC with FASER's Emulsion Detector","abstract":"This paper presents the first results of the study of high-energy electron and muon neutrino charged-current interactions in the FASER$\\nu$ emulsion/tungsten detector of the FASER experiment at the LHC. A subset of the FASER$\\nu$ volume, which corresponds to a target mass of 128.6~kg, was exposed to neutrinos from the LHC $pp$ collisions with a centre-of-mass energy of 13.6~TeV and an integrated luminosity of 9.5 fb$^{-1}$. Applying stringent selections requiring electrons with reconstructed energy above 200~GeV, four electron neutrino interaction candidate events are observed with an expected background of $0.025^{+0.015}_{-0.010}$, leading to a statistical significance of 5.2$\\sigma$. This is the first direct observation of electron neutrino interactions at a particle collider. Eight muon neutrino interaction candidate events are also detected, with an expected background of $0.22^{+0.09}_{-0.07}$, leading to a statistical significance of 5.7$\\sigma$. The signal events include neutrinos with energies in the TeV range, the highest-energy electron and muon neutrinos ever detected from an artificial source. The energy-independent part of the interaction cross section per nucleon is measured over an energy range of 560--1740 GeV (520--1760 GeV) for $\\nu_e$ ($\\nu_{\\mu}$) to be $(1.2_{-0.7}^{+0.8}) \\times 10^{-38}~\\mathrm{cm}^{2}\\,\\mathrm{GeV}^{-1}$ ($(0.5\\pm0.2) \\times 10^{-38}~\\mathrm{cm}^{2}\\,\\mathrm{GeV}^{-1}$), consistent with Standard Model predictions. These are the first measurements of neutrino interaction cross sections in those energy ranges.","sentences":["This paper presents the first results of the study of high-energy electron and muon neutrino charged-current interactions in the FASER$\\nu$ emulsion/tungsten detector of the FASER experiment at the LHC.","A subset of the FASER$\\nu$ volume, which corresponds to a target mass of 128.6~kg, was exposed to neutrinos from the LHC $pp$ collisions with a centre-of-mass energy of 13.6~TeV and an integrated luminosity of 9.5 fb$^{-1}$. Applying stringent selections requiring electrons with reconstructed energy above 200~GeV, four electron neutrino interaction candidate events are observed with an expected background of $0.025^{+0.015}_{-0.010}$, leading to a statistical significance of 5.2$\\sigma$.","This is the first direct observation of electron neutrino interactions at a particle collider.","Eight muon neutrino interaction candidate events are also detected, with an expected background of $0.22^{+0.09}_{-0.07}$, leading to a statistical significance of 5.7$\\sigma$.","The signal events include neutrinos with energies in the TeV range, the highest-energy electron and muon neutrinos ever detected from an artificial source.","The energy-independent part of the interaction cross section per nucleon is measured over an energy range of 560--1740 GeV (520--1760 GeV) for $\\nu_e$ ($\\nu_{\\mu}$) to be $(1.2_{-0.7}^{+0.8})","\\times 10^{-38}~\\mathrm{cm}^{2}\\,\\mathrm{GeV}^{-1}$ ($(0.5\\pm0.2)","\\times 10^{-38}~\\mathrm{cm}^{2}\\,\\mathrm{GeV}^{-1}$), consistent with Standard Model predictions.","These are the first measurements of neutrino interaction cross sections in those energy ranges."],"url":"http://arxiv.org/abs/2403.12520v1","category":"hep-ex"}
{"created":"2024-03-19 07:13:47","title":"Nanoscale brittle-to-ductile transition of the C15 CaAl$_2$ Laves phase","abstract":"The influence of temperature on the deformation behaviour of the C15 CaAl$_2$ Laves phase, a key constituent for enhancing the mechanical properties of Mg alloys up to service temperatures of 200 {\\deg}C, remains largely unexplored. This study presents, for the first time, the nanoscale brittle-to-ductile transition (BDT) of this intermetallic phase through in situ testing including nanoindentation, scratch testing, and micropillar splitting conducted at elevated temperatures. By correlating observations from these techniques, changes in deformation of CaAl$_2$ were identified in relation to temperature. High-temperature nanoindentation quantitatively determined the temperature range for the BDT, and revealed that CaAl$_2$ undergoes a BDT at ~0.55T$_m$, exhibiting an intermediate region of microplasticity. A noticeable decrease in nanoindentation hardness was observed at ~450-500 {\\deg}C, accompanied by an increase in residual indent size, while indentation cracking was not observed above 300 {\\deg}C. Results from high-temperature micropillar splitting revealed cracking and brittle pillar splitting up to 300 {\\deg}C, with an increase in apparent fracture toughness from 0.9 $\\pm$ 0.1 MPa$\\cdot\\sqrt m$ to 2.8 $\\pm$ 0.3 MPa$\\cdot\\sqrt m$, and subsequent crack-free plastic deformation from 400 {\\deg}C. Transmission electron microscopy analysis of the deformed material from nanoindentation revealed that the BDT of CaAl$_2$ may be attributed to enhanced dislocation plasticity with increasing temperature.","sentences":["The influence of temperature on the deformation behaviour of the C15 CaAl$_2$ Laves phase, a key constituent for enhancing the mechanical properties of Mg alloys up to service temperatures of 200 {\\deg}C, remains largely unexplored.","This study presents, for the first time, the nanoscale brittle-to-ductile transition (BDT) of this intermetallic phase through in situ testing including nanoindentation, scratch testing, and micropillar splitting conducted at elevated temperatures.","By correlating observations from these techniques, changes in deformation of CaAl$_2$ were identified in relation to temperature.","High-temperature nanoindentation quantitatively determined the temperature range for the BDT, and revealed that CaAl$_2$ undergoes a BDT at ~0.55T$_m$, exhibiting an intermediate region of microplasticity.","A noticeable decrease in nanoindentation hardness was observed at ~450-500 {\\deg}C, accompanied by an increase in residual indent size, while indentation cracking was not observed above 300 {\\deg}C. Results from high-temperature micropillar splitting revealed cracking and brittle pillar splitting up to 300 {\\deg}C, with an increase in apparent fracture toughness from 0.9 $\\pm$ 0.1 MPa$\\cdot\\sqrt m$ to 2.8 $\\pm$ 0.3 MPa$\\cdot\\sqrt m$, and subsequent crack-free plastic deformation from 400 {\\deg}C. Transmission electron microscopy analysis of the deformed material from nanoindentation revealed that the BDT of CaAl$_2$ may be attributed to enhanced dislocation plasticity with increasing temperature."],"url":"http://arxiv.org/abs/2403.12507v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 04:54:20","title":"Observational signatures of Rotating compact objects in Plasma space-time","abstract":"We have investigated the characteristics of shadows cast by the Kerr black hole in the presence of plasma and compared them to those of a rotating wormhole in a uniform plasma space-time for an observer at infinity. Interestingly, for the same uniform plasma density, the apparent shadow size of the rotating wormhole is always greater than that of the Kerr black hole. To further distinguish the two compact objects we studied the deflection angle and did a comparative study in the presence of the uniform and non-uniform plasma profiles. The goal of this whole exercise is to deepen our understanding of the observational phenomena of these astrophysical objects. The analysis reveals the importance of specific plasma distribution profiles, the impact of plasma on the shadow diameter, and the behavior of deflection angles in different plasma scenarios. We have calculated constraints on the plasma parameters by considering observational data and employing analytical formulations, we have calculated constraints on the plasma parameters. Our work therefore provides valuable insights into the behavior of light rays near compact objects in plasma space-time.","sentences":["We have investigated the characteristics of shadows cast by the Kerr black hole in the presence of plasma and compared them to those of a rotating wormhole in a uniform plasma space-time for an observer at infinity.","Interestingly, for the same uniform plasma density, the apparent shadow size of the rotating wormhole is always greater than that of the Kerr black hole.","To further distinguish the two compact objects we studied the deflection angle and did a comparative study in the presence of the uniform and non-uniform plasma profiles.","The goal of this whole exercise is to deepen our understanding of the observational phenomena of these astrophysical objects.","The analysis reveals the importance of specific plasma distribution profiles, the impact of plasma on the shadow diameter, and the behavior of deflection angles in different plasma scenarios.","We have calculated constraints on the plasma parameters by considering observational data and employing analytical formulations, we have calculated constraints on the plasma parameters.","Our work therefore provides valuable insights into the behavior of light rays near compact objects in plasma space-time."],"url":"http://arxiv.org/abs/2403.12439v1","category":"gr-qc"}
{"created":"2024-03-19 04:44:09","title":"Prototipo de video juego activo basado en una c\u00e1mara 3D para motivar la actividad f\u00edsica en ni\u00f1os y adultos mayores","abstract":"This document describes the development of a video game prototype designed to encourage physical activity among children and older adults. The prototype consists of a laptop, a camera with 3D sensors, and optionally requires an LCD screen or a projector. The programming component of this prototype was developed in Scratch, a programming language geared towards children, which greatly facilitates the creation of a game tailored to the users' preferences. The idea to create such a prototype originated from the desire to offer an option that promotes physical activity among children and adults, given that a lack of physical exercise is a predominant factor in the development of chronic degenerative diseases such as diabetes and hypertension, to name the most common. As a result of this initiative, an active video game prototype was successfully developed, based on a ping-pong game, which allows both children and adults to interact in a fun way while encouraging the performance of physical activities that can positively impact the users' health.","sentences":["This document describes the development of a video game prototype designed to encourage physical activity among children and older adults.","The prototype consists of a laptop, a camera with 3D sensors, and optionally requires an LCD screen or a projector.","The programming component of this prototype was developed in Scratch, a programming language geared towards children, which greatly facilitates the creation of a game tailored to the users' preferences.","The idea to create such a prototype originated from the desire to offer an option that promotes physical activity among children and adults, given that a lack of physical exercise is a predominant factor in the development of chronic degenerative diseases such as diabetes and hypertension, to name the most common.","As a result of this initiative, an active video game prototype was successfully developed, based on a ping-pong game, which allows both children and adults to interact in a fun way while encouraging the performance of physical activities that can positively impact the users' health."],"url":"http://arxiv.org/abs/2403.12432v1","category":"cs.HC"}
{"created":"2024-03-19 04:20:16","title":"A Quantum trace map for 3-manifolds","abstract":"We define a quantum trace map from the skein module of a 3-manifold with torus boundary components to a module (left and right quotient of a quantum torus) constructed from an ideal triangulation. Our map is a 3-dimensional version of the well-known quantum trace map on surfaces introduced by Bonahon and Wong and further developed by L\\^e.","sentences":["We define a quantum trace map from the skein module of a 3-manifold with torus boundary components to a module (left and right quotient of a quantum torus) constructed from an ideal triangulation.","Our map is a 3-dimensional version of the well-known quantum trace map on surfaces introduced by Bonahon and Wong and further developed by L\\^e."],"url":"http://arxiv.org/abs/2403.12424v1","category":"math.GT"}
{"created":"2024-03-19 03:09:24","title":"OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation","abstract":"This paper studies a new open-set problem, the open-vocabulary category-level object pose and size estimation. Given human text descriptions of arbitrary novel object categories, the robot agent seeks to predict the position, orientation, and size of the target object in the observed scene image. To enable such generalizability, we first introduce OO3D-9D, a large-scale photorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the largest and most diverse dataset in the field of category-level object pose and size estimation. It includes additional annotations for the symmetry axis of each category, which help resolve symmetric ambiguity. Apart from the large-scale dataset, we find another key to enabling such generalizability is leveraging the strong prior knowledge in pre-trained visual-language foundation models. We then propose a framework built on pre-trained DinoV2 and text-to-image stable diffusion models to infer the normalized object coordinate space (NOCS) maps of the target instances. This framework fully leverages the visual semantic prior from DinoV2 and the aligned visual and language knowledge within the text-to-image diffusion model, which enables generalization to various text descriptions of novel categories. Comprehensive quantitative and qualitative experiments demonstrate that the proposed open-vocabulary method, trained on our large-scale synthesized data, significantly outperforms the baseline and can effectively generalize to real-world images of unseen categories. The project page is at https://ov9d.github.io.","sentences":["This paper studies a new open-set problem, the open-vocabulary category-level object pose and size estimation.","Given human text descriptions of arbitrary novel object categories, the robot agent seeks to predict the position, orientation, and size of the target object in the observed scene image.","To enable such generalizability, we first introduce OO3D-9D, a large-scale photorealistic dataset for this task.","Derived from OmniObject3D, OO3D-9D is the largest and most diverse dataset in the field of category-level object pose and size estimation.","It includes additional annotations for the symmetry axis of each category, which help resolve symmetric ambiguity.","Apart from the large-scale dataset, we find another key to enabling such generalizability is leveraging the strong prior knowledge in pre-trained visual-language foundation models.","We then propose a framework built on pre-trained DinoV2 and text-to-image stable diffusion models to infer the normalized object coordinate space (NOCS) maps of the target instances.","This framework fully leverages the visual semantic prior from DinoV2 and the aligned visual and language knowledge within the text-to-image diffusion model, which enables generalization to various text descriptions of novel categories.","Comprehensive quantitative and qualitative experiments demonstrate that the proposed open-vocabulary method, trained on our large-scale synthesized data, significantly outperforms the baseline and can effectively generalize to real-world images of unseen categories.","The project page is at https://ov9d.github.io."],"url":"http://arxiv.org/abs/2403.12396v1","category":"cs.CV"}
{"created":"2024-03-19 02:46:42","title":"Slowly decaying ringdown of a rapidly spinning black hole II: Inferring the masses and spins of supermassive black holes with LISA","abstract":"Electromagnetic observations reveal that almost all galaxies have supermassive black holes (SMBHs) at their centers, but their properties, especially their spins, are not fully understood. Some of the authors have recently shown [Oshita and Tsuna (2023)] that rapid spins of $>0.9$, inferred for masses around $10^7\\ M_\\odot$ from observations of local SMBHs and cosmological simulations, source {\\it long-lived} ringdowns that enhance the precision of black hole spectroscopy to test gravity in the near-extreme Kerr spacetime. In this work, we estimate the statistical errors in the SMBH mass-spin inference in anticipation of the LISA's detection of extreme mass-ratio mergers. We show that for rapidly spinning SMBHs, more precise mass and spin measurements are expected due to the excitations of higher angular modes. For a near-extremal SMBH of mass $10^7M_\\odot$ merging with a smaller BH with mass ratio $10^{-3}$ at a luminosity distance of $\\lesssim 10\\:\\mathrm{Gpc}$ (redshift $z \\lesssim 1.37$), the measurement errors in the mass and spin of the SMBH would be $\\sim 1\\:\\mathrm{\\%}$ and $\\sim 10^{-1}\\:\\mathrm{\\%}$ respectively.","sentences":["Electromagnetic observations reveal that almost all galaxies have supermassive black holes (SMBHs) at their centers, but their properties, especially their spins, are not fully understood.","Some of the authors have recently shown [Oshita and Tsuna (2023)] that rapid spins of $>0.9$, inferred for masses around $10^7\\ M_\\odot$ from observations of local SMBHs and cosmological simulations, source {\\it long-lived} ringdowns that enhance the precision of black hole spectroscopy to test gravity in the near-extreme Kerr spacetime.","In this work, we estimate the statistical errors in the SMBH mass-spin inference in anticipation of the LISA's detection of extreme mass-ratio mergers.","We show that for rapidly spinning SMBHs, more precise mass and spin measurements are expected due to the excitations of higher angular modes.","For a near-extremal SMBH of mass $10^7M_\\odot$ merging with a smaller BH with mass ratio $10^{-3}$ at a luminosity distance of $\\lesssim 10\\:\\mathrm{Gpc}$ (redshift $z \\lesssim 1.37$), the measurement errors in the mass and spin of the SMBH would be $\\sim 1\\:\\mathrm{\\%}$ and $\\sim 10^{-1}\\:\\mathrm{\\%}$ respectively."],"url":"http://arxiv.org/abs/2403.12380v1","category":"gr-qc"}
{"created":"2024-03-19 02:10:57","title":"Multi-State, Ultra-thin, BEOL-Compatible AlScN Ferroelectric Diodes","abstract":"The growth in data generation necessitates efficient data processing technologies to address the von Neumann bottleneck in conventional computer architecture. Memory-driven computing, which integrates non-volatile memory (NVM) devices in a 3D stack, is gaining attention, with CMOS back-end-of-line (BEOL) compatible ferroelectric (FE) diodes being ideal due to their two-terminal design and inherently selector-free nature, facilitating high-density crossbar arrays. Here, we demonstrate BEOL-compatible, high-performance FE-diodes scaled to 5, 10, and 20 nm FE Al0.72Sc0.28N/Al0.64Sc0.36N films. Through interlayer (IL) engineering, we show substantial improvements in the ON/OFF ratios (>166 times) and rectification ratios (>176 times) in these scaled devices. The superlative characteristics also enables 5-bit multi-state operation with a stable retention. We also experimentally and theoretically demonstrate the counterintuitive result that the inclusion of an IL can lead to a decrease in the ferroelectric switching voltage of the device. An in-depth analysis into the device transport mechanisms is performed, and our compact model aligns seamlessly with the experimental results. Our results suggest the possibility of using scaled AlxSc1-xN FE-diodes for high performance, low-power, embedded NVM.","sentences":["The growth in data generation necessitates efficient data processing technologies to address the von Neumann bottleneck in conventional computer architecture.","Memory-driven computing, which integrates non-volatile memory (NVM) devices in a 3D stack, is gaining attention, with CMOS back-end-of-line (BEOL) compatible ferroelectric (FE) diodes being ideal due to their two-terminal design and inherently selector-free nature, facilitating high-density crossbar arrays.","Here, we demonstrate BEOL-compatible, high-performance FE-diodes scaled to 5, 10, and 20 nm FE Al0.72Sc0.28N/Al0.64Sc0.36N films.","Through interlayer (IL) engineering, we show substantial improvements in the ON/OFF ratios (>166 times) and rectification ratios (>176 times) in these scaled devices.","The superlative characteristics also enables 5-bit multi-state operation with a stable retention.","We also experimentally and theoretically demonstrate the counterintuitive result that the inclusion of an IL can lead to a decrease in the ferroelectric switching voltage of the device.","An in-depth analysis into the device transport mechanisms is performed, and our compact model aligns seamlessly with the experimental results.","Our results suggest the possibility of using scaled AlxSc1-xN FE-diodes for high performance, low-power, embedded NVM."],"url":"http://arxiv.org/abs/2403.12361v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-19 02:08:17","title":"Hydrodynamics, anomaly inflow and bosonic effective field theory","abstract":"Euler hydrodynamics of perfect fluids can be viewed as an effective bosonic field theory. In cases when the underlying microscopic system involves Dirac fermions, the quantum anomalies should be properly described. In 1+1 dimensions the action formulation of hydrodynamics at zero temperature is reconsidered and shown to be equal to standard field-theory bosonization. Furthermore, it can be derived from a topological gauge theory in one extra dimension, which identifies the fluid variables through the anomaly inflow relations. Extending this framework to 3+1 dimensions yields an effective field theory/hydrodynamics model, capable of elucidating the mixed axial-vector and axial-gravitational anomalies of Dirac fermions. This formulation provides a platform for bosonization in higher dimensions. Moreover, the connection with 4+1 dimensional topological theories suggests some generalizations of fluid dynamics involving additional degrees of freedom.","sentences":["Euler hydrodynamics of perfect fluids can be viewed as an effective bosonic field theory.","In cases when the underlying microscopic system involves Dirac fermions, the quantum anomalies should be properly described.","In 1+1 dimensions the action formulation of hydrodynamics at zero temperature is reconsidered and shown to be equal to standard field-theory bosonization.","Furthermore, it can be derived from a topological gauge theory in one extra dimension, which identifies the fluid variables through the anomaly inflow relations.","Extending this framework to 3+1 dimensions yields an effective field theory/hydrodynamics model, capable of elucidating the mixed axial-vector and axial-gravitational anomalies of Dirac fermions.","This formulation provides a platform for bosonization in higher dimensions.","Moreover, the connection with 4+1 dimensional topological theories suggests some generalizations of fluid dynamics involving additional degrees of freedom."],"url":"http://arxiv.org/abs/2403.12360v1","category":"hep-th"}
{"created":"2024-03-19 02:02:01","title":"Mass Flows in Expanding Coronal Loops","abstract":"An expansion of cross-sectional area directly impacts the mass flow along a coronal loop, and significantly alters the radiative and hydrodynamic evolution of that loop as a result. Previous studies have found that an area expansion from chromosphere to corona significantly lengthens the cooling time of the corona, and appears to suppress draining from the corona. In this work, we examine the fluid dynamics to understand how the mass flow rate, the energy balance, and the cooling and draining timescales are affected by a non-uniform area. We find that in loops with moderate or large expansion (cross-sectional area expansion factors of 2, 3, 10, 30, 100 from photosphere to apex), impulsive heating, for either direct thermal heating or electron beam heating, induces a steady flow into the corona, so that the coronal density continues to rise during the cooling phase, whereas a uniform loop drains during the cooling phase. The induced upflow carries energy into the corona, balancing the losses from thermal conduction, and continues until thermal conduction weakens enough so that it can no longer support the radiative losses of the transition region (TR). As a result, the plasma cools primarily radiatively until the onset of catastrophic collapse. The speed and duration of the induced upflow both increase in proportion to the rate of area expansion. We argue that observations of blue-shifted spectral lines, therefore, could place a constraint on a loop's area expansion.","sentences":["An expansion of cross-sectional area directly impacts the mass flow along a coronal loop, and significantly alters the radiative and hydrodynamic evolution of that loop as a result.","Previous studies have found that an area expansion from chromosphere to corona significantly lengthens the cooling time of the corona, and appears to suppress draining from the corona.","In this work, we examine the fluid dynamics to understand how the mass flow rate, the energy balance, and the cooling and draining timescales are affected by a non-uniform area.","We find that in loops with moderate or large expansion (cross-sectional area expansion factors of 2, 3, 10, 30, 100 from photosphere to apex), impulsive heating, for either direct thermal heating or electron beam heating, induces a steady flow into the corona, so that the coronal density continues to rise during the cooling phase, whereas a uniform loop drains during the cooling phase.","The induced upflow carries energy into the corona, balancing the losses from thermal conduction, and continues until thermal conduction weakens enough so that it can no longer support the radiative losses of the transition region (TR).","As a result, the plasma cools primarily radiatively until the onset of catastrophic collapse.","The speed and duration of the induced upflow both increase in proportion to the rate of area expansion.","We argue that observations of blue-shifted spectral lines, therefore, could place a constraint on a loop's area expansion."],"url":"http://arxiv.org/abs/2403.12358v1","category":"astro-ph.SR"}
{"created":"2024-03-19 01:58:59","title":"MoodSmith: Enabling Mood-Consistent Multimedia for AI-Generated Advocacy Campaigns","abstract":"Emotion is vital to information and message processing, playing a key role in attitude formation. Consequently, creating a mood that evokes an emotional response is essential to any compelling piece of outreach communication. Many nonprofits and charities, despite having established messages, face challenges in creating advocacy campaign videos for social media. It requires significant creative and cognitive efforts to ensure that videos achieve the desired mood across multiple dimensions: script, visuals, and audio. We introduce MoodSmith, an AI-powered system that helps users explore mood possibilities for their message and create advocacy campaigns that are mood-consistent across dimensions. To achieve this, MoodSmith uses emotive language and plotlines for scripts, artistic style and color palette for visuals, and positivity and energy for audio. Our studies show that MoodSmith can effectively achieve a variety of moods, and the produced videos are consistent across media dimensions.","sentences":["Emotion is vital to information and message processing, playing a key role in attitude formation.","Consequently, creating a mood that evokes an emotional response is essential to any compelling piece of outreach communication.","Many nonprofits and charities, despite having established messages, face challenges in creating advocacy campaign videos for social media.","It requires significant creative and cognitive efforts to ensure that videos achieve the desired mood across multiple dimensions: script, visuals, and audio.","We introduce MoodSmith, an AI-powered system that helps users explore mood possibilities for their message and create advocacy campaigns that are mood-consistent across dimensions.","To achieve this, MoodSmith uses emotive language and plotlines for scripts, artistic style and color palette for visuals, and positivity and energy for audio.","Our studies show that MoodSmith can effectively achieve a variety of moods, and the produced videos are consistent across media dimensions."],"url":"http://arxiv.org/abs/2403.12356v1","category":"cs.HC"}
{"created":"2024-03-19 01:43:32","title":"Investigation of non-equilibrium ionization plasma during a giant flare of UX Arietis triggered with MAXI and observed with NICER","abstract":"We detected a giant X-ray flare from the RS-CVn type binary star UX Ari using MAXI on 2020 August 17 and started a series of NICER observations 89 minutes later. For a week, the entire duration of the flare was covered with 32 snapshot observations including the rising phase. The X-ray luminosity reached 2$\\times$10$^{33}$ erg s$^{-1}$ and the entire energy release was $\\sim 10^{38}$ erg in the 0.5--8.0~keV band. X-ray spectra characterized by continuum emission with lines of Fe XXV He$\\alpha$ and Fe XXVI Ly$\\alpha$ were obtained. We found that the temperature peaks before that of the flux, which suggests that the period of plasma formation in the magnetic flare loop was captured. Using the continuum information (temperature, flux, and their delay time), we estimated the flare loop size to be $\\sim 3 \\times 10^{11}$ cm and the peak electron density to be $\\sim 4\\times10^{10}$ cm$^{-3}$. Furthermore, using the line ratio of Fe XXV and Fe XXVI, we investigated any potential indications of deviation from collisional ionization equilibrium (CIE). The X-ray spectra were consistent with CIE plasma throughout the flare, but the possibility of an ionizing plasma away from CIE was not rejected in the flux rising phase.","sentences":["We detected a giant X-ray flare from the RS-CVn type binary star UX Ari using MAXI on 2020 August 17 and started a series of NICER observations 89 minutes later.","For a week, the entire duration of the flare was covered with 32 snapshot observations including the rising phase.","The X-ray luminosity reached 2$\\times$10$^{33}$ erg s$^{-1}$ and the entire energy release was $\\sim 10^{38}$ erg in the 0.5--8.0~keV band.","X-ray spectra characterized by continuum emission with lines of Fe XXV He$\\alpha$ and Fe XXVI Ly$\\alpha$ were obtained.","We found that the temperature peaks before that of the flux, which suggests that the period of plasma formation in the magnetic flare loop was captured.","Using the continuum information (temperature, flux, and their delay time), we estimated the flare loop size to be $\\sim 3 \\times 10^{11}$ cm and the peak electron density to be $\\sim 4\\times10^{10}$ cm$^{-3}$. Furthermore, using the line ratio of Fe XXV and Fe XXVI, we investigated any potential indications of deviation from collisional ionization equilibrium (CIE).","The X-ray spectra were consistent with CIE plasma throughout the flare, but the possibility of an ionizing plasma away from CIE was not rejected in the flux rising phase."],"url":"http://arxiv.org/abs/2403.12351v1","category":"astro-ph.SR"}
{"created":"2024-03-19 01:11:43","title":"A fast low-rank inversion algorithm of dielectric matrix in GW approximation","abstract":"The dielectric response function and its inverse are crucial physical quantities in materials science. We propose an accurate and efficient strategy to invert the dielectric function matrix. The GW approximation, a powerful approach to accurately describe many-body excited states, is taken as an application to demonstrate accuracy and efficiency. We incorporate the interpolative separable density fitting (ISDF) algorithm with Sherman--Morrison--Woodbury (SMW) formula to accelerate the inversion process by exploiting low-rank properties of dielectric function in plane-wave GW calculations. Our ISDF--SMW strategy produces accurate quasiparticle energies with $O(N_{\\mathrm{r}}N_{\\mathrm{e}}^2)$ computational cost $(N_{\\mathrm{e}}$ is the number of electrons and $N_{\\mathrm{r}}=100$--$1000N_{\\mathrm{e}}$ is the number of grid points) with negligible small error of $0.03$~eV for both complex molecules and solids. This new strategy for inverting the dielectric matrix can be \\(50\\times\\) faster than the current state-of-the-art implementation in BerkeleyGW, resulting in two orders of magnitude speedup for total GW calculations.","sentences":["The dielectric response function and its inverse are crucial physical quantities in materials science.","We propose an accurate and efficient strategy to invert the dielectric function matrix.","The GW approximation, a powerful approach to accurately describe many-body excited states, is taken as an application to demonstrate accuracy and efficiency.","We incorporate the interpolative separable density fitting (ISDF) algorithm with Sherman--Morrison--Woodbury (SMW) formula to accelerate the inversion process by exploiting low-rank properties of dielectric function in plane-wave GW calculations.","Our ISDF--SMW strategy produces accurate quasiparticle energies with $O(N_{\\mathrm{r}}N_{\\mathrm{e}}^2)$ computational cost $(N_{\\mathrm{e}}$ is the number of electrons and $N_{\\mathrm{r}}=100$--$1000N_{\\mathrm{e}}$ is the number of grid points) with negligible small error of $0.03$~eV for both complex molecules and solids.","This new strategy for inverting the dielectric matrix can be \\(50\\times\\) faster than the current state-of-the-art implementation in BerkeleyGW, resulting in two orders of magnitude speedup for total GW calculations."],"url":"http://arxiv.org/abs/2403.12340v1","category":"math.NA"}
{"created":"2024-03-19 00:05:31","title":"Exact thermal eigenstates of nonintegrable spin chains at infinite temperature","abstract":"The eigenstate thermalization hypothesis (ETH) plays a major role in explaining thermalization of isolated quantum many-body systems. However, there has been no proof of the ETH in realistic systems due to the difficulty in the theoretical treatment of thermal energy eigenstates of nonintegrable systems. Here, we write down analytically, for the first time, thermal eigenstates of nonintegrable spin chains. We introduce a class of theoretically tractable volume-law states, which we call entangled antipodal pair (EAP) states. These states are thermal, in the most strict sense that they are indistinguishable from the Gibbs state with respect to all local observables, with infinite temperature. We then identify Hamiltonians having the EAP state as an eigenstate and rigorously show that some of these Hamiltonians are nonintegrable. Furthermore, a thermal pure state at an arbitrary temperature is obtained by the imaginary time evolution of an EAP state. Our results offer a potential avenue for providing a provable example of the ETH.","sentences":["The eigenstate thermalization hypothesis (ETH) plays a major role in explaining thermalization of isolated quantum many-body systems.","However, there has been no proof of the ETH in realistic systems due to the difficulty in the theoretical treatment of thermal energy eigenstates of nonintegrable systems.","Here, we write down analytically, for the first time, thermal eigenstates of nonintegrable spin chains.","We introduce a class of theoretically tractable volume-law states, which we call entangled antipodal pair (EAP) states.","These states are thermal, in the most strict sense that they are indistinguishable from the Gibbs state with respect to all local observables, with infinite temperature.","We then identify Hamiltonians having the EAP state as an eigenstate and rigorously show that some of these Hamiltonians are nonintegrable.","Furthermore, a thermal pure state at an arbitrary temperature is obtained by the imaginary time evolution of an EAP state.","Our results offer a potential avenue for providing a provable example of the ETH."],"url":"http://arxiv.org/abs/2403.12330v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-18 23:30:38","title":"The Amaterasu Cosmic Ray as a Magnetic Monopole and Implications for Extensions of the Standard Model","abstract":"The Amaterasu cosmic ray particle appears to have come from the direction of the local cosmic void. We take this as evidence that it is a magnetic monopole rather than a proton or nucleus. This in turn strongly suggests physics at high energy is described by a quiver gauge theory.","sentences":["The Amaterasu cosmic ray particle appears to have come from the direction of the local cosmic void.","We take this as evidence that it is a magnetic monopole rather than a proton or nucleus.","This in turn strongly suggests physics at high energy is described by a quiver gauge theory."],"url":"http://arxiv.org/abs/2403.12322v1","category":"hep-ph"}
{"created":"2024-03-18 23:23:39","title":"Orbifold Indices in Four Dimensions","abstract":"We introduce supersymmetric indices for four-dimensional gauge theories defined on $\\mathscr O \\times S^1$, where $\\mathscr O $ is a circle bundle over the weighted complex projective line informally known as spindle. Trivial fibrations yield a four-dimensional version of the spindle index, which we obtain by applying localization to partition functions of theories on the direct product of a spindle and a two-dimensional torus. Conversely, non-trivial fibrations lead to the branched lens index, which we compute by localizing theories on the direct product of a circle and a branched covering of the lens space, possibly endowed with conical singularities. The branched lens index encompasses the maximally refined four-dimensional lens index as a special case.","sentences":["We introduce supersymmetric indices for four-dimensional gauge theories defined on $\\mathscr O \\times S^1$, where $\\mathscr O $ is a circle bundle over the weighted complex projective line informally known as spindle.","Trivial fibrations yield a four-dimensional version of the spindle index, which we obtain by applying localization to partition functions of theories on the direct product of a spindle and a two-dimensional torus.","Conversely, non-trivial fibrations lead to the branched lens index, which we compute by localizing theories on the direct product of a circle and a branched covering of the lens space, possibly endowed with conical singularities.","The branched lens index encompasses the maximally refined four-dimensional lens index as a special case."],"url":"http://arxiv.org/abs/2403.12318v1","category":"hep-th"}
{"created":"2024-03-18 23:19:33","title":"Quantifying the efficiency of principal signal transmission modes in proteins","abstract":"On the microscopic level, biological signal transmission relies on coordinated structural changes in allosteric proteins that involve sensor and effector modules. The timescales and microscopic details of signal transmission in proteins are often unclear, despite a plethora of structural information on signaling proteins. Based on linear-response theory, we develop a theoretical framework to define frequency-dependent force and displacement transmit functions through proteins and, more generally, viscoelastic media. Transmit functions quantify the fraction of a local time-dependent perturbation at one site, be it a deformation, a force, or a combination thereof, that survives at a second site. They are defined in terms of equilibrium fluctuations from simulations or experimental observations. We apply the framework to our all-atom molecular dynamics simulation data of a parallel, homodimeric coiled-coil (CC) motif that connects sensor and effector modules of a blue-light-regulated histidine kinase from bacterial signaling systems extensively studied in experiments. Our analysis reveals that signal transmission through the CC is possible via shift, splay, and twist deformation modes. Based on the results of mutation experiments, we infer that the most relevant mode for the biological function of the histidine kinase protein is the splay deformation.","sentences":["On the microscopic level, biological signal transmission relies on coordinated structural changes in allosteric proteins that involve sensor and effector modules.","The timescales and microscopic details of signal transmission in proteins are often unclear, despite a plethora of structural information on signaling proteins.","Based on linear-response theory, we develop a theoretical framework to define frequency-dependent force and displacement transmit functions through proteins and, more generally, viscoelastic media.","Transmit functions quantify the fraction of a local time-dependent perturbation at one site, be it a deformation, a force, or a combination thereof, that survives at a second site.","They are defined in terms of equilibrium fluctuations from simulations or experimental observations.","We apply the framework to our all-atom molecular dynamics simulation data of a parallel, homodimeric coiled-coil (CC) motif that connects sensor and effector modules of a blue-light-regulated histidine kinase from bacterial signaling systems extensively studied in experiments.","Our analysis reveals that signal transmission through the CC is possible via shift, splay, and twist deformation modes.","Based on the results of mutation experiments, we infer that the most relevant mode for the biological function of the histidine kinase protein is the splay deformation."],"url":"http://arxiv.org/abs/2403.12312v1","category":"cond-mat.soft"}
{"created":"2024-03-18 23:18:40","title":"Prototipo de un Contador Bidireccional Autom\u00e1tico de Personas basado en sensores de visi\u00f3n 3D","abstract":"3D sensors, also known as RGB-D sensors, utilize depth images where each pixel measures the distance from the camera to objects, using principles like structured light or time-of-flight. Advances in artificial vision have led to affordable 3D cameras capable of real-time object detection without object movement, surpassing 2D cameras in information depth. These cameras can identify objects of varying colors and reflectivities and are less affected by lighting changes. The described prototype uses RGB-D sensors for bidirectional people counting in venues, aiding security and surveillance in spaces like stadiums or airports. It determines real-time occupancy and checks against maximum capacity, crucial during emergencies. The system includes a RealSense D415 depth camera and a mini-computer running object detection algorithms to count people and a 2D camera for identity verification. The system supports statistical analysis and uses C++, Python, and PHP with OpenCV for image processing, demonstrating a comprehensive approach to monitoring venue occupancy.","sentences":["3D sensors, also known as RGB-D sensors, utilize depth images where each pixel measures the distance from the camera to objects, using principles like structured light or time-of-flight.","Advances in artificial vision have led to affordable 3D cameras capable of real-time object detection without object movement, surpassing 2D cameras in information depth.","These cameras can identify objects of varying colors and reflectivities and are less affected by lighting changes.","The described prototype uses RGB-D sensors for bidirectional people counting in venues, aiding security and surveillance in spaces like stadiums or airports.","It determines real-time occupancy and checks against maximum capacity, crucial during emergencies.","The system includes a RealSense D415 depth camera and a mini-computer running object detection algorithms to count people and a 2D camera for identity verification.","The system supports statistical analysis and uses C++, Python, and PHP with OpenCV for image processing, demonstrating a comprehensive approach to monitoring venue occupancy."],"url":"http://arxiv.org/abs/2403.12310v1","category":"cs.CV"}
{"created":"2024-03-18 22:45:04","title":"Simultaneous NICER and NuSTAR observations of the Ultraluminous source NGC 4190 ULX-1","abstract":"We present an X-ray analysis of three different XMM-Newton observations together with simultaneous NICER and NuSTAR observations of the ultraluminous X-ray source NGC 4190 ULX-1. Our goal is to constrain the structure of the accretion disk and the geometrical properties of the source. We performed a temporal and spectral analyses in the 0.4--30 keV energy range where the source is significantly detected in dedicated XMM-Newton, NICER and NuSTAR observations. The temporal analysis shows no flaring activity in the light curves. No pulsation is detected throughout. The source exhibits a typical ULX spectrum, which can be fitted with two thermal blackbody components plus a Comptonization tail at high energies. The luminosity-temperature relation of each thermal spectral component is consistent with the $L \\propto T^{2}$ relation expected from an advection-dominated supercritical disk. We interpret these results as a super-Eddington accreting black hole seen almost face-on. A dense wind ejected from the disk obscures the central source, and a hot electron plasma is evacuated through the funnel formed above the hole. Geometric beaming is responsible for the ULX soft emission, whereas the hard tail is the result of Comptonization of soft photons by the electrons ejected through the funnel.","sentences":["We present an X-ray analysis of three different XMM-Newton observations together with simultaneous NICER and NuSTAR observations of the ultraluminous X-ray source NGC 4190 ULX-1.","Our goal is to constrain the structure of the accretion disk and the geometrical properties of the source.","We performed a temporal and spectral analyses in the 0.4--30 keV energy range where the source is significantly detected in dedicated XMM-Newton, NICER and NuSTAR observations.","The temporal analysis shows no flaring activity in the light curves.","No pulsation is detected throughout.","The source exhibits a typical ULX spectrum, which can be fitted with two thermal blackbody components plus a Comptonization tail at high energies.","The luminosity-temperature relation of each thermal spectral component is consistent with the $L \\propto T^{2}$ relation expected from an advection-dominated supercritical disk.","We interpret these results as a super-Eddington accreting black hole seen almost face-on.","A dense wind ejected from the disk obscures the central source, and a hot electron plasma is evacuated through the funnel formed above the hole.","Geometric beaming is responsible for the ULX soft emission, whereas the hard tail is the result of Comptonization of soft photons by the electrons ejected through the funnel."],"url":"http://arxiv.org/abs/2403.12300v1","category":"astro-ph.HE"}
{"created":"2024-03-18 22:42:11","title":"A new method to search for highly ionizing exotic particles, monopoles and beyond, using time projection chamber","abstract":"Measuring the energy loss and mass of highly ionizing particles predicted by theories from beyond the Standard Model pose considerable challenges to conventional detection techniques. Such particles are predicted to experience energy loss to matter they pass through that exceeds the dynamic range specified for most readout chips, leading to saturation of the detectors' electronics. Consequently, achieving precise energy loss and mass measurements becomes unattainable. We present a new approach to detect such highly ionizing particles using time projection chambers that overcomes this limitation and provide a case study for triggering on magnetic monopoles.","sentences":["Measuring the energy loss and mass of highly ionizing particles predicted by theories from beyond the Standard Model pose considerable challenges to conventional detection techniques.","Such particles are predicted to experience energy loss to matter they pass through that exceeds the dynamic range specified for most readout chips, leading to saturation of the detectors' electronics.","Consequently, achieving precise energy loss and mass measurements becomes unattainable.","We present a new approach to detect such highly ionizing particles using time projection chambers that overcomes this limitation and provide a case study for triggering on magnetic monopoles."],"url":"http://arxiv.org/abs/2403.12299v1","category":"physics.ins-det"}
{"created":"2024-03-18 22:41:05","title":"Electrical/piezoresistive effects in bent Alpide MAPS","abstract":"The ITS3 upgrade baseline design employs MAPS (Monolithic Active Pixel Sensor) in bent state. Bending experiments with the existing ITS2 MAPS (=Alpide chip) show it remains functional but with relative large analog supply current changes. It is shown that by the piezoresistive effect, rotation of current mirror FETs can be responsible which was confirmed after validating the layout. Measured Gauge Factor has proper sign but is 3 times lower than typical values derived from literature. The magnitude of the measured strain induced PMOS V$_{th}$ shift is as expected but the sign differs for compressive strain with some of the literature.","sentences":["The ITS3 upgrade baseline design employs MAPS (Monolithic Active Pixel Sensor) in bent state.","Bending experiments with the existing ITS2 MAPS (=Alpide chip) show it remains functional but with relative large analog supply current changes.","It is shown that by the piezoresistive effect, rotation of current mirror FETs can be responsible which was confirmed after validating the layout.","Measured Gauge Factor has proper sign but is 3 times lower than typical values derived from literature.","The magnitude of the measured strain induced PMOS V$_{th}$ shift is as expected but the sign differs for compressive strain with some of the literature."],"url":"http://arxiv.org/abs/2403.12298v1","category":"physics.ins-det"}
{"created":"2024-03-18 22:33:11","title":"Computational modeling of the physical features that influence breast cancer invasion into adipose tissue","abstract":"Breast cancer invasion into adipose tissue strongly influences disease progression and metastasis. The degree of cancer cell invasion into adipose tissue depends on numerous biochemical and physical properties of cancer cells, adipocytes, and other key components of adipose tissue. We model breast cancer invasion into adipose tissue as a physical process by carrying out simulations of active, cohesive spherical particles (cancer cells) invading into confluent packings of deformable polyhedra (adipocytes). We quantify the degree of invasion by calculating the interfacial area $A_t$ between cancer cells and adipocytes. We determine the long-time value of $A_t$ versus the activity and strength of the cohesion between cancer cells, as well as mechanical properties of the adipocytes and extracellular matrix (ECM) in which the adipocytes are embedded. We show that the degree of invasion collapses onto a master curve by plotting it versus a dimensionless energy scale $E_c$, which grows linearly with mean-square fluctuations and persistence time of the cancer cell velocities, is inversely proportional to the pressure of the system, and has an offset that increases with the cancer cell cohesive energy. The condition, $E_c \\gg 1$, indicates that cancer cells will invade the adipose tissue, whereas for $E_c \\ll 1$, the cancer cells and adipocytes remain demixed. We also show that constraints on adipocyte positions by the ECM decrease $A_t$ relative to that obtained for unconstrained adipocytes. Finally, spatial heterogeneity in structural and mechanical properties of the adipocytes in the presence of ECM impedes invasion relative to adipose tissue with uniform properties.","sentences":["Breast cancer invasion into adipose tissue strongly influences disease progression and metastasis.","The degree of cancer cell invasion into adipose tissue depends on numerous biochemical and physical properties of cancer cells, adipocytes, and other key components of adipose tissue.","We model breast cancer invasion into adipose tissue as a physical process by carrying out simulations of active, cohesive spherical particles (cancer cells) invading into confluent packings of deformable polyhedra (adipocytes).","We quantify the degree of invasion by calculating the interfacial area $A_t$ between cancer cells and adipocytes.","We determine the long-time value of $A_t$ versus the activity and strength of the cohesion between cancer cells, as well as mechanical properties of the adipocytes and extracellular matrix (ECM) in which the adipocytes are embedded.","We show that the degree of invasion collapses onto a master curve by plotting it versus a dimensionless energy scale $E_c$, which grows linearly with mean-square fluctuations and persistence time of the cancer cell velocities, is inversely proportional to the pressure of the system, and has an offset that increases with the cancer cell cohesive energy.","The condition, $E_c \\gg 1$, indicates that cancer cells will invade the adipose tissue, whereas for $E_c \\ll 1$, the cancer cells and adipocytes remain demixed.","We also show that constraints on adipocyte positions by the ECM decrease $A_t$ relative to that obtained for unconstrained adipocytes.","Finally, spatial heterogeneity in structural and mechanical properties of the adipocytes in the presence of ECM impedes invasion relative to adipose tissue with uniform properties."],"url":"http://arxiv.org/abs/2403.12293v1","category":"physics.bio-ph"}
{"created":"2024-03-18 22:23:03","title":"BostonTwin: the Boston Digital Twin for Ray-Tracing in 6G Networks","abstract":"Digital twins are now a staple of wireless networks design and evolution. Creating an accurate digital copy of a real system offers numerous opportunities to study and analyze its performance and issues. It also allows designing and testing new solutions in a risk-free environment, and applying them back to the real system after validation. A candidate technology that will heavily rely on digital twins for design and deployment is 6G, which promises robust and ubiquitous networks for eXtended Reality (XR) and immersive communications solutions. In this paper, we present BostonTwin, a dataset that merges a high-fidelity 3D model of the city of Boston, MA, with the existing geospatial data on cellular base stations deployments, in a ray-tracing-ready format. Thus, BostonTwin enables not only the instantaneous rendering and programmatic access to the building models, but it also allows for an accurate representation of the electromagnetic propagation environment in the real-world city of Boston. The level of detail and accuracy of this characterization is crucial to designing 6G networks that can support the strict requirements of sensitive and high-bandwidth applications, such as XR and immersive communication.","sentences":["Digital twins are now a staple of wireless networks design and evolution.","Creating an accurate digital copy of a real system offers numerous opportunities to study and analyze its performance and issues.","It also allows designing and testing new solutions in a risk-free environment, and applying them back to the real system after validation.","A candidate technology that will heavily rely on digital twins for design and deployment is 6G, which promises robust and ubiquitous networks for eXtended Reality (XR) and immersive communications solutions.","In this paper, we present BostonTwin, a dataset that merges a high-fidelity 3D model of the city of Boston, MA, with the existing geospatial data on cellular base stations deployments, in a ray-tracing-ready format.","Thus, BostonTwin enables not only the instantaneous rendering and programmatic access to the building models, but it also allows for an accurate representation of the electromagnetic propagation environment in the real-world city of Boston.","The level of detail and accuracy of this characterization is crucial to designing 6G networks that can support the strict requirements of sensitive and high-bandwidth applications, such as XR and immersive communication."],"url":"http://arxiv.org/abs/2403.12289v1","category":"cs.NI"}
{"created":"2024-03-18 22:12:15","title":"Bounds on the photon mass via the Shapiro effect in the solar system","abstract":"We study the effects of a finite mass for the photon on its propagation in a weak gravitational field. In particular, we analyse the gravitational time delay, also known as the Shapiro effect. We work in isotropic coordinates in the weak-field limit and find that the mass-dependent corrections enhance the gravitational time delay. Doppler-tracking data from the Cassini mission allow us to set upper bounds on the photon mass at the level of $m_\\gamma \\lesssim 3 \\times 10^{-7} \\, {\\rm eV/c^2}$. We also discuss next-generation solar-system tests of general relativity that could improve this upper limit, potentially reaching $m_\\gamma \\lesssim 8 \\times 10^{-8} \\, {\\rm eV/c^2}$. Though not competitive with the currently best limits, our projected bounds are at the ballpark of earlier ones based on the gravitational deflection angle.","sentences":["We study the effects of a finite mass for the photon on its propagation in a weak gravitational field.","In particular, we analyse the gravitational time delay, also known as the Shapiro effect.","We work in isotropic coordinates in the weak-field limit and find that the mass-dependent corrections enhance the gravitational time delay.","Doppler-tracking data from the Cassini mission allow us to set upper bounds on the photon mass at the level of $m_\\gamma \\lesssim 3 \\times 10^{-7} \\, {\\rm eV/c^2}$. We also discuss next-generation solar-system tests of general relativity that could improve this upper limit, potentially reaching $m_\\gamma \\lesssim 8 \\times 10^{-8} \\, {\\rm eV/c^2}$. Though not competitive with the currently best limits, our projected bounds are at the ballpark of earlier ones based on the gravitational deflection angle."],"url":"http://arxiv.org/abs/2403.12286v1","category":"gr-qc"}
{"created":"2024-03-18 22:05:21","title":"Energy Consumption in RES-Aware 5G Networks","abstract":"In this work, the impact of using Renewable Energy Source (RES) generators in next-generation (5G) cellular systems on total power consumption (PC) has been investigated. The paper highlights the gain related to the use of photovoltaic (PV) panels and wind turbines (WTs) in the form of two factors - the average extension of battery lifetime (AEBL) powering a single network cell and the average reduction in energy consumption (AREC) within the whole network. The examination has been conducted for four different seasons of the year and various configurations of available power sources. The provided system scenario was based on real data on weather conditions, building placement, and implemented mobile networks for the city of Poznan in Poland. Used RES generators were designed in accordance with the specifications of real devices.","sentences":["In this work, the impact of using Renewable Energy Source (RES) generators in next-generation (5G) cellular systems on total power consumption (PC) has been investigated.","The paper highlights the gain related to the use of photovoltaic (PV) panels and wind turbines (WTs) in the form of two factors - the average extension of battery lifetime (AEBL) powering a single network cell and the average reduction in energy consumption (AREC) within the whole network.","The examination has been conducted for four different seasons of the year and various configurations of available power sources.","The provided system scenario was based on real data on weather conditions, building placement, and implemented mobile networks for the city of Poznan in Poland.","Used RES generators were designed in accordance with the specifications of real devices."],"url":"http://arxiv.org/abs/2403.12283v1","category":"cs.NI"}
{"created":"2024-03-18 21:53:44","title":"Adsorbate Dissociation Due to Heteromolecular Electronic Energy Transfer from Fluorobenzene Thin Films","abstract":"Study of the near-UV photodissociation dynamics for monolayer (ML) quantities of CH$_3$I on thin films of a series of fluorobenzenes and benzene (1--25ML) grown on a Cu(100) substrate finds that in addition to gas-phase-like neutral photodissociation, CH$_3$I dissociation can be enhanced via photoabsorption in several of the thin films studied. Distinct CH$_3$ photofragment kinetic energy distributions are found for CH$_3$I photodissociation on C$_6$H$_5$F, 1,4-C$_6$H$_4$F$_2$ and C$_6$H$_6$ thin films, and distinguished from neutral photodissociation pathways using polarized incident light. The effective photodissociation cross section for CH$_3$I on these thin films is increased as compared to that for the higher F-count fluorobenzene thin films due to the additional photodissociation pathway available. Quenching by the metal substrate of the photoexcitation via this new pathway suggests a significantly longer timescale for excitation than that of neutral CH$_3$I photodissociation. The observations support a mechanism in which neutral photoexcitation in the thin film (i.e. an exciton) is transported to the interface with CH$_3$I, and transferring the electronic excitation to the CH$_3$I which then dissociates. The unimodal CH$_3$ photofragment distribution and observed kinetic energies on the fluorobenzene thin films suggest that the dissociation occurs via the $^3Q_1$ excited state of CH$_3$I.","sentences":["Study of the near-UV photodissociation dynamics for monolayer (ML) quantities of CH$_3$I on thin films of a series of fluorobenzenes and benzene (1--25ML) grown on a Cu(100) substrate finds that in addition to gas-phase-like neutral photodissociation, CH$_3$I dissociation can be enhanced via photoabsorption in several of the thin films studied.","Distinct CH$_3$ photofragment kinetic energy distributions are found for CH$_3$I photodissociation on C$_6$H$_5$F, 1,4-C$_6$H$_4$F$_2$ and C$_6$H$_6$ thin films, and distinguished from neutral photodissociation pathways using polarized incident light.","The effective photodissociation cross section for CH$_3$I on these thin films is increased as compared to that for the higher F-count fluorobenzene thin films due to the additional photodissociation pathway available.","Quenching by the metal substrate of the photoexcitation via this new pathway suggests a significantly longer timescale for excitation than that of neutral CH$_3$I photodissociation.","The observations support a mechanism in which neutral photoexcitation in the thin film (i.e. an exciton) is transported to the interface with CH$_3$I, and transferring the electronic excitation to the CH$_3$I which then dissociates.","The unimodal CH$_3$ photofragment distribution and observed kinetic energies on the fluorobenzene thin films suggest that the dissociation occurs via the $^3Q_1$ excited state of CH$_3$I."],"url":"http://arxiv.org/abs/2403.12277v1","category":"physics.chem-ph"}
{"created":"2024-03-18 21:36:48","title":"Serial Properties, Selector Proofs, and the Provability of Consistency","abstract":"For Hilbert, the consistency of a formal theory T is an infinite series of statements \"D is free of contradictions\" for each derivation D and a consistency proof is i) an operation that, given D, yields a proof that D is free of contradictions, and ii) a proof that (i) works for all inputs D. Hilbert's two-stage approach to proving consistency naturally generalizes to the notion of a finite proof of a series of sentences in a given theory. Such proofs, which we call selector proofs, have already been tacitly employed in mathematics. Selector proofs of consistency, including Hilbert's epsilon substitution method, do not aim at deriving the G\\\"odelian consistency formula Con(T) and are thus not precluded by G\\\"odel's second incompleteness theorem. We give a selector proof of consistency of Peano Arithmetic PA and formalize this proof in PA.","sentences":["For Hilbert, the consistency of a formal theory T is an infinite series of statements \"D is free of contradictions\" for each derivation D and a consistency proof is i) an operation that, given D, yields a proof that D is free of contradictions, and ii) a proof that (i) works for all inputs D. Hilbert's two-stage approach to proving consistency naturally generalizes to the notion of a finite proof of a series of sentences in a given theory.","Such proofs, which we call selector proofs, have already been tacitly employed in mathematics.","Selector proofs of consistency, including Hilbert's epsilon substitution method, do not aim at deriving the G\\\"odelian consistency formula Con(T) and are thus not precluded by G\\\"odel's second incompleteness theorem.","We give a selector proof of consistency of Peano Arithmetic PA and formalize this proof in PA."],"url":"http://arxiv.org/abs/2403.12272v1","category":"math.LO"}
