{"created":"2024-03-20 17:59:58","title":"On Pretraining Data Diversity for Self-Supervised Learning","abstract":"We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .","sentences":["We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget.","Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal.","Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge.","Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days.","Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL ."],"url":"http://arxiv.org/abs/2403.13808v1","category":"cs.CV"}
{"created":"2024-03-20 17:59:55","title":"RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition","abstract":"CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the model's comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.","sentences":["CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items.","Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora.","However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size.","To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs.","We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window.","During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions.","Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the model's comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks.","Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting."],"url":"http://arxiv.org/abs/2403.13805v1","category":"cs.CV"}
{"created":"2024-03-20 17:59:14","title":"ZigMa: Zigzag Mamba Diffusion Model","abstract":"The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO $256\\times 256$. Code will be released at https://taohu.me/zigma/","sentences":["The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures.","In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation.","Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba.","Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines.","Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO $256\\times 256$. Code will be released at https://taohu.me/zigma/"],"url":"http://arxiv.org/abs/2403.13802v1","category":"cs.CV"}
{"created":"2024-03-20 17:58:12","title":"Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs","abstract":"We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.","sentences":["We demonstrate experimental results with LLMs that address robotics action planning problems.","Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes.","In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies.","Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence.","Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks."],"url":"http://arxiv.org/abs/2403.13801v1","category":"cs.RO"}
{"created":"2024-03-20 17:55:35","title":"Reverse Training to Nurse the Reversal Curse","abstract":"Large language models (LLMs) have a surprising failure: when trained on \"A has a feature B\", they do not generalize to \"B is a feature of A\", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.","sentences":["Large language models (LLMs) have a surprising failure: when trained on \"A has a feature B\", they do not generalize to \"B is a feature of A\", which is termed the Reversal Curse.","Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet.","This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens.","The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities.","We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue."],"url":"http://arxiv.org/abs/2403.13799v1","category":"cs.CL"}
{"created":"2024-03-20 17:55:21","title":"Hierarchical NeuroSymbolic Approach for Action Quality Assessment","abstract":"Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action. Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth. To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols. We take diving as the case study. We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving. Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence. As verified by a group of domain experts, this report may be used to assist judges in scoring, help train judges, and provide feedback to divers. We will open-source all of our annotated training data and code for ease of reproducibility.","sentences":["Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action.","Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth.","To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols.","We take diving as the case study.","We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving.","Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence.","As verified by a group of domain experts, this report may be used to assist judges in scoring, help train judges, and provide feedback to divers.","We will open-source all of our annotated training data and code for ease of reproducibility."],"url":"http://arxiv.org/abs/2403.13798v1","category":"cs.CV"}
{"created":"2024-03-20 17:54:42","title":"The VELOCE Modulation Zoo I. Spectroscopic detection of non-radial modes in the first-overtone Cepheids BG Crucis, QZ Normae, V0391 Normae, and V0411 Lacertae","abstract":"The photometric observations from the recent decade revolutionized our view on classical pulsators. Low-amplitude signals have been detected photometrically in addition to the dominant high-amplitude radial mode pulsations in many RR Lyrae stars and classical Cepheids. First overtone (1O) pulsators with an additional low-amplitude signal at a period ratio of around 0.61 with the main mode, the so-called 0.61 stars, form the most populous group among these stars. The nature of this signal has been attributed to non-radial pulsations. Another mysterious group are stars, where the additional signal forms a period ratio of around 0.68 - the 0.68 stars. The origin of the signal remains unknown. Here, we search for similar phenomena in spectroscopic observations of 1O classical Cepheids collected as part of the VELOCE project. We performed frequency analysis of several parameters derived from cross-correlation functions (CCFs), including radial velocity, FWHM, bisector inverse span, and CCF depth. Using standard prewhitening, we searched for additional low-amplitude signals. We identify the location of these stars in various sequences of the Petersen diagram. We detect additional signals in four 1O classical Cepheids: BG Cru, QZ Nor, V391 Nor, and V411 Lac. We classified BG Cru, QZ Nor, and V391 Nor as 0.61 stars based on period ratios. V411 Lac, however, exhibits a ratio of 0.68 between the two modes, and the additional signal has a longer period. This kind of multiperiodicity remains unexplained. VELOCE CCFs yield the first spectroscopic detections of non-radial pulsation modes in classical Cepheids. This opens an asteroseismic window for pursuing a more detailed understanding of these important stars. While the 0.61 signal of BG Cru, QZ Nor, V391 Nor is understood to originate due to non-radial modes of moderate degrees, the 0.68 signal of V411 Lac still lacks a physical explanation.","sentences":["The photometric observations from the recent decade revolutionized our view on classical pulsators.","Low-amplitude signals have been detected photometrically in addition to the dominant high-amplitude radial mode pulsations in many RR Lyrae stars and classical Cepheids.","First overtone (1O) pulsators with an additional low-amplitude signal at a period ratio of around 0.61 with the main mode, the so-called 0.61 stars, form the most populous group among these stars.","The nature of this signal has been attributed to non-radial pulsations.","Another mysterious group are stars, where the additional signal forms a period ratio of around 0.68 - the 0.68 stars.","The origin of the signal remains unknown.","Here, we search for similar phenomena in spectroscopic observations of 1O classical Cepheids collected as part of the VELOCE project.","We performed frequency analysis of several parameters derived from cross-correlation functions (CCFs), including radial velocity, FWHM, bisector inverse span, and CCF depth.","Using standard prewhitening, we searched for additional low-amplitude signals.","We identify the location of these stars in various sequences of the Petersen diagram.","We detect additional signals in four 1O classical Cepheids: BG Cru, QZ Nor, V391 Nor, and V411 Lac.","We classified BG Cru, QZ Nor, and V391 Nor as 0.61 stars based on period ratios.","V411 Lac, however, exhibits a ratio of 0.68 between the two modes, and the additional signal has a longer period.","This kind of multiperiodicity remains unexplained.","VELOCE CCFs yield the first spectroscopic detections of non-radial pulsation modes in classical Cepheids.","This opens an asteroseismic window for pursuing a more detailed understanding of these important stars.","While the 0.61 signal of BG Cru, QZ Nor, V391 Nor is understood to originate due to non-radial modes of moderate degrees, the 0.68 signal of V411 Lac still lacks a physical explanation."],"url":"http://arxiv.org/abs/2403.13796v1","category":"astro-ph.SR"}
{"created":"2024-03-20 17:49:54","title":"RewardBench: Evaluating Reward Models for Language Modeling","abstract":"Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.","sentences":["Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models.","Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them.","To date, very few descriptors of capabilities, training methods, or open-source reward models exist.","In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models.","The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries.","We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another.","On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets.","We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process."],"url":"http://arxiv.org/abs/2403.13787v1","category":"cs.LG"}
{"created":"2024-03-20 17:47:08","title":"The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI","abstract":"Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many \"open-source\" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as \"openwashing.\" We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adopted without restrictions. Wide adoption of the MOF will foster a more open AI ecosystem, accelerating research, innovation, and adoption.","sentences":["Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety.","Many \"open-source\" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as \"openwashing.\"","We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access.","The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses.","This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adopted without restrictions.","Wide adoption of the MOF will foster a more open AI ecosystem, accelerating research, innovation, and adoption."],"url":"http://arxiv.org/abs/2403.13784v1","category":"cs.LG"}
{"created":"2024-03-20 17:42:08","title":"Information-Theoretic Distillation for Reference-less Summarization","abstract":"The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on this formulation, we start off from Pythia-2.8B as the teacher model, which is not yet capable of summarization, then self-train the model to optimize for the information-centric measures of ideal summaries. Distilling from the improved teacher, we arrive at a compact but powerful summarizer with only 568M parameters that performs competitively against ChatGPT, without ever relying on ChatGPT's capabilities. Extensive analysis demonstrates that our approach outperforms in-domain supervised models in human evaluation, let alone state-of-the-art unsupervised methods, and wins over ChatGPT in controllable summarization.","sentences":["The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models.","While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer.","We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references.","To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary.","Based on this formulation, we start off from Pythia-2.8B as the teacher model, which is not yet capable of summarization, then self-train the model to optimize for the information-centric measures of ideal summaries.","Distilling from the improved teacher, we arrive at a compact but powerful summarizer with only 568M parameters that performs competitively against ChatGPT, without ever relying on ChatGPT's capabilities.","Extensive analysis demonstrates that our approach outperforms in-domain supervised models in human evaluation, let alone state-of-the-art unsupervised methods, and wins over ChatGPT in controllable summarization."],"url":"http://arxiv.org/abs/2403.13780v1","category":"cs.CL"}
{"created":"2024-03-20 17:28:17","title":"Towards Principled Representation Learning from Videos for Reinforcement Learning","abstract":"We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling. We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise. We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data. This partially explains why reinforcement learning with video pre-training is hard. We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings.","sentences":["We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing.","Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent.","We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data.","We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background.","We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling.","We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise.","We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity.","When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data.","This partially explains why reinforcement learning with video pre-training is hard.","We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings."],"url":"http://arxiv.org/abs/2403.13765v1","category":"cs.LG"}
{"created":"2024-03-20 17:03:38","title":"Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model","abstract":"We present a knowledge augmentation strategy for assessing the diagnostic groups and gait impairment from monocular gait videos. Based on a large-scale pre-trained Vision Language Model (VLM), our model learns and improves visual, textual, and numerical representations of patient gait videos, through a collective learning across three distinct modalities: gait videos, class-specific descriptions, and numerical gait parameters. Our specific contributions are two-fold: First, we adopt a knowledge-aware prompt tuning strategy to utilize the class-specific medical description in guiding the text prompt learning. Second, we integrate the paired gait parameters in the form of numerical texts to enhance the numeracy of the textual representation. Results demonstrate that our model not only significantly outperforms state-of-the-art (SOTA) in video-based classification tasks but also adeptly decodes the learned class-specific text features into natural language descriptions using the vocabulary of quantitative gait parameters. The code and the model will be made available at our project page.","sentences":["We present a knowledge augmentation strategy for assessing the diagnostic groups and gait impairment from monocular gait videos.","Based on a large-scale pre-trained Vision Language Model (VLM), our model learns and improves visual, textual, and numerical representations of patient gait videos, through a collective learning across three distinct modalities: gait videos, class-specific descriptions, and numerical gait parameters.","Our specific contributions are two-fold: First, we adopt a knowledge-aware prompt tuning strategy to utilize the class-specific medical description in guiding the text prompt learning.","Second, we integrate the paired gait parameters in the form of numerical texts to enhance the numeracy of the textual representation.","Results demonstrate that our model not only significantly outperforms state-of-the-art (SOTA) in video-based classification tasks but also adeptly decodes the learned class-specific text features into natural language descriptions using the vocabulary of quantitative gait parameters.","The code and the model will be made available at our project page."],"url":"http://arxiv.org/abs/2403.13756v1","category":"cs.CV"}
{"created":"2024-03-20 16:50:36","title":"Quantum-Secure Certificate-Less Conditional Privacy-Preserving Authentication for VANET","abstract":"Vehicular Ad-hoc Networks (VANETs) marked a pronounced change in the Intelligent Transport System and Smart Cities through seamless vehicle communication to intensify safety and efficacy. However, a few authentication schemes have been devised in the literature to ensure the authenticity of the source and information in the post-quantum era. The most popular base for such construction is lattice-based cryptography. However, existing lattice-based authentication schemes fall short of addressing the potential challenges of the leakage of the master secret key and key-escrow problem. By ingeniously addressing both issues, the paper proposes the \\emph{first} quantum secure authentication scheme to eliminate the flaws while maintaining the system's overall efficiency intact. Compared to the state-of-the-art schemes, the provable security and overall performance assessment highlight the suitability of the proposed approach.","sentences":["Vehicular Ad-hoc Networks (VANETs) marked a pronounced change in the Intelligent Transport System and Smart Cities through seamless vehicle communication to intensify safety and efficacy.","However, a few authentication schemes have been devised in the literature to ensure the authenticity of the source and information in the post-quantum era.","The most popular base for such construction is lattice-based cryptography.","However, existing lattice-based authentication schemes fall short of addressing the potential challenges of the leakage of the master secret key and key-escrow problem.","By ingeniously addressing both issues, the paper proposes the \\emph{first} quantum secure authentication scheme to eliminate the flaws while maintaining the system's overall efficiency intact.","Compared to the state-of-the-art schemes, the provable security and overall performance assessment highlight the suitability of the proposed approach."],"url":"http://arxiv.org/abs/2403.13743v1","category":"cs.CR"}
{"created":"2024-03-20 16:47:53","title":"Hyper Strategy Logic","abstract":"Strategy logic (SL) is a powerful temporal logic that enables strategic reasoning in multi-agent systems. SL supports explicit (first-order) quantification over strategies and provides a logical framework to express many important properties such as Nash equilibria, dominant strategies, etc. While in SL the same strategy can be used in multiple strategy profiles, each such profile is evaluated w.r.t. a path-property, i.e., a property that considers the single path resulting from a particular strategic interaction. In this paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the outcome of multiple strategy profiles can be compared w.r.t. a hyperproperty, i.e., a property that relates multiple paths. We show that HyperSL can capture important properties that cannot be expressed in SL, including non-interference, quantitative Nash equilibria, optimal adversarial planning, and reasoning under imperfect information. On the algorithmic side, we identify an expressive fragment of HyperSL with decidable model checking and present a model-checking algorithm. We contribute a prototype implementation of our algorithm and report on encouraging experimental results.","sentences":["Strategy logic (SL) is a powerful temporal logic that enables strategic reasoning in multi-agent systems.","SL supports explicit (first-order) quantification over strategies and provides a logical framework to express many important properties such as Nash equilibria, dominant strategies, etc.","While in SL the same strategy can be used in multiple strategy profiles, each such profile is evaluated w.r.t.","a path-property, i.e., a property that considers the single path resulting from a particular strategic interaction.","In this paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the outcome of multiple strategy profiles can be compared w.r.t.","a hyperproperty, i.e., a property that relates multiple paths.","We show that HyperSL can capture important properties that cannot be expressed in SL, including non-interference, quantitative Nash equilibria, optimal adversarial planning, and reasoning under imperfect information.","On the algorithmic side, we identify an expressive fragment of HyperSL with decidable model checking and present a model-checking algorithm.","We contribute a prototype implementation of our algorithm and report on encouraging experimental results."],"url":"http://arxiv.org/abs/2403.13741v1","category":"cs.MA"}
{"created":"2024-03-20 16:39:17","title":"Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study","abstract":"In a recent study, Reinforcement Learning (RL) used in combination with many-objective search, has been shown to outperform alternative techniques (random search and many-objective search) for online testing of Deep Neural Network-enabled systems. The empirical evaluation of these techniques was conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a replication and extension of that empirical study. Our replication shows that RL does not outperform pure random test generation in a comparison conducted under the same settings of the original study, but with no confounding factor coming from the way collisions are measured. Our extension aims at eliminating some of the possible reasons for the poor performance of RL observed in our replication: (1) the presence of reward components providing contrasting or useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning) which requires discretization of an intrinsically continuous state space. Results show that our new RL agent is able to converge to an effective policy that outperforms random testing. Results also highlight other possible improvements, which open to further investigations on how to best leverage RL for online ADS testing.","sentences":["In a recent study, Reinforcement Learning (RL) used in combination with many-objective search, has been shown to outperform alternative techniques (random search and many-objective search) for online testing of Deep Neural Network-enabled systems.","The empirical evaluation of these techniques was conducted on a state-of-the-art Autonomous Driving System (ADS).","This work is a replication and extension of that empirical study.","Our replication shows that RL does not outperform pure random test generation in a comparison conducted under the same settings of the original study, but with no confounding factor coming from the way collisions are measured.","Our extension aims at eliminating some of the possible reasons for the poor performance of RL observed in our replication: (1) the presence of reward components providing contrasting or useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning) which requires discretization of an intrinsically continuous state space.","Results show that our new RL agent is able to converge to an effective policy that outperforms random testing.","Results also highlight other possible improvements, which open to further investigations on how to best leverage RL for online ADS testing."],"url":"http://arxiv.org/abs/2403.13729v1","category":"cs.SE"}
{"created":"2024-03-20 16:38:26","title":"M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling","abstract":"When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem. To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term. The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems. The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method is multiplier-free and operates at the timescale of epochs, thus saves tremendous computational resources compared to full training cycle multiplier tuning. We applied it to domain invariant variational auto-encoding with 6 loss terms on the PACS domain generalization task, and observed robust performance across a range of controller hyperparameters, as well as different multiplier initial conditions, outperforming other multiplier scheduling methods. We offered modular implementation of our method, admitting custom definition of many loss terms for applying our multi-objective hierarchical output feedback training scheme to other deep learning fields.","sentences":["When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem.","To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term.","The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems.","The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term.","Our method is multiplier-free and operates at the timescale of epochs, thus saves tremendous computational resources compared to full training cycle multiplier tuning.","We applied it to domain invariant variational auto-encoding with 6 loss terms on the PACS domain generalization task, and observed robust performance across a range of controller hyperparameters, as well as different multiplier initial conditions, outperforming other multiplier scheduling methods.","We offered modular implementation of our method, admitting custom definition of many loss terms for applying our multi-objective hierarchical output feedback training scheme to other deep learning fields."],"url":"http://arxiv.org/abs/2403.13728v1","category":"cs.LG"}
{"created":"2024-03-20 16:29:52","title":"Large Language Models meet Network Slicing Management and Orchestration","abstract":"Network slicing, a cornerstone technology for future networks, enables the creation of customized virtual networks on a shared physical infrastructure. This fosters innovation and agility by providing dedicated resources tailored to specific applications. However, current orchestration and management approaches face limitations in handling the complexity of new service demands within multi-administrative domain environments. This paper proposes a future vision for network slicing powered by Large Language Models (LLMs) and multi-agent systems, offering a framework that can be integrated with existing Management and Orchestration (MANO) frameworks. This framework leverages LLMs to translate user intent into technical requirements, map network functions to infrastructure, and manage the entire slice lifecycle, while multi-agent systems facilitate collaboration across different administrative domains. We also discuss the challenges associated with implementing this framework and potential solutions to mitigate them.","sentences":["Network slicing, a cornerstone technology for future networks, enables the creation of customized virtual networks on a shared physical infrastructure.","This fosters innovation and agility by providing dedicated resources tailored to specific applications.","However, current orchestration and management approaches face limitations in handling the complexity of new service demands within multi-administrative domain environments.","This paper proposes a future vision for network slicing powered by Large Language Models (LLMs) and multi-agent systems, offering a framework that can be integrated with existing Management and Orchestration (MANO) frameworks.","This framework leverages LLMs to translate user intent into technical requirements, map network functions to infrastructure, and manage the entire slice lifecycle, while multi-agent systems facilitate collaboration across different administrative domains.","We also discuss the challenges associated with implementing this framework and potential solutions to mitigate them."],"url":"http://arxiv.org/abs/2403.13721v1","category":"cs.NI"}
{"created":"2024-03-20 16:22:54","title":"Hyperradiance, photon blockade and concurrence in a pair of qubits inside a driven cavity","abstract":"We theoretically study the radiance properties of a pair of qubits inside a single-mode cavity driven by a two-photon drive. Our results show that, when the two qubits are strongly coupled to the cavity field, the collective radiation emitted from the qubits exhibits hyperradiance which can be detected as a signature of two qubit entanglement in the weak-driving regime. We quantify the entanglement in terms of concurrence. Additionally, we study the radiance behaviour in the presence of an intracavity Kerr-nonlinear medium that leads to two-photon blockade. Our results suggest that this system with nonlinearity may act as a quadrature-squeezed and hyperradiant two-photon source.","sentences":["We theoretically study the radiance properties of a pair of qubits inside a single-mode cavity driven by a two-photon drive.","Our results show that, when the two qubits are strongly coupled to the cavity field, the collective radiation emitted from the qubits exhibits hyperradiance which can be detected as a signature of two qubit entanglement in the weak-driving regime.","We quantify the entanglement in terms of concurrence.","Additionally, we study the radiance behaviour in the presence of an intracavity Kerr-nonlinear medium that leads to two-photon blockade.","Our results suggest that this system with nonlinearity may act as a quadrature-squeezed and hyperradiant two-photon source."],"url":"http://arxiv.org/abs/2403.13717v2","category":"physics.atom-ph"}
{"created":"2024-03-20 16:20:54","title":"DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with Multiple Sensors for Large-Scale Localization and Mapping","abstract":"Visual simultaneous localization and mapping (VSLAM) has broad applications, with state-of-the-art methods leveraging deep neural networks for better robustness and applicability. However, there is a lack of research in fusing these learning-based methods with multi-sensor information, which could be indispensable to push related applications to large-scale and complex scenarios. In this paper, we tightly integrate the trainable deep dense bundle adjustment (DBA) with multi-sensor information through a factor graph. In the framework, recurrent optical flow and DBA are performed among sequential images. The Hessian information derived from DBA is fed into a generic factor graph for multi-sensor fusion, which employs a sliding window and supports probabilistic marginalization. A pipeline for visual-inertial integration is firstly developed, which provides the minimum ability of metric-scale localization and mapping. Furthermore, other sensors (e.g., global navigation satellite system) are integrated for driftless and geo-referencing functionality. Extensive tests are conducted on both public datasets and self-collected datasets. The results validate the superior localization performance of our approach, which enables real-time dense mapping in large-scale environments. The code has been made open-source (https://github.com/GREAT-WHU/DBA-Fusion).","sentences":["Visual simultaneous localization and mapping (VSLAM) has broad applications, with state-of-the-art methods leveraging deep neural networks for better robustness and applicability.","However, there is a lack of research in fusing these learning-based methods with multi-sensor information, which could be indispensable to push related applications to large-scale and complex scenarios.","In this paper, we tightly integrate the trainable deep dense bundle adjustment (DBA) with multi-sensor information through a factor graph.","In the framework, recurrent optical flow and DBA are performed among sequential images.","The Hessian information derived from DBA is fed into a generic factor graph for multi-sensor fusion, which employs a sliding window and supports probabilistic marginalization.","A pipeline for visual-inertial integration is firstly developed, which provides the minimum ability of metric-scale localization and mapping.","Furthermore, other sensors (e.g., global navigation satellite system) are integrated for driftless and geo-referencing functionality.","Extensive tests are conducted on both public datasets and self-collected datasets.","The results validate the superior localization performance of our approach, which enables real-time dense mapping in large-scale environments.","The code has been made open-source (https://github.com/GREAT-WHU/DBA-Fusion)."],"url":"http://arxiv.org/abs/2403.13714v1","category":"cs.RO"}
{"created":"2024-03-20 16:08:57","title":"Research Re: search & Re-search","abstract":"Search algorithms are often categorized by their node expansion strategy. One option is the depth-first strategy, a simple backtracking strategy that traverses the search space in the order in which successor nodes are generated. An alternative is the best-first strategy, which was designed to make it possible to use domain-specific heuristic information. By exploring promising parts of the search space first, best-first algorithms are usually more efficient than depth-first algorithms.   In programs that play minimax games such as chess and checkers, the efficiency of the search is of crucial importance. Given the success of best-first algorithms in other domains, one would expect them to be used for minimax games too. However, all high-performance game-playing programs are based on a depth-first algorithm.   This study takes a closer look at a depth-first algorithm, AB, and a best-first algorithm, SSS. The prevailing opinion on these algorithms is that SSS offers the potential for a more efficient search, but that its complicated formulation and exponential memory requirements render it impractical. The theoretical part of this work shows that there is a surprisingly straightforward link between the two algorithms -- for all practical purposes, SSS is a special case of AB. Subsequent empirical evidence proves the prevailing opinion on SSS to be wrong: it is not a complicated algorithm, it does not need too much memory, and it is also not more efficient than depth-first search.","sentences":["Search algorithms are often categorized by their node expansion strategy.","One option is the depth-first strategy, a simple backtracking strategy that traverses the search space in the order in which successor nodes are generated.","An alternative is the best-first strategy, which was designed to make it possible to use domain-specific heuristic information.","By exploring promising parts of the search space first, best-first algorithms are usually more efficient than depth-first algorithms.   ","In programs that play minimax games such as chess and checkers, the efficiency of the search is of crucial importance.","Given the success of best-first algorithms in other domains, one would expect them to be used for minimax games too.","However, all high-performance game-playing programs are based on a depth-first algorithm.   ","This study takes a closer look at a depth-first algorithm, AB, and a best-first algorithm, SSS.","The prevailing opinion on these algorithms is that SSS offers the potential for a more efficient search, but that its complicated formulation and exponential memory requirements render it impractical.","The theoretical part of this work shows that there is a surprisingly straightforward link between the two algorithms -- for all practical purposes, SSS is a special case of AB.","Subsequent empirical evidence proves the prevailing opinion on SSS to be wrong: it is not a complicated algorithm, it does not need too much memory, and it is also not more efficient than depth-first search."],"url":"http://arxiv.org/abs/2403.13705v1","category":"cs.AI"}
{"created":"2024-03-20 16:07:04","title":"Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization","abstract":"Transmission line detection technology is crucial for automatic monitoring and ensuring the safety of electrical facilities. The YOLOv5 series is currently one of the most advanced and widely used methods for object detection. However, it faces inherent challenges, such as high computational load on devices and insufficient detection accuracy. To address these concerns, this paper presents an enhanced lightweight YOLOv5 technique customized for mobile devices, specifically intended for identifying objects associated with transmission lines. The C3Ghost module is integrated into the convolutional network of YOLOv5 to reduce floating point operations per second (FLOPs) in the feature channel fusion process and improve feature expression performance. In addition, a FasterNet module is introduced to replace the c3 module in the YOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only a portion of the input channels, improving feature extraction efficiency and reducing computational overhead. To address the imbalance between simple and challenging samples in the dataset and the diversity of aspect ratios of bounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate the performance of the proposed approach, Experiments are conducted on a custom dataset of transmission line poles. The results show that the proposed model achieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a 26% decrease in model parameters compared to the existing YOLOv5.In the ablation experiment, it was also discovered that while the Fastnet module and the CSghost module improved the precision of the original YOLOv5 baseline model, they caused a decrease in the mAP@.5-.95 metric. However, the improvement of the wIoUv3 loss function significantly mitigated the decline of the mAP@.5-.95 metric.","sentences":["Transmission line detection technology is crucial for automatic monitoring and ensuring the safety of electrical facilities.","The YOLOv5 series is currently one of the most advanced and widely used methods for object detection.","However, it faces inherent challenges, such as high computational load on devices and insufficient detection accuracy.","To address these concerns, this paper presents an enhanced lightweight YOLOv5 technique customized for mobile devices, specifically intended for identifying objects associated with transmission lines.","The C3Ghost module is integrated into the convolutional network of YOLOv5 to reduce floating point operations per second (FLOPs) in the feature channel fusion process and improve feature expression performance.","In addition, a FasterNet module is introduced to replace the c3 module in the YOLOv5 Backbone.","The FasterNet module uses Partial Convolutions to process only a portion of the input channels, improving feature extraction efficiency and reducing computational overhead.","To address the imbalance between simple and challenging samples in the dataset and the diversity of aspect ratios of bounding boxes, the wIoU v3 LOSS is adopted as the loss function.","To validate the performance of the proposed approach, Experiments are conducted on a custom dataset of transmission line poles.","The results show that the proposed model achieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a 26% decrease in model parameters compared to the existing YOLOv5.In the ablation experiment, it was also discovered that while the Fastnet module and the CSghost module improved the precision of the original YOLOv5 baseline model, they caused a decrease in the mAP@.5-.95 metric.","However, the improvement of the wIoUv3 loss function significantly mitigated the decline of the mAP@.5-.95 metric."],"url":"http://arxiv.org/abs/2403.13703v1","category":"cs.CV"}
{"created":"2024-03-20 15:57:44","title":"Loss Regularizing Robotic Terrain Classification","abstract":"Locomotion mechanics of legged robots are suitable when pacing through difficult terrains. Recognising terrains for such robots are important to fully yoke the versatility of their movements. Consequently, robotic terrain classification becomes significant to classify terrains in real time with high accuracy. The conventional classifiers suffer from overfitting problem, low accuracy problem, high variance problem, and not suitable for live dataset. On the other hand, classifying a growing dataset is difficult for convolution based terrain classification. Supervised recurrent models are also not practical for this classification. Further, the existing recurrent architectures are still evolving to improve accuracy of terrain classification based on live variable-length sensory data collected from legged robots. This paper proposes a new semi-supervised method for terrain classification of legged robots, avoiding preprocessing of long variable-length dataset. The proposed method has a stacked Long Short-Term Memory architecture, including a new loss regularization. The proposed method solves the existing problems and improves accuracy. Comparison with the existing architectures show the improvements.","sentences":["Locomotion mechanics of legged robots are suitable when pacing through difficult terrains.","Recognising terrains for such robots are important to fully yoke the versatility of their movements.","Consequently, robotic terrain classification becomes significant to classify terrains in real time with high accuracy.","The conventional classifiers suffer from overfitting problem, low accuracy problem, high variance problem, and not suitable for live dataset.","On the other hand, classifying a growing dataset is difficult for convolution based terrain classification.","Supervised recurrent models are also not practical for this classification.","Further, the existing recurrent architectures are still evolving to improve accuracy of terrain classification based on live variable-length sensory data collected from legged robots.","This paper proposes a new semi-supervised method for terrain classification of legged robots, avoiding preprocessing of long variable-length dataset.","The proposed method has a stacked Long Short-Term Memory architecture, including a new loss regularization.","The proposed method solves the existing problems and improves accuracy.","Comparison with the existing architectures show the improvements."],"url":"http://arxiv.org/abs/2403.13695v1","category":"cs.RO"}
{"created":"2024-03-20 15:53:07","title":"MotorEase: Automated Detection of Motor Impairment Accessibility Issues in Mobile App UIs","abstract":"Recent research has begun to examine the potential of automatically finding and fixing accessibility issues that manifest in software. However, while recent work makes important progress, it has generally been skewed toward identifying issues that affect users with certain disabilities, such as those with visual or hearing impairments. However, there are other groups of users with different types of disabilities that also need software tooling support to improve their experience. As such, this paper aims to automatically identify accessibility issues that affect users with motor-impairments.   To move toward this goal, this paper introduces a novel approach, called MotorEase, capable of identifying accessibility issues in mobile app UIs that impact motor-impaired users. Motor-impaired users often have limited ability to interact with touch-based devices, and instead may make use of a switch or other assistive mechanism -- hence UIs must be designed to support both limited touch gestures and the use of assistive devices. MotorEase adapts computer vision and text processing techniques to enable a semantic understanding of app UI screens, enabling the detection of violations related to four popular, previously unexplored UI design guidelines that support motor-impaired users, including: (i) visual touch target size, (ii) expanding sections, (iii) persisting elements, and (iv) adjacent icon visual distance. We evaluate MotorEase on a newly derived benchmark, called MotorCheck, that contains 555 manually annotated examples of violations to the above accessibility guidelines, across 1599 screens collected from 70 applications via a mobile app testing tool. Our experiments illustrate that MotorEase is able to identify violations with an average accuracy of ~90%, and a false positive rate of less than 9%, outperforming baseline techniques.","sentences":["Recent research has begun to examine the potential of automatically finding and fixing accessibility issues that manifest in software.","However, while recent work makes important progress, it has generally been skewed toward identifying issues that affect users with certain disabilities, such as those with visual or hearing impairments.","However, there are other groups of users with different types of disabilities that also need software tooling support to improve their experience.","As such, this paper aims to automatically identify accessibility issues that affect users with motor-impairments.   ","To move toward this goal, this paper introduces a novel approach, called MotorEase, capable of identifying accessibility issues in mobile app UIs that impact motor-impaired users.","Motor-impaired users often have limited ability to interact with touch-based devices, and instead may make use of a switch or other assistive mechanism -- hence UIs must be designed to support both limited touch gestures and the use of assistive devices.","MotorEase adapts computer vision and text processing techniques to enable a semantic understanding of app UI screens, enabling the detection of violations related to four popular, previously unexplored UI design guidelines that support motor-impaired users, including: (i) visual touch target size, (ii) expanding sections, (iii) persisting elements, and (iv) adjacent icon visual distance.","We evaluate MotorEase on a newly derived benchmark, called MotorCheck, that contains 555 manually annotated examples of violations to the above accessibility guidelines, across 1599 screens collected from 70 applications via a mobile app testing tool.","Our experiments illustrate that MotorEase is able to identify violations with an average accuracy of ~90%, and a false positive rate of less than 9%, outperforming baseline techniques."],"url":"http://arxiv.org/abs/2403.13690v1","category":"cs.SE"}
{"created":"2024-03-20 15:41:39","title":"SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning","abstract":"Generalized Category Discovery (GCD) aims to classify unlabelled images from both `seen' and `unseen' classes by transferring knowledge from a set of labelled `seen' class images. A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task. An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model. As such, in this paper, we introduce a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., prompt learning). Furthermore, we propose a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes. We thoroughly evaluate our SPTNet on standard benchmarks and demonstrate that our method outperforms existing GCD methods. Notably, we find our method achieves an average accuracy of 61.4% on the SSB, surpassing prior state-of-the-art methods by approximately 10%. The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117% of those in the backbone architecture. Project page: https://visual-ai.github.io/sptnet.","sentences":["Generalized Category Discovery (GCD) aims to classify unlabelled images from both `seen' and `unseen' classes by transferring knowledge from a set of labelled `seen' class images.","A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task.","An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model.","As such, in this paper, we introduce a two-stage adaptation approach termed SPTNet, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., prompt learning).","Furthermore, we propose a novel spatial prompt tuning method (SPT) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes.","We thoroughly evaluate our SPTNet on standard benchmarks and demonstrate that our method outperforms existing GCD methods.","Notably, we find our method achieves an average accuracy of 61.4% on the SSB, surpassing prior state-of-the-art methods by approximately 10%.","The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117% of those in the backbone architecture.","Project page: https://visual-ai.github.io/sptnet."],"url":"http://arxiv.org/abs/2403.13684v1","category":"cs.CV"}
{"created":"2024-03-20 15:40:18","title":"Threats, Attacks, and Defenses in Machine Unlearning: A Survey","abstract":"Recently, Machine Unlearning (MU) has gained considerable attention for its potential to improve AI safety by removing the influence of specific data from trained Machine Learning (ML) models. This process, known as knowledge removal, addresses concerns about data such as sensitivity, copyright restrictions, obsolescence, or low quality. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten (RTBF). Therefore, strategic knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the ethical use and reliability of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service (MLaaS), allowing users to submit requests to erase data. However, recent research highlights vulnerabilities in machine unlearning systems, such as information leakage and malicious unlearning requests, that can lead to significant security and privacy concerns. Moreover, extensive research indicates that unlearning methods and prevalent attacks fulfill diverse roles within MU systems. For instance, unlearning can act as a mechanism to recover models from backdoor attacks, while backdoor attacks themselves can serve as an evaluation metric for unlearning effectiveness. This underscores the intricate relationship and complex interplay between these elements in maintaining system functionality and safety. Therefore, this survey seeks to bridge the gap between the extensive number of studies on threats, attacks, and defenses in machine unlearning and the absence of a comprehensive review that categorizes their taxonomy, methods, and solutions, thus offering valuable insights for future research directions and practical implementations.","sentences":["Recently, Machine Unlearning (MU) has gained considerable attention for its potential to improve AI safety by removing the influence of specific data from trained Machine Learning (ML) models.","This process, known as knowledge removal, addresses concerns about data such as sensitivity, copyright restrictions, obsolescence, or low quality.","This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten (RTBF).","Therefore, strategic knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the ethical use and reliability of AI systems.","Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service (MLaaS), allowing users to submit requests to erase data.","However, recent research highlights vulnerabilities in machine unlearning systems, such as information leakage and malicious unlearning requests, that can lead to significant security and privacy concerns.","Moreover, extensive research indicates that unlearning methods and prevalent attacks fulfill diverse roles within MU systems.","For instance, unlearning can act as a mechanism to recover models from backdoor attacks, while backdoor attacks themselves can serve as an evaluation metric for unlearning effectiveness.","This underscores the intricate relationship and complex interplay between these elements in maintaining system functionality and safety.","Therefore, this survey seeks to bridge the gap between the extensive number of studies on threats, attacks, and defenses in machine unlearning and the absence of a comprehensive review that categorizes their taxonomy, methods, and solutions, thus offering valuable insights for future research directions and practical implementations."],"url":"http://arxiv.org/abs/2403.13682v1","category":"cs.CR"}
{"created":"2024-03-20 15:39:54","title":"PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents","abstract":"In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, various legal contracts, and legal documents, were able to learn the domain knowledge required for drafting various legal contracts and legal clauses, and generalize to draft legal contracts and legal clauses with limited instruction tuning. Hence, we conclude that for a strong domain-specialized generative language model (such as legal), very large amounts of data are not required to develop models from scratch. We believe that this work is the first attempt to make a dedicated generative legal language model from scratch for Indian Supreme Court jurisdiction or in legal NLP overall. We plan to release our Paramanu-Ayn model at https://www.bharatgpts.com.","sentences":["In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code.","The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192.","We evaluated our pretrained legal model on perplexity metrics.","We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc.","We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10.","Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed.","We found that our models, despite not being pretrained on legal books, various legal contracts, and legal documents, were able to learn the domain knowledge required for drafting various legal contracts and legal clauses, and generalize to draft legal contracts and legal clauses with limited instruction tuning.","Hence, we conclude that for a strong domain-specialized generative language model (such as legal), very large amounts of data are not required to develop models from scratch.","We believe that this work is the first attempt to make a dedicated generative legal language model from scratch for Indian Supreme Court jurisdiction or in legal NLP overall.","We plan to release our Paramanu-Ayn model at https://www.bharatgpts.com."],"url":"http://arxiv.org/abs/2403.13681v1","category":"cs.CL"}
{"created":"2024-03-20 15:38:36","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","abstract":"Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group. Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.","sentences":["Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors.","While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence.","In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions.","The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances.","We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs.","We find that agents excelling in individual level does not imply their proficiency in group level.","Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group.","Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents.","The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract."],"url":"http://arxiv.org/abs/2403.13679v2","category":"cs.CL"}
{"created":"2024-03-20 14:58:40","title":"Learning User Embeddings from Human Gaze for Personalised Saliency Prediction","abstract":"Reusable embeddings of user behaviour have shown significant performance improvements for the personalised saliency prediction task. However, prior works require explicit user characteristics and preferences as input, which are often difficult to obtain. We present a novel method to extract user embeddings from pairs of natural images and corresponding saliency maps generated from a small amount of user-specific eye tracking data. At the core of our method is a Siamese convolutional neural encoder that learns the user embeddings by contrasting the image and personal saliency map pairs of different users. Evaluations on two public saliency datasets show that the generated embeddings have high discriminative power, are effective at refining universal saliency maps to the individual users, and generalise well across users and images. Finally, based on our model's ability to encode individual user characteristics, our work points towards other applications that can benefit from reusable embeddings of gaze behaviour.","sentences":["Reusable embeddings of user behaviour have shown significant performance improvements for the personalised saliency prediction task.","However, prior works require explicit user characteristics and preferences as input, which are often difficult to obtain.","We present a novel method to extract user embeddings from pairs of natural images and corresponding saliency maps generated from a small amount of user-specific eye tracking data.","At the core of our method is a Siamese convolutional neural encoder that learns the user embeddings by contrasting the image and personal saliency map pairs of different users.","Evaluations on two public saliency datasets show that the generated embeddings have high discriminative power, are effective at refining universal saliency maps to the individual users, and generalise well across users and images.","Finally, based on our model's ability to encode individual user characteristics, our work points towards other applications that can benefit from reusable embeddings of gaze behaviour."],"url":"http://arxiv.org/abs/2403.13653v1","category":"cs.CV"}
{"created":"2024-03-20 14:43:23","title":"Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation","abstract":"Traffic signal control is important in intelligent transportation system, of which cooperative control is difficult to realize but yet vital. Many methods model multi-intersection traffic networks as grids and address the problem using multi-agent reinforcement learning (RL). Despite these existing studies, there is an opportunity to further enhance our understanding of the connectivity and globality of the traffic networks by capturing the spatiotemporal traffic information with efficient neural networks in deep RL. In this paper, we propose a novel multi-agent actor-critic framework based on an interpretable influence mechanism with a centralized learning and decentralized execution method. Specifically, we first construct an actor-critic framework, for which the piecewise linear neural network (PWLNN), named biased ReLU (BReLU), is used as the function approximator to obtain a more accurate and theoretically grounded approximation. Finally, our proposed framework is validated on two synthetic traffic networks to coordinate signal control between intersections, achieving lower traffic delays across the entire traffic network compared to state-of-the-art (SOTA) performance.","sentences":["Traffic signal control is important in intelligent transportation system, of which cooperative control is difficult to realize but yet vital.","Many methods model multi-intersection traffic networks as grids and address the problem using multi-agent reinforcement learning (RL).","Despite these existing studies, there is an opportunity to further enhance our understanding of the connectivity and globality of the traffic networks by capturing the spatiotemporal traffic information with efficient neural networks in deep RL.","In this paper, we propose a novel multi-agent actor-critic framework based on an interpretable influence mechanism with a centralized learning and decentralized execution method.","Specifically, we first construct an actor-critic framework, for which the piecewise linear neural network (PWLNN), named biased ReLU (BReLU), is used as the function approximator to obtain a more accurate and theoretically grounded approximation.","Finally, our proposed framework is validated on two synthetic traffic networks to coordinate signal control between intersections, achieving lower traffic delays across the entire traffic network compared to state-of-the-art (SOTA) performance."],"url":"http://arxiv.org/abs/2403.13639v1","category":"cs.MA"}
{"created":"2024-03-20 14:41:01","title":"Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese","abstract":"In this paper, we explore the utility of Translationese as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56% poorer on NLU tasks and 1.51% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight TinyLMs pre-trained on clean data to filter synthetic data efficiently which significantly improves the performance of our models. We also find that LMs trained on synthetic data strongly benefit from extended pretraining on a tiny fraction (10%) of clean data. We release the data we collected and created as a part of this work, IndicMonoDoc, the largest collection of monolingual document-level corpora, which we hope will help bridge the gap between English and non-English performance for large language models.","sentences":["In this paper, we explore the utility of Translationese as synthetic data created using machine translation for pre-training language models (LMs).","Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English.","Recently, there has been a growing interest in using synthetic data to address this data scarcity.","We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language.","Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic).","We show that their performance on downstream natural language understanding and generative tasks is only 3.56% poorer on NLU tasks and 1.51% on NLG tasks than LMs pre-trained on clean data.","Further, we propose the use of lightweight TinyLMs pre-trained on clean data to filter synthetic data efficiently which significantly improves the performance of our models.","We also find that LMs trained on synthetic data strongly benefit from extended pretraining on a tiny fraction (10%) of clean data.","We release the data we collected and created as a part of this work, IndicMonoDoc, the largest collection of monolingual document-level corpora, which we hope will help bridge the gap between English and non-English performance for large language models."],"url":"http://arxiv.org/abs/2403.13638v2","category":"cs.CL"}
{"created":"2024-03-20 14:36:23","title":"Spectrum of density and spin density fluctuations of an attractive two-dimensional Fermi gas","abstract":"We leverage unbiased auxiliary-field quantum Monte Carlo methods interfaced with cutting-edge analytic continuation tools to compute from first-principles dynamical correlations for a dilute homogeneous two-dimensional attractive Fermi gas. Our main purpose is to quantitatively study the collective excitations of the system to shed light into fermionic superfluidity in the strongly correlated regime. Despite the fact that we are somewhat limited by finite-size effects, our study pinpoints a clear peak in the spin channel at low momentum, and we discuss the possibility that such a peak can be interpreted as the Higgs mode in the superfluid. Our study is complemented by a systematic comparison with the predictions of generalized random phase approximation, with the dual purpose to directly compare with auxiliary-field quantum Monte Carlo and to quantitatively study finite-size effects.","sentences":["We leverage unbiased auxiliary-field quantum Monte Carlo methods interfaced with cutting-edge analytic continuation tools to compute from first-principles dynamical correlations for a dilute homogeneous two-dimensional attractive Fermi gas.","Our main purpose is to quantitatively study the collective excitations of the system to shed light into fermionic superfluidity in the strongly correlated regime.","Despite the fact that we are somewhat limited by finite-size effects, our study pinpoints a clear peak in the spin channel at low momentum, and we discuss the possibility that such a peak can be interpreted as the Higgs mode in the superfluid.","Our study is complemented by a systematic comparison with the predictions of generalized random phase approximation, with the dual purpose to directly compare with auxiliary-field quantum Monte Carlo and to quantitatively study finite-size effects."],"url":"http://arxiv.org/abs/2403.13636v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-20 14:25:54","title":"Scalable Scalar-on-Image Cortical Surface Regression with a Relaxed-Thresholded Gaussian Process Prior","abstract":"In addressing the challenge of analysing the large-scale Adolescent Brain Cognition Development (ABCD) fMRI dataset, involving over 5,000 subjects and extensive neuroimaging data, we propose a scalable Bayesian scalar-on-image regression model for computational feasibility and efficiency. Our model employs a relaxed-thresholded Gaussian process (RTGP), integrating piecewise-smooth, sparse, and continuous functions capable of both hard- and soft-thresholding. This approach introduces additional flexibility in feature selection in scalar-on-image regression and leads to scalable posterior computation by adopting a variational approximation and utilising the Karhunen-Lo\\`eve expansion for Gaussian processes. This advancement substantially reduces the computational costs in vertex-wise analysis of cortical surface data in large-scale Bayesian spatial models. The model's parameter estimation and prediction accuracy and feature selection performance are validated through extensive simulation studies and an application to the ABCD study. Here, we perform regression analysis correlating intelligence scores with task-based functional MRI data, taking into account confounding factors including age, sex, and parental education level. This validation highlights our model's capability to handle large-scale neuroimaging data while maintaining computational feasibility and accuracy.","sentences":["In addressing the challenge of analysing the large-scale Adolescent Brain Cognition Development (ABCD) fMRI dataset, involving over 5,000 subjects and extensive neuroimaging data, we propose a scalable Bayesian scalar-on-image regression model for computational feasibility and efficiency.","Our model employs a relaxed-thresholded Gaussian process (RTGP), integrating piecewise-smooth, sparse, and continuous functions capable of both hard- and soft-thresholding.","This approach introduces additional flexibility in feature selection in scalar-on-image regression and leads to scalable posterior computation by adopting a variational approximation and utilising the Karhunen-Lo\\`eve expansion for Gaussian processes.","This advancement substantially reduces the computational costs in vertex-wise analysis of cortical surface data in large-scale Bayesian spatial models.","The model's parameter estimation and prediction accuracy and feature selection performance are validated through extensive simulation studies and an application to the ABCD study.","Here, we perform regression analysis correlating intelligence scores with task-based functional MRI data, taking into account confounding factors including age, sex, and parental education level.","This validation highlights our model's capability to handle large-scale neuroimaging data while maintaining computational feasibility and accuracy."],"url":"http://arxiv.org/abs/2403.13628v1","category":"stat.ME"}
{"created":"2024-03-20 14:13:44","title":"Dynamic Resource Allocation for Virtual Machine Migration Optimization using Machine Learning","abstract":"The paragraph is grammatically correct and logically coherent. It discusses the importance of mobile terminal cloud computing migration technology in meeting the demands of evolving computer and cloud computing technologies. It emphasizes the need for efficient data access and storage, as well as the utilization of cloud computing migration technology to prevent additional time delays. The paragraph also highlights the contributions of cloud computing migration technology to expanding cloud computing services. Additionally, it acknowledges the role of virtualization as a fundamental capability of cloud computing while emphasizing that cloud computing and virtualization are not inherently interconnected. Finally, it introduces machine learning-based virtual machine migration optimization and dynamic resource allocation as a critical research direction in cloud computing, citing the limitations of static rules or manual settings in traditional cloud computing environments. Overall, the paragraph effectively communicates the importance of machine learning technology in addressing resource allocation and virtual machine migration challenges in cloud computing.","sentences":["The paragraph is grammatically correct and logically coherent.","It discusses the importance of mobile terminal cloud computing migration technology in meeting the demands of evolving computer and cloud computing technologies.","It emphasizes the need for efficient data access and storage, as well as the utilization of cloud computing migration technology to prevent additional time delays.","The paragraph also highlights the contributions of cloud computing migration technology to expanding cloud computing services.","Additionally, it acknowledges the role of virtualization as a fundamental capability of cloud computing while emphasizing that cloud computing and virtualization are not inherently interconnected.","Finally, it introduces machine learning-based virtual machine migration optimization and dynamic resource allocation as a critical research direction in cloud computing, citing the limitations of static rules or manual settings in traditional cloud computing environments.","Overall, the paragraph effectively communicates the importance of machine learning technology in addressing resource allocation and virtual machine migration challenges in cloud computing."],"url":"http://arxiv.org/abs/2403.13619v1","category":"cs.DC"}
{"created":"2024-03-20 13:44:30","title":"No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)","abstract":"Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not been well addressed today. In this paper, we investigate the query optimization ability of LLM and use LLM to design LaPuda, a novel LLM and Policy based multi-modal query optimizer. Instead of enumerating specific and detailed rules, LaPuda only needs a few abstract policies to guide LLM in the optimization, by which much time and human effort are saved. Furthermore, to prevent LLM from making mistakes or negative optimization, we borrow the idea of gradient descent and propose a guided cost descent (GCD) algorithm to perform the optimization, such that the optimization can be kept in the correct direction. In our evaluation, our methods consistently outperform the baselines in most cases. For example, the optimized plans generated by our methods result in 1~3x higher execution speed than those by the baselines.","sentences":["Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning.","Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries.","However, there is no work on the query optimization capability of LLM.","As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed.","From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation.","Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not been well addressed today.","In this paper, we investigate the query optimization ability of LLM and use LLM to design LaPuda, a novel LLM and Policy based multi-modal query optimizer.","Instead of enumerating specific and detailed rules, LaPuda only needs a few abstract policies to guide LLM in the optimization, by which much time and human effort are saved.","Furthermore, to prevent LLM from making mistakes or negative optimization, we borrow the idea of gradient descent and propose a guided cost descent (GCD) algorithm to perform the optimization, such that the optimization can be kept in the correct direction.","In our evaluation, our methods consistently outperform the baselines in most cases.","For example, the optimized plans generated by our methods result in 1~3x higher execution speed than those by the baselines."],"url":"http://arxiv.org/abs/2403.13597v1","category":"cs.DB"}
{"created":"2024-03-20 13:37:00","title":"Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models","abstract":"As Pre-trained Language Models (PLMs), a popular approach for code intelligence, continue to grow in size, the computational cost of their usage has become prohibitively expensive. Prompt learning, a recent development in the field of natural language processing, emerges as a potential solution to address this challenge. In this paper, we investigate the effectiveness of prompt learning in code intelligence tasks. We unveil its reliance on manually designed prompts, which often require significant human effort and expertise. Moreover, we discover existing automatic prompt design methods are very limited to code intelligence tasks due to factors including gradient dependence, high computational demands, and limited applicability. To effectively address both issues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate genetic algorithm to automatically design prompts. With GenAP, non-experts can effortlessly generate superior prompts compared to meticulously manual-designed ones. GenAP operates without the need for gradients or additional computational costs, rendering it gradient-free and cost-effective. Moreover, GenAP supports both understanding and generation types of code intelligence tasks, exhibiting great applicability. We conduct GenAP on three popular code intelligence PLMs with three canonical code intelligence tasks including defect prediction, code summarization, and code translation. The results suggest that GenAP can effectively automate the process of designing prompts. Specifically, GenAP outperforms all other methods across all three tasks (e.g., improving accuracy by an average of 2.13% for defect prediction). To the best of our knowledge, GenAP is the first work to automatically design prompts for code intelligence PLMs.","sentences":["As Pre-trained Language Models (PLMs), a popular approach for code intelligence, continue to grow in size, the computational cost of their usage has become prohibitively expensive.","Prompt learning, a recent development in the field of natural language processing, emerges as a potential solution to address this challenge.","In this paper, we investigate the effectiveness of prompt learning in code intelligence tasks.","We unveil its reliance on manually designed prompts, which often require significant human effort and expertise.","Moreover, we discover existing automatic prompt design methods are very limited to code intelligence tasks due to factors including gradient dependence, high computational demands, and limited applicability.","To effectively address both issues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate genetic algorithm to automatically design prompts.","With GenAP, non-experts can effortlessly generate superior prompts compared to meticulously manual-designed ones.","GenAP operates without the need for gradients or additional computational costs, rendering it gradient-free and cost-effective.","Moreover, GenAP supports both understanding and generation types of code intelligence tasks, exhibiting great applicability.","We conduct GenAP on three popular code intelligence PLMs with three canonical code intelligence tasks including defect prediction, code summarization, and code translation.","The results suggest that GenAP can effectively automate the process of designing prompts.","Specifically, GenAP outperforms all other methods across all three tasks (e.g., improving accuracy by an average of 2.13% for defect prediction).","To the best of our knowledge, GenAP is the first work to automatically design prompts for code intelligence PLMs."],"url":"http://arxiv.org/abs/2403.13588v1","category":"cs.SE"}
{"created":"2024-03-20 13:14:29","title":"A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation","abstract":"In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience. However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling. In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation. Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender. The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling. Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying user preferences from heterogeneous interaction behaviors. In order to integrate the merits of the SR model and the supplemental LLM recommender, we design a twostage training paradigm. The first stage is personalized preference alignment, which aims to align the preference representations from both components, thereby enhancing the semantics of the SR model. The second stage is recommendation-oriented fine-tuning, in which the alignment-enhanced SR model is fine-tuned according to specific objectives. Extensive experiments in both video and comment recommendation tasks demonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the KuaiShou platform verifies the actual benefits brought by our approach. In particular, we achieve a significant overall gain of 4.13% in comment watch time.","sentences":["In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience.","However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling.","In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation.","Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender.","The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling.","Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying user preferences from heterogeneous interaction behaviors.","In order to integrate the merits of the SR model and the supplemental LLM recommender, we design a twostage training paradigm.","The first stage is personalized preference alignment, which aims to align the preference representations from both components, thereby enhancing the semantics of the SR model.","The second stage is recommendation-oriented fine-tuning, in which the alignment-enhanced SR model is fine-tuned according to specific objectives.","Extensive experiments in both video and comment recommendation tasks demonstrate the effectiveness of LSVCR.","Additionally, online A/B testing on the KuaiShou platform verifies the actual benefits brought by our approach.","In particular, we achieve a significant overall gain of 4.13% in comment watch time."],"url":"http://arxiv.org/abs/2403.13574v1","category":"cs.IR"}
{"created":"2024-03-20 12:51:30","title":"Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments","abstract":"In this work, we tackle the limitations of current LiDAR-based 3D object detection systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new object classes. Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained vision-language models (VLMs) with multi-sensor data. We design and benchmark a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies. While effective, these methods exhibit certain limitations, such as missing novel objects in 3D box estimation or applying rigorous priors, leading to biases towards objects near the camera or of rectangular geometries. To overcome these limitations, we introduce a universal \\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the recall of novel objects and propagating this detection capability to more distant areas thereby progressively capturing more. In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker. Additionally, the inherent bias towards camera-proximal objects is alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank. Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel object classes. The source code is made available in the supplementary material.","sentences":["In this work, we tackle the limitations of current LiDAR-based 3D object detection systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new object classes.","Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained vision-language models (VLMs) with multi-sensor data.","We design and benchmark a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies.","While effective, these methods exhibit certain limitations, such as missing novel objects in 3D box estimation or applying rigorous priors, leading to biases towards objects near the camera or of rectangular geometries.","To overcome these limitations, we introduce a universal \\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the recall of novel objects and propagating this detection capability to more distant areas thereby progressively capturing more.","In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker.","Additionally, the inherent bias towards camera-proximal objects is alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank.","Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors.","Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel object classes.","The source code is made available in the supplementary material."],"url":"http://arxiv.org/abs/2403.13556v1","category":"cs.CV"}
{"created":"2024-03-20 12:46:02","title":"VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model","abstract":"Conversational artificial intelligence can already independently engage in brief conversations with clients with psychological problems and provide evidence-based psychological interventions. The main objective of this study is to improve the effectiveness and credibility of the large language model in psychological intervention by creating a specialized agent, the VCounselor, to address the limitations observed in popular large language models such as ChatGPT in domain applications. We achieved this goal by proposing a new affective interaction structure and knowledge-enhancement structure. In order to evaluate VCounselor, this study compared the general large language model, the fine-tuned large language model, and VCounselor's knowledge-enhanced large language model. At the same time, the general large language model and the fine-tuned large language model will also be provided with an avatar to compare them as an agent with VCounselor. The comparison results indicated that the affective interaction structure and knowledge-enhancement structure of VCounselor significantly improved the effectiveness and credibility of the psychological intervention, and VCounselor significantly provided positive tendencies for clients' emotions. The conclusion of this study strongly supports that VConselor has a significant advantage in providing psychological support to clients by being able to analyze the patient's problems with relative accuracy and provide professional-level advice that enhances support for clients.","sentences":["Conversational artificial intelligence can already independently engage in brief conversations with clients with psychological problems and provide evidence-based psychological interventions.","The main objective of this study is to improve the effectiveness and credibility of the large language model in psychological intervention by creating a specialized agent, the VCounselor, to address the limitations observed in popular large language models such as ChatGPT in domain applications.","We achieved this goal by proposing a new affective interaction structure and knowledge-enhancement structure.","In order to evaluate VCounselor, this study compared the general large language model, the fine-tuned large language model, and VCounselor's knowledge-enhanced large language model.","At the same time, the general large language model and the fine-tuned large language model will also be provided with an avatar to compare them as an agent with VCounselor.","The comparison results indicated that the affective interaction structure and knowledge-enhancement structure of VCounselor significantly improved the effectiveness and credibility of the psychological intervention, and VCounselor significantly provided positive tendencies for clients' emotions.","The conclusion of this study strongly supports that VConselor has a significant advantage in providing psychological support to clients by being able to analyze the patient's problems with relative accuracy and provide professional-level advice that enhances support for clients."],"url":"http://arxiv.org/abs/2403.13553v1","category":"cs.HC"}
{"created":"2024-03-20 12:14:54","title":"What explains the success of cross-modal fine-tuning with ORCA?","abstract":"ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.","sentences":["ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data.","The technique consists primarily of training an embedder and fine-tuning the embedder and model.","Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success.","Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits.","In 1D tasks, some amount of embedder training is necessary but more is not better.","In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference.","Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA."],"url":"http://arxiv.org/abs/2403.13537v1","category":"cs.CL"}
{"created":"2024-03-20 11:51:04","title":"Compress3D: a Compressed Latent Space for 3D Generation from a Single Image","abstract":"3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU.","sentences":["3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging.","In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information.","Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space.","Subsequently, we train a diffusion model on this refined latent space.","In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions.","Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding.","Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time.","Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU."],"url":"http://arxiv.org/abs/2403.13524v1","category":"cs.CV"}
{"created":"2024-03-20 11:50:16","title":"Have You Poisoned My Data? Defending Neural Networks against Data Poisoning","abstract":"The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years. However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal.   This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting. We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution. Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space. We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multiple architectures, datasets, and poison budgets. Our evaluation shows that our proposal outperforms existing approaches in defense rate and final trained model performance across all experimental settings.","sentences":["The unprecedented availability of training data fueled the rapid development of powerful neural networks in recent years.","However, the need for such large amounts of data leads to potential threats such as poisoning attacks: adversarial manipulations of the training data aimed at compromising the learned model to achieve a given adversarial goal.   ","This paper investigates defenses against clean-label poisoning attacks and proposes a novel approach to detect and filter poisoned datapoints in the transfer learning setting.","We define a new characteristic vector representation of datapoints and show that it effectively captures the intrinsic properties of the data distribution.","Through experimental analysis, we demonstrate that effective poisons can be successfully differentiated from clean points in the characteristic vector space.","We thoroughly evaluate our proposed approach and compare it to existing state-of-the-art defenses using multiple architectures, datasets, and poison budgets.","Our evaluation shows that our proposal outperforms existing approaches in defense rate and final trained model performance across all experimental settings."],"url":"http://arxiv.org/abs/2403.13523v1","category":"cs.LG"}
{"created":"2024-03-20 11:38:30","title":"Motion Generation from Fine-grained Textual Descriptions","abstract":"The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements. While most existing works are confined to coarse-grained motion descriptions (e.g., \"A man squats.\"), fine-grained ones specifying movements of relevant body parts are barely explored. Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information. Our experiments show that FineMotionDiffuse trained on FineHumanML3D acquires good results in quantitative evaluation. We also find this model can better generate spatially/chronologically composite motions by learning the implicit mappings from simple descriptions to the corresponding basic motions.","sentences":["The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements.","While most existing works are confined to coarse-grained motion descriptions (e.g., \"A man squats.","\"), fine-grained ones specifying movements of relevant body parts are barely explored.","Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions.","In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts.","Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information.","Our experiments show that FineMotionDiffuse trained on FineHumanML3D acquires good results in quantitative evaluation.","We also find this model can better generate spatially/chronologically composite motions by learning the implicit mappings from simple descriptions to the corresponding basic motions."],"url":"http://arxiv.org/abs/2403.13518v1","category":"cs.AI"}
{"created":"2024-03-20 11:27:20","title":"What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models","abstract":"This paper presents a way of enhancing the reliability of Large Multimodal Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses. Without additional instruction tuning paradigm, we introduce Counterfactual Inception, a novel method that implants counterfactual thoughts into LMMs using carefully chosen, misaligned counterfactual keywords. This method is grounded in the concept of counterfactual thinking, a cognitive process where humans consider alternative realities and outcomes. By applying this human-like reasoning mechanism to LMMs, we aim to reduce hallucination effects and improve the models' trustworthiness. We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal counterfactual keywords to trigger counterfactual thinking into LMMs, concurrently considering visual and linguistic context. Our extensive experiments across various LMMs, including both open-source and proprietary models, corroborate that our method significantly mitigates hallucination phenomena across different datasets.","sentences":["This paper presents a way of enhancing the reliability of Large Multimodal Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses.","Without additional instruction tuning paradigm, we introduce Counterfactual Inception, a novel method that implants counterfactual thoughts into LMMs using carefully chosen, misaligned counterfactual keywords.","This method is grounded in the concept of counterfactual thinking, a cognitive process where humans consider alternative realities and outcomes.","By applying this human-like reasoning mechanism to LMMs, we aim to reduce hallucination effects and improve the models' trustworthiness.","We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal counterfactual keywords to trigger counterfactual thinking into LMMs, concurrently considering visual and linguistic context.","Our extensive experiments across various LMMs, including both open-source and proprietary models, corroborate that our method significantly mitigates hallucination phenomena across different datasets."],"url":"http://arxiv.org/abs/2403.13513v1","category":"cs.CV"}
{"created":"2024-03-20 11:21:22","title":"Scale Decoupled Distillation","abstract":"Logit knowledge distillation attracts increasing attention due to its practicality in recent studies. However, it often suffers inferior performance compared to the feature knowledge distillation. In this paper, we argue that existing logit-based methods may be sub-optimal since they only leverage the global logit output that couples multiple semantic knowledge. This may transfer ambiguous knowledge to the student and mislead its learning. To this end, we propose a simple but effective method, i.e., Scale Decoupled Distillation (SDD), for logit knowledge distillation. SDD decouples the global logit output into multiple local logit outputs and establishes distillation pipelines for them. This helps the student to mine and inherit fine-grained and unambiguous logit knowledge. Moreover, the decoupled knowledge can be further divided into consistent and complementary logit knowledge that transfers the semantic information and sample ambiguity, respectively. By increasing the weight of complementary parts, SDD can guide the student to focus more on ambiguous samples, improving its discrimination ability. Extensive experiments on several benchmark datasets demonstrate the effectiveness of SDD for wide teacher-student pairs, especially in the fine-grained classification task. Code is available at: https://github.com/shicaiwei123/SDD-CVPR2024","sentences":["Logit knowledge distillation attracts increasing attention due to its practicality in recent studies.","However, it often suffers inferior performance compared to the feature knowledge distillation.","In this paper, we argue that existing logit-based methods may be sub-optimal since they only leverage the global logit output that couples multiple semantic knowledge.","This may transfer ambiguous knowledge to the student and mislead its learning.","To this end, we propose a simple but effective method, i.e., Scale Decoupled Distillation (SDD), for logit knowledge distillation.","SDD decouples the global logit output into multiple local logit outputs and establishes distillation pipelines for them.","This helps the student to mine and inherit fine-grained and unambiguous logit knowledge.","Moreover, the decoupled knowledge can be further divided into consistent and complementary logit knowledge that transfers the semantic information and sample ambiguity, respectively.","By increasing the weight of complementary parts, SDD can guide the student to focus more on ambiguous samples, improving its discrimination ability.","Extensive experiments on several benchmark datasets demonstrate the effectiveness of SDD for wide teacher-student pairs, especially in the fine-grained classification task.","Code is available at: https://github.com/shicaiwei123/SDD-CVPR2024"],"url":"http://arxiv.org/abs/2403.13512v1","category":"cs.CV"}
{"created":"2024-03-20 10:58:58","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","abstract":"Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to different visual states of longer videos, and 2) Temporal Attention Regularization (TAR) - a regularization technique to refine the temporal attention units of the pre-trained T2V diffusion models, which enables control over the video dynamics. We experimentally showcase the superiority of the proposed approach in generating longer, visually appealing videos over existing open-sourced T2V models. We additionally analyze the temporal attention maps realized with and without VSTAR, demonstrating the importance of applying our method to mitigate neglect of the desired visual change over time.","sentences":["Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content.","They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt.","At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable.","To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos.","We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to different visual states of longer videos, and 2) Temporal Attention Regularization (TAR) - a regularization technique to refine the temporal attention units of the pre-trained T2V diffusion models, which enables control over the video dynamics.","We experimentally showcase the superiority of the proposed approach in generating longer, visually appealing videos over existing open-sourced T2V models.","We additionally analyze the temporal attention maps realized with and without VSTAR, demonstrating the importance of applying our method to mitigate neglect of the desired visual change over time."],"url":"http://arxiv.org/abs/2403.13501v1","category":"cs.CV"}
{"created":"2024-03-20 10:44:03","title":"The future of generative AI chatbots in higher education","abstract":"The integration of generative Artificial Intelligence (AI) chatbots in higher education institutions (HEIs) is reshaping the educational landscape, offering opportunities for enhanced student support, and administrative and research efficiency. This study explores the future implications of generative AI chatbots in HEIs, aiming to understand their potential impact on teaching and learning, and research processes. Utilizing a narrative literature review (NLR) methodology, this study synthesizes existing research on generative AI chatbots in higher education from diverse sources, including academic databases and scholarly publications. The findings highlight the transformative potential of generative AI chatbots in streamlining administrative tasks, enhancing student learning experiences, and supporting research activities. However, challenges such as academic integrity concerns, user input understanding, and resource allocation pose significant obstacles to the effective integration of generative AI chatbots in HEIs. This study underscores the importance of proactive measures to address ethical considerations, provide comprehensive training for stakeholders, and establish clear guidelines for the responsible use of generative AI chatbots in higher education. By navigating these challenges, and leveraging the benefits of generative AI technologies, HEIs can harness the full potential of generative AI chatbots to create a more efficient, effective, inclusive, and innovative educational environment.","sentences":["The integration of generative Artificial Intelligence (AI) chatbots in higher education institutions (HEIs) is reshaping the educational landscape, offering opportunities for enhanced student support, and administrative and research efficiency.","This study explores the future implications of generative AI chatbots in HEIs, aiming to understand their potential impact on teaching and learning, and research processes.","Utilizing a narrative literature review (NLR) methodology, this study synthesizes existing research on generative AI chatbots in higher education from diverse sources, including academic databases and scholarly publications.","The findings highlight the transformative potential of generative AI chatbots in streamlining administrative tasks, enhancing student learning experiences, and supporting research activities.","However, challenges such as academic integrity concerns, user input understanding, and resource allocation pose significant obstacles to the effective integration of generative AI chatbots in HEIs.","This study underscores the importance of proactive measures to address ethical considerations, provide comprehensive training for stakeholders, and establish clear guidelines for the responsible use of generative AI chatbots in higher education.","By navigating these challenges, and leveraging the benefits of generative AI technologies, HEIs can harness the full potential of generative AI chatbots to create a more efficient, effective, inclusive, and innovative educational environment."],"url":"http://arxiv.org/abs/2403.13487v1","category":"cs.CY"}
{"created":"2024-03-20 10:34:40","title":"A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels","abstract":"Cross-modal retrieval (CMR) aims to establish interaction between different modalities, among which supervised CMR is emerging due to its flexibility in learning semantic category discrimination. Despite the remarkable performance of previous supervised CMR methods, much of their success can be attributed to the well-annotated data. However, even for unimodal data, precise annotation is expensive and time-consuming, and it becomes more challenging with the multimodal scenario. In practice, massive multimodal data are collected from the Internet with coarse annotation, which inevitably introduces noisy labels. Training with such misleading labels would bring two key challenges -- enforcing the multimodal samples to \\emph{align incorrect semantics} and \\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To tackle these challenges, this work proposes UOT-RCL, a Unified framework based on Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a semantic alignment based on partial OT to progressively correct the noisy labels, where a novel cross-modal consistent cost function is designed to blend different modalities and provide precise transport cost. Second, to narrow the discrepancy in multi-modal data, an OT-based relation alignment is proposed to infer the semantic-level cross-modal matching. Both of these two components leverage the inherent correlation among multi-modal data to facilitate effective cost function. The experiments on three widely-used cross-modal retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art approaches and significantly improves the robustness against noisy labels.","sentences":["Cross-modal retrieval (CMR) aims to establish interaction between different modalities, among which supervised CMR is emerging due to its flexibility in learning semantic category discrimination.","Despite the remarkable performance of previous supervised CMR methods, much of their success can be attributed to the well-annotated data.","However, even for unimodal data, precise annotation is expensive and time-consuming, and it becomes more challenging with the multimodal scenario.","In practice, massive multimodal data are collected from the Internet with coarse annotation, which inevitably introduces noisy labels.","Training with such misleading labels would bring two key challenges -- enforcing the multimodal samples to \\emph{align incorrect semantics} and \\emph{widen the heterogeneous gap}, resulting in poor retrieval performance.","To tackle these challenges, this work proposes UOT-RCL, a Unified framework based on Optimal Transport (OT) for Robust Cross-modal Retrieval.","First, we propose a semantic alignment based on partial OT to progressively correct the noisy labels, where a novel cross-modal consistent cost function is designed to blend different modalities and provide precise transport cost.","Second, to narrow the discrepancy in multi-modal data, an OT-based relation alignment is proposed to infer the semantic-level cross-modal matching.","Both of these two components leverage the inherent correlation among multi-modal data to facilitate effective cost function.","The experiments on three widely-used cross-modal retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art approaches and significantly improves the robustness against noisy labels."],"url":"http://arxiv.org/abs/2403.13480v1","category":"cs.CV"}
{"created":"2024-03-20 10:33:10","title":"Deepfake Detection without Deepfakes: Generalization via Synthetic Frequency Patterns Injection","abstract":"Deepfake detectors are typically trained on large sets of pristine and generated images, resulting in limited generalization capacity; they excel at identifying deepfakes created through methods encountered during training but struggle with those generated by unknown techniques. This paper introduces a learning approach aimed at significantly enhancing the generalization capabilities of deepfake detectors. Our method takes inspiration from the unique \"fingerprints\" that image generation processes consistently introduce into the frequency domain. These fingerprints manifest as structured and distinctly recognizable frequency patterns. We propose to train detectors using only pristine images injecting in part of them crafted frequency patterns, simulating the effects of various deepfake generation techniques without being specific to any. These synthetic patterns are based on generic shapes, grids, or auras. We evaluated our approach using diverse architectures across 25 different generation methods. The models trained with our approach were able to perform state-of-the-art deepfake detection, demonstrating also superior generalization capabilities in comparison with previous methods. Indeed, they are untied to any specific generation technique and can effectively identify deepfakes regardless of how they were made.","sentences":["Deepfake detectors are typically trained on large sets of pristine and generated images, resulting in limited generalization capacity; they excel at identifying deepfakes created through methods encountered during training but struggle with those generated by unknown techniques.","This paper introduces a learning approach aimed at significantly enhancing the generalization capabilities of deepfake detectors.","Our method takes inspiration from the unique \"fingerprints\" that image generation processes consistently introduce into the frequency domain.","These fingerprints manifest as structured and distinctly recognizable frequency patterns.","We propose to train detectors using only pristine images injecting in part of them crafted frequency patterns, simulating the effects of various deepfake generation techniques without being specific to any.","These synthetic patterns are based on generic shapes, grids, or auras.","We evaluated our approach using diverse architectures across 25 different generation methods.","The models trained with our approach were able to perform state-of-the-art deepfake detection, demonstrating also superior generalization capabilities in comparison with previous methods.","Indeed, they are untied to any specific generation technique and can effectively identify deepfakes regardless of how they were made."],"url":"http://arxiv.org/abs/2403.13479v1","category":"cs.CV"}
{"created":"2024-03-20 10:19:37","title":"Data-Driven Reduced-Order Unknown-Input Observers","abstract":"In this paper we propose a data-driven approach to the design of reduced-order unknown-input observers (rUIOs). We first recall the model-based solution, by assuming a problem set-up slightly different from those traditionally adopted in the literature, in order to be able to easily adapt it to the data-driven scenario. Necessary and sufficient conditions for the existence of a reduced-order unknown-input observer, whose matrices can be derived from a sufficiently rich set of collected historical data, are first derived and then proved to be equivalent to the ones obtained in the model-based framework. Finally, a numerical example is presented, to validate the effectiveness of the proposed scheme.","sentences":["In this paper we propose a data-driven approach to the design of reduced-order unknown-input observers (rUIOs).","We first recall the model-based solution, by assuming a problem set-up slightly different from those traditionally adopted in the literature, in order to be able to easily adapt it to the data-driven scenario.","Necessary and sufficient conditions for the existence of a reduced-order unknown-input observer, whose matrices can be derived from a sufficiently rich set of collected historical data, are first derived and then proved to be equivalent to the ones obtained in the model-based framework.","Finally, a numerical example is presented, to validate the effectiveness of the proposed scheme."],"url":"http://arxiv.org/abs/2403.13471v1","category":"math.DS"}
{"created":"2024-03-20 10:19:05","title":"Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion","abstract":"Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods. Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data.","sentences":["Computer vision techniques play a central role in the perception stack of autonomous vehicles.","Such methods are employed to perceive the vehicle surroundings given sensor data.","3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene.","However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds.","In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation.","Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan.","Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods.","Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale.","Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process.","Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods.","We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data."],"url":"http://arxiv.org/abs/2403.13470v1","category":"cs.CV"}
{"created":"2024-03-20 10:16:40","title":"An AI-Assisted Skincare Routine Recommendation System in XR","abstract":"In recent years, there has been an increasing interest in the use of artificial intelligence (AI) and extended reality (XR) in the beauty industry. In this paper, we present an AI-assisted skin care recommendation system integrated into an XR platform. The system uses a convolutional neural network (CNN) to analyse an individual's skin type and recommend personalised skin care products in an immersive and interactive manner. Our methodology involves collecting data from individuals through a questionnaire and conducting skin analysis using a provided facial image in an immersive environment. This data is then used to train the CNN model, which recognises the skin type and existing issues and allows the recommendation engine to suggest personalised skin care products. We evaluate our system in terms of the accuracy of the CNN model, which achieves an average score of 93% in correctly classifying existing skin issues. Being integrated into an XR system, this approach has the potential to significantly enhance the beauty industry by providing immersive and engaging experiences to users, leading to more efficient and consistent skincare routines.","sentences":["In recent years, there has been an increasing interest in the use of artificial intelligence (AI) and extended reality (XR) in the beauty industry.","In this paper, we present an AI-assisted skin care recommendation system integrated into an XR platform.","The system uses a convolutional neural network (CNN) to analyse an individual's skin type and recommend personalised skin care products in an immersive and interactive manner.","Our methodology involves collecting data from individuals through a questionnaire and conducting skin analysis using a provided facial image in an immersive environment.","This data is then used to train the CNN model, which recognises the skin type and existing issues and allows the recommendation engine to suggest personalised skin care products.","We evaluate our system in terms of the accuracy of the CNN model, which achieves an average score of 93% in correctly classifying existing skin issues.","Being integrated into an XR system, this approach has the potential to significantly enhance the beauty industry by providing immersive and engaging experiences to users, leading to more efficient and consistent skincare routines."],"url":"http://arxiv.org/abs/2403.13466v1","category":"cs.CV"}
{"created":"2024-03-20 10:07:51","title":"Uncertainty quantification for data-driven weather models","abstract":"Artificial intelligence (AI)-based data-driven weather forecasting models have experienced rapid progress over the last years. Recent studies, with models trained on reanalysis data, achieve impressive results and demonstrate substantial improvements over state-of-the-art physics-based numerical weather prediction models across a range of variables and evaluation metrics. Beyond improved predictions, the main advantages of data-driven weather models are their substantially lower computational costs and the faster generation of forecasts, once a model has been trained. However, most efforts in data-driven weather forecasting have been limited to deterministic, point-valued predictions, making it impossible to quantify forecast uncertainties, which is crucial in research and for optimal decision making in applications. Our overarching aim is to systematically study and compare uncertainty quantification methods to generate probabilistic weather forecasts from a state-of-the-art deterministic data-driven weather model, Pangu-Weather. Specifically, we compare approaches for quantifying forecast uncertainty based on generating ensemble forecasts via perturbations to the initial conditions, with the use of statistical and machine learning methods for post-hoc uncertainty quantification. In a case study on medium-range forecasts of selected weather variables over Europe, the probabilistic forecasts obtained by using the Pangu-Weather model in concert with uncertainty quantification methods show promising results and provide improvements over ensemble forecasts from the physics-based ensemble weather model of the European Centre for Medium-Range Weather Forecasts for lead times of up to 5 days.","sentences":["Artificial intelligence (AI)-based data-driven weather forecasting models have experienced rapid progress over the last years.","Recent studies, with models trained on reanalysis data, achieve impressive results and demonstrate substantial improvements over state-of-the-art physics-based numerical weather prediction models across a range of variables and evaluation metrics.","Beyond improved predictions, the main advantages of data-driven weather models are their substantially lower computational costs and the faster generation of forecasts, once a model has been trained.","However, most efforts in data-driven weather forecasting have been limited to deterministic, point-valued predictions, making it impossible to quantify forecast uncertainties, which is crucial in research and for optimal decision making in applications.","Our overarching aim is to systematically study and compare uncertainty quantification methods to generate probabilistic weather forecasts from a state-of-the-art deterministic data-driven weather model, Pangu-Weather.","Specifically, we compare approaches for quantifying forecast uncertainty based on generating ensemble forecasts via perturbations to the initial conditions, with the use of statistical and machine learning methods for post-hoc uncertainty quantification.","In a case study on medium-range forecasts of selected weather variables over Europe, the probabilistic forecasts obtained by using the Pangu-Weather model in concert with uncertainty quantification methods show promising results and provide improvements over ensemble forecasts from the physics-based ensemble weather model of the European Centre for Medium-Range Weather Forecasts for lead times of up to 5 days."],"url":"http://arxiv.org/abs/2403.13458v1","category":"physics.ao-ph"}
{"created":"2024-03-20 09:42:43","title":"HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models","abstract":"Recent advancements indicate that scaling up Multimodal Large Language Models (MLLMs) effectively enhances performance on downstream multimodal tasks. The prevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into text-like tokens using a \\emph{static} vision-language mapper, thereby enabling \\emph{static} LLMs to develop the capability to comprehend visual information through visual instruction tuning. Although promising, the \\emph{static} tuning strategy~\\footnote{The static tuning refers to the trained model with static parameters.} that shares the same parameters may constrain performance across different downstream multimodal tasks. In light of this, we introduce HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters, in conjunction with a dynamic visual expert and language expert, respectively. These experts are derived from HyperNetworks, which generates adaptive parameter shifts through visual and language guidance, enabling dynamic projector and LLM modeling in two-stage training.   Our experiments demonstrate that our solution significantly surpasses LLaVA on existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and LLaVA-Bench. ~\\footnote{Our project is available on the link https://github.com/DCDmllm/HyperLLaVA}.","sentences":["Recent advancements indicate that scaling up Multimodal Large Language Models (MLLMs) effectively enhances performance on downstream multimodal tasks.","The prevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into text-like tokens using a \\emph{static} vision-language mapper, thereby enabling \\emph{static} LLMs to develop the capability to comprehend visual information through visual instruction tuning.","Although promising, the \\emph{static} tuning strategy~\\footnote{The static tuning refers to the trained model with static parameters.}","that shares the same parameters may constrain performance across different downstream multimodal tasks.","In light of this, we introduce HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters, in conjunction with a dynamic visual expert and language expert, respectively.","These experts are derived from HyperNetworks, which generates adaptive parameter shifts through visual and language guidance, enabling dynamic projector and LLM modeling in two-stage training.   ","Our experiments demonstrate that our solution significantly surpasses LLaVA on existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and LLaVA-Bench.","~\\footnote{Our project is available on the link https://github.com/DCDmllm/HyperLLaVA}."],"url":"http://arxiv.org/abs/2403.13447v1","category":"cs.AI"}
{"created":"2024-03-20 09:34:38","title":"Robustness Verifcation in Neural Networks","abstract":"In this paper we investigate formal verification problems for Neural Network computations. Of central importance will be various robustness and minimization problems such as: Given symbolic specifications of allowed inputs and outputs in form of Linear Programming instances, one question is whether there do exist valid inputs such that the network computes a valid output? And does this property hold for all valid inputs? Do two given networks compute the same function? Is there a smaller network computing the same function?   The complexity of these questions have been investigated recently from a practical point of view and approximated by heuristic algorithms. We complement these achievements by giving a theoretical framework that enables us to interchange security and efficiency questions in neural networks and analyze their computational complexities. We show that the problems are conquerable in a semi-linear setting, meaning that for piecewise linear activation functions and when the sum- or maximum metric is used, most of them are in P or in NP at most.","sentences":["In this paper we investigate formal verification problems for Neural Network computations.","Of central importance will be various robustness and minimization problems such as: Given symbolic specifications of allowed inputs and outputs in form of Linear Programming instances, one question is whether there do exist valid inputs such that the network computes a valid output?","And does this property hold for all valid inputs?","Do two given networks compute the same function?","Is there a smaller network computing the same function?   ","The complexity of these questions have been investigated recently from a practical point of view and approximated by heuristic algorithms.","We complement these achievements by giving a theoretical framework that enables us to interchange security and efficiency questions in neural networks and analyze their computational complexities.","We show that the problems are conquerable in a semi-linear setting, meaning that for piecewise linear activation functions and when the sum- or maximum metric is used, most of them are in P or in NP at most."],"url":"http://arxiv.org/abs/2403.13441v1","category":"cs.AI"}
{"created":"2024-03-20 09:24:03","title":"Search for $\u0394S=2$ nonleptonic hyperon decays $\u03a9^-\\to\u03a3^{0}\u03c0^{-}$ and $\u03a9^-\\to nK^{-}$","abstract":"Using $(27.12 \\pm 0.14) \\times 10^{8}$ $\\psi(3686)$ events collected by the BESIII detector at the center-of-mass energy of $\\sqrt{s} = 3.686$ GeV, we search for the first time for two nonleptonic hyperon decays that change strangeness by two units, $\\Omega^-\\to\\Sigma^{0}\\pi^-$ and $\\Omega^-\\to nK^{-}$. No significant signal is observed. The upper limits on their decay branching fractions are determined to be $\\mathcal{B}(\\Omega^-\\to\\Sigma^{0}\\pi^-) < 5.4\\times 10^{-4}$ and $\\mathcal{B}(\\Omega^-\\to nK^{-}) < 2.4\\times 10^{-4}$ at the $90\\%$ confidence level.","sentences":["Using $(27.12 \\pm 0.14) \\times 10^{8}$ $\\psi(3686)$ events collected by the BESIII detector at the center-of-mass energy of $\\sqrt{s} = 3.686$ GeV, we search for the first time for two nonleptonic hyperon decays that change strangeness by two units, $\\Omega^-\\to\\Sigma^{0}\\pi^-$ and $\\Omega^-\\to nK^{-}$. No significant signal is observed.","The upper limits on their decay branching fractions are determined to be $\\mathcal{B}(\\Omega^-\\to\\Sigma^{0}\\pi^-) <","5.4\\times 10^{-4}$ and $\\mathcal{B}(\\Omega^-\\to nK^{-})","< 2.4\\times 10^{-4}$ at the $90\\%$ confidence level."],"url":"http://arxiv.org/abs/2403.13437v1","category":"hep-ex"}
{"created":"2024-03-20 09:21:32","title":"Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior","abstract":"To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing substantial alignment with human expectations, facilitating more extensive information exchange within the simulation ensures greater orderliness amidst diversity, which leads to the emergence of more unexpected and meaningful emergent behaviors. The code is open source in https://github.com/MikeGu721/AgentGroup, and online platform will be open soon.","sentences":["To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings.","Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant.","Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines.","By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations.","We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters.","Our findings reveal that under the premise of agents possessing substantial alignment with human expectations, facilitating more extensive information exchange within the simulation ensures greater orderliness amidst diversity, which leads to the emergence of more unexpected and meaningful emergent behaviors.","The code is open source in https://github.com/MikeGu721/AgentGroup, and online platform will be open soon."],"url":"http://arxiv.org/abs/2403.13433v1","category":"cs.AI"}
{"created":"2024-03-20 09:18:19","title":"Automatic Navigation Map Generation for Mobile Robots in Urban Environments","abstract":"A fundamental prerequisite for safe and efficient navigation of mobile robots is the availability of reliable navigation maps upon which trajectories can be planned. With the increasing industrial interest in mobile robotics, especially in urban environments, the process of generating navigation maps has become of particular interest, being a labor intensive step of the deployment process. Automating this step is challenging and becomes even more arduous when the perception capabilities are limited by cost considerations. This paper proposes an algorithm to automatically generate navigation maps using a typical navigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor. The proposed method is designed and validated with the urban environment as the main use case: it is shown to be able to produce accurate maps featuring different terrain types, positive obstacles of different heights as well as negative obstacles. The algorithm is applied to data collected in a typical urban environment with a wheeled inverted pendulum robot, showing its robustness against localization, perception and dynamic uncertainties. The generated map is validated against a human-made map.","sentences":["A fundamental prerequisite for safe and efficient navigation of mobile robots is the availability of reliable navigation maps upon which trajectories can be planned.","With the increasing industrial interest in mobile robotics, especially in urban environments, the process of generating navigation maps has become of particular interest, being a labor intensive step of the deployment process.","Automating this step is challenging and becomes even more arduous when the perception capabilities are limited by cost considerations.","This paper proposes an algorithm to automatically generate navigation maps using a typical navigation-oriented sensor setup: a single top-mounted 3D LiDAR sensor.","The proposed method is designed and validated with the urban environment as the main use case: it is shown to be able to produce accurate maps featuring different terrain types, positive obstacles of different heights as well as negative obstacles.","The algorithm is applied to data collected in a typical urban environment with a wheeled inverted pendulum robot, showing its robustness against localization, perception and dynamic uncertainties.","The generated map is validated against a human-made map."],"url":"http://arxiv.org/abs/2403.13431v1","category":"cs.RO"}
{"created":"2024-03-20 09:07:23","title":"Caching-Augmented Lifelong Multi-Agent Path Finding","abstract":"Multi-Agent Path Finding (MAPF), which involves finding collision-free paths for multiple robots, is crucial in various applications. Lifelong MAPF, where targets are reassigned to agents as soon as they complete their initial objectives, offers a more accurate approximation of real-world warehouse planning. In this paper, we present a novel mechanism named Caching-Augmented Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have developed a new map grid type called cache for temporary item storage and replacement and designed a lock mechanism for it to improve the stability of the planning solution. This cache mechanism was evaluated using various cache replacement policies and a spectrum of input task distributions. We identified three main factors significantly impacting CAL-MAPF performance through experimentation: suitable input task distribution, high cache hit rate, and smooth traffic. Overall, CAL-MAPF has demonstrated potential for performance improvements in certain task distributions, maps and agent configurations.","sentences":["Multi-Agent Path Finding (MAPF), which involves finding collision-free paths for multiple robots, is crucial in various applications.","Lifelong MAPF, where targets are reassigned to agents as soon as they complete their initial objectives, offers a more accurate approximation of real-world warehouse planning.","In this paper, we present a novel mechanism named Caching-Augmented Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.","We have developed a new map grid type called cache for temporary item storage and replacement and designed a lock mechanism for it to improve the stability of the planning solution.","This cache mechanism was evaluated using various cache replacement policies and a spectrum of input task distributions.","We identified three main factors significantly impacting CAL-MAPF performance through experimentation: suitable input task distribution, high cache hit rate, and smooth traffic.","Overall, CAL-MAPF has demonstrated potential for performance improvements in certain task distributions, maps and agent configurations."],"url":"http://arxiv.org/abs/2403.13421v1","category":"cs.RO"}
{"created":"2024-03-20 08:52:34","title":"Nonparametric density estimation for stationary processes under multiplicative measurement errors","abstract":"This paper focuses on estimating the invariant density function $f_X$ of the strongly mixing stationary process $X_t$ in the multiplicative measurement errors model $Y_t = X_t U_t$, where $U_t$ is also a strongly mixing stationary process. We propose a novel approach to handle non-independent data, typical in real-world scenarios. For instance, data collected from various groups may exhibit interdependencies within each group, resembling data generated from $m$-dependent stationary processes, a subset of stationary processes. This study extends the applicability of the model $Y_t = X_t U_t$ to diverse scientific domains dealing with complex dependent data. The paper outlines our estimation techniques, discusses convergence rates, establishes a lower bound on the minimax risk, and demonstrates the asymptotic normality of the estimator for $f_X$ under smooth error distributions. Through examples and simulations, we showcase the efficacy of our estimator. The paper concludes by providing proofs for the presented theoretical results.v","sentences":["This paper focuses on estimating the invariant density function $f_X$ of the strongly mixing stationary process $X_t$ in the multiplicative measurement errors model $Y_t = X_t U_t$, where $U_t$ is also a strongly mixing stationary process.","We propose a novel approach to handle non-independent data, typical in real-world scenarios.","For instance, data collected from various groups may exhibit interdependencies within each group, resembling data generated from $m$-dependent stationary processes, a subset of stationary processes.","This study extends the applicability of the model $Y_t = X_t","U_t$ to diverse scientific domains dealing with complex dependent data.","The paper outlines our estimation techniques, discusses convergence rates, establishes a lower bound on the minimax risk, and demonstrates the asymptotic normality of the estimator for $f_X$ under smooth error distributions.","Through examples and simulations, we showcase the efficacy of our estimator.","The paper concludes by providing proofs for the presented theoretical results.v"],"url":"http://arxiv.org/abs/2403.13410v1","category":"math.ST"}
{"created":"2024-03-20 08:50:15","title":"S2DM: Sector-Shaped Diffusion Models for Video Generation","abstract":"Diffusion models have achieved great success in image generation. However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames. This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features. In this work, we propose a novel Sector-Shaped Diffusion Model (S2DM) whose sector-shaped diffusion region is formed by a set of ray-shaped reverse diffusion processes starting at the same noise point. S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions. We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions. Our experimental results show that S2DM outperforms many existing methods in the task of video generation without any temporal-feature modelling modules. For text-to-video generation tasks where temporal conditions are not explicitly given, we propose a two-stage generation strategy which can decouple the generation of temporal features from semantic-content features. We show that, without additional training, our model integrated with another temporal conditions generative model can still achieve comparable performance with existing works. Our results can be viewd at https://s2dm.github.io/S2DM/.","sentences":["Diffusion models have achieved great success in image generation.","However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames.","This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features.","In this work, we propose a novel Sector-Shaped Diffusion Model (S2DM) whose sector-shaped diffusion region is formed by a set of ray-shaped reverse diffusion processes starting at the same noise point.","S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions.","We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions.","Our experimental results show that S2DM outperforms many existing methods in the task of video generation without any temporal-feature modelling modules.","For text-to-video generation tasks where temporal conditions are not explicitly given, we propose a two-stage generation strategy which can decouple the generation of temporal features from semantic-content features.","We show that, without additional training, our model integrated with another temporal conditions generative model can still achieve comparable performance with existing works.","Our results can be viewd at https://s2dm.github.io/S2DM/."],"url":"http://arxiv.org/abs/2403.13408v1","category":"cs.CV"}
{"created":"2024-03-20 08:47:51","title":"DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation","abstract":"Depth-based 3D hand pose estimation is an important but challenging research task in human-machine interaction community. Recently, dense regression methods have attracted increasing attention in 3D hand pose estimation task, which provide a low computational burden and high accuracy regression way by densely regressing hand joint offset maps. However, large-scale regression offset values are often affected by noise and outliers, leading to a significant drop in accuracy. To tackle this, we re-formulate 3D hand pose estimation as a dense ordinal regression problem and propose a novel Dense Ordinal Regression 3D Pose Network (DOR3D-Net). Specifically, we first decompose offset value regression into sub-tasks of binary classifications with ordinal constraints. Then, each binary classifier can predict the probability of a binary spatial relationship relative to joint, which is easier to train and yield much lower level of noise. The estimated hand joint positions are inferred by aggregating the ordinal regression results at local positions with a weighted sum. Furthermore, both joint regression loss and ordinal regression loss are used to train our DOR3D-Net in an end-to-end manner. Extensive experiments on public datasets (ICVL, MSRA, NYU and HANDS2017) show that our design provides significant improvements over SOTA methods.","sentences":["Depth-based 3D hand pose estimation is an important but challenging research task in human-machine interaction community.","Recently, dense regression methods have attracted increasing attention in 3D hand pose estimation task, which provide a low computational burden and high accuracy regression way by densely regressing hand joint offset maps.","However, large-scale regression offset values are often affected by noise and outliers, leading to a significant drop in accuracy.","To tackle this, we re-formulate 3D hand pose estimation as a dense ordinal regression problem and propose a novel Dense Ordinal Regression 3D Pose Network (DOR3D-Net).","Specifically, we first decompose offset value regression into sub-tasks of binary classifications with ordinal constraints.","Then, each binary classifier can predict the probability of a binary spatial relationship relative to joint, which is easier to train and yield much lower level of noise.","The estimated hand joint positions are inferred by aggregating the ordinal regression results at local positions with a weighted sum.","Furthermore, both joint regression loss and ordinal regression loss are used to train our DOR3D-Net in an end-to-end manner.","Extensive experiments on public datasets (ICVL, MSRA, NYU and HANDS2017) show that our design provides significant improvements over SOTA methods."],"url":"http://arxiv.org/abs/2403.13405v1","category":"cs.CV"}
{"created":"2024-03-20 17:59:16","title":"Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments","abstract":"Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest. However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability. In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes. We compute the box stability score (BoS score) to reflect this stability. Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout. To obtain BoS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set. We contribute to finding that BoS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments. This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection. Code and data are available at https://github.com/YangYangGirl/BoS.","sentences":["Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest.","However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability.","In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes.","We compute the box stability score (BoS score) to reflect this stability.","Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout.","To obtain BoS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set.","We contribute to finding that BoS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments.","This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection.","Code and data are available at https://github.com/YangYangGirl/BoS."],"url":"http://arxiv.org/abs/2403.13803v1","category":"cs.CV"}
{"created":"2024-03-20 17:54:58","title":"Bridge the Modality and Capacity Gaps in Vision-Language Model Selection","abstract":"Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the \"Capability Gap\" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of these two gaps. SWAB first adopts optimal transport to capture the relevance between open-source datasets and target dataset with a transportation matrix. It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging those two gaps and enhancing the VLM's capacity estimation for VLM selection. Experiments across various VLMs and image classification datasets validate SWAB's effectiveness.","sentences":["Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names.","The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks.","Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images.","In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the \"Capability Gap\" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance.","We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of these two gaps.","SWAB first adopts optimal transport to capture the relevance between open-source datasets and target dataset with a transportation matrix.","It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging those two gaps and enhancing the VLM's capacity estimation for VLM selection.","Experiments across various VLMs and image classification datasets validate SWAB's effectiveness."],"url":"http://arxiv.org/abs/2403.13797v1","category":"cs.LG"}
{"created":"2024-03-20 17:54:31","title":"Cosmic shear with small scales: DES-Y3, KiDS-1000 and HSC-DR1","abstract":"We present a cosmological analysis of the combination of the DES-Y3, KiDS-1000 and HSC-DR1 weak lensing samples under a joint harmonic-space pipeline making use of angular scales down to $\\ell_{\\rm max}=4500$, corresponding to significantly smaller scales ($\\delta\\theta\\sim2.4'$) than those commonly used in cosmological weak lensing studies. We are able to do so by accurately modelling non-linearities and the impact of baryonic effects using Baccoemu. We find $S_8\\equiv\\sigma_8\\sqrt{\\Omega_{\\rm m}/0.3}=0.795^{+0.015}_{-0.017}$, in relatively good agreement with CMB constraints from Planck (less than $\\sim1.8\\sigma$ tension), although we obtain a low value of $\\Omega_{\\rm m}=0.212^{+0.017}_{-0.032}$, in tension with Planck at the $\\sim3\\sigma$ level. We show that this can be recast as an $H_0$ tension if one parametrises the amplitude of fluctuations and matter abundance in terms of variables without hidden dependence on $H_0$. Furthermore, we find that this tension reduces significantly after including a prior on the distance-redshift relationship from BAO data, without worsening the fit. In terms of baryonic effects, we show that failing to model and marginalise over them on scales $\\ell\\lesssim2000$ does not significantly affect the posterior constraints for DES-Y3 and KiDS-1000, but has a mild effect on deeper samples, such as HSC-DR1. This is in agreement with our ability to only mildly constrain the parameters of the Baryon Correction Model with these data","sentences":["We present a cosmological analysis of the combination of the DES-Y3, KiDS-1000 and HSC-DR1 weak lensing samples under a joint harmonic-space pipeline making use of angular scales down to $\\ell_{\\rm max}=4500$, corresponding to significantly smaller scales ($\\delta\\theta\\sim2.4'$) than those commonly used in cosmological weak lensing studies.","We are able to do so by accurately modelling non-linearities and the impact of baryonic effects using Baccoemu.","We find $S_8\\equiv\\sigma_8\\sqrt{\\Omega_{\\rm m}/0.3}=0.795^{+0.015}_{-0.017}$, in relatively good agreement with CMB constraints from Planck (less than $\\sim1.8\\sigma$ tension), although we obtain a low value of $\\Omega_{\\rm m}=0.212^{+0.017}_{-0.032}$, in tension with Planck at the $\\sim3\\sigma$ level.","We show that this can be recast as an $H_0$ tension if one parametrises the amplitude of fluctuations and matter abundance in terms of variables without hidden dependence on $H_0$. Furthermore, we find that this tension reduces significantly after including a prior on the distance-redshift relationship from BAO data, without worsening the fit.","In terms of baryonic effects, we show that failing to model and marginalise over them on scales $\\ell\\lesssim2000$ does not significantly affect the posterior constraints for DES-Y3 and KiDS-1000, but has a mild effect on deeper samples, such as HSC-DR1.","This is in agreement with our ability to only mildly constrain the parameters of the Baryon Correction Model with these data"],"url":"http://arxiv.org/abs/2403.13794v1","category":"astro-ph.CO"}
{"created":"2024-03-20 17:47:49","title":"Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts","abstract":"Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice. To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists. This approach enables large language models to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding. Experiments on real-world datasets can prove the effectiveness and flexibility of our prompting method with multiple state-of-the-art LLMs over existing prompting baselines. We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying LLMs for psychotherapy behavior understanding.","sentences":["Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction.","While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice.","To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions.","The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists.","This approach enables large language models to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding.","Experiments on real-world datasets can prove the effectiveness and flexibility of our prompting method with multiple state-of-the-art LLMs over existing prompting baselines.","We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying LLMs for psychotherapy behavior understanding."],"url":"http://arxiv.org/abs/2403.13786v1","category":"cs.CL"}
{"created":"2024-03-20 17:47:25","title":"Towards an extension of Fault Trees in the Predictive Maintenance Scenario","abstract":"One of the most appreciated features of Fault Trees (FTs) is their simplicity, making them fit into industrial processes. As such processes evolve in time, considering new aspects of large modern systems, modelling techniques based on FTs have adapted to these needs. This paper proposes an extension of FTs to take into account the problem of Predictive Maintenance, one of the challenges of the modern dependability field of study. The paper sketches the Predictive Fault Tree language and proposes some use cases to support their modelling and analysis in concrete industrial settings.","sentences":["One of the most appreciated features of Fault Trees (FTs) is their simplicity, making them fit into industrial processes.","As such processes evolve in time, considering new aspects of large modern systems, modelling techniques based on FTs have adapted to these needs.","This paper proposes an extension of FTs to take into account the problem of Predictive Maintenance, one of the challenges of the modern dependability field of study.","The paper sketches the Predictive Fault Tree language and proposes some use cases to support their modelling and analysis in concrete industrial settings."],"url":"http://arxiv.org/abs/2403.13785v1","category":"cs.LG"}
{"created":"2024-03-20 17:32:56","title":"A convergent adaptive finite element stochastic Galerkin method based on multilevel expansions of random fields","abstract":"The subject of this work is an adaptive stochastic Galerkin finite element method for parametric or random elliptic partial differential equations, which generates sparse product polynomial expansions with respect to the parametric variables of solutions. For the corresponding spatial approximations, an independently refined finite element mesh is used for each polynomial coefficient. The method relies on multilevel expansions of input random fields and achieves error reduction with uniform rate. In particular, the saturation property for the refinement process is ensured by the algorithm. The results are illustrated by numerical experiments, including cases with random fields of low regularity.","sentences":["The subject of this work is an adaptive stochastic Galerkin finite element method for parametric or random elliptic partial differential equations, which generates sparse product polynomial expansions with respect to the parametric variables of solutions.","For the corresponding spatial approximations, an independently refined finite element mesh is used for each polynomial coefficient.","The method relies on multilevel expansions of input random fields and achieves error reduction with uniform rate.","In particular, the saturation property for the refinement process is ensured by the algorithm.","The results are illustrated by numerical experiments, including cases with random fields of low regularity."],"url":"http://arxiv.org/abs/2403.13770v1","category":"math.NA"}
{"created":"2024-03-20 17:20:48","title":"When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather","abstract":"In Federated Learning (FL), multiple clients collaboratively train a global model without sharing private data. In semantic segmentation, the Federated source Free Domain Adaptation (FFreeDA) setting is of particular interest, where clients undergo unsupervised training after supervised pretraining at the server side. While few recent works address FL for autonomous vehicles, intrinsic real-world challenges such as the presence of adverse weather conditions and the existence of different autonomous agents are still unexplored. To bridge this gap, we address both problems and introduce a new federated semantic segmentation setting where both car and drone clients co-exist and collaborate. Specifically, we propose a novel approach for this setting which exploits a batch-norm weather-aware strategy to dynamically adapt the model to the different weather conditions, while hyperbolic space prototypes are used to align the heterogeneous client representations. Finally, we introduce FLYAWARE, the first semantic segmentation dataset with adverse weather data for aerial vehicles.","sentences":["In Federated Learning (FL), multiple clients collaboratively train a global model without sharing private data.","In semantic segmentation, the Federated source Free Domain Adaptation (FFreeDA) setting is of particular interest, where clients undergo unsupervised training after supervised pretraining at the server side.","While few recent works address FL for autonomous vehicles, intrinsic real-world challenges such as the presence of adverse weather conditions and the existence of different autonomous agents are still unexplored.","To bridge this gap, we address both problems and introduce a new federated semantic segmentation setting where both car and drone clients co-exist and collaborate.","Specifically, we propose a novel approach for this setting which exploits a batch-norm weather-aware strategy to dynamically adapt the model to the different weather conditions, while hyperbolic space prototypes are used to align the heterogeneous client representations.","Finally, we introduce FLYAWARE, the first semantic segmentation dataset with adverse weather data for aerial vehicles."],"url":"http://arxiv.org/abs/2403.13762v1","category":"cs.CV"}
{"created":"2024-03-20 16:53:45","title":"Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation","abstract":"Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion model's generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIA's superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks. Notably, these advancements are achieved without necessitating extensive, task-specific tuning.","sentences":["Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency.","Existing methods fall short in either generation quality or flexibility.","We introduce MOTIA Mastering Video","Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting.","MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting.","The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video.","This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting.","The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes.","Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion model's generative prior and the acquired video patterns from source videos.","Extensive evaluations underscore MOTIA's superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks.","Notably, these advancements are achieved without necessitating extensive, task-specific tuning."],"url":"http://arxiv.org/abs/2403.13745v1","category":"cs.CV"}
{"created":"2024-03-20 16:09:43","title":"Adaptive estimation for Weakly Dependent Functional Times Series","abstract":"The local regularity of functional time series is studied under $L^p-m-$appro\\-ximability assumptions. The sample paths are observed with error at possibly random design points. Non-asymptotic concentration bounds of the regularity estimators are derived. As an application, we build nonparametric mean and autocovariance functions estimators that adapt to the regularity and the design, which can be sparse or dense. We also derive the asymptotic normality of the mean estimator, which allows honest inference for irregular mean functions. Simulations and a real data application illustrate the performance of the new estimators.","sentences":["The local regularity of functional time series is studied under $L^p-m-$appro\\-ximability assumptions.","The sample paths are observed with error at possibly random design points.","Non-asymptotic concentration bounds of the regularity estimators are derived.","As an application, we build nonparametric mean and autocovariance functions estimators that adapt to the regularity and the design, which can be sparse or dense.","We also derive the asymptotic normality of the mean estimator, which allows honest inference for irregular mean functions.","Simulations and a real data application illustrate the performance of the new estimators."],"url":"http://arxiv.org/abs/2403.13706v1","category":"math.ST"}
{"created":"2024-03-20 16:08:27","title":"Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer through an Implicit-Explicit (IMEX) time-stepping approach","abstract":"The Adam optimizer, often used in Machine Learning for neural network training, corresponds to an underlying ordinary differential equation (ODE) in the limit of very small learning rates. This work shows that the classical Adam algorithm is a first order implicit-explicit (IMEX) Euler discretization of the underlying ODE. Employing the time discretization point of view, we propose new extensions of the Adam scheme obtained by using higher order IMEX methods to solve the ODE. Based on this approach, we derive a new optimization algorithm for neural network training that performs better than classical Adam on several regression and classification problems.","sentences":["The Adam optimizer, often used in Machine Learning for neural network training, corresponds to an underlying ordinary differential equation (ODE) in the limit of very small learning rates.","This work shows that the classical Adam algorithm is a first order implicit-explicit (IMEX) Euler discretization of the underlying ODE.","Employing the time discretization point of view, we propose new extensions of the Adam scheme obtained by using higher order IMEX methods to solve the ODE.","Based on this approach, we derive a new optimization algorithm for neural network training that performs better than classical Adam on several regression and classification problems."],"url":"http://arxiv.org/abs/2403.13704v1","category":"cs.CE"}
{"created":"2024-03-20 15:37:19","title":"AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts","abstract":"Leveraging the synergy of both audio data and visual data is essential for understanding human emotions and behaviors, especially in in-the-wild setting. Traditional methods for integrating such multimodal information often stumble, leading to less-than-ideal outcomes in the task of facial action unit detection. To overcome these shortcomings, we propose a novel approach utilizing audio-visual multimodal data. This method enhances audio feature extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel spectrogram features alongside a pre-trained VGGish network. Moreover, this paper adaptively captures fusion features across modalities by modeling the temporal relationships, and ultilizes a pre-trained GPT-2 model for sophisticated context-aware fusion of multimodal information. Our method notably improves the accuracy of AU detection by understanding the temporal and contextual nuances of the data, showcasing significant advancements in the comprehension of intricate scenarios. These findings underscore the potential of integrating temporal dynamics and contextual interpretation, paving the way for future research endeavors.","sentences":["Leveraging the synergy of both audio data and visual data is essential for understanding human emotions and behaviors, especially in in-the-wild setting.","Traditional methods for integrating such multimodal information often stumble, leading to less-than-ideal outcomes in the task of facial action unit detection.","To overcome these shortcomings, we propose a novel approach utilizing audio-visual multimodal data.","This method enhances audio feature extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel spectrogram features alongside a pre-trained VGGish network.","Moreover, this paper adaptively captures fusion features across modalities by modeling the temporal relationships, and ultilizes a pre-trained GPT-2 model for sophisticated context-aware fusion of multimodal information.","Our method notably improves the accuracy of AU detection by understanding the temporal and contextual nuances of the data, showcasing significant advancements in the comprehension of intricate scenarios.","These findings underscore the potential of integrating temporal dynamics and contextual interpretation, paving the way for future research endeavors."],"url":"http://arxiv.org/abs/2403.13678v1","category":"cs.CV"}
{"created":"2024-03-20 15:32:56","title":"Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections","abstract":"In this work, we present a reward-driven automated curriculum reinforcement learning approach for interaction-aware self-driving at unsignalized intersections, taking into account the uncertainties associated with surrounding vehicles (SVs). These uncertainties encompass the uncertainty of SVs' driving intention and also the quantity of SVs. To deal with this problem, the curriculum set is specifically designed to accommodate a progressively increasing number of SVs. By implementing an automated curriculum selection mechanism, the importance weights are rationally allocated across various curricula, thereby facilitating improved sample efficiency and training outcomes. Furthermore, the reward function is meticulously designed to guide the agent towards effective policy exploration. Thus the proposed framework could proactively address the above uncertainties at unsignalized intersections by employing the automated curriculum learning technique that progressively increases task difficulty, and this ensures safe self-driving through effective interaction with SVs. Comparative experiments are conducted in $Highway\\_Env$, and the results indicate that our approach achieves the highest task success rate, attains strong robustness to initialization parameters of the curriculum selection module, and exhibits superior adaptability to diverse situational configurations at unsignalized intersections. Furthermore, the effectiveness of the proposed method is validated using the high-fidelity CARLA simulator.","sentences":["In this work, we present a reward-driven automated curriculum reinforcement learning approach for interaction-aware self-driving at unsignalized intersections, taking into account the uncertainties associated with surrounding vehicles (SVs).","These uncertainties encompass the uncertainty of SVs' driving intention and also the quantity of SVs.","To deal with this problem, the curriculum set is specifically designed to accommodate a progressively increasing number of SVs.","By implementing an automated curriculum selection mechanism, the importance weights are rationally allocated across various curricula, thereby facilitating improved sample efficiency and training outcomes.","Furthermore, the reward function is meticulously designed to guide the agent towards effective policy exploration.","Thus the proposed framework could proactively address the above uncertainties at unsignalized intersections by employing the automated curriculum learning technique that progressively increases task difficulty, and this ensures safe self-driving through effective interaction with SVs.","Comparative experiments are conducted in $Highway\\_Env$, and the results indicate that our approach achieves the highest task success rate, attains strong robustness to initialization parameters of the curriculum selection module, and exhibits superior adaptability to diverse situational configurations at unsignalized intersections.","Furthermore, the effectiveness of the proposed method is validated using the high-fidelity CARLA simulator."],"url":"http://arxiv.org/abs/2403.13674v1","category":"cs.RO"}
{"created":"2024-03-20 15:15:28","title":"Adaptive Reconstruction of Nonlinear Systems States via DREM with Perturbation Annihilation","abstract":"A new adaptive observer is proposed for a certain class of nonlinear systems with bounded unknown input and parametric uncertainty. Unlike most existing solutions, the proposed approach ensures asymptotic convergence of the unknown parameters, state and perturbation estimates to an arbitrarily small neighborhood of the equilibrium point. The solution is based on the novel augmentation of a high-gain observer with the dynamic regressor extension and mixing (DREM) procedure enhanced with a perturbation annihilation algorithm. The aforementioned properties of the proposed solution are verified via numerical experiments.","sentences":["A new adaptive observer is proposed for a certain class of nonlinear systems with bounded unknown input and parametric uncertainty.","Unlike most existing solutions, the proposed approach ensures asymptotic convergence of the unknown parameters, state and perturbation estimates to an arbitrarily small neighborhood of the equilibrium point.","The solution is based on the novel augmentation of a high-gain observer with the dynamic regressor extension and mixing (DREM) procedure enhanced with a perturbation annihilation algorithm.","The aforementioned properties of the proposed solution are verified via numerical experiments."],"url":"http://arxiv.org/abs/2403.13664v1","category":"eess.SY"}
{"created":"2024-03-20 14:58:09","title":"ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer","abstract":"Deep learning models achieve high accuracy in segmentation tasks among others, yet domain shift often degrades the models' performance, which can be critical in real-world scenarios where no target images are available. This paper proposes a zero-shot domain adaptation method based on diffusion models, called ZoDi, which is two-fold by the design: zero-shot image transfer and model adaptation. First, we utilize an off-the-shelf diffusion model to synthesize target-like images by transferring the domain of source images to the target domain. In this we specifically try to maintain the layout and content by utilising layout-to-image diffusion models with stochastic inversion. Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two domains to learn domain-robust representations. Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods. It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the model's performance without target images by inspecting generated images. Our implementation will be publicly available.","sentences":["Deep learning models achieve high accuracy in segmentation tasks among others, yet domain shift often degrades the models' performance, which can be critical in real-world scenarios where no target images are available.","This paper proposes a zero-shot domain adaptation method based on diffusion models, called ZoDi, which is two-fold by the design: zero-shot image transfer and model adaptation.","First, we utilize an off-the-shelf diffusion model to synthesize target-like images by transferring the domain of source images to the target domain.","In this we specifically try to maintain the layout and content by utilising layout-to-image diffusion models with stochastic inversion.","Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two domains to learn domain-robust representations.","Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods.","It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the model's performance without target images by inspecting generated images.","Our implementation will be publicly available."],"url":"http://arxiv.org/abs/2403.13652v1","category":"cs.CV"}
{"created":"2024-03-20 14:57:02","title":"Network bottlenecks and task structure control the evolution of interpretable learning rules in a foraging agent","abstract":"Developing reliable mechanisms for continuous local learning is a central challenge faced by biological and artificial systems. Yet, how the environmental factors and structural constraints on the learning network influence the optimal plasticity mechanisms remains obscure even for simple settings. To elucidate these dependencies, we study meta-learning via evolutionary optimization of simple reward-modulated plasticity rules in embodied agents solving a foraging task. We show that unconstrained meta-learning leads to the emergence of diverse plasticity rules. However, regularization and bottlenecks to the model help reduce this variability, resulting in interpretable rules. Our findings indicate that the meta-learning of plasticity rules is very sensitive to various parameters, with this sensitivity possibly reflected in the learning rules found in biological networks. When included in models, these dependencies can be used to discover potential objective functions and details of biological learning via comparisons with experimental observations.","sentences":["Developing reliable mechanisms for continuous local learning is a central challenge faced by biological and artificial systems.","Yet, how the environmental factors and structural constraints on the learning network influence the optimal plasticity mechanisms remains obscure even for simple settings.","To elucidate these dependencies, we study meta-learning via evolutionary optimization of simple reward-modulated plasticity rules in embodied agents solving a foraging task.","We show that unconstrained meta-learning leads to the emergence of diverse plasticity rules.","However, regularization and bottlenecks to the model help reduce this variability, resulting in interpretable rules.","Our findings indicate that the meta-learning of plasticity rules is very sensitive to various parameters, with this sensitivity possibly reflected in the learning rules found in biological networks.","When included in models, these dependencies can be used to discover potential objective functions and details of biological learning via comparisons with experimental observations."],"url":"http://arxiv.org/abs/2403.13649v1","category":"q-bio.NC"}
{"created":"2024-03-20 14:49:52","title":"H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation","abstract":"In the field of medical image segmentation, variant models based on Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base modules have been very widely developed and applied. However, CNNs are often limited in their ability to deal with long sequences of information, while the low sensitivity of ViTs to local feature information and the problem of secondary computational complexity limit their development. Recently, the emergence of state-space models (SSMs), especially 2D-selective-scan (SS2D), has had an impact on the longtime dominance of traditional CNNs and ViTs as the foundational modules of visual neural networks. In this paper, we extend the adaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for medical image segmentation. Among them, the proposed High-order 2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant information during SS2D operations through higher-order interactions. In addition, the proposed Local-SS2D module improves the learning ability of local features of SS2D at each order of interaction. We conducted comparison and ablation experiments on three publicly available medical image datasets (ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the strong competitiveness of H-vmunet in medical image segmentation tasks. The code is available from https://github.com/wurenkai/H-vmunet .","sentences":["In the field of medical image segmentation, variant models based on Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base modules have been very widely developed and applied.","However, CNNs are often limited in their ability to deal with long sequences of information, while the low sensitivity of ViTs to local feature information and the problem of secondary computational complexity limit their development.","Recently, the emergence of state-space models (SSMs), especially 2D-selective-scan (SS2D), has had an impact on the longtime dominance of traditional CNNs and ViTs as the foundational modules of visual neural networks.","In this paper, we extend the adaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for medical image segmentation.","Among them, the proposed High-order 2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant information during SS2D operations through higher-order interactions.","In addition, the proposed Local-SS2D module improves the learning ability of local features of SS2D at each order of interaction.","We conducted comparison and ablation experiments on three publicly available medical image datasets (ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the strong competitiveness of H-vmunet in medical image segmentation tasks.","The code is available from https://github.com/wurenkai/H-vmunet ."],"url":"http://arxiv.org/abs/2403.13642v1","category":"cs.CV"}
{"created":"2024-03-20 14:32:07","title":"Deep Learning and IACT: Bridging the gap between Monte-Carlo simulations and LST-1 data using domain adaptation","abstract":"The Cherenkov Telescope Array Observatory (CTAO) is the next generation of observatories employing the imaging air Cherenkov technique for the study of very high energy gamma rays. The deployment of deep learning methods for the reconstruction of physical attributes of incident particles has evinced promising outcomes when conducted on simulations. However, the transition of this approach to observational data is accompanied by challenges, as deep learning-based models are susceptible to domain shifts. In this paper, we integrate domain adaptation in the physics-based context of the CTAO and shed light on the gain in performance that these techniques bring using LST-1 real acquisitions.","sentences":["The Cherenkov Telescope Array Observatory (CTAO) is the next generation of observatories employing the imaging air Cherenkov technique for the study of very high energy gamma rays.","The deployment of deep learning methods for the reconstruction of physical attributes of incident particles has evinced promising outcomes when conducted on simulations.","However, the transition of this approach to observational data is accompanied by challenges, as deep learning-based models are susceptible to domain shifts.","In this paper, we integrate domain adaptation in the physics-based context of the CTAO and shed light on the gain in performance that these techniques bring using LST-1 real acquisitions."],"url":"http://arxiv.org/abs/2403.13633v1","category":"astro-ph.IM"}
{"created":"2024-03-20 14:27:49","title":"CheckMate: Evaluating Checkpointing Protocols for Streaming Dataflows","abstract":"Stream processing in the last decade has seen broad adoption in both commercial and research settings. One key element for this success is the ability of modern stream processors to handle failures while ensuring exactly-once processing guarantees. At the moment of writing, virtually all stream processors that guarantee exactly-once processing implement a variant of Apache Flink's coordinated checkpoints - an extension of the original Chandy-Lamport checkpoints from 1985. However, the reasons behind this prevalence of the coordinated approach remain anecdotal, as reported by practitioners of the stream processing community. At the same time, common checkpointing approaches, such as the uncoordinated and the communication-induced ones, remain largely unexplored.   This paper is the first to address this gap by i) shedding light on why practitioners have favored the coordinated approach and ii) by investigating whether there are viable alternatives. To this end, we implement three checkpointing approaches that we surveyed and adapted for the distinct needs of streaming dataflows. Our analysis shows that the coordinated approach outperforms the uncoordinated and communication-induced protocols under uniformly distributed workloads. To our surprise, however, the uncoordinated approach is not only competitive to the coordinated one in uniformly distributed workloads, but it also outperforms the coordinated approach in skewed workloads. We conclude that rather than blindly employing coordinated checkpointing, research should focus on optimizing the very promising uncoordinated approach, as it can address issues with skew and support prevalent cyclic queries. We believe that our findings can trigger further research into checkpointing mechanisms.","sentences":["Stream processing in the last decade has seen broad adoption in both commercial and research settings.","One key element for this success is the ability of modern stream processors to handle failures while ensuring exactly-once processing guarantees.","At the moment of writing, virtually all stream processors that guarantee exactly-once processing implement a variant of Apache Flink's coordinated checkpoints - an extension of the original Chandy-Lamport checkpoints from 1985.","However, the reasons behind this prevalence of the coordinated approach remain anecdotal, as reported by practitioners of the stream processing community.","At the same time, common checkpointing approaches, such as the uncoordinated and the communication-induced ones, remain largely unexplored.   ","This paper is the first to address this gap by i) shedding light on why practitioners have favored the coordinated approach and ii) by investigating whether there are viable alternatives.","To this end, we implement three checkpointing approaches that we surveyed and adapted for the distinct needs of streaming dataflows.","Our analysis shows that the coordinated approach outperforms the uncoordinated and communication-induced protocols under uniformly distributed workloads.","To our surprise, however, the uncoordinated approach is not only competitive to the coordinated one in uniformly distributed workloads, but it also outperforms the coordinated approach in skewed workloads.","We conclude that rather than blindly employing coordinated checkpointing, research should focus on optimizing the very promising uncoordinated approach, as it can address issues with skew and support prevalent cyclic queries.","We believe that our findings can trigger further research into checkpointing mechanisms."],"url":"http://arxiv.org/abs/2403.13629v1","category":"cs.DC"}
{"created":"2024-03-20 14:23:17","title":"Efficient exploration of high-Tc superconductors by a gradient-based composition design","abstract":"We propose a material design method via gradient-based optimization on compositions, overcoming the limitations of traditional methods: exhaustive database searches and conditional generation models. It optimizes inputs via backpropagation, aligning the model's output closely with the target property and facilitating the discovery of unlisted materials and precise property determination. Our method is also capable of adaptive optimization under new conditions without retraining. Applying to exploring high-Tc superconductors, we identified potential compositions beyond existing databases and discovered new hydrogen superconductors via conditional optimization. This method is versatile and significantly advances material design by enabling efficient, extensive searches and adaptability to new constraints.","sentences":["We propose a material design method via gradient-based optimization on compositions, overcoming the limitations of traditional methods: exhaustive database searches and conditional generation models.","It optimizes inputs via backpropagation, aligning the model's output closely with the target property and facilitating the discovery of unlisted materials and precise property determination.","Our method is also capable of adaptive optimization under new conditions without retraining.","Applying to exploring high-Tc superconductors, we identified potential compositions beyond existing databases and discovered new hydrogen superconductors via conditional optimization.","This method is versatile and significantly advances material design by enabling efficient, extensive searches and adaptability to new constraints."],"url":"http://arxiv.org/abs/2403.13627v1","category":"cond-mat.supr-con"}
{"created":"2024-03-20 13:55:04","title":"Optimal control of continuous-time symmetric systems with unknown dynamics and noisy measurements","abstract":"An iterative learning algorithm is presented for continuous-time linear-quadratic optimal control problems where the system is externally symmetric with unknown dynamics. Both finite-horizon and infinite-horizon problems are considered. It is shown that the proposed algorithm is globally convergent to the optimal solution and has some advantages over adaptive dynamic programming, including being unbiased under noisy measurements and having a relatively low computational burden. Numerical experiments show the effectiveness of the results.","sentences":["An iterative learning algorithm is presented for continuous-time linear-quadratic optimal control problems where the system is externally symmetric with unknown dynamics.","Both finite-horizon and infinite-horizon problems are considered.","It is shown that the proposed algorithm is globally convergent to the optimal solution and has some advantages over adaptive dynamic programming, including being unbiased under noisy measurements and having a relatively low computational burden.","Numerical experiments show the effectiveness of the results."],"url":"http://arxiv.org/abs/2403.13605v1","category":"math.OC"}
{"created":"2024-03-20 13:42:57","title":"Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs","abstract":"Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.","sentences":["Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance.","We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context.","We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire.","Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context.","The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science."],"url":"http://arxiv.org/abs/2403.13592v1","category":"cs.CL"}
{"created":"2024-03-20 13:33:55","title":"CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing","abstract":"Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality and reliability of LLMs in generating intricate code.","sentences":["Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code.","However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.","To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement.","CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications.","CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets.","It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality and reliability of LLMs in generating intricate code."],"url":"http://arxiv.org/abs/2403.13583v1","category":"cs.SE"}
{"created":"2024-03-20 13:02:20","title":"Certified Constraint Propagation and Dual Proof Analysis in a Numerically Exact MIP Solver","abstract":"This paper presents the integration of constraint propagation and dual proof analysis in an exact, roundoff-error-free MIP solver. The authors employ safe rounding methods to ensure that all results remain provably correct, while sacrificing as little computational performance as possible in comparison to a pure floating-point implementation. The study also addresses the adaptation of certification techniques for correctness verification. Computational studies demonstrate the effectiveness of these techniques, showcasing a 23% performance improvement on the MIPLIB 2017 benchmark test set.","sentences":["This paper presents the integration of constraint propagation and dual proof analysis in an exact, roundoff-error-free MIP solver.","The authors employ safe rounding methods to ensure that all results remain provably correct, while sacrificing as little computational performance as possible in comparison to a pure floating-point implementation.","The study also addresses the adaptation of certification techniques for correctness verification.","Computational studies demonstrate the effectiveness of these techniques, showcasing a 23% performance improvement on the MIPLIB 2017 benchmark test set."],"url":"http://arxiv.org/abs/2403.13567v1","category":"math.OC"}
{"created":"2024-03-20 12:58:46","title":"AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression","abstract":"We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectiveness of the proposed method is validated using both synthetic and real data.","sentences":["We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size.","To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures.","We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure.","To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample.","The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases.","The effectiveness of the proposed method is validated using both synthetic and real data."],"url":"http://arxiv.org/abs/2403.13565v1","category":"stat.ML"}
{"created":"2024-03-20 12:44:00","title":"Thermal transport across a Josephson junction in a dissipative environment","abstract":"At zero temperature, a Josephson junction coupled to an ohmic environment displays a quantum phase transition between superconducting and insulating phases, depending whether the resistance of the environment is below or above the resistance quantum. At finite temperature, this so-called Schmid transition turns into a crossover. We determine the conditions under which the temperature dependence of the thermal conductance, which characterizes heat flow from a hot to cold resistor across the Josephson junction, displays universal scaling characteristic of the Schmid transition. We also discuss conditions for heat rectification to happen in the circuit. Our work can serve as a guide for identifying signatures of the Schmid transition in heat transport experiments.","sentences":["At zero temperature, a Josephson junction coupled to an ohmic environment displays a quantum phase transition between superconducting and insulating phases, depending whether the resistance of the environment is below or above the resistance quantum.","At finite temperature, this so-called Schmid transition turns into a crossover.","We determine the conditions under which the temperature dependence of the thermal conductance, which characterizes heat flow from a hot to cold resistor across the Josephson junction, displays universal scaling characteristic of the Schmid transition.","We also discuss conditions for heat rectification to happen in the circuit.","Our work can serve as a guide for identifying signatures of the Schmid transition in heat transport experiments."],"url":"http://arxiv.org/abs/2403.13552v1","category":"cond-mat.supr-con"}
{"created":"2024-03-20 12:40:06","title":"The Tribal Theater Model: Social Regulation for Dynamic User Adaptation in Virtual Interactive Environments","abstract":"This paper proposes a social regulation model for dynamic adaptation according to user characteristics in virtual interactive environments, namely the tribal theater model. The model focuses on organizational regulation and builds an interaction scheme with more resilient user performance by improving the subjectivity of the user. This paper discusses the sociological theoretical basis of this model and how it was migrated to an engineering implementation of a virtual interactive environment. The model defines user interactions within a field that are regulated by a matrix through the allocation of resources. To verify the effectiveness of the tribal theater model, we designed an experimental scene using a chatroom as an example. We trained the matrix as an AI model using a temporal transformer and compared it with an interaction field with different levels of control. The experimental results showed that the tribal theater model can improve users' interactive experience, enhance resilient user performance, and effectively complete environmental interaction tasks under rule-based interaction.","sentences":["This paper proposes a social regulation model for dynamic adaptation according to user characteristics in virtual interactive environments, namely the tribal theater model.","The model focuses on organizational regulation and builds an interaction scheme with more resilient user performance by improving the subjectivity of the user.","This paper discusses the sociological theoretical basis of this model and how it was migrated to an engineering implementation of a virtual interactive environment.","The model defines user interactions within a field that are regulated by a matrix through the allocation of resources.","To verify the effectiveness of the tribal theater model, we designed an experimental scene using a chatroom as an example.","We trained the matrix as an AI model using a temporal transformer and compared it with an interaction field with different levels of control.","The experimental results showed that the tribal theater model can improve users' interactive experience, enhance resilient user performance, and effectively complete environmental interaction tasks under rule-based interaction."],"url":"http://arxiv.org/abs/2403.13550v1","category":"cs.HC"}
{"created":"2024-03-20 11:55:53","title":"Photonic heat transport through a Josephson junction in a resistive environment","abstract":"Motivated by recent experiments (Subero et. al. Nature Comm. $\\bf{14}$, 7924 (2023)) we analyze photonic heat transport through a Josephson junction in a dissipative environment. For this purpose we derive the general expressions for the heat current in terms of non-equilibrium Green functions for the junction coupled in series or in parallel with two environmental impedances at different temperatures. We show that even on the insulating side of the Schmid transition the heat current is sensitive to the Josephson coupling exhibiting an opposite behavior for the series and parallel connection and in qualitative agreement with experiments. We also predict that this device should exhibit heat rectification properties and provide simple expressions to account for them in terms of the system parameters.","sentences":["Motivated by recent experiments (Subero et.","al.","Nature Comm.","$\\bf{14}$, 7924 (2023)) we analyze photonic heat transport through a Josephson junction in a dissipative environment.","For this purpose we derive the general expressions for the heat current in terms of non-equilibrium Green functions for the junction coupled in series or in parallel with two environmental impedances at different temperatures.","We show that even on the insulating side of the Schmid transition the heat current is sensitive to the Josephson coupling exhibiting an opposite behavior for the series and parallel connection and in qualitative agreement with experiments.","We also predict that this device should exhibit heat rectification properties and provide simple expressions to account for them in terms of the system parameters."],"url":"http://arxiv.org/abs/2403.13526v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-20 11:45:48","title":"Hybrid skin-topological effect induced by eight-site cells and arbitrary adjustment of the localization of topological edge states","abstract":"The hybrid skin-topological effect (HSTE) in non-Hermitian systems exhibits both the skin effect and topological protection, offering a novel mechanism for the localization of topological edge states (TESs) in electrons, circuits, and photons. However, it remains unclear whether the HSTE can be realized in quasicrystals, and the unique structure of quasicrystals with multi-site cells may provide novel localization phenomena for TESs induced by the HSTE. We propose an eight-site cell in two-dimensional quasicrystals and realize the HSTE with eight-site nonreciprocal intracell hoppings. Furthermore, we can arbitrarily adjust the eigenfield distributions of the TESs and discover domain walls associated with effective dissipation and their correlation with localization. We present a new scheme to precisely adjust the energy distribution in non-Hermitian quasicrystals with arbitrary polygonal outer boundaries.","sentences":["The hybrid skin-topological effect (HSTE) in non-Hermitian systems exhibits both the skin effect and topological protection, offering a novel mechanism for the localization of topological edge states (TESs) in electrons, circuits, and photons.","However, it remains unclear whether the HSTE can be realized in quasicrystals, and the unique structure of quasicrystals with multi-site cells may provide novel localization phenomena for TESs induced by the HSTE.","We propose an eight-site cell in two-dimensional quasicrystals and realize the HSTE with eight-site nonreciprocal intracell hoppings.","Furthermore, we can arbitrarily adjust the eigenfield distributions of the TESs and discover domain walls associated with effective dissipation and their correlation with localization.","We present a new scheme to precisely adjust the energy distribution in non-Hermitian quasicrystals with arbitrary polygonal outer boundaries."],"url":"http://arxiv.org/abs/2403.13521v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-20 11:12:57","title":"High-confidence pseudo-labels for domain adaptation in COVID-19 detection","abstract":"This paper outlines our submission for the 4th COV19D competition as part of the `Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis' (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition Conference (CVPR). The competition consists of two challenges. The first is to train a classifier to detect the presence of COVID-19 from over one thousand CT scans from the COV19-CT-DB database. The second challenge is to perform domain adaptation by taking the dataset from Challenge 1 and adding a small number of scans (some annotated and other not) for a different distribution. We preprocessed the CT scans to segment the lungs, and output volumes with the lungs individually and together. We then trained 3D ResNet and Swin Transformer models on these inputs. We annotated the unlabeled CT scans using an ensemble of these models and chose the high-confidence predictions as pseudo-labels for fine-tuning. This resulted in a best cross-validation mean F1 score of 93.39\\% for Challenge 1 and a mean F1 score of 92.15 for Challenge 2.","sentences":["This paper outlines our submission for the 4th COV19D competition as part of the `Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis' (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition Conference (CVPR).","The competition consists of two challenges.","The first is to train a classifier to detect the presence of COVID-19 from over one thousand CT scans from the COV19-CT-DB database.","The second challenge is to perform domain adaptation by taking the dataset from Challenge 1 and adding a small number of scans (some annotated and other not) for a different distribution.","We preprocessed the CT scans to segment the lungs, and output volumes with the lungs individually and together.","We then trained 3D ResNet and Swin Transformer models on these inputs.","We annotated the unlabeled CT scans using an ensemble of these models and chose the high-confidence predictions as pseudo-labels for fine-tuning.","This resulted in a best cross-validation mean F1 score of 93.39\\% for Challenge 1 and a mean F1 score of 92.15 for Challenge 2."],"url":"http://arxiv.org/abs/2403.13509v1","category":"eess.IV"}
{"created":"2024-03-20 10:48:00","title":"Distance Comparison Operators for Approximate Nearest Neighbor Search: Exploration and Benchmark","abstract":"Approximate nearest neighbor search (ANNS) on high-dimensional vectors has become a fundamental and essential component in various machine learning tasks. Prior research has shown that the distance comparison operation is the bottleneck of ANNS, which determines the query and indexing performance. To overcome this challenge, some novel methods have been proposed recently. The basic idea is to estimate the actual distance with fewer calculations, at the cost of accuracy loss. Inspired by this, we also propose that some classical techniques and deep learning models can also be adapted to this purpose. In this paper, we systematically categorize the techniques that have been or can be used to accelerate distance approximation. And to help the users understand the pros and cons of different techniques, we design a fair and comprehensive benchmark, Fudist implements these techniques with the same base index and evaluates them on 16 real datasets with several evaluation metrics. Designed as an independent and portable library, Fudist is orthogonal to the specific index structure and thus can be easily utilized in the current ANNS library to achieve significant improvements.","sentences":["Approximate nearest neighbor search (ANNS) on high-dimensional vectors has become a fundamental and essential component in various machine learning tasks.","Prior research has shown that the distance comparison operation is the bottleneck of ANNS, which determines the query and indexing performance.","To overcome this challenge, some novel methods have been proposed recently.","The basic idea is to estimate the actual distance with fewer calculations, at the cost of accuracy loss.","Inspired by this, we also propose that some classical techniques and deep learning models can also be adapted to this purpose.","In this paper, we systematically categorize the techniques that have been or can be used to accelerate distance approximation.","And to help the users understand the pros and cons of different techniques, we design a fair and comprehensive benchmark, Fudist implements these techniques with the same base index and evaluates them on 16 real datasets with several evaluation metrics.","Designed as an independent and portable library, Fudist is orthogonal to the specific index structure and thus can be easily utilized in the current ANNS library to achieve significant improvements."],"url":"http://arxiv.org/abs/2403.13491v1","category":"cs.DB"}
{"created":"2024-03-20 10:22:13","title":"Distributed Cooperative Formation Control of Nonlinear Multi-Agent System (UGV) Using Neural Network","abstract":"The paper presented in this article deals with the issue of distributed cooperative formation of multi-agent systems (MASs). It proposes the use of appropriate neural network control methods to address formation requirements (uncertainties dynamic model). It considers an adaptive leader-follower distributed cooperative formation control based on neural networks (NNs) developed for a class of second-order nonlinear multi-agent systems and neural networks Neural networks are used to compute system data that inputs layer (position, velocity), hidden layers, and output layer. Through collaboration between leader-follower approaches and neural networks with complex systems or complex conditions receive an effective cooperative formation control method. The sufficient conditions for the system stability were derived using Lyapunov stability theory, graph theory, and state space methods. By simulation, the results of this study can be obtained from the main data of the multi-agent system in formation control and verified that the system can process consistency, stability, reliability, and accuracy in cooperative formation.","sentences":["The paper presented in this article deals with the issue of distributed cooperative formation of multi-agent systems (MASs).","It proposes the use of appropriate neural network control methods to address formation requirements (uncertainties dynamic model).","It considers an adaptive leader-follower distributed cooperative formation control based on neural networks (NNs) developed for a class of second-order nonlinear multi-agent systems and neural networks Neural networks are used to compute system data that inputs layer (position, velocity), hidden layers, and output layer.","Through collaboration between leader-follower approaches and neural networks with complex systems or complex conditions receive an effective cooperative formation control method.","The sufficient conditions for the system stability were derived using Lyapunov stability theory, graph theory, and state space methods.","By simulation, the results of this study can be obtained from the main data of the multi-agent system in formation control and verified that the system can process consistency, stability, reliability, and accuracy in cooperative formation."],"url":"http://arxiv.org/abs/2403.13473v1","category":"eess.SY"}
{"created":"2024-03-20 10:18:05","title":"DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts","abstract":"Open-domain question answering requires retrieval systems able to cope with the diverse and varied nature of questions, providing accurate answers across a broad spectrum of query types and topics. To deal with such topic heterogeneity through a unique model, we propose DESIRE-ME, a neural information retrieval model that leverages the Mixture-of-Experts framework to combine multiple specialized neural models. We rely on Wikipedia data to train an effective neural gating mechanism that classifies the incoming query and that weighs the predictions of the different domain-specific experts correspondingly. This allows DESIRE-ME to specialize adaptively in multiple domains. Through extensive experiments on publicly available datasets, we show that our proposal can effectively generalize domain-enhanced neural models. DESIRE-ME excels in handling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and 22% in P@1, the underlying state-of-the-art dense retrieval model.","sentences":["Open-domain question answering requires retrieval systems able to cope with the diverse and varied nature of questions, providing accurate answers across a broad spectrum of query types and topics.","To deal with such topic heterogeneity through a unique model, we propose DESIRE-ME, a neural information retrieval model that leverages the Mixture-of-Experts framework to combine multiple specialized neural models.","We rely on Wikipedia data to train an effective neural gating mechanism that classifies the incoming query and that weighs the predictions of the different domain-specific experts correspondingly.","This allows DESIRE-ME to specialize adaptively in multiple domains.","Through extensive experiments on publicly available datasets, we show that our proposal can effectively generalize domain-enhanced neural models.","DESIRE-ME excels in handling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and 22% in P@1, the underlying state-of-the-art dense retrieval model."],"url":"http://arxiv.org/abs/2403.13468v1","category":"cs.IR"}
{"created":"2024-03-20 10:09:13","title":"Quantum control by the environment: Turing uncomputability, Optimization over Stiefel manifolds, Reachable sets, and Incoherent GRAPE","abstract":"The ability to control quantum systems is necessary for many applications of quantum technologies ranging from gate generation in quantum computation to NMR and laser control of chemical reactions. In many practical situations, the controlled quantum systems are open, i.e., interacting with the environment. While often influence of the environment is considered as an obstacle for controlling the systems, in some cases it can be exploited as a useful resource. In this note, we briefly review some results on control of open quantum systems using environment as a resource, including control by engineered environments and by non-selective measurements, Turing uncomputability of discrete quantum control, parametrization of Kraus maps by points of the Stiefel manifolds and corresponding Riemanninan optimization, control by dissipation and time-dependent decoherence rates, reachable sets, and incoherent GRAPE (Gradient Ascent Pulse Engineering) -- inGRAPE -- for gradient-based optimization.","sentences":["The ability to control quantum systems is necessary for many applications of quantum technologies ranging from gate generation in quantum computation to NMR and laser control of chemical reactions.","In many practical situations, the controlled quantum systems are open, i.e., interacting with the environment.","While often influence of the environment is considered as an obstacle for controlling the systems, in some cases it can be exploited as a useful resource.","In this note, we briefly review some results on control of open quantum systems using environment as a resource, including control by engineered environments and by non-selective measurements, Turing uncomputability of discrete quantum control, parametrization of Kraus maps by points of the Stiefel manifolds and corresponding Riemanninan optimization, control by dissipation and time-dependent decoherence rates, reachable sets, and incoherent GRAPE (Gradient Ascent Pulse Engineering) -- inGRAPE -- for gradient-based optimization."],"url":"http://arxiv.org/abs/2403.13461v1","category":"quant-ph"}
{"created":"2024-03-20 09:59:58","title":"Adaptive time step selection for Spectral Deferred Corrections","abstract":"Spectral Deferred Corrections (SDC) is an iterative method for the numerical solution of ordinary differential equations. It works by refining the numerical solution for an initial value problem by approximately solving differential equations for the error, and can be interpreted as a preconditioned fixed-point iteration for solving the fully implicit collocation problem. We adopt techniques from embedded Runge-Kutta Methods (RKM) to SDC in order to provide a mechanism for adaptive time step size selection and thus increase computational efficiency of SDC. We propose two SDC-specific estimates of the local error that are generic and require only minimal problem specific tuning. We demonstrate a gain in efficiency over standard SDC with fixed step size, compare efficiency favorably against state-of-the-art adaptive RKM and show that due to its iterative nature, adaptive SDC can cope efficiently with silent data corruption.","sentences":["Spectral Deferred Corrections (SDC) is an iterative method for the numerical solution of ordinary differential equations.","It works by refining the numerical solution for an initial value problem by approximately solving differential equations for the error, and can be interpreted as a preconditioned fixed-point iteration for solving the fully implicit collocation problem.","We adopt techniques from embedded Runge-Kutta Methods (RKM) to SDC in order to provide a mechanism for adaptive time step size selection and thus increase computational efficiency of SDC.","We propose two SDC-specific estimates of the local error that are generic and require only minimal problem specific tuning.","We demonstrate a gain in efficiency over standard SDC with fixed step size, compare efficiency favorably against state-of-the-art adaptive RKM and show that due to its iterative nature, adaptive SDC can cope efficiently with silent data corruption."],"url":"http://arxiv.org/abs/2403.13454v1","category":"math.NA"}
{"created":"2024-03-20 09:17:12","title":"Detecting and Triaging Spoofing using Temporal Convolutional Networks","abstract":"As algorithmic trading and electronic markets continue to transform the landscape of financial markets, detecting and deterring rogue agents to maintain a fair and efficient marketplace is crucial. The explosion of large datasets and the continually changing tricks of the trade make it difficult to adapt to new market conditions and detect bad actors. To that end, we propose a framework that can be adapted easily to various problems in the space of detecting market manipulation. Our approach entails initially employing a labelling algorithm which we use to create a training set to learn a weakly supervised model to identify potentially suspicious sequences of order book states. The main goal here is to learn a representation of the order book that can be used to easily compare future events. Subsequently, we posit the incorporation of expert assessment to scrutinize specific flagged order book states. In the event of an expert's unavailability, recourse is taken to the application of a more complex algorithm on the identified suspicious order book states. We then conduct a similarity search between any new representation of the order book against the expert labelled representations to rank the results of the weak learner. We show some preliminary results that are promising to explore further in this direction","sentences":["As algorithmic trading and electronic markets continue to transform the landscape of financial markets, detecting and deterring rogue agents to maintain a fair and efficient marketplace is crucial.","The explosion of large datasets and the continually changing tricks of the trade make it difficult to adapt to new market conditions and detect bad actors.","To that end, we propose a framework that can be adapted easily to various problems in the space of detecting market manipulation.","Our approach entails initially employing a labelling algorithm which we use to create a training set to learn a weakly supervised model to identify potentially suspicious sequences of order book states.","The main goal here is to learn a representation of the order book that can be used to easily compare future events.","Subsequently, we posit the incorporation of expert assessment to scrutinize specific flagged order book states.","In the event of an expert's unavailability, recourse is taken to the application of a more complex algorithm on the identified suspicious order book states.","We then conduct a similarity search between any new representation of the order book against the expert labelled representations to rank the results of the weak learner.","We show some preliminary results that are promising to explore further in this direction"],"url":"http://arxiv.org/abs/2403.13429v1","category":"q-fin.TR"}
{"created":"2024-03-20 09:00:19","title":"Diversified and Personalized Multi-rater Medical Image Segmentation","abstract":"Annotation ambiguity due to inherent data uncertainties such as blurred boundaries in medical scans and different observer expertise and preferences has become a major obstacle for training deep-learning based medical image segmentation models. To address it, the common practice is to gather multiple annotations from different experts, leading to the setting of multi-rater medical image segmentation. Existing works aim to either merge different annotations into the \"groundtruth\" that is often unattainable in numerous medical contexts, or generate diverse results, or produce personalized results corresponding to individual expert raters. Here, we bring up a more ambitious goal for multi-rater medical image segmentation, i.e., obtaining both diversified and personalized results. Specifically, we propose a two-stage framework named D-Persona (first Diversification and then Personalization). In Stage I, we exploit multiple given annotations to train a Probabilistic U-Net model, with a bound-constrained loss to improve the prediction diversity. In this way, a common latent space is constructed in Stage I, where different latent codes denote diversified expert opinions. Then, in Stage II, we design multiple attention-based projection heads to adaptively query the corresponding expert prompts from the shared latent space, and then perform the personalized medical image segmentation. We evaluated the proposed model on our in-house Nasopharyngeal Carcinoma dataset and the public lung nodule dataset (i.e., LIDC-IDRI). Extensive experiments demonstrated our D-Persona can provide diversified and personalized results at the same time, achieving new SOTA performance for multi-rater medical image segmentation. Our code will be released at https://github.com/ycwu1997/D-Persona.","sentences":["Annotation ambiguity due to inherent data uncertainties such as blurred boundaries in medical scans and different observer expertise and preferences has become a major obstacle for training deep-learning based medical image segmentation models.","To address it, the common practice is to gather multiple annotations from different experts, leading to the setting of multi-rater medical image segmentation.","Existing works aim to either merge different annotations into the \"groundtruth\" that is often unattainable in numerous medical contexts, or generate diverse results, or produce personalized results corresponding to individual expert raters.","Here, we bring up a more ambitious goal for multi-rater medical image segmentation, i.e., obtaining both diversified and personalized results.","Specifically, we propose a two-stage framework named D-Persona (first Diversification and then Personalization).","In Stage I, we exploit multiple given annotations to train a Probabilistic U-Net model, with a bound-constrained loss to improve the prediction diversity.","In this way, a common latent space is constructed in Stage I, where different latent codes denote diversified expert opinions.","Then, in Stage II, we design multiple attention-based projection heads to adaptively query the corresponding expert prompts from the shared latent space, and then perform the personalized medical image segmentation.","We evaluated the proposed model on our in-house Nasopharyngeal Carcinoma dataset and the public lung nodule dataset (i.e., LIDC-IDRI).","Extensive experiments demonstrated our D-Persona can provide diversified and personalized results at the same time, achieving new SOTA performance for multi-rater medical image segmentation.","Our code will be released at https://github.com/ycwu1997/D-Persona."],"url":"http://arxiv.org/abs/2403.13417v1","category":"cs.CV"}
{"created":"2024-03-20 08:23:34","title":"Dynamic variable step size LMS adaptation algorithms -- Application to adaptive feedforward noise attenuation","abstract":"The paper explores in detail the use of dynamic adaptation gain/step size (DAG) for improving the adaptation transient performance of variable step-size LMS (VS-LMS) adaptation algorithms. A generic form for the implementation of the DAG within the VS-LMS algorithms is provided. Criteria for the selection of the coefficients of the DAG filter which is required to be a strictly positive real transfer operator are given. The potential of the VS-LMS adaptation algorithms using a DAG is then illustrated by experimental results obtained on a relevant adaptive active noise attenuation system.","sentences":["The paper explores in detail the use of dynamic adaptation gain/step size (DAG) for improving the adaptation transient performance of variable step-size LMS (VS-LMS) adaptation algorithms.","A generic form for the implementation of the DAG within the VS-LMS algorithms is provided.","Criteria for the selection of the coefficients of the DAG filter which is required to be a strictly positive real transfer operator are given.","The potential of the VS-LMS adaptation algorithms using a DAG is then illustrated by experimental results obtained on a relevant adaptive active noise attenuation system."],"url":"http://arxiv.org/abs/2403.13381v1","category":"math.OC"}
{"created":"2024-03-20 08:15:08","title":"Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity","abstract":"This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and $\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optimal solution is proved to obtainable as data heterogeneity vanishes. Experimental results corroborate the robustness of RAGA to Byzantine attacks and verifies the advantage of RAGA over baselines on convergence performance under various intensity of Byzantine attacks, for heterogeneous dataset.","sentences":["This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity.","A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating.","Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset.","According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and $\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function.","Moreover, stationary point or global optimal solution is proved to obtainable as data heterogeneity vanishes.","Experimental results corroborate the robustness of RAGA to Byzantine attacks and verifies the advantage of RAGA over baselines on convergence performance under various intensity of Byzantine attacks, for heterogeneous dataset."],"url":"http://arxiv.org/abs/2403.13374v1","category":"cs.LG"}
{"created":"2024-03-20 08:08:54","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","abstract":"Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.","sentences":["Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks.","However, it requires non-trivial efforts to implement these methods on different models.","We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods.","It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard.","We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks.","It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks."],"url":"http://arxiv.org/abs/2403.13372v2","category":"cs.CL"}
{"created":"2024-03-20 08:01:33","title":"Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting","abstract":"Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classification model by 30.5% accuracy. Our results serve as a process-oriented guideline for clinical information extraction projects working with low-resource.","sentences":["Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations.","Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods.","We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters.","We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions.","We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classification model by 30.5% accuracy.","Our results serve as a process-oriented guideline for clinical information extraction projects working with low-resource."],"url":"http://arxiv.org/abs/2403.13369v1","category":"cs.CL"}
{"created":"2024-03-20 07:51:53","title":"Centroidal State Estimation based on the Koopman Embedding for Dynamic Legged Locomotion","abstract":"In this paper, we introduce a novel approach to centroidal state estimation, which plays a crucial role in predictive model-based control strategies for dynamic legged locomotion. Our approach uses the Koopman operator theory to transform the robot's complex nonlinear dynamics into a linear system, by employing dynamic mode decomposition and deep learning for model construction. We evaluate both models on their linearization accuracy and capability to capture both fast and slow dynamic system responses. We then select the most suitable model for estimation purposes, and integrate it within a moving horizon estimator. This estimator is formulated as a convex quadratic program, to facilitate robust, real-time centroidal state estimation. Through extensive simulation experiments on a quadruped robot executing various dynamic gaits, our data-driven framework outperforms conventional filtering techniques based on nonlinear dynamics. Our estimator addresses challenges posed by force/torque measurement noise in highly dynamic motions and accurately recovers the centroidal states, demonstrating the adaptability and effectiveness of the Koopman-based linear representation for complex locomotive behaviors. Importantly, our model based on dynamic mode decomposition, trained with two locomotion patterns (trot and jump), successfully estimates the centroidal states for a different motion (bound) without retraining.","sentences":["In this paper, we introduce a novel approach to centroidal state estimation, which plays a crucial role in predictive model-based control strategies for dynamic legged locomotion.","Our approach uses the Koopman operator theory to transform the robot's complex nonlinear dynamics into a linear system, by employing dynamic mode decomposition and deep learning for model construction.","We evaluate both models on their linearization accuracy and capability to capture both fast and slow dynamic system responses.","We then select the most suitable model for estimation purposes, and integrate it within a moving horizon estimator.","This estimator is formulated as a convex quadratic program, to facilitate robust, real-time centroidal state estimation.","Through extensive simulation experiments on a quadruped robot executing various dynamic gaits, our data-driven framework outperforms conventional filtering techniques based on nonlinear dynamics.","Our estimator addresses challenges posed by force/torque measurement noise in highly dynamic motions and accurately recovers the centroidal states, demonstrating the adaptability and effectiveness of the Koopman-based linear representation for complex locomotive behaviors.","Importantly, our model based on dynamic mode decomposition, trained with two locomotion patterns (trot and jump), successfully estimates the centroidal states for a different motion (bound) without retraining."],"url":"http://arxiv.org/abs/2403.13366v1","category":"cs.RO"}
{"created":"2024-03-20 07:48:32","title":"ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics","abstract":"Robotic manipulation in everyday scenarios, especially in unstructured environments, requires skills in pose-aware object manipulation (POM), which adapts robots' grasping and handling according to an object's 6D pose. Recognizing an object's position and orientation is crucial for effective manipulation. For example, if a mug is lying on its side, it's more effective to grasp it by the rim rather than the handle. Despite its importance, research in POM skills remains limited, because learning manipulation skills requires pose-varying simulation environments and datasets. This paper introduces ManiPose, a pioneering benchmark designed to advance the study of pose-varying manipulation tasks. ManiPose encompasses: 1) Simulation environments for POM feature tasks ranging from 6D pose-specific pick-and-place of single objects to cluttered scenes, further including interactions with articulated objects. 2) A comprehensive dataset featuring geometrically consistent and manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects and 100 articulated objects across 59 categories. 3) A baseline for POM, leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the relationship between 6D pose and task-specific requirements, offers enhanced pose-aware grasp prediction and motion planning capabilities. Our benchmark demonstrates notable advancements in pose estimation, pose-aware manipulation, and real-robot skill transfer, setting new standards for POM research. We will open-source the ManiPose benchmark with the final version paper, inviting the community to engage with our resources, available at our website:https://sites.google.com/view/manipose.","sentences":["Robotic manipulation in everyday scenarios, especially in unstructured environments, requires skills in pose-aware object manipulation (POM), which adapts robots' grasping and handling according to an object's 6D pose.","Recognizing an object's position and orientation is crucial for effective manipulation.","For example, if a mug is lying on its side, it's more effective to grasp it by the rim rather than the handle.","Despite its importance, research in POM skills remains limited, because learning manipulation skills requires pose-varying simulation environments and datasets.","This paper introduces ManiPose, a pioneering benchmark designed to advance the study of pose-varying manipulation tasks.","ManiPose encompasses: 1) Simulation environments for POM feature tasks ranging from 6D pose-specific pick-and-place of single objects to cluttered scenes, further including interactions with articulated objects.","2) A comprehensive dataset featuring geometrically consistent and manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects and 100 articulated objects across 59 categories.","3) A baseline for POM, leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the relationship between 6D pose and task-specific requirements, offers enhanced pose-aware grasp prediction and motion planning capabilities.","Our benchmark demonstrates notable advancements in pose estimation, pose-aware manipulation, and real-robot skill transfer, setting new standards for POM research.","We will open-source the ManiPose benchmark with the final version paper, inviting the community to engage with our resources, available at our website:https://sites.google.com/view/manipose."],"url":"http://arxiv.org/abs/2403.13365v1","category":"cs.RO"}
{"created":"2024-03-20 07:34:21","title":"KunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario","abstract":"This work aims to promote Chinese opera research in both musical and speech domains, with a primary focus on overcoming the data limitations. We introduce KunquDB, a relatively large-scale, well-annotated audio-visual dataset comprising 339 speakers and 128 hours of content. Originating from the Kunqu Opera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by dialogue lines, providing explicit annotations including character names, speaker names, gender information, vocal manner classifications, and accompanied by preliminary text transcriptions. KunquDB provides a versatile foundation for role-centric acoustic studies and advancements in speech-related research, including Automatic Speaker Verification (ASV). Beyond enriching opera research, this dataset bridges the gap between artistic expression and technological innovation. Pioneering the exploration of ASV in Chinese opera, we construct four test trials considering two distinct vocal manners in opera voices: stage speech (ST) and singing (S). Implementing domain adaptation methods effectively mitigates domain mismatches induced by these vocal manner variations while there is still room for further improvement as a benchmark.","sentences":["This work aims to promote Chinese opera research in both musical and speech domains, with a primary focus on overcoming the data limitations.","We introduce KunquDB, a relatively large-scale, well-annotated audio-visual dataset comprising 339 speakers and 128 hours of content.","Originating from the Kunqu Opera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by dialogue lines, providing explicit annotations including character names, speaker names, gender information, vocal manner classifications, and accompanied by preliminary text transcriptions.","KunquDB provides a versatile foundation for role-centric acoustic studies and advancements in speech-related research, including Automatic Speaker Verification (ASV).","Beyond enriching opera research, this dataset bridges the gap between artistic expression and technological innovation.","Pioneering the exploration of ASV in Chinese opera, we construct four test trials considering two distinct vocal manners in opera voices: stage speech (ST) and singing (S).","Implementing domain adaptation methods effectively mitigates domain mismatches induced by these vocal manner variations while there is still room for further improvement as a benchmark."],"url":"http://arxiv.org/abs/2403.13356v1","category":"eess.AS"}
{"created":"2024-03-20 06:46:01","title":"Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion Prediction with T1-MRI-based Brain Network","abstract":"Prediction the conversion to early-stage dementia is critical for mitigating its progression but remains challenging due to subtle cognitive impairments and structural brain changes. Traditional T1-weighted magnetic resonance imaging (T1-MRI) research focus on identifying brain atrophy regions but often fails to address the intricate connectivity between them. This limitation underscores the necessity of focuing on inter-regional connectivity for a comprehensive understand of the brain's complex network. Moreover, there is a pressing demand for methods that adaptively preserve and extract critical information, particularly specialized subgraph mining techniques for brain networks. These are essential for developing high-quality feature representations that reveal critical spatial impacts of structural brain changes and its topology. In this paper, we propose Brain-SubGNN, a novel graph representation network to mine and enhance critical subgraphs based on T1-MRI. This network provides a subgraph-level interpretation, enhancing interpretability and insights for graph analysis. The process begins by extracting node features and a correlation matrix between nodes to construct a task-oriented brain network. Brain-SubGNN then adaptively identifies and enhances critical subgraphs, capturing both loop and neighbor subgraphs. This method reflects the loop topology and local changes, indicative of long-range connections, and maintains local and global brain attributes. Extensive experiments validate the effectiveness and advantages of Brain-SubGNN, demonstrating its potential as a powerful tool for understanding and diagnosing early-stage dementia. Source code is available at https://github.com/Leng-10/Brain-SubGNN.","sentences":["Prediction the conversion to early-stage dementia is critical for mitigating its progression but remains challenging due to subtle cognitive impairments and structural brain changes.","Traditional T1-weighted magnetic resonance imaging (T1-MRI) research focus on identifying brain atrophy regions but often fails to address the intricate connectivity between them.","This limitation underscores the necessity of focuing on inter-regional connectivity for a comprehensive understand of the brain's complex network.","Moreover, there is a pressing demand for methods that adaptively preserve and extract critical information, particularly specialized subgraph mining techniques for brain networks.","These are essential for developing high-quality feature representations that reveal critical spatial impacts of structural brain changes and its topology.","In this paper, we propose Brain-SubGNN, a novel graph representation network to mine and enhance critical subgraphs based on T1-MRI.","This network provides a subgraph-level interpretation, enhancing interpretability and insights for graph analysis.","The process begins by extracting node features and a correlation matrix between nodes to construct a task-oriented brain network.","Brain-SubGNN then adaptively identifies and enhances critical subgraphs, capturing both loop and neighbor subgraphs.","This method reflects the loop topology and local changes, indicative of long-range connections, and maintains local and global brain attributes.","Extensive experiments validate the effectiveness and advantages of Brain-SubGNN, demonstrating its potential as a powerful tool for understanding and diagnosing early-stage dementia.","Source code is available at https://github.com/Leng-10/Brain-SubGNN."],"url":"http://arxiv.org/abs/2403.13338v1","category":"cs.CV"}
{"created":"2024-03-20 06:38:13","title":"Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection","abstract":"Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task. We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability. Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.8% to 99.2% on an in-distribution test set and from 62.9% to 72.5% on an out-of-distribution test set. The results indicate the effectiveness, good generalization ability, and great potential of adaptive ensemble algorithms in LLM-generated text detection.","sentences":["Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media.","Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task.","We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability.","Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset.","To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.8% to 99.2% on an in-distribution test set and from 62.9% to 72.5% on an out-of-distribution test set.","The results indicate the effectiveness, good generalization ability, and great potential of adaptive ensemble algorithms in LLM-generated text detection."],"url":"http://arxiv.org/abs/2403.13335v1","category":"cs.LG"}
{"created":"2024-03-20 06:19:41","title":"Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion","abstract":"High-quality scene reconstruction and novel view synthesis based on Gaussian Splatting (3DGS) typically require steady, high-quality photographs, often impractical to capture with handheld cameras. We present a method that adapts to camera motion and allows high-quality scene reconstruction with handheld video data suffering from motion blur and rolling shutter distortion. Our approach is based on detailed modelling of the physical image formation process and utilizes velocities estimated using visual-inertial odometry (VIO). Camera poses are considered non-static during the exposure time of a single image frame and camera poses are further optimized in the reconstruction process. We formulate a differentiable rendering pipeline that leverages screen space approximation to efficiently incorporate rolling-shutter and motion blur effects into the 3DGS framework. Our results with both synthetic and real data demonstrate superior performance in mitigating camera motion over existing methods, thereby advancing 3DGS in naturalistic settings.","sentences":["High-quality scene reconstruction and novel view synthesis based on Gaussian Splatting (3DGS) typically require steady, high-quality photographs, often impractical to capture with handheld cameras.","We present a method that adapts to camera motion and allows high-quality scene reconstruction with handheld video data suffering from motion blur and rolling shutter distortion.","Our approach is based on detailed modelling of the physical image formation process and utilizes velocities estimated using visual-inertial odometry (VIO).","Camera poses are considered non-static during the exposure time of a single image frame and camera poses are further optimized in the reconstruction process.","We formulate a differentiable rendering pipeline that leverages screen space approximation to efficiently incorporate rolling-shutter and motion blur effects into the 3DGS framework.","Our results with both synthetic and real data demonstrate superior performance in mitigating camera motion over existing methods, thereby advancing 3DGS in naturalistic settings."],"url":"http://arxiv.org/abs/2403.13327v1","category":"cs.CV"}
{"created":"2024-03-20 06:09:30","title":"Harnessing Large Language Models for Text-Rich Sequential Recommendation","abstract":"Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) models in user modeling, we introduce two unique summarization techniques in this paper, respectively hierarchical summarization and recurrent summarization. Then, we construct a prompt text encompassing the user preference summary, recent user interactions, and candidate item information into an LLM-based recommender, which is subsequently fine-tuned using Supervised Fine-Tuning (SFT) techniques to yield our final recommendation model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient Fine-Tuning (PEFT). We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach.","sentences":["Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS).","However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence.","This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance.","To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR).","Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks.","Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) models in user modeling, we introduce two unique summarization techniques in this paper, respectively hierarchical summarization and recurrent summarization.","Then, we construct a prompt text encompassing the user preference summary, recent user interactions, and candidate item information into an LLM-based recommender, which is subsequently fine-tuned using Supervised Fine-Tuning (SFT) techniques to yield our final recommendation model.","We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient Fine-Tuning (PEFT).","We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.13325v1","category":"cs.IR"}
{"created":"2024-03-20 05:55:13","title":"Direct search for stochastic optimization in random subspaces with zeroth-, first-, and second-order convergence and expected complexity","abstract":"The work presented here is motivated by the development of StoDARS, a framework for large-scale stochastic blackbox optimization that not only is both an algorithmic and theoretical extension of the stochastic directional direct-search (SDDS) framework but also extends to noisy objectives a recent framework of direct-search algorithms in reduced spaces (DARS). Unlike SDDS, StoDARS achieves scalability by using~$m$ search directions generated in random subspaces defined through the columns of Johnson--Lindenstrauss transforms (JLTs) obtained from Haar-distributed orthogonal matrices. For theoretical needs, the quality of these subspaces and the accuracy of random estimates used by the algorithm are required to hold with sufficiently large, but fixed, probabilities. By leveraging an existing supermartingale-based framework, the expected complexity of StoDARS is proved to be similar to that of SDDS and other stochastic full-space methods up to constants, when the objective function is continuously differentiable. By dropping the latter assumption, the convergence of StoDARS to Clarke stationary points with probability one is established. Moreover, the analysis of the second-order behavior of the mesh adaptive direct-search (MADS) algorithm using a second-order-like extension of the Rademacher's theorem-based definition of the Clarke subdifferential (so-called generalized Hessian) is extended to the StoDARS framework, making it the first in a stochastic direct-search setting, to the best of our knowledge.","sentences":["The work presented here is motivated by the development of StoDARS, a framework for large-scale stochastic blackbox optimization that not only is both an algorithmic and theoretical extension of the stochastic directional direct-search (SDDS) framework but also extends to noisy objectives a recent framework of direct-search algorithms in reduced spaces (DARS).","Unlike SDDS, StoDARS achieves scalability by using~$m$ search directions generated in random subspaces defined through the columns of Johnson--Lindenstrauss transforms (JLTs) obtained from Haar-distributed orthogonal matrices.","For theoretical needs, the quality of these subspaces and the accuracy of random estimates used by the algorithm are required to hold with sufficiently large, but fixed, probabilities.","By leveraging an existing supermartingale-based framework, the expected complexity of StoDARS is proved to be similar to that of SDDS and other stochastic full-space methods up to constants, when the objective function is continuously differentiable.","By dropping the latter assumption, the convergence of StoDARS to Clarke stationary points with probability one is established.","Moreover, the analysis of the second-order behavior of the mesh adaptive direct-search (MADS) algorithm using a second-order-like extension of the Rademacher's theorem-based definition of the Clarke subdifferential (so-called generalized Hessian) is extended to the StoDARS framework, making it the first in a stochastic direct-search setting, to the best of our knowledge."],"url":"http://arxiv.org/abs/2403.13320v1","category":"math.OC"}
{"created":"2024-03-20 05:46:56","title":"Workload Estimation for Unknown Tasks: A Survey of Machine Learning Under Distribution Shift","abstract":"Human-robot teams involve humans and robots collaborating to achieve tasks under various environmental conditions. Successful teaming will require robots to adapt autonomously to a human teammate's internal state. An important element of such adaptation is the ability to estimate the human teammates' workload in unknown situations. Existing workload models use machine learning to model the relationships between physiological metrics and workload; however, these methods are susceptible to individual differences and are heavily influenced by other factors. These methods cannot generalize to unknown tasks, as they rely on standard machine learning approaches that assume data consists of independent and identically distributed (IID) samples. This assumption does not necessarily hold for estimating workload for new tasks. A survey of non-IID machine learning techniques is presented, where commonly used techniques are evaluated using three criteria: portability, model complexity, and adaptability. These criteria are used to argue which techniques are most applicable for estimating workload for unknown tasks in dynamic, real-time environments.","sentences":["Human-robot teams involve humans and robots collaborating to achieve tasks under various environmental conditions.","Successful teaming will require robots to adapt autonomously to a human teammate's internal state.","An important element of such adaptation is the ability to estimate the human teammates' workload in unknown situations.","Existing workload models use machine learning to model the relationships between physiological metrics and workload; however, these methods are susceptible to individual differences and are heavily influenced by other factors.","These methods cannot generalize to unknown tasks, as they rely on standard machine learning approaches that assume data consists of independent and identically distributed (IID) samples.","This assumption does not necessarily hold for estimating workload for new tasks.","A survey of non-IID machine learning techniques is presented, where commonly used techniques are evaluated using three criteria: portability, model complexity, and adaptability.","These criteria are used to argue which techniques are most applicable for estimating workload for unknown tasks in dynamic, real-time environments."],"url":"http://arxiv.org/abs/2403.13318v1","category":"cs.RO"}
{"created":"2024-03-20 05:23:24","title":"Multi-Robot Connected Fermat Spiral Coverage","abstract":"We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts Connected Fermat Spiral (CFS) from the computer graphics community to multi-robot coordination for the first time. MCFS uniquely enables the orchestration of multiple robots to generate coverage paths that contour around arbitrarily shaped obstacles, a feature that is notably lacking in traditional methods. Our framework not only enhances area coverage and optimizes task performance, particularly in terms of makespan, for workspaces rich in irregular obstacles but also addresses the challenges of path continuity and curvature critical for non-holonomic robots by generating smooth paths without decomposing the workspace. MCFS solves MCPP by constructing a graph of isolines and transforming MCPP into a combinatorial optimization problem, aiming to minimize the makespan while covering all vertices. Our contributions include developing a unified CFS version for scalable and adaptable MCPP, extending it to MCPP with novel optimization techniques for cost reduction and path continuity and smoothness, and demonstrating through extensive experiments that MCFS outperforms existing MCPP methods in makespan, path curvature, coverage ratio, and overlapping ratio. Our research marks a significant step in MCPP, showcasing the fusion of computer graphics and automated planning principles to advance the capabilities of multi-robot systems in complex environments. Our code is available at https://github.com/reso1/MCFS.","sentences":["We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts Connected Fermat Spiral (CFS) from the computer graphics community to multi-robot coordination for the first time.","MCFS uniquely enables the orchestration of multiple robots to generate coverage paths that contour around arbitrarily shaped obstacles, a feature that is notably lacking in traditional methods.","Our framework not only enhances area coverage and optimizes task performance, particularly in terms of makespan, for workspaces rich in irregular obstacles but also addresses the challenges of path continuity and curvature critical for non-holonomic robots by generating smooth paths without decomposing the workspace.","MCFS solves MCPP by constructing a graph of isolines and transforming MCPP into a combinatorial optimization problem, aiming to minimize the makespan while covering all vertices.","Our contributions include developing a unified CFS version for scalable and adaptable MCPP, extending it to MCPP with novel optimization techniques for cost reduction and path continuity and smoothness, and demonstrating through extensive experiments that MCFS outperforms existing MCPP methods in makespan, path curvature, coverage ratio, and overlapping ratio.","Our research marks a significant step in MCPP, showcasing the fusion of computer graphics and automated planning principles to advance the capabilities of multi-robot systems in complex environments.","Our code is available at https://github.com/reso1/MCFS."],"url":"http://arxiv.org/abs/2403.13311v1","category":"cs.AI"}
{"created":"2024-03-20 04:56:02","title":"Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning","abstract":"The intricate process of bubble growth dynamics involves a broad spectrum of physical phenomena from microscale mechanics of bubble formation to macroscale interplay between bubbles and surrounding thermo-hydrodynamics. Traditional bubble dynamics models including atomistic approaches and continuum-based methods segment the bubble dynamics into distinct scale-specific models. In order to bridge the gap between microscale stochastic fluid models and continuum-based fluid models for bubble dynamics, we develop a composite neural operator model to unify the analysis of nonlinear bubble dynamics across microscale and macroscale regimes by integrating a many-body dissipative particle dynamics (mDPD) model with a continuum-based Rayleigh-Plesset (RP) model through a novel neural network architecture, which consists of a deep operator network for learning the mean behavior of bubble growth subject to pressure variations and a long short-term memory network for learning the statistical features of correlated fluctuations in microscale bubble dynamics. Training and testing data are generated by conducting mDPD and RP simulations for nonlinear bubble dynamics with initial bubble radii ranging from 0.1 to 1.5 micrometers. Results show that the trained composite neural operator model can accurately predict bubble dynamics across scales, with a 99% accuracy for the time evaluation of the bubble radius under varying external pressure while containing correct size-dependent stochastic fluctuations in microscale bubble growth dynamics. The composite neural operator is the first deep learning surrogate for multiscale bubble growth dynamics that can capture correct stochastic fluctuations in microscopic fluid phenomena, which sets a new direction for future research in multiscale fluid dynamics modeling.","sentences":["The intricate process of bubble growth dynamics involves a broad spectrum of physical phenomena from microscale mechanics of bubble formation to macroscale interplay between bubbles and surrounding thermo-hydrodynamics.","Traditional bubble dynamics models including atomistic approaches and continuum-based methods segment the bubble dynamics into distinct scale-specific models.","In order to bridge the gap between microscale stochastic fluid models and continuum-based fluid models for bubble dynamics, we develop a composite neural operator model to unify the analysis of nonlinear bubble dynamics across microscale and macroscale regimes by integrating a many-body dissipative particle dynamics (mDPD) model with a continuum-based Rayleigh-Plesset (RP) model through a novel neural network architecture, which consists of a deep operator network for learning the mean behavior of bubble growth subject to pressure variations and a long short-term memory network for learning the statistical features of correlated fluctuations in microscale bubble dynamics.","Training and testing data are generated by conducting mDPD and RP simulations for nonlinear bubble dynamics with initial bubble radii ranging from 0.1 to 1.5 micrometers.","Results show that the trained composite neural operator model can accurately predict bubble dynamics across scales, with a 99% accuracy for the time evaluation of the bubble radius under varying external pressure while containing correct size-dependent stochastic fluctuations in microscale bubble growth dynamics.","The composite neural operator is the first deep learning surrogate for multiscale bubble growth dynamics that can capture correct stochastic fluctuations in microscopic fluid phenomena, which sets a new direction for future research in multiscale fluid dynamics modeling."],"url":"http://arxiv.org/abs/2403.13299v1","category":"physics.flu-dyn"}
{"created":"2024-03-20 03:47:53","title":"AdaViPro: Region-based Adaptive Visual Prompt for Large-Scale Models Adapting","abstract":"Recently, prompt-based methods have emerged as a new alternative `parameter-efficient fine-tuning' paradigm, which only fine-tunes a small number of additional parameters while keeping the original model frozen. However, despite achieving notable results, existing prompt methods mainly focus on `what to add', while overlooking the equally important aspect of `where to add', typically relying on the manually crafted placement. To this end, we propose a region-based Adaptive Visual Prompt, named AdaViPro, which integrates the `where to add' optimization of the prompt into the learning process. Specifically, we reconceptualize the `where to add' optimization as a problem of regional decision-making. During inference, AdaViPro generates a regionalized mask map for the whole image, which is composed of 0 and 1, to designate whether to apply or discard the prompt in each specific area. Therefore, we employ Gumbel-Softmax sampling to enable AdaViPro's end-to-end learning through standard back-propagation. Extensive experiments demonstrate that our AdaViPro yields new efficiency and accuracy trade-offs for adapting pre-trained models.","sentences":["Recently, prompt-based methods have emerged as a new alternative `parameter-efficient fine-tuning' paradigm, which only fine-tunes a small number of additional parameters while keeping the original model frozen.","However, despite achieving notable results, existing prompt methods mainly focus on `what to add', while overlooking the equally important aspect of `where to add', typically relying on the manually crafted placement.","To this end, we propose a region-based Adaptive Visual Prompt, named AdaViPro, which integrates the `where to add' optimization of the prompt into the learning process.","Specifically, we reconceptualize the `where to add' optimization as a problem of regional decision-making.","During inference, AdaViPro generates a regionalized mask map for the whole image, which is composed of 0 and 1, to designate whether to apply or discard the prompt in each specific area.","Therefore, we employ Gumbel-Softmax sampling to enable AdaViPro's end-to-end learning through standard back-propagation.","Extensive experiments demonstrate that our AdaViPro yields new efficiency and accuracy trade-offs for adapting pre-trained models."],"url":"http://arxiv.org/abs/2403.13282v1","category":"cs.CV"}
{"created":"2024-03-20 03:07:50","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models","abstract":"We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the trainability requirements of LoRA paths at different modules and the freezing schedule for the different projection matrices. Code will be released.","sentences":["We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA).","Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector.","Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting.","Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\\times$ fewer average trainable parameters.","While compared in terms of runtime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar PEFT alternatives.","Besides the practical utility of our approach, we provide insights on the trainability requirements of LoRA paths at different modules and the freezing schedule for the different projection matrices.","Code will be released."],"url":"http://arxiv.org/abs/2403.13269v1","category":"cs.CL"}
{"created":"2024-03-20 03:07:30","title":"Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network","abstract":"Graph Neural Networks (GNNs) have shown promising performance in various graph learning tasks, but at the cost of resource-intensive computations. The primary overhead of GNN update stems from graph propagation and weight transformation, both involving operations on graph-scale matrices. Previous studies attempt to reduce the computational budget by leveraging graph-level or network-level sparsification techniques, resulting in downsized graph or weights. In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency. The entry-wise design of Unifews enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification. Theoretically, we establish a novel framework to characterize sparsified GNN learning in view of a graph optimization process, and prove that Unifews effectively approximates the learning objective with bounded error and reduced computational load. We conduct extensive experiments to evaluate the performance of our method in diverse settings. Unifews is advantageous in jointly removing more than 90% of edges and weight entries with comparable or better accuracy than baseline models. The sparsification offers remarkable efficiency improvements including 10-20x matrix operation reduction and up to 100x acceleration in graph propagation time for the largest graph at the billion-edge scale.","sentences":["Graph Neural Networks (GNNs) have shown promising performance in various graph learning tasks, but at the cost of resource-intensive computations.","The primary overhead of GNN update stems from graph propagation and weight transformation, both involving operations on graph-scale matrices.","Previous studies attempt to reduce the computational budget by leveraging graph-level or network-level sparsification techniques, resulting in downsized graph or weights.","In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency.","The entry-wise design of Unifews enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification.","Theoretically, we establish a novel framework to characterize sparsified GNN learning in view of a graph optimization process, and prove that Unifews effectively approximates the learning objective with bounded error and reduced computational load.","We conduct extensive experiments to evaluate the performance of our method in diverse settings.","Unifews is advantageous in jointly removing more than 90% of edges and weight entries with comparable or better accuracy than baseline models.","The sparsification offers remarkable efficiency improvements including 10-20x matrix operation reduction and up to 100x acceleration in graph propagation time for the largest graph at the billion-edge scale."],"url":"http://arxiv.org/abs/2403.13268v1","category":"cs.LG"}
{"created":"2024-03-20 02:32:13","title":"Frequency-aware convolution for sound event detection","abstract":"In sound event detection (SED), convolution neural networks (CNNs) are widely used to extract time-frequency patterns from the input spectrogram. However, features extracted by CNN can be insensitive to the shift of time-frequency patterns along the frequency axis. To address this issue, frequency dynamic convolution (FDY) has been proposed, which applies different kernels to different frequency components. Compared to the vannila CNN, FDY requires several times more parameters. In this paper, a more efficient solution named frequency-aware convolution (FAC) is proposed. In FAC, frequency-positional information is encoded in a vector and added to the input spectrogram. To match the amplitude of input, the encoding vector is scaled adaptively and channel-independently. Experiments are carried out in the context of DCASE 2022 task 4, and the results demonstrate that FAC can achieve comparable performance to that of FDY with only 515 additional parameters, while FDY requires 8.02 million additional parameters. The ablation study shows that scaling the encoding vector adaptively and channel-independently is critical to the performance of FAC.","sentences":["In sound event detection (SED), convolution neural networks (CNNs) are widely used to extract time-frequency patterns from the input spectrogram.","However, features extracted by CNN can be insensitive to the shift of time-frequency patterns along the frequency axis.","To address this issue, frequency dynamic convolution (FDY) has been proposed, which applies different kernels to different frequency components.","Compared to the vannila CNN, FDY requires several times more parameters.","In this paper, a more efficient solution named frequency-aware convolution (FAC) is proposed.","In FAC, frequency-positional information is encoded in a vector and added to the input spectrogram.","To match the amplitude of input, the encoding vector is scaled adaptively and channel-independently.","Experiments are carried out in the context of DCASE 2022 task 4, and the results demonstrate that FAC can achieve comparable performance to that of FDY with only 515 additional parameters, while FDY requires 8.02 million additional parameters.","The ablation study shows that scaling the encoding vector adaptively and channel-independently is critical to the performance of FAC."],"url":"http://arxiv.org/abs/2403.13252v1","category":"cs.SD"}
{"created":"2024-03-20 02:21:44","title":"A Unified and General Framework for Continual Learning","abstract":"Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. An intriguing finding is that despite their diverse origins, these methods share common mathematical structures. This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnectedness through a shared underlying optimization objective. Moreover, the proposed general framework introduces an innovative concept called refresh learning, specifically designed to enhance the CL performance. This novel approach draws inspiration from neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information. In essence, refresh learning operates by initially unlearning current data and subsequently relearning it. It serves as a versatile plug-in that seamlessly integrates with existing CL methods, offering an adaptable and effective enhancement to the learning process. Extensive experiments on CL benchmarks and theoretical analysis demonstrate the effectiveness of the proposed refresh learning. Code is available at \\url{https://github.com/joey-wang123/CL-refresh-learning}.","sentences":["Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge.","Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques.","However, these methods lack a unified framework and common terminology for describing their approaches.","This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies.","Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective.","An intriguing finding is that despite their diverse origins, these methods share common mathematical structures.","This observation highlights the compatibility of these seemingly distinct techniques, revealing their interconnectedness through a shared underlying optimization objective.","Moreover, the proposed general framework introduces an innovative concept called refresh learning, specifically designed to enhance the CL performance.","This novel approach draws inspiration from neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information.","In essence, refresh learning operates by initially unlearning current data and subsequently relearning it.","It serves as a versatile plug-in that seamlessly integrates with existing CL methods, offering an adaptable and effective enhancement to the learning process.","Extensive experiments on CL benchmarks and theoretical analysis demonstrate the effectiveness of the proposed refresh learning.","Code is available at \\url{https://github.com/joey-wang123/CL-refresh-learning}."],"url":"http://arxiv.org/abs/2403.13249v1","category":"cs.LG"}
{"created":"2024-03-20 02:16:54","title":"Federated reinforcement learning for robot motion planning with zero-shot generalization","abstract":"This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pareto improvement and optimality gap are also provided. Monte Carlo simulation is conducted to evaluate the proposed framework.","sentences":["This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments.","We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data.","In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners.","Each learner then selects between its local control policy and that from the Cloud for next iteration.","The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety.","Theoretical guarantees on almost-sure convergence, almost consensus, Pareto improvement and optimality gap are also provided.","Monte Carlo simulation is conducted to evaluate the proposed framework."],"url":"http://arxiv.org/abs/2403.13245v1","category":"eess.SY"}
{"created":"2024-03-20 02:15:55","title":"Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model","abstract":"While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also exhibits adaptability through zero-shot testing, creating molecules that satisfy combinations of properties that have not been encountered. It can comprehend text inputs with various language styles, extending beyond the confines of outlined prompts, as confirmed through empirical validation. Additionally, the knowledge distillation feature of TSMMG contributes to the continuous enhancement of small models, while the innovative approach to dataset construction effectively addresses the issues of data scarcity and quality, which positions TSMMG as a promising tool in the domains of drug discovery and materials science. Code is available at https://github.com/HHW-zhou/TSMMG.","sentences":["While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge.","Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'.","To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts.","We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively.","The model also exhibits adaptability through zero-shot testing, creating molecules that satisfy combinations of properties that have not been encountered.","It can comprehend text inputs with various language styles, extending beyond the confines of outlined prompts, as confirmed through empirical validation.","Additionally, the knowledge distillation feature of TSMMG contributes to the continuous enhancement of small models, while the innovative approach to dataset construction effectively addresses the issues of data scarcity and quality, which positions TSMMG as a promising tool in the domains of drug discovery and materials science.","Code is available at https://github.com/HHW-zhou/TSMMG."],"url":"http://arxiv.org/abs/2403.13244v1","category":"cs.CL"}
{"created":"2024-03-20 01:57:24","title":"AMCO: Adaptive Multimodal Coupling of Vision and Proprioception for Quadruped Robot Navigation in Outdoor Environments","abstract":"We present AMCO, a novel navigation method for quadruped robots that adaptively combines vision-based and proprioception-based perception capabilities. Our approach uses three cost maps: general knowledge map; traversability history map; and current proprioception map; which are derived from a robot's vision and proprioception data, and couples them to obtain a coupled traversability cost map for navigation. The general knowledge map encodes terrains semantically segmented from visual sensing, and represents a terrain's typically expected traversability. The traversability history map encodes the robot's recent proprioceptive measurements on a terrain and its semantic segmentation as a cost map. Further, the robot's present proprioceptive measurement is encoded as a cost map in the current proprioception map. As the general knowledge map and traversability history map rely on semantic segmentation, we evaluate the reliability of the visual sensory data by estimating the brightness and motion blur of input RGB images and accordingly combine the three cost maps to obtain the coupled traversability cost map used for navigation. Leveraging this adaptive coupling, the robot can depend on the most reliable input modality available. Finally, we present a novel planner that selects appropriate gaits and velocities for traversing challenging outdoor environments using the coupled traversability cost map. We demonstrate AMCO's navigation performance in different real-world outdoor environments and observe 10.8%-34.9% reduction w.r.t. two stability metrics, and up to 50% improvement in terms of success rate compared to current navigation methods.","sentences":["We present AMCO, a novel navigation method for quadruped robots that adaptively combines vision-based and proprioception-based perception capabilities.","Our approach uses three cost maps: general knowledge map; traversability history map; and current proprioception map; which are derived from a robot's vision and proprioception data, and couples them to obtain a coupled traversability cost map for navigation.","The general knowledge map encodes terrains semantically segmented from visual sensing, and represents a terrain's typically expected traversability.","The traversability history map encodes the robot's recent proprioceptive measurements on a terrain and its semantic segmentation as a cost map.","Further, the robot's present proprioceptive measurement is encoded as a cost map in the current proprioception map.","As the general knowledge map and traversability history map rely on semantic segmentation, we evaluate the reliability of the visual sensory data by estimating the brightness and motion blur of input RGB images and accordingly combine the three cost maps to obtain the coupled traversability cost map used for navigation.","Leveraging this adaptive coupling, the robot can depend on the most reliable input modality available.","Finally, we present a novel planner that selects appropriate gaits and velocities for traversing challenging outdoor environments using the coupled traversability cost map.","We demonstrate AMCO's navigation performance in different real-world outdoor environments and observe 10.8%-34.9% reduction w.r.t.","two stability metrics, and up to 50% improvement in terms of success rate compared to current navigation methods."],"url":"http://arxiv.org/abs/2403.13235v1","category":"cs.RO"}
{"created":"2024-03-20 01:07:46","title":"Modeling the Label Distributions for Weakly-Supervised Semantic Segmentation","abstract":"Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models by weak labels, which is receiving significant attention due to its low annotation cost. Existing approaches focus on generating pseudo labels for supervision while largely ignoring to leverage the inherent semantic correlation among different pseudo labels. We observe that pseudo-labeled pixels that are close to each other in the feature space are more likely to share the same class, and those closer to the distribution centers tend to have higher confidence. Motivated by this, we propose to model the underlying label distributions and employ cross-label constraints to generate more accurate pseudo labels. In this paper, we develop a unified WSSS framework named Adaptive Gaussian Mixtures Model, which leverages a GMM to model the label distributions. Specifically, we calculate the feature distribution centers of pseudo-labeled pixels and build the GMM by measuring the distance between the centers and each pseudo-labeled pixel. Then, we introduce an Online Expectation-Maximization (OEM) algorithm and a novel maximization loss to optimize the GMM adaptively, aiming to learn more discriminative decision boundaries between different class-wise Gaussian mixtures. Based on the label distributions, we leverage the GMM to generate high-quality pseudo labels for more reliable supervision. Our framework is capable of solving different forms of weak labels: image-level labels, points, scribbles, blocks, and bounding-boxes. Extensive experiments on PASCAL, COCO, Cityscapes, and ADE20K datasets demonstrate that our framework can effectively provide more reliable supervision and outperform the state-of-the-art methods under all settings. Code will be available at https://github.com/Luffy03/AGMM-SASS.","sentences":["Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models by weak labels, which is receiving significant attention due to its low annotation cost.","Existing approaches focus on generating pseudo labels for supervision while largely ignoring to leverage the inherent semantic correlation among different pseudo labels.","We observe that pseudo-labeled pixels that are close to each other in the feature space are more likely to share the same class, and those closer to the distribution centers tend to have higher confidence.","Motivated by this, we propose to model the underlying label distributions and employ cross-label constraints to generate more accurate pseudo labels.","In this paper, we develop a unified WSSS framework named Adaptive Gaussian Mixtures Model, which leverages a GMM to model the label distributions.","Specifically, we calculate the feature distribution centers of pseudo-labeled pixels and build the GMM by measuring the distance between the centers and each pseudo-labeled pixel.","Then, we introduce an Online Expectation-Maximization (OEM) algorithm and a novel maximization loss to optimize the GMM adaptively, aiming to learn more discriminative decision boundaries between different class-wise Gaussian mixtures.","Based on the label distributions, we leverage the GMM to generate high-quality pseudo labels for more reliable supervision.","Our framework is capable of solving different forms of weak labels: image-level labels, points, scribbles, blocks, and bounding-boxes.","Extensive experiments on PASCAL, COCO, Cityscapes, and ADE20K datasets demonstrate that our framework can effectively provide more reliable supervision and outperform the state-of-the-art methods under all settings.","Code will be available at https://github.com/Luffy03/AGMM-SASS."],"url":"http://arxiv.org/abs/2403.13225v1","category":"eess.IV"}
{"created":"2024-03-20 00:23:42","title":"Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy","abstract":"The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes. We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures. Nellie adapts to image metadata, eliminating user input. Nellie's preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions. Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis. Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users. We demonstrate Nellie's wide variety of use cases with two examples: unmixing multiple organelles from a single channel using feature-based classification and training an unsupervised graph autoencoder on mitochondrial multi-mesh graphs to quantify latent space embedding changes following ionomycin treatment.","sentences":["The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes.","We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures.","Nellie adapts to image metadata, eliminating user input.","Nellie's preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions.","Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation.","Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis.","Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users.","We demonstrate Nellie's wide variety of use cases with two examples: unmixing multiple organelles from a single channel using feature-based classification and training an unsupervised graph autoencoder on mitochondrial multi-mesh graphs to quantify latent space embedding changes following ionomycin treatment."],"url":"http://arxiv.org/abs/2403.13214v1","category":"cs.CV"}
{"created":"2024-03-19 23:13:40","title":"ADAPT to Robustify Prompt Tuning Vision Transformers","abstract":"The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters.","sentences":["The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks.","Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models.","These defenses require storing a copy of the entire model, that can have billions of parameters, for each task.","At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies.","In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness.","We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks.","We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm.","Our method achieves competitive robust accuracy of ~40% w.r.t.","SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters."],"url":"http://arxiv.org/abs/2403.13196v1","category":"cs.LG"}
{"created":"2024-03-19 22:57:03","title":"Reflectivity Is All You Need!: Advancing LiDAR Semantic Segmentation","abstract":"LiDAR semantic segmentation frameworks predominantly leverage geometry-based features to differentiate objects within a scan. While these methods excel in scenarios with clear boundaries and distinct shapes, their performance declines in environments where boundaries are blurred, particularly in off-road contexts. To address this, recent strides in 3D segmentation algorithms have focused on harnessing raw LiDAR intensity measurements to improve prediction accuracy. Despite these efforts, current learning-based models struggle to correlate the intricate connections between raw intensity and factors such as distance, incidence angle, material reflectivity, and atmospheric conditions. Building upon our prior work, this paper delves into the advantages of employing calibrated intensity (also referred to as reflectivity) within learning-based LiDAR semantic segmentation frameworks. We initially establish that incorporating reflectivity as an input enhances the existing LiDAR semantic segmentation model. Furthermore, we present findings that enable the model to learn to calibrate intensity can boost its performance. Through extensive experimentation on the off-road dataset Rellis-3D, we demonstrate notable improvements. Specifically, converting intensity to reflectivity results in a 4% increase in mean Intersection over Union (mIoU) when compared to using raw intensity in Off-road scenarios. Additionally, we also investigate the possible benefits of using calibrated intensity in semantic segmentation in urban environments (SemanticKITTI) and cross-sensor domain adaptation.","sentences":["LiDAR semantic segmentation frameworks predominantly leverage geometry-based features to differentiate objects within a scan.","While these methods excel in scenarios with clear boundaries and distinct shapes, their performance declines in environments where boundaries are blurred, particularly in off-road contexts.","To address this, recent strides in 3D segmentation algorithms have focused on harnessing raw LiDAR intensity measurements to improve prediction accuracy.","Despite these efforts, current learning-based models struggle to correlate the intricate connections between raw intensity and factors such as distance, incidence angle, material reflectivity, and atmospheric conditions.","Building upon our prior work, this paper delves into the advantages of employing calibrated intensity (also referred to as reflectivity) within learning-based LiDAR semantic segmentation frameworks.","We initially establish that incorporating reflectivity as an input enhances the existing LiDAR semantic segmentation model.","Furthermore, we present findings that enable the model to learn to calibrate intensity can boost its performance.","Through extensive experimentation on the off-road dataset Rellis-3D, we demonstrate notable improvements.","Specifically, converting intensity to reflectivity results in a 4% increase in mean Intersection over Union (mIoU) when compared to using raw intensity in Off-road scenarios.","Additionally, we also investigate the possible benefits of using calibrated intensity in semantic segmentation in urban environments (SemanticKITTI) and cross-sensor domain adaptation."],"url":"http://arxiv.org/abs/2403.13188v1","category":"cs.CV"}
{"created":"2024-03-19 22:18:19","title":"Fast Value Tracking for Deep Reinforcement Learning","abstract":"Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncertainties associated with the value function and model parameters but also allows us to monitor these uncertainties during policy updates throughout the training phase. The LKTD algorithm paves the way for more robust and adaptable reinforcement learning approaches.","sentences":["Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment.","However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification.","Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning.","This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters.","Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution.","This convergence not only enables us to quantify uncertainties associated with the value function and model parameters but also allows us to monitor these uncertainties during policy updates throughout the training phase.","The LKTD algorithm paves the way for more robust and adaptable reinforcement learning approaches."],"url":"http://arxiv.org/abs/2403.13178v1","category":"stat.ML"}
{"created":"2024-03-19 22:06:37","title":"User-customizable Shared Control for Fine Teleoperation via Virtual Reality","abstract":"Shared control can ease and enhance a human operator's ability to teleoperate robots, particularly for intricate tasks demanding fine control over multiple degrees of freedom. However, the arbitration process dictating how much autonomous assistance to administer in shared control can confuse novice operators and impede their understanding of the robot's behavior. To overcome these adverse side-effects, we propose a novel formulation of shared control that enables operators to tailor the arbitration to their unique capabilities and preferences. Unlike prior approaches to customizable shared control where users could indirectly modify the latent parameters of the arbitration function by issuing a feedback command, we instead make these parameters observable and directly editable via a virtual reality (VR) interface. We present our user-customizable shared control method for a teleoperation task in SE(3), known as the buzz wire game. A user study is conducted with participants teleoperating a robotic arm in VR to complete the game. The experiment spanned two weeks per subject to investigate longitudinal trends. Our findings reveal that users allowed to interactively tune the arbitration parameters across trials generalize well to adaptations in the task, exhibiting improvements in precision and fluency over direct teleoperation and conventional shared control.","sentences":["Shared control can ease and enhance a human operator's ability to teleoperate robots, particularly for intricate tasks demanding fine control over multiple degrees of freedom.","However, the arbitration process dictating how much autonomous assistance to administer in shared control can confuse novice operators and impede their understanding of the robot's behavior.","To overcome these adverse side-effects, we propose a novel formulation of shared control that enables operators to tailor the arbitration to their unique capabilities and preferences.","Unlike prior approaches to customizable shared control where users could indirectly modify the latent parameters of the arbitration function by issuing a feedback command, we instead make these parameters observable and directly editable via a virtual reality (VR) interface.","We present our user-customizable shared control method for a teleoperation task in SE(3), known as the buzz wire game.","A user study is conducted with participants teleoperating a robotic arm in VR to complete the game.","The experiment spanned two weeks per subject to investigate longitudinal trends.","Our findings reveal that users allowed to interactively tune the arbitration parameters across trials generalize well to adaptations in the task, exhibiting improvements in precision and fluency over direct teleoperation and conventional shared control."],"url":"http://arxiv.org/abs/2403.13177v1","category":"cs.RO"}
{"created":"2024-03-19 21:40:20","title":"Improved EATFormer: A Vision Transformer for Medical Image Classification","abstract":"The accurate analysis of medical images is vital for diagnosing and predicting medical conditions. Traditional approaches relying on radiologists and clinicians suffer from inconsistencies and missed diagnoses. Computer-aided diagnosis systems can assist in achieving early, accurate, and efficient diagnoses. This paper presents an improved Evolutionary Algorithm-based Transformer architecture for medical image classification using Vision Transformers. The proposed EATFormer architecture combines the strengths of Convolutional Neural Networks and Vision Transformers, leveraging their ability to identify patterns in data and adapt to specific characteristics. The architecture incorporates novel components, including the Enhanced EA-based Transformer block with Feed-Forward Network, Global and Local Interaction , and Multi-Scale Region Aggregation modules. It also introduces the Modulated Deformable MSA module for dynamic modeling of irregular locations. The paper discusses the Vision Transformer (ViT) model's key features, such as patch-based processing, positional context incorporation, and Multi-Head Attention mechanism. It introduces the Multi-Scale Region Aggregation module, which aggregates information from different receptive fields to provide an inductive bias. The Global and Local Interaction module enhances the MSA-based global module by introducing a local path for extracting discriminative local information. Experimental results on the Chest X-ray and Kvasir datasets demonstrate that the proposed EATFormer significantly improves prediction speed and accuracy compared to baseline models.","sentences":["The accurate analysis of medical images is vital for diagnosing and predicting medical conditions.","Traditional approaches relying on radiologists and clinicians suffer from inconsistencies and missed diagnoses.","Computer-aided diagnosis systems can assist in achieving early, accurate, and efficient diagnoses.","This paper presents an improved Evolutionary Algorithm-based Transformer architecture for medical image classification using Vision Transformers.","The proposed EATFormer architecture combines the strengths of Convolutional Neural Networks and Vision Transformers, leveraging their ability to identify patterns in data and adapt to specific characteristics.","The architecture incorporates novel components, including the Enhanced EA-based Transformer block with Feed-Forward Network, Global and Local Interaction , and Multi-Scale Region Aggregation modules.","It also introduces the Modulated Deformable MSA module for dynamic modeling of irregular locations.","The paper discusses the Vision Transformer (ViT) model's key features, such as patch-based processing, positional context incorporation, and Multi-Head Attention mechanism.","It introduces the Multi-Scale Region Aggregation module, which aggregates information from different receptive fields to provide an inductive bias.","The Global and Local Interaction module enhances the MSA-based global module by introducing a local path for extracting discriminative local information.","Experimental results on the Chest X-ray and Kvasir datasets demonstrate that the proposed EATFormer significantly improves prediction speed and accuracy compared to baseline models."],"url":"http://arxiv.org/abs/2403.13167v1","category":"cs.CV"}
{"created":"2024-03-19 21:31:56","title":"VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning","abstract":"Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \\emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.","sentences":["Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights.","Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding.","However, investigations into \\emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations.","The broader capabilities and limitations of multimodal ICL remain under-explored.","In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}.","We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging.","By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL.","The code and dataset are available at https://github.com/ys-zong/VL-ICL."],"url":"http://arxiv.org/abs/2403.13164v1","category":"cs.LG"}
{"created":"2024-03-19 21:24:57","title":"Revisiting shear stress tensor evolution: Non-resistive magnetohydrodynamics with momentum-dependent relaxation time","abstract":"This study aims to develop second-order relativistic viscous magnetohydrodynamics (MHD) derived from kinetic theory within an extended relaxation time approximation (momentum/energy dependent) for the collision kernel. The investigation involves a detailed examination of shear stress tensor evolution equations and associated transport coefficients. The Boltzmann equation is solved using a Chapman-Enskog-like gradient expansion for a charge-conserved conformal system, incorporating a momentum-dependent relaxation time. The derived relativistic non-resistive, viscous second-order MHD equations for the shear stress tensor reveal significant modifications in the coupling with dissipative charge current and magnetic field due to the momentum dependence of the relaxation time. By utilizing a power law parametrization to quantify the momentum dependence of the relaxation time, the anisotropic magnetic field-dependent shear coefficients in the Navier-Stokes limit have been investigated. The resulting viscous coefficients are seen to be sensitive to the momentum dependence of the relaxation time.","sentences":["This study aims to develop second-order relativistic viscous magnetohydrodynamics (MHD) derived from kinetic theory within an extended relaxation time approximation (momentum/energy dependent) for the collision kernel.","The investigation involves a detailed examination of shear stress tensor evolution equations and associated transport coefficients.","The Boltzmann equation is solved using a Chapman-Enskog-like gradient expansion for a charge-conserved conformal system, incorporating a momentum-dependent relaxation time.","The derived relativistic non-resistive, viscous second-order MHD equations for the shear stress tensor reveal significant modifications in the coupling with dissipative charge current and magnetic field due to the momentum dependence of the relaxation time.","By utilizing a power law parametrization to quantify the momentum dependence of the relaxation time, the anisotropic magnetic field-dependent shear coefficients in the Navier-Stokes limit have been investigated.","The resulting viscous coefficients are seen to be sensitive to the momentum dependence of the relaxation time."],"url":"http://arxiv.org/abs/2403.13160v1","category":"physics.plasm-ph"}
{"created":"2024-03-19 20:50:20","title":"Meta-Learning for Fast Adaptation in Intent Inferral on a Robotic Hand Orthosis for Stroke","abstract":"We propose MetaEMG, a meta-learning approach for fast adaptation in intent inferral on a robotic hand orthosis for stroke. One key challenge in machine learning for assistive and rehabilitative robotics with disabled-bodied subjects is the difficulty of collecting labeled training data. Muscle tone and spasticity often vary significantly among stroke subjects, and hand function can even change across different use sessions of the device for the same subject. We investigate the use of meta-learning to mitigate the burden of data collection needed to adapt high-capacity neural networks to a new session or subject. Our experiments on real clinical data collected from five stroke subjects show that MetaEMG can improve the intent inferral accuracy with a small session- or subject-specific dataset and very few fine-tuning epochs. To the best of our knowledge, we are the first to formulate intent inferral on stroke subjects as a meta-learning problem and demonstrate fast adaptation to a new session or subject for controlling a robotic hand orthosis with EMG signals.","sentences":["We propose MetaEMG, a meta-learning approach for fast adaptation in intent inferral on a robotic hand orthosis for stroke.","One key challenge in machine learning for assistive and rehabilitative robotics with disabled-bodied subjects is the difficulty of collecting labeled training data.","Muscle tone and spasticity often vary significantly among stroke subjects, and hand function can even change across different use sessions of the device for the same subject.","We investigate the use of meta-learning to mitigate the burden of data collection needed to adapt high-capacity neural networks to a new session or subject.","Our experiments on real clinical data collected from five stroke subjects show that MetaEMG can improve the intent inferral accuracy with a small session- or subject-specific dataset and very few fine-tuning epochs.","To the best of our knowledge, we are the first to formulate intent inferral on stroke subjects as a meta-learning problem and demonstrate fast adaptation to a new session or subject for controlling a robotic hand orthosis with EMG signals."],"url":"http://arxiv.org/abs/2403.13147v1","category":"cs.RO"}
{"created":"2024-03-19 19:57:37","title":"AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information","abstract":"Recent advancements in large-scale pretrained models have significantly improved performance across a variety of tasks in natural language processing and computer vision. However, the extensive number of parameters in these models necessitates substantial memory and computational resources for full training. To adapt these models for downstream tasks or specific application-oriented datasets, parameter-efficient fine-tuning methods leveraging pretrained parameters have gained considerable attention. However, it can still be time-consuming due to lots of parameters and epochs. In this work, we introduce AdaFish, an efficient algorithm of the second-order type designed to expedite the training process within low-rank decomposition-based fine-tuning frameworks. Our key observation is that the associated generalized Fisher information matrix is either low-rank or extremely small-scaled. Such a generalized Fisher information matrix is shown to be equivalent to the Hessian matrix. Moreover, we prove the global convergence of AdaFish, along with its iteration/oracle complexity. Numerical experiments show that our algorithm is quite competitive with the state-of-the-art AdamW method.","sentences":["Recent advancements in large-scale pretrained models have significantly improved performance across a variety of tasks in natural language processing and computer vision.","However, the extensive number of parameters in these models necessitates substantial memory and computational resources for full training.","To adapt these models for downstream tasks or specific application-oriented datasets, parameter-efficient fine-tuning methods leveraging pretrained parameters have gained considerable attention.","However, it can still be time-consuming due to lots of parameters and epochs.","In this work, we introduce AdaFish, an efficient algorithm of the second-order type designed to expedite the training process within low-rank decomposition-based fine-tuning frameworks.","Our key observation is that the associated generalized Fisher information matrix is either low-rank or extremely small-scaled.","Such a generalized Fisher information matrix is shown to be equivalent to the Hessian matrix.","Moreover, we prove the global convergence of AdaFish, along with its iteration/oracle complexity.","Numerical experiments show that our algorithm is quite competitive with the state-of-the-art AdamW method."],"url":"http://arxiv.org/abs/2403.13128v1","category":"cs.LG"}
{"created":"2024-03-19 19:09:34","title":"Uniform vorticity depletion and inviscid damping for periodic shear flows in the high Reynolds number regime","abstract":"We study the dynamics of the two dimensional Navier-Stokes equations linearized around a shear flow on a (non-square) torus which possesses exactly two non-degenerate critical points. We obtain linear inviscid damping and vorticity depletion estimates for the linearized flow that are uniform with respect to the viscosity, and enhanced dissipation type decay estimates. The main task is to understand the associated Rayleigh and Orr-Sommerfeld equations, under the natural assumption that the linearized operator around the shear flow in the inviscid case has no discrete eigenvalues. The key difficulty is to understand the behavior of the solution to Orr-Sommerfeld equations in three distinct regimes depending on the spectral parameter: the non-degenerate case when the spectral parameter is away from the critical values, the intermediate case when the spectral parameter is close to but still separated from the critical values, and the most singular case when the spectral parameter is inside the viscous layer.","sentences":["We study the dynamics of the two dimensional Navier-Stokes equations linearized around a shear flow on a (non-square) torus which possesses exactly two non-degenerate critical points.","We obtain linear inviscid damping and vorticity depletion estimates for the linearized flow that are uniform with respect to the viscosity, and enhanced dissipation type decay estimates.","The main task is to understand the associated Rayleigh and Orr-Sommerfeld equations, under the natural assumption that the linearized operator around the shear flow in the inviscid case has no discrete eigenvalues.","The key difficulty is to understand the behavior of the solution to Orr-Sommerfeld equations in three distinct regimes depending on the spectral parameter: the non-degenerate case when the spectral parameter is away from the critical values, the intermediate case when the spectral parameter is close to but still separated from the critical values, and the most singular case when the spectral parameter is inside the viscous layer."],"url":"http://arxiv.org/abs/2403.13104v1","category":"math.AP"}
{"created":"2024-03-19 19:05:24","title":"AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks","abstract":"The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance communication-computing latency and training convergence. Extensive simulations across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve a target accuracy than benchmarks, demonstrating the effectiveness of the proposed strategies.","sentences":["The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices.","To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices.","However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted.","In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation.","Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems.","Specifically, AdaptSFL adaptively controls client-side MA and MS to balance communication-computing latency and training convergence.","Extensive simulations across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve a target accuracy than benchmarks, demonstrating the effectiveness of the proposed strategies."],"url":"http://arxiv.org/abs/2403.13101v1","category":"cs.LG"}
{"created":"2024-03-19 19:00:43","title":"Interspecific dispersal constraints suppress pattern formation in metacommunities","abstract":"Decisions to disperse from a habitat stand out among organismal behaviors as pivotal drivers of ecosystem dynamics across scales. Encounters with other species are an important component of adaptive decision-making in dispersal, resulting in widespread behaviors like tracking resources or avoiding consumers in space. Despite this, metacommunity models often treat dispersal as a function of intraspecific density alone. We show, focusing initially on three-species network motifs, that interspecific dispersal rules generally drive a transition in metacommunities from homogeneous steady states to self-organized heterogeneous spatial patterns. However, when ecologically realistic constraints reflecting adaptive behaviors are imposed -- prey tracking and predator avoidance -- a pronounced homogenizing effect emerges where spatial pattern formation is suppressed. We demonstrate this effect for each motif by computing master stability functions that separate the contributions of local and spatial interactions to pattern formation. We extend this result to species rich food webs using a random matrix approach, where we find that eventually webs become large enough to override the homogenizing effect of adaptive dispersal behaviors, leading once again to predominately pattern forming dynamics. Our results emphasize the critical role of interspecific dispersal rules in shaping spatial patterns across landscapes, highlighting the need to incorporate adaptive behavioral constraints in efforts to link local species interactions and metacommunity structure.","sentences":["Decisions to disperse from a habitat stand out among organismal behaviors as pivotal drivers of ecosystem dynamics across scales.","Encounters with other species are an important component of adaptive decision-making in dispersal, resulting in widespread behaviors like tracking resources or avoiding consumers in space.","Despite this, metacommunity models often treat dispersal as a function of intraspecific density alone.","We show, focusing initially on three-species network motifs, that interspecific dispersal rules generally drive a transition in metacommunities from homogeneous steady states to self-organized heterogeneous spatial patterns.","However, when ecologically realistic constraints reflecting adaptive behaviors are imposed -- prey tracking and predator avoidance -- a pronounced homogenizing effect emerges where spatial pattern formation is suppressed.","We demonstrate this effect for each motif by computing master stability functions that separate the contributions of local and spatial interactions to pattern formation.","We extend this result to species rich food webs using a random matrix approach, where we find that eventually webs become large enough to override the homogenizing effect of adaptive dispersal behaviors, leading once again to predominately pattern forming dynamics.","Our results emphasize the critical role of interspecific dispersal rules in shaping spatial patterns across landscapes, highlighting the need to incorporate adaptive behavioral constraints in efforts to link local species interactions and metacommunity structure."],"url":"http://arxiv.org/abs/2403.13098v1","category":"q-bio.PE"}
{"created":"2024-03-19 18:38:50","title":"Digital Twin-Driven Reinforcement Learning for Obstacle Avoidance in Robot Manipulators: A Self-Improving Online Training Framework","abstract":"The evolution and growing automation of collaborative robots introduce more complexity and unpredictability to systems, highlighting the crucial need for robot's adaptability and flexibility to address the increasing complexities of their environment. In typical industrial production scenarios, robots are often required to be re-programmed when facing a more demanding task or even a few changes in workspace conditions. To increase productivity, efficiency and reduce human effort in the design process, this paper explores the potential of using digital twin combined with Reinforcement Learning (RL) to enable robots to generate self-improving collision-free trajectories in real time. The digital twin, acting as a virtual counterpart of the physical system, serves as a 'forward run' for monitoring, controlling, and optimizing the physical system in a safe and cost-effective manner. The physical system sends data to synchronize the digital system through the video feeds from cameras, which allows the virtual robot to update its observation and policy based on real scenarios. The bidirectional communication between digital and physical systems provides a promising platform for hardware-in-the-loop RL training through trial and error until the robot successfully adapts to its new environment. The proposed online training framework is demonstrated on the Unfactory Xarm5 collaborative robot, where the robot end-effector aims to reach the target position while avoiding obstacles. The experiment suggest that proposed framework is capable of performing policy online training, and that there remains significant room for improvement.","sentences":["The evolution and growing automation of collaborative robots introduce more complexity and unpredictability to systems, highlighting the crucial need for robot's adaptability and flexibility to address the increasing complexities of their environment.","In typical industrial production scenarios, robots are often required to be re-programmed when facing a more demanding task or even a few changes in workspace conditions.","To increase productivity, efficiency and reduce human effort in the design process, this paper explores the potential of using digital twin combined with Reinforcement Learning (RL) to enable robots to generate self-improving collision-free trajectories in real time.","The digital twin, acting as a virtual counterpart of the physical system, serves as a 'forward run' for monitoring, controlling, and optimizing the physical system in a safe and cost-effective manner.","The physical system sends data to synchronize the digital system through the video feeds from cameras, which allows the virtual robot to update its observation and policy based on real scenarios.","The bidirectional communication between digital and physical systems provides a promising platform for hardware-in-the-loop RL training through trial and error until the robot successfully adapts to its new environment.","The proposed online training framework is demonstrated on the Unfactory Xarm5 collaborative robot, where the robot end-effector aims to reach the target position while avoiding obstacles.","The experiment suggest that proposed framework is capable of performing policy online training, and that there remains significant room for improvement."],"url":"http://arxiv.org/abs/2403.13090v1","category":"cs.RO"}
{"created":"2024-03-19 18:26:39","title":"Parameter Estimation from Single Patient, Single Time-Point Sequencing Data of Recurrent Tumors","abstract":"In this study, we develop consistent estimators for key parameters that govern the dynamics of tumor cell populations when subjected to pharmacological treatments. While these treatments often lead to an initial reduction in the abundance of drug-sensitive cells, a population of drug-resistant cells frequently emerges over time, resulting in cancer recurrence. Samples from recurrent tumors present as an invaluable data source that can offer crucial insights into the ability of cancer cells to adapt and withstand treatment interventions. To effectively utilize the data obtained from recurrent tumors, we derive several large number limit theorems, specifically focusing on the metrics that quantify the clonal diversity of cancer cell populations at the time of cancer recurrence. These theorems then serve as the foundation for constructing our estimators. A distinguishing feature of our approach is that our estimators only require a single time-point sequencing data from a single tumor, thereby enhancing the practicality of our approach and enabling the understanding of cancer recurrence at the individual level.","sentences":["In this study, we develop consistent estimators for key parameters that govern the dynamics of tumor cell populations when subjected to pharmacological treatments.","While these treatments often lead to an initial reduction in the abundance of drug-sensitive cells, a population of drug-resistant cells frequently emerges over time, resulting in cancer recurrence.","Samples from recurrent tumors present as an invaluable data source that can offer crucial insights into the ability of cancer cells to adapt and withstand treatment interventions.","To effectively utilize the data obtained from recurrent tumors, we derive several large number limit theorems, specifically focusing on the metrics that quantify the clonal diversity of cancer cell populations at the time of cancer recurrence.","These theorems then serve as the foundation for constructing our estimators.","A distinguishing feature of our approach is that our estimators only require a single time-point sequencing data from a single tumor, thereby enhancing the practicality of our approach and enabling the understanding of cancer recurrence at the individual level."],"url":"http://arxiv.org/abs/2403.13081v1","category":"stat.AP"}
{"created":"2024-03-19 18:15:35","title":"Current-Based Impedance Control for Interacting with Mobile Manipulators","abstract":"As robots shift from industrial to human-centered spaces, adopting mobile manipulators, which expand workspace capabilities, becomes crucial. In these settings, seamless interaction with humans necessitates compliant control. Two common methods for safe interaction, admittance, and impedance control, require force or torque sensors, often absent in lower-cost or lightweight robots. This paper presents an adaption of impedance control that can be used on current-controlled robots without the use of force or torque sensors and its application for compliant control of a mobile manipulator. A calibration method is designed that enables estimation of the actuators' current/torque ratios and frictions, used by the adapted impedance controller, and that can handle model errors. The calibration method and the performance of the designed controller are experimentally validated using the Kinova GEN3 Lite arm. Results show that the calibration method is consistent and that the designed controller for the arm is compliant while also being able to track targets with five-millimeter precision when no interaction is present. Additionally, this paper presents two operational modes for interacting with the mobile manipulator: one for guiding the robot around the workspace through interacting with the arm and another for executing a tracking task, both maintaining compliance to external forces. These operational modes were tested in real-world experiments, affirming their practical applicability and effectiveness.","sentences":["As robots shift from industrial to human-centered spaces, adopting mobile manipulators, which expand workspace capabilities, becomes crucial.","In these settings, seamless interaction with humans necessitates compliant control.","Two common methods for safe interaction, admittance, and impedance control, require force or torque sensors, often absent in lower-cost or lightweight robots.","This paper presents an adaption of impedance control that can be used on current-controlled robots without the use of force or torque sensors and its application for compliant control of a mobile manipulator.","A calibration method is designed that enables estimation of the actuators' current/torque ratios and frictions, used by the adapted impedance controller, and that can handle model errors.","The calibration method and the performance of the designed controller are experimentally validated using the Kinova GEN3 Lite arm.","Results show that the calibration method is consistent and that the designed controller for the arm is compliant while also being able to track targets with five-millimeter precision when no interaction is present.","Additionally, this paper presents two operational modes for interacting with the mobile manipulator: one for guiding the robot around the workspace through interacting with the arm and another for executing a tracking task, both maintaining compliance to external forces.","These operational modes were tested in real-world experiments, affirming their practical applicability and effectiveness."],"url":"http://arxiv.org/abs/2403.13079v1","category":"cs.RO"}
{"created":"2024-03-19 18:01:29","title":"SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model","abstract":"We introduce SceneScript, a method that directly produces full scene models as a sequence of structured language commands using an autoregressive, token-based approach. Our proposed scene representation is inspired by recent successes in transformers & LLMs, and departs from more traditional methods which commonly describe scenes as meshes, voxel grids, point clouds or radiance fields. Our method infers the set of structured language commands directly from encoded visual data using a scene language encoder-decoder architecture. To train SceneScript, we generate and release a large-scale synthetic dataset called Aria Synthetic Environments consisting of 100k high-quality in-door scenes, with photorealistic and ground-truth annotated renders of egocentric scene walkthroughs. Our method gives state-of-the art results in architectural layout estimation, and competitive results in 3D object detection. Lastly, we explore an advantage for SceneScript, which is the ability to readily adapt to new commands via simple additions to the structured language, which we illustrate for tasks such as coarse 3D object part reconstruction.","sentences":["We introduce SceneScript, a method that directly produces full scene models as a sequence of structured language commands using an autoregressive, token-based approach.","Our proposed scene representation is inspired by recent successes in transformers & LLMs, and departs from more traditional methods which commonly describe scenes as meshes, voxel grids, point clouds or radiance fields.","Our method infers the set of structured language commands directly from encoded visual data using a scene language encoder-decoder architecture.","To train SceneScript, we generate and release a large-scale synthetic dataset called Aria Synthetic Environments consisting of 100k high-quality in-door scenes, with photorealistic and ground-truth annotated renders of egocentric scene walkthroughs.","Our method gives state-of-the art results in architectural layout estimation, and competitive results in 3D object detection.","Lastly, we explore an advantage for SceneScript, which is the ability to readily adapt to new commands via simple additions to the structured language, which we illustrate for tasks such as coarse 3D object part reconstruction."],"url":"http://arxiv.org/abs/2403.13064v1","category":"cs.CV"}
{"created":"2024-03-19 17:59:58","title":"Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos","abstract":"We propose a generative model that, given a coarsely edited image, synthesizes a photorealistic output that follows the prescribed layout. Our method transfers fine details from the original image and preserves the identity of its parts. Yet, it adapts it to the lighting and context defined by the new layout. Our key insight is that videos are a powerful source of supervision for this task: objects and camera motions provide many observations of how the world changes with viewpoint, lighting, and physical interactions. We construct an image dataset in which each sample is a pair of source and target frames extracted from the same video at randomly chosen time intervals. We warp the source frame toward the target using two motion models that mimic the expected test-time user edits. We supervise our model to translate the warped image into the ground truth, starting from a pretrained diffusion model. Our model design explicitly enables fine detail transfer from the source frame to the generated image, while closely following the user-specified layout. We show that by using simple segmentations and coarse 2D manipulations, we can synthesize a photorealistic edit faithful to the user's input while addressing second-order effects like harmonizing the lighting and physical interactions between edited objects.","sentences":["We propose a generative model that, given a coarsely edited image, synthesizes a photorealistic output that follows the prescribed layout.","Our method transfers fine details from the original image and preserves the identity of its parts.","Yet, it adapts it to the lighting and context defined by the new layout.","Our key insight is that videos are a powerful source of supervision for this task: objects and camera motions provide many observations of how the world changes with viewpoint, lighting, and physical interactions.","We construct an image dataset in which each sample is a pair of source and target frames extracted from the same video at randomly chosen time intervals.","We warp the source frame toward the target using two motion models that mimic the expected test-time user edits.","We supervise our model to translate the warped image into the ground truth, starting from a pretrained diffusion model.","Our model design explicitly enables fine detail transfer from the source frame to the generated image, while closely following the user-specified layout.","We show that by using simple segmentations and coarse 2D manipulations, we can synthesize a photorealistic edit faithful to the user's input while addressing second-order effects like harmonizing the lighting and physical interactions between edited objects."],"url":"http://arxiv.org/abs/2403.13044v1","category":"cs.CV"}
{"created":"2024-03-20 17:43:58","title":"Sparse Implementation of Versatile Graph-Informed Layers","abstract":"Graph Neural Networks (GNNs) have emerged as effective tools for learning tasks on graph-structured data. Recently, Graph-Informed (GI) layers were introduced to address regression tasks on graph nodes, extending their applicability beyond classic GNNs. However, existing implementations of GI layers lack efficiency due to dense memory allocation. This paper presents a sparse implementation of GI layers, leveraging the sparsity of adjacency matrices to reduce memory usage significantly. Additionally, a versatile general form of GI layers is introduced, enabling their application to subsets of graph nodes. The proposed sparse implementation improves the concrete computational efficiency and scalability of the GI layers, permitting to build deeper Graph-Informed Neural Networks (GINNs) and facilitating their scalability to larger graphs.","sentences":["Graph Neural Networks (GNNs) have emerged as effective tools for learning tasks on graph-structured data.","Recently, Graph-Informed (GI) layers were introduced to address regression tasks on graph nodes, extending their applicability beyond classic GNNs.","However, existing implementations of GI layers lack efficiency due to dense memory allocation.","This paper presents a sparse implementation of GI layers, leveraging the sparsity of adjacency matrices to reduce memory usage significantly.","Additionally, a versatile general form of GI layers is introduced, enabling their application to subsets of graph nodes.","The proposed sparse implementation improves the concrete computational efficiency and scalability of the GI layers, permitting to build deeper Graph-Informed Neural Networks (GINNs) and facilitating their scalability to larger graphs."],"url":"http://arxiv.org/abs/2403.13781v1","category":"cs.LG"}
{"created":"2024-03-20 17:36:31","title":"Accurate heat currents via reorganised master equation","abstract":"The accurate characterisation of energy exchanges between nanoscale quantum systems and their environments is of paramount importance for quantum technologies, and central to quantum thermodynamics. Here, we show that, in order to accurately approximate steady-state heat currents via perturbative master equations, the coupling-induced reorganisation correction to the system's energy must be carefully taken into account. Not doing so, may yield sizeable errors, especially at low, or even moderate temperatures. In particular, we show how a 'reorganised master equation' can produce very accurate estimates for the heat currents when the reorganisation energy is weak and one works with environments with a broad spectrum. Notably, such master equation outperforms its 'non-reorganised' counterpart in the calculation of heat currents, at modelling dynamics, and at correctly capturing equilibration. This is so even if both types of equation are derived to the same order of perturbation theory. Most importantly, working with reorganised master equations does not involve additional complications when compared with alternative approaches. Also, invoking the secular approximation to secure thermodynamic consistency does not compromise their precision.","sentences":["The accurate characterisation of energy exchanges between nanoscale quantum systems and their environments is of paramount importance for quantum technologies, and central to quantum thermodynamics.","Here, we show that, in order to accurately approximate steady-state heat currents via perturbative master equations, the coupling-induced reorganisation correction to the system's energy must be carefully taken into account.","Not doing so, may yield sizeable errors, especially at low, or even moderate temperatures.","In particular, we show how a 'reorganised master equation' can produce very accurate estimates for the heat currents when the reorganisation energy is weak and one works with environments with a broad spectrum.","Notably, such master equation outperforms its 'non-reorganised' counterpart in the calculation of heat currents, at modelling dynamics, and at correctly capturing equilibration.","This is so even if both types of equation are derived to the same order of perturbation theory.","Most importantly, working with reorganised master equations does not involve additional complications when compared with alternative approaches.","Also, invoking the secular approximation to secure thermodynamic consistency does not compromise their precision."],"url":"http://arxiv.org/abs/2403.13776v1","category":"quant-ph"}
{"created":"2024-03-20 17:27:59","title":"Positive sectional curvature is not preserved under the Ricci flow in dimensions seven and thirteen","abstract":"We prove that there exist $\\mathsf{SU}_{3}$-invariant metrics on Aloff-Wallach spaces $W^7_{k_1, k_2}$, as well as $\\mathsf{SU}_{5}$-invariant metrics on the Berger space $B^{13}$, which have positive sectional curvature and evolve under the Ricci flow to metrics with non-positively curved planes.","sentences":["We prove that there exist $\\mathsf{SU}_{3}$-invariant metrics on Aloff-Wallach spaces $W^7_{k_1, k_2}$, as well as $\\mathsf{SU}_{5}$-invariant metrics on the Berger space $B^{13}$, which have positive sectional curvature and evolve under the Ricci flow to metrics with non-positively curved planes."],"url":"http://arxiv.org/abs/2403.13764v1","category":"math.DG"}
{"created":"2024-03-20 17:10:31","title":"Scale Invariance at the Edge","abstract":"Some aspects of the theory of fermions living on three dimensional spacetime with a flat co-dimension one boundary are discussed, particularly a case where the boundary condition preserves scale and translation invariance but violates the residual Lorentz and conformal symmetries. This case is interesting because the Dirac equation has normalizable stationary edge states which must be taken into account in the quantization. We show that a consequence of the edge states for quantization of the Dirac field is that there are no states of the Dirac field theory which are simultaneously scale, C and P invariant even when these are good symmetries of the theory. The scale invariant states of the Dirac field contain either a nonzero scale covariant momentum density or a U(1) charge density concentrated near the edge.","sentences":["Some aspects of the theory of fermions living on three dimensional spacetime with a flat co-dimension one boundary are discussed, particularly a case where the boundary condition preserves scale and translation invariance but violates the residual Lorentz and conformal symmetries.","This case is interesting because the Dirac equation has normalizable stationary edge states which must be taken into account in the quantization.","We show that a consequence of the edge states for quantization of the Dirac field is that there are no states of the Dirac field theory which are simultaneously scale, C and P invariant even when these are good symmetries of the theory.","The scale invariant states of the Dirac field contain either a nonzero scale covariant momentum density or a U(1) charge density concentrated near the edge."],"url":"http://arxiv.org/abs/2403.13758v1","category":"hep-th"}
{"created":"2024-03-20 17:08:45","title":"Heavy States in 3d Gravity and 2d CFT","abstract":"We discuss correlators of light fields in heavy states in 3d gravity and holographic 2d CFTs. In the bulk, the propagator of free fields in AdS backgrounds containing a conical defect or a BTZ black hole can be obtained by solving the wave equation, as well as by the method of images. On the boundary, these geometries are sourced by heavy operator insertions, and the propagator is dual to a heavy-light (HHLL) correlator. By matching its expansion in Virasoro blocks to our bulk results, we determine the OPE coefficients of all contributing states in both the s and t channels. In the s channel, these states are excitations of the light field on top of the heavy state, and their OPE coefficients are the amplitudes to create them. The t-channel OPE is dominated by the Virasoro vacuum block, but there is also an infinite family of light two-particle states that contribute to the correlator. The OPE coefficients that couple these states to heavy operators represent their expectation values in heavy backgrounds. We determine them exactly, derive their asymptotic form at large twist, and discuss their behavior near and above the BTZ threshold, where they become thermal one-point functions.","sentences":["We discuss correlators of light fields in heavy states in 3d gravity and holographic 2d CFTs.","In the bulk, the propagator of free fields in AdS backgrounds containing a conical defect or a BTZ black hole can be obtained by solving the wave equation, as well as by the method of images.","On the boundary, these geometries are sourced by heavy operator insertions, and the propagator is dual to a heavy-light (HHLL) correlator.","By matching its expansion in Virasoro blocks to our bulk results, we determine the OPE coefficients of all contributing states in both the s and t channels.","In the s channel, these states are excitations of the light field on top of the heavy state, and their OPE coefficients are the amplitudes to create them.","The t-channel OPE is dominated by the Virasoro vacuum block, but there is also an infinite family of light two-particle states that contribute to the correlator.","The OPE coefficients that couple these states to heavy operators represent their expectation values in heavy backgrounds.","We determine them exactly, derive their asymptotic form at large twist, and discuss their behavior near and above the BTZ threshold, where they become thermal one-point functions."],"url":"http://arxiv.org/abs/2403.13757v1","category":"hep-th"}
{"created":"2024-03-20 16:54:55","title":"Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval","abstract":"Deep hashing techniques have emerged as the predominant approach for efficient image retrieval. Traditionally, these methods utilize pre-trained convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature extractors. However, the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval. In this study, we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks. Specifically, we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task, termed High-Resolution Hashing Network (HHNet). Our approach demonstrates superior performance compared to existing methods across all tested benchmark datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance improvement is more pronounced for complex datasets, which highlights the need to learn high-resolution features for intricate image retrieval tasks. Furthermore, we conduct a comprehensive analysis of different HRNet configurations and provide insights into the optimal architecture for the deep hashing task","sentences":["Deep hashing techniques have emerged as the predominant approach for efficient image retrieval.","Traditionally, these methods utilize pre-trained convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature extractors.","However, the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval.","In this study, we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks.","Specifically, we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task, termed High-Resolution Hashing Network (HHNet).","Our approach demonstrates superior performance compared to existing methods across all tested benchmark datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet.","This performance improvement is more pronounced for complex datasets, which highlights the need to learn high-resolution features for intricate image retrieval tasks.","Furthermore, we conduct a comprehensive analysis of different HRNet configurations and provide insights into the optimal architecture for the deep hashing task"],"url":"http://arxiv.org/abs/2403.13747v1","category":"cs.CV"}
{"created":"2024-03-20 16:47:28","title":"Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks","abstract":"The lack of transparency of Deep Neural Networks continues to be a limitation that severely undermines their reliability and usage in high-stakes applications. Promising approaches to overcome such limitations are Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions rely on the similarity between the input at hand and a set of prototypical representations of the output classes, offering therefore a deep, yet transparent-by-design, architecture. So far, such models have been designed by considering pointwise estimates for the prototypes, which remain fixed after the learning phase of the model. In this paper, we introduce a probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for the prototypes with probability distributions over their values. This provides not only a more flexible framework for an end-to-end learning of prototypes, but can also capture the explanatory uncertainty of the model, which is a missing feature in previous approaches. In addition, since the prototypes determine both the explanation and the prediction, Prob-PSENNs allow us to detect when the model is making uninformed or uncertain predictions, and to obtain valid explanations for them. Our experiments demonstrate that Prob-PSENNs provide more meaningful and robust explanations than their non-probabilistic counterparts, thus enhancing the explainability and reliability of the models.","sentences":["The lack of transparency of Deep Neural Networks continues to be a limitation that severely undermines their reliability and usage in high-stakes applications.","Promising approaches to overcome such limitations are Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions rely on the similarity between the input at hand and a set of prototypical representations of the output classes, offering therefore a deep, yet transparent-by-design, architecture.","So far, such models have been designed by considering pointwise estimates for the prototypes, which remain fixed after the learning phase of the model.","In this paper, we introduce a probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for the prototypes with probability distributions over their values.","This provides not only a more flexible framework for an end-to-end learning of prototypes, but can also capture the explanatory uncertainty of the model, which is a missing feature in previous approaches.","In addition, since the prototypes determine both the explanation and the prediction, Prob-PSENNs allow us to detect when the model is making uninformed or uncertain predictions, and to obtain valid explanations for them.","Our experiments demonstrate that Prob-PSENNs provide more meaningful and robust explanations than their non-probabilistic counterparts, thus enhancing the explainability and reliability of the models."],"url":"http://arxiv.org/abs/2403.13740v1","category":"cs.LG"}
{"created":"2024-03-20 16:41:59","title":"Existence and uniqueness of the Levi-Civita connection on noncommutative differential forms","abstract":"We combine Hilbert module and algebraic techniques to give necessary and sufficient conditions for the existence of an Hermitian torsion-free connection on the bimodule of differential one-forms of a first order differential calculus. In the presence of the extra structure of a bimodule connection, we give sufficient conditions for uniqueness. We prove that any $\\theta$-deformation of a compact Riemannian manifold admits a unique Hermitian torsion-free bimodule connection and provide an explicit construction of it. Specialising to classical Riemannian manifolds yields a novel construction of the Levi-Civita connection on the cotangent bundle.","sentences":["We combine Hilbert module and algebraic techniques to give necessary and sufficient conditions for the existence of an Hermitian torsion-free connection on the bimodule of differential one-forms of a first order differential calculus.","In the presence of the extra structure of a bimodule connection, we give sufficient conditions for uniqueness.","We prove that any $\\theta$-deformation of a compact Riemannian manifold admits a unique Hermitian torsion-free bimodule connection and provide an explicit construction of it.","Specialising to classical Riemannian manifolds yields a novel construction of the Levi-Civita connection on the cotangent bundle."],"url":"http://arxiv.org/abs/2403.13735v1","category":"math.QA"}
{"created":"2024-03-20 16:33:06","title":"Probabilistic Forecasting with Stochastic Interpolants and F\u00f6llmer Processes","abstract":"We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by square loss regression over the time-series data. We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a F\\\"ollmer process. We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets.","sentences":["We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling.","Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state.","To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target.","We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias.","This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts.","We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by square loss regression over the time-series data.","We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a F\\\"ollmer process.","We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets."],"url":"http://arxiv.org/abs/2403.13724v1","category":"cs.LG"}
{"created":"2024-03-20 16:27:13","title":"UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge","abstract":"We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge. The challenge focuses on using discrete speech unit learned from large speech corpora for some tasks. We submitted our UTDUSS system to two text-to-speech tracks: Vocoder and Acoustic+Vocoder. Our system incorporates neural audio codec (NAC) pre-trained on only speech corpora, which makes the learned codec represent rich acoustic features that are necessary for high-fidelity speech reconstruction. For the acoustic+vocoder track, we trained an acoustic model based on Transformer encoder-decoder that predicted the pre-trained NAC tokens from text input. We describe our strategies to build these models, such as data selection, downsampling, and hyper-parameter tuning. Our system ranked in second and first for the Vocoder and Acoustic+Vocoder tracks, respectively.","sentences":["We present UTDUSS, the UTokyo-SaruLab system submitted to Interspeech2024","Speech Processing Using Discrete Speech Unit Challenge.","The challenge focuses on using discrete speech unit learned from large speech corpora for some tasks.","We submitted our UTDUSS system to two text-to-speech tracks: Vocoder and Acoustic+Vocoder.","Our system incorporates neural audio codec (NAC) pre-trained on only speech corpora, which makes the learned codec represent rich acoustic features that are necessary for high-fidelity speech reconstruction.","For the acoustic+vocoder track, we trained an acoustic model based on Transformer encoder-decoder that predicted the pre-trained NAC tokens from text input.","We describe our strategies to build these models, such as data selection, downsampling, and hyper-parameter tuning.","Our system ranked in second and first for the Vocoder and Acoustic+Vocoder tracks, respectively."],"url":"http://arxiv.org/abs/2403.13720v1","category":"cs.SD"}
{"created":"2024-03-20 16:23:20","title":"Paving Matroids: Defining Equations and Associated Varieties","abstract":"We study paving matroids, their realization spaces, and their closures, along with matroid varieties and circuit varieties. Within this context, we introduce three distinct methods for generating polynomials within the associated ideals of these varieties across any dimension. Additionally, we explain the relationship between polynomials constructed using these different methods. We then compute a comprehensive and finite set of defining equations for matroid varieties associated with specific classes of paving matroids. Finally, we focus on the class of paving matroids of rank $3$, known as point-line configurations, which essentially contain simple matroids of rank $3$. Furthermore, we provide a decomposition for the associated circuit variety of point-line configurations, where all points have a degree less than $3$. Lastly, we present several examples applying our results and compare them with the known cases in the literature.","sentences":["We study paving matroids, their realization spaces, and their closures, along with matroid varieties and circuit varieties.","Within this context, we introduce three distinct methods for generating polynomials within the associated ideals of these varieties across any dimension.","Additionally, we explain the relationship between polynomials constructed using these different methods.","We then compute a comprehensive and finite set of defining equations for matroid varieties associated with specific classes of paving matroids.","Finally, we focus on the class of paving matroids of rank $3$, known as point-line configurations, which essentially contain simple matroids of rank $3$. Furthermore, we provide a decomposition for the associated circuit variety of point-line configurations, where all points have a degree less than $3$. Lastly, we present several examples applying our results and compare them with the known cases in the literature."],"url":"http://arxiv.org/abs/2403.13718v1","category":"math.AG"}
{"created":"2024-03-20 16:11:09","title":"Non-fluid like Boltzmann code architecture for early times f(T) cosmologies","abstract":"There have been several works that have studied scalar cosmological perturbations in $f(T)$ teleparallel gravity theories to understand early cosmic times dynamics. In this direction, the perturbations presented have been performed by considering $f(T)$ extensions as an effective fluid-like scheme, where the equation-of-state contains extra terms due to the torsion. In this work, we discuss introducing a non-fluid-like approach as a direct consequence of $f(T)$ extensions, particularly for $f(T)$ power law model scenarios. This approach will be compared using CMB constraints data from Planck 2018 and SDSS catalogs, showing a change in about 17% in $C_{l}$ at $l< 10^{1}$ from the ones reported in the literature as a fluid-like approach, which will bring significant changes in the analysis on cosmological tensions at early cosmic times.","sentences":["There have been several works that have studied scalar cosmological perturbations in $f(T)$ teleparallel gravity theories to understand early cosmic times dynamics.","In this direction, the perturbations presented have been performed by considering $f(T)$ extensions as an effective fluid-like scheme, where the equation-of-state contains extra terms due to the torsion.","In this work, we discuss introducing a non-fluid-like approach as a direct consequence of $f(T)$ extensions, particularly for $f(T)$ power law model scenarios.","This approach will be compared using CMB constraints data from Planck 2018 and SDSS catalogs, showing a change in about 17% in $C_{l}$ at $l< 10^{1}$ from the ones reported in the literature as a fluid-like approach, which will bring significant changes in the analysis on cosmological tensions at early cosmic times."],"url":"http://arxiv.org/abs/2403.13708v1","category":"gr-qc"}
{"created":"2024-03-20 16:06:01","title":"What Matters for Active Texture Recognition With Vision-Based Tactile Sensors","abstract":"This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures. We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of probabilistic models. Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition. Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of data augmentation, and dataset variability. By evaluating our method on a previously published Active Clothing Perception Dataset and on a real robotic system, we establish that the choice of the active exploration strategy has only a minor influence on the recognition accuracy, whereas data augmentation and dropout rate play a significantly larger role. In a comparison study, while humans achieve 66.9% recognition accuracy, our best approach reaches 90.0% in under 5 touches, highlighting that vision-based tactile sensors are highly effective for fabric texture recognition.","sentences":["This paper explores active sensing strategies that employ vision-based tactile sensors for robotic perception and classification of fabric textures.","We formalize the active sampling problem in the context of tactile fabric recognition and provide an implementation of information-theoretic exploration strategies based on minimizing predictive entropy and variance of probabilistic models.","Through ablation studies and human experiments, we investigate which components are crucial for quick and reliable texture recognition.","Along with the active sampling strategies, we evaluate neural network architectures, representations of uncertainty, influence of data augmentation, and dataset variability.","By evaluating our method on a previously published Active Clothing Perception Dataset and on a real robotic system, we establish that the choice of the active exploration strategy has only a minor influence on the recognition accuracy, whereas data augmentation and dropout rate play a significantly larger role.","In a comparison study, while humans achieve 66.9% recognition accuracy, our best approach reaches 90.0% in under 5 touches, highlighting that vision-based tactile sensors are highly effective for fabric texture recognition."],"url":"http://arxiv.org/abs/2403.13701v1","category":"cs.RO"}
{"created":"2024-03-20 16:04:57","title":"Taming Differentiable Logics with Coq Formalisation","abstract":"For performance and verification in machine learning, new methods have recently been proposed that optimise learning systems to satisfy formally expressed logical properties. Among these methods, differentiable logics (DLs) are used to translate propositional or first-order formulae into loss functions deployed for optimisation in machine learning. At the same time, recent attempts to give programming language support for verification of neural networks showed that DLs can be used to compile verification properties to machine-learning backends. This situation is calling for stronger guarantees about the soundness of such compilers, the soundness and compositionality of DLs, and the differentiability and performance of the resulting loss functions. In this paper, we propose an approach to formalise existing DLs using the Mathematical Components library in the Coq proof assistant. Thanks to this formalisation, we are able to give uniform semantics to otherwise disparate DLs, give formal proofs to existing informal arguments, find errors in previous work, and provide formal proofs to missing conjectured properties. This work is meant as a stepping stone for the development of programming language support for verification of machine learning.","sentences":["For performance and verification in machine learning, new methods have recently been proposed that optimise learning systems to satisfy formally expressed logical properties.","Among these methods, differentiable logics (DLs) are used to translate propositional or first-order formulae into loss functions deployed for optimisation in machine learning.","At the same time, recent attempts to give programming language support for verification of neural networks showed that DLs can be used to compile verification properties to machine-learning backends.","This situation is calling for stronger guarantees about the soundness of such compilers, the soundness and compositionality of DLs, and the differentiability and performance of the resulting loss functions.","In this paper, we propose an approach to formalise existing DLs using the Mathematical Components library in the Coq proof assistant.","Thanks to this formalisation, we are able to give uniform semantics to otherwise disparate DLs, give formal proofs to existing informal arguments, find errors in previous work, and provide formal proofs to missing conjectured properties.","This work is meant as a stepping stone for the development of programming language support for verification of machine learning."],"url":"http://arxiv.org/abs/2403.13700v1","category":"cs.LO"}
{"created":"2024-03-20 15:54:39","title":"Optimal Regularity for the 2D Euler Equations in the Yudovich class","abstract":"We analyze the optimal regularity that is exactly propagated by a transport equation driven by a velocity field with BMO gradient. As an application, we study the 2D Euler equations in case the initial vorticity is bounded. The sharpness of our result for the Euler equations follows from a variation of Bahouri and Chemin's vortex patch example.","sentences":["We analyze the optimal regularity that is exactly propagated by a transport equation driven by a velocity field with BMO gradient.","As an application, we study the 2D Euler equations in case the initial vorticity is bounded.","The sharpness of our result for the Euler equations follows from a variation of Bahouri and Chemin's vortex patch example."],"url":"http://arxiv.org/abs/2403.13691v1","category":"math.AP"}
{"created":"2024-03-20 15:33:46","title":"Bridging deep learning force fields and electronic structures with a physics-informed approach","abstract":"This work presents a physics-informed neural network approach bridging deep-learning force field and electronic structure simulations, illustrated through twisted two-dimensional large-scale material systems. The deep potential molecular dynamics model is adopted as the backbone, and electronic structure simulation is integrated. Using Wannier functions as the basis, we categorize Wannier Hamiltonian elements based on physical principles to incorporate diverse information from a deep-learning force field model. This information-sharing mechanism streamlines the architecture of our multifunctional model, enhancing its efficiency and effectiveness. Utilizing Wannier functions as the basis lays the groundwork for predicting more physical quantities. This approach serves as a powerful tool to explore both the structural and electronic properties of large-scale systems characterized by low periodicities. By endowing an existing well-developed machine-learning force field with electronic structure simulation capabilities, the study marks a significant advancement in developing multimodal machine-learning-based computational methods that can achieve multiple functionalities traditionally exclusive to first-principles calculations.","sentences":["This work presents a physics-informed neural network approach bridging deep-learning force field and electronic structure simulations, illustrated through twisted two-dimensional large-scale material systems.","The deep potential molecular dynamics model is adopted as the backbone, and electronic structure simulation is integrated.","Using Wannier functions as the basis, we categorize Wannier Hamiltonian elements based on physical principles to incorporate diverse information from a deep-learning force field model.","This information-sharing mechanism streamlines the architecture of our multifunctional model, enhancing its efficiency and effectiveness.","Utilizing Wannier functions as the basis lays the groundwork for predicting more physical quantities.","This approach serves as a powerful tool to explore both the structural and electronic properties of large-scale systems characterized by low periodicities.","By endowing an existing well-developed machine-learning force field with electronic structure simulation capabilities, the study marks a significant advancement in developing multimodal machine-learning-based computational methods that can achieve multiple functionalities traditionally exclusive to first-principles calculations."],"url":"http://arxiv.org/abs/2403.13675v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-20 15:26:52","title":"Topological and geometric restrictions on hyperconvex representations","abstract":"We study the geometry of hyperconvex representations of hyperbolic groups in ${\\rm PSL}(d,\\mathbb{C})$ and establish two structural results: a group admitting a hyperconvex representation is virtually isomorphic to a Kleinian group, and its hyperconvex limit set in the appropriate flag manifold has Hausdorff dimension strictly smaller than $2$.","sentences":["We study the geometry of hyperconvex representations of hyperbolic groups in ${\\rm PSL}(d,\\mathbb{C})$ and establish two structural results: a group admitting a hyperconvex representation is virtually isomorphic to a Kleinian group, and its hyperconvex limit set in the appropriate flag manifold has Hausdorff dimension strictly smaller than $2$."],"url":"http://arxiv.org/abs/2403.13668v1","category":"math.GT"}
{"created":"2024-03-20 15:20:30","title":"Grounding Spatial Relations in Text-Only Language Models","abstract":"This paper shows that text-only Language Models (LM) can learn to ground spatial relations like \"left of\" or \"below\" if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vision-and-Language Models and setting the new state-of-the-art for the VSR dataset. Our analysis show that our text-only LMs can generalize beyond the relations seen in the synthetic dataset to some extent, learning also more useful information than that encoded in the spatial rules we used to create the synthetic dataset itself.","sentences":["This paper shows that text-only Language Models (LM) can learn to ground spatial relations like \"left of\" or \"below\" if they are provided with explicit location information of objects and they are properly trained to leverage those locations.","We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image.","We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form.","Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens.","We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vision-and-Language Models and setting the new state-of-the-art for the VSR dataset.","Our analysis show that our text-only LMs can generalize beyond the relations seen in the synthetic dataset to some extent, learning also more useful information than that encoded in the spatial rules we used to create the synthetic dataset itself."],"url":"http://arxiv.org/abs/2403.13666v1","category":"cs.CL"}
{"created":"2024-03-20 15:12:51","title":"Mean Field Decoupling of Single Impurity Anderson Model through Auxiliary Majorana Fermions","abstract":"We present a method to study the time evolution of the single impurity Anderson model which exploits a mean field decoupling of the interacting impurity and the non-interacting bath (in form of a chain). This is achieved by the introduction of a pair of auxiliary Majorana fermions between the impurity and the chain. After decoupling, we obtain a self-consistent set of equations for the impurity and chain. First, we study the behavior of the system in equilibrium at zero temperature. We obtain a phase transition as a function of the interaction at the impurity and the coupling between the impurity and the chain between the Kondo regime, where the mean field parameters are zero and, hence, we have a well-defined spin at the impurity, to a phase where mean field parameters acquire finite values leading to a screening of the impurity spin by conduction bath electrons. In the latter case, we observe charge and spin fluctuations at the impurity site. Starting from this equilibrium ground state at zero temperature we quench in the interaction strength at the impurity and/or the hybridization strength between the impurity and the chain and study the time evolution of the system. We find that for quenches to weak to intermediate coupling the system converges to the equilibrium state defined by the final set of parameters after the quench. We analyze the oscillation frequency and as well as the thermalization rate during this quench. A quench to a strong interaction value results in persistent oscillations and a trapping of the system in a non-thermal state. We speculate that these two regimes of different long-time behavior are separated by a dynamical phase transition.","sentences":["We present a method to study the time evolution of the single impurity Anderson model which exploits a mean field decoupling of the interacting impurity and the non-interacting bath (in form of a chain).","This is achieved by the introduction of a pair of auxiliary Majorana fermions between the impurity and the chain.","After decoupling, we obtain a self-consistent set of equations for the impurity and chain.","First, we study the behavior of the system in equilibrium at zero temperature.","We obtain a phase transition as a function of the interaction at the impurity and the coupling between the impurity and the chain between the Kondo regime, where the mean field parameters are zero and, hence, we have a well-defined spin at the impurity, to a phase where mean field parameters acquire finite values leading to a screening of the impurity spin by conduction bath electrons.","In the latter case, we observe charge and spin fluctuations at the impurity site.","Starting from this equilibrium ground state at zero temperature we quench in the interaction strength at the impurity and/or the hybridization strength between the impurity and the chain and study the time evolution of the system.","We find that for quenches to weak to intermediate coupling the system converges to the equilibrium state defined by the final set of parameters after the quench.","We analyze the oscillation frequency and as well as the thermalization rate during this quench.","A quench to a strong interaction value results in persistent oscillations and a trapping of the system in a non-thermal state.","We speculate that these two regimes of different long-time behavior are separated by a dynamical phase transition."],"url":"http://arxiv.org/abs/2403.13661v1","category":"cond-mat.str-el"}
{"created":"2024-03-20 14:13:46","title":"Upper bound for Steklov eigenvalues of warped products with fiber of dimension 2","abstract":"In this note, we investigate the Steklov spectrum of the warped product $[0,L]\\times_h \\Sigma$ equipped with the metric $dt^2+h(t)^2g_\\Sigma$, where $\\Sigma$ is a compact surface. We find sharp upper bounds for the Steklov eigenvalues in terms of the eigenvalues of the Laplacian on $\\Sigma$. We apply our method to the case of metric of revolution on the 3-dimensional ball and we obtain a sharp estimate on the spectral gap between two consecutive Steklov eigenvalues.","sentences":["In this note, we investigate the Steklov spectrum of the warped product $[0,L]\\times_h \\Sigma$ equipped with the metric $dt^2+h(t)^2g_\\Sigma$, where $\\Sigma$ is a compact surface.","We find sharp upper bounds for the Steklov eigenvalues in terms of the eigenvalues of the Laplacian on $\\Sigma$. We apply our method to the case of metric of revolution on the 3-dimensional ball and we obtain a sharp estimate on the spectral gap between two consecutive Steklov eigenvalues."],"url":"http://arxiv.org/abs/2403.13620v1","category":"math.SP"}
{"created":"2024-03-20 14:05:17","title":"MIMO Channel as a Neural Function: Implicit Neural Representations for Extreme CSI Compression in Massive MIMO Systems","abstract":"Acquiring and utilizing accurate channel state information (CSI) can significantly improve transmission performance, thereby holding a crucial role in realizing the potential advantages of massive multiple-input multiple-output (MIMO) technology. Current prevailing CSI feedback approaches improve precision by employing advanced deep-learning methods to learn representative CSI features for a subsequent compression process. Diverging from previous works, we treat the CSI compression problem in the context of implicit neural representations. Specifically, each CSI matrix is viewed as a neural function that maps the CSI coordinates (antenna number and subchannel) to the corresponding channel gains. Instead of transmitting the parameters of the implicit neural functions directly, we transmit modulations based on the CSI matrix derived through a meta-learning algorithm. Modulations are then applied to a shared base network to generate the elements of the CSI matrix. Modulations corresponding to the CSI matrix are quantized and entropy-coded to further reduce the communication bandwidth, thus achieving extreme CSI compression ratios. Numerical results show that our proposed approach achieves state-of-the-art performance and showcases flexibility in feedback strategies.","sentences":["Acquiring and utilizing accurate channel state information (CSI) can significantly improve transmission performance, thereby holding a crucial role in realizing the potential advantages of massive multiple-input multiple-output (MIMO) technology.","Current prevailing CSI feedback approaches improve precision by employing advanced deep-learning methods to learn representative CSI features for a subsequent compression process.","Diverging from previous works, we treat the CSI compression problem in the context of implicit neural representations.","Specifically, each CSI matrix is viewed as a neural function that maps the CSI coordinates (antenna number and subchannel) to the corresponding channel gains.","Instead of transmitting the parameters of the implicit neural functions directly, we transmit modulations based on the CSI matrix derived through a meta-learning algorithm.","Modulations are then applied to a shared base network to generate the elements of the CSI matrix.","Modulations corresponding to the CSI matrix are quantized and entropy-coded to further reduce the communication bandwidth, thus achieving extreme CSI compression ratios.","Numerical results show that our proposed approach achieves state-of-the-art performance and showcases flexibility in feedback strategies."],"url":"http://arxiv.org/abs/2403.13615v1","category":"cs.IT"}
{"created":"2024-03-20 14:03:57","title":"Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?","abstract":"Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects. Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   Objectives: The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distributions. Five different DP-synthetic data generation methods are evaluated, including two basic DP histogram release methods and MWEM, Private-PGM, and DP GAN algorithms.   Conclusion: Most of the tested DP-synthetic data generation methods showed inflated Type I error, especially at privacy budget levels of $\\epsilon\\leq 1$. This result calls for caution when releasing and analyzing DP-synthetic data: low p-values may be obtained in statistical tests simply as a byproduct of the noise added to protect privacy. A DP smoothed histogram-based synthetic data generation method was shown to produce valid Type I error for all privacy levels tested but required a large original dataset size and a modest privacy budget ($\\epsilon\\geq 5$) in order to have reasonable Type II error levels.","sentences":["Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets.","Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects.","Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   ","Objectives:","The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   ","Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distributions.","Five different DP-synthetic data generation methods are evaluated, including two basic DP histogram release methods and MWEM, Private-PGM, and DP GAN algorithms.   ","Conclusion: Most of the tested DP-synthetic data generation methods showed inflated Type I error, especially at privacy budget levels of $\\epsilon\\leq 1$.","This result calls for caution when releasing and analyzing DP-synthetic data: low p-values may be obtained in statistical tests simply as a byproduct of the noise added to protect privacy.","A DP smoothed histogram-based synthetic data generation method was shown to produce valid Type I error for all privacy levels tested but required a large original dataset size and a modest privacy budget ($\\epsilon\\geq 5$) in order to have reasonable Type II error levels."],"url":"http://arxiv.org/abs/2403.13612v1","category":"cs.LG"}
{"created":"2024-03-20 13:59:23","title":"Tidal effects based on GUP-induced effective metric","abstract":"In this paper, we study tidal forces in the Schwarzschild black hole whose metric includes explicitly a generalized uncertainty principle (GUP) effect. We also investigate interesting features of the geodesic equations and tidal effects dependent on the GUP parameter $\\alpha$ related to a minimum length. Then, by solving geodesic deviation equations explicitly with appropriate boundary conditions, we show that $\\alpha$ in the effective metric affects both the radial and angular components of the geodesic equation, particularly near the singularities.","sentences":["In this paper, we study tidal forces in the Schwarzschild black hole whose metric includes explicitly a generalized uncertainty principle (GUP) effect.","We also investigate interesting features of the geodesic equations and tidal effects dependent on the GUP parameter $\\alpha$ related to a minimum length.","Then, by solving geodesic deviation equations explicitly with appropriate boundary conditions, we show that $\\alpha$ in the effective metric affects both the radial and angular components of the geodesic equation, particularly near the singularities."],"url":"http://arxiv.org/abs/2403.13608v1","category":"gr-qc"}
{"created":"2024-03-20 13:53:02","title":"A strange identity of an MF (Mahler function)","abstract":"We relate two different solutions of a Mahler equation; one solution is only defined at certain roots of unity, while the other is an analytic function inside the unit disk.","sentences":["We relate two different solutions of a Mahler equation; one solution is only defined at certain roots of unity, while the other is an analytic function inside the unit disk."],"url":"http://arxiv.org/abs/2403.13604v1","category":"math.NT"}
{"created":"2024-03-20 13:49:09","title":"Bayesian Physics-informed Neural Networks for System Identification of Inverter-dominated Power Systems","abstract":"While the uncertainty in generation and demand increases, accurately estimating the dynamic characteristics of power systems becomes crucial for employing the appropriate control actions to maintain their stability. In our previous work, we have shown that Bayesian Physics-informed Neural Networks (BPINNs) outperform conventional system identification methods in identifying the power system dynamic behavior under measurement noise. This paper takes the next natural step and addresses the more significant challenge, exploring how BPINN perform in estimating power system dynamics under increasing uncertainty from many Inverter-based Resources (IBRs) connected to the grid. These introduce a different type of uncertainty, compared to noisy measurements. The BPINN combines the advantages of Physics-informed Neural Networks (PINNs), such as inverse problem applicability, with Bayesian approaches for uncertainty quantification. We explore the BPINN performance on a wide range of systems, starting from a single machine infinite bus (SMIB) system and 3-bus system to extract important insights, to the 14-bus CIGRE distribution grid, and the large IEEE 118-bus system. We also investigate approaches that can accelerate the BPINN training, such as pretraining and transfer learning. Throughout this paper, we show that in presence of uncertainty, the BPINN achieves orders of magnitude lower errors than the widely popular method for system identification SINDy and significantly lower errors than PINN, while transfer learning helps reduce training time by up to 80 %.","sentences":["While the uncertainty in generation and demand increases, accurately estimating the dynamic characteristics of power systems becomes crucial for employing the appropriate control actions to maintain their stability.","In our previous work, we have shown that Bayesian Physics-informed Neural Networks (BPINNs) outperform conventional system identification methods in identifying the power system dynamic behavior under measurement noise.","This paper takes the next natural step and addresses the more significant challenge, exploring how BPINN perform in estimating power system dynamics under increasing uncertainty from many Inverter-based Resources (IBRs) connected to the grid.","These introduce a different type of uncertainty, compared to noisy measurements.","The BPINN combines the advantages of Physics-informed Neural Networks (PINNs), such as inverse problem applicability, with Bayesian approaches for uncertainty quantification.","We explore the BPINN performance on a wide range of systems, starting from a single machine infinite bus (SMIB) system and 3-bus system to extract important insights, to the 14-bus CIGRE distribution grid, and the large IEEE 118-bus system.","We also investigate approaches that can accelerate the BPINN training, such as pretraining and transfer learning.","Throughout this paper, we show that in presence of uncertainty, the BPINN achieves orders of magnitude lower errors than the widely popular method for system identification SINDy and significantly lower errors than PINN, while transfer learning helps reduce training time by up to 80 %."],"url":"http://arxiv.org/abs/2403.13602v1","category":"eess.SY"}
{"created":"2024-03-20 13:23:57","title":"HCiM: ADC-Less Hybrid Analog-Digital Compute in Memory Accelerator for Deep Learning Workloads","abstract":"Analog Compute-in-Memory (CiM) accelerators are increasingly recognized for their efficiency in accelerating Deep Neural Networks (DNN). However, their dependence on Analog-to-Digital Converters (ADCs) for accumulating partial sums from crossbars leads to substantial power and area overhead. Moreover, the high area overhead of ADCs constrains the throughput due to the limited number of ADCs that can be integrated per crossbar. An approach to mitigate this issue involves the adoption of extreme low-precision quantization (binary or ternary) for partial sums. Training based on such an approach eliminates the need for ADCs. While this strategy effectively reduces ADC costs, it introduces the challenge of managing numerous floating-point scale factors, which are trainable parameters like DNN weights. These scale factors must be multiplied with the binary or ternary outputs at the columns of the crossbar to ensure system accuracy. To that effect, we propose an algorithm-hardware co-design approach, where DNNs are first trained with quantization-aware training. Subsequently, we introduce HCiM, an ADC-Less Hybrid Analog-Digital CiM accelerator. HCiM uses analog CiM crossbars for performing Matrix-Vector Multiplication operations coupled with a digital CiM array dedicated to processing scale factors. This digital CiM array can execute both addition and subtraction operations within the memory array, thus enhancing processing speed. Additionally, it exploits the inherent sparsity in ternary quantization to achieve further energy savings. Compared to an analog CiM baseline architecture using 7 and 4-bit ADC, HCiM achieves energy reductions up to 28% and 12%, respectively","sentences":["Analog Compute-in-Memory (CiM) accelerators are increasingly recognized for their efficiency in accelerating Deep Neural Networks (DNN).","However, their dependence on Analog-to-Digital Converters (ADCs) for accumulating partial sums from crossbars leads to substantial power and area overhead.","Moreover, the high area overhead of ADCs constrains the throughput due to the limited number of ADCs that can be integrated per crossbar.","An approach to mitigate this issue involves the adoption of extreme low-precision quantization (binary or ternary) for partial sums.","Training based on such an approach eliminates the need for ADCs.","While this strategy effectively reduces ADC costs, it introduces the challenge of managing numerous floating-point scale factors, which are trainable parameters like DNN weights.","These scale factors must be multiplied with the binary or ternary outputs at the columns of the crossbar to ensure system accuracy.","To that effect, we propose an algorithm-hardware co-design approach, where DNNs are first trained with quantization-aware training.","Subsequently, we introduce HCiM, an ADC-Less Hybrid Analog-Digital CiM accelerator.","HCiM uses analog CiM crossbars for performing Matrix-Vector Multiplication operations coupled with a digital CiM array dedicated to processing scale factors.","This digital CiM array can execute both addition and subtraction operations within the memory array, thus enhancing processing speed.","Additionally, it exploits the inherent sparsity in ternary quantization to achieve further energy savings.","Compared to an analog CiM baseline architecture using 7 and 4-bit ADC, HCiM achieves energy reductions up to 28% and 12%, respectively"],"url":"http://arxiv.org/abs/2403.13577v1","category":"cs.AR"}
{"created":"2024-03-20 13:02:26","title":"The stability of irrotational shocks and the Landau law of decay","abstract":"We consider the long-time behavior of irrotational solutions of the three-dimensional compressible Euler equations with shocks, hypersurfaces of discontinuity across which the Rankine-Hugoniot conditions for irrotational flow hold. Our analysis is motivated by Landau's analysis of spherically-symmetric shock waves, who predicted that at large times, not just one, but two shocks emerge. These shocks are logarithmically-separated from the Minkowskian light cone and the fluid velocity decays at the non-time-integrable rate 1/(t(\\log t)^{1/2}). We show that for initial data, which need not be spherically-symmetric, with two shocks in it and which is sufficiently close, in appropriately weighted Sobolev norms, to an N-wave profile, the solution to the shock-front initial value problem can be continued for all time and does not develop any further singularities. In particular this is the first proof of global existence for solutions (which are necessarily singular) of a quasilinear wave equation in three space dimensions which does not verify the null condition. The proof requires carefully-constructed multiplier estimates and analysis of the geometry of the shock surfaces.","sentences":["We consider the long-time behavior of irrotational solutions of the three-dimensional compressible Euler equations with shocks, hypersurfaces of discontinuity across which the Rankine-Hugoniot conditions for irrotational flow hold.","Our analysis is motivated by Landau's analysis of spherically-symmetric shock waves, who predicted that at large times, not just one, but two shocks emerge.","These shocks are logarithmically-separated from the Minkowskian light cone and the fluid velocity decays at the non-time-integrable rate 1/(t(\\log t)^{1/2}).","We show that for initial data, which need not be spherically-symmetric, with two shocks in it and which is sufficiently close, in appropriately weighted Sobolev norms, to an N-wave profile, the solution to the shock-front initial value problem can be continued for all time and does not develop any further singularities.","In particular this is the first proof of global existence for solutions (which are necessarily singular) of a quasilinear wave equation in three space dimensions which does not verify the null condition.","The proof requires carefully-constructed multiplier estimates and analysis of the geometry of the shock surfaces."],"url":"http://arxiv.org/abs/2403.13568v1","category":"math.AP"}
{"created":"2024-03-20 12:56:40","title":"DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs","abstract":"This study introduces a refined Flooding Injection Rate-adjustable Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame Fusion (2F) for DoS detection and localization. Two Convolutional Neural Networks models for classification and segmentation were developed to detect and localize DoS respectively. It achieves detection and localization accuracies of 95.8\\% and 91.7\\%, and precision rates of 98.5\\% and 99.3\\% in a 16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\\% when scaling from 8x8 to 16x16 NoCs, and it requires 42.4\\% less hardware compared to state-of-the-arts. This advancement demonstrates DL2Fence's effectiveness in balancing outstanding detection performance in large-scale NoCs with extremely low hardware overhead.","sentences":["This study introduces a refined Flooding Injection Rate-adjustable Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame Fusion (2F) for DoS detection and localization.","Two Convolutional Neural Networks models for classification and segmentation were developed to detect and localize DoS respectively.","It achieves detection and localization accuracies of 95.8\\% and 91.7\\%, and precision rates of 98.5\\% and 99.3\\% in a 16x16 mesh NoC.","The framework's hardware overhead notably decreases by 76.3\\% when scaling from 8x8 to 16x16 NoCs, and it requires 42.4\\% less hardware compared to state-of-the-arts.","This advancement demonstrates DL2Fence's effectiveness in balancing outstanding detection performance in large-scale NoCs with extremely low hardware overhead."],"url":"http://arxiv.org/abs/2403.13563v1","category":"cs.CR"}
{"created":"2024-03-20 12:38:06","title":"Asymptotic behavior of solutions of the linearized Euler equations near a shear layer","abstract":"In this article, thanks to a new and detailed study of the Green's function of Rayleigh equation near the extrema of the velocity of a shear layer, we obtain optimal bounds on the asymptotic behaviour of solutions to the linearized incompressible Euler equations both in the whole plane, the half plane and the periodic case, and improve the description of the so called \"vorticity depletion property\" discovered by F. Bouchet and H. Morita by putting into light a localization property of the solutions of Rayleigh equation near an extremal velocity.","sentences":["In this article, thanks to a new and detailed study of the Green's function of Rayleigh equation near the extrema of the velocity of a shear layer, we obtain optimal bounds on the asymptotic behaviour of solutions to the linearized incompressible Euler equations both in the whole plane, the half plane and the periodic case, and improve the description of the so called \"vorticity depletion property\" discovered by F. Bouchet and H. Morita by putting into light a localization property of the solutions of Rayleigh equation near an extremal velocity."],"url":"http://arxiv.org/abs/2403.13549v1","category":"math.AP"}
{"created":"2024-03-20 12:31:55","title":"Long-time behavior of an Arc-shaped Vortex Filament and its Application to the Stability of a Circular Vortex Filament","abstract":"We consider a nonlinear model equation, known as the Localized Induction Equation, describing the motion of a vortex filament immersed in an incompressible and inviscid fluid. We show stability estimates for an arc-shaped vortex filament, which is an exact solution to an initial-boundary value problem for the Localized Induction Equation. An arc-shaped filament travels along an axis at a constant speed without changing its shape, and is oriented in such a way that the arc stays in a plane that is perpendicular to the axis. We prove that an arc-shaped filament is stable in the Lyapunov sense for general perturbations except in the axis-direction, for which the perturbation can grow linearly in time. We also show that this estimate is optimal. We then apply the obtained stability estimates to study the stability of a circular vortex filament under some symmetry assumptions on the initial perturbation. We do this by dividing the circular filament into arcs, apply the stability estimate to each arc-shaped filament, and combine the estimates to obtain estimates for the whole circle. The optimality of the stability estimates for an arc-shaped filament also shows that a circular filament is not stable in the Lyapunov sense, namely, certain perturbations can grow linearly in time.","sentences":["We consider a nonlinear model equation, known as the Localized Induction Equation, describing the motion of a vortex filament immersed in an incompressible and inviscid fluid.","We show stability estimates for an arc-shaped vortex filament, which is an exact solution to an initial-boundary value problem for the Localized Induction Equation.","An arc-shaped filament travels along an axis at a constant speed without changing its shape, and is oriented in such a way that the arc stays in a plane that is perpendicular to the axis.","We prove that an arc-shaped filament is stable in the Lyapunov sense for general perturbations except in the axis-direction, for which the perturbation can grow linearly in time.","We also show that this estimate is optimal.","We then apply the obtained stability estimates to study the stability of a circular vortex filament under some symmetry assumptions on the initial perturbation.","We do this by dividing the circular filament into arcs, apply the stability estimate to each arc-shaped filament, and combine the estimates to obtain estimates for the whole circle.","The optimality of the stability estimates for an arc-shaped filament also shows that a circular filament is not stable in the Lyapunov sense, namely, certain perturbations can grow linearly in time."],"url":"http://arxiv.org/abs/2403.13546v1","category":"math.AP"}
{"created":"2024-03-20 12:21:28","title":"Solution of the Bj\u00f6rling problem by discrete approximation","abstract":"The Bj\\\"orling problem amounts to the construction of a minimal surface from a real-analytic curve with a given real-analytic normal vector field. We approximate that solution locally by discrete minimal surfaces as special discrete isothermic surfaces (as defined by Bobenko and Pinkall in 1996). The main step in our construction is the approximation of the sought surface's Weierstrass data by discrete conformal maps. We prove that the approximation error is of the order of the square of the mesh size.","sentences":["The Bj\\\"orling problem amounts to the construction of a minimal surface from a real-analytic curve with a given real-analytic normal vector field.","We approximate that solution locally by discrete minimal surfaces as special discrete isothermic surfaces (as defined by Bobenko and Pinkall in 1996).","The main step in our construction is the approximation of the sought surface's Weierstrass data by discrete conformal maps.","We prove that the approximation error is of the order of the square of the mesh size."],"url":"http://arxiv.org/abs/2403.13540v1","category":"math.DG"}
{"created":"2024-03-20 17:59:57","title":"Editing Massive Concepts in Text-to-Image Diffusion Models","abstract":"Text-to-image diffusion models suffer from the risk of generating outdated, copyrighted, incorrect, and biased content. While previous methods have mitigated the issues on a small scale, it is essential to handle them simultaneously in larger-scale real-world scenarios. We propose a two-stage method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage performs memory optimization for each individual concept with dual self-distillation from text alignment loss and diffusion noise prediction loss. The second stage conducts massive concept editing with multi-layer, closed form model editing. We further propose a comprehensive benchmark, named ImageNet Concept Editing Benchmark (ICEB), for evaluating massive concept editing for T2I models with two subtasks, free-form prompts, massive concept categories, and extensive evaluation metrics. Extensive experiments conducted on our proposed benchmark and previous benchmarks demonstrate the superior scalability of EMCID for editing up to 1,000 concepts, providing a practical approach for fast adjustment and re-deployment of T2I diffusion models in real-world applications.","sentences":["Text-to-image diffusion models suffer from the risk of generating outdated, copyrighted, incorrect, and biased content.","While previous methods have mitigated the issues on a small scale, it is essential to handle them simultaneously in larger-scale real-world scenarios.","We propose a two-stage method, Editing Massive Concepts In Diffusion Models (EMCID).","The first stage performs memory optimization for each individual concept with dual self-distillation from text alignment loss and diffusion noise prediction loss.","The second stage conducts massive concept editing with multi-layer, closed form model editing.","We further propose a comprehensive benchmark, named ImageNet Concept Editing Benchmark (ICEB), for evaluating massive concept editing for T2I models with two subtasks, free-form prompts, massive concept categories, and extensive evaluation metrics.","Extensive experiments conducted on our proposed benchmark and previous benchmarks demonstrate the superior scalability of EMCID for editing up to 1,000 concepts, providing a practical approach for fast adjustment and re-deployment of T2I diffusion models in real-world applications."],"url":"http://arxiv.org/abs/2403.13807v1","category":"cs.CV"}
{"created":"2024-03-20 17:59:55","title":"RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS","abstract":"Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS.","sentences":["Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds.","While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering.","Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes.","In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes.","Our main contributions are threefold.","First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization.","Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds.","Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes.","We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS."],"url":"http://arxiv.org/abs/2403.13806v1","category":"cs.CV"}
{"created":"2024-03-20 17:59:43","title":"Learning from Models and Data for Visual Grounding","abstract":"We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model. The knowledge transfer from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model. Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the grounding capabilities of an off-the-shelf vision-and-language model. Particularly, SynGround improves the pointing game accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to 63.67%.","sentences":["We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model.","The knowledge transfer from the models initiates the generation of image descriptions through an image description generator.","These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model.","Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts.","We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations.","The resulting model improves the grounding capabilities of an off-the-shelf vision-and-language model.","Particularly, SynGround improves the pointing game accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to 63.67%."],"url":"http://arxiv.org/abs/2403.13804v1","category":"cs.CV"}
{"created":"2024-03-20 17:51:53","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","abstract":"Monocular depth estimation is crucial for numerous downstream vision tasks and applications. Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature. Rather than starting from noise, we seek a direct mapping from input image to depth map. We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality. Our study demonstrates that a pre-trained image diffusion model can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images. We find that an auxiliary surface normals loss further improves the depth estimates. Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates. On standard benchmarks of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data.","sentences":["Monocular depth estimation is crucial for numerous downstream vision tasks and applications.","Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature.","Rather than starting from noise, we seek a direct mapping from input image to depth map.","We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality.","Our study demonstrates that a pre-trained image diffusion model can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images.","We find that an auxiliary surface normals loss further improves the depth estimates.","Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates.","On standard benchmarks of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data."],"url":"http://arxiv.org/abs/2403.13788v1","category":"cs.CV"}
{"created":"2024-03-20 17:44:33","title":"A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies","abstract":"In this paper, we introduce a novel convex formulation that seamlessly integrates the Material Point Method (MPM) with articulated rigid body dynamics in frictional contact scenarios. We extend the linear corotational hyperelastic model into the realm of elastoplasticity and include an efficient return mapping algorithm. This approach is particularly effective for MPM simulations involving significant deformation and topology changes, while preserving the convexity of the optimization problem. Our method ensures global convergence, enabling the use of large simulation time steps without compromising robustness. We have validated our approach through rigorous testing and performance evaluations, highlighting its superior capabilities in managing complex simulations relevant to robotics. Compared to previous MPM based robotic simulators, our method significantly improves the stability of contact resolution -- a critical factor in robot manipulation tasks. We make our method available in the open-source robotics toolkit, Drake.","sentences":["In this paper, we introduce a novel convex formulation that seamlessly integrates the Material Point Method (MPM) with articulated rigid body dynamics in frictional contact scenarios.","We extend the linear corotational hyperelastic model into the realm of elastoplasticity and include an efficient return mapping algorithm.","This approach is particularly effective for MPM simulations involving significant deformation and topology changes, while preserving the convexity of the optimization problem.","Our method ensures global convergence, enabling the use of large simulation time steps without compromising robustness.","We have validated our approach through rigorous testing and performance evaluations, highlighting its superior capabilities in managing complex simulations relevant to robotics.","Compared to previous MPM based robotic simulators, our method significantly improves the stability of contact resolution -- a critical factor in robot manipulation tasks.","We make our method available in the open-source robotics toolkit, Drake."],"url":"http://arxiv.org/abs/2403.13783v1","category":"cs.RO"}
{"created":"2024-03-20 17:41:35","title":"Certified Human Trajectory Prediction","abstract":"Trajectory prediction plays an essential role in autonomous vehicles. While numerous strategies have been developed to enhance the robustness of trajectory prediction models, these methods are predominantly heuristic and do not offer guaranteed robustness against adversarial attacks and noisy observations. In this work, we propose a certification approach tailored for the task of trajectory prediction. To this end, we address the inherent challenges associated with trajectory prediction, including unbounded outputs, and mutli-modality, resulting in a model that provides guaranteed robustness. Furthermore, we integrate a denoiser into our method to further improve the performance. Through comprehensive evaluations, we demonstrate the effectiveness of the proposed technique across various baselines and using standard trajectory prediction datasets. The code will be made available online: https://s-attack.github.io/","sentences":["Trajectory prediction plays an essential role in autonomous vehicles.","While numerous strategies have been developed to enhance the robustness of trajectory prediction models, these methods are predominantly heuristic and do not offer guaranteed robustness against adversarial attacks and noisy observations.","In this work, we propose a certification approach tailored for the task of trajectory prediction.","To this end, we address the inherent challenges associated with trajectory prediction, including unbounded outputs, and mutli-modality, resulting in a model that provides guaranteed robustness.","Furthermore, we integrate a denoiser into our method to further improve the performance.","Through comprehensive evaluations, we demonstrate the effectiveness of the proposed technique across various baselines and using standard trajectory prediction datasets.","The code will be made available online: https://s-attack.github.io/"],"url":"http://arxiv.org/abs/2403.13778v1","category":"cs.CV"}
{"created":"2024-03-20 17:30:32","title":"Sensitivity of Bayesian 21 cm power spectrum estimation to foreground model errors","abstract":"Power spectrum estimators are an important tool in efforts to detect the 21 cm brightness temperature fluctuations from neutral hydrogen at early times. An initial detection will likely be statistical in nature, meaning that it will not be possible to make a coherent map of the brightness temperature fluctuations; instead, only their variance will be measured against a background of noise and residual systematic effects. Optimal Quadratic Estimator (OQE)-based methods often apply an inverse covariance weighting to the data. However, inaccurate covariance modelling can lead to reduced sensitivity and, in some cases, severe signal loss. We recently proposed a Bayesian method to jointly estimate the 21 cm fluctuations, their power spectrum, and foreground emission. Instead of requiring a fixed a priori estimate of the covariance, we estimate the covariance as part of the inference. Choices of parametrization, particularly of the foregrounds, are subject to model errors and could lead to biases and other ill effects if not properly controlled. In this paper, we investigate the effects of inaccurate foreground models on 21 cm power spectrum recovery. Using simulated visibilities, we find that, even in the most extreme scenarios tested, our approach is capable of recovering 21 cm delay power spectrum estimates consistent with a known input signal for delays $\\gtrsim300$ ns ($\\sim$88\\% of the available Fourier modes). This is true even when using foreground models derived from modified foreground catalogs containing spatial and spectral perturbations at the quoted level of uncertainty on our foreground catalogs.","sentences":["Power spectrum estimators are an important tool in efforts to detect the 21 cm brightness temperature fluctuations from neutral hydrogen at early times.","An initial detection will likely be statistical in nature, meaning that it will not be possible to make a coherent map of the brightness temperature fluctuations; instead, only their variance will be measured against a background of noise and residual systematic effects.","Optimal Quadratic Estimator (OQE)-based methods often apply an inverse covariance weighting to the data.","However, inaccurate covariance modelling can lead to reduced sensitivity and, in some cases, severe signal loss.","We recently proposed a Bayesian method to jointly estimate the 21 cm fluctuations, their power spectrum, and foreground emission.","Instead of requiring a fixed a priori estimate of the covariance, we estimate the covariance as part of the inference.","Choices of parametrization, particularly of the foregrounds, are subject to model errors and could lead to biases and other ill effects if not properly controlled.","In this paper, we investigate the effects of inaccurate foreground models on 21 cm power spectrum recovery.","Using simulated visibilities, we find that, even in the most extreme scenarios tested, our approach is capable of recovering 21 cm delay power spectrum estimates consistent with a known input signal for delays $\\gtrsim300$ ns ($\\sim$88\\% of the available Fourier modes).","This is true even when using foreground models derived from modified foreground catalogs containing spatial and spectral perturbations at the quoted level of uncertainty on our foreground catalogs."],"url":"http://arxiv.org/abs/2403.13767v1","category":"astro-ph.CO"}
{"created":"2024-03-20 17:30:01","title":"Statistical estimation of full-sky radio maps from 21cm array visibility data using Gaussian Constrained Realisations","abstract":"An important application of next-generation wide-field radio interferometers is making high dynamic range maps of radio emission. Traditional deconvolution methods like CLEAN can give poor recovery of diffuse structure, prompting the development of wide-field alternatives like Direct Optimal Mapping and $m$-mode analysis. In this paper, we propose an alternative Bayesian method to infer the coefficients of a full-sky spherical harmonic basis for a drift-scan telescope with potentially thousands of baselines. The can precisely encode the uncertainties and correlations between the parameters used to build the recovered image. We use Gaussian Constrained Realisations (GCR) to efficiently draw samples of the spherical harmonic coefficients, despite the very large parameter space and extensive sky-regions of missing data. Each GCR solution provides a complete, statistically-consistent gap-free realisation of a full-sky map conditioned on the available data, even when the interferometer's field of view is small. Many realisations can be generated and used for further analysis and robust propagation of statistical uncertainties. In this paper, we present the mathematical formalism of the spherical harmonic GCR-method for radio interferometers. We focus on the recovery of diffuse emission as a use case, along with validation of the method against simulations with a known diffuse emission component.","sentences":["An important application of next-generation wide-field radio interferometers is making high dynamic range maps of radio emission.","Traditional deconvolution methods like CLEAN can give poor recovery of diffuse structure, prompting the development of wide-field alternatives like Direct Optimal Mapping and $m$-mode analysis.","In this paper, we propose an alternative Bayesian method to infer the coefficients of a full-sky spherical harmonic basis for a drift-scan telescope with potentially thousands of baselines.","The can precisely encode the uncertainties and correlations between the parameters used to build the recovered image.","We use Gaussian Constrained Realisations (GCR) to efficiently draw samples of the spherical harmonic coefficients, despite the very large parameter space and extensive sky-regions of missing data.","Each GCR solution provides a complete, statistically-consistent gap-free realisation of a full-sky map conditioned on the available data, even when the interferometer's field of view is small.","Many realisations can be generated and used for further analysis and robust propagation of statistical uncertainties.","In this paper, we present the mathematical formalism of the spherical harmonic GCR-method for radio interferometers.","We focus on the recovery of diffuse emission as a use case, along with validation of the method against simulations with a known diffuse emission component."],"url":"http://arxiv.org/abs/2403.13766v1","category":"astro-ph.IM"}
{"created":"2024-03-20 16:41:49","title":"Active Nematic Ratchet in Asymmetric Obstacle Arrays","abstract":"We numerically investigate the effect of a periodic array of asymmetric obstacles in a two-dimensional active nematic. We find that activity in conjunction with the asymmetry leads to a ratchet effect or unidirectional flow of the fluid along the asymmetry direction. The directional flow is still present even in the active turbulent phase when the gap between obstacles is sufficiently small. We demonstrate that the dynamics of the topological defects transition from flow-mirroring to smectic-like as the gap between obstacles is made smaller, and explain this transition in terms of the pinning of negative winding number defects between obstacles. This also leads to a non-monotonic ratchet effect magnitude as a function of obstacle size, so that there is an optimal obstacle size for ratcheting at fixed activity.","sentences":["We numerically investigate the effect of a periodic array of asymmetric obstacles in a two-dimensional active nematic.","We find that activity in conjunction with the asymmetry leads to a ratchet effect or unidirectional flow of the fluid along the asymmetry direction.","The directional flow is still present even in the active turbulent phase when the gap between obstacles is sufficiently small.","We demonstrate that the dynamics of the topological defects transition from flow-mirroring to smectic-like as the gap between obstacles is made smaller, and explain this transition in terms of the pinning of negative winding number defects between obstacles.","This also leads to a non-monotonic ratchet effect magnitude as a function of obstacle size, so that there is an optimal obstacle size for ratcheting at fixed activity."],"url":"http://arxiv.org/abs/2403.13733v1","category":"cond-mat.soft"}
{"created":"2024-03-20 16:39:48","title":"Projection-free computation of robust controllable sets with constrained zonotopes","abstract":"We study the problem of computing robust controllable sets for discrete-time linear systems with additive uncertainty. We propose a tractable and scalable approach to inner- and outer-approximate robust controllable sets using constrained zonotopes, when the additive uncertainty set is a symmetric, convex, and compact set. Our least-squares-based approach uses novel closed-form approximations of the Pontryagin difference between a constrained zonotopic minuend and a symmetric, convex, and compact subtrahend. Unlike existing approaches, our approach does not rely on convex optimization solvers, and is projection-free for ellipsoidal and zonotopic uncertainty sets. We also propose a least-squares-based approach to compute a convex, polyhedral outer-approximation to constrained zonotopes, and characterize sufficient conditions under which all these approximations are exact. We demonstrate the computational efficiency and scalability of our approach in several case studies, including the design of abort-safe rendezvous trajectories for a spacecraft in near-rectilinear halo orbit under uncertainty. Our approach can inner-approximate a 20-step robust controllable set for a 100-dimensional linear system in under 15 seconds on a standard computer.","sentences":["We study the problem of computing robust controllable sets for discrete-time linear systems with additive uncertainty.","We propose a tractable and scalable approach to inner- and outer-approximate robust controllable sets using constrained zonotopes, when the additive uncertainty set is a symmetric, convex, and compact set.","Our least-squares-based approach uses novel closed-form approximations of the Pontryagin difference between a constrained zonotopic minuend and a symmetric, convex, and compact subtrahend.","Unlike existing approaches, our approach does not rely on convex optimization solvers, and is projection-free for ellipsoidal and zonotopic uncertainty sets.","We also propose a least-squares-based approach to compute a convex, polyhedral outer-approximation to constrained zonotopes, and characterize sufficient conditions under which all these approximations are exact.","We demonstrate the computational efficiency and scalability of our approach in several case studies, including the design of abort-safe rendezvous trajectories for a spacecraft in near-rectilinear halo orbit under uncertainty.","Our approach can inner-approximate a 20-step robust controllable set for a 100-dimensional linear system in under 15 seconds on a standard computer."],"url":"http://arxiv.org/abs/2403.13730v1","category":"math.OC"}
{"created":"2024-03-20 16:36:04","title":"Robust Inference in Locally Misspecified Bipartite Networks","abstract":"This paper introduces a methodology to conduct robust inference in bipartite networks under local misspecification. We focus on a class of dyadic network models with misspecified conditional moment restrictions. The framework of misspecification is local, as the effect of misspecification varies with the sample size. We utilize this local asymptotic approach to construct a robust estimator that is minimax optimal for the mean square error within a neighborhood of misspecification. Additionally, we introduce bias-aware confidence intervals that account for the effect of the local misspecification. These confidence intervals have the correct asymptotic coverage for the true parameter of interest under sparse network asymptotics. Monte Carlo experiments demonstrate that the robust estimator performs well in finite samples and sparse networks. As an empirical illustration, we study the formation of a scientific collaboration network among economists.","sentences":["This paper introduces a methodology to conduct robust inference in bipartite networks under local misspecification.","We focus on a class of dyadic network models with misspecified conditional moment restrictions.","The framework of misspecification is local, as the effect of misspecification varies with the sample size.","We utilize this local asymptotic approach to construct a robust estimator that is minimax optimal for the mean square error within a neighborhood of misspecification.","Additionally, we introduce bias-aware confidence intervals that account for the effect of the local misspecification.","These confidence intervals have the correct asymptotic coverage for the true parameter of interest under sparse network asymptotics.","Monte Carlo experiments demonstrate that the robust estimator performs well in finite samples and sparse networks.","As an empirical illustration, we study the formation of a scientific collaboration network among economists."],"url":"http://arxiv.org/abs/2403.13725v1","category":"econ.EM"}
{"created":"2024-03-20 16:10:36","title":"On Optimal Management of Energy Storage Systems in Renewable Energy Communities","abstract":"Renewable energy communities are legal entities involving the association of citizens, organizations and local businesses aimed at contributing to the green energy transition and providing social, environmental and economic benefits to their members. This goal is pursued through the cooperative efforts of the community actors and by increasing the local energy self-consumption. In this paper, the optimal energy community operation in the presence of energy storage units is addressed. By exploiting the flexibility provided by the storage facilities, the main task is to minimize the community energy bill by taking advantage of incentives related to local self-consumption. Optimality conditions are derived, and an explicit optimal solution is devised. Numerical simulations are provided to assess the performance of the proposed solution.","sentences":["Renewable energy communities are legal entities involving the association of citizens, organizations and local businesses aimed at contributing to the green energy transition and providing social, environmental and economic benefits to their members.","This goal is pursued through the cooperative efforts of the community actors and by increasing the local energy self-consumption.","In this paper, the optimal energy community operation in the presence of energy storage units is addressed.","By exploiting the flexibility provided by the storage facilities, the main task is to minimize the community energy bill by taking advantage of incentives related to local self-consumption.","Optimality conditions are derived, and an explicit optimal solution is devised.","Numerical simulations are provided to assess the performance of the proposed solution."],"url":"http://arxiv.org/abs/2403.13707v1","category":"eess.SY"}
{"created":"2024-03-20 15:55:35","title":"Highly Efficient Decomposition of n-Qubit Quantum Gates Based on Block-ZXZ Decomposition","abstract":"This paper proposes a new optimized quantum block-ZXZ decomposition method [4,5,6] that results in the construction of more optimal quantum circuits than the quantum Shannon decomposition (QSD) [17] can achieve, which has been the most optimal decomposition method since 2006. With the proposed decomposition, a general 3-qubit gate can be decomposed using 19 CNOT gates (rather than 20). For general n-qubit gates, the proposed decomposition generates circuits that have $(22/48) 4^n - (3/2) 2^n + (5/3)$ CNOT gates , which is less that the best known exact decomposition algorithm by $(4^{n-2} -1)/3$ CNOT gates.","sentences":["This paper proposes a new optimized quantum block-ZXZ decomposition method","[4,5,6] that results in the construction of more optimal quantum circuits than the quantum Shannon decomposition (QSD)","[17] can achieve, which has been the most optimal decomposition method since 2006.","With the proposed decomposition, a general 3-qubit gate can be decomposed using 19 CNOT gates (rather than 20).","For general n-qubit gates, the proposed decomposition generates circuits that have $(22/48) 4^n - (3/2) 2^n + (5/3)$ CNOT gates , which is less that the best known exact decomposition algorithm by $(4^{n-2} -1)/3$ CNOT gates."],"url":"http://arxiv.org/abs/2403.13692v1","category":"quant-ph"}
{"created":"2024-03-20 15:51:03","title":"Scalable Projection-Free Optimization Methods via MultiRadial Duality Theory","abstract":"Recent works have developed new projection-free first-order methods based on utilizing linesearches and normal vector computations to maintain feasibility. These oracles can be cheaper than orthogonal projection or linear optimization subroutines but have the drawback of requiring a known strictly feasible point to do these linesearches with respect to. In this work, we develop new theory and algorithms which can operate using these cheaper linesearches while only requiring knowledge of points strictly satisfying each constraint separately. Convergence theory for several resulting ``multiradial'' gradient methods is established. We also provide preliminary numerics showing performance is essentially independent of how one selects the reference points for synthetic quadratically constrained quadratic programs.","sentences":["Recent works have developed new projection-free first-order methods based on utilizing linesearches and normal vector computations to maintain feasibility.","These oracles can be cheaper than orthogonal projection or linear optimization subroutines but have the drawback of requiring a known strictly feasible point to do these linesearches with respect to.","In this work, we develop new theory and algorithms which can operate using these cheaper linesearches while only requiring knowledge of points strictly satisfying each constraint separately.","Convergence theory for several resulting ``multiradial'' gradient methods is established.","We also provide preliminary numerics showing performance is essentially independent of how one selects the reference points for synthetic quadratically constrained quadratic programs."],"url":"http://arxiv.org/abs/2403.13688v1","category":"math.OC"}
{"created":"2024-03-20 15:29:59","title":"Machine Learning Optimized Approach for Parameter Selection in MESHFREE Simulations","abstract":"Meshfree simulation methods are emerging as compelling alternatives to conventional mesh-based approaches, particularly in the fields of Computational Fluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a comprehensive overview of our research combining Machine Learning (ML) and Fraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a numerical point cloud in a Generalized Finite Difference Method (GFDM). This tool enables the effective handling of complex flow domains, moving geometries, and free surfaces, while allowing users to finely tune local refinement and quality parameters for an optimal balance between computation time and results accuracy. However, manually determining the optimal parameter combination poses challenges, especially for less experienced users. We introduce a novel ML-optimized approach, using active learning, regression trees, and visualization on MESHFREE simulation data, demonstrating the impact of input combinations on results quality and computation time. This research contributes valuable insights into parameter optimization in meshfree simulations, enhancing accessibility and usability for a broader user base in scientific and engineering applications.","sentences":["Meshfree simulation methods are emerging as compelling alternatives to conventional mesh-based approaches, particularly in the fields of Computational Fluid Dynamics (CFD) and continuum mechanics.","In this publication, we provide a comprehensive overview of our research combining Machine Learning (ML) and Fraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a numerical point cloud in a Generalized Finite Difference Method (GFDM).","This tool enables the effective handling of complex flow domains, moving geometries, and free surfaces, while allowing users to finely tune local refinement and quality parameters for an optimal balance between computation time and results accuracy.","However, manually determining the optimal parameter combination poses challenges, especially for less experienced users.","We introduce a novel ML-optimized approach, using active learning, regression trees, and visualization on MESHFREE simulation data, demonstrating the impact of input combinations on results quality and computation time.","This research contributes valuable insights into parameter optimization in meshfree simulations, enhancing accessibility and usability for a broader user base in scientific and engineering applications."],"url":"http://arxiv.org/abs/2403.13672v1","category":"cs.LG"}
{"created":"2024-03-20 14:59:10","title":"A globalized and preconditioned Newton-CG solver for metric-aware curved high-order mesh optimization","abstract":"We present a specific-purpose globalized and preconditioned Newton-CG solver to minimize a metric-aware curved high-order mesh distortion. The solver is specially devised to optimize curved high-order meshes for high polynomial degrees with a target metric featuring non-uniform sizing, high stretching ratios, and curved alignment -- exactly the features that stiffen the optimization problem. To this end, we consider two ingredients: a specific-purpose globalization and a specific-purpose Jacobi-$\\text{iLDL}^{\\text{T}}(0)$ preconditioning with varying accuracy and curvature tolerances (dynamic forcing terms) for the CG method. These improvements are critical in stiff problems because, without them, the large number of non-linear and linear iterations makes curved optimization impractical. Finally, to analyze the performance of our method, the results compare the specific-purpose solver with standard optimization methods. For this, we measure the matrix-vector products indicating the solver computational cost and the line-search iterations indicating the total amount of objective function evaluations. When we combine the globalization and the linear solver ingredients, we conclude that the specific-purpose Newton-CG solver reduces the total number of matrix-vector products by one order of magnitude. Moreover, the number of non-linear and line-search iterations is mainly smaller but of similar magnitude.","sentences":["We present a specific-purpose globalized and preconditioned Newton-CG solver to minimize a metric-aware curved high-order mesh distortion.","The solver is specially devised to optimize curved high-order meshes for high polynomial degrees with a target metric featuring non-uniform sizing, high stretching ratios, and curved alignment -- exactly the features that stiffen the optimization problem.","To this end, we consider two ingredients: a specific-purpose globalization and a specific-purpose Jacobi-$\\text{iLDL}^{\\text{T}}(0)$ preconditioning with varying accuracy and curvature tolerances (dynamic forcing terms) for the CG method.","These improvements are critical in stiff problems because, without them, the large number of non-linear and linear iterations makes curved optimization impractical.","Finally, to analyze the performance of our method, the results compare the specific-purpose solver with standard optimization methods.","For this, we measure the matrix-vector products indicating the solver computational cost and the line-search iterations indicating the total amount of objective function evaluations.","When we combine the globalization and the linear solver ingredients, we conclude that the specific-purpose Newton-CG solver reduces the total number of matrix-vector products by one order of magnitude.","Moreover, the number of non-linear and line-search iterations is mainly smaller but of similar magnitude."],"url":"http://arxiv.org/abs/2403.13654v1","category":"cs.CE"}
{"created":"2024-03-20 14:43:51","title":"LaCE-LHMP: Airflow Modelling-Inspired Long-Term Human Motion Prediction By Enhancing Laminar Characteristics in Human Flow","abstract":"Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments. It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring. However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions. The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP). To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP). Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions. Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction. We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns.","sentences":["Long-term human motion prediction (LHMP) is essential for safely operating autonomous robots and vehicles in populated environments.","It is fundamental for various applications, including motion planning, tracking, human-robot interaction and safety monitoring.","However, accurate prediction of human trajectories is challenging due to complex factors, including, for example, social norms and environmental conditions.","The influence of such factors can be captured through Maps of Dynamics (MoDs), which encode spatial motion patterns learned from (possibly scattered and partial) past observations of motion in the environment and which can be used for data-efficient, interpretable motion prediction (MoD-LHMP).","To address the limitations of prior work, especially regarding accuracy and sensitivity to anomalies in long-term prediction, we propose the Laminar Component Enhanced LHMP approach (LaCE-LHMP).","Our approach is inspired by data-driven airflow modelling, which estimates laminar and turbulent flow components and uses predominantly the laminar components to make flow predictions.","Based on the hypothesis that human trajectory patterns also manifest laminar flow (that represents predictable motion) and turbulent flow components (that reflect more unpredictable and arbitrary motion), LaCE-LHMP extracts the laminar patterns in human dynamics and uses them for human motion prediction.","We demonstrate the superior prediction performance of LaCE-LHMP through benchmark comparisons with state-of-the-art LHMP methods, offering an unconventional perspective and a more intuitive understanding of human movement patterns."],"url":"http://arxiv.org/abs/2403.13640v1","category":"cs.RO"}
{"created":"2024-03-20 14:14:17","title":"Importance of accounting for student identities and intersectionality for creating equitable and inclusive physics learning environments","abstract":"This research focuses on the experiences of seven undergraduate women who were majoring in physics at a small liberal arts college. In the semi-structured, empathetic interviews we conducted, the women discuss how they decided to major in physics, their interactions with their peers and instructors, who has supported them during their physics trajectory, and suggestions that would improve their experiences in physics. We use Standpoint Theory and focus on the experiences of undergraduate women to get a holistic perspective of how they became interested in physics, how they have been supported in their physics journey as well as identify any challenges that they faced in their undergraduate physics program due to their identity. Using synergistic frameworks such as the domains of power and the Holistic Ecosystem for Learning Physics in an Inclusive and Equitable Environment (HELPIEE), we analyze how those in the position of power, e.g., instructors, can play important roles in establishing and maintaining safe, equitable, and inclusive environments for students, which is especially important for historically marginalized students such as women and ethnic and racial minority students in physics. Using these frameworks, we compare the experiences of these women with those in Johnson's research at a small college and our prior investigation at a large research university. We also discuss the suggestions provided by the undergraduate women to implement in the future to support current and future undergraduate women in physics and astronomy. Their suggestions are separated as personal advice for peers and suggestions for physics instructors.","sentences":["This research focuses on the experiences of seven undergraduate women who were majoring in physics at a small liberal arts college.","In the semi-structured, empathetic interviews we conducted, the women discuss how they decided to major in physics, their interactions with their peers and instructors, who has supported them during their physics trajectory, and suggestions that would improve their experiences in physics.","We use Standpoint Theory and focus on the experiences of undergraduate women to get a holistic perspective of how they became interested in physics, how they have been supported in their physics journey as well as identify any challenges that they faced in their undergraduate physics program due to their identity.","Using synergistic frameworks such as the domains of power and the Holistic Ecosystem for Learning Physics in an Inclusive and Equitable Environment (HELPIEE), we analyze how those in the position of power, e.g., instructors, can play important roles in establishing and maintaining safe, equitable, and inclusive environments for students, which is especially important for historically marginalized students such as women and ethnic and racial minority students in physics.","Using these frameworks, we compare the experiences of these women with those in Johnson's research at a small college and our prior investigation at a large research university.","We also discuss the suggestions provided by the undergraduate women to implement in the future to support current and future undergraduate women in physics and astronomy.","Their suggestions are separated as personal advice for peers and suggestions for physics instructors."],"url":"http://arxiv.org/abs/2403.13621v1","category":"physics.ed-ph"}
{"created":"2024-03-20 13:49:06","title":"Lattice piecewise affine approximation of explicit model predictive control with application to satellite attitude control","abstract":"Satellite attitude cotrol is a crucial part of aerospace technology, and model predictive control(MPC) is one of the most promising controllers in this area, which will be less effective if real-time online optimization can not be achieved. Explicit MPC converts the online calculation into a table lookup process, however the solution is difficult to obtain if the system dimension is high or the constraints are complex. The lattice piecewise affine(PWA) function was used to represent the control law of explicit MPC, although the online calculation complexity is reduced, the offline calculation is still prohibitive for complex problems. In this paper, we use the sample points in the feasible region with their corresponding affine functions to construct the lattice PWA approximation of the optimal MPC controller designed for satellite attitude control. The asymptotic stability of satellite attitude control system under lattice PWA approximation has been proven, and simulations are executed to verify that the proposed method can achieve almost the same performance as linear online MPC with much lower online computational complexity and use less fuel than LQR method.","sentences":["Satellite attitude cotrol is a crucial part of aerospace technology, and model predictive control(MPC) is one of the most promising controllers in this area, which will be less effective if real-time online optimization can not be achieved.","Explicit MPC converts the online calculation into a table lookup process, however the solution is difficult to obtain if the system dimension is high or the constraints are complex.","The lattice piecewise affine(PWA) function was used to represent the control law of explicit MPC, although the online calculation complexity is reduced, the offline calculation is still prohibitive for complex problems.","In this paper, we use the sample points in the feasible region with their corresponding affine functions to construct the lattice PWA approximation of the optimal MPC controller designed for satellite attitude control.","The asymptotic stability of satellite attitude control system under lattice PWA approximation has been proven, and simulations are executed to verify that the proposed method can achieve almost the same performance as linear online MPC with much lower online computational complexity and use less fuel than LQR method."],"url":"http://arxiv.org/abs/2403.13601v1","category":"eess.SY"}
{"created":"2024-03-20 13:43:57","title":"Optimizing Transparent Electrodes: Interplay of High Purity SWCNTs network and a Polymer","abstract":"The discovery of transparent electrodes led to the development of optoelectronic devices such as OLEDs, LCDs, touchscreens, IR sensors, etc. Since ITO has many drawbacks in respect of its production cost and limited transparency in IR, carbon nanotubes (CNTs) have been a potential replacement for ITO due to their exceptional electrical and optical properties, especially in the IR region. In this work, we present the development of a CNT-polymer composite thin film that exhibits outstanding transparency across both visible and IR spectra prepared by layer-by-layer (LbL) technique. This approach not only ensures uniform integration and crosslinking of CNTs into lightweight matrices, but also represents a cost-effective method for producing transparent electrodes with remarkable optical and electrical properties. The produced films achieved a transparency above 80% in the UV-VIS range and approximately 70% in the mid-IR range. The sheet resistance of the fabricated thin films was measured at about 4 kOhm/sq, showing a tendency to decrease with the number of bilayers. Furthermore, in this work we have investigated electrical properties and transport mechanisms in more detail with computational analysis. Computational analysis was performed to better understand the electrical behavior of nanotube-polymer junctions in the interbundle structure. Based on all results, we propose that the transparent electrodes with 4 and 6 bilayers are the most optimal structures in terms of optical and electrical properties.","sentences":["The discovery of transparent electrodes led to the development of optoelectronic devices such as OLEDs, LCDs, touchscreens, IR sensors, etc.","Since ITO has many drawbacks in respect of its production cost and limited transparency in IR, carbon nanotubes (CNTs) have been a potential replacement for ITO due to their exceptional electrical and optical properties, especially in the IR region.","In this work, we present the development of a CNT-polymer composite thin film that exhibits outstanding transparency across both visible and IR spectra prepared by layer-by-layer (LbL) technique.","This approach not only ensures uniform integration and crosslinking of CNTs into lightweight matrices, but also represents a cost-effective method for producing transparent electrodes with remarkable optical and electrical properties.","The produced films achieved a transparency above 80% in the UV-VIS range and approximately 70% in the mid-IR range.","The sheet resistance of the fabricated thin films was measured at about 4 kOhm/sq, showing a tendency to decrease with the number of bilayers.","Furthermore, in this work we have investigated electrical properties and transport mechanisms in more detail with computational analysis.","Computational analysis was performed to better understand the electrical behavior of nanotube-polymer junctions in the interbundle structure.","Based on all results, we propose that the transparent electrodes with 4 and 6 bilayers are the most optimal structures in terms of optical and electrical properties."],"url":"http://arxiv.org/abs/2403.13594v1","category":"physics.app-ph"}
{"created":"2024-03-20 13:24:41","title":"Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation","abstract":"In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models.","sentences":["In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation.","We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses.","We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously.","Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training.","Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models."],"url":"http://arxiv.org/abs/2403.13578v1","category":"cs.CL"}
{"created":"2024-03-20 13:12:59","title":"Movable Antenna Enabled Interference Network: Joint Antenna Position and Beamforming Design","abstract":"This paper investigates the utility of movable antenna (MA) assistance for the multiple-input single-output (MISO) interference channel. We exploit an additional design degree of freedom provided by MA to enhance the desired signal and suppress interference so as to reduce the total transmit power of interference network. To this end, we jointly optimize the MA positions and transmit beamforming, subject to the signal-to-interference-plus-noise ratio constraints of users. To address the non-convex optimization problem, we propose an efficient iterative algorithm to alternately optimize the MA positions via successive convex approximation method and the transmit beamforming via second-order cone program approach. Numerical results demonstrate that the proposed MA-enabled MISO interference network outperforms its conventional counterpart without MA, which significantly enhances the capability of inter-cell frequency reuse and reduces the complexity of transmitter design.","sentences":["This paper investigates the utility of movable antenna (MA) assistance for the multiple-input single-output (MISO) interference channel.","We exploit an additional design degree of freedom provided by MA to enhance the desired signal and suppress interference so as to reduce the total transmit power of interference network.","To this end, we jointly optimize the MA positions and transmit beamforming, subject to the signal-to-interference-plus-noise ratio constraints of users.","To address the non-convex optimization problem, we propose an efficient iterative algorithm to alternately optimize the MA positions via successive convex approximation method and the transmit beamforming via second-order cone program approach.","Numerical results demonstrate that the proposed MA-enabled MISO interference network outperforms its conventional counterpart without MA, which significantly enhances the capability of inter-cell frequency reuse and reduces the complexity of transmitter design."],"url":"http://arxiv.org/abs/2403.13573v1","category":"cs.IT"}
{"created":"2024-03-20 12:26:51","title":"Scalability of quantum error mitigation techniques: from utility to advantage","abstract":"Error mitigation has elevated quantum computing to the scale of hundreds of qubits and tens of layers; however, yet larger scales (deeper circuits) are needed to fully exploit the potential of quantum computing to solve practical problems otherwise intractable. Here we demonstrate three key results that pave the way for the leap from quantum utility to quantum advantage: (1) we present a thorough derivation of random and systematic errors associated to the most advanced error mitigation strategies, including probabilistic error cancellation (PEC), zero noise extrapolation (ZNE) with probabilistic error amplification, and tensor-network error mitigation (TEM); (2) we prove that TEM (i) has the lowest sampling overhead among all three techniques under realistic noise, (ii) is optimal, in the sense that it saturates the universal lower cost bound for error mitigation, and (iii) is therefore the most promising approach to quantum advantage; (3) we propose a concrete notion of practical quantum advantage in terms of the universality of algorithms, stemming from the commercial need for a problem-independent quantum simulation device. We also establish a connection between error mitigation, relying on additional measurements, and error correction, relying on additional qubits, by demonstrating that TEM with a sufficient bond dimension works similarly to an error correcting code of distance 3. We foresee that the interplay and trade-off between the two resources will be the key to a smooth transition between error mitigation and error correction, and hence between near-term and fault-tolerant quantum computers. Meanwhile, we argue that quantum computing with optimal error mitigation, relying on modest classical computer power for tensor network contraction, has the potential to reach larger scales in accurate simulation than classical methods alone.","sentences":["Error mitigation has elevated quantum computing to the scale of hundreds of qubits and tens of layers; however, yet larger scales (deeper circuits) are needed to fully exploit the potential of quantum computing to solve practical problems otherwise intractable.","Here we demonstrate three key results that pave the way for the leap from quantum utility to quantum advantage: (1) we present a thorough derivation of random and systematic errors associated to the most advanced error mitigation strategies, including probabilistic error cancellation (PEC), zero noise extrapolation (ZNE) with probabilistic error amplification, and tensor-network error mitigation (TEM); (2) we prove that TEM (i) has the lowest sampling overhead among all three techniques under realistic noise, (ii) is optimal, in the sense that it saturates the universal lower cost bound for error mitigation, and (iii) is therefore the most promising approach to quantum advantage; (3) we propose a concrete notion of practical quantum advantage in terms of the universality of algorithms, stemming from the commercial need for a problem-independent quantum simulation device.","We also establish a connection between error mitigation, relying on additional measurements, and error correction, relying on additional qubits, by demonstrating that TEM with a sufficient bond dimension works similarly to an error correcting code of distance 3.","We foresee that the interplay and trade-off between the two resources will be the key to a smooth transition between error mitigation and error correction, and hence between near-term and fault-tolerant quantum computers.","Meanwhile, we argue that quantum computing with optimal error mitigation, relying on modest classical computer power for tensor network contraction, has the potential to reach larger scales in accurate simulation than classical methods alone."],"url":"http://arxiv.org/abs/2403.13542v1","category":"quant-ph"}
{"created":"2024-03-20 12:02:51","title":"Picosecond Femtojoule Resistive Switching in Nanoscale VO$_{2}$ Memristors","abstract":"Beyond-Moore computing technologies are expected to provide a sustainable alternative to the von Neumann approach not only due to their down-scaling potential but also via exploiting device-level functional complexity at the lowest possible energy consumption. The dynamics of the Mott transition in correlated electron oxides, such as vanadium dioxide, has been identified as a rich and reliable source of such functional complexity. However, its full potential in high-speed and low-power operation has been largely unexplored. We fabricated nanoscale VO$_{2}$ devices embedded in a broad-band test circuit to study the speed and energy limitations of their resistive switching operation. Our picosecond time-resolution, real-time resistive switching experiments and numerical simulations demonstrate that tunable low-resistance states can be set by the application of 20~ps long, $<$1.7~V amplitude voltage pulses at 15~ps incubation times and switching energies starting from a few femtojoule. Moreover, we demonstrate that at nanometer-scale device sizes not only the electric field induced insulator-to-metal transition, but also the thermal conduction limited metal-to-insulator transition can take place at timescales of 100's of picoseconds. These orders of magnitude breakthroughs open the route to the design of high-speed and low-power dynamical circuits for a plethora of neuromorphic computing applications from pattern recognition to numerical optimization.","sentences":["Beyond-Moore computing technologies are expected to provide a sustainable alternative to the von Neumann approach not only due to their down-scaling potential but also via exploiting device-level functional complexity at the lowest possible energy consumption.","The dynamics of the Mott transition in correlated electron oxides, such as vanadium dioxide, has been identified as a rich and reliable source of such functional complexity.","However, its full potential in high-speed and low-power operation has been largely unexplored.","We fabricated nanoscale VO$_{2}$ devices embedded in a broad-band test circuit to study the speed and energy limitations of their resistive switching operation.","Our picosecond time-resolution, real-time resistive switching experiments and numerical simulations demonstrate that tunable low-resistance states can be set by the application of 20~ps long, $<$1.7~V amplitude voltage pulses at 15~ps incubation times and switching energies starting from a few femtojoule.","Moreover, we demonstrate that at nanometer-scale device sizes not only the electric field induced insulator-to-metal transition, but also the thermal conduction limited metal-to-insulator transition can take place at timescales of 100's of picoseconds.","These orders of magnitude breakthroughs open the route to the design of high-speed and low-power dynamical circuits for a plethora of neuromorphic computing applications from pattern recognition to numerical optimization."],"url":"http://arxiv.org/abs/2403.13530v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-20 12:01:02","title":"Defining metric-aware size-shape measures to validate and optimize curved high-order meshes","abstract":"We define a regularized size-shape distortion (quality) measure for curved high-order elements on a Riemannian space. To this end, we measure the deviation of a given element, straight-sided or curved, from the stretching, alignment, and sizing determined by a target metric. The defined distortion (quality) is suitable to check the validity and the quality of straight-sided and curved elements on Riemannian spaces determined by constant and point-wise varying metrics. The examples illustrate that the distortion can be minimized to curve (deform) the elements of a given high-order (linear) mesh and try to match with curved (linear) elements the point-wise stretching, alignment, and sizing of a discrete target metric tensor. In addition, the resulting meshes simultaneously match the curved features of the target metric and boundary. Finally, to verify if the minimization of the metric-aware size-shape distortion leads to meshes approximating the target metric, we compute the Riemannian measures for the element edges, faces, and cells. The results show that, when compared to anisotropic straight-sided meshes, the Riemannian measures of the curved high-order mesh entities are closer to unit. Furthermore, the optimized meshes illustrate the potential of curved $r$-adaptation to improve the accuracy of a function representation.","sentences":["We define a regularized size-shape distortion (quality) measure for curved high-order elements on a Riemannian space.","To this end, we measure the deviation of a given element, straight-sided or curved, from the stretching, alignment, and sizing determined by a target metric.","The defined distortion (quality) is suitable to check the validity and the quality of straight-sided and curved elements on Riemannian spaces determined by constant and point-wise varying metrics.","The examples illustrate that the distortion can be minimized to curve (deform) the elements of a given high-order (linear) mesh and try to match with curved (linear) elements the point-wise stretching, alignment, and sizing of a discrete target metric tensor.","In addition, the resulting meshes simultaneously match the curved features of the target metric and boundary.","Finally, to verify if the minimization of the metric-aware size-shape distortion leads to meshes approximating the target metric, we compute the Riemannian measures for the element edges, faces, and cells.","The results show that, when compared to anisotropic straight-sided meshes, the Riemannian measures of the curved high-order mesh entities are closer to unit.","Furthermore, the optimized meshes illustrate the potential of curved $r$-adaptation to improve the accuracy of a function representation."],"url":"http://arxiv.org/abs/2403.13528v1","category":"cs.CE"}
{"created":"2024-03-20 11:42:42","title":"The defect, the Malgrange functor, and linear control systems","abstract":"The notion of defect of a finitely presented functor on a module category is extended to arbitrary additive functors. The new defect and the contravariant Yoneda embedding form a right adjoint pair. The main result identifies the defect of the covariant Hom modulo projectives with the Bass torsion of the fixed argument. When applied to a linear control systems, it shows that the defect of the Malgrange functor of the system modulo projectives is isomorphic to the autonomy of the system. Furthermore, the defect of the contravariant Hom modulo injectives is shown to be isomorphic to the cotorsion coradical of the fixed argument. Since the Auslander-Gruson-Jensen transform of cotorsion is isomorphic to torsion, the above results raise two important questions: a) what is a control-theoretic interpretation of the covariant Yoneda embedding of the Malgrange module modulo injectives, and b) what is a control-theoretic interpretation of the Auslander-Gruson-Jensen duality?","sentences":["The notion of defect of a finitely presented functor on a module category is extended to arbitrary additive functors.","The new defect and the contravariant Yoneda embedding form a right adjoint pair.","The main result identifies the defect of the covariant Hom modulo projectives with the Bass torsion of the fixed argument.","When applied to a linear control systems, it shows that the defect of the Malgrange functor of the system modulo projectives is isomorphic to the autonomy of the system.","Furthermore, the defect of the contravariant Hom modulo injectives is shown to be isomorphic to the cotorsion coradical of the fixed argument.","Since the Auslander-Gruson-Jensen transform of cotorsion is isomorphic to torsion, the above results raise two important questions: a) what is a control-theoretic interpretation of the covariant Yoneda embedding of the Malgrange module modulo injectives, and b) what is a control-theoretic interpretation of the Auslander-Gruson-Jensen duality?"],"url":"http://arxiv.org/abs/2403.13520v1","category":"math.RT"}
{"created":"2024-03-20 11:31:39","title":"Efficient numerical methods for the Maxey-Riley equations with Basset history term","abstract":"The Maxey-Riley equations (MRE) describe the motion of a finite-sized, spherical particle in a fluid. Because of wake effects, the force acting on a particle depends on its past trajectory. This is modelled by an integral term in the MRE, also called Basset force, that makes its numerical solution challenging and memory intensive. A recent approach proposed by Prasath, Vasan and Govindarajan exploits connections between the integral term and fractional derivatives to reformulate the MRE as a time-dependent partial differential equation on a semi-infinite pseudo-space. They also propose a numerical algorithm based on polynomial expansions. This paper develops a numerical approach based on finite difference instead, by adopting techniques by Koleva and Fazio and Janelli to cope with the issues of having an unbounded spatial domain. We compare convergence order and computational efficiency for particles of varying size and density of the polynomial expansion by Prasath et al., our finite difference schemes and a direct integrator for the MRE based on multi-step methods proposed by Daitche.","sentences":["The Maxey-Riley equations (MRE) describe the motion of a finite-sized, spherical particle in a fluid.","Because of wake effects, the force acting on a particle depends on its past trajectory.","This is modelled by an integral term in the MRE, also called Basset force, that makes its numerical solution challenging and memory intensive.","A recent approach proposed by Prasath, Vasan and Govindarajan exploits connections between the integral term and fractional derivatives to reformulate the MRE as a time-dependent partial differential equation on a semi-infinite pseudo-space.","They also propose a numerical algorithm based on polynomial expansions.","This paper develops a numerical approach based on finite difference instead, by adopting techniques by Koleva and Fazio and Janelli to cope with the issues of having an unbounded spatial domain.","We compare convergence order and computational efficiency for particles of varying size and density of the polynomial expansion by Prasath et al., our finite difference schemes and a direct integrator for the MRE based on multi-step methods proposed by Daitche."],"url":"http://arxiv.org/abs/2403.13515v1","category":"math.NA"}
{"created":"2024-03-20 10:57:17","title":"Improved Baselines for Data-efficient Perceptual Augmentation of LLMs","abstract":"The abilities of large language models (LLMs) have recently progressed to unprecedented levels, paving the way to novel applications in a wide variety of areas. In computer vision, LLMs can be used to prime vision-language tasks such image captioning and visual question answering when coupled with pre-trained vision backbones. While different approaches have been explored to interface LLMs with ``perceptual backbones'' that process, e.g., visual or audio data, they are often explored for different tasks, different datasets, and using different perceptual backbones and language models, hindering direct comparison of the interfacing mechanisms. To remedy this lack of comparability between methods, we present an extensive experimental evaluation of different interfacing mechanisms, across multiple tasks (including image, video, and audio captioning as well as visual question answering), datasets and backbones, paying special attention to low-data settings. We find improved performance using existing mechanisms over state-of-the-art results, and identify a new interfacing mechanism that yields (near) optimal results across different tasks, while obtaining a 4x reduction in training time.","sentences":["The abilities of large language models (LLMs) have recently progressed to unprecedented levels, paving the way to novel applications in a wide variety of areas.","In computer vision, LLMs can be used to prime vision-language tasks such image captioning and visual question answering when coupled with pre-trained vision backbones.","While different approaches have been explored to interface LLMs with ``perceptual backbones'' that process, e.g., visual or audio data, they are often explored for different tasks, different datasets, and using different perceptual backbones and language models, hindering direct comparison of the interfacing mechanisms.","To remedy this lack of comparability between methods, we present an extensive experimental evaluation of different interfacing mechanisms, across multiple tasks (including image, video, and audio captioning as well as visual question answering), datasets and backbones, paying special attention to low-data settings.","We find improved performance using existing mechanisms over state-of-the-art results, and identify a new interfacing mechanism that yields (near) optimal results across different tasks, while obtaining a 4x reduction in training time."],"url":"http://arxiv.org/abs/2403.13499v1","category":"cs.CV"}
{"created":"2024-03-20 10:46:29","title":"Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications","abstract":"In this paper, we present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypo-elliptic. In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical simulation of such schemes. Motivated by recent developments, we introduce a new MLMC estimator of expectations, which does not require simulation of intractable L\\'evy areas but has a weak error of order 2 and achieves the optimal computational complexity. We then show how this approach can be used in the context of the filtering problem associated to partially observed diffusions with discrete time observations. We illustrate with numerical simulations that our new approaches provide efficiency gains for several problems relative to some existing methods.","sentences":["In this paper, we present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypo-elliptic.","In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical simulation of such schemes.","Motivated by recent developments, we introduce a new MLMC estimator of expectations, which does not require simulation of intractable L\\'evy areas but has a weak error of order 2 and achieves the optimal computational complexity.","We then show how this approach can be used in the context of the filtering problem associated to partially observed diffusions with discrete time observations.","We illustrate with numerical simulations that our new approaches provide efficiency gains for several problems relative to some existing methods."],"url":"http://arxiv.org/abs/2403.13489v1","category":"math.NA"}
{"created":"2024-03-20 10:44:00","title":"Tensor Quantum Programming","abstract":"Running quantum algorithms often involves implementing complex quantum circuits with such a large number of multi-qubit gates that the challenge of tackling practical applications appears daunting. To date, no experiments have successfully demonstrated a quantum advantage due to the ease with which the results can be adequately replicated on classical computers through the use of tensor network algorithms. Additionally, it remains unclear even in theory where exactly these advantages are rooted within quantum systems because the logarithmic complexity commonly associated with quantum algorithms is also present in algorithms based on tensor networks. In this article, we propose a novel approach called Tensor Quantum Programming, which leverages tensor networks for hybrid quantum computing. Our key insight is that the primary challenge of algorithms based on tensor networks lies in their high ranks (bond dimensions). Quantum computing offers a potential solution to this challenge, as an ideal quantum computer can represent tensors with arbitrarily high ranks in contrast to classical counterparts, which indicates the way towards quantum advantage. While tensor-based vector-encoding and state-readout are known procedures, the matrix-encoding required for performing matrix-vector multiplications directly on quantum devices remained unsolved. Here, we developed an algorithm that encodes Matrix Product Operators into quantum circuits with a depth that depends linearly on the number of qubits. It demonstrates effectiveness on up to 50 qubits for several matrices frequently encountered in differential equations, optimization problems, and quantum chemistry. We view this work as an initial stride towards the creation of genuinely practical quantum algorithms.","sentences":["Running quantum algorithms often involves implementing complex quantum circuits with such a large number of multi-qubit gates that the challenge of tackling practical applications appears daunting.","To date, no experiments have successfully demonstrated a quantum advantage due to the ease with which the results can be adequately replicated on classical computers through the use of tensor network algorithms.","Additionally, it remains unclear even in theory where exactly these advantages are rooted within quantum systems because the logarithmic complexity commonly associated with quantum algorithms is also present in algorithms based on tensor networks.","In this article, we propose a novel approach called Tensor Quantum Programming, which leverages tensor networks for hybrid quantum computing.","Our key insight is that the primary challenge of algorithms based on tensor networks lies in their high ranks (bond dimensions).","Quantum computing offers a potential solution to this challenge, as an ideal quantum computer can represent tensors with arbitrarily high ranks in contrast to classical counterparts, which indicates the way towards quantum advantage.","While tensor-based vector-encoding and state-readout are known procedures, the matrix-encoding required for performing matrix-vector multiplications directly on quantum devices remained unsolved.","Here, we developed an algorithm that encodes Matrix Product Operators into quantum circuits with a depth that depends linearly on the number of qubits.","It demonstrates effectiveness on up to 50 qubits for several matrices frequently encountered in differential equations, optimization problems, and quantum chemistry.","We view this work as an initial stride towards the creation of genuinely practical quantum algorithms."],"url":"http://arxiv.org/abs/2403.13486v1","category":"quant-ph"}
{"created":"2024-03-20 10:24:54","title":"A metric counterpart of the Gu-Yung formula","abstract":"In this note we consider a generalisation to the metric setting of the recent work [Gu-Yung, JFA 281 (2021), 109075]. In particular, we show that under relatively weak conditions on a metric measure space $(X,d,\\nu)$, it holds true that \\[ \\bigg[ \\frac{u(x)-u(y)}{d(x,y)^{\\frac{s}{p}}} \\bigg]_{L^p_w(X \\times X, \\nu \\otimes \\nu)} \\approx \\| u \\|_{L^p(X,\\nu)}, \\] where $s$ is a generalised dimension associated to $X$ and $[\\cdot]_{L^p_w}$ is the weak Lebesgue norm. We provide some counterexamples which show that our assumptions are optimal.","sentences":["In this note we consider a generalisation to the metric setting of the recent work [Gu-Yung, JFA 281 (2021), 109075].","In particular, we show that under relatively weak conditions on a metric measure space $(X,d,\\nu)$, it holds true that \\[ \\bigg[ \\frac{u(x)-u(y)}{d(x,y)^{\\frac{s}{p}}} \\bigg]_{L^p_w(X \\times X, \\nu \\otimes \\nu)} \\approx \\| u \\|_{L^p(X,\\nu)}, \\] where $s$ is a generalised dimension associated to $X$ and $[\\cdot]_{L^p_w}$ is the weak Lebesgue norm.","We provide some counterexamples which show that our assumptions are optimal."],"url":"http://arxiv.org/abs/2403.13475v1","category":"math.FA"}
{"created":"2024-03-20 10:22:22","title":"Iterative Active-Inactive Obstacle Classification for Time-Optimal Collision Avoidance","abstract":"Time-optimal obstacle avoidance is a prevalent problem encountered in various fields, including robotics and autonomous vehicles, where the task involves determining a path for a moving vehicle to reach its goal while navigating around obstacles within its environment. This problem becomes increasingly challenging as the number of obstacles in the environment rises. We propose an iterative active-inactive obstacle approach, which involves identifying a subset of the obstacles as \"active\", that considers solely the effect of the \"active\" obstacles on the path of the moving vehicle. The remaining obstacles are considered \"inactive\" and are not considered in the path planning process. The obstacles are classified as 'active' on the basis of previous findings derived from prior iterations. This approach allows for a more efficient calculation of the optimal path by reducing the number of obstacles that need to be considered. The effectiveness of the proposed method is demonstrated with two different dynamic models using the various number of obstacles. The results show that the proposed method is able to find the optimal path in a timely manner, while also being able to handle a large number of obstacles in the environment and the constraints on the motion of the object.","sentences":["Time-optimal obstacle avoidance is a prevalent problem encountered in various fields, including robotics and autonomous vehicles, where the task involves determining a path for a moving vehicle to reach its goal while navigating around obstacles within its environment.","This problem becomes increasingly challenging as the number of obstacles in the environment rises.","We propose an iterative active-inactive obstacle approach, which involves identifying a subset of the obstacles as \"active\", that considers solely the effect of the \"active\" obstacles on the path of the moving vehicle.","The remaining obstacles are considered \"inactive\" and are not considered in the path planning process.","The obstacles are classified as 'active' on the basis of previous findings derived from prior iterations.","This approach allows for a more efficient calculation of the optimal path by reducing the number of obstacles that need to be considered.","The effectiveness of the proposed method is demonstrated with two different dynamic models using the various number of obstacles.","The results show that the proposed method is able to find the optimal path in a timely manner, while also being able to handle a large number of obstacles in the environment and the constraints on the motion of the object."],"url":"http://arxiv.org/abs/2403.13474v1","category":"cs.RO"}
{"created":"2024-03-20 10:18:20","title":"Progressive trajectory matching for medical dataset distillation","abstract":"It is essential but challenging to share medical image datasets due to privacy issues, which prohibit building foundation models and knowledge transfer. In this paper, we propose a novel dataset distillation method to condense the original medical image datasets into a synthetic one that preserves useful information for building an analysis model without accessing the original datasets. Existing methods tackle only natural images by randomly matching parts of the training trajectories of the model parameters trained by the whole real datasets. However, through extensive experiments on medical image datasets, the training process is extremely unstable and achieves inferior distillation results. To solve these barriers, we propose to design a novel progressive trajectory matching strategy to improve the training stability for medical image dataset distillation. Additionally, it is observed that improved stability prevents the synthetic dataset diversity and final performance improvements. Therefore, we propose a dynamic overlap mitigation module that improves the synthetic dataset diversity by dynamically eliminating the overlap across different images and retraining parts of the synthetic images for better convergence. Finally, we propose a new medical image dataset distillation benchmark of various modalities and configurations to promote fair evaluations. It is validated that our proposed method achieves 8.33% improvement over previous state-of-the-art methods on average, and 11.7% improvement when ipc=2 (i.e., image per class is 2). Codes and benchmarks will be released.","sentences":["It is essential but challenging to share medical image datasets due to privacy issues, which prohibit building foundation models and knowledge transfer.","In this paper, we propose a novel dataset distillation method to condense the original medical image datasets into a synthetic one that preserves useful information for building an analysis model without accessing the original datasets.","Existing methods tackle only natural images by randomly matching parts of the training trajectories of the model parameters trained by the whole real datasets.","However, through extensive experiments on medical image datasets, the training process is extremely unstable and achieves inferior distillation results.","To solve these barriers, we propose to design a novel progressive trajectory matching strategy to improve the training stability for medical image dataset distillation.","Additionally, it is observed that improved stability prevents the synthetic dataset diversity and final performance improvements.","Therefore, we propose a dynamic overlap mitigation module that improves the synthetic dataset diversity by dynamically eliminating the overlap across different images and retraining parts of the synthetic images for better convergence.","Finally, we propose a new medical image dataset distillation benchmark of various modalities and configurations to promote fair evaluations.","It is validated that our proposed method achieves 8.33% improvement over previous state-of-the-art methods on average, and 11.7% improvement when ipc=2 (i.e., image per class is 2).","Codes and benchmarks will be released."],"url":"http://arxiv.org/abs/2403.13469v1","category":"cs.CV"}
{"created":"2024-03-20 10:08:05","title":"Tikhonov regularized exterior penalty dynamics for constrained variational inequalities","abstract":"Solving equilibrium problems under constraints is an important problem in optimization and optimal control. In this context an important practical challenge is the efficient incorporation of constraints. We develop a continuous-time method for solving constrained variational inequalities based on a new penalty regulated dynamical system in a general potentially infinite-dimensional Hilbert space. In order to obtain strong convergence of the issued trajectory of our method, we incorporate an explicit Tikhonov regularization parameter in our method, leading to a class of time-varying monotone inclusion problems featuring multiscale aspects. Besides strong convergence, we illustrate the practical efficiency of our developed method in solving constrained min-max problems.","sentences":["Solving equilibrium problems under constraints is an important problem in optimization and optimal control.","In this context an important practical challenge is the efficient incorporation of constraints.","We develop a continuous-time method for solving constrained variational inequalities based on a new penalty regulated dynamical system in a general potentially infinite-dimensional Hilbert space.","In order to obtain strong convergence of the issued trajectory of our method, we incorporate an explicit Tikhonov regularization parameter in our method, leading to a class of time-varying monotone inclusion problems featuring multiscale aspects.","Besides strong convergence, we illustrate the practical efficiency of our developed method in solving constrained min-max problems."],"url":"http://arxiv.org/abs/2403.13460v1","category":"math.OC"}
{"created":"2024-03-20 10:01:52","title":"FACT: Fast and Active Coordinate Initialization for Vision-based Drone Swarms","abstract":"Swarm robots have sparked remarkable developments across a range of fields. While it is necessary for various applications in swarm robots, a fast and robust coordinate initialization in vision-based drone swarms remains elusive. To this end, our paper proposes a complete system to recover a swarm's initial relative pose on platforms with size, weight, and power (SWaP) constraints. To overcome limited coverage of field-of-view (FoV), the drones rotate in place to obtain observations. To tackle the anonymous measurements, we formulate a non-convex rotation estimation problem and transform it into a semi-definite programming (SDP) problem, which can steadily obtain global optimal values. Then we utilize the Hungarian algorithm to recover relative translation and correspondences between observations and drone identities. To safely acquire complete observations, we actively search for positions and generate feasible trajectories to avoid collisions. To validate the practicability of our system, we conduct experiments on a vision-based drone swarm with only stereo cameras and inertial measurement units (IMUs) as sensors. The results demonstrate that the system can robustly get accurate relative poses in real time with limited onboard computation resources. The source code is released.","sentences":["Swarm robots have sparked remarkable developments across a range of fields.","While it is necessary for various applications in swarm robots, a fast and robust coordinate initialization in vision-based drone swarms remains elusive.","To this end, our paper proposes a complete system to recover a swarm's initial relative pose on platforms with size, weight, and power (SWaP) constraints.","To overcome limited coverage of field-of-view (FoV), the drones rotate in place to obtain observations.","To tackle the anonymous measurements, we formulate a non-convex rotation estimation problem and transform it into a semi-definite programming (SDP) problem, which can steadily obtain global optimal values.","Then we utilize the Hungarian algorithm to recover relative translation and correspondences between observations and drone identities.","To safely acquire complete observations, we actively search for positions and generate feasible trajectories to avoid collisions.","To validate the practicability of our system, we conduct experiments on a vision-based drone swarm with only stereo cameras and inertial measurement units (IMUs) as sensors.","The results demonstrate that the system can robustly get accurate relative poses in real time with limited onboard computation resources.","The source code is released."],"url":"http://arxiv.org/abs/2403.13455v1","category":"cs.RO"}
{"created":"2024-03-20 09:42:55","title":"Improved modelling for dark photon detection with dish antennas","abstract":"A vector dark matter candidate, also known as dark photon, would induce an oscillating electric field through kinetic mixing. One detection strategy uses a spherical reflector to focus the induced emission at its center of curvature. On one hand, we investigate the effects of diffraction in this type of experiment from an analytical standpoint, making use of the Kirchhoff integral theorem in the low-curvature dish limit. On the other hand, we estimate the impact of mode-matching, in the case of detection by a pyramidal horn antenna. We show that the expected signal intensity can be significantly reduced compared to usual estimates. Our method is applied to the re-interpretation of the SHUKET experiment data, the results of which are shown to be degraded by a factor of $\\sim$~50 due to both diffraction and mode-matching. The analytical method allows optimizing some experimental parameters to gain sensitivity in future runs. Our results can be applied to any dish antenna experiment using a low curvature reflector.","sentences":["A vector dark matter candidate, also known as dark photon, would induce an oscillating electric field through kinetic mixing.","One detection strategy uses a spherical reflector to focus the induced emission at its center of curvature.","On one hand, we investigate the effects of diffraction in this type of experiment from an analytical standpoint, making use of the Kirchhoff integral theorem in the low-curvature dish limit.","On the other hand, we estimate the impact of mode-matching, in the case of detection by a pyramidal horn antenna.","We show that the expected signal intensity can be significantly reduced compared to usual estimates.","Our method is applied to the re-interpretation of the SHUKET experiment data, the results of which are shown to be degraded by a factor of $\\sim$~50 due to both diffraction and mode-matching.","The analytical method allows optimizing some experimental parameters to gain sensitivity in future runs.","Our results can be applied to any dish antenna experiment using a low curvature reflector."],"url":"http://arxiv.org/abs/2403.13448v1","category":"hep-ex"}
{"created":"2024-03-20 08:56:18","title":"Phenotypic plasticity trade-offs in an age-structured model of bacterial growth under stress","abstract":"Under low concentrations of antibiotics causing DNA damage, \\textit{Escherichia coli} bacteria can trigger stochastically a stress response known as the SOS response. While the expression of this stress response can make individual cells transiently able to overcome antibiotic treatment, it can also delay cell division, thus impacting the whole population's ability to grow and survive. In order to study the trade-offs that emerge from this phenomenon, we propose a bi-type age-structured population model that captures the phenotypic plasticity observed in the stress response. Individuals can belong to two types: either a fast-dividing but prone to death ``vulnerable'' type, or a slow-dividing but ``tolerant'' type. We study the survival probability of the population issued from a single cell as well as the population growth rate in constant and periodic environments. We show that the sensitivity of these two different notions of fitness with respect to the parameters describing the phenotypic plasticity differs between the stochastic approach (survival probability) and the deterministic approach (population growth rate). Moreover, under a more realistic configuration of periodic stress, our results indicate that optimal population growth can only be achieved through fine-tuning simultaneously both the induction of the stress response and the repair efficiency of the damage caused by the antibiotic.","sentences":["Under low concentrations of antibiotics causing DNA damage, \\textit{Escherichia coli} bacteria can trigger stochastically a stress response known as the SOS response.","While the expression of this stress response can make individual cells transiently able to overcome antibiotic treatment, it can also delay cell division, thus impacting the whole population's ability to grow and survive.","In order to study the trade-offs that emerge from this phenomenon, we propose a bi-type age-structured population model that captures the phenotypic plasticity observed in the stress response.","Individuals can belong to two types: either a fast-dividing but prone to death ``vulnerable'' type, or a slow-dividing but ``tolerant'' type.","We study the survival probability of the population issued from a single cell as well as the population growth rate in constant and periodic environments.","We show that the sensitivity of these two different notions of fitness with respect to the parameters describing the phenotypic plasticity differs between the stochastic approach (survival probability) and the deterministic approach (population growth rate).","Moreover, under a more realistic configuration of periodic stress, our results indicate that optimal population growth can only be achieved through fine-tuning simultaneously both the induction of the stress response and the repair efficiency of the damage caused by the antibiotic."],"url":"http://arxiv.org/abs/2403.13415v1","category":"math.AP"}
{"created":"2024-03-20 08:53:35","title":"Optimal Fixed Priority Scheduling in Multi-Stage Multi-Resource Distributed Real-Time Systems","abstract":"This work studies fixed priority (FP) scheduling of real-time jobs with end-to-end deadlines in a distributed system. Specifically, given a multi-stage pipeline with multiple heterogeneous resources of the same type at each stage, the problem is to assign priorities to a set of real-time jobs with different release times to access a resource at each stage of the pipeline subject to the end-to-end deadline constraints. Note, in such a system, jobs may compete with different sets of jobs at different stages of the pipeline depending on the job-to-resource mapping. To this end, following are the two major contributions of this work. We show that an OPA-compatible schedulability test based on the delay composition algebra can be constructed, which we then use with an optimal priority assignment algorithm to compute a priority ordering. Further, we establish the versatility of pairwise priority assignment in such a multi-stage multi-resource system, compared to a total priority ordering. In particular, we show that a pairwise priority assignment may be feasible even if a priority ordering does not exist. We propose an integer linear programming formulation and a scalable heuristic to compute a pairwise priority assignment. We also show through simulation experiments that the proposed approaches can be used for the holistic scheduling of real-time jobs in edge computing systems.","sentences":["This work studies fixed priority (FP) scheduling of real-time jobs with end-to-end deadlines in a distributed system.","Specifically, given a multi-stage pipeline with multiple heterogeneous resources of the same type at each stage, the problem is to assign priorities to a set of real-time jobs with different release times to access a resource at each stage of the pipeline subject to the end-to-end deadline constraints.","Note, in such a system, jobs may compete with different sets of jobs at different stages of the pipeline depending on the job-to-resource mapping.","To this end, following are the two major contributions of this work.","We show that an OPA-compatible schedulability test based on the delay composition algebra can be constructed, which we then use with an optimal priority assignment algorithm to compute a priority ordering.","Further, we establish the versatility of pairwise priority assignment in such a multi-stage multi-resource system, compared to a total priority ordering.","In particular, we show that a pairwise priority assignment may be feasible even if a priority ordering does not exist.","We propose an integer linear programming formulation and a scalable heuristic to compute a pairwise priority assignment.","We also show through simulation experiments that the proposed approaches can be used for the holistic scheduling of real-time jobs in edge computing systems."],"url":"http://arxiv.org/abs/2403.13411v1","category":"cs.DC"}
{"created":"2024-03-20 08:37:19","title":"A unified framework for bounding causal effects on the always-survivor and other populations","abstract":"We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured. Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions. Previous work on partial identification under the principal stratification framework has primarily focused on the `always-survivor' subpopulation. In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes. These bounds are derived on the `always-survivor', `protected', and `harmed' subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance. The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions. The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects. Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration.","sentences":["We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured.","Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions.","Previous work on partial identification under the principal stratification framework has primarily focused on the `always-survivor' subpopulation.","In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes.","These bounds are derived on the `always-survivor', `protected', and `harmed' subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance.","The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions.","The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects.","Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration."],"url":"http://arxiv.org/abs/2403.13398v1","category":"stat.ME"}
{"created":"2024-03-20 08:37:10","title":"On the definition of zero resonances for the Schr{\u00f6}dinger operator with optimal scaling potentials","abstract":"We consider the Schr{\\\"o}dinger operator --$\\Delta$ + V on the Euclidean space with potential in the Lorentz space L^{n/2,1} and we find necessary and sufficient conditions for zero to be a resonance or an eigenvalue. We consider functions with gradient in L^2 and that verify the equation (--$\\Delta$ + V)$\\psi$ = 0, namely the kernel of (--$\\Delta$ + V) in the homogeneous Sobolev space of order one. We prove that a function in this set is either in a weak Lebesgue space or in L^2 , in the latter case we have a zero eigenfunction. The set of eigenfunctions is the hyperplane of functions that are orthogonal to V, furthermore we show that under some classic orthogonality conditions a zero eigenfunction belongs to the weak Lebesgue space of order one or to L^1. We study dimensions n $\\ge$ 3 and in dimension three we generalize a result proved by Beceanu.","sentences":["We consider the Schr{\\\"o}dinger operator --$\\Delta$ + V on the Euclidean space with potential in the Lorentz space L^{n/2,1} and we find necessary and sufficient conditions for zero to be a resonance or an eigenvalue.","We consider functions with gradient in L^2 and that verify the equation (--$\\Delta$ + V)$\\psi$ = 0, namely the kernel of (--$\\Delta$ + V) in the homogeneous Sobolev space of order one.","We prove that a function in this set is either in a weak Lebesgue space or in L^2 , in the latter case we have a zero eigenfunction.","The set of eigenfunctions is the hyperplane of functions that are orthogonal to V, furthermore we show that under some classic orthogonality conditions a zero eigenfunction belongs to the weak Lebesgue space of order one or to L^1.","We study dimensions n $\\ge$ 3 and in dimension three we generalize a result proved by Beceanu."],"url":"http://arxiv.org/abs/2403.13397v1","category":"math.SP"}
{"created":"2024-03-20 08:29:57","title":"Optimal VPPI strategy under Omega ratio with stochastic benchmark","abstract":"This paper studies a variable proportion portfolio insurance (VPPI) strategy. The objective is to determine the risk multiplier by maximizing the extended Omega ratio of the investor's cushion, using a binary stochastic benchmark. When the stock index declines, investors aim to maintain the minimum guarantee. Conversely, when the stock index rises, investors seek to track some excess returns. The optimization problem involves the combination of a non-concave objective function with a stochastic benchmark, which is effectively solved based on the stochastic version of concavification technique. We derive semi-analytical solutions for the optimal risk multiplier, and the value functions are categorized into three distinct cases. Intriguingly, the classification criteria are determined by the relationship between the optimal risky multiplier in Zieling et al. (2014 and the value of 1. Simulation results confirm the effectiveness of the VPPI strategy when applied to real market data calibrations.","sentences":["This paper studies a variable proportion portfolio insurance (VPPI) strategy.","The objective is to determine the risk multiplier by maximizing the extended Omega ratio of the investor's cushion, using a binary stochastic benchmark.","When the stock index declines, investors aim to maintain the minimum guarantee.","Conversely, when the stock index rises, investors seek to track some excess returns.","The optimization problem involves the combination of a non-concave objective function with a stochastic benchmark, which is effectively solved based on the stochastic version of concavification technique.","We derive semi-analytical solutions for the optimal risk multiplier, and the value functions are categorized into three distinct cases.","Intriguingly, the classification criteria are determined by the relationship between the optimal risky multiplier in Zieling et al.","(2014 and the value of 1.","Simulation results confirm the effectiveness of the VPPI strategy when applied to real market data calibrations."],"url":"http://arxiv.org/abs/2403.13388v1","category":"econ.GN"}
{"created":"2024-03-20 08:26:39","title":"A multilevel framework for accelerating uSARA in radio-interferometric imaging","abstract":"This paper presents a multilevel algorithm specifically designed for radio-interferometric imaging in astronomy. The proposed algorithm is used to solve the uSARA (unconstrained Sparsity Averaging Reweighting Analysis) formulation of this image restoration problem. Multilevel algorithms rely on a hierarchy of approximations of the objective function to accelerate its optimization. In contrast to the usual multilevel approaches where this hierarchy is derived in the parameter space, here we construct the hierarchy of approximations in the observation space. The proposed approach is compared to a reweighted forward-backward procedure, which is the backbone iteration scheme for solving the uSARA problem.","sentences":["This paper presents a multilevel algorithm specifically designed for radio-interferometric imaging in astronomy.","The proposed algorithm is used to solve the uSARA (unconstrained Sparsity Averaging Reweighting Analysis) formulation of this image restoration problem.","Multilevel algorithms rely on a hierarchy of approximations of the objective function to accelerate its optimization.","In contrast to the usual multilevel approaches where this hierarchy is derived in the parameter space, here we construct the hierarchy of approximations in the observation space.","The proposed approach is compared to a reweighted forward-backward procedure, which is the backbone iteration scheme for solving the uSARA problem."],"url":"http://arxiv.org/abs/2403.13385v1","category":"math.OC"}
{"created":"2024-03-20 08:25:57","title":"Optimizing Ride-Pooling Revenue: Pricing Strategies and Driver-Traveller Dynamics","abstract":"Ride-pooling, to gain momentum, needs to be attractive for all the parties involved. This includes also drivers, who are naturally reluctant to serve pooled rides. This can be controlled by the platform's pricing strategy, which can stimulate drivers to serve pooled rides. Here, we propose an agent-based framework, where drivers serve rides that maximise their utility. We simulate a series of scenarios in Delft and compare three strategies. Our results show that drivers, when they maximize their profits, earn more than in both the solo-rides and only-pooled rides scenarios. This shows that serving pooled rides can be beneficial as well for drivers, yet typically not all pooled rides are attractive for drivers. The proposed framework may be further applied to propose discriminative pricing in which the full potential of ride-pooling is exploited, with benefits for the platform, travellers, and (which is novel here) to the drivers.","sentences":["Ride-pooling, to gain momentum, needs to be attractive for all the parties involved.","This includes also drivers, who are naturally reluctant to serve pooled rides.","This can be controlled by the platform's pricing strategy, which can stimulate drivers to serve pooled rides.","Here, we propose an agent-based framework, where drivers serve rides that maximise their utility.","We simulate a series of scenarios in Delft and compare three strategies.","Our results show that drivers, when they maximize their profits, earn more than in both the solo-rides and only-pooled rides scenarios.","This shows that serving pooled rides can be beneficial as well for drivers, yet typically not all pooled rides are attractive for drivers.","The proposed framework may be further applied to propose discriminative pricing in which the full potential of ride-pooling is exploited, with benefits for the platform, travellers, and (which is novel here) to the drivers."],"url":"http://arxiv.org/abs/2403.13384v1","category":"cs.MA"}
{"created":"2024-03-20 08:21:35","title":"Application of advanced ultrasonic testing methods to Dissimilar Metal Welds -- Comparison of simulated and experimental results","abstract":"Widely present in the primary circuit of Nuclear Power Plants (NPP), Dissimilar Metal Welds (DMW) are inspected using Ultrasonic nondestructive Testing (UT) techniques to ensure the integrity of the structure and detect defects such as Stress Corrosion Cracking (SCC).In a previous collaborative research, CRIEPI and CEA have worked on the understanding of the propagation of ultrasonic waves in complex materials. Indeed, the ultrasonic propagation can be disturbed due to the anisotropic and inhomogeneous properties of the medium and the interpretation of inspection results can then be difficult. An analytical model, based on a dynamic ray theory, developed by CEA-LIST and implemented in the CIVA software had been used to predict the ultrasonic propagation in a DMW. The model evaluates the ray trajectories, the travel-time and the computation of the amplitude along the ray tube in a medium described thanks to a continuously varying description of its physical properties. In this study, the weld had been described by an analytical law of the crystallographic orientation. The simulated results of the detection of calibrated notches located in the buttering and the weld had been compared with experimental data and had shown a good agreement.The new collaborative program presented in this paper aims at detecting a real SCC defect located close to the root of the DMW. Thus, simulations have been performed for a DMW described with an analytical law and a smooth cartography of the crystallographic orientation. Furthermore, advanced ultrasonic testing methods have been used to inspect the specimen and detect the real SCC defect. Experimental and simulated results of the mock-up inspection have been compared.","sentences":["Widely present in the primary circuit of Nuclear Power Plants (NPP), Dissimilar Metal Welds (DMW) are inspected using Ultrasonic nondestructive Testing (UT) techniques to ensure the integrity of the structure and detect defects such as Stress Corrosion Cracking (SCC).In a previous collaborative research, CRIEPI and CEA have worked on the understanding of the propagation of ultrasonic waves in complex materials.","Indeed, the ultrasonic propagation can be disturbed due to the anisotropic and inhomogeneous properties of the medium and the interpretation of inspection results can then be difficult.","An analytical model, based on a dynamic ray theory, developed by CEA-LIST and implemented in the CIVA software had been used to predict the ultrasonic propagation in a DMW.","The model evaluates the ray trajectories, the travel-time and the computation of the amplitude along the ray tube in a medium described thanks to a continuously varying description of its physical properties.","In this study, the weld had been described by an analytical law of the crystallographic orientation.","The simulated results of the detection of calibrated notches located in the buttering and the weld had been compared with experimental data and had shown a good agreement.","The new collaborative program presented in this paper aims at detecting a real SCC defect located close to the root of the DMW.","Thus, simulations have been performed for a DMW described with an analytical law and a smooth cartography of the crystallographic orientation.","Furthermore, advanced ultrasonic testing methods have been used to inspect the specimen and detect the real SCC defect.","Experimental and simulated results of the mock-up inspection have been compared."],"url":"http://arxiv.org/abs/2403.13379v1","category":"cs.CE"}
{"created":"2024-03-20 07:36:43","title":"GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot","abstract":"Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes. Additionally, we uncover its potential to acquire emergent skills. Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning. This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community.","sentences":["Multi-task robot learning holds significant importance in tackling diverse and complex scenarios.","However, current approaches are hindered by performance issues and difficulties in collecting training datasets.","In this paper, we propose GeRM (Generalist Robotic Model).","We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations.","Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions.","By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs.","Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also validating its efficiency in both training and inference processes.","Additionally, we uncover its potential to acquire emergent skills.","Additionally, we contribute the QUARD-Auto dataset, collected automatically to support our training approach and foster advancements in multi-task quadruped robot learning.","This work presents a new paradigm for reducing the cost of collecting robot data and driving progress in the multi-task learning community."],"url":"http://arxiv.org/abs/2403.13358v1","category":"cs.RO"}
