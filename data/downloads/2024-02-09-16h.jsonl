{"created":"2024-02-07 18:59:31","title":"Edu-ConvoKit: An Open-Source Library for Education Conversation Data","abstract":"We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository.","sentences":["We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education.","Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access.","We address these challenges with Edu-ConvoKit.","Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ).","Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- .","We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository."],"url":"http://arxiv.org/abs/2402.05111v1","category":"cs.CL"}
{"created":"2024-02-07 18:58:50","title":"Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding","abstract":"To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively. Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding.","sentences":["To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework.","To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model.","One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states.","To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation.","In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy.","Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads.","We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively.","Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding."],"url":"http://arxiv.org/abs/2402.05109v1","category":"cs.LG"}
{"created":"2024-02-07 18:57:37","title":"Image captioning for Brazilian Portuguese using GRIT model","abstract":"This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.","sentences":["This work presents the early development of a model of image captioning for the Brazilian Portuguese language.","We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work.","GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions.","The GRIT method emerged as a proposal to be a more efficient way to generate image captioning.","In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language."],"url":"http://arxiv.org/abs/2402.05106v1","category":"cs.CV"}
{"created":"2024-02-07 18:47:32","title":"Motile bacteria crossing liquid-liquid interfaces","abstract":"Real-life bacteria often swim in complex fluids, but our understanding of the interactions between bacteria and complex surroundings is still evolving. In this work, rod-like $\\textit{Bacillus subtilis}$ swims in a quasi-2D environment with aqueous liquid-liquid interfaces, i.e., the isotropic-nematic coexistence phase of an aqueous chromonic liquid crystal. Focusing on the bacteria motion near and at the liquid-liquid interfaces, we collect and quantify bacterial trajectories from the isotropic to the nematic phase. Despite its small magnitude, the interfacial tension of the order of 10 $\\mathrm{\\mu N/m}$ at the isotropic-nematic interface justifies our observations that bacteria swimming more perpendicular to the interface have a higher probability of crossing the interface. Our force-balance model, considering the interfacial tension, further predicts how the length and speed of the bacteria affect their crossing behaviors.","sentences":["Real-life bacteria often swim in complex fluids, but our understanding of the interactions between bacteria and complex surroundings is still evolving.","In this work, rod-like $\\textit{Bacillus subtilis}$ swims in a quasi-2D environment with aqueous liquid-liquid interfaces, i.e., the isotropic-nematic coexistence phase of an aqueous chromonic liquid crystal.","Focusing on the bacteria motion near and at the liquid-liquid interfaces, we collect and quantify bacterial trajectories from the isotropic to the nematic phase.","Despite its small magnitude, the interfacial tension of the order of 10 $\\mathrm{\\mu N/m}$ at the isotropic-nematic interface justifies our observations that bacteria swimming more perpendicular to the interface have a higher probability of crossing the interface.","Our force-balance model, considering the interfacial tension, further predicts how the length and speed of the bacteria affect their crossing behaviors."],"url":"http://arxiv.org/abs/2402.05095v1","category":"cond-mat.soft"}
{"created":"2024-02-07 18:31:36","title":"Fluctuation-Induced First Order Transition to Collective Motion","abstract":"The nature of the transition to collective motion in assemblies of aligning self-propelled particles remains a long-standing matter of debate. In this article, we focus on dry active matter and show that weak fluctuations suffice to generically turn second-order mean-field transitions into a `discontinuous' coexistence scenario. Our theory shows how fluctuations induce a density-dependence of the polar-field mass, even when this effect is absent at mean-field level. In turn, this dependency on density triggers a feedback loop between ordering and advection that ultimately leads to an inhomogeneous transition to collective motion and the emergence of non-linear travelling `flocks'. Importantly, we show that such a fluctuation-induced first order transition is present in both metric models, in which particles align with neighbors within a finite distance, and in topological ones, in which alignment is not based on relative distances. We compute analytically the noise-induced renormalization of the polar-field mass using stochastic calculus, which we further back up by a one-loop field-theoretical analysis. Finally, we confirm our analytical predictions by numerical simulations of fluctuating hydrodynamics as well as of topological microscopic models with either $k$-nearest neighbors or Voronoi alignment.","sentences":["The nature of the transition to collective motion in assemblies of aligning self-propelled particles remains a long-standing matter of debate.","In this article, we focus on dry active matter and show that weak fluctuations suffice to generically turn second-order mean-field transitions into a `discontinuous' coexistence scenario.","Our theory shows how fluctuations induce a density-dependence of the polar-field mass, even when this effect is absent at mean-field level.","In turn, this dependency on density triggers a feedback loop between ordering and advection that ultimately leads to an inhomogeneous transition to collective motion and the emergence of non-linear travelling `flocks'.","Importantly, we show that such a fluctuation-induced first order transition is present in both metric models, in which particles align with neighbors within a finite distance, and in topological ones, in which alignment is not based on relative distances.","We compute analytically the noise-induced renormalization of the polar-field mass using stochastic calculus, which we further back up by a one-loop field-theoretical analysis.","Finally, we confirm our analytical predictions by numerical simulations of fluctuating hydrodynamics as well as of topological microscopic models with either $k$-nearest neighbors or Voronoi alignment."],"url":"http://arxiv.org/abs/2402.05078v1","category":"cond-mat.soft"}
{"created":"2024-02-07 18:30:04","title":"Markovian Analysis of Information Cascades with Fake Agents","abstract":"People often learn from other's actions when they make decisions while doing online shopping. This kind of observational learning may lead to information cascades, which means agents might ignore their own signals and follow the 'trend' created collectively by the actions of their predecessors. It is well-known that with rational agents, such a cascade model can result in either correct or incorrect cascades. In this paper, we additionally consider the presence of fake agents who always take fixed actions and we investigate their influence on the outcome of these cascades. We propose an infinite Markov Chain sequence structure and a tree structure to analyze how the fraction and the type of such fake agents impacts behavior of the upcoming agents. We show that an increase in the fraction of fake agents may reduce the chances of their preferred outcome, and also there is a certain lower bound for the probability of a wrong cascade. In particular, we discuss the probability of an agent being fake tends to 1 and the effect of a constant portion of fake agents.","sentences":["People often learn from other's actions when they make decisions while doing online shopping.","This kind of observational learning may lead to information cascades, which means agents might ignore their own signals and follow the 'trend' created collectively by the actions of their predecessors.","It is well-known that with rational agents, such a cascade model can result in either correct or incorrect cascades.","In this paper, we additionally consider the presence of fake agents who always take fixed actions and we investigate their influence on the outcome of these cascades.","We propose an infinite Markov Chain sequence structure and a tree structure to analyze how the fraction and the type of such fake agents impacts behavior of the upcoming agents.","We show that an increase in the fraction of fake agents may reduce the chances of their preferred outcome, and also there is a certain lower bound for the probability of a wrong cascade.","In particular, we discuss the probability of an agent being fake tends to 1 and the effect of a constant portion of fake agents."],"url":"http://arxiv.org/abs/2402.05076v1","category":"cs.SI"}
{"created":"2024-02-07 18:21:17","title":"A Roadmap to Pluralistic Alignment","abstract":"With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.","sentences":["With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives.","However, aligning models to serve pluralistic human values remains an open research question.","In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed.","We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution.","We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings.","We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment."],"url":"http://arxiv.org/abs/2402.05070v1","category":"cs.AI"}
{"created":"2024-02-07 18:17:54","title":"Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning","abstract":"Operation of Autonomous Mobile Robots (AMRs) of all forms that include wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS denied environments without a-priori maps, exclusively using onboard sensors, is an unsolved problem that has potential to transform the economy, and vastly improve humanity's capabilities with improvements to agriculture, manufacturing, disaster response, military and space exploration. Conventional AMR automation approaches are modularized into perception, motion planning and control which is computationally inefficient, and requires explicit feature extraction and engineering, that inhibits generalization, and deployment at scale. Few works have focused on real-world end-to-end approaches that directly map sensor inputs to control outputs due to the large amount of well curated training data required for supervised Deep Learning (DL) which is time consuming and labor intensive to collect and label, and sample inefficiency and challenges to bridging the simulation to reality gap using Deep Reinforcement Learning (DRL). This paper presents a novel method to efficiently train DRL for robust end-to-end AMR exploration, in a constrained environment at physical limits in simulation, transferred zero-shot to the real-world. The representation learned in a compact parameter space with 2 fully connected layers with 64 nodes each is demonstrated to exhibit emergent behavior for out-of-distribution generalization to navigation in new environments that include unstructured terrain without maps, and dynamic obstacle avoidance. The learned policy outperforms conventional navigation algorithms while consuming a fraction of the computation resources, enabling execution on a range of AMR forms with varying embedded computer payloads.","sentences":["Operation of Autonomous Mobile Robots (AMRs) of all forms that include wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS denied environments without a-priori maps, exclusively using onboard sensors, is an unsolved problem that has potential to transform the economy, and vastly improve humanity's capabilities with improvements to agriculture, manufacturing, disaster response, military and space exploration.","Conventional AMR automation approaches are modularized into perception, motion planning and control which is computationally inefficient, and requires explicit feature extraction and engineering, that inhibits generalization, and deployment at scale.","Few works have focused on real-world end-to-end approaches that directly map sensor inputs to control outputs due to the large amount of well curated training data required for supervised Deep Learning (DL) which is time consuming and labor intensive to collect and label, and sample inefficiency and challenges to bridging the simulation to reality gap using Deep Reinforcement Learning (DRL).","This paper presents a novel method to efficiently train DRL for robust end-to-end AMR exploration, in a constrained environment at physical limits in simulation, transferred zero-shot to the real-world.","The representation learned in a compact parameter space with 2 fully connected layers with 64 nodes each is demonstrated to exhibit emergent behavior for out-of-distribution generalization to navigation in new environments that include unstructured terrain without maps, and dynamic obstacle avoidance.","The learned policy outperforms conventional navigation algorithms while consuming a fraction of the computation resources, enabling execution on a range of AMR forms with varying embedded computer payloads."],"url":"http://arxiv.org/abs/2402.05066v1","category":"cs.RO"}
{"created":"2024-02-07 17:41:15","title":"How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation","abstract":"Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal. Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI. In some cases, even works from mathematics, statistics, and engineering would be affected. Worryingly, AI misdefinitions are observed from Western societies to the Global South. In this paper, we propose a framework to score how \\textit{validated as appropriately-defined for regulation} (VADER) an AI definition is. Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works. Regarding the latter, our score is based on a dataset of representative AI, non-AI ICT, and non-ICT examples. We demonstrate our contribution by reviewing the AI regulation proposals of key players, namely the United States, United Kingdom, European Union, and Brazil. Importantly, none of the proposals assessed achieve the appropriateness score, ranging from a revision need to a concrete risk to ICT systems and works from other fields.","sentences":["Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs.","Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal.","Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI.","In some cases, even works from mathematics, statistics, and engineering would be affected.","Worryingly, AI misdefinitions are observed from Western societies to the Global South.","In this paper, we propose a framework to score how \\textit{validated as appropriately-defined for regulation} (VADER) an AI definition is.","Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works.","Regarding the latter, our score is based on a dataset of representative AI, non-AI ICT, and non-ICT examples.","We demonstrate our contribution by reviewing the AI regulation proposals of key players, namely the United States, United Kingdom, European Union, and Brazil.","Importantly, none of the proposals assessed achieve the appropriateness score, ranging from a revision need to a concrete risk to ICT systems and works from other fields."],"url":"http://arxiv.org/abs/2402.05048v1","category":"cs.AI"}
{"created":"2024-02-07 17:33:54","title":"SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models","abstract":"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under https://github.com/OpenSafetyLab/SALAD-BENCH.","sentences":["In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount.","To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods.","Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.","SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice.","To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation.","Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility.","Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics.","Data and evaluator are released under https://github.com/OpenSafetyLab/SALAD-BENCH."],"url":"http://arxiv.org/abs/2402.05044v2","category":"cs.CL"}
{"created":"2024-02-07 17:08:27","title":"A Survey on Domain Generalization for Medical Image Analysis","abstract":"Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years. However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue. In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions. This paper presents the a comprehensive review of substantial developments in this area. First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings. Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints. Furthermore, we introduce the commonly used datasets. Finally, we summarize existing literature and present some potential research topics for the future. For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA","sentences":["Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years.","However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue.","In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions.","This paper presents the a comprehensive review of substantial developments in this area.","First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings.","Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints.","Furthermore, we introduce the commonly used datasets.","Finally, we summarize existing literature and present some potential research topics for the future.","For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA"],"url":"http://arxiv.org/abs/2402.05035v1","category":"cs.CV"}
{"created":"2024-02-07 16:53:09","title":"Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing","abstract":"Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph.","sentences":["Graph-based environments pose unique challenges to multi-agent reinforcement learning.","In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations.","The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead.","This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph.","We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors.","Agents receive the resulting learned graph observations based on their location in the graph.","Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice.","We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph."],"url":"http://arxiv.org/abs/2402.05027v1","category":"cs.MA"}
{"created":"2024-02-07 16:43:27","title":"Once-in-a-lifetime encounter models for neutrino media: From coherent oscillation to flavor equilibration","abstract":"Collective neutrino oscillations are typically studied using the lowest-order quantum kinetic equation, also known as the mean-field approximation. However, some recent quantum many-body simulations suggest that quantum entanglement among neutrinos may be important and may result in flavor equilibration of the neutrino gas. In this work, we develop new quantum many-body models for neutrino gases in which any pair of neutrinos can interact at most once in their lifetimes. A key parameter of our models is $\\gamma=\\mu \\Delta z$, where $\\mu$ is the neutrino coupling strength, which is proportional to the neutrino density, and $\\Delta z$ is the duration over which a pair of neutrinos can interact each time. Our models reduce to the mean-field approach in the limit $\\gamma\\to0$ and achieve flavor equilibration in time $t \\gg (\\gamma\\mu)^{-1}$. These models demonstrate the emergence of coherent flavor oscillations from the particle perspective and may help elucidate the role of quantum entanglement in collective neutrino oscillations.","sentences":["Collective neutrino oscillations are typically studied using the lowest-order quantum kinetic equation, also known as the mean-field approximation.","However, some recent quantum many-body simulations suggest that quantum entanglement among neutrinos may be important and may result in flavor equilibration of the neutrino gas.","In this work, we develop new quantum many-body models for neutrino gases in which any pair of neutrinos can interact at most once in their lifetimes.","A key parameter of our models is $\\gamma=\\mu \\Delta z$, where $\\mu$ is the neutrino coupling strength, which is proportional to the neutrino density, and $\\Delta z$ is the duration over which a pair of neutrinos can interact each time.","Our models reduce to the mean-field approach in the limit $\\gamma\\to0$ and achieve flavor equilibration in time $t \\gg (\\gamma\\mu)^{-1}$. These models demonstrate the emergence of coherent flavor oscillations from the particle perspective and may help elucidate the role of quantum entanglement in collective neutrino oscillations."],"url":"http://arxiv.org/abs/2402.05022v1","category":"hep-ph"}
{"created":"2024-02-07 16:32:55","title":"When the Body Became Data: Historical Data Cultures and Anatomical Illustration","abstract":"With changing attitudes around knowledge, medicine, art, and technology, the human body has become a source of information and, ultimately, shareable and analyzable data. Centuries of illustrations and visualizations of the body occur within particular historical, social, and political contexts. These contexts are enmeshed in different so-called data cultures: ways that data, knowledge, and information are conceptualized and collected, structured and shared. In this work, we explore how information about the body was collected as well as the circulation, impact, and persuasive force of the resulting images. We show how mindfulness of data cultural influences remain crucial for today's designers, researchers, and consumers of visualizations. We conclude with a call for the field to reflect on how visualizations are not timeless and contextless mirrors on objective data, but as much a product of our time and place as the visualizations of the past.","sentences":["With changing attitudes around knowledge, medicine, art, and technology, the human body has become a source of information and, ultimately, shareable and analyzable data.","Centuries of illustrations and visualizations of the body occur within particular historical, social, and political contexts.","These contexts are enmeshed in different so-called data cultures: ways that data, knowledge, and information are conceptualized and collected, structured and shared.","In this work, we explore how information about the body was collected as well as the circulation, impact, and persuasive force of the resulting images.","We show how mindfulness of data cultural influences remain crucial for today's designers, researchers, and consumers of visualizations.","We conclude with a call for the field to reflect on how visualizations are not timeless and contextless mirrors on objective data, but as much a product of our time and place as the visualizations of the past."],"url":"http://arxiv.org/abs/2402.05014v1","category":"cs.HC"}
{"created":"2024-02-07 16:29:39","title":"A Review on Trajectory Datasets on Advanced Driver Assistance System","abstract":"This paper presents a comprehensive review of trajectory data of Advanced Driver Assistance System equipped-vehicle, with the aim of precisely model of Autonomous Vehicles (AVs) behavior. This study emphasizes the importance of trajectory data in the development of AV models, especially in car-following scenarios. We introduce and evaluate several datasets: the OpenACC Dataset, the Connected & Autonomous Transportation Systems Laboratory Open Dataset, the Vanderbilt ACC Dataset, the Central Ohio Dataset, and the Waymo Open Dataset. Each dataset offers unique insights into AV behaviors, yet they share common challenges in terms of data availability, processing, and standardization. After a series of data cleaning, outlier removal and statistical analysis, this paper transforms datasets of varied formats into a uniform standard, thereby improving their applicability for modeling AV car-following behavior. Key contributions of this study include: 1. the transformation of all datasets into a unified standard format, enhancing their utility for broad research applications; 2. a comparative analysis of these datasets, highlighting their distinct characteristics and implications for car-following model development; 3. the provision of guidelines for future data collection projects, along with the open-source release of all processed data and code for use by the research community.","sentences":["This paper presents a comprehensive review of trajectory data of Advanced Driver Assistance System equipped-vehicle, with the aim of precisely model of Autonomous Vehicles (AVs) behavior.","This study emphasizes the importance of trajectory data in the development of AV models, especially in car-following scenarios.","We introduce and evaluate several datasets: the OpenACC Dataset, the Connected & Autonomous Transportation Systems Laboratory Open Dataset, the Vanderbilt ACC Dataset, the Central Ohio Dataset, and the Waymo Open Dataset.","Each dataset offers unique insights into AV behaviors, yet they share common challenges in terms of data availability, processing, and standardization.","After a series of data cleaning, outlier removal and statistical analysis, this paper transforms datasets of varied formats into a uniform standard, thereby improving their applicability for modeling AV car-following behavior.","Key contributions of this study include: 1.","the transformation of all datasets into a unified standard format, enhancing their utility for broad research applications; 2. a comparative analysis of these datasets, highlighting their distinct characteristics and implications for car-following model development; 3.","the provision of guidelines for future data collection projects, along with the open-source release of all processed data and code for use by the research community."],"url":"http://arxiv.org/abs/2402.05009v1","category":"stat.AP"}
{"created":"2024-02-07 16:28:36","title":"EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss","abstract":"We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.","sentences":["We present EfficientViT-SAM, a new family of accelerated segment anything models.","We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset.","Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance.","Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit."],"url":"http://arxiv.org/abs/2402.05008v1","category":"cs.CV"}
{"created":"2024-02-07 16:28:04","title":"Example-based Explanations for Random Forests using Machine Unlearning","abstract":"Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes machine unlearning to estimate the change in the tree structures of the random forest when parts of the underlying training data are removed, and then leverages the Apriori algorithm from frequent itemset mining to reduce the subset search space. We empirically evaluate our approach on three real-world datasets, and demonstrate that the explanations generated by FairDebugger are consistent with insights from prior studies on these datasets.","sentences":["Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation.","Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes.","Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior.","However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   ","We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier.","FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness.","Toward this goal, FairDebugger first utilizes machine unlearning to estimate the change in the tree structures of the random forest when parts of the underlying training data are removed, and then leverages the Apriori algorithm from frequent itemset mining to reduce the subset search space.","We empirically evaluate our approach on three real-world datasets, and demonstrate that the explanations generated by FairDebugger are consistent with insights from prior studies on these datasets."],"url":"http://arxiv.org/abs/2402.05007v1","category":"cs.LG"}
{"created":"2024-02-07 16:14:37","title":"Local interactions in active matter are reinforced by spatial structure","abstract":"The flocking of self-propelled particles in heterogeneous environments is relevant to both natural and artificial systems. The Vicsek model is a canonical choice to investigate such systems due to the minimal number of parameters required to define flocking. Prior research on the Vicsek model has investigated the effects of interaction rules, particle speed, and obstacle packing on the flocking behavior, but the effect of interaction radius remains an open question. Unlike obstacle-free domains, the locality of interactions not only affects how quickly the system can become polarized, but also how well the flocks can align or realign after colliding with obstacles. In this letter, we delve into this subtle relationship that exists in the scale of the perception of Vicsek particles in the presence of obstacles. We demonstrate that the presence of obstacles impacts group density, which provides the basis to identify distinct phases for collective behavior. This leads to the counter-intuitive result that obstacles, while generally confounding for macroscopic order, may enable global order even as noise in the system increases.","sentences":["The flocking of self-propelled particles in heterogeneous environments is relevant to both natural and artificial systems.","The Vicsek model is a canonical choice to investigate such systems due to the minimal number of parameters required to define flocking.","Prior research on the Vicsek model has investigated the effects of interaction rules, particle speed, and obstacle packing on the flocking behavior, but the effect of interaction radius remains an open question.","Unlike obstacle-free domains, the locality of interactions not only affects how quickly the system can become polarized, but also how well the flocks can align or realign after colliding with obstacles.","In this letter, we delve into this subtle relationship that exists in the scale of the perception of Vicsek particles in the presence of obstacles.","We demonstrate that the presence of obstacles impacts group density, which provides the basis to identify distinct phases for collective behavior.","This leads to the counter-intuitive result that obstacles, while generally confounding for macroscopic order, may enable global order even as noise in the system increases."],"url":"http://arxiv.org/abs/2402.04996v1","category":"physics.comp-ph"}
{"created":"2024-02-07 15:58:51","title":"Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for Energy Consumption Prediction","abstract":"This paper presents an approach integrating explainable artificial intelligence (XAI) techniques with adaptive learning to enhance energy consumption prediction models, with a focus on handling data distribution shifts. Leveraging SHAP clustering, our method provides interpretable explanations for model predictions and uses these insights to adaptively refine the model, balancing model complexity with predictive performance. We introduce a three-stage process: (1) obtaining SHAP values to explain model predictions, (2) clustering SHAP values to identify distinct patterns and outliers, and (3) refining the model based on the derived SHAP clustering characteristics. Our approach mitigates overfitting and ensures robustness in handling data distribution shifts. We evaluate our method on a comprehensive dataset comprising energy consumption records of buildings, as well as two additional datasets to assess the transferability of our approach to other domains, regression, and classification problems. Our experiments demonstrate the effectiveness of our approach in both task types, resulting in improved predictive performance and interpretable model explanations.","sentences":["This paper presents an approach integrating explainable artificial intelligence (XAI) techniques with adaptive learning to enhance energy consumption prediction models, with a focus on handling data distribution shifts.","Leveraging SHAP clustering, our method provides interpretable explanations for model predictions and uses these insights to adaptively refine the model, balancing model complexity with predictive performance.","We introduce a three-stage process: (1) obtaining SHAP values to explain model predictions, (2) clustering SHAP values to identify distinct patterns and outliers, and (3) refining the model based on the derived SHAP clustering characteristics.","Our approach mitigates overfitting and ensures robustness in handling data distribution shifts.","We evaluate our method on a comprehensive dataset comprising energy consumption records of buildings, as well as two additional datasets to assess the transferability of our approach to other domains, regression, and classification problems.","Our experiments demonstrate the effectiveness of our approach in both task types, resulting in improved predictive performance and interpretable model explanations."],"url":"http://arxiv.org/abs/2402.04982v1","category":"cs.LG"}
{"created":"2024-02-07 15:57:28","title":"Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training","abstract":"Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications. The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene. We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices. Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability.","sentences":["Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications.","The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene.","We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices.","Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects.","Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability."],"url":"http://arxiv.org/abs/2402.04979v1","category":"cs.CV"}
{"created":"2024-02-07 15:56:17","title":"An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration","abstract":"While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.","sentences":["While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.","To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs.","This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning.","The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process.","Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results.","Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work.","Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues."],"url":"http://arxiv.org/abs/2402.04978v1","category":"cs.CL"}
{"created":"2024-02-07 15:55:51","title":"ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12","abstract":"As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children's autonomous Scratch learning: artist's block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist's block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children.","sentences":["As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12).","Through formative investigation with Scratch experts, we uncover three key obstacles to children's autonomous Scratch learning: artist's block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation.","To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children.","ChatScratch employs structured interactive storyboards and visual cues to overcome artist's block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance.","Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children."],"url":"http://arxiv.org/abs/2402.04975v1","category":"cs.HC"}
{"created":"2024-02-07 15:50:20","title":"Multi-Sender Persuasion -- A Computational Perspective","abstract":"We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems.","sentences":["We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions.","Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives.","The core solution concept here is the Nash equilibrium of senders' signaling policies.","Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard.","Given these intrinsic difficulties, we turn to finding local Nash equilibria.","We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities.","Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks.","Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems."],"url":"http://arxiv.org/abs/2402.04971v2","category":"cs.AI"}
{"created":"2024-02-07 15:44:55","title":"Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?","abstract":"This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $\\Delta$F1 of 0.18.","sentences":["This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings.","We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset.","The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting.","Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02.","Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%).","Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $\\Delta$F1 of 0.18."],"url":"http://arxiv.org/abs/2402.04967v1","category":"cs.CL"}
{"created":"2024-02-07 15:43:42","title":"Collective Departure Time Allocation in Large-scale Urban Networks: A Flexible Modeling Framework with Trip Length and Desired Arrival Time Distributions","abstract":"Urban traffic congestion remains a persistent issue for cities worldwide. Recent macroscopic models have adopted a mathematically well-defined relation between network flow and density to characterize traffic states over an urban region. Despite advances in these models, capturing the complex dynamics of urban traffic congestion requires considering the heterogeneous characteristics of trips. Classic macroscopic models, e.g., bottleneck and bathtub models and their extensions, have attempted to account for these characteristics, such as trip-length distribution and desired arrival times. However, they often make assumptions that fall short of reflecting real-world conditions. To address this, generalized bathtub models were recently proposed, introducing a new state variable to capture any distribution of remaining trip lengths. This study builds upon this work to formulate and solve the social optimum, a solution minimizing the sum of all users' generalized (i.e., social and monetary) costs for a departure time choice model. The proposed framework can accommodate any distribution for desired arrival time and trip length, making it more adaptable to the diverse array of trip characteristics in an urban setting. In addition, the existence of the solution is proven, and the proposed solution method calculates the social optimum analytically. The numerical results show that the method is computationally efficient. The proposed methodology is validated on the real test case of Lyon North City, benchmarking with deterministic and stochastic user equilibria.","sentences":["Urban traffic congestion remains a persistent issue for cities worldwide.","Recent macroscopic models have adopted a mathematically well-defined relation between network flow and density to characterize traffic states over an urban region.","Despite advances in these models, capturing the complex dynamics of urban traffic congestion requires considering the heterogeneous characteristics of trips.","Classic macroscopic models, e.g., bottleneck and bathtub models and their extensions, have attempted to account for these characteristics, such as trip-length distribution and desired arrival times.","However, they often make assumptions that fall short of reflecting real-world conditions.","To address this, generalized bathtub models were recently proposed, introducing a new state variable to capture any distribution of remaining trip lengths.","This study builds upon this work to formulate and solve the social optimum, a solution minimizing the sum of all users' generalized (i.e., social and monetary) costs for a departure time choice model.","The proposed framework can accommodate any distribution for desired arrival time and trip length, making it more adaptable to the diverse array of trip characteristics in an urban setting.","In addition, the existence of the solution is proven, and the proposed solution method calculates the social optimum analytically.","The numerical results show that the method is computationally efficient.","The proposed methodology is validated on the real test case of Lyon North City, benchmarking with deterministic and stochastic user equilibria."],"url":"http://arxiv.org/abs/2402.04963v2","category":"math.OC"}
{"created":"2024-02-07 15:39:07","title":"Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems","abstract":"Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks. Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner. However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial. As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and perceived performance than intent-based systems, suggesting that switching NLP techniques should be investigated further.","sentences":["Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks.","Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns.","However, this rigidness does not handle the diversity of natural language well.","Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner.","However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial.","As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability.","This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and perceived performance than intent-based systems, suggesting that switching NLP techniques should be investigated further."],"url":"http://arxiv.org/abs/2402.04955v1","category":"cs.HC"}
{"created":"2024-02-07 15:16:21","title":"An approach to automated videogame beta testing","abstract":"Videogames developed in the 1970s and 1980s were modest programs created in a couple of months by a single person, who played the roles of designer, artist and programmer. Since then, videogames have evolved to become a multi-million dollar industry. Today, AAA game development involves hundreds of people working together over several years. Management and engineering requirements have changed at the same pace. Although many of the processes have been adapted over time, this is not quite true for quality assurance tasks, which are still done mainly manually by human beta testers due to the specific peculiarities of videogames. This paper presents an approach to automate this beta testing.","sentences":["Videogames developed in the 1970s and 1980s were modest programs created in a couple of months by a single person, who played the roles of designer, artist and programmer.","Since then, videogames have evolved to become a multi-million dollar industry.","Today, AAA game development involves hundreds of people working together over several years.","Management and engineering requirements have changed at the same pace.","Although many of the processes have been adapted over time, this is not quite true for quality assurance tasks, which are still done mainly manually by human beta testers due to the specific peculiarities of videogames.","This paper presents an approach to automate this beta testing."],"url":"http://arxiv.org/abs/2402.04938v1","category":"cs.AI"}
{"created":"2024-02-07 15:15:14","title":"Charting the COVID Long Haul Experience -- A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence","abstract":"COVID Long Haul (CLH) is an emerging chronic illness with varied patient experiences. Our understanding of CLH is often limited to data from electronic health records (EHRs), such as diagnoses or problem lists, which do not capture the volatility and severity of symptoms or their impact. To better understand the unique presentation of CLH, we conducted a 3-month long cohort study with 14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective (weekly surveys, interviews) data. Our findings reveal a complex presentation of symptoms, associated uncertainty, and the ensuing impact CLH has on patients' personal and professional lives. We identify patient needs, practices, and challenges around adhering to clinical recommendations, engaging with health data, and establishing \"new normals\" post COVID. We reflect on the potential found at the intersection of these various data streams and the persuasive heuristics possible when designing for this new population and their specific needs.","sentences":["COVID Long Haul (CLH) is an emerging chronic illness with varied patient experiences.","Our understanding of CLH is often limited to data from electronic health records (EHRs), such as diagnoses or problem lists, which do not capture the volatility and severity of symptoms or their impact.","To better understand the unique presentation of CLH, we conducted a 3-month long cohort study with 14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective (weekly surveys, interviews) data.","Our findings reveal a complex presentation of symptoms, associated uncertainty, and the ensuing impact CLH has on patients' personal and professional lives.","We identify patient needs, practices, and challenges around adhering to clinical recommendations, engaging with health data, and establishing \"new normals\" post COVID.","We reflect on the potential found at the intersection of these various data streams and the persuasive heuristics possible when designing for this new population and their specific needs."],"url":"http://arxiv.org/abs/2402.04937v1","category":"cs.HC"}
{"created":"2024-02-07 14:56:13","title":"Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation","abstract":"This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.","sentences":["This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA).","Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process.","Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model.","We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data.","We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA.","The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images."],"url":"http://arxiv.org/abs/2402.04929v1","category":"cs.CV"}
{"created":"2024-02-07 14:44:42","title":"Prompting Implicit Discourse Relation Annotation","abstract":"Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.","sentences":["Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers.","Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches.","This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations.","In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks.","Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings."],"url":"http://arxiv.org/abs/2402.04918v1","category":"cs.CL"}
{"created":"2024-02-07 14:28:04","title":"The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer","abstract":"In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.","sentences":["In this paper, we present a novel sequential team selection model in soccer.","Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data.","Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability.","We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season.","Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams."],"url":"http://arxiv.org/abs/2402.04898v1","category":"cs.AI"}
{"created":"2024-02-07 14:24:41","title":"Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning","abstract":"Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experiments show that our method enables more efficient target detection compared to state-of-the-art learning and non-learning baselines. We also show the applicability of our approach for orchard monitoring using an unmanned aerial vehicle in a photorealistic simulator.","sentences":["Autonomous robots are often employed for data collection due to their efficiency and low labour costs.","A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life.","Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions.","To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments.","A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest.","For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest.","Our experiments show that our method enables more efficient target detection compared to state-of-the-art learning and non-learning baselines.","We also show the applicability of our approach for orchard monitoring using an unmanned aerial vehicle in a photorealistic simulator."],"url":"http://arxiv.org/abs/2402.04894v1","category":"cs.RO"}
{"created":"2024-02-07 14:24:04","title":"A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration","abstract":"The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.","sentences":["The probabilistic formal verification (PFV) of AI systems is in its infancy.","So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   ","We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   ","Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   ","We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework."],"url":"http://arxiv.org/abs/2402.04892v1","category":"cs.AI"}
{"created":"2024-02-07 14:19:03","title":"Comparing Methods for Creating a National Random Sample of Twitter Users","abstract":"Twitter data has been widely used by researchers across various social and computer science disciplines. A common aim when working with Twitter data is the construction of a random sample of users from a given country. However, while several methods have been proposed in the literature, their comparative performance is mostly unexplored. In this paper, we implement four methods to collect a random sample of Twitter users in the US: 1% Stream, Bounding Box, Location Query, and Language Query. Then, we compare the methods according to their tweet- and user-level metrics as well as their accuracy in estimating US population with and without using inclusion probabilities of various demographics. Our results show that the 1% Stream method performs differently than others and best for the construction of a population representative sample, though its statistical significance is questionable due to large confidence intervals. We discuss the conditions under which the 1% Stream method may not be suitable and suggest the Bounding Box method as the second-best method to use.","sentences":["Twitter data has been widely used by researchers across various social and computer science disciplines.","A common aim when working with Twitter data is the construction of a random sample of users from a given country.","However, while several methods have been proposed in the literature, their comparative performance is mostly unexplored.","In this paper, we implement four methods to collect a random sample of Twitter users in the US: 1% Stream, Bounding Box, Location Query, and Language Query.","Then, we compare the methods according to their tweet- and user-level metrics as well as their accuracy in estimating US population with and without using inclusion probabilities of various demographics.","Our results show that the 1% Stream method performs differently than others and best for the construction of a population representative sample, though its statistical significance is questionable due to large confidence intervals.","We discuss the conditions under which the 1% Stream method may not be suitable and suggest the Bounding Box method as the second-best method to use."],"url":"http://arxiv.org/abs/2402.04879v1","category":"cs.SI"}
{"created":"2024-02-07 14:09:34","title":"Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy","abstract":"As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that allow direct intervention in the state space, we design the root cause localization task in our simulated fault alarm environment and then empirically show the effectiveness and robustness of the proposed method against state-of-the-art baselines. Theoretical analysis shows that our performance improvement attributes to the virtuous cycle of causal-guided policy learning and causal structure learning, which aligns with our experimental results.","sentences":["As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space.","However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL.","In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy.","We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment.","To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation.","Due to the lack of public benchmarks that allow direct intervention in the state space, we design the root cause localization task in our simulated fault alarm environment and then empirically show the effectiveness and robustness of the proposed method against state-of-the-art baselines.","Theoretical analysis shows that our performance improvement attributes to the virtuous cycle of causal-guided policy learning and causal structure learning, which aligns with our experimental results."],"url":"http://arxiv.org/abs/2402.04869v1","category":"cs.LG"}
{"created":"2024-02-07 13:55:27","title":"CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay","abstract":"Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines.","sentences":["Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability.","However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC).","In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt).","Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay.","By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis.","Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization.","CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset.","Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines."],"url":"http://arxiv.org/abs/2402.04858v1","category":"cs.AI"}
{"created":"2024-02-07 13:54:38","title":"Explaining Learned Reward Functions with Counterfactual Trajectories","abstract":"Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajectories and generalises to out-of-distribution examples. Although CTEs do not lead to a perfect understanding of the reward, our method, and more generally the adaptation of XAI methods, are presented as a fruitful approach for interpreting learned reward functions.","sentences":["Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions.","Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions.","We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive.","We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria.","Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs.","CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories.","Further, it learns to accurately judge differences in rewards between trajectories and generalises to out-of-distribution examples.","Although CTEs do not lead to a perfect understanding of the reward, our method, and more generally the adaptation of XAI methods, are presented as a fruitful approach for interpreting learned reward functions."],"url":"http://arxiv.org/abs/2402.04856v1","category":"cs.AI"}
{"created":"2024-02-07 13:54:06","title":"Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey","abstract":"Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between \"Issue resolved\" and \"Issue finding\" that they hope to obtain. To address these issues, this study aims to support research insight surveys for beginner researchers by establishing a hierarchical tree-structured knowledge graph that reflects the inheritance insight of research topics and the relevance insight among the academic papers.","sentences":["Research surveys have always posed a challenge for beginner researchers who lack of research training.","These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time.","One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers.","However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly.","Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended.","They may lack of grasp important information about the insight connection between \"Issue resolved\" and \"Issue finding\" that they hope to obtain.","To address these issues, this study aims to support research insight surveys for beginner researchers by establishing a hierarchical tree-structured knowledge graph that reflects the inheritance insight of research topics and the relevance insight among the academic papers."],"url":"http://arxiv.org/abs/2402.04854v1","category":"cs.DL"}
{"created":"2024-02-07 13:44:28","title":"Reconfigurable Intelligent Surface for Industrial Automation: mmWave Propagation Measurement, Simulation, and Control Algorithm Requirements","abstract":"Reconfigurable intelligent surfaces (RISs) enable reliable low-latency millimeter wave (mmWave) communication links in cases of a blocked line-of-sight (LoS) between the base station (BS) and the user equipment (UE), i.e. a RIS mounted on a wall or the ceiling provides a bypass for the radio communication link. We present an active RIS with 127 patch antenna elements arranged in a hexagonal grid for a center frequency of 23.8 GHz. Each RIS element uses an orthogonal polarization transformation to enable amplification using a field-effect transistor (FET). The source and drain voltages of each FET is controlled using two bits. We assume that the coordinates of the UE in an industrial control scenario are known to the RIS. We measure the received power on a 2D grid of 60 cm by 100 cm with the RIS working in reflective and active mode. The results show that the RIS can successfully focus the radio signal at the desired target points. The half-power beam width is characterized in axial and radial directions with respect to the RIS position, obtaining a practical RIS configuration update criterion for a mobile UE. These results clearly show that RISs are prominent solutions for enabling reliable wireless communication in indoor industrial scenarios.","sentences":["Reconfigurable intelligent surfaces (RISs) enable reliable low-latency millimeter wave (mmWave) communication links in cases of a blocked line-of-sight (LoS) between the base station (BS) and the user equipment (UE), i.e. a RIS mounted on a wall or the ceiling provides a bypass for the radio communication link.","We present an active RIS with 127 patch antenna elements arranged in a hexagonal grid for a center frequency of 23.8 GHz.","Each RIS element uses an orthogonal polarization transformation to enable amplification using a field-effect transistor (FET).","The source and drain voltages of each FET is controlled using two bits.","We assume that the coordinates of the UE in an industrial control scenario are known to the RIS.","We measure the received power on a 2D grid of 60 cm by 100 cm with the RIS working in reflective and active mode.","The results show that the RIS can successfully focus the radio signal at the desired target points.","The half-power beam width is characterized in axial and radial directions with respect to the RIS position, obtaining a practical RIS configuration update criterion for a mobile UE.","These results clearly show that RISs are prominent solutions for enabling reliable wireless communication in indoor industrial scenarios."],"url":"http://arxiv.org/abs/2402.04844v1","category":"eess.SY"}
{"created":"2024-02-07 13:39:38","title":"PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition","abstract":"In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.","sentences":["In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs).","The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length.","To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications.","PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency.","Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese.","Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets."],"url":"http://arxiv.org/abs/2402.04838v1","category":"cs.CL"}
{"created":"2024-02-07 13:32:53","title":"On the Completeness of Invariant Geometric Deep Learning Models","abstract":"Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness of three well-established geometric models: DimeNet, GemNet and SphereNet. Our results fill the gap in the theoretical power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities. Experimentally, GeoNGNN exhibits good inductive bias in capturing local environments, and achieves competitive results w.r.t. complicated models relying on high-order invariant/equivariant representations while exhibiting significantly faster computational speed.","sentences":["Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features.","These models are characterized by their simplicity, good experimental results and computational efficiency.","However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models.","In this work, we concentrate on characterizing the theoretical expressiveness of invariant models.","We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs.","To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN.","Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness of three well-established geometric models: DimeNet, GemNet and SphereNet.","Our results fill the gap in the theoretical power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities.","Experimentally, GeoNGNN exhibits good inductive bias in capturing local environments, and achieves competitive results w.r.t.","complicated models relying on high-order invariant/equivariant representations while exhibiting significantly faster computational speed."],"url":"http://arxiv.org/abs/2402.04836v1","category":"cs.LG"}
{"created":"2024-02-07 13:31:59","title":"Structured d-DNNF Is Not Closed Under Negation","abstract":"Both structured d-DNNF and SDD can be exponentially more succinct than OBDD. Moreover, SDD is essentially as tractable as OBDD. But this has left two important open questions. Firstly, does OBDD support more tractable transformations than structured d-DNNF? And secondly, is structured d-DNNF more succinct than SDD? In this paper, we answer both questions in the affirmative. For the first question we show that, unlike OBDD, structured d-DNNF does not support polytime negation, disjunction, or existential quantification operations. As a corollary, we deduce that there are functions with an equivalent polynomial-sized structured d-DNNF but with no such representation as an SDD, thus answering the second question. We also lift this second result to arithmetic circuits (AC) to show a succinctness gap between PSDD and the monotone AC analogue to structured d-DNNF.","sentences":["Both structured d-DNNF and SDD can be exponentially more succinct than OBDD.","Moreover, SDD is essentially as tractable as OBDD.","But this has left two important open questions.","Firstly, does OBDD support more tractable transformations than structured d-DNNF?","And secondly, is structured d-DNNF more succinct than SDD?","In this paper, we answer both questions in the affirmative.","For the first question we show that, unlike OBDD, structured d-DNNF does not support polytime negation, disjunction, or existential quantification operations.","As a corollary, we deduce that there are functions with an equivalent polynomial-sized structured d-DNNF","but with no such representation as an SDD, thus answering the second question.","We also lift this second result to arithmetic circuits (AC) to show a succinctness gap between PSDD and the monotone AC analogue to structured d-DNNF."],"url":"http://arxiv.org/abs/2402.04832v1","category":"cs.AI"}
{"created":"2024-02-07 18:59:12","title":"Opening the AI black box: program synthesis via mechanistic interpretability","abstract":"We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.","sentences":["We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code.","We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30).","MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm.","As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub.","We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy."],"url":"http://arxiv.org/abs/2402.05110v1","category":"cs.LG"}
{"created":"2024-02-07 18:55:22","title":"Tighter Generalisation Bounds via Interpolation","abstract":"This paper contains a recipe for deriving new PAC-Bayes generalisation bounds based on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to KL, Wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties. We explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases. We also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances.","sentences":["This paper contains a recipe for deriving new PAC-Bayes generalisation bounds based on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to KL, Wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties.","We explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases.","We also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances."],"url":"http://arxiv.org/abs/2402.05101v1","category":"stat.ML"}
{"created":"2024-02-07 18:49:21","title":"Convergence of spatial branching processes to $\u03b1$-stable CSBPs: Genealogy of semi-pushed fronts","abstract":"We consider inhomogeneous branching diffusions on an infinite domain of $\\mathbb{R}^d$. The first aim of this article is to derive a general criterium under which the size process (number of particles) and the genealogy of the particle system become undistinguishable from the ones of an $\\alpha$-stable CSBP, with $\\alpha\\in(1,2)$. The branching diffusion is encoded as a random metric space capturing all the information about the positions and the genealogical structure of the population. Our convergence criterium is based on the convergence of the moments for random metric spaces, which in turn can be efficiently computed through many-to-few formulas. It requires an extension of the method of moments to general CSBPs (with or without finite second moment).   In a recent work, Tourniaire introduced a branching Brownian motion which can be thought of as a toy model for fluctuating pushed fronts. The size process was shown to converge to an $\\alpha$-stable CSBP and it was conjectured that a genealogical convergence should occur jointly. We prove this result as an application of our general methodology.","sentences":["We consider inhomogeneous branching diffusions on an infinite domain of $\\mathbb{R}^d$. The first aim of this article is to derive a general criterium under which the size process (number of particles) and the genealogy of the particle system become undistinguishable from the ones of an $\\alpha$-stable CSBP, with $\\alpha\\in(1,2)$. The branching diffusion is encoded as a random metric space capturing all the information about the positions and the genealogical structure of the population.","Our convergence criterium is based on the convergence of the moments for random metric spaces, which in turn can be efficiently computed through many-to-few formulas.","It requires an extension of the method of moments to general CSBPs (with or without finite second moment).   ","In a recent work, Tourniaire introduced a branching Brownian motion which can be thought of as a toy model for fluctuating pushed fronts.","The size process was shown to converge to an $\\alpha$-stable CSBP and it was conjectured that a genealogical convergence should occur jointly.","We prove this result as an application of our general methodology."],"url":"http://arxiv.org/abs/2402.05096v1","category":"math.PR"}
{"created":"2024-02-07 18:46:34","title":"Extreme value statistics of non-Markovian processes from a new class of integrable nonlinear differential equation","abstract":"This research presents a universal mapping between two key concepts in the study of stochastic processes: nonequilibrium steady-states under confinement and extreme value statistics, focusing on one-dimensional systems. The mapping holds irrespectively of the statistics of the noise driving the dynamics. This result is based on a novel approach that provides an exact trajectory-wise solution for first-order dynamics near a hard-wall boundary. I first demonstrate the applicability of this method by efficiently reproducing known results from Brownian motion theory, such as the distribution of the running maximum, and by deriving new ones, such as the survival probability below a specific curve with conditioning on the endpoint value. I then extend the analysis to non-Markovian processes, focusing on run-and-tumble particles. The mapping to a steady-state allows to compute several quantities of interest, such as the long-time probability that run-and-tumble and Brownian particles do not cross each other.","sentences":["This research presents a universal mapping between two key concepts in the study of stochastic processes: nonequilibrium steady-states under confinement and extreme value statistics, focusing on one-dimensional systems.","The mapping holds irrespectively of the statistics of the noise driving the dynamics.","This result is based on a novel approach that provides an exact trajectory-wise solution for first-order dynamics near a hard-wall boundary.","I first demonstrate the applicability of this method by efficiently reproducing known results from Brownian motion theory, such as the distribution of the running maximum, and by deriving new ones, such as the survival probability below a specific curve with conditioning on the endpoint value.","I then extend the analysis to non-Markovian processes, focusing on run-and-tumble particles.","The mapping to a steady-state allows to compute several quantities of interest, such as the long-time probability that run-and-tumble and Brownian particles do not cross each other."],"url":"http://arxiv.org/abs/2402.05091v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 18:40:02","title":"Electronic structure and magnetic tendencies of trilayer La$_4$Ni$_3$O$_{10}$ under pressure: structural transition, molecular orbitals, and layer differentiation","abstract":"Motivated by the recent observation of superconductivity in the pressurized trilayer La$_4$Ni$_3$O$_{10}$ Ruddlesden-Popper (RP) nickelate, we explore its structural, electronic, and magnetic properties as a function of hydrostatic pressure from first-principles calculations. We find that in both the bilayer and trilayer nickelates, an orthorhombic(monoclinic)-to-tetragonal transition under pressure takes place concomitantly with the onset of superconductivity. The electronic structure of La$_4$Ni$_3$O$_{10}$ can be understood using a molecular trimer basis wherein $n$ molecular subbands arise as the $d_{z^2}$ orbitals hybridize strongly along the $c$-axis within the trilayer. The magnetic tendencies indicate that the ground state at ambient pressure is formed by nonmagnetic inner planes and stripe-ordered outer planes that are antiferromagnetically coupled along the $c$ axis, resulting in an unusual $\\uparrow$, 0, $\\downarrow$ stacking that is consistent with the spin density wave model suggested by neutron diffraction. Such a state is destabilized by the pressures wherein superconductivity arises. Despite the presence of $d_{z^2}$ states at the Fermi level, the $d_{x^2-y^2}$ orbitals also play a key role in the electronic structure of La$_4$Ni$_3$O$_{10}$. This active role of the $d_{x^2-y^2}$ states in the low-energy physics of the trilayer RP nickelate, together with the distinct electronic behavior of inner and outer planes, resembles the physics of multilayer cuprates.","sentences":["Motivated by the recent observation of superconductivity in the pressurized trilayer La$_4$Ni$_3$O$_{10}$ Ruddlesden-Popper (RP) nickelate, we explore its structural, electronic, and magnetic properties as a function of hydrostatic pressure from first-principles calculations.","We find that in both the bilayer and trilayer nickelates, an orthorhombic(monoclinic)-to-tetragonal transition under pressure takes place concomitantly with the onset of superconductivity.","The electronic structure of La$_4$Ni$_3$O$_{10}$ can be understood using a molecular trimer basis wherein $n$ molecular subbands arise as the $d_{z^2}$ orbitals hybridize strongly along the $c$-axis within the trilayer.","The magnetic tendencies indicate that the ground state at ambient pressure is formed by nonmagnetic inner planes and stripe-ordered outer planes that are antiferromagnetically coupled along the $c$ axis, resulting in an unusual $\\uparrow$, 0, $\\downarrow$ stacking that is consistent with the spin density wave model suggested by neutron diffraction.","Such a state is destabilized by the pressures wherein superconductivity arises.","Despite the presence of $d_{z^2}$ states at the Fermi level, the $d_{x^2-y^2}$ orbitals also play a key role in the electronic structure of La$_4$Ni$_3$O$_{10}$. This active role of the $d_{x^2-y^2}$ states in the low-energy physics of the trilayer RP nickelate, together with the distinct electronic behavior of inner and outer planes, resembles the physics of multilayer cuprates."],"url":"http://arxiv.org/abs/2402.05085v1","category":"cond-mat.supr-con"}
{"created":"2024-02-07 18:33:04","title":"Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation","abstract":"In recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks. While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability. Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.","sentences":["In recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks.","While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms.","However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation.","Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability.","Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network.","This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images.","We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance.","We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset.","The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting.","The source code and baseline implementations are available."],"url":"http://arxiv.org/abs/2402.05079v1","category":"eess.IV"}
{"created":"2024-02-07 18:31:06","title":"Cycle-factors in oriented graphs","abstract":"Let $k$ be a positive integer. A $k$-cycle-factor of an oriented graph is a set of disjoint cycles of length $k$ that covers all vertices of the graph. In this paper, we prove that there exists a positive constant $c$ such that for $n$ sufficiently large, any oriented graph on $n$ vertices with both minimum out-degree and minimum in-degree at least $(1/2-c)n$ contains a $k$-cycle-factor for any $k\\geq4$. Additionally, under the same hypotheses, we also show that for any sequence $n_1, \\ldots, n_t$ with $\\sum^t_{i=1}n_i=n$ and the number of the $n_i$ equal to $3$ is $\\alpha n$, where $\\alpha$ is any real number with $0<\\alpha<1/3$, the oriented graph $D$ contains $t$ disjoint cycles of lengths $n_1, \\ldots, n_t$. This conclusion is the best possible in some sense and refines a result of Keevash and Sudakov.","sentences":["Let $k$ be a positive integer.","A $k$-cycle-factor of an oriented graph is a set of disjoint cycles of length $k$ that covers all vertices of the graph.","In this paper, we prove that there exists a positive constant $c$ such that for $n$ sufficiently large, any oriented graph on $n$ vertices with both minimum out-degree and minimum in-degree at least $(1/2-c)n$ contains a $k$-cycle-factor for any $k\\geq4$. Additionally, under the same hypotheses, we also show that for any sequence $n_1, \\ldots, n_t$ with $\\sum^t_{i=1}n_i=n$ and the number of the $n_i$ equal to $3$ is $\\alpha n$, where $\\alpha$ is any real number with $0<\\alpha<1/3$, the oriented graph $D$ contains $t$ disjoint cycles of lengths $n_1, \\ldots, n_t$.","This conclusion is the best possible in some sense and refines a result of Keevash and Sudakov."],"url":"http://arxiv.org/abs/2402.05077v1","category":"math.CO"}
{"created":"2024-02-07 18:29:38","title":"ARCollab: Towards Multi-User Interactive Cardiovascular Surgical Planning in Mobile Augmented Reality","abstract":"Surgical planning for congenital heart diseases requires a collaborative approach, traditionally involving the 3D-printing of physical heart models for inspection by surgeons and cardiologists. Recent advancements in mobile augmented reality (AR) technologies have offered a promising alternative, noted for their ease-of-use and portability. Despite this progress, there remains a gap in research exploring the use of multi-user mobile AR environments for facilitating collaborative cardiovascular surgical planning. We are developing ARCollab, an iOS AR application designed to allow multiple surgeons and cardiologists to interact with patient-specific 3D heart models in a shared environment. ARCollab allows surgeons and cardiologists to import heart models, perform gestures to manipulate the heart, and collaborate with other users without having to produce a physical heart model. We are excited by the potential for ARCollab to make long-term real-world impact, thanks to the ubiquity of iOS devices that will allow for ARCollab's easy distribution, deployment and adoption.","sentences":["Surgical planning for congenital heart diseases requires a collaborative approach, traditionally involving the 3D-printing of physical heart models for inspection by surgeons and cardiologists.","Recent advancements in mobile augmented reality (AR) technologies have offered a promising alternative, noted for their ease-of-use and portability.","Despite this progress, there remains a gap in research exploring the use of multi-user mobile AR environments for facilitating collaborative cardiovascular surgical planning.","We are developing ARCollab, an iOS AR application designed to allow multiple surgeons and cardiologists to interact with patient-specific 3D heart models in a shared environment.","ARCollab allows surgeons and cardiologists to import heart models, perform gestures to manipulate the heart, and collaborate with other users without having to produce a physical heart model.","We are excited by the potential for ARCollab to make long-term real-world impact, thanks to the ubiquity of iOS devices that will allow for ARCollab's easy distribution, deployment and adoption."],"url":"http://arxiv.org/abs/2402.05075v1","category":"cs.HC"}
{"created":"2024-02-07 18:10:54","title":"Connecting Kani's Lemma and path-finding in the Bruhat-Tits tree to compute supersingular endomorphism rings","abstract":"We give a deterministic polynomial time algorithm to compute the endomorphism ring of a supersingular elliptic curve in characteristic p, provided that we are given two noncommuting endomorphisms and the factorization of the discriminant of the ring $\\mathcal{O}_0$ they generate. At each prime $q$ for which $\\mathcal{O}_0$ is not maximal, we compute the endomorphism ring locally by computing a q-maximal order containing it and, when $q \\neq p$, recovering a path to $\\text{End}(E) \\otimes \\mathbb{Z}_q$ in the Bruhat-Tits tree. We use techniques of higher-dimensional isogenies to navigate towards the local endomorphism ring. Our algorithm improves on a previous algorithm which requires a restricted input and runs in subexponential time under certain heuristics. Page and Wesolowski give a probabilistic polynomial time algorithm to compute the endomorphism ring on input of a single non-scalar endomorphism. Beyond using techniques of higher-dimensional isogenies to divide endomorphisms by a scalar, our methods are completely different.","sentences":["We give a deterministic polynomial time algorithm to compute the endomorphism ring of a supersingular elliptic curve in characteristic p, provided that we are given two noncommuting endomorphisms and the factorization of the discriminant of the ring $\\mathcal{O}_0$ they generate.","At each prime $q$ for which $\\mathcal{O}_0$ is not maximal, we compute the endomorphism ring locally by computing a q-maximal order containing it and, when $q \\neq p$, recovering a path to $\\text{End}(E)","\\otimes \\mathbb{Z}_q$ in the Bruhat-Tits tree.","We use techniques of higher-dimensional isogenies to navigate towards the local endomorphism ring.","Our algorithm improves on a previous algorithm which requires a restricted input and runs in subexponential time under certain heuristics.","Page and Wesolowski give a probabilistic polynomial time algorithm to compute the endomorphism ring on input of a single non-scalar endomorphism.","Beyond using techniques of higher-dimensional isogenies to divide endomorphisms by a scalar, our methods are completely different."],"url":"http://arxiv.org/abs/2402.05059v1","category":"math.NT"}
{"created":"2024-02-07 17:51:38","title":"Causal Representation Learning from Multiple Distributions: A General Setting","abstract":"In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way. In some cases, each latent variable can even be recovered up to component-wise transformations. Experimental results verify our theoretical claims.","sentences":["In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects).","For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\\mathcal{G}_Z$. This problem has recently been known as causal representation learning.","This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes.","We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions.","We show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way.","In some cases, each latent variable can even be recovered up to component-wise transformations.","Experimental results verify our theoretical claims."],"url":"http://arxiv.org/abs/2402.05052v1","category":"cs.LG"}
{"created":"2024-02-07 17:38:00","title":"Monitoring the energy of a cavity by observing the emission of a repeatedly excited qubit","abstract":"The number of excitations in a large quantum system (harmonic oscillator or qudit) can be measured in a quantum non demolition manner using a dispersively coupled qubit. It typically requires a series of qubit pulses that encode various binary questions about the photon number. Recently, a method based on the fluorescence measurement of a qubit driven by a train of identical pulses was introduced to track the photon number in a cavity, hence simplifying its monitoring and raising interesting questions about the measurement backaction of this scheme. A first realization with superconducting circuits demonstrated how the average number of photons could be measured in this way. Here we present an experiment that reaches single shot photocounting and number tracking owing to a cavity decay rate 4 orders of magnitude smaller than both the dispersive coupling rate and the qubit emission rate. An innovative notch filter and pogo-pin based galvanic contact makes possible these seemingly incompatible features. The qubit dynamics under the pulse train is characterized. We observe quantum jumps by monitoring the photon number via the qubit fluorescence as photons leave the cavity one at a time. Besides, we extract the measurement rate and induced dephasing rate and compare them to theoretical models. Our method could be applied to quantum error correction protocols on bosonic codes or qudits.","sentences":["The number of excitations in a large quantum system (harmonic oscillator or qudit) can be measured in a quantum non demolition manner using a dispersively coupled qubit.","It typically requires a series of qubit pulses that encode various binary questions about the photon number.","Recently, a method based on the fluorescence measurement of a qubit driven by a train of identical pulses was introduced to track the photon number in a cavity, hence simplifying its monitoring and raising interesting questions about the measurement backaction of this scheme.","A first realization with superconducting circuits demonstrated how the average number of photons could be measured in this way.","Here we present an experiment that reaches single shot photocounting and number tracking owing to a cavity decay rate 4 orders of magnitude smaller than both the dispersive coupling rate and the qubit emission rate.","An innovative notch filter and pogo-pin based galvanic contact makes possible these seemingly incompatible features.","The qubit dynamics under the pulse train is characterized.","We observe quantum jumps by monitoring the photon number via the qubit fluorescence as photons leave the cavity one at a time.","Besides, we extract the measurement rate and induced dephasing rate and compare them to theoretical models.","Our method could be applied to quantum error correction protocols on bosonic codes or qudits."],"url":"http://arxiv.org/abs/2402.05046v1","category":"quant-ph"}
{"created":"2024-02-07 17:34:32","title":"Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty","abstract":"Multi-modal sensor data fusion takes advantage of complementary or reinforcing information from each sensor and can boost overall performance in applications such as scene classification and target detection. This paper presents a new method for fusing multi-modal and multi-resolution remote sensor data without requiring pixel-level training labels, which can be difficult to obtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion (MIMRF) framework that addresses label uncertainty for fusion, but it can be slow to train due to the large search space for the fuzzy measures used to integrate sensor data sources. We propose a new method based on binary fuzzy measures, which reduces the search space and significantly improves the efficiency of the MIMRF framework. We present experimental results on synthetic data and a real-world remote sensing detection task and show that the proposed MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution fusion given remote sensing data with uncertainty.","sentences":["Multi-modal sensor data fusion takes advantage of complementary or reinforcing information from each sensor and can boost overall performance in applications such as scene classification and target detection.","This paper presents a new method for fusing multi-modal and multi-resolution remote sensor data without requiring pixel-level training labels, which can be difficult to obtain.","Previously, we developed a Multiple Instance Multi-Resolution Fusion (MIMRF) framework that addresses label uncertainty for fusion, but it can be slow to train due to the large search space for the fuzzy measures used to integrate sensor data sources.","We propose a new method based on binary fuzzy measures, which reduces the search space and significantly improves the efficiency of the MIMRF framework.","We present experimental results on synthetic data and a real-world remote sensing detection task and show that the proposed MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution fusion given remote sensing data with uncertainty."],"url":"http://arxiv.org/abs/2402.05045v1","category":"cs.CV"}
{"created":"2024-02-07 17:28:09","title":"Sticky Fingers: Resilience of Satellite Fingerprinting against Jamming Attacks","abstract":"In the wake of increasing numbers of attacks on radio communication systems, a range of techniques are being deployed to increase the security of these systems. One such technique is radio fingerprinting, in which the transmitter can be identified and authenticated by observing small hardware differences expressed in the signal. Fingerprinting has been explored in particular in the defense of satellite systems, many of which are insecure and cannot be retrofitted with cryptographic security.   In this paper, we evaluate the effectiveness of radio fingerprinting techniques under interference and jamming attacks, usually intended to deny service. By taking a pre-trained fingerprinting model and gathering a new dataset in which different levels of Gaussian noise and tone jamming have been added to the legitimate signal, we assess the attacker power required in order to disrupt the transmitter fingerprint such that it can no longer be recognized. We compare this to Gaussian jamming on the data portion of the signal, obtaining the remarkable result that transmitter fingerprints are still recognizable even in the presence of moderate levels of noise. Through deeper analysis of the results, we conclude that it takes a similar amount of jamming power in order to disrupt the fingerprint as it does to jam the message contents itself, so it is safe to include a fingerprinting system to authenticate satellite communication without opening up the system to easier denial-of-service attacks.","sentences":["In the wake of increasing numbers of attacks on radio communication systems, a range of techniques are being deployed to increase the security of these systems.","One such technique is radio fingerprinting, in which the transmitter can be identified and authenticated by observing small hardware differences expressed in the signal.","Fingerprinting has been explored in particular in the defense of satellite systems, many of which are insecure and cannot be retrofitted with cryptographic security.   ","In this paper, we evaluate the effectiveness of radio fingerprinting techniques under interference and jamming attacks, usually intended to deny service.","By taking a pre-trained fingerprinting model and gathering a new dataset in which different levels of Gaussian noise and tone jamming have been added to the legitimate signal, we assess the attacker power required in order to disrupt the transmitter fingerprint such that it can no longer be recognized.","We compare this to Gaussian jamming on the data portion of the signal, obtaining the remarkable result that transmitter fingerprints are still recognizable even in the presence of moderate levels of noise.","Through deeper analysis of the results, we conclude that it takes a similar amount of jamming power in order to disrupt the fingerprint as it does to jam the message contents itself, so it is safe to include a fingerprinting system to authenticate satellite communication without opening up the system to easier denial-of-service attacks."],"url":"http://arxiv.org/abs/2402.05042v1","category":"cs.CR"}
{"created":"2024-02-07 17:23:50","title":"Blow-up of solutions for a semilinear parabolic equation with nonlinear memory and absorption under nonlinear nonlocal boundary condition","abstract":"In this paper we consider initial boundary value problem for a parabolic equation with nonlinear memory and absorption under nonlinear nonlocal boundary condition. We prove global existence and blow-up of solutions.","sentences":["In this paper we consider initial boundary value problem for a parabolic equation with nonlinear memory and absorption under nonlinear nonlocal boundary condition.","We prove global existence and blow-up of solutions."],"url":"http://arxiv.org/abs/2402.05040v1","category":"math.AP"}
{"created":"2024-02-07 17:23:15","title":"PAC Learnability under Explanation-Preserving Graph Perturbations","abstract":"Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others. Graph neural networks (GNN) are neural models that operate over graphs, enabling the model to leverage the complex relationships and dependencies in graph-structured data. A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph. This work considers two methods for leveraging such perturbation invariances in the design and training of GNNs. First, explanation-assisted learning rules are considered. It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. Next, explanation-assisted data augmentation is considered, where the training set is enlarged by artificially producing new training samples via perturbation of the non-explanation edges in the original training set. It is shown that such data augmentation methods may improve performance if the augmented data is in-distribution, however, it may also lead to worse sample complexity compared to explanation-agnostic learning rules if the augmented data is out-of-distribution. Extensive empirical evaluations are provided to verify the theoretical analysis.","sentences":["Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others.","Graph neural networks (GNN) are neural models that operate over graphs, enabling the model to leverage the complex relationships and dependencies in graph-structured data.","A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label.","Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph.","This work considers two methods for leveraging such perturbation invariances in the design and training of GNNs.","First, explanation-assisted learning rules are considered.","It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning.","Next, explanation-assisted data augmentation is considered, where the training set is enlarged by artificially producing new training samples via perturbation of the non-explanation edges in the original training set.","It is shown that such data augmentation methods may improve performance if the augmented data is in-distribution, however, it may also lead to worse sample complexity compared to explanation-agnostic learning rules if the augmented data is out-of-distribution.","Extensive empirical evaluations are provided to verify the theoretical analysis."],"url":"http://arxiv.org/abs/2402.05039v1","category":"cs.LG"}
{"created":"2024-02-07 17:17:14","title":"Determining the nanoflare heating frequency of an X-ray Bright Point observed by MaGIXS","abstract":"Nanoflares are thought to be one of the prime candidates that can heat the solar corona to its multi-million kelvin temperature. Individual nanoflares are difficult to detect with the present generation instruments, however their presence can be inferred by comparing simulated nanoflare-heated plasma emissions with the observed emission. Using HYDRAD coronal loop simulations, we model the emission from an X-ray bright point (XBP) observed by the Marshall Grazing Incidence X-ray Spectrometer (MaGIXS), along with nearest-available observations from the Atmospheric Imaging Assembly (AIA) onboard Solar Dynamics Observatory (SDO) and X-Ray Telescope (XRT) onboard Hinode observatory. The length and magnetic field strength of the coronal loops are derived from the linear-force-free extrapolation of the observed photospheric magnetogram by Helioseismic and Magnetic Imager (HMI) onboard SDO. Each loop is assumed to be heated by random nanoflares, whose magnitude and frequency are determined by the loop length and magnetic field strength. The simulation results are then compared and matched against the measured intensity from AIA, XRT, and MaGIXS. Our model results indicate the observed emissions from the XBP under study could be well matched by a distribution of nanoflares with average delay times 1500 s to 3000 s, which suggest that the heating is dominated by high-frequency events. Further, we demonstrate the high sensitivity of MaGIXS and XRT to diagnose the heating frequency using this method, while AIA passbands are found to be the least sensitive.","sentences":["Nanoflares are thought to be one of the prime candidates that can heat the solar corona to its multi-million kelvin temperature.","Individual nanoflares are difficult to detect with the present generation instruments, however their presence can be inferred by comparing simulated nanoflare-heated plasma emissions with the observed emission.","Using HYDRAD coronal loop simulations, we model the emission from an X-ray bright point (XBP) observed by the Marshall Grazing Incidence X-ray Spectrometer (MaGIXS), along with nearest-available observations from the Atmospheric Imaging Assembly (AIA) onboard Solar Dynamics Observatory (SDO) and X-Ray Telescope (XRT) onboard Hinode observatory.","The length and magnetic field strength of the coronal loops are derived from the linear-force-free extrapolation of the observed photospheric magnetogram by Helioseismic and Magnetic Imager (HMI) onboard SDO.","Each loop is assumed to be heated by random nanoflares, whose magnitude and frequency are determined by the loop length and magnetic field strength.","The simulation results are then compared and matched against the measured intensity from AIA, XRT, and MaGIXS.","Our model results indicate the observed emissions from the XBP under study could be well matched by a distribution of nanoflares with average delay times 1500 s to 3000 s, which suggest that the heating is dominated by high-frequency events.","Further, we demonstrate the high sensitivity of MaGIXS and XRT to diagnose the heating frequency using this method, while AIA passbands are found to be the least sensitive."],"url":"http://arxiv.org/abs/2402.05036v1","category":"astro-ph.SR"}
{"created":"2024-02-07 16:56:45","title":"Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment","abstract":"This study evaluates the health effects of long-term exposure to PM10 in Seoul. Building on the preliminary model Shin and Bithell (2019), an in-silico agent-based model (ABM) is used to simulate the travel patterns of individuals according to their origins and destinations. During the simulation, each person, with their inherent socio-economic attributes and allocated origin and destination location, is assumed to commute to and from the same places for 10 consecutive years. A nominal measure of their health is set to decrease whenever the concentration of PM10 exceeds the national standard. Sensitivity analysis on calibrated parameters reveals increased vulnerability among certain demographic groups, particularly those aged over 65 and under 15, with a significant health decline associated with road proximity. The study reveals a substantial health disparity after 7,000 simulation ticks (equivalent to 10 years), especially under scenarios of a 3% annual increase in pollution levels. Long-term exposure to PM10 has a significant impact on health vulnerabilities, despite initial resilience being minimal. The study emphasises the importance of future research that takes into account different pollution thresholds as well as more detailed models of population dynamics and pollution generation in order to better understand and mitigate the health effects of air pollution on diverse urban populations.","sentences":["This study evaluates the health effects of long-term exposure to PM10 in Seoul.","Building on the preliminary model Shin and Bithell (2019), an in-silico agent-based model (ABM) is used to simulate the travel patterns of individuals according to their origins and destinations.","During the simulation, each person, with their inherent socio-economic attributes and allocated origin and destination location, is assumed to commute to and from the same places for 10 consecutive years.","A nominal measure of their health is set to decrease whenever the concentration of PM10 exceeds the national standard.","Sensitivity analysis on calibrated parameters reveals increased vulnerability among certain demographic groups, particularly those aged over 65 and under 15, with a significant health decline associated with road proximity.","The study reveals a substantial health disparity after 7,000 simulation ticks (equivalent to 10 years), especially under scenarios of a 3% annual increase in pollution levels.","Long-term exposure to PM10 has a significant impact on health vulnerabilities, despite initial resilience being minimal.","The study emphasises the importance of future research that takes into account different pollution thresholds as well as more detailed models of population dynamics and pollution generation in order to better understand and mitigate the health effects of air pollution on diverse urban populations."],"url":"http://arxiv.org/abs/2402.05029v1","category":"cs.MA"}
{"created":"2024-02-07 16:54:52","title":"Community detection problem based on polarization measures:an application to Twitter: the COVID-19 case in Spain","abstract":"In this paper, we address one of the most important topics in the field of Social Networks Analysis: the community detection problem with additional information. That additional information is modeled by a fuzzy measure that represents the risk of polarization. Particularly, we are interested in dealing with the problem of taking into account the polarization of nodes in the community detection problem. Adding this type of information to the community detection problem makes it more realistic, as a community is more likely to be defined if the corresponding elements are willing to maintain a peaceful dialogue. The polarization capacity is modeled by a fuzzy measure based on the JDJpol measure of polarization related to two poles. We also present an efficient algorithm for finding groups whose elements are no polarized. Hereafter, we work in a real case. It is a network obtained from Twitter, concerning the political position against the Spanish government taken by several influential users. We analyze how the partitions obtained change when some additional information related to how polarized that society is, is added to the problem.","sentences":["In this paper, we address one of the most important topics in the field of Social Networks Analysis: the community detection problem with additional information.","That additional information is modeled by a fuzzy measure that represents the risk of polarization.","Particularly, we are interested in dealing with the problem of taking into account the polarization of nodes in the community detection problem.","Adding this type of information to the community detection problem makes it more realistic, as a community is more likely to be defined if the corresponding elements are willing to maintain a peaceful dialogue.","The polarization capacity is modeled by a fuzzy measure based on the JDJpol measure of polarization related to two poles.","We also present an efficient algorithm for finding groups whose elements are no polarized.","Hereafter, we work in a real case.","It is a network obtained from Twitter, concerning the political position against the Spanish government taken by several influential users.","We analyze how the partitions obtained change when some additional information related to how polarized that society is, is added to the problem."],"url":"http://arxiv.org/abs/2402.05028v1","category":"cs.SI"}
{"created":"2024-02-07 17:46:37","title":"Federated Learning Can Find Friends That Are Beneficial","abstract":"In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.","sentences":["In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges.","While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental.","In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective.","We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution.","Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches.","This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years."],"url":"http://arxiv.org/abs/2402.05050v2","category":"cs.LG"}
{"created":"2024-02-07 17:03:20","title":"Optimal input reverberation and homeostatic self-organization towards the edge of synchronization","abstract":"Transient or partial synchronization can be used to do computations, although a fully synchronized network is frequently related to epileptic seizures. Here, we propose a homeostatic mechanism that is capable of maintaining a neuronal network at the edge of a synchronization transition, thereby avoiding the harmful consequences of a fully synchronized network. We model neurons by maps since they are dynamically richer than integrate-and-fire models and more computationally efficient than conductance-based approaches. We first describe the synchronization phase transition of a dense network of neurons with different tonic spiking frequencies coupled by gap junctions. We show that at the transition critical point, inputs optimally reverberate through the network activity through transient synchronization. Then, we introduce a local homeostatic dynamic in the synaptic coupling and show that it produces a robust self-organization toward the edge of this phase transition. We discuss the potential biological consequences of this self-organization process, such as its relation to the Brain Criticality hypothesis, its input processing capacity, and how its malfunction could lead to pathological synchronization.","sentences":["Transient or partial synchronization can be used to do computations, although a fully synchronized network is frequently related to epileptic seizures.","Here, we propose a homeostatic mechanism that is capable of maintaining a neuronal network at the edge of a synchronization transition, thereby avoiding the harmful consequences of a fully synchronized network.","We model neurons by maps since they are dynamically richer than integrate-and-fire models and more computationally efficient than conductance-based approaches.","We first describe the synchronization phase transition of a dense network of neurons with different tonic spiking frequencies coupled by gap junctions.","We show that at the transition critical point, inputs optimally reverberate through the network activity through transient synchronization.","Then, we introduce a local homeostatic dynamic in the synaptic coupling and show that it produces a robust self-organization toward the edge of this phase transition.","We discuss the potential biological consequences of this self-organization process, such as its relation to the Brain Criticality hypothesis, its input processing capacity, and how its malfunction could lead to pathological synchronization."],"url":"http://arxiv.org/abs/2402.05032v1","category":"nlin.AO"}
{"created":"2024-02-07 16:19:50","title":"Efficient Invariant Kalman Filter for Inertial-based Odometry with Large-sample Environmental Measurements","abstract":"A filter for inertial-based odometry is a recursive method used to estimate the pose from measurements of ego-motion and relative pose. Currently, there is no known filter that guarantees the computation of a globally optimal solution for the non-linear measurement model. In this paper, we demonstrate that an innovative filter, with the state being $SE_2(3)$ and the $\\sqrt{n}$-\\textit{consistent} pose as the initialization, efficiently achieves \\textit{asymptotic optimality} in terms of minimum mean square error. This approach is tailored for real-time SLAM and inertial-based odometry applications.   Our first contribution is that we propose an iterative filtering method based on the Gauss-Newton method on Lie groups which is numerically to solve the estimation of states from a priori and non-linear measurements. The filtering stands out due to its iterative mechanism and adaptive initialization. Second, when dealing with environmental measurements of the surroundings, we utilize a $\\sqrt{n}$-consistent pose as the initial value for the update step in a single iteration. The solution is closed in form and has computational complexity $O(n)$. Third, we theoretically show that the approach can achieve asymptotic optimality in the sense of minimum mean square error from the a priori and virtual relative pose measurements (see Problem~\\ref{prob:new update problem}). Finally, to validate our method, we carry out extensive numerical and experimental evaluations. Our results consistently demonstrate that our approach outperforms other state-of-the-art filter-based methods, including the iterated extended Kalman filter and the invariant extended Kalman filter, in terms of accuracy and running time.","sentences":["A filter for inertial-based odometry is a recursive method used to estimate the pose from measurements of ego-motion and relative pose.","Currently, there is no known filter that guarantees the computation of a globally optimal solution for the non-linear measurement model.","In this paper, we demonstrate that an innovative filter, with the state being $SE_2(3)$ and the $\\sqrt{n}$-\\textit{consistent} pose as the initialization, efficiently achieves \\textit{asymptotic optimality} in terms of minimum mean square error.","This approach is tailored for real-time SLAM and inertial-based odometry applications.   ","Our first contribution is that we propose an iterative filtering method based on the Gauss-Newton method on Lie groups which is numerically to solve the estimation of states from a priori and non-linear measurements.","The filtering stands out due to its iterative mechanism and adaptive initialization.","Second, when dealing with environmental measurements of the surroundings, we utilize a $\\sqrt{n}$-consistent pose as the initial value for the update step in a single iteration.","The solution is closed in form and has computational complexity $O(n)$. Third, we theoretically show that the approach can achieve asymptotic optimality in the sense of minimum mean square error from the a priori and virtual relative pose measurements (see Problem~\\ref{prob:new update problem}).","Finally, to validate our method, we carry out extensive numerical and experimental evaluations.","Our results consistently demonstrate that our approach outperforms other state-of-the-art filter-based methods, including the iterated extended Kalman filter and the invariant extended Kalman filter, in terms of accuracy and running time."],"url":"http://arxiv.org/abs/2402.05003v1","category":"cs.RO"}
{"created":"2024-02-07 16:06:20","title":"PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses","abstract":"This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the PriorBoost algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that PriorBoost regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms.","sentences":["This work studies algorithms for learning from aggregate responses.","We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions.","We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering.","Further, we theoretically quantify the advantage of using curated bags over random bags.","We then propose the PriorBoost algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality.","We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that PriorBoost regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms."],"url":"http://arxiv.org/abs/2402.04987v1","category":"cs.LG"}
{"created":"2024-02-07 15:57:54","title":"SWAP algorithm for lattice spin models","abstract":"We adapted the SWAP molecular dynamics algorithm for use in lattice Ising spin models. We dressed the spins with a randomly distributed length and we alternated long-range spin exchanges with conventional single spin flip Monte Carlo updates, both accepted with a stochastic rule which respects detailed balance. We show that this algorithm, when applied to the bidimensional Edwards-Anderson model, speeds up significantly the relaxation at low temperatures and manages to find ground states with high efficiency and little computational cost. The exploration of spin models should help in understanding why SWAP accelerates the evolution of particle systems and shed light on relations between dynamics and free-energy landscapes.","sentences":["We adapted the SWAP molecular dynamics algorithm for use in lattice Ising spin models.","We dressed the spins with a randomly distributed length and we alternated long-range spin exchanges with conventional single spin flip Monte Carlo updates, both accepted with a stochastic rule which respects detailed balance.","We show that this algorithm, when applied to the bidimensional Edwards-Anderson model, speeds up significantly the relaxation at low temperatures and manages to find ground states with high efficiency and little computational cost.","The exploration of spin models should help in understanding why SWAP accelerates the evolution of particle systems and shed light on relations between dynamics and free-energy landscapes."],"url":"http://arxiv.org/abs/2402.04981v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 15:57:30","title":"Asymptotics of feature learning in two-layer networks after one gradient-step","abstract":"In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond perturbative finite width corrections of the conjugate and neural tangent kernels.","sentences":["In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step.","Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate.","We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime.","To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond perturbative finite width corrections of the conjugate and neural tangent kernels."],"url":"http://arxiv.org/abs/2402.04980v1","category":"stat.ML"}
{"created":"2024-02-07 15:43:50","title":"ConvLoRA and AdaBN based Domain Adaptation via Self-Training","abstract":"Existing domain adaptation (DA) methods often involve pre-training on the source domain and fine-tuning on the target domain. For multi-target domain adaptation, having a dedicated/separate fine-tuned network for each target domain, that retain all the pre-trained model parameters, is prohibitively expensive. To address this limitation, we propose Convolutional Low-Rank Adaptation (ConvLoRA). ConvLoRA freezes pre-trained model weights, adds trainable low-rank decomposition matrices to convolutional layers, and backpropagates the gradient through these matrices thus greatly reducing the number of trainable parameters. To further boost adaptation, we utilize Adaptive Batch Normalization (AdaBN) which computes target-specific running statistics and use it along with ConvLoRA. Our method has fewer trainable parameters and performs better or on-par with large independent fine-tuned networks (with less than 0.9% trainable parameters of the total base model) when tested on the segmentation of Calgary-Campinas dataset containing brain MRI images. Our approach is simple, yet effective and can be applied to any deep learning-based architecture which uses convolutional and batch normalization layers. Code is available at: https://github.com/aleemsidra/ConvLoRA.","sentences":["Existing domain adaptation (DA) methods often involve pre-training on the source domain and fine-tuning on the target domain.","For multi-target domain adaptation, having a dedicated/separate fine-tuned network for each target domain, that retain all the pre-trained model parameters, is prohibitively expensive.","To address this limitation, we propose Convolutional Low-Rank Adaptation (ConvLoRA).","ConvLoRA freezes pre-trained model weights, adds trainable low-rank decomposition matrices to convolutional layers, and backpropagates the gradient through these matrices thus greatly reducing the number of trainable parameters.","To further boost adaptation, we utilize Adaptive Batch Normalization (AdaBN) which computes target-specific running statistics and use it along with ConvLoRA.","Our method has fewer trainable parameters and performs better or on-par with large independent fine-tuned networks (with less than 0.9% trainable parameters of the total base model) when tested on the segmentation of Calgary-Campinas dataset containing brain MRI images.","Our approach is simple, yet effective and can be applied to any deep learning-based architecture which uses convolutional and batch normalization layers.","Code is available at: https://github.com/aleemsidra/ConvLoRA."],"url":"http://arxiv.org/abs/2402.04964v1","category":"cs.CV"}
{"created":"2024-02-07 15:41:01","title":"Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation","abstract":"Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution. For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets. To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference. Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks. It is implemented by recalculating batch normalization statistics on test batches. Prior work has focused on analysis with test data that has the same label distribution as the training data. However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure. This presents a risk in applying test time adaptation methods in deployment. We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts. Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes. We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts. We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters.","sentences":["Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution.","For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets.","To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference.","Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks.","It is implemented by recalculating batch normalization statistics on test batches.","Prior work has focused on analysis with test data that has the same label distribution as the training data.","However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure.","This presents a risk in applying test time adaptation methods in deployment.","We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts.","Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes.","We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts.","We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters."],"url":"http://arxiv.org/abs/2402.04958v1","category":"cs.CV"}
{"created":"2024-02-07 15:31:51","title":"Energy Dissipation to Tungsten Surfaces upon Eley-Rideal Recombination of N2 and H2","abstract":"Quasiclassical molecular dynamics simulations are performed to investigate energy dissipation to the (100) and (110) tungsten surfaces upon Eley-Rideal (ER) recombination of H2 and N2. Calculations are carried out within the single adsorbate limit under normal incidence. A generalized Langevin surface oscillator (GLO) scheme is used to simulate the coupling to phonons, whereas electron-hole (e-h) pair excitations are implemented using the local density friction approximation (LDFA). Phonon excitations are found to reduce the ER reactivity for N2 recombination, but do not affect H abstraction. In contrast, the effect of e-h pair excitations on the ER recombination cross section is small for N2, but can be important for H2. The analysis of the energy lost by the recombined species shows that most of the energy is dissipated into phonon excitations in the N2 recombination and into electronic excitations in the H2 recombination. In all cases, the energy dissipated into e-h pairs is taken away from the translational kinetic energy of the formed molecules, whereas dissipation to phonons, only significant for N2, also affects vibration. Interestingly, the electron mediated energy losses are found to be smaller in the case of N2 when surface motion is allowed.","sentences":["Quasiclassical molecular dynamics simulations are performed to investigate energy dissipation to the (100) and (110) tungsten surfaces upon Eley-Rideal (ER) recombination of H2 and N2.","Calculations are carried out within the single adsorbate limit under normal incidence.","A generalized Langevin surface oscillator (GLO) scheme is used to simulate the coupling to phonons, whereas electron-hole (e-h) pair excitations are implemented using the local density friction approximation (LDFA).","Phonon excitations are found to reduce the ER reactivity for N2 recombination, but do not affect H abstraction.","In contrast, the effect of e-h pair excitations on the ER recombination cross section is small for N2, but can be important for H2.","The analysis of the energy lost by the recombined species shows that most of the energy is dissipated into phonon excitations in the N2 recombination and into electronic excitations in the H2 recombination.","In all cases, the energy dissipated into e-h pairs is taken away from the translational kinetic energy of the formed molecules, whereas dissipation to phonons, only significant for N2, also affects vibration.","Interestingly, the electron mediated energy losses are found to be smaller in the case of N2 when surface motion is allowed."],"url":"http://arxiv.org/abs/2402.04949v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 15:30:53","title":"A geometric model for semilinear locally gentle algebras","abstract":"We consider certain generalizations of gentle algebras that we call semilinear locally gentle algebras. These rings are examples of semilinear clannish algebras as introduced by the second author and Crawley-Boevey. We generalise the notion of a nodal algebra from work of Burban and Drozd and prove that semilinear gentle algebras are nodal by adapting a theorem of Zembyk. We also provide a geometric realization of Zembyk's proof, which involves cutting the surface into simpler pieces in order to endow our locally gentle algebra with a semilinear structure. We then consider this surface glued back together, with the seams in place, and use it to give a geometric model for the finite-dimensional modules over the semilinear locally gentle algebra.","sentences":["We consider certain generalizations of gentle algebras that we call semilinear locally gentle algebras.","These rings are examples of semilinear clannish algebras as introduced by the second author and Crawley-Boevey.","We generalise the notion of a nodal algebra from work of Burban and Drozd and prove that semilinear gentle algebras are nodal by adapting a theorem of Zembyk.","We also provide a geometric realization of Zembyk's proof, which involves cutting the surface into simpler pieces in order to endow our locally gentle algebra with a semilinear structure.","We then consider this surface glued back together, with the seams in place, and use it to give a geometric model for the finite-dimensional modules over the semilinear locally gentle algebra."],"url":"http://arxiv.org/abs/2402.04947v1","category":"math.RT"}
{"created":"2024-02-07 14:41:17","title":"Moco: A Learnable Meta Optimizer for Combinatorial Optimization","abstract":"Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum Independent Set (MIS) and show that it outperforms other approaches on MIS and is overall competitive on the TSP, especially outperforming related approaches, partially even if they use additional local search.","sentences":["Relevant combinatorial optimization problems (COPs) are often NP-hard.","While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data.","Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time.","Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state.","This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget.","This allows Moco to adapt to varying circumstances such as different computational budgets.","Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition.","We test Moco on the Traveling Salesman Problem (TSP) and Maximum Independent Set (MIS) and show that it outperforms other approaches on MIS and is overall competitive on the TSP, especially outperforming related approaches, partially even if they use additional local search."],"url":"http://arxiv.org/abs/2402.04915v1","category":"cs.LG"}
{"created":"2024-02-07 14:35:05","title":"L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ","abstract":"Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size for LLMs, aiming to enhance generality. The simultaneous quantization-and-fine-tuning process of L4Q is applicable to high-precision models, yielding linearly quantized weights with superior accuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families using an instructional dataset, showcase L4Q's capabilities in language comprehension and few-shot in-context learning, achieving sub-4-bit precision while maintaining comparable training times to applying PEFT on a quantized model.","sentences":["Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs).","In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy.","Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques.","However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration.","Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance.","To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training.","L4Q leverages LoRA-wise learned quantization step size for LLMs, aiming to enhance generality.","The simultaneous quantization-and-fine-tuning process of L4Q is applicable to high-precision models, yielding linearly quantized weights with superior accuracy.","Our experiments, conducted on the LLaMA and LLaMA2 model families using an instructional dataset, showcase L4Q's capabilities in language comprehension and few-shot in-context learning, achieving sub-4-bit precision while maintaining comparable training times to applying PEFT on a quantized model."],"url":"http://arxiv.org/abs/2402.04902v1","category":"cs.LG"}
{"created":"2024-02-07 14:23:58","title":"Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource management in multi-user video transcoding","abstract":"The coexistence of parallel applications in shared computing nodes, each one featuring different Quality of Service (QoS) requirements, carries out new challenges to improve resource occupation while keeping acceptable rates in terms of QoS. As more application-specific and system-wide metrics are included as QoS dimensions, or under situations in which resource-usage limits are strict, building and serving the most appropriate set of actions (application control knobs and system resource assignment) to concurrent applications in an automatic and optimal fashion becomes mandatory. In this paper, we propose strategies to build and serve this type of knowledge to concurrent applications by leveraging Reinforcement Learning techniques. Taking multi-user video transcoding as a driving example, our experimental results reveal an excellent adaptation of resource and knob management to heterogeneous QoS requests, and increases in the amount of concurrently served users up to 1.24x compared with alternative approaches considering homogeneous QoS requests.","sentences":["The coexistence of parallel applications in shared computing nodes, each one featuring different Quality of Service (QoS) requirements, carries out new challenges to improve resource occupation while keeping acceptable rates in terms of QoS.","As more application-specific and system-wide metrics are included as QoS dimensions, or under situations in which resource-usage limits are strict, building and serving the most appropriate set of actions (application control knobs and system resource assignment) to concurrent applications in an automatic and optimal fashion becomes mandatory.","In this paper, we propose strategies to build and serve this type of knowledge to concurrent applications by leveraging Reinforcement Learning techniques.","Taking multi-user video transcoding as a driving example, our experimental results reveal an excellent adaptation of resource and knob management to heterogeneous QoS requests, and increases in the amount of concurrently served users up to 1.24x compared with alternative approaches considering homogeneous QoS requests."],"url":"http://arxiv.org/abs/2402.04891v1","category":"cs.DC"}
{"created":"2024-02-07 13:54:56","title":"Advancing Anomaly Detection: An Adaptation Model and a New Dataset","abstract":"Industry surveillance is widely applicable in sectors like retail, manufacturing, education, and smart cities, each presenting unique anomalies requiring specialized detection. However, adapting anomaly detection models to novel viewpoints within the same scenario poses challenges. Extending these models to entirely new scenarios necessitates retraining or fine-tuning, a process that can be time consuming. To address these challenges, we propose the Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the few-shot learning framework for faster adaptation of pre-trained models to new concepts. Despite this approach, a significant challenge emerges from the absence of a comprehensive dataset with diverse scenarios and camera views. In response, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14 distinct scenarios captured from various camera views. This real-world dataset is the first high-resolution anomaly detection dataset, offering a solid foundation for training superior models. MSAD includes diverse normal motion patterns, incorporating challenging variations like different lighting and weather conditions. Through experimentation, we validate the efficacy of SA2D, particularly when trained on the MSAD dataset. Our results show that SA2D not only excels under novel viewpoints within the same scenario but also demonstrates competitive performance when faced with entirely new scenarios. This highlights our method's potential in addressing challenges in detecting anomalies across diverse and evolving surveillance scenarios.","sentences":["Industry surveillance is widely applicable in sectors like retail, manufacturing, education, and smart cities, each presenting unique anomalies requiring specialized detection.","However, adapting anomaly detection models to novel viewpoints within the same scenario poses challenges.","Extending these models to entirely new scenarios necessitates retraining or fine-tuning, a process that can be time consuming.","To address these challenges, we propose the Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the few-shot learning framework for faster adaptation of pre-trained models to new concepts.","Despite this approach, a significant challenge emerges from the absence of a comprehensive dataset with diverse scenarios and camera views.","In response, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14 distinct scenarios captured from various camera views.","This real-world dataset is the first high-resolution anomaly detection dataset, offering a solid foundation for training superior models.","MSAD includes diverse normal motion patterns, incorporating challenging variations like different lighting and weather conditions.","Through experimentation, we validate the efficacy of SA2D, particularly when trained on the MSAD dataset.","Our results show that SA2D not only excels under novel viewpoints within the same scenario but also demonstrates competitive performance when faced with entirely new scenarios.","This highlights our method's potential in addressing challenges in detecting anomalies across diverse and evolving surveillance scenarios."],"url":"http://arxiv.org/abs/2402.04857v1","category":"cs.CV"}
{"created":"2024-02-07 13:54:15","title":"Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction","abstract":"Transformers have recently emerged as a significant force in the field of image deraining. Existing image deraining methods utilize extensive research on self-attention. Though showcasing impressive results, they tend to neglect critical frequency information, as self-attention is generally less adept at capturing high-frequency details. To overcome this shortcoming, we have developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that integrates information from both spatial and frequency domains through Spatial Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block (FFEBlock). We have further introduced an effective Adaptive Fusion Module (AFM) for the dual-path feature aggregation. Extensive experiments on six public deraining benchmarks and downstream vision tasks have demonstrated that our proposed method not only outperforms the existing state-of-the-art deraining method but also achieves visually pleasuring results with excellent robustness on downstream vision tasks.","sentences":["Transformers have recently emerged as a significant force in the field of image deraining.","Existing image deraining methods utilize extensive research on self-attention.","Though showcasing impressive results, they tend to neglect critical frequency information, as self-attention is generally less adept at capturing high-frequency details.","To overcome this shortcoming, we have developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that integrates information from both spatial and frequency domains through Spatial Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block (FFEBlock).","We have further introduced an effective Adaptive Fusion Module (AFM) for the dual-path feature aggregation.","Extensive experiments on six public deraining benchmarks and downstream vision tasks have demonstrated that our proposed method not only outperforms the existing state-of-the-art deraining method but also achieves visually pleasuring results with excellent robustness on downstream vision tasks."],"url":"http://arxiv.org/abs/2402.04855v1","category":"cs.CV"}
{"created":"2024-02-07 13:51:26","title":"Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning","abstract":"In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis.","sentences":["In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning.","Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively.","Our strategy encompasses two-stage training: (i).","a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii).","fine-tuning for multi-patch prediction in the targeted time-series context.","A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding.","Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations.","aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis."],"url":"http://arxiv.org/abs/2402.04852v1","category":"cs.LG"}
{"created":"2024-02-07 13:41:53","title":"Data-efficient Large Vision Models through Sequential Autoregression","abstract":"Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding. These models are intended to not only comprehend but also seamlessly transit to out-of-domain tasks. However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens. In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset. We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase. Our empirical evaluations underscore the model's agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models. The code is available at https://github.com/ggjy/DeLVM.","sentences":["Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding.","These models are intended to not only comprehend but also seamlessly transit to out-of-domain tasks.","However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens.","In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset.","We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase.","Our empirical evaluations underscore the model's agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models.","The code is available at https://github.com/ggjy/DeLVM."],"url":"http://arxiv.org/abs/2402.04841v1","category":"cs.CV"}
{"created":"2024-02-07 13:22:17","title":"Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game","abstract":"Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem \"due to the essentially unconstrained nature of what other agents may do\". In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategies that are less verbose (staying silent in some of the steps) and that with respect to that the Guide's strategies indeed adapt to the partner's level of confidence and autonomy.","sentences":["Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem \"due to the essentially unconstrained nature of what other agents may do\".","In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game.","In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors.","We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy.","We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort.","Our results indicate that this novel ingredient leads to communicative strategies that are less verbose (staying silent in some of the steps) and that with respect to that the Guide's strategies indeed adapt to the partner's level of confidence and autonomy."],"url":"http://arxiv.org/abs/2402.04824v1","category":"cs.CL"}
{"created":"2024-02-07 13:06:11","title":"Microwave control of collective quantum jump statistics of a dissipative Rydberg gas","abstract":"Quantum many-body systems near phase transitions respond collectively to externally applied perturbations. We explore this phenomenon in a laser-driven dissipative Rydberg gas that is tuned to a bistable regime. Here two metastable phases coexist, which feature a low and high density of Rydberg atoms, respectively. The ensuing collective dynamics, which we monitor in situ, is characterized by stochastic collective jumps between these two macroscopically distinct many-body phases. We show that the statistics of these jumps can be controlled using a dual-tone microwave field. In particular, we find that the distribution of jump times develops peaks corresponding to subharmonics of the relative microwave detuning. Our study demonstrates the control of collective statistical properties of dissipative quantum many-body systems without the necessity of fine-tuning or of ultra cold temperatures. Such robust phenomena may find technological applications in quantum sensing and metrology.","sentences":["Quantum many-body systems near phase transitions respond collectively to externally applied perturbations.","We explore this phenomenon in a laser-driven dissipative Rydberg gas that is tuned to a bistable regime.","Here two metastable phases coexist, which feature a low and high density of Rydberg atoms, respectively.","The ensuing collective dynamics, which we monitor in situ, is characterized by stochastic collective jumps between these two macroscopically distinct many-body phases.","We show that the statistics of these jumps can be controlled using a dual-tone microwave field.","In particular, we find that the distribution of jump times develops peaks corresponding to subharmonics of the relative microwave detuning.","Our study demonstrates the control of collective statistical properties of dissipative quantum many-body systems without the necessity of fine-tuning or of ultra cold temperatures.","Such robust phenomena may find technological applications in quantum sensing and metrology."],"url":"http://arxiv.org/abs/2402.04815v1","category":"quant-ph"}
{"created":"2024-02-07 12:52:22","title":"Progressive unsupervised domain adaptation for ASR using ensemble models and multi-stage training","abstract":"In Automatic Speech Recognition (ASR), teacher-student (T/S) training has shown to perform well for domain adaptation with small amount of training data. However, adaption without ground-truth labels is still challenging. A previous study has shown the effectiveness of using ensemble teacher models in T/S training for unsupervised domain adaptation (UDA) but its performance still lags behind compared to the model trained on in-domain data. This paper proposes a method to yield better UDA by training multi-stage students with ensemble teacher models. Initially, multiple teacher models are trained on labelled data from read and meeting domains. These teachers are used to train a student model on unlabelled out-of-domain telephone speech data. To improve the adaptation, subsequent student models are trained sequentially considering previously trained model as their teacher. Experiments are conducted with three teachers trained on AMI, WSJ and LibriSpeech and three stages of students on SwitchBoard data. Results shown on eval00 test set show significant WER improvement with multi-stage training with an absolute gain of 9.8%, 7.7% and 3.3% at each stage.","sentences":["In Automatic Speech Recognition (ASR), teacher-student (T/S) training has shown to perform well for domain adaptation with small amount of training data.","However, adaption without ground-truth labels is still challenging.","A previous study has shown the effectiveness of using ensemble teacher models in T/S training for unsupervised domain adaptation (UDA) but its performance still lags behind compared to the model trained on in-domain data.","This paper proposes a method to yield better UDA by training multi-stage students with ensemble teacher models.","Initially, multiple teacher models are trained on labelled data from read and meeting domains.","These teachers are used to train a student model on unlabelled out-of-domain telephone speech data.","To improve the adaptation, subsequent student models are trained sequentially considering previously trained model as their teacher.","Experiments are conducted with three teachers trained on AMI, WSJ and LibriSpeech and three stages of students on SwitchBoard data.","Results shown on eval00 test set show significant WER improvement with multi-stage training with an absolute gain of 9.8%, 7.7% and 3.3% at each stage."],"url":"http://arxiv.org/abs/2402.04805v1","category":"eess.AS"}
{"created":"2024-02-07 12:39:47","title":"Strongly Polynomial Frame Scaling to High Precision","abstract":"The frame scaling problem is: given vectors $U := \\{u_{1}, ..., u_{n} \\} \\subseteq \\mathbb{R}^{d}$, marginals $c \\in \\mathbb{R}^{n}_{++}$, and precision $\\varepsilon > 0$, find left and right scalings $L \\in \\mathbb{R}^{d \\times d}, r \\in \\mathbb{R}^n$ such that $(v_1,\\dots,v_n) := (Lu_1 r_1,\\dots,Lu_nr_n)$ simultaneously satisfies $\\sum_{i=1}^n v_i v_i^{\\mathsf{T}} = I_d$ and $\\|v_{j}\\|_{2}^{2} = c_{j}, \\forall j \\in [n]$, up to error $\\varepsilon$. This problem has appeared in a variety of fields throughout linear algebra and computer science. In this work, we give a strongly polynomial algorithm for frame scaling with $\\log(1/\\varepsilon)$ convergence. This answers a question of Diakonikolas, Tzamos and Kane (STOC 2023), who gave the first strongly polynomial randomized algorithm with poly$(1/\\varepsilon)$ convergence for the special case $c = \\frac{d}{n} 1_{n}$. Our algorithm is deterministic, applies for general $c \\in \\mathbb{R}^{n}_{++}$, and requires $O(n^{3} \\log(n/\\varepsilon))$ iterations as compared to $O(n^{5} d^{11}/\\varepsilon^{5})$ iterations of DTK. By lifting the framework of Linial, Samorodnitsky and Wigderson (Combinatorica 2000) for matrix scaling to frames, we are able to simplify both the algorithm and analysis. Our main technical contribution is to generalize the potential analysis of LSW to the frame setting and compute an update step in strongly polynomial time that achieves geometric progress in each iteration. In fact, we can adapt our results to give an improved analysis of strongly polynomial matrix scaling, reducing the $O(n^{5} \\log(n/\\varepsilon))$ iteration bound of LSW to $O(n^{3} \\log(n/\\varepsilon))$. Additionally, we prove a novel bound on the size of approximate frame scaling solutions, involving the condition measure $\\bar{\\chi}$ studied in the linear programming literature, which may be of independent interest.","sentences":["The frame scaling problem is: given vectors $U := \\{u_{1}, ..., u_{n} \\} \\subseteq \\mathbb{R}^{d}$, marginals $c \\in \\mathbb{R}^{n}_{++}$, and precision $\\varepsilon > 0$, find left and right scalings $L \\in \\mathbb{R}^{d \\times d}, r \\in \\mathbb{R}^n$ such that $(v_1,\\dots,v_n) := (Lu_1 r_1,\\dots,Lu_nr_n)$ simultaneously satisfies $\\sum_{i=1}^n v_i v_i^{\\mathsf{T}} = I_d$ and $\\|v_{j}\\|_{2}^{2} = c_{j}, \\forall j \\in","[n]$, up to error $\\varepsilon$. This problem has appeared in a variety of fields throughout linear algebra and computer science.","In this work, we give a strongly polynomial algorithm for frame scaling with $\\log(1/\\varepsilon)$ convergence.","This answers a question of Diakonikolas, Tzamos and Kane (STOC 2023), who gave the first strongly polynomial randomized algorithm with poly$(1/\\varepsilon)$ convergence for the special case $c = \\frac{d}{n} 1_{n}$. Our algorithm is deterministic, applies for general $c \\in \\mathbb{R}^{n}_{++}$, and requires $O(n^{3} \\log(n/\\varepsilon))$ iterations as compared to $O(n^{5} d^{11}/\\varepsilon^{5})$ iterations of DTK.","By lifting the framework of Linial, Samorodnitsky and Wigderson (Combinatorica 2000) for matrix scaling to frames, we are able to simplify both the algorithm and analysis.","Our main technical contribution is to generalize the potential analysis of LSW to the frame setting and compute an update step in strongly polynomial time that achieves geometric progress in each iteration.","In fact, we can adapt our results to give an improved analysis of strongly polynomial matrix scaling, reducing the $O(n^{5} \\log(n/\\varepsilon))$ iteration bound of LSW to $O(n^{3} \\log(n/\\varepsilon))$. Additionally, we prove a novel bound on the size of approximate frame scaling solutions, involving the condition measure $\\bar{\\chi}$ studied in the linear programming literature, which may be of independent interest."],"url":"http://arxiv.org/abs/2402.04799v1","category":"cs.DS"}
{"created":"2024-02-07 12:36:54","title":"Mesh-based Gaussian Splatting for Real-time Large-scale Deformation","abstract":"Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion. Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology. To address this, we develop a novel GS-based method that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average).","sentences":["Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene.","Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion.","Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views.","However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology.","To address this, we develop a novel GS-based method that enables interactive deformation.","Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation.","3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians.","Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation.","Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh.","Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation.","Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average)."],"url":"http://arxiv.org/abs/2402.04796v1","category":"cs.GR"}
{"created":"2024-02-07 12:03:03","title":"Performance at maximum figure of merit for a Brownian Carnot refrigerator","abstract":"This paper focuses on the coefficient of performance (COP) at maximum figure of merit $\\chi$ for a Brownian Carnot-like refrigerator, within the context of symmetric Low-Dissipation approach. Our proposal is based on the Langevin equation for a Brownian particle bounded to a harmonic potential trap, which can perform Carnot-like cycles at finite time. We show that under quasistatic conditions the COP has the same expression as the macroscopic Carnot refrigerator. However, for irreversible cycles at finite time and under symmetric dissipation, the optimal COP is the counterpart of Curzon-Ahlborn efficiency for irreversible macroscopic refrigerators.","sentences":["This paper focuses on the coefficient of performance (COP) at maximum figure of merit $\\chi$ for a Brownian Carnot-like refrigerator, within the context of symmetric Low-Dissipation approach.","Our proposal is based on the Langevin equation for a Brownian particle bounded to a harmonic potential trap, which can perform Carnot-like cycles at finite time.","We show that under quasistatic conditions the COP has the same expression as the macroscopic Carnot refrigerator.","However, for irreversible cycles at finite time and under symmetric dissipation, the optimal COP is the counterpart of Curzon-Ahlborn efficiency for irreversible macroscopic refrigerators."],"url":"http://arxiv.org/abs/2402.04780v2","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 10:49:23","title":"Application-Layer FEC Scheme Configuration Optimization via Hybrid Simulated Annealing","abstract":"An optimization technique based on an adapted combination of simulated annealing (SA) and tabu search (TS) is presented. This method aims at finding near-optimal unequal error protection (UEP) application-layer FEC code configurations. This approach is intended to smartly protect audio and video transmission over IP networks when hard time restrictions apply. The considered code is a UEP version of the widely-used Pro-MPEG COP3 codes enabling the use of several matrices of dissimilar size and thus of unequal recovery capability. Finding the optimal configuration frequently requires the evaluation of a large solution space. So, to fulfill the imposed constraints, SA is adapted to the specifics of the scenario. In particular, the annealing schedule is conditioned by the real-time restrictions. Furthermore, solution neighborhood structures are determined by a proposed definition of distance between protection configurations, which, jointly with TS, conditions the selection of candidate solutions. Experimental results show a significantly improved performance of the optimization process, which invariably fulfills imposed timing constraints, at the expense of a very low distortion increase, when compared to using exhaustive search. These results allow the use of UEP Pro-MPEG COP3 codes for protecting video and audio transmission, which distinctly outperforms the standard code in a wide range of scenarios.","sentences":["An optimization technique based on an adapted combination of simulated annealing (SA) and tabu search (TS) is presented.","This method aims at finding near-optimal unequal error protection (UEP) application-layer FEC code configurations.","This approach is intended to smartly protect audio and video transmission over IP networks when hard time restrictions apply.","The considered code is a UEP version of the widely-used Pro-MPEG COP3 codes enabling the use of several matrices of dissimilar size and thus of unequal recovery capability.","Finding the optimal configuration frequently requires the evaluation of a large solution space.","So, to fulfill the imposed constraints, SA is adapted to the specifics of the scenario.","In particular, the annealing schedule is conditioned by the real-time restrictions.","Furthermore, solution neighborhood structures are determined by a proposed definition of distance between protection configurations, which, jointly with TS, conditions the selection of candidate solutions.","Experimental results show a significantly improved performance of the optimization process, which invariably fulfills imposed timing constraints, at the expense of a very low distortion increase, when compared to using exhaustive search.","These results allow the use of UEP Pro-MPEG COP3 codes for protecting video and audio transmission, which distinctly outperforms the standard code in a wide range of scenarios."],"url":"http://arxiv.org/abs/2402.04739v1","category":"eess.IV"}
{"created":"2024-02-07 10:41:14","title":"Review of Cetacean's click detection algorithms","abstract":"The detection of echolocation clicks is key in understanding the intricate behaviors of cetaceans and monitoring their populations. Cetacean species relying on clicks for navigation, foraging and even communications are sperm whales (Physeter macrocephalus) and a variety of dolphin groups. Echolocation clicks are wideband signals of short duration that are often emitted in sequences of varying inter-click-intervals. While datasets and models for clicks exist, the detection and classification of clicks present a significant challenge, mostly due to the diversity of clicks' structures, overlapping signals from simultaneously emitting animals, and the abundance of noise transients from, for example, snapping shrimps and shipping cavitation noise. This paper provides a survey of the many detection and classification methodologies of clicks, ranging from 2002 to 2023. We divide the surveyed techniques into categories by their methodology. Specifically, feature analysis (e.g., phase, ICI and duration), frequency content, energy based detection, supervised and unsupervised machine learning, template matching and adaptive detection approaches. Also surveyed are open access platforms for click detections, and databases openly available for testing. Details of the method applied for each paper are given along with advantages and limitations, and for each category we analyze the remaining challenges. The paper also includes a performance comparison for several schemes over a shared database. Finally, we provide tables summarizing the existing detection schemes in terms of challenges address, methods, detection and classification tools applied, features used and applications.","sentences":["The detection of echolocation clicks is key in understanding the intricate behaviors of cetaceans and monitoring their populations.","Cetacean species relying on clicks for navigation, foraging and even communications are sperm whales (Physeter macrocephalus) and a variety of dolphin groups.","Echolocation clicks are wideband signals of short duration that are often emitted in sequences of varying inter-click-intervals.","While datasets and models for clicks exist, the detection and classification of clicks present a significant challenge, mostly due to the diversity of clicks' structures, overlapping signals from simultaneously emitting animals, and the abundance of noise transients from, for example, snapping shrimps and shipping cavitation noise.","This paper provides a survey of the many detection and classification methodologies of clicks, ranging from 2002 to 2023.","We divide the surveyed techniques into categories by their methodology.","Specifically, feature analysis (e.g., phase, ICI and duration), frequency content, energy based detection, supervised and unsupervised machine learning, template matching and adaptive detection approaches.","Also surveyed are open access platforms for click detections, and databases openly available for testing.","Details of the method applied for each paper are given along with advantages and limitations, and for each category we analyze the remaining challenges.","The paper also includes a performance comparison for several schemes over a shared database.","Finally, we provide tables summarizing the existing detection schemes in terms of challenges address, methods, detection and classification tools applied, features used and applications."],"url":"http://arxiv.org/abs/2402.04735v1","category":"cs.SD"}
{"created":"2024-02-07 10:35:31","title":"The Galactic center excess at the highest energies: morphology and photon-count statistics","abstract":"The nature of the GeV gamma-ray Galactic center excess (GCE) in the data of Fermi-Large Area Telescope (LAT) is still to be unveiled. We present a new analysis of the inner Galaxy Fermi-LAT data at energies above 10 GeV, based on an innovative method which combines the skyFACT adaptive template fitting with and the 1pPDF pixel-count statistics. We find a strong evidence for the GCE also at high energies, $\\sigma > 5$ regardless of the GCE spatial template. Remarkably, our fits prefer the bulge morphological model over the dark matter one at high significance, and show no evidence for an additional dark matter template on top of the bulge component. Through the 1pPDF analysis, we find that the model best describing the gamma-ray data requires a smooth, diffuse GCE following a bulge morphology, together with sub-threshold point sources. The 1pPDF fit reconstructs a consistent population of faint point sources down at least to $10^{-12}$ ph cm$^{-2}$ s$^{-1}$. Between $10^{-12}$ ph cm$^{-2}$ s$^{-1}$ and $10^{-11}$ ph cm$^{-2}$ s$^{-1}$ the 1pPDF measures a number of point sources significantly higher than the ones in the Fermi 4FGL catalog. The robustness of our results brings further support to the attempt of explaining, at least partially, the high-energy tail of the GCE in terms of a population of point sources, likely corresponding to millisecond pulsars.","sentences":["The nature of the GeV gamma-ray Galactic center excess (GCE) in the data of Fermi-Large Area Telescope (LAT) is still to be unveiled.","We present a new analysis of the inner Galaxy Fermi-LAT data at energies above 10 GeV, based on an innovative method which combines the skyFACT adaptive template fitting with and the 1pPDF pixel-count statistics.","We find a strong evidence for the GCE also at high energies, $\\sigma > 5$ regardless of the GCE spatial template.","Remarkably, our fits prefer the bulge morphological model over the dark matter one at high significance, and show no evidence for an additional dark matter template on top of the bulge component.","Through the 1pPDF analysis, we find that the model best describing the gamma-ray data requires a smooth, diffuse GCE following a bulge morphology, together with sub-threshold point sources.","The 1pPDF fit reconstructs a consistent population of faint point sources down at least to $10^{-12}$ ph cm$^{-2}$ s$^{-1}$. Between $10^{-12}$ ph cm$^{-2}$ s$^{-1}$ and $10^{-11}$ ph cm$^{-2}$ s$^{-1}$ the 1pPDF measures a number of point sources significantly higher than the ones in the Fermi 4FGL catalog.","The robustness of our results brings further support to the attempt of explaining, at least partially, the high-energy tail of the GCE in terms of a population of point sources, likely corresponding to millisecond pulsars."],"url":"http://arxiv.org/abs/2402.04733v1","category":"astro-ph.HE"}
{"created":"2024-02-07 10:32:40","title":"Model Predictive Trajectory Optimization With Dynamically Changing Waypoints for Serial Manipulators","abstract":"Systematically including dynamically changing waypoints as desired discrete actions, for instance, resulting from superordinate task planning, has been challenging for online model predictive trajectory optimization with short planning horizons. This paper presents a novel waypoint model predictive control (wMPC) concept for online replanning tasks. The main idea is to split the planning horizon at the waypoint when it becomes reachable within the current planning horizon and reduce the horizon length towards the waypoints and goal points. This approach keeps the computational load low and provides flexibility in adapting to changing conditions in real time. The presented approach achieves competitive path lengths and trajectory durations compared to (global) offline RRT-type planners in a multi-waypoint scenario. Moreover, the ability of wMPC to dynamically replan tasks online is experimentally demonstrated on a KUKA LBR iiwa 14 R820 robot in a dynamic pick-and-place scenario.","sentences":["Systematically including dynamically changing waypoints as desired discrete actions, for instance, resulting from superordinate task planning, has been challenging for online model predictive trajectory optimization with short planning horizons.","This paper presents a novel waypoint model predictive control (wMPC) concept for online replanning tasks.","The main idea is to split the planning horizon at the waypoint when it becomes reachable within the current planning horizon and reduce the horizon length towards the waypoints and goal points.","This approach keeps the computational load low and provides flexibility in adapting to changing conditions in real time.","The presented approach achieves competitive path lengths and trajectory durations compared to (global) offline RRT-type planners in a multi-waypoint scenario.","Moreover, the ability of wMPC to dynamically replan tasks online is experimentally demonstrated on a KUKA LBR iiwa 14 R820 robot in a dynamic pick-and-place scenario."],"url":"http://arxiv.org/abs/2402.04730v1","category":"cs.RO"}
{"created":"2024-02-07 10:12:46","title":"Investigating Driving Interactions: A Robust Multi-Agent Simulation Framework for Autonomous Vehicles","abstract":"Current validation methods often rely on recorded data and basic functional checks, which may not be sufficient to encompass the scenarios an autonomous vehicle might encounter. In addition, there is a growing need for complex scenarios with changing vehicle interactions for comprehensive validation. This work introduces a novel synchronous multi-agent simulation framework for autonomous vehicles in interactive scenarios. Our approach creates an interactive scenario and incorporates publicly available edge-case scenarios wherein simulated vehicles are replaced by agents navigating to predefined destinations. We provide a platform that enables the integration of different autonomous driving planning methodologies and includes a set of evaluation metrics to assess autonomous driving behavior. Our study explores different planning setups and adjusts simulation complexity to test the framework's adaptability and performance. Results highlight the critical role of simulating vehicle interactions to enhance autonomous driving systems. Our setup offers unique insights for developing advanced algorithms for complex driving tasks to accelerate future investigations and developments in this field. The multi-agent simulation framework is available as open-source software: https://github.com/TUM-AVS/Frenetix-Motion-Planner","sentences":["Current validation methods often rely on recorded data and basic functional checks, which may not be sufficient to encompass the scenarios an autonomous vehicle might encounter.","In addition, there is a growing need for complex scenarios with changing vehicle interactions for comprehensive validation.","This work introduces a novel synchronous multi-agent simulation framework for autonomous vehicles in interactive scenarios.","Our approach creates an interactive scenario and incorporates publicly available edge-case scenarios wherein simulated vehicles are replaced by agents navigating to predefined destinations.","We provide a platform that enables the integration of different autonomous driving planning methodologies and includes a set of evaluation metrics to assess autonomous driving behavior.","Our study explores different planning setups and adjusts simulation complexity to test the framework's adaptability and performance.","Results highlight the critical role of simulating vehicle interactions to enhance autonomous driving systems.","Our setup offers unique insights for developing advanced algorithms for complex driving tasks to accelerate future investigations and developments in this field.","The multi-agent simulation framework is available as open-source software:","https://github.com/TUM-AVS/Frenetix-Motion-Planner"],"url":"http://arxiv.org/abs/2402.04720v1","category":"cs.RO"}
{"created":"2024-02-07 10:10:21","title":"Quantum Theory of Spin-Transfer and Spin-Pumping in Collinear Antiferromagnets and Ferrimagnets","abstract":"Antiferromagnets are promising candidates as active components in spintronic applications. They share features with ferrimagnets in that opposing spin orientations exist in two or more sublattices. Spin transfer torque and spin pumping are essential ingredients in antiferromagnetic and ferrimagnet spintronics. This paper develops an out-of-equilibrium quantum theory of the spin dynamics of collinear magnets containing many spins coupled to normal metal reservoirs. At equilibrium, the spins are parallel or antiparallel to the easy axis. The theory, therefore, covers collinear antiferromagnets and ferrimagnets. We focus on the resulting semi-classical spin dynamics. The dissipation in the spin dynamics is enhanced due to spin-pumping. Spin accumulations in the normal metals induce deterministic spin-transfer torques on the magnet. Additionally, each electron's discrete spin angular momentum causes stochastic fluctuating torques on the antiferromagnet or ferrimagnet. We derive these fluctuating torques. The fluctuation-dissipation theorem holds at high temperatures, including the effects of spin-pumping. At low temperatures, we derive shot noise contributions to the fluctuations.","sentences":["Antiferromagnets are promising candidates as active components in spintronic applications.","They share features with ferrimagnets in that opposing spin orientations exist in two or more sublattices.","Spin transfer torque and spin pumping are essential ingredients in antiferromagnetic and ferrimagnet spintronics.","This paper develops an out-of-equilibrium quantum theory of the spin dynamics of collinear magnets containing many spins coupled to normal metal reservoirs.","At equilibrium, the spins are parallel or antiparallel to the easy axis.","The theory, therefore, covers collinear antiferromagnets and ferrimagnets.","We focus on the resulting semi-classical spin dynamics.","The dissipation in the spin dynamics is enhanced due to spin-pumping.","Spin accumulations in the normal metals induce deterministic spin-transfer torques on the magnet.","Additionally, each electron's discrete spin angular momentum causes stochastic fluctuating torques on the antiferromagnet or ferrimagnet.","We derive these fluctuating torques.","The fluctuation-dissipation theorem holds at high temperatures, including the effects of spin-pumping.","At low temperatures, we derive shot noise contributions to the fluctuations."],"url":"http://arxiv.org/abs/2402.04719v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 10:10:06","title":"Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying","abstract":"This paper investigates the efficiency of nonsingular fast terminal sliding mode and adaptive smooth control method for the distributed space telescope demonstration mission. The distributed space telescope has a flexible focal length that corresponds to the relative position of the formation flying concept. The precise formation flying technology by CubeSats enhances the utility of distributed space systems with low costs. The propulsion systems for CubeSats usually have restricted degrees of freedom. Since the scientific mission requires continuous orbit control, the attitude and orbit control system mutually affect the control performance. The nonsingular fast terminal sliding mode has the advantage of a fast convergence rate and is able to improve the control performance. The adaptive smooth controller designed for the SISO system is expanded and applied to the attitude and orbit control system. The simulation results verify the efficiency of the adaptive smooth controller based on the nonsingular fast terminal sliding mode.","sentences":["This paper investigates the efficiency of nonsingular fast terminal sliding mode and adaptive smooth control method for the distributed space telescope demonstration mission.","The distributed space telescope has a flexible focal length that corresponds to the relative position of the formation flying concept.","The precise formation flying technology by CubeSats enhances the utility of distributed space systems with low costs.","The propulsion systems for CubeSats usually have restricted degrees of freedom.","Since the scientific mission requires continuous orbit control, the attitude and orbit control system mutually affect the control performance.","The nonsingular fast terminal sliding mode has the advantage of a fast convergence rate and is able to improve the control performance.","The adaptive smooth controller designed for the SISO system is expanded and applied to the attitude and orbit control system.","The simulation results verify the efficiency of the adaptive smooth controller based on the nonsingular fast terminal sliding mode."],"url":"http://arxiv.org/abs/2402.04718v1","category":"eess.SY"}
{"created":"2024-02-07 10:05:42","title":"Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search","abstract":"We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS). We introduce novel concepts: $b\\textit{-monotonic path}$ and $B\\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET. We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work. Empirically, we validate the method's effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with out-of-distribution data and hard instances. Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications.","sentences":["We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS).","We introduce novel concepts: $b\\textit{-monotonic path}$ and $B\\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET.","We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work.","Empirically, we validate the method's effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with out-of-distribution data and hard instances.","Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications."],"url":"http://arxiv.org/abs/2402.04713v1","category":"cs.IR"}
{"created":"2024-02-07 09:58:52","title":"High-dimensional multidisciplinary design optimization for aircraft eco-design / Optimisation multi-disciplinaire en grande dimension pour l'\u00e9co-conception avion en avant-projet","abstract":"The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the \"DRAGON\" hybrid airplane to reduce their climate impact.","sentences":["The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables).","The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models.","EGO is a heuristic BO method that performs well in terms of solution quality.","However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases.","For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out.","The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space.","In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables.","Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the \"DRAGON\" hybrid airplane to reduce their climate impact."],"url":"http://arxiv.org/abs/2402.04711v1","category":"math.OC"}
{"created":"2024-02-07 09:50:00","title":"Decoherence Rate in Random Lindblad Dynamics","abstract":"Open quantum systems undergo decoherence, responsible for the transition from quantum to classical behavior. The time scale in which decoherence takes place can be analyzed using upper limits to its rate. We examine the dynamics of open chaotic quantum systems governed by random Lindblad operators, sourced from Gaussian and Ginibre ensembles with Wigner-Dyson symmetry classes. In these systems, the ensemble-averaged purity decays monotonically as function of time. This decay is governed by the decoherence rate, which is upper bounded by the dimension of their Hilbert space, and is independent of the ensemble symmetry. These findings hold upon mixing different ensembles, indicating the universal character of the decoherence rate limit. Moreover, our findings reveal that open chaotic quantum systems, governed by random Lindbladians, inherently tend to exhibit the most rapid decoherence, regardless of the initial state. This phenomenon is associated with the concentration of the decoherence rate near its upper bound. Our work identifies primary features of decoherence in dissipative quantum chaos, with applications ranging from quantum foundations to high-energy physics and quantum technologies.","sentences":["Open quantum systems undergo decoherence, responsible for the transition from quantum to classical behavior.","The time scale in which decoherence takes place can be analyzed using upper limits to its rate.","We examine the dynamics of open chaotic quantum systems governed by random Lindblad operators, sourced from Gaussian and Ginibre ensembles with Wigner-Dyson symmetry classes.","In these systems, the ensemble-averaged purity decays monotonically as function of time.","This decay is governed by the decoherence rate, which is upper bounded by the dimension of their Hilbert space, and is independent of the ensemble symmetry.","These findings hold upon mixing different ensembles, indicating the universal character of the decoherence rate limit.","Moreover, our findings reveal that open chaotic quantum systems, governed by random Lindbladians, inherently tend to exhibit the most rapid decoherence, regardless of the initial state.","This phenomenon is associated with the concentration of the decoherence rate near its upper bound.","Our work identifies primary features of decoherence in dissipative quantum chaos, with applications ranging from quantum foundations to high-energy physics and quantum technologies."],"url":"http://arxiv.org/abs/2402.04705v1","category":"quant-ph"}
{"created":"2024-02-07 09:30:15","title":"Near-equilibrium growth of vapor bubbles","abstract":"This study delves into the near-equilibrium dynamics of vapor bubbles. Utilizing a regular perturbation method, we derive analytical solutions to capture the bubble's initial growth stages, wherein surface tension and viscous dissipation forces impede the bubble's growth. The accuracy of these solutions is validated with numeric solutions of the complete Rayleigh-Plesset equation. Although limited to near-critical bubble radii, our analytical solutions predict the initial delay period as a function of two nondimensional parameters, signifying the initial deviation from the critical radius and the surface tension to viscosity ratio. We found a transition in the dominant delay mechanism, marking a shift from surface tension to viscous damping dominance. Our findings emphasize the significance of considering the surface tension delay, particularly for short timescales, and highlight its crucial role in accurately modeling the bubble's initial growth dynamics. The derived analytical solutions and the obtained correlation for surface-tension-induced delay may prove a practical tool and could be integrated into existing models of vapor bubble growth.","sentences":["This study delves into the near-equilibrium dynamics of vapor bubbles.","Utilizing a regular perturbation method, we derive analytical solutions to capture the bubble's initial growth stages, wherein surface tension and viscous dissipation forces impede the bubble's growth.","The accuracy of these solutions is validated with numeric solutions of the complete Rayleigh-Plesset equation.","Although limited to near-critical bubble radii, our analytical solutions predict the initial delay period as a function of two nondimensional parameters, signifying the initial deviation from the critical radius and the surface tension to viscosity ratio.","We found a transition in the dominant delay mechanism, marking a shift from surface tension to viscous damping dominance.","Our findings emphasize the significance of considering the surface tension delay, particularly for short timescales, and highlight its crucial role in accurately modeling the bubble's initial growth dynamics.","The derived analytical solutions and the obtained correlation for surface-tension-induced delay may prove a practical tool and could be integrated into existing models of vapor bubble growth."],"url":"http://arxiv.org/abs/2402.04690v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 08:59:28","title":"Streamlined Hybrid Annotation Framework using Scalable Codestream for Bandwidth-Restricted UAV Object Detection","abstract":"Emergency response missions depend on the fast relay of visual information, a task to which unmanned aerial vehicles are well adapted. However, the effective use of unmanned aerial vehicles is often compromised by bandwidth limitations that impede fast data transmission, thereby delaying the quick decision-making necessary in emergency situations. To address these challenges, this paper presents a streamlined hybrid annotation framework that utilizes the JPEG 2000 compression algorithm to facilitate object detection under limited bandwidth. The proposed framework employs a fine-tuned deep learning network for initial image annotation at lower resolutions and uses JPEG 2000's scalable codestream to selectively enhance the image resolution in critical areas that require human expert annotation. We show that our proposed hybrid framework reduces the response time by a factor of 34 in emergency situations compared to a baseline approach.","sentences":["Emergency response missions depend on the fast relay of visual information, a task to which unmanned aerial vehicles are well adapted.","However, the effective use of unmanned aerial vehicles is often compromised by bandwidth limitations that impede fast data transmission, thereby delaying the quick decision-making necessary in emergency situations.","To address these challenges, this paper presents a streamlined hybrid annotation framework that utilizes the JPEG 2000 compression algorithm to facilitate object detection under limited bandwidth.","The proposed framework employs a fine-tuned deep learning network for initial image annotation at lower resolutions and uses JPEG 2000's scalable codestream to selectively enhance the image resolution in critical areas that require human expert annotation.","We show that our proposed hybrid framework reduces the response time by a factor of 34 in emergency situations compared to a baseline approach."],"url":"http://arxiv.org/abs/2402.04673v1","category":"eess.IV"}
{"created":"2024-02-07 08:42:48","title":"Open-Vocabulary Calibration for Vision-Language Models","abstract":"Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed.","sentences":["Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few.","In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning.","However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world.","This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting.","To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes.","The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed."],"url":"http://arxiv.org/abs/2402.04655v1","category":"cs.LG"}
{"created":"2024-02-07 08:18:09","title":"Latent Plan Transformer: Planning as Latent Variable Inference","abstract":"In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories. It achieves competitive performance across several benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting capabilities of nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. These results validate that latent variable inference can be a strong alternative to step-wise reward prompting.","sentences":["In tasks aiming for long-term returns, planning becomes necessary.","We study generative modeling for planning with datasets repurposed from offline reinforcement learning.","Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge.","We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return.","LPT can be learned with maximum likelihood estimation on trajectory-return pairs.","In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context.","During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference.","It then guides the autoregressive policy throughout the episode, functioning as a plan.","Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories.","It achieves competitive performance across several benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting capabilities of nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies.","These results validate that latent variable inference can be a strong alternative to step-wise reward prompting."],"url":"http://arxiv.org/abs/2402.04647v1","category":"cs.LG"}
{"created":"2024-02-07 08:18:06","title":"Learning with Diversification from Block Sparse Signal","abstract":"This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.","sentences":["This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data.","By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting.","Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation.","Moreover, we establish the global and local optimality theory of our model.","Experiments validate the advantages of DivSBL over existing algorithms."],"url":"http://arxiv.org/abs/2402.04646v1","category":"cs.LG"}
{"created":"2024-02-07 08:16:40","title":"LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views","abstract":"Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a novel generalizable fine-tuning method LEVI, where the pre-trained model is adaptively ensembled layer-wise with a small task-specific model, while preserving training and inference efficiencies. By combining two complementing models, LEVI effectively suppresses problematic features in both the fine-tuning data and pre-trained model and preserves useful features for new tasks. Broad experiments with large language and vision models show that LEVI greatly improves fine-tuning generalization via emphasizing different views from fine-tuning data and pre-trained features.","sentences":["Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks.","While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD).","To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data.","However, potential limitations in the pre-training data and models are often ignored.","In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization.","It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data.","To address the issues in both pre-training and fine-tuning data, we propose a novel generalizable fine-tuning method LEVI, where the pre-trained model is adaptively ensembled layer-wise with a small task-specific model, while preserving training and inference efficiencies.","By combining two complementing models, LEVI effectively suppresses problematic features in both the fine-tuning data and pre-trained model and preserves useful features for new tasks.","Broad experiments with large language and vision models show that LEVI greatly improves fine-tuning generalization via emphasizing different views from fine-tuning data and pre-trained features."],"url":"http://arxiv.org/abs/2402.04644v1","category":"cs.LG"}
{"created":"2024-02-07 07:16:12","title":"Noise Map Guidance: Inversion with Spatial Context for Real Image Editing","abstract":"Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images. However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity. Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization. Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing. Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality. Our empirical investigations highlight NMG's adaptability across various editing techniques and its robustness to variants of DDIM inversions.","sentences":["Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images.","However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity.","Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization.","Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing.","Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality.","Our empirical investigations highlight NMG's adaptability across various editing techniques and its robustness to variants of DDIM inversions."],"url":"http://arxiv.org/abs/2402.04625v1","category":"cs.CV"}
{"created":"2024-02-07 07:01:08","title":"Multi-Scale Semantic Segmentation with Modified MBConv Blocks","abstract":"Recently, MBConv blocks, initially designed for efficiency in resource-limited settings and later adapted for cutting-edge image classification performances, have demonstrated significant potential in image classification tasks. Despite their success, their application in semantic segmentation has remained relatively unexplored. This paper introduces a novel adaptation of MBConv blocks specifically tailored for semantic segmentation. Our modification stems from the insight that semantic segmentation requires the extraction of more detailed spatial information than image classification. We argue that to effectively perform multi-scale semantic segmentation, each branch of a U-Net architecture, regardless of its resolution, should possess equivalent segmentation capabilities. By implementing these changes, our approach achieves impressive mean Intersection over Union (IoU) scores of 84.5% and 84.0% on the Cityscapes test and validation datasets, respectively, demonstrating the efficacy of our proposed modifications in enhancing semantic segmentation performance.","sentences":["Recently, MBConv blocks, initially designed for efficiency in resource-limited settings and later adapted for cutting-edge image classification performances, have demonstrated significant potential in image classification tasks.","Despite their success, their application in semantic segmentation has remained relatively unexplored.","This paper introduces a novel adaptation of MBConv blocks specifically tailored for semantic segmentation.","Our modification stems from the insight that semantic segmentation requires the extraction of more detailed spatial information than image classification.","We argue that to effectively perform multi-scale semantic segmentation, each branch of a U-Net architecture, regardless of its resolution, should possess equivalent segmentation capabilities.","By implementing these changes, our approach achieves impressive mean Intersection over Union (IoU) scores of 84.5% and 84.0% on the Cityscapes test and validation datasets, respectively, demonstrating the efficacy of our proposed modifications in enhancing semantic segmentation performance."],"url":"http://arxiv.org/abs/2402.04618v1","category":"cs.CV"}
{"created":"2024-02-07 05:56:54","title":"Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector","abstract":"Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection. Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance.","sentences":["Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs).","While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs.","In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs.","Our method first trains a correction model to generate an initial correction of the source sentence.","Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection.","Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction.","Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection.","Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance."],"url":"http://arxiv.org/abs/2402.04601v1","category":"cs.CL"}
{"created":"2024-02-07 05:43:57","title":"CMSA algorithm for solving the prioritized pairwise test data generation problem in software product lines","abstract":"In Software Product Lines (SPLs) it may be difficult or even impossible to test all the products of the family because of the large number of valid feature combinations that may exist. Thus, we want to find a minimal subset of the product family that allows us to test all these possible combinations (pairwise). Furthermore, when testing a single product is a great effort, it is desirable to first test products composed of a set of priority features. This problem is called Prioritized Pairwise Test Data Generation Problem.   State-of-the-art algorithms based on Integer Linear Programming for this problema are faster enough for small and medium instances. However, there exists some real instances that are too large to be computed with these algorithms in a reasonable time because of the exponential growth of the number of candidate solutions. Also, these heuristics not always lead us to the best solutions. In this work we propose a new approach based on a hybrid metaheuristic algorithm called Construct, Merge, Solve & Adapt. We compare this matheuristic with four algorithms: a Hybrid algorithm based on Integer Linear Programming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming (HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm called prioritized-ICPL. The analysis reveals that CMSA results in statistically significantly better quality solutions in most instances and for most levels of weighted coverage, although it requires more execution time.","sentences":["In Software Product Lines (SPLs) it may be difficult or even impossible to test all the products of the family because of the large number of valid feature combinations that may exist.","Thus, we want to find a minimal subset of the product family that allows us to test all these possible combinations (pairwise).","Furthermore, when testing a single product is a great effort, it is desirable to first test products composed of a set of priority features.","This problem is called Prioritized Pairwise Test Data Generation Problem.   ","State-of-the-art algorithms based on Integer Linear Programming for this problema are faster enough for small and medium instances.","However, there exists some real instances that are too large to be computed with these algorithms in a reasonable time because of the exponential growth of the number of candidate solutions.","Also, these heuristics not always lead us to the best solutions.","In this work we propose a new approach based on a hybrid metaheuristic algorithm called Construct, Merge, Solve & Adapt.","We compare this matheuristic with four algorithms: a Hybrid algorithm based on Integer Linear Programming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming (HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm called prioritized-ICPL.","The analysis reveals that CMSA results in statistically significantly better quality solutions in most instances and for most levels of weighted coverage, although it requires more execution time."],"url":"http://arxiv.org/abs/2402.04597v1","category":"cs.AI"}
{"created":"2024-02-07 05:36:06","title":"Ransomware Detection Dynamics: Insights and Implications","abstract":"The rise of ransomware attacks has necessitated the development of effective strategies for identifying and mitigating these threats. This research investigates the utilization of a feature selection algorithm for distinguishing ransomware-related and benign transactions in both Bitcoin (BTC) and United States Dollar (USD). Leveraging the UGRansome dataset, a comprehensive repository of ransomware related BTC and USD transactions, we propose a set of novel features designed to capture the distinct characteristics of ransomware activity within the cryptocurrency ecosystem. These features encompass transaction metadata, ransom analysis, and behavioral patterns, offering a multifaceted view of ransomware-related financial transactions. Through rigorous experimentation and evaluation, we demonstrate the effectiveness of our feature set in accurately extracting BTC and USD transactions, thereby aiding in the early detection and prevention of ransomware-related financial flows. We introduce a Ransomware Feature Selection Algorithm (RFSA) based on Gini Impurity and Mutual Information (MI) for selecting crucial ransomware features from the UGRansome dataset. Insights from the visualization highlight the potential of Gini Impurity and MI-based feature selection to enhance ransomware detection systems by effectively discriminating between ransomware classes. The analysis reveals that approximately 68% of ransomware incidents involve BTC transactions within the range of 1.46 to 2.56, with an average of 2.01 BTC transactions per attack. The findings emphasize the dynamic and adaptable nature of ransomware demands, suggesting that there is no fixed amount for specific cyberattacks, highlighting the evolving landscape of ransomware threats.","sentences":["The rise of ransomware attacks has necessitated the development of effective strategies for identifying and mitigating these threats.","This research investigates the utilization of a feature selection algorithm for distinguishing ransomware-related and benign transactions in both Bitcoin (BTC) and United States Dollar (USD).","Leveraging the UGRansome dataset, a comprehensive repository of ransomware related BTC and USD transactions, we propose a set of novel features designed to capture the distinct characteristics of ransomware activity within the cryptocurrency ecosystem.","These features encompass transaction metadata, ransom analysis, and behavioral patterns, offering a multifaceted view of ransomware-related financial transactions.","Through rigorous experimentation and evaluation, we demonstrate the effectiveness of our feature set in accurately extracting BTC and USD transactions, thereby aiding in the early detection and prevention of ransomware-related financial flows.","We introduce a Ransomware Feature Selection Algorithm (RFSA) based on Gini Impurity and Mutual Information (MI) for selecting crucial ransomware features from the UGRansome dataset.","Insights from the visualization highlight the potential of Gini Impurity and MI-based feature selection to enhance ransomware detection systems by effectively discriminating between ransomware classes.","The analysis reveals that approximately 68% of ransomware incidents involve BTC transactions within the range of 1.46 to 2.56, with an average of 2.01 BTC transactions per attack.","The findings emphasize the dynamic and adaptable nature of ransomware demands, suggesting that there is no fixed amount for specific cyberattacks, highlighting the evolving landscape of ransomware threats."],"url":"http://arxiv.org/abs/2402.04594v1","category":"cs.CR"}
{"created":"2024-02-07 04:44:41","title":"Boosting Reinforcement Learning Algorithms in Continuous Robotic Reaching Tasks using Adaptive Potential Functions","abstract":"In reinforcement learning, reward shaping is an efficient way to guide the learning process of an agent, as the reward can indicate the optimal policy of the task. The potential-based reward shaping framework was proposed to guarantee policy invariance after reward shaping, where a potential function is used to calculate the shaping reward. In former work, we proposed a novel adaptive potential function (APF) method to learn the potential function concurrently with training the agent based on information collected by the agent during the training process, and examined the APF method in discrete action space scenarios. This paper investigates the feasibility of using APF in solving continuous-reaching tasks in a real-world robotic scenario with continuous action space. We combine the Deep Deterministic Policy Gradient (DDPG) algorithm and our proposed method to form a new algorithm called APF-DDPG. To compare APF-DDPG with DDPG, we designed a task where the agent learns to control Baxter's right arm to reach a goal position. The experimental results show that the APF-DDPG algorithm outperforms the DDPG algorithm on both learning speed and robustness.","sentences":["In reinforcement learning, reward shaping is an efficient way to guide the learning process of an agent, as the reward can indicate the optimal policy of the task.","The potential-based reward shaping framework was proposed to guarantee policy invariance after reward shaping, where a potential function is used to calculate the shaping reward.","In former work, we proposed a novel adaptive potential function (APF) method to learn the potential function concurrently with training the agent based on information collected by the agent during the training process, and examined the APF method in discrete action space scenarios.","This paper investigates the feasibility of using APF in solving continuous-reaching tasks in a real-world robotic scenario with continuous action space.","We combine the Deep Deterministic Policy Gradient (DDPG) algorithm and our proposed method to form a new algorithm called APF-DDPG.","To compare APF-DDPG with DDPG, we designed a task where the agent learns to control Baxter's right arm to reach a goal position.","The experimental results show that the APF-DDPG algorithm outperforms the DDPG algorithm on both learning speed and robustness."],"url":"http://arxiv.org/abs/2402.04581v1","category":"cs.RO"}
{"created":"2024-02-07 04:11:25","title":"Progressive Conservative Adaptation for Evolving Target Domains","abstract":"Conventional domain adaptation typically transfers knowledge from a source domain to a stationary target domain. However, in many real-world cases, target data usually emerge sequentially and have continuously evolving distributions. Restoring and adapting to such target data results in escalating computational and resource consumption over time. Hence, it is vital to devise algorithms to address the evolving domain adaptation (EDA) problem, \\emph{i.e.,} adapting models to evolving target domains without access to historic target domains. To achieve this goal, we propose a simple yet effective approach, termed progressive conservative adaptation (PCAda). To manage new target data that diverges from previous distributions, we fine-tune the classifier head based on the progressively updated class prototypes. Moreover, as adjusting to the most recent target domain can interfere with the features learned from previous target domains, we develop a conservative sparse attention mechanism. This mechanism restricts feature adaptation within essential dimensions, thus easing the inference related to historical knowledge. The proposed PCAda is implemented with a meta-learning framework, which achieves the fast adaptation of the classifier with the help of the progressively updated class prototypes in the inner loop and learns a generalized feature without severely interfering with the historic knowledge via the conservative sparse attention in the outer loop. Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate the effectiveness of our method.","sentences":["Conventional domain adaptation typically transfers knowledge from a source domain to a stationary target domain.","However, in many real-world cases, target data usually emerge sequentially and have continuously evolving distributions.","Restoring and adapting to such target data results in escalating computational and resource consumption over time.","Hence, it is vital to devise algorithms to address the evolving domain adaptation (EDA) problem, \\emph{i.e.,} adapting models to evolving target domains without access to historic target domains.","To achieve this goal, we propose a simple yet effective approach, termed progressive conservative adaptation (PCAda).","To manage new target data that diverges from previous distributions, we fine-tune the classifier head based on the progressively updated class prototypes.","Moreover, as adjusting to the most recent target domain can interfere with the features learned from previous target domains, we develop a conservative sparse attention mechanism.","This mechanism restricts feature adaptation within essential dimensions, thus easing the inference related to historical knowledge.","The proposed PCAda is implemented with a meta-learning framework, which achieves the fast adaptation of the classifier with the help of the progressively updated class prototypes in the inner loop and learns a generalized feature without severely interfering with the historic knowledge via the conservative sparse attention in the outer loop.","Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.04573v1","category":"cs.CV"}
{"created":"2024-02-07 04:06:53","title":"OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences","abstract":"Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about agents' behavioural data, from which we derive two features for anomaly detection. The intuition behind our method is that the action optimality feature derived from the Q function can differentiate the optimal action from others at each local state, and the sequential association feature derived from the state value function has the potential to maintain the temporal correlations between decisions (state-action pairs). Our experiments show that OIL-AD can achieve outstanding online anomaly detection performance with up to 34.8% improvement in F1 score over comparable baselines.","sentences":["Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task.","Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment.","To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association.","Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories.","We propose that the Q function and the state value function can provide sufficient information about agents' behavioural data, from which we derive two features for anomaly detection.","The intuition behind our method is that the action optimality feature derived from the Q function can differentiate the optimal action from others at each local state, and the sequential association feature derived from the state value function has the potential to maintain the temporal correlations between decisions (state-action pairs).","Our experiments show that OIL-AD can achieve outstanding online anomaly detection performance with up to 34.8% improvement in F1 score over comparable baselines."],"url":"http://arxiv.org/abs/2402.04567v1","category":"cs.LG"}
{"created":"2024-02-07 03:18:34","title":"BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial Imagery","abstract":"In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality.","sentences":["In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery.","Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models.","(2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs.","(3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity.","Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene.","This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments.","Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process.","Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results.","We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality."],"url":"http://arxiv.org/abs/2402.04554v1","category":"cs.CV"}
{"created":"2024-02-07 02:06:51","title":"Understanding multiple timescales in quantum dissipative dynamics: Insights from quantum trajectories","abstract":"Open quantum systems with nearly degenerate energy levels have been shown to exhibit long-lived metastable states in the approach to equilibrium, even when modelled with certain Lindblad-form quantum master equations. This is a result of dramatic separation of timescales due to differences between Liouvillian eigenvalues. These metastable states often have nonzero coherences which die off only in the long time limit once the system reaches thermal equilibrium. We examine two distinct situations that give rise to this effect: one in which dissipative dynamics couple together states only within a nearly degenerate subspace, and one in which they give rise to jumps over finite energy splittings, between separate nearly degenerate subspaces. We find, in each case, that a change of basis can often lead to a representation which more naturally captures the impact of the system-bath interaction than does the energy eigenbasis, revealing that separate timescales are associated with separate processes (e.g. decoherence into a non-energy eigenbasis, decay of population correlations to the initial state). This approach is paired with the inspection of quantum trajectories, which further provide intuition as to how open system evolution is characterized when coherent oscillations, thermal relaxation, and decoherence all occur simultaneously.","sentences":["Open quantum systems with nearly degenerate energy levels have been shown to exhibit long-lived metastable states in the approach to equilibrium, even when modelled with certain Lindblad-form quantum master equations.","This is a result of dramatic separation of timescales due to differences between Liouvillian eigenvalues.","These metastable states often have nonzero coherences which die off only in the long time limit once the system reaches thermal equilibrium.","We examine two distinct situations that give rise to this effect: one in which dissipative dynamics couple together states only within a nearly degenerate subspace, and one in which they give rise to jumps over finite energy splittings, between separate nearly degenerate subspaces.","We find, in each case, that a change of basis can often lead to a representation which more naturally captures the impact of the system-bath interaction than does the energy eigenbasis, revealing that separate timescales are associated with separate processes (e.g. decoherence into a non-energy eigenbasis, decay of population correlations to the initial state).","This approach is paired with the inspection of quantum trajectories, which further provide intuition as to how open system evolution is characterized when coherent oscillations, thermal relaxation, and decoherence all occur simultaneously."],"url":"http://arxiv.org/abs/2402.04524v1","category":"quant-ph"}
{"created":"2024-02-07 01:57:39","title":"FLAGRED -- Fuzzy Logic-based Algorithm Generalizing Risk Estimation for Drones","abstract":"Accurately estimating risk in real-time is essential for ensuring the safety and efficiency of many applications involving autonomous robot systems. This paper presents a novel, generalizable algorithm for the real-time estimation of risks created by external disturbances on multirotors. Unlike conventional approaches, our method requires no additional sensors, accurate drone models, or large datasets. It employs motor command data in a fuzzy logic system, overcoming barriers to real-world implementation. Inherently adaptable, it utilizes fundamental drone characteristics, making it applicable to diverse drone models. The efficiency of the algorithm has been confirmed through comprehensive real-world testing on various platforms. It proficiently discerned between high and low-risk scenarios resulting from diverse wind disturbances and varying thrust-to-weight ratios. The algorithm surpassed the widely-recognized ArduCopter wind estimation algorithm in performance and demonstrated its capability to promptly detect brief gusts.","sentences":["Accurately estimating risk in real-time is essential for ensuring the safety and efficiency of many applications involving autonomous robot systems.","This paper presents a novel, generalizable algorithm for the real-time estimation of risks created by external disturbances on multirotors.","Unlike conventional approaches, our method requires no additional sensors, accurate drone models, or large datasets.","It employs motor command data in a fuzzy logic system, overcoming barriers to real-world implementation.","Inherently adaptable, it utilizes fundamental drone characteristics, making it applicable to diverse drone models.","The efficiency of the algorithm has been confirmed through comprehensive real-world testing on various platforms.","It proficiently discerned between high and low-risk scenarios resulting from diverse wind disturbances and varying thrust-to-weight ratios.","The algorithm surpassed the widely-recognized ArduCopter wind estimation algorithm in performance and demonstrated its capability to promptly detect brief gusts."],"url":"http://arxiv.org/abs/2402.04518v1","category":"cs.RO"}
{"created":"2024-02-07 01:49:03","title":"Generalized Sobolev Transport for Probability Measures on a Graph","abstract":"We study the optimal transport (OT) problem for measures supported on a graph metric space. Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation. However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures. In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function. An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \\emph{Orlicz geometric structure}. Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches. Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation. In this work, we leverage a specific class of convex functions for Orlicz structure to propose the generalized Sobolev transport (GST). GST encompasses the ST as its special case, and can be utilized for prior structures beyond the $L^p$ geometry. In connection with the OW, we show that one only needs to simply solve a univariate optimization problem to compute the GST, unlike the complex two-level optimization problem in OW. We empirically illustrate that GST is several-order faster than the OW. Moreover, we provide preliminary evidences on the advantages of GST for document classification and for several tasks in topological data analysis.","sentences":["We study the optimal transport (OT) problem for measures supported on a graph metric space.","Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation.","However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures.","In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function.","An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \\emph{Orlicz geometric structure}.","Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches.","Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation.","In this work, we leverage a specific class of convex functions for Orlicz structure to propose the generalized Sobolev transport (GST).","GST encompasses the ST as its special case, and can be utilized for prior structures beyond the $L^p$ geometry.","In connection with the OW, we show that one only needs to simply solve a univariate optimization problem to compute the GST, unlike the complex two-level optimization problem in OW.","We empirically illustrate that GST is several-order faster than the OW.","Moreover, we provide preliminary evidences on the advantages of GST for document classification and for several tasks in topological data analysis."],"url":"http://arxiv.org/abs/2402.04516v1","category":"stat.ML"}
{"created":"2024-02-07 01:48:29","title":"A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks","abstract":"Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics. The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability. However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms. These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks. In this paper, we design and develop a deep reinforcement learning (DRL) approach for adaptive traffic routing. We design a deep graph convolutional neural network (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes. We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need for a labeled training dataset, enabling the framework to quickly adapt to traffic dynamics. The model leverages q-value estimates to select the routing path for every traffic flow request, balancing exploration and exploitation. We perform extensive experiments with various traffic patterns and compare the performance of the proposed approach with the Open Shortest Path First (OSPF) protocol. The experimental results show the effectiveness and adaptiveness of the proposed framework by increasing the network throughput by up to 7.8% and reducing the traffic delay by up to 16.1% compared to OSPF.","sentences":["Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics.","The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability.","However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms.","These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks.","In this paper, we design and develop a deep reinforcement learning (DRL) approach for adaptive traffic routing.","We design a deep graph convolutional neural network (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes.","We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need for a labeled training dataset, enabling the framework to quickly adapt to traffic dynamics.","The model leverages q-value estimates to select the routing path for every traffic flow request, balancing exploration and exploitation.","We perform extensive experiments with various traffic patterns and compare the performance of the proposed approach with the Open Shortest Path First (OSPF) protocol.","The experimental results show the effectiveness and adaptiveness of the proposed framework by increasing the network throughput by up to 7.8% and reducing the traffic delay by up to 16.1% compared to OSPF."],"url":"http://arxiv.org/abs/2402.04515v1","category":"cs.NI"}
{"created":"2024-02-07 01:46:50","title":"Online Cascade Learning for Efficient Inference over Streams","abstract":"Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a \"cascade\" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.","sentences":["Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks.","We propose online cascade learning, the first approach to addressing this challenge.","The objective here is to learn a \"cascade\" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input.","We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem.","Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing."],"url":"http://arxiv.org/abs/2402.04513v1","category":"cs.LG"}
{"created":"2024-02-07 01:18:49","title":"Text2Street: Controllable Text-to-image Generation for Street Views","abstract":"Text-to-image generation has made remarkable progress with the emergence of diffusion models. However, it is still a difficult task to generate images for street views based on text, mainly because the road topology of street scenes is complex, the traffic status is diverse and the weather condition is various, which makes conventional text-to-image models difficult to deal with. To address these challenges, we propose a novel controllable text-to-image framework, named \\textbf{Text2Street}. In the framework, we first introduce the lane-aware road topology generator, which achieves text-to-map generation with the accurate road structure and lane lines armed with the counting adapter, realizing the controllable road topology generation. Then, the position-based object layout generator is proposed to obtain text-to-layout generation through an object-level bounding box diffusion strategy, realizing the controllable traffic object layout generation. Finally, the multiple control image generator is designed to integrate the road topology, object layout and weather description to realize controllable street-view image generation. Extensive experiments show that the proposed approach achieves controllable street-view text-to-image generation and validates the effectiveness of the Text2Street framework for street views.","sentences":["Text-to-image generation has made remarkable progress with the emergence of diffusion models.","However, it is still a difficult task to generate images for street views based on text, mainly because the road topology of street scenes is complex, the traffic status is diverse and the weather condition is various, which makes conventional text-to-image models difficult to deal with.","To address these challenges, we propose a novel controllable text-to-image framework, named \\textbf{Text2Street}.","In the framework, we first introduce the lane-aware road topology generator, which achieves text-to-map generation with the accurate road structure and lane lines armed with the counting adapter, realizing the controllable road topology generation.","Then, the position-based object layout generator is proposed to obtain text-to-layout generation through an object-level bounding box diffusion strategy, realizing the controllable traffic object layout generation.","Finally, the multiple control image generator is designed to integrate the road topology, object layout and weather description to realize controllable street-view image generation.","Extensive experiments show that the proposed approach achieves controllable street-view text-to-image generation and validates the effectiveness of the Text2Street framework for street views."],"url":"http://arxiv.org/abs/2402.04504v1","category":"cs.CV"}
{"created":"2024-02-06 23:45:20","title":"Quantum vortex phases of charged pion condensates induced by rotation in a magnetic field","abstract":"Using the relativistic complex scalar field model with a repulsive self-interaction, we discuss the ground state structure of charged pion condensation under the coexistence of parallel rotation and magnetic field. Our previous study found that the density distribution profile of the condensates is a supergiant quantum vortex phase and change with rotational speed and coupling constant. In this work, we further discover vortex lattice structures in the condensates under conditions of small rotation and strong coupling constant. This mechanism can be thought of as electrical superconductivity: Vortex lattices are created to better adapt to changes in rotation and interaction. Furthermore, large rotation and weak coupling constant are more likely to cause the vortex lattices to be destroyed and form a giant quantum vortex similar to a doughnut. We expect this phenomenon can be observed in the relativistic non-central heavy ion collisions with large rotation and strong magnetic field.","sentences":["Using the relativistic complex scalar field model with a repulsive self-interaction, we discuss the ground state structure of charged pion condensation under the coexistence of parallel rotation and magnetic field.","Our previous study found that the density distribution profile of the condensates is a supergiant quantum vortex phase and change with rotational speed and coupling constant.","In this work, we further discover vortex lattice structures in the condensates under conditions of small rotation and strong coupling constant.","This mechanism can be thought of as electrical superconductivity: Vortex lattices are created to better adapt to changes in rotation and interaction.","Furthermore, large rotation and weak coupling constant are more likely to cause the vortex lattices to be destroyed and form a giant quantum vortex similar to a doughnut.","We expect this phenomenon can be observed in the relativistic non-central heavy ion collisions with large rotation and strong magnetic field."],"url":"http://arxiv.org/abs/2402.04475v1","category":"hep-ph"}
{"created":"2024-02-06 23:26:12","title":"DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems","abstract":"Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics. However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system. In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases. We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives. Our Dynamics Stable Learning by Invariant Measures (DySLIM) objective enables model training that achieves better point-wise tracking and long-term statistical accuracy relative to other learning objectives. By targeting the distribution with a scalable regularization term, we hope that this approach can be extended to more complex systems exhibiting slowly-variant distributions, such as weather and climate models.","sentences":["Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics.","However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system.","In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases.","We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives.","Our Dynamics Stable Learning by Invariant Measures (DySLIM) objective enables model training that achieves better point-wise tracking and long-term statistical accuracy relative to other learning objectives.","By targeting the distribution with a scalable regularization term, we hope that this approach can be extended to more complex systems exhibiting slowly-variant distributions, such as weather and climate models."],"url":"http://arxiv.org/abs/2402.04467v1","category":"cs.LG"}
{"created":"2024-02-06 22:44:27","title":"Evolving Mobile Cloud Gaming with 5G Standalone Network Telemetry","abstract":"Mobile cloud gaming places the simultaneous demands of high capacity and low latency on the wireless network, demands that Private and Metropolitan-Area Standalone 5G networks are poised to meet. However, lacking introspection into the 5G Radio Access Network (RAN), cloud gaming servers are ill-poised to cope with the vagaries of the wireless last hop to a mobile client, while 5G network operators run mostly closed networks, limiting their potential for co-design with the wider internet and user applications. This paper presents Telesa, a passive, incrementally-deployable, and independently-deployable Standalone 5G network telemetry system that streams fine-grained RAN capacity, latency, and retransmission information to application servers to enable better millisecond scale, application-level decisions on offered load and bit rate adaptation than end-to-end latency measurements or end-to-end packet losses currently permit. We design, implement, and evaluate a Telesa telemetry-enhanced game streaming platform, demonstrating exact congestion-control that can better adapt game video bitrate while simultaneously controlling end-to-end latency, thus maximizing game quality of experience. Our experimental evaluation on a production 5G Standalone network demonstrates a 178-249% Quality of Experience improvement versus two state-of-the-art cloud gaming applications.","sentences":["Mobile cloud gaming places the simultaneous demands of high capacity and low latency on the wireless network, demands that Private and Metropolitan-Area","Standalone 5G networks are poised to meet.","However, lacking introspection into the 5G Radio Access Network (RAN), cloud gaming servers are ill-poised to cope with the vagaries of the wireless last hop to a mobile client, while 5G network operators run mostly closed networks, limiting their potential for co-design with the wider internet and user applications.","This paper presents Telesa, a passive, incrementally-deployable, and independently-deployable Standalone 5G network telemetry system that streams fine-grained RAN capacity, latency, and retransmission information to application servers to enable better millisecond scale, application-level decisions on offered load and bit rate adaptation than end-to-end latency measurements or end-to-end packet losses currently permit.","We design, implement, and evaluate a Telesa telemetry-enhanced game streaming platform, demonstrating exact congestion-control that can better adapt game video bitrate while simultaneously controlling end-to-end latency, thus maximizing game quality of experience.","Our experimental evaluation on a production 5G Standalone network demonstrates a 178-249% Quality of Experience improvement versus two state-of-the-art cloud gaming applications."],"url":"http://arxiv.org/abs/2402.04454v1","category":"cs.NI"}
{"created":"2024-02-06 22:24:56","title":"Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations","abstract":"Effective communication between healthcare providers and patients is crucial to providing high-quality patient care. In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems. By analyzing embeddings such as bag-of-words, character n-grams, Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations. Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner. Overall, Word2Vec, GloVe and Character n-grams embeddings performed well, indicating their suitability for modeling targeted to this task. GPT2 embedding also shows notable performance, indicating its suitability for models tailored to this task as well. Our machine learning architectures significantly improved the quality of health conversations when training data are scarce, improving communication between patients and healthcare providers.","sentences":["Effective communication between healthcare providers and patients is crucial to providing high-quality patient care.","In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems.","By analyzing embeddings such as bag-of-words, character n-grams, Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations.","Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner.","Overall, Word2Vec, GloVe and Character n-grams embeddings performed well, indicating their suitability for modeling targeted to this task.","GPT2 embedding also shows notable performance, indicating its suitability for models tailored to this task as well.","Our machine learning architectures significantly improved the quality of health conversations when training data are scarce, improving communication between patients and healthcare providers."],"url":"http://arxiv.org/abs/2402.04442v1","category":"cs.CL"}
{"created":"2024-02-06 21:18:34","title":"The VampPrior Mixture Model","abstract":"Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.","sentences":["Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations.","Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously.","We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs.","We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters.","Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets.","Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters."],"url":"http://arxiv.org/abs/2402.04412v1","category":"cs.LG"}
{"created":"2024-02-06 21:03:52","title":"Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning","abstract":"Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users' personal PEFT parameters, they can own and use their LLMs personally. OPPU integrates parametric user knowledge in the personal PEFT parameters with the non-parametric knowledge acquired through retrieval and profile. This integration adapts individual LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further in-depth studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different active levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.","sentences":["Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences.","Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles.","However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues.","Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic.","To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences.","By plugging in users' personal PEFT parameters, they can own and use their LLMs personally.","OPPU integrates parametric user knowledge in the personal PEFT parameters with the non-parametric knowledge acquired through retrieval and profile.","This integration adapts individual LLMs to user behavior shifts.","Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark.","Further in-depth studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different active levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods."],"url":"http://arxiv.org/abs/2402.04401v1","category":"cs.CL"}
{"created":"2024-02-06 20:57:16","title":"A Repeated Auction Model for Load-Aware Dynamic Resource Allocation in Multi-Access Edge Computing","abstract":"Multi-access edge computing (MEC) is one of the enabling technologies for high-performance computing at the edge of the 6 G networks, supporting high data rates and ultra-low service latency. Although MEC is a remedy to meet the growing demand for computation-intensive applications, the scarcity of resources at the MEC servers degrades its performance. Hence, effective resource management is essential; nevertheless, state-of-the-art research lacks efficient economic models to support the exponential growth of the MEC-enabled applications market. We focus on designing a MEC offloading service market based on a repeated auction model with multiple resource sellers (e.g., network operators and service providers) that compete to sell their computing resources to the offloading users. We design a computationally-efficient modified Generalized Second Price (GSP)-based algorithm that decides on pricing and resource allocation by considering the dynamic offloading requests arrival and the servers' computational workloads. Besides, we propose adaptive best-response bidding strategies for the resource sellers, satisfying the symmetric Nash equilibrium (SNE) and individual rationality properties. Finally, via intensive numerical results, we show the effectiveness of our proposed resource allocation mechanism.","sentences":["Multi-access edge computing (MEC) is one of the enabling technologies for high-performance computing at the edge of the 6 G networks, supporting high data rates and ultra-low service latency.","Although MEC is a remedy to meet the growing demand for computation-intensive applications, the scarcity of resources at the MEC servers degrades its performance.","Hence, effective resource management is essential; nevertheless, state-of-the-art research lacks efficient economic models to support the exponential growth of the MEC-enabled applications market.","We focus on designing a MEC offloading service market based on a repeated auction model with multiple resource sellers (e.g., network operators and service providers) that compete to sell their computing resources to the offloading users.","We design a computationally-efficient modified Generalized Second Price (GSP)-based algorithm that decides on pricing and resource allocation by considering the dynamic offloading requests arrival and the servers' computational workloads.","Besides, we propose adaptive best-response bidding strategies for the resource sellers, satisfying the symmetric Nash equilibrium (SNE) and individual rationality properties.","Finally, via intensive numerical results, we show the effectiveness of our proposed resource allocation mechanism."],"url":"http://arxiv.org/abs/2402.04399v1","category":"cs.GT"}
{"created":"2024-02-06 20:47:58","title":"Factorial Basis Method for q-Series Applications","abstract":"The Factorial Basis method, initially designed for quasi-triangular, shift-compatible factorial bases, provides solutions to linear recurrence equations in the form of definite-sums. This paper extends the Factorial Basis method to its q-analog, enabling its application in q-calculus. We demonstrate the adaptation of the method to q-sequences and its utility in the realm of q-combinatorics. The extended technique is employed to automatically prove established identities and unveil novel ones, particularly some associated with the Rogers-Ramanujan identities.","sentences":["The Factorial Basis method, initially designed for quasi-triangular, shift-compatible factorial bases, provides solutions to linear recurrence equations in the form of definite-sums.","This paper extends the Factorial Basis method to its q-analog, enabling its application in q-calculus.","We demonstrate the adaptation of the method to q-sequences and its utility in the realm of q-combinatorics.","The extended technique is employed to automatically prove established identities and unveil novel ones, particularly some associated with the Rogers-Ramanujan identities."],"url":"http://arxiv.org/abs/2402.04392v1","category":"cs.SC"}
{"created":"2024-02-06 20:44:37","title":"Control of seizure-like dynamics in neuronal populations with excitability adaptation related to ketogenic diet","abstract":"We consider a heterogeneous, globally coupled population of excitatory quadratic integrate-and-fire neurons with excitability adaptation due to a metabolic feedback associated with ketogenic diet, a form of therapy for epilepsy. Bifurcation analysis of a three-dimensional mean-field system derived in the framework of next-generation neural mass models allows us to explain the scenarios and suggest control strategies for the transitions between the neurophysiologically desired asynchronous states and the synchronous, seizure-like states featuring collective oscillations. We reveal two qualitatively different scenarios for the onset of synchrony. For weaker couplings, a bistability region between the lower- and the higher-activity asynchronous states unfolds from the cusp point, and the collective oscillations emerge via a supercritical Hopf bifurcation. For stronger couplings, one finds seven co-dimension two bifurcation points, including pairs of Bogdanov-Takens and generalized Hopf points, such that both lower- and higher-activity asynchronous states undergo transitions to collective oscillations, with hysteresis and jump-like behavior observed in vicinity of subcritical Hopf bifurcations. We demonstrate three control mechanisms for switching between asynchronous and synchronous states, involving parametric perturbation of the adenosine triphosphate (ATP) production rate, external stimulation currents, or pulse-like ATP shocks, and indicate a potential therapeutic advantage of hysteretic scenarios.","sentences":["We consider a heterogeneous, globally coupled population of excitatory quadratic integrate-and-fire neurons with excitability adaptation due to a metabolic feedback associated with ketogenic diet, a form of therapy for epilepsy.","Bifurcation analysis of a three-dimensional mean-field system derived in the framework of next-generation neural mass models allows us to explain the scenarios and suggest control strategies for the transitions between the neurophysiologically desired asynchronous states and the synchronous, seizure-like states featuring collective oscillations.","We reveal two qualitatively different scenarios for the onset of synchrony.","For weaker couplings, a bistability region between the lower- and the higher-activity asynchronous states unfolds from the cusp point, and the collective oscillations emerge via a supercritical Hopf bifurcation.","For stronger couplings, one finds seven co-dimension two bifurcation points, including pairs of Bogdanov-Takens and generalized Hopf points, such that both lower- and higher-activity asynchronous states undergo transitions to collective oscillations, with hysteresis and jump-like behavior observed in vicinity of subcritical Hopf bifurcations.","We demonstrate three control mechanisms for switching between asynchronous and synchronous states, involving parametric perturbation of the adenosine triphosphate (ATP) production rate, external stimulation currents, or pulse-like ATP shocks, and indicate a potential therapeutic advantage of hysteretic scenarios."],"url":"http://arxiv.org/abs/2402.04388v1","category":"nlin.AO"}
{"created":"2024-02-06 20:13:34","title":"Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception","abstract":"This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality. It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations. While previous models of pedestrian behaviour have been either 'black-box' machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches. Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use reinforcement learning to learn bounded optimal behaviour policy. The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timing of the stopping distance of the yielding vehicle. Notably, our findings suggest that behaviours previously framed as 'biases' in decision-making, such as speed-dependent gap acceptance, might instead be a product of rational adaptation to the constraints of visual perception. Our approach also permits fitting the parameters of cognitive constraints and rewards per individual, to better account for individual differences. To conclude, by leveraging both RL and mechanistic modelling, our model offers novel insights about pedestrian behaviour, and may provide a useful foundation for more accurate and scalable pedestrian models.","sentences":["This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality.","It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations.","While previous models of pedestrian behaviour have been either 'black-box' machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches.","Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use reinforcement learning to learn bounded optimal behaviour policy.","The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timing of the stopping distance of the yielding vehicle.","Notably, our findings suggest that behaviours previously framed as 'biases' in decision-making, such as speed-dependent gap acceptance, might instead be a product of rational adaptation to the constraints of visual perception.","Our approach also permits fitting the parameters of cognitive constraints and rewards per individual, to better account for individual differences.","To conclude, by leveraging both RL and mechanistic modelling, our model offers novel insights about pedestrian behaviour, and may provide a useful foundation for more accurate and scalable pedestrian models."],"url":"http://arxiv.org/abs/2402.04370v1","category":"cs.AI"}
{"created":"2024-02-06 19:49:23","title":"Adaptive Inference: Theoretical Limits and Unexplored Opportunities","abstract":"This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms. We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties. Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces.","sentences":["This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms.","We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties.","Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces."],"url":"http://arxiv.org/abs/2402.04359v1","category":"cs.LG"}
{"created":"2024-02-06 19:42:18","title":"Bidirectional Autoregressive Diffusion Model for Dance Generation","abstract":"Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge. Recently, diffusion models have showcased remarkable generative abilities across various domains. They hold promise for human motion generation due to their adaptable many-to-many nature. Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement. When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions. To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions. To make the generated dance motion smoother, a local information decoder is built for local motion enhancement. The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions. To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements. Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation.","sentences":["Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge.","Recently, diffusion models have showcased remarkable generative abilities across various domains.","They hold promise for human motion generation due to their adaptable many-to-many nature.","Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement.","When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions.","To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions.","To make the generated dance motion smoother, a local information decoder is built for local motion enhancement.","The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions.","To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements.","Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation."],"url":"http://arxiv.org/abs/2402.04356v1","category":"cs.SD"}
{"created":"2024-02-06 19:37:05","title":"3D printer-controlled syringe pumps for dual, active, regulable and simultaneous dispensing of reagents. Manufacturing of immunochromatographic test strips","abstract":"Lateral flow immunoassays (LFIA) are widely used worldwide for the detection of different analytes because they combine multiple advantages such as low production cost, simplicity, and portability, which allows biomarkers detection without requiring infrastructure or highly trained personnel. Here we propose to provide solutions to the manufacturing process of LFIA at laboratory-scale, particularly to the controlled and active dispensing of the reagents in the form the Test Lines (TL) and the Control Lines (CL). To accomplish this task, we adapted a 3D printer to also control Syringe Pumps (SP), since the proposed adaptation of a 3D printer is easy, free and many laboratories already have it in their infrastructure. In turn, the standard function of the 3D printer can be easily restored by disconnecting the SPs and reconnecting the extruder. Additionally, the unified control of the 3D printer enables dual, active, regulable and simultaneous dispensing, four features that are typically found only in certain high-cost commercial equipment. With the proposed setup, the challenge of dispensing simultaneously at least 2 lines (CL and TL) with SPs controlled by a 3D printer was addressed, including regulation in the width of dispensed lines within experimental limits. Also, the construction of a LFIA for the detection of leptospirosis is shown as a practical example of automatized reagent dispensing.","sentences":["Lateral flow immunoassays (LFIA) are widely used worldwide for the detection of different analytes because they combine multiple advantages such as low production cost, simplicity, and portability, which allows biomarkers detection without requiring infrastructure or highly trained personnel.","Here we propose to provide solutions to the manufacturing process of LFIA at laboratory-scale, particularly to the controlled and active dispensing of the reagents in the form the Test Lines (TL) and the Control Lines (CL).","To accomplish this task, we adapted a 3D printer to also control Syringe Pumps (SP), since the proposed adaptation of a 3D printer is easy, free and many laboratories already have it in their infrastructure.","In turn, the standard function of the 3D printer can be easily restored by disconnecting the SPs and reconnecting the extruder.","Additionally, the unified control of the 3D printer enables dual, active, regulable and simultaneous dispensing, four features that are typically found only in certain high-cost commercial equipment.","With the proposed setup, the challenge of dispensing simultaneously at least 2 lines (CL and TL) with SPs controlled by a 3D printer was addressed, including regulation in the width of dispensed lines within experimental limits.","Also, the construction of a LFIA for the detection of leptospirosis is shown as a practical example of automatized reagent dispensing."],"url":"http://arxiv.org/abs/2402.04354v1","category":"cs.RO"}
{"created":"2024-02-06 19:31:26","title":"The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry","abstract":"Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) \"finetuned-conversion\" of task-specific Transformers into linear versions that recover task performance, and (3) \"pretrained-conversion\" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or \"spiky\") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.","sentences":["Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length.","This holds exciting promise for (1) training linear Transformers from scratch, (2) \"finetuned-conversion\" of task-specific Transformers into linear versions that recover task performance, and (3) \"pretrained-conversion\" of Transformers such as large language models into linear versions finetunable on downstream tasks.","However, linear attentions often underperform standard softmax attention in quality.","To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or \"spiky\") weights and dot-product monotonicity.","We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention.","We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity.","Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention.","Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs.","Hedgehog also enables pretrained-conversion.","Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models.","We finally turn a pretrained Llama-2 7B into a viable linear attention Llama.","With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops."],"url":"http://arxiv.org/abs/2402.04347v1","category":"cs.LG"}
{"created":"2024-02-06 19:23:29","title":"CausalMetaR: An R package for performing causally interpretable meta-analyses","abstract":"Researchers would often like to leverage data from a collection of sources (e.g., primary studies in a meta-analysis) to estimate causal effects in a target population of interest. However, traditional meta-analytic methods do not produce causally interpretable estimates for a well-defined target population. In this paper, we present the CausalMetaR R package, which implements efficient and robust methods to estimate causal effects in a given internal or external target population using multi-source data. The package includes estimators of average and subgroup treatment effects for the entire target population. To produce efficient and robust estimates of causal effects, the package implements doubly robust and non-parametric efficient estimators and supports using flexible data-adaptive (e.g., machine learning techniques) methods and cross-fitting techniques to estimate the nuisance models (e.g., the treatment model, the outcome model). We describe the key features of the package and demonstrate how to use the package through an example.","sentences":["Researchers would often like to leverage data from a collection of sources (e.g., primary studies in a meta-analysis) to estimate causal effects in a target population of interest.","However, traditional meta-analytic methods do not produce causally interpretable estimates for a well-defined target population.","In this paper, we present the CausalMetaR R package, which implements efficient and robust methods to estimate causal effects in a given internal or external target population using multi-source data.","The package includes estimators of average and subgroup treatment effects for the entire target population.","To produce efficient and robust estimates of causal effects, the package implements doubly robust and non-parametric efficient estimators and supports using flexible data-adaptive (e.g., machine learning techniques) methods and cross-fitting techniques to estimate the nuisance models (e.g., the treatment model, the outcome model).","We describe the key features of the package and demonstrate how to use the package through an example."],"url":"http://arxiv.org/abs/2402.04341v1","category":"stat.ME"}
{"created":"2024-02-06 19:18:04","title":"LESS: Selecting Influential Data for Targeted Instruction Tuning","abstract":"Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application.","sentences":["Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots.","However, real-world applications often require a specialized suite of skills (e.g., reasoning).","The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning.","We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt","Similarity Search for instruction data selection.","Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data.","LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability.","Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks.","Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families.","Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application."],"url":"http://arxiv.org/abs/2402.04333v1","category":"cs.CL"}
{"created":"2024-02-06 19:16:25","title":"Proactive Blockage Prediction for UAV assisted Handover in Future Wireless Network","abstract":"The future wireless communication applications demand seamless connectivity, higher throughput, and low latency, for which the millimeter-wave (mmWave) band is considered a potential technology. Nevertheless, line-of-sight (LoS) is often mandatory for mmWave band communication, and it renders these waves sensitive to sudden changes in the environment. Therefore, it is necessary to maintain the LoS link for a reliable connection. One such technique to maintain LoS is using proactive handover (HO). However, proactive HO is challenging, requiring continuous information about the surrounding wireless network to anticipate potential blockage. This paper presents a proactive blockage prediction mechanism where an unmanned aerial vehicle (UAV) is used as the base station for HO. The proposed scheme uses computer vision (CV) to obtain potential blocking objects, user speed, and location. To assess the effectiveness of the proposed scheme, the system is evaluated using a publicly available dataset for blockage prediction. The study integrates scenarios from Vision-based Wireless (ViWi) and UAV channel modeling, generating wireless data samples relevant to UAVs. The antenna modeling on the UAV end incorporates a polarization-matched scenario to optimize signal reception. The results demonstrate that UAV-assisted Handover not only ensures seamless connectivity but also enhances overall network performance by 20%. This research contributes to the advancement of proactive blockage mitigation strategies in wireless networks, showcasing the potential of UAVs as dynamic and adaptable base stations.","sentences":["The future wireless communication applications demand seamless connectivity, higher throughput, and low latency, for which the millimeter-wave (mmWave) band is considered a potential technology.","Nevertheless, line-of-sight (LoS) is often mandatory for mmWave band communication, and it renders these waves sensitive to sudden changes in the environment.","Therefore, it is necessary to maintain the LoS link for a reliable connection.","One such technique to maintain LoS is using proactive handover (HO).","However, proactive HO is challenging, requiring continuous information about the surrounding wireless network to anticipate potential blockage.","This paper presents a proactive blockage prediction mechanism where an unmanned aerial vehicle (UAV) is used as the base station for HO.","The proposed scheme uses computer vision (CV) to obtain potential blocking objects, user speed, and location.","To assess the effectiveness of the proposed scheme, the system is evaluated using a publicly available dataset for blockage prediction.","The study integrates scenarios from Vision-based Wireless (ViWi) and UAV channel modeling, generating wireless data samples relevant to UAVs.","The antenna modeling on the UAV end incorporates a polarization-matched scenario to optimize signal reception.","The results demonstrate that UAV-assisted Handover not only ensures seamless connectivity but also enhances overall network performance by 20%.","This research contributes to the advancement of proactive blockage mitigation strategies in wireless networks, showcasing the potential of UAVs as dynamic and adaptable base stations."],"url":"http://arxiv.org/abs/2402.04332v1","category":"eess.SP"}
{"created":"2024-02-06 19:05:22","title":"Homogeneity problem for basis expansion of functional data with applications to resistive memories","abstract":"The homogeneity problem for testing if more than two different samples come from the same population is considered for the case of functional data. The methodological results are motivated by the study of homogeneity of electronic devices fabricated by different materials and active layer thicknesses. In the case of normality distribution of the stochastic processes associated with each sample, this problem is known as Functional ANOVA problem and is reduced to test the equality of the mean group functions (FANOVA). The problem is that the current/voltage curves associated with Resistive Random Access Memories (RRAM) are not generated by a Gaussian process so that a different approach is necessary for testing homogeneity. To solve this problem two different parametric and nonparametric approaches based on basis expansion of the sample curves are proposed. The first consists of testing multivariate homogeneity tests on a vector of basis coefficients of the sample curves. The second is based on dimension reduction by using functional principal component analysis of the sample curves (FPCA) and testing multivariate homogeneity on a vector of principal components scores. Different approximation numerical techniques are employed to adapt the experimental data for the statistical study. An extensive simulation study is developed for analyzing the performance of both approaches in the parametric and non-parametric cases. Finally, the proposed methodologies are applied on three samples of experimental reset curves measured in three different RRAM technologies.","sentences":["The homogeneity problem for testing if more than two different samples come from the same population is considered for the case of functional data.","The methodological results are motivated by the study of homogeneity of electronic devices fabricated by different materials and active layer thicknesses.","In the case of normality distribution of the stochastic processes associated with each sample, this problem is known as Functional ANOVA problem and is reduced to test the equality of the mean group functions (FANOVA).","The problem is that the current/voltage curves associated with Resistive Random Access Memories (RRAM) are not generated by a Gaussian process so that a different approach is necessary for testing homogeneity.","To solve this problem two different parametric and nonparametric approaches based on basis expansion of the sample curves are proposed.","The first consists of testing multivariate homogeneity tests on a vector of basis coefficients of the sample curves.","The second is based on dimension reduction by using functional principal component analysis of the sample curves (FPCA) and testing multivariate homogeneity on a vector of principal components scores.","Different approximation numerical techniques are employed to adapt the experimental data for the statistical study.","An extensive simulation study is developed for analyzing the performance of both approaches in the parametric and non-parametric cases.","Finally, the proposed methodologies are applied on three samples of experimental reset curves measured in three different RRAM technologies."],"url":"http://arxiv.org/abs/2402.04321v1","category":"stat.ME"}
{"created":"2024-02-06 19:02:33","title":"Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments","abstract":"In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments. Traditional approaches often rely on computational methods such as time-series analysis. Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs. We introduce a novel \"adaptive visual sector\" mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. Additionally, we develop a \"dynamic traffic graph\" using Convolutional Neural Networks (CNN) and Graph Attention Networks (GAT) to capture spatio-temporal dependencies among agents. Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively. Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs. The code for the proposed model is available at our Github.","sentences":["In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments.","Traditional approaches often rely on computational methods such as time-series analysis.","Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs.","We introduce a novel \"adaptive visual sector\" mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed.","Additionally, we develop a \"dynamic traffic graph\" using Convolutional Neural Networks (CNN) and Graph Attention Networks (GAT) to capture spatio-temporal dependencies among agents.","Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively.","Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs.","The code for the proposed model is available at our Github."],"url":"http://arxiv.org/abs/2402.04318v1","category":"cs.RO"}
{"created":"2024-02-07 18:56:20","title":"On gauge transformations in twistless torsional Newton--Cartan geometry","abstract":"We observe that in type I twistless torsional Newton--Cartan (TTNC) geometry, one can always find (at least locally) a gauge transformation that transforms a specific locally Galilei-invariant function -- that we dub the `locally Galilei-invariant potential' -- to zero, due to the corresponding equation for the gauge parameter taking the form of a Hamilton--Jacobi equation. In the case of type II TTNC geometry, the same gauge fixing may locally be performed by subleading spatial diffeomorphisms. We show (a) how this generalises a classical result in standard Newton--Cartan geometry, and (b) how it allows to parametrise the metric structure of a Galilei manifold as well as the gauge equivalence class of the Bargmann form of TTNC geometry in terms of just the space metric and a unit timelike vector field.","sentences":["We observe that in type I twistless torsional Newton--Cartan (TTNC) geometry, one can always find (at least locally) a gauge transformation that transforms a specific locally Galilei-invariant function -- that we dub the `locally Galilei-invariant potential' -- to zero, due to the corresponding equation for the gauge parameter taking the form of a Hamilton--Jacobi equation.","In the case of type II TTNC geometry, the same gauge fixing may locally be performed by subleading spatial diffeomorphisms.","We show (a) how this generalises a classical result in standard Newton--Cartan geometry, and (b) how it allows to parametrise the metric structure of a Galilei manifold as well as the gauge equivalence class of the Bargmann form of TTNC geometry in terms of just the space metric and a unit timelike vector field."],"url":"http://arxiv.org/abs/2402.05105v1","category":"gr-qc"}
{"created":"2024-02-07 18:46:47","title":"Interacting particle approximation of cross-diffusion systems","abstract":"We derive a class of multi-species cross-diffusion systems from stochastic interacting particles. We prove existence of weak solutions of the limiting cross-diffusion system as well as the propagation of chaos by means of nonlocal approximation of the nonlinear diffusion terms, coupling methods and compactness arguments. We show that these equations capture the macroscopic behavior of the interacting particle system if the localisation parameter is chosen logarithmically with respect to the number of particles.","sentences":["We derive a class of multi-species cross-diffusion systems from stochastic interacting particles.","We prove existence of weak solutions of the limiting cross-diffusion system as well as the propagation of chaos by means of nonlocal approximation of the nonlinear diffusion terms, coupling methods and compactness arguments.","We show that these equations capture the macroscopic behavior of the interacting particle system if the localisation parameter is chosen logarithmically with respect to the number of particles."],"url":"http://arxiv.org/abs/2402.05094v1","category":"math.AP"}
{"created":"2024-02-07 18:46:46","title":"Moduli Parameters of Complex Singularities with Non-Degenerate Newton Boundary","abstract":"Our recent extension of Arnold's classification includes all singularities of corank up to two equivalent to a germ with a non-degenerate Newton boundary, thus broadening the classification's scope significantly by a class which is unbounded with respect to modality and Milnor number. This method is based on proving that all right-equivalence classes within a mu-constant stratum can be represented by a single normal form derived from a regular basis of a suitably selected special fiber. While both Arnold's and our preceding work on normal forms addresses the determination of a normal form family containing the given germ, this paper takes the next natural step: We present an algorithm for computing for a given germ the values of the moduli parameters in its normal form family, that is, a normal form equation in its stable equivalence class. This algorithm will be crucial for understanding the moduli stacks of such singularities. The implementation of this algorithm, along with the foundational classification techniques, is implemented in the library arnold.lib for the computer algebra system Singular.","sentences":["Our recent extension of Arnold's classification includes all singularities of corank up to two equivalent to a germ with a non-degenerate Newton boundary, thus broadening the classification's scope significantly by a class which is unbounded with respect to modality and Milnor number.","This method is based on proving that all right-equivalence classes within a mu-constant stratum can be represented by a single normal form derived from a regular basis of a suitably selected special fiber.","While both Arnold's and our preceding work on normal forms addresses the determination of a normal form family containing the given germ, this paper takes the next natural step: We present an algorithm for computing for a given germ the values of the moduli parameters in its normal form family, that is, a normal form equation in its stable equivalence class.","This algorithm will be crucial for understanding the moduli stacks of such singularities.","The implementation of this algorithm, along with the foundational classification techniques, is implemented in the library arnold.lib for the computer algebra system Singular."],"url":"http://arxiv.org/abs/2402.05093v1","category":"math.AG"}
{"created":"2024-02-07 18:34:34","title":"Information and Configurational Entropy in Glassy Systems","abstract":"It is often stated that if one is presented with a snapshot of the positions of the molecules of a glass and one of a liquid, one is unable to tell the difference. Here we argue instead that given several such snapshots taken over a time-interval, even without specifying the times, there is a definite procedure to assess precisely the level of glassiness: it suffices to concatenate the snapshots side-by-side, and to subject the joint picture to a lossless compression protocol. We argue that the size of the compressed file yields a direct and unambiguous measure of the `vibrational' and `configurational' entropies, and may be used to study the associated glass length scale in or out of equilibrium through the size and frequency of the repeated motifs essential to the compression, a quantity that would diverge at a putative glass transition.","sentences":["It is often stated that if one is presented with a snapshot of the positions of the molecules of a glass and one of a liquid, one is unable to tell the difference.","Here we argue instead that given several such snapshots taken over a time-interval, even without specifying the times, there is a definite procedure to assess precisely the level of glassiness: it suffices to concatenate the snapshots side-by-side, and to subject the joint picture to a lossless compression protocol.","We argue that the size of the compressed file yields a direct and unambiguous measure of the `vibrational' and `configurational' entropies, and may be used to study the associated glass length scale in or out of equilibrium through the size and frequency of the repeated motifs essential to the compression, a quantity that would diverge at a putative glass transition."],"url":"http://arxiv.org/abs/2402.05081v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-07 18:33:28","title":"Designing three-way entangled and nonlocal two-way entangled single particle states via alternate quantum walks","abstract":"Entanglement with single-particle states is advantageous in quantum technology because of their ability to encode and process information more securely than their multi-particle analogs. Three-way and nonlocal two-way entangled single-particle states are desirable in this context. Herein, we generate three-way entanglement from an initially separable state involving three degrees of freedom of a quantum particle, which evolves via a 2D alternate quantum walk employing a resource-saving single-qubit coin. We achieve maximum possible values for the three-way entanglement quantified by the $\\pi$-tangle between the three degrees of freedom. We also generate optimal two-way nonlocal entanglement, quantified by the negativity between the nonlocal position degrees of freedom of the particle. This prepared architecture using quantum walks can be experimentally realized with a photon.","sentences":["Entanglement with single-particle states is advantageous in quantum technology because of their ability to encode and process information more securely than their multi-particle analogs.","Three-way and nonlocal two-way entangled single-particle states are desirable in this context.","Herein, we generate three-way entanglement from an initially separable state involving three degrees of freedom of a quantum particle, which evolves via a 2D alternate quantum walk employing a resource-saving single-qubit coin.","We achieve maximum possible values for the three-way entanglement quantified by the $\\pi$-tangle between the three degrees of freedom.","We also generate optimal two-way nonlocal entanglement, quantified by the negativity between the nonlocal position degrees of freedom of the particle.","This prepared architecture using quantum walks can be experimentally realized with a photon."],"url":"http://arxiv.org/abs/2402.05080v2","category":"quant-ph"}
{"created":"2024-02-07 18:27:29","title":"NITO: Neural Implicit Fields for Resolution-free Topology Optimization","abstract":"Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance. We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning. NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization. NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time. In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive simulation-based approaches. Crucially, NITO circumvents the domain and resolution limitations that restrict Convolutional Neural Network (CNN) models to a structured domain of fixed size -- limitations that hinder the widespread adoption of CNNs in engineering applications. This generalizability allows a single NITO model to train and generate solutions in countless domains, eliminating the need for numerous domain-specific CNNs and their extensive datasets. Despite its generalizability, NITO outperforms SOTA models even in specialized tasks, is an order of magnitude smaller, and is practically trainable at high resolutions that would be restrictive for CNNs. This combination of versatility, efficiency, and performance underlines NITO's potential to transform the landscape of engineering design optimization problems through implicit fields.","sentences":["Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance.","We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning.","NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization.","NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time.","In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive simulation-based approaches.","Crucially, NITO circumvents the domain and resolution limitations that restrict Convolutional Neural Network (CNN) models to a structured domain of fixed size -- limitations that hinder the widespread adoption of CNNs in engineering applications.","This generalizability allows a single NITO model to train and generate solutions in countless domains, eliminating the need for numerous domain-specific CNNs and their extensive datasets.","Despite its generalizability, NITO outperforms SOTA models even in specialized tasks, is an order of magnitude smaller, and is practically trainable at high resolutions that would be restrictive for CNNs.","This combination of versatility, efficiency, and performance underlines NITO's potential to transform the landscape of engineering design optimization problems through implicit fields."],"url":"http://arxiv.org/abs/2402.05073v1","category":"cs.LG"}
{"created":"2024-02-07 18:19:51","title":"Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems","abstract":"Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel decoupling solving mode is proposed through modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system. A Spectral Physics-informed Neural Network (PINN) is developed to characterize the small-scale system in an efficient and accurate way. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky equation, two- and three-dimensional Navier-Stokes equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. The discussions about these scenarios contribute to a comprehensive understanding of the method's capabilities and limitations. This paper presents a valuable and promising approach to enhance the computational simulations of multiscale spatiotemporal systems, which enables the acquisition of large-scale data with minimal computational demands, followed by Spectral PINN to capture small-scale dynamics with improved efficiency and accuracy.","sentences":["Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems.","In this paper, a novel decoupling solving mode is proposed through modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system.","A Spectral Physics-informed Neural Network (PINN) is developed to characterize the small-scale system in an efficient and accurate way.","The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky equation, two- and three-dimensional Navier-Stokes equations, showcasing its versatility in addressing problems of fluid dynamics.","Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics.","The discussions about these scenarios contribute to a comprehensive understanding of the method's capabilities and limitations.","This paper presents a valuable and promising approach to enhance the computational simulations of multiscale spatiotemporal systems, which enables the acquisition of large-scale data with minimal computational demands, followed by Spectral PINN to capture small-scale dynamics with improved efficiency and accuracy."],"url":"http://arxiv.org/abs/2402.05067v2","category":"physics.flu-dyn"}
{"created":"2024-02-07 18:15:28","title":"Tuning the feedback controller gains is a simple way to improve autonomous driving performance","abstract":"Typical autonomous driving systems are a combination of machine learning algorithms (often involving neural networks) and classical feedback controllers. Whilst significant progress has been made in recent years on the neural network side of these systems, only limited progress has been made on the feedback controller side. Often, the feedback control gains are simply passed from paper to paper with little re-tuning taking place, even though the changes to the neural networks can alter the vehicle's closed loop dynamics. The aim of this paper is to highlight the limitations of this approach; it is shown that re-tuning the feedback controller can be a simple way to improve autonomous driving performance. To demonstrate this, the PID gains of the longitudinal controller in the TCP autonomous vehicle algorithm are tuned. This causes the driving score in CARLA to increase from 73.21 to 77.38, with the results averaged over 16 driving scenarios. Moreover, it was observed that the performance benefits were most apparent during challenging driving scenarios, such as during rain or night time, as the tuned controller led to a more assertive driving style. These results demonstrate the value of developing both the neural network and feedback control policies of autonomous driving systems simultaneously, as this can be a simple and methodical way to improve autonomous driving system performance and robustness.","sentences":["Typical autonomous driving systems are a combination of machine learning algorithms (often involving neural networks) and classical feedback controllers.","Whilst significant progress has been made in recent years on the neural network side of these systems, only limited progress has been made on the feedback controller side.","Often, the feedback control gains are simply passed from paper to paper with little re-tuning taking place, even though the changes to the neural networks can alter the vehicle's closed loop dynamics.","The aim of this paper is to highlight the limitations of this approach; it is shown that re-tuning the feedback controller can be a simple way to improve autonomous driving performance.","To demonstrate this, the PID gains of the longitudinal controller in the TCP autonomous vehicle algorithm are tuned.","This causes the driving score in CARLA to increase from 73.21 to 77.38, with the results averaged over 16 driving scenarios.","Moreover, it was observed that the performance benefits were most apparent during challenging driving scenarios, such as during rain or night time, as the tuned controller led to a more assertive driving style.","These results demonstrate the value of developing both the neural network and feedback control policies of autonomous driving systems simultaneously, as this can be a simple and methodical way to improve autonomous driving system performance and robustness."],"url":"http://arxiv.org/abs/2402.05064v1","category":"eess.SY"}
{"created":"2024-02-07 18:14:55","title":"Critical behavior of a phase transition in the dynamics of interacting populations","abstract":"Many-variable differential equations with random coefficients provide powerful models for the dynamics of many interacting species in ecology. These models are known to exhibit a dynamical phase transition from a phase where population sizes reach a fixed point, to a phase where they fluctuate indefinitely. This transition has parallels with models developed in other fields, but also distinct features that stem from the requirement that the variables represent non-negative population sizes.   Abstract Here we provide a theory for the critical behavior close to the phase transition. We show that there are three different universality classes, depending on the distance from the critical point, and the migration rate which couples the system to its surroundings. We derive scaling relations for two parameters, the size of the temporal fluctuations, and the correlation timescale. We show that the temporal fluctuations grow continuously upon crossing the transition, and that timescales diverge near the transition (a critical slowing down). We define and calculate the corresponding critical exponents.","sentences":["Many-variable differential equations with random coefficients provide powerful models for the dynamics of many interacting species in ecology.","These models are known to exhibit a dynamical phase transition from a phase where population sizes reach a fixed point, to a phase where they fluctuate indefinitely.","This transition has parallels with models developed in other fields, but also distinct features that stem from the requirement that the variables represent non-negative population sizes.   ","Abstract Here we provide a theory for the critical behavior close to the phase transition.","We show that there are three different universality classes, depending on the distance from the critical point, and the migration rate which couples the system to its surroundings.","We derive scaling relations for two parameters, the size of the temporal fluctuations, and the correlation timescale.","We show that the temporal fluctuations grow continuously upon crossing the transition, and that timescales diverge near the transition (a critical slowing down).","We define and calculate the corresponding critical exponents."],"url":"http://arxiv.org/abs/2402.05063v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 18:13:04","title":"$H_{\\infty}$-Optimal Estimator Synthesis for Coupled Linear 2D PDEs using Convex Optimization","abstract":"It has recently been shown that, any suitably well-posed PDE in one or two spatial dimensions can be equivalently represented as a Partial Integral Equation (PIE), expressing the dynamics in terms of Partial Integral (PI) operators. Parameterizing storage functionals by PI operators as well, $L_2$-gain analysis of the PDE can then be posed as a linear operator inequality on PI operators, which can be solved using convex optimization. In this paper, we build on this result to derive a convex-optimization-based test for constructing an $H_{\\infty}$-optimal estimator for 2D PDEs, extending a similar result for 1D PDEs. In particular, we first show how a well-posed 2D PDE with infinite-dimensional outputs can be equivalently represented as a PIE, and we parameterize an associated Luenberger-type estimator by a 2D PI operator $\\mathcal{L}$. Parameterizing a storage functional for the resulting error dynamics by another PI operator $\\mathcal{P}$, we prove that the $H_{\\infty}$-norm of the error dynamics can be minimized by solving a linear operator inequality on PI operator variables $\\mathcal{P}$ and $\\mathcal{W}:=\\mathcal{P}\\mathcal{L}$. Finally, we derive an explicit expression for the inverse of the operator $\\mathcal{P}$, and propose a parameterization of variables $\\mathcal{P}$ and $\\mathcal{W}$ by matrices, posing the problem of finding an $H_{\\infty}$-optimal estimator gain $\\mathcal{L}=\\mathcal{P}^{-1}\\mathcal{W}$ as a convex optimization problem. We implement this test in the PIETOOLS software suite, and apply this software to construct an estimator for a 2D heat equation with state observations along the boundary.","sentences":["It has recently been shown that, any suitably well-posed PDE in one or two spatial dimensions can be equivalently represented as a Partial Integral Equation (PIE), expressing the dynamics in terms of Partial Integral (PI) operators.","Parameterizing storage functionals by PI operators as well, $L_2$-gain analysis of the PDE can then be posed as a linear operator inequality on PI operators, which can be solved using convex optimization.","In this paper, we build on this result to derive a convex-optimization-based test for constructing an $H_{\\infty}$-optimal estimator for 2D PDEs, extending a similar result for 1D PDEs.","In particular, we first show how a well-posed 2D PDE with infinite-dimensional outputs can be equivalently represented as a PIE, and we parameterize an associated Luenberger-type estimator by a 2D PI operator $\\mathcal{L}$. Parameterizing a storage functional for the resulting error dynamics by another PI operator $\\mathcal{P}$, we prove that the $H_{\\infty}$-norm of the error dynamics can be minimized by solving a linear operator inequality on PI operator variables $\\mathcal{P}$ and $\\mathcal{W}:=\\mathcal{P}\\mathcal{L}$. Finally, we derive an explicit expression for the inverse of the operator $\\mathcal{P}$, and propose a parameterization of variables $\\mathcal{P}$ and $\\mathcal{W}$ by matrices, posing the problem of finding an $H_{\\infty}$-optimal estimator gain $\\mathcal{L}=\\mathcal{P}^{-1}\\mathcal{W}$ as a convex optimization problem.","We implement this test in the PIETOOLS software suite, and apply this software to construct an estimator for a 2D heat equation with state observations along the boundary."],"url":"http://arxiv.org/abs/2402.05061v1","category":"math.OC"}
{"created":"2024-02-07 18:00:48","title":"Measuring Neutron Star Radius with second and third generation Gravitational Wave Detector Networks","abstract":"The next generation of ground-based interferometric gravitational wave detectors will observe mergers of black holes and neutron stars throughout cosmic time. A large number of the binary neutron star merger events will be observed with extreme high fidelity, and will provide stringent constraints on the equation of state of nuclear matter. In this paper, we investigate the systematic improvement in the measurability of the equation of state with increase in detector sensitivity by combining constraints obtained on the radius of a $1.4 \\, \\mathrm{M}_{\\odot}$ neutron star from a simulated source population. Since the measurability of the equation of state depends on its stiffness, we consider a range of realistic equations of state that span the current observational constraints. We show that a single 40km Cosmic Explorer detector can pin down the neutron star radius for a soft, medium and stiff equation of state to an accuracy of 10m within a decade, whereas the current generation of ground-based detectors like the Advanced LIGO-Virgo network would take $\\mathcal{O}(10^5)$ years to do so for a soft equation of state.","sentences":["The next generation of ground-based interferometric gravitational wave detectors will observe mergers of black holes and neutron stars throughout cosmic time.","A large number of the binary neutron star merger events will be observed with extreme high fidelity, and will provide stringent constraints on the equation of state of nuclear matter.","In this paper, we investigate the systematic improvement in the measurability of the equation of state with increase in detector sensitivity by combining constraints obtained on the radius of a $1.4 \\, \\mathrm{M}_{\\odot}$ neutron star from a simulated source population.","Since the measurability of the equation of state depends on its stiffness, we consider a range of realistic equations of state that span the current observational constraints.","We show that a single 40km Cosmic Explorer detector can pin down the neutron star radius for a soft, medium and stiff equation of state to an accuracy of 10m within a decade, whereas the current generation of ground-based detectors like the Advanced LIGO-Virgo network would take $\\mathcal{O}(10^5)$ years to do so for a soft equation of state."],"url":"http://arxiv.org/abs/2402.05056v1","category":"astro-ph.HE"}
{"created":"2024-02-07 17:57:03","title":"LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation","abstract":"3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.","sentences":["3D content creation has achieved significant progress in terms of both quality and speed.","Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training.","In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images.","Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering.","2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models.","Extensive experiments demonstrate the high fidelity and efficiency of our approach.","Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation."],"url":"http://arxiv.org/abs/2402.05054v1","category":"cs.CV"}
{"created":"2024-02-07 16:47:07","title":"Strong convexity-guided hyper-parameter optimization for flatter losses","abstract":"We propose a novel white-box approach to hyper-parameter optimization. Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness. Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss. By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion. Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime.","sentences":["We propose a novel white-box approach to hyper-parameter optimization.","Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness.","Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss.","By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion.","Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime."],"url":"http://arxiv.org/abs/2402.05025v1","category":"cs.LG"}
{"created":"2024-02-07 16:32:58","title":"A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?","abstract":"Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate. Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data.","sentences":["Automation is one of the cornerstones of contemporary material discovery.","Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space.","While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs).","However, existing work thus far has only explored LLMs for heuristic materials searches.","Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs.","In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space.","We take a sober, dispassionate stance in answering this question.","This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate.","Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data."],"url":"http://arxiv.org/abs/2402.05015v1","category":"cs.LG"}
{"created":"2024-02-07 16:32:02","title":"Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching","abstract":"Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \\textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching. Moreover, we design a loss function to further extract knowledge from the expert trajectories. Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets. Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM.","sentences":["Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs.","Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation.","To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one.","This significantly limits both the scale and efficacy of the condensed graph.","In this paper, we make the first attempt toward \\textit{lossless graph condensation} by bridging the previously neglected supervision signals.","Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching.","Moreover, we design a loss function to further extract knowledge from the expert trajectories.","Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets.","Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM."],"url":"http://arxiv.org/abs/2402.05011v1","category":"cs.LG"}
{"created":"2024-02-07 16:10:13","title":"Challenges and opportunities in the supervised learning of quantum circuit outputs","abstract":"Recently, deep neural networks have proven capable of predicting some output properties of relevant random quantum circuits, indicating a strategy to emulate quantum computers alternative to direct simulation methods such as, e.g., tensor-network methods. However, the reach of this alternative strategy is not yet clear. Here we investigate if and to what extent neural networks can learn to predict the output expectation values of circuits often employed in variational quantum algorithms, namely, circuits formed by layers of CNOT gates alternated with random single-qubit rotations. On the one hand, we find that the computational cost of supervised learning scales exponentially with the inter-layer variance of the random angles. This allows entering a regime where quantum computers can easily outperform classical neural networks. On the other hand, circuits featuring only inter-qubit angle variations are easily emulated. In fact, thanks to a suitable scalable design, neural networks accurately predict the output of larger and deeper circuits than those used for training, even reaching circuit sizes which turn out to be intractable for the most common simulation libraries, considering both state-vector and tensor-network algorithms. We provide a repository of testing data in this regime, to be used for future benchmarking of quantum devices and novel classical algorithms.","sentences":["Recently, deep neural networks have proven capable of predicting some output properties of relevant random quantum circuits, indicating a strategy to emulate quantum computers alternative to direct simulation methods such as, e.g., tensor-network methods.","However, the reach of this alternative strategy is not yet clear.","Here we investigate if and to what extent neural networks can learn to predict the output expectation values of circuits often employed in variational quantum algorithms, namely, circuits formed by layers of CNOT gates alternated with random single-qubit rotations.","On the one hand, we find that the computational cost of supervised learning scales exponentially with the inter-layer variance of the random angles.","This allows entering a regime where quantum computers can easily outperform classical neural networks.","On the other hand, circuits featuring only inter-qubit angle variations are easily emulated.","In fact, thanks to a suitable scalable design, neural networks accurately predict the output of larger and deeper circuits than those used for training, even reaching circuit sizes which turn out to be intractable for the most common simulation libraries, considering both state-vector and tensor-network algorithms.","We provide a repository of testing data in this regime, to be used for future benchmarking of quantum devices and novel classical algorithms."],"url":"http://arxiv.org/abs/2402.04992v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-07 16:08:36","title":"The possibility of panspermia in the deep cosmos by means of the planetary dust grains","abstract":"By obtaining the assumption that planetary dust particles can escape from the gravitational attraction of a planet, we consider the possibility for the dust grains to leave the star's system by means of the radiation pressure. By taking the typical dust parameters into account, we consider their dynamics and show that they can reach the deep cosmos, taking part in panspermia. It has been shown that, during $5$ billion years, the dust grains will reach $10^5$ stellar systems, and by taking the Drake equation into account, it has been shown that the whole galaxy will be full of planetary dust particles.","sentences":["By obtaining the assumption that planetary dust particles can escape from the gravitational attraction of a planet, we consider the possibility for the dust grains to leave the star's system by means of the radiation pressure.","By taking the typical dust parameters into account, we consider their dynamics and show that they can reach the deep cosmos, taking part in panspermia.","It has been shown that, during $5$ billion years, the dust grains will reach $10^5$ stellar systems, and by taking the Drake equation into account, it has been shown that the whole galaxy will be full of planetary dust particles."],"url":"http://arxiv.org/abs/2402.04990v1","category":"astro-ph.EP"}
{"created":"2024-02-07 15:53:49","title":"Existence of infinitely many solutions for a critical Hartree type equation with potential: local Poho\u017eaev identities methods","abstract":"This paper deals with the following equation $$-\\Delta u =K(|x'|, x'')\\Big(|x|^{-\\alpha}\\ast (K(|x'|, x'')|u|^{2^{\\ast}_{\\alpha}})\\Big) |u|^{2^{\\ast}_{\\alpha}-2}u\\quad\\mbox{in}\\ \\mathbb{R}^N,$$ where $N\\geq5$, $\\alpha>5-\\frac{6}{N-2}$, $2^{\\ast}_{\\alpha}=\\frac{2N-\\alpha}{N-2}$ is the so-called upper critical exponent in the Hardy-Littlewood-Sobolev inequality and $K(|x'|, x'')$, where $(x',x'')\\in \\mathbb{R}^2\\times\\mathbb{R}^{N-2}$, is bounded and nonnegative. Under proper assumptions on the potential function $K$, we obtain the existence of infinitely many solutions for the nonlocal critical equation by using a finite dimensional reduction argument and local Poho\\v{z}aev identities. It is a remarkable fact that the order of the Riesz potential influences the existence/non-existence of solutions.","sentences":["This paper deals with the following equation $$-\\Delta u =K(|x'|, x'')\\Big(|x|^{-\\alpha}\\ast (K(|x'|, x'')|u|^{2^{\\ast}_{\\alpha}})\\Big) |u|^{2^{\\ast}_{\\alpha}-2}u\\quad\\mbox{in}\\ \\mathbb{R}^N,$$ where $N\\geq5$, $\\alpha>5-\\frac{6}{N-2}$, $2^{\\ast}_{\\alpha}=\\frac{2N-\\alpha}{N-2}$ is the so-called upper critical exponent in the Hardy-Littlewood-Sobolev inequality and $K(|x'|, x'')$, where $(x',x'')\\in \\mathbb{R}^2\\times\\mathbb{R}^{N-2}$, is bounded and nonnegative.","Under proper assumptions on the potential function $K$, we obtain the existence of infinitely many solutions for the nonlocal critical equation by using a finite dimensional reduction argument and local Poho\\v{z}aev identities.","It is a remarkable fact that the order of the Riesz potential influences the existence/non-existence of solutions."],"url":"http://arxiv.org/abs/2402.04974v1","category":"math.AP"}
{"created":"2024-02-07 15:44:04","title":"On the Cahn-Hilliard equation with kinetic rate dependent dynamic boundary conditions and non-smooth potentials: Well-posedness and asymptotic limits","abstract":"We consider a class of Cahn-Hilliard equation with kinetic rate dependent dynamic boundary conditions that describe possible short-range interactions between the binary mixture and the solid boundary. In the presence of surface diffusion on the boundary, the initial boundary value problem can be viewed as a transmission problem consisting of Cahn-Hilliard type equations both in the bulk and on the boundary. We first prove existence, uniqueness and continuous dependence of global weak solutions. In the construction of solutions, an explicit convergence rate in terms of the parameter for the Yosida approximation is established. Under some additional assumptions, we also obtain the existence and uniqueness of global strong solutions. Next, we study the asymptotic limit as the coefficient of the boundary diffusion goes to zero and show that the limit problem with a forward-backward dynamic boundary condition is well-posed in a suitable weak formulation. Besides, we investigate the asymptotic limit as the kinetic rate tends to zero and infinity, respectively. Our results are valid for a general class of bulk and boundary potentials with double-well structure, including the physically relevant logarithmic potential and the non-smooth double-obstacle potential.","sentences":["We consider a class of Cahn-Hilliard equation with kinetic rate dependent dynamic boundary conditions that describe possible short-range interactions between the binary mixture and the solid boundary.","In the presence of surface diffusion on the boundary, the initial boundary value problem can be viewed as a transmission problem consisting of Cahn-Hilliard type equations both in the bulk and on the boundary.","We first prove existence, uniqueness and continuous dependence of global weak solutions.","In the construction of solutions, an explicit convergence rate in terms of the parameter for the Yosida approximation is established.","Under some additional assumptions, we also obtain the existence and uniqueness of global strong solutions.","Next, we study the asymptotic limit as the coefficient of the boundary diffusion goes to zero and show that the limit problem with a forward-backward dynamic boundary condition is well-posed in a suitable weak formulation.","Besides, we investigate the asymptotic limit as the kinetic rate tends to zero and infinity, respectively.","Our results are valid for a general class of bulk and boundary potentials with double-well structure, including the physically relevant logarithmic potential and the non-smooth double-obstacle potential."],"url":"http://arxiv.org/abs/2402.04965v1","category":"math.AP"}
{"created":"2024-02-07 15:41:24","title":"Non-relativistic trace anomaly and equation of state in dense fermionic matter","abstract":"We theoretically investigate a non-relativistic trace anomaly and its impact on the low-temperature equation of state in spatially one-dimensional three-component fermionic systems with a three-body interaction, which exhibit a non-trivial three-body crossover from a bound trimer gas to dense fermionic matter with increasing density. By applying the $G$-matrix approach to the three-body interaction, we obtain the analytical expression for the ground-state equation of state relevant to the high-density degenerate regime and thereby address how the three-body contact or, equivalently, the trace anomaly emerges. The analytical results are compared with the recent quantum Monte Carlo data. Our study of the trace anomaly and the sound speed could have some relevance to the physics of hadron-quark crossover in compact stars.","sentences":["We theoretically investigate a non-relativistic trace anomaly and its impact on the low-temperature equation of state in spatially one-dimensional three-component fermionic systems with a three-body interaction, which exhibit a non-trivial three-body crossover from a bound trimer gas to dense fermionic matter with increasing density.","By applying the $G$-matrix approach to the three-body interaction, we obtain the analytical expression for the ground-state equation of state relevant to the high-density degenerate regime and thereby address how the three-body contact or, equivalently, the trace anomaly emerges.","The analytical results are compared with the recent quantum Monte Carlo data.","Our study of the trace anomaly and the sound speed could have some relevance to the physics of hadron-quark crossover in compact stars."],"url":"http://arxiv.org/abs/2402.04960v1","category":"hep-ph"}
{"created":"2024-02-07 15:39:14","title":"The fractional Hopf differential and a weak formulation of stationarity for the half Dirichlet energy","abstract":"We obtain a weak formulation of the stationarity condition for the half Dirichlet energy, which can be expressed in terms of a fractional analogous to the Hopf differential. As an application we show that conformal harmonic maps from the disc are precisely the harmonic extensions of stationary points of the half Dirichlet energy on the circle. We also derive a Noether theorem and a Pohozaev identity for stationary points of the half Dirichlet energy.","sentences":["We obtain a weak formulation of the stationarity condition for the half Dirichlet energy, which can be expressed in terms of a fractional analogous to the Hopf differential.","As an application we show that conformal harmonic maps from the disc are precisely the harmonic extensions of stationary points of the half Dirichlet energy on the circle.","We also derive a Noether theorem and a Pohozaev identity for stationary points of the half Dirichlet energy."],"url":"http://arxiv.org/abs/2402.04956v1","category":"math.AP"}
{"created":"2024-02-07 15:36:15","title":"Gradient continuity for the parabolic $(1,\\,p)$-Laplace equation under the subcritical case","abstract":"This paper is concerned with the gradient continuity for the parabolic $(1,\\,p)$-Laplace equation. In the supercritical case $\\frac{2n}{n+2}<p<\\infty$, where $n\\ge 2$ denotes the space dimension, this gradient regularity result has been proved recently by the author. In this paper, we would like to prove that the same regularity holds even for the subcritical case $1<p\\le \\frac{2n}{n+2}$ with $n\\ge 3$, on the condition that a weak solution admits the $L^{s}$-integrability with $s>n(2-p)/p$. The gradient continuity is proved, similarly to the supercritical case, once the local gradient bounds of solutions are verified. Hence, this paper mainly aims to show the local boundedness of a solution and its gradient by Moser's iteration. The proof is completed by considering a parabolic approximate problem, and showing a priori gradient estimates of a bounded weak solution to the relaxed equation.","sentences":["This paper is concerned with the gradient continuity for the parabolic $(1,\\,p)$-Laplace equation.","In the supercritical case $\\frac{2n}{n+2}<p<\\infty$, where $n\\ge 2$ denotes the space dimension, this gradient regularity result has been proved recently by the author.","In this paper, we would like to prove that the same regularity holds even for the subcritical case $1<p\\le \\frac{2n}{n+2}$ with $n\\ge 3$, on the condition that a weak solution admits the $L^{s}$-integrability with $s>n(2-p)/p$.","The gradient continuity is proved, similarly to the supercritical case, once the local gradient bounds of solutions are verified.","Hence, this paper mainly aims to show the local boundedness of a solution and its gradient by Moser's iteration.","The proof is completed by considering a parabolic approximate problem, and showing a priori gradient estimates of a bounded weak solution to the relaxed equation."],"url":"http://arxiv.org/abs/2402.04951v1","category":"math.AP"}
{"created":"2024-02-07 15:34:28","title":"Global hypoellipticity for a class of first order evolution equations on compact Lie groups","abstract":"We present necessary and sufficient conditions to have global hypoellipticity for a class of complex-valued coefficient first order evolution equations defined on $\\mathbb{T}^1 \\times G$, where $G$ is a compact Lie group. First, we show that the global hypoellipticity of the constant coefficient operator related to this operator is a necessary condition, but not a sufficient condition. Under certain hypothesis, we show that the global hypoellipticity of this class of operator is completely characterized by Nirenberg-Treves' condition $(\\mathcal{P})$.","sentences":["We present necessary and sufficient conditions to have global hypoellipticity for a class of complex-valued coefficient first order evolution equations defined on $\\mathbb{T}^1 \\times G$, where $G$ is a compact Lie group.","First, we show that the global hypoellipticity of the constant coefficient operator related to this operator is a necessary condition, but not a sufficient condition.","Under certain hypothesis, we show that the global hypoellipticity of this class of operator is completely characterized by Nirenberg-Treves' condition $(\\mathcal{P})$."],"url":"http://arxiv.org/abs/2402.04950v1","category":"math.AP"}
{"created":"2024-02-07 15:25:20","title":"Elastic Analysis of Augmented Curves and Constrained Surfaces","abstract":"The square root velocity transformation is crucial for efficiently employing the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks.","sentences":["The square root velocity transformation is crucial for efficiently employing the elastic approach in functional and shape data analysis of curves.","We study fundamental geometric properties of curves under this transformation.","Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks."],"url":"http://arxiv.org/abs/2402.04944v1","category":"math.DG"}
{"created":"2024-02-07 15:04:01","title":"Growth in the universal cover under large simplicial volume","abstract":"Consider a closed manifold $M$ with two Riemannian metrics: one hyperbolic metric, and one other metric $g$. What hypotheses on $g$ guarantee that for a given radius $r$, there are balls of radius $r$ in the universal cover of $(M, g)$ with greather-than-hyperbolic volumes? We show that this conclusion holds for all $r \\geq 1$ if $(\\mathrm{Vol} (M, g))^2$ is less than a small constant times the hyperbolic volume of $M$. This strengthens a theorem of Sabourau and is partial progress toward a conjecture of Guth.","sentences":["Consider a closed manifold $M$ with two Riemannian metrics: one hyperbolic metric, and one other metric $g$. What hypotheses on $g$ guarantee that for a given radius $r$, there are balls of radius $r$ in the universal cover of $(M, g)$ with greather-than-hyperbolic volumes?","We show that this conclusion holds for all $r \\geq 1$ if $(\\mathrm{Vol} (M, g))^2$ is less than a small constant times the hyperbolic volume of $M$. This strengthens a theorem of Sabourau and is partial progress toward a conjecture of Guth."],"url":"http://arxiv.org/abs/2402.04932v1","category":"math.DG"}
{"created":"2024-02-07 14:47:23","title":"Jet transport coefficients by elastic and radiative scatterings in the strongly interacting quark-gluon plasma","abstract":"We extend the investigation on jet transport coefficients within the effective Dynamical QuasiParticle Model (DQPM) -- constructed for the description of non-perturbative QCD phenomena of the strongly interacting quark-gluon plasma (sQGP) in line with the lattice QCD equation-of-state -- by accounting for inelastic $2\\to 3$ reactions with gluon radiation additionally to the elastic scattering of partons. The elastic and inelastic reactions are calculated explicitly within leading-order Feynman diagrams with effective propagators and vertices from the DQPM by accounting for all channels and their interferences. We present the results for the jet transport coefficients such as the transverse momentum transfer squared $\\hat{q}$ per unit length as well as the energy loss $\\Delta E = dE/dx$ per unit length in the sQGP and investigate their dependence on the temperature $T$ and momentum of the jet parton depending on the choice of the strong coupling constant $\\alpha_s$ in thermal, jet parton and radiative vertices. For the latter we consider different scenarios used in the literature and find a very strong dependence of $\\hat q$ and $\\Delta E$ on the choice of $\\alpha_s$. Moreover, we explore the relation of $\\hat{q}/T^3$ to the ratio of specific shear viscosity to entropy density $\\eta/s$ and show that the ratio $T^3/\\hat{q}$ to $\\eta/s$ has a strong $T$ dependence -- especially when approaching to $T_c$ -- on the choice of $\\alpha_s$ in scattering vertices.","sentences":["We extend the investigation on jet transport coefficients within the effective Dynamical QuasiParticle Model (DQPM) -- constructed for the description of non-perturbative QCD phenomena of the strongly interacting quark-gluon plasma (sQGP) in line with the lattice QCD equation-of-state -- by accounting for inelastic $2\\to 3$ reactions with gluon radiation additionally to the elastic scattering of partons.","The elastic and inelastic reactions are calculated explicitly within leading-order Feynman diagrams with effective propagators and vertices from the DQPM by accounting for all channels and their interferences.","We present the results for the jet transport coefficients such as the transverse momentum transfer squared $\\hat{q}$ per unit length as well as the energy loss $\\Delta E = dE/dx$ per unit length in the sQGP and investigate their dependence on the temperature $T$ and momentum of the jet parton depending on the choice of the strong coupling constant $\\alpha_s$ in thermal, jet parton and radiative vertices.","For the latter we consider different scenarios used in the literature and find a very strong dependence of $\\hat q$ and $\\Delta E$ on the choice of $\\alpha_s$. Moreover, we explore the relation of $\\hat{q}/T^3$ to the ratio of specific shear viscosity to entropy density $\\eta/s$ and show that the ratio $T^3/\\hat{q}$ to $\\eta/s$ has a strong $T$ dependence -- especially when approaching to $T_c$ -- on the choice of $\\alpha_s$ in scattering vertices."],"url":"http://arxiv.org/abs/2402.04923v1","category":"hep-ph"}
{"created":"2024-02-07 14:44:50","title":"Modeling and Calibration of Gaia, Hipparcos, and Tycho-2 astrometric data for the detection of dark companions","abstract":"Hidden within the Gaia satellite's multiple data releases lies a valuable cache of dark companions. To facilitate the efficient and reliable detection of these companions via combined analyses involving Gaia, Hipparcos, and Tycho-2 catalogs, we introduce an astrometric modeling framework. This method incorporates analytical least square minimization and nonlinear parameter optimization techniques to a set of common calibration sources across the different space-based astrometric catalogues. This enables us to discern the error inflation, astrometric jitter, differential parallax zero-point, and frame rotation of various catalogues relative to Gaia DR3. Our findings yield the most precise Gaia DR2 calibration parameters to date, revealing notable dependencies on magnitude and color. Intriguingly, we identify sub-mas frame rotation between Gaia DR1 and DR3, along with an estimated astrometric jitter of 2.16 mas for the revised Hipparcos catalog. In a thorough comparative analysis with previous studies, we offer recommendations on calibrating and utilizing different catalogs for companion detection. Furthermore, we provide a user-friendly pipeline (https://github.com/ruiyicheng/Download_HIP_Gaia_GOST) for catalog download and bias correction, enhancing accessibility and usability within the scientific community.","sentences":["Hidden within the Gaia satellite's multiple data releases lies a valuable cache of dark companions.","To facilitate the efficient and reliable detection of these companions via combined analyses involving Gaia, Hipparcos, and Tycho-2 catalogs, we introduce an astrometric modeling framework.","This method incorporates analytical least square minimization and nonlinear parameter optimization techniques to a set of common calibration sources across the different space-based astrometric catalogues.","This enables us to discern the error inflation, astrometric jitter, differential parallax zero-point, and frame rotation of various catalogues relative to Gaia DR3.","Our findings yield the most precise Gaia DR2 calibration parameters to date, revealing notable dependencies on magnitude and color.","Intriguingly, we identify sub-mas frame rotation between Gaia DR1 and DR3, along with an estimated astrometric jitter of 2.16 mas for the revised Hipparcos catalog.","In a thorough comparative analysis with previous studies, we offer recommendations on calibrating and utilizing different catalogs for companion detection.","Furthermore, we provide a user-friendly pipeline (https://github.com/ruiyicheng/Download_HIP_Gaia_GOST) for catalog download and bias correction, enhancing accessibility and usability within the scientific community."],"url":"http://arxiv.org/abs/2402.04919v1","category":"astro-ph.SR"}
{"created":"2024-02-07 14:39:11","title":"Towards Biologically Plausible and Private Gene Expression Data Generation","abstract":"Generative models trained with Differential Privacy (DP) are becoming increasingly prominent in the creation of synthetic data for downstream applications. Existing literature, however, primarily focuses on basic benchmarking datasets and tends to report promising results only for elementary metrics and relatively simple data distributions. In this paper, we initiate a systematic analysis of how DP generative models perform in their natural application scenarios, specifically focusing on real-world gene expression data. We conduct a comprehensive analysis of five representative DP generation methods, examining them from various angles, such as downstream utility, statistical properties, and biological plausibility. Our extensive evaluation illuminates the unique characteristics of each DP generation method, offering critical insights into the strengths and weaknesses of each approach, and uncovering intriguing possibilities for future developments. Perhaps surprisingly, our analysis reveals that most methods are capable of achieving seemingly reasonable downstream utility, according to the standard evaluation metrics considered in existing literature. Nevertheless, we find that none of the DP methods are able to accurately capture the biological characteristics of the real dataset. This observation suggests a potential over-optimistic assessment of current methodologies in this field and underscores a pressing need for future enhancements in model design.","sentences":["Generative models trained with Differential Privacy (DP) are becoming increasingly prominent in the creation of synthetic data for downstream applications.","Existing literature, however, primarily focuses on basic benchmarking datasets and tends to report promising results only for elementary metrics and relatively simple data distributions.","In this paper, we initiate a systematic analysis of how DP generative models perform in their natural application scenarios, specifically focusing on real-world gene expression data.","We conduct a comprehensive analysis of five representative DP generation methods, examining them from various angles, such as downstream utility, statistical properties, and biological plausibility.","Our extensive evaluation illuminates the unique characteristics of each DP generation method, offering critical insights into the strengths and weaknesses of each approach, and uncovering intriguing possibilities for future developments.","Perhaps surprisingly, our analysis reveals that most methods are capable of achieving seemingly reasonable downstream utility, according to the standard evaluation metrics considered in existing literature.","Nevertheless, we find that none of the DP methods are able to accurately capture the biological characteristics of the real dataset.","This observation suggests a potential over-optimistic assessment of current methodologies in this field and underscores a pressing need for future enhancements in model design."],"url":"http://arxiv.org/abs/2402.04912v1","category":"cs.CR"}
{"created":"2024-02-07 18:56:06","title":"Millimeter-scale freestanding superconducting infinite-layer nickelate membranes","abstract":"Progress in the study of infinite-layer nickelates has always been highly linked to materials advances. In particular, the recent development of superconductivity via hole-doping was predicated on the controlled synthesis of Ni in a very high oxidation state, and subsequent topotactic reduction to a very low oxidation state, currently limited to epitaxial thin films. Here we demonstrate a process to combine these steps with a heterostructure which includes an epitaxial soluble buffer layer, enabling the release of freestanding membranes of (Nd,Sr)NiO2 encapsulated in SrTiO3, which serves as a protective layer. The membranes have comparable structural and electronic properties to that of optimized thin films, and range in lateral dimensions from millimeters to ~100 micron fragments, depending on the degree of strain released with respect to the initial substrate. The changes in the superconducting transition temperature associated with membrane release are quite similar to those reported for substrate and pressure variations, suggestive of a common underlying mechanism. These membranes structures should provide a versatile platform for a range of experimental studies and devices free from substrate constraints.","sentences":["Progress in the study of infinite-layer nickelates has always been highly linked to materials advances.","In particular, the recent development of superconductivity via hole-doping was predicated on the controlled synthesis of Ni in a very high oxidation state, and subsequent topotactic reduction to a very low oxidation state, currently limited to epitaxial thin films.","Here we demonstrate a process to combine these steps with a heterostructure which includes an epitaxial soluble buffer layer, enabling the release of freestanding membranes of (Nd,Sr)NiO2 encapsulated in SrTiO3, which serves as a protective layer.","The membranes have comparable structural and electronic properties to that of optimized thin films, and range in lateral dimensions from millimeters to ~100 micron fragments, depending on the degree of strain released with respect to the initial substrate.","The changes in the superconducting transition temperature associated with membrane release are quite similar to those reported for substrate and pressure variations, suggestive of a common underlying mechanism.","These membranes structures should provide a versatile platform for a range of experimental studies and devices free from substrate constraints."],"url":"http://arxiv.org/abs/2402.05104v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 18:55:15","title":"Large deviations for dynamical Schr\u00f6dinger problems","abstract":"We establish large deviations for dynamical Schr\\\"{o}dinger problems driven by perturbed Brownian motions when the noise parameter tends to zero. Our results show that Schr\\\"{o}dinger bridges charge exponentially small masses outside of the support of the limiting law that agrees with the optimal solution to the dynamical Monge-Kantorovich optimal transport problem. Our proofs build on mixture representations of Schr\\\"{o}dinger bridges and establishing exponential continuity of Brownian bridges with respect to the initial and terminal points.","sentences":["We establish large deviations for dynamical Schr\\\"{o}dinger problems driven by perturbed Brownian motions when the noise parameter tends to zero.","Our results show that Schr\\\"{o}dinger bridges charge exponentially small masses outside of the support of the limiting law that agrees with the optimal solution to the dynamical Monge-Kantorovich optimal transport problem.","Our proofs build on mixture representations of Schr\\\"{o}dinger bridges and establishing exponential continuity of Brownian bridges with respect to the initial and terminal points."],"url":"http://arxiv.org/abs/2402.05100v1","category":"math.PR"}
{"created":"2024-02-07 18:37:17","title":"Non-Markovian Quantum Control via Model Maximum Likelihood Estimation and Reinforcement Learning","abstract":"Reinforcement Learning (RL) techniques have been increasingly applied in optimizing control systems. However, their application in quantum systems is hampered by the challenge of performing closed-loop control due to the difficulty in measuring these systems. This often leads to reliance on assumed models, introducing model bias, a problem that is exacerbated in open quantum dynamics where Markovian approximations are not valid. To address these challenges, we propose a novel approach that incorporates the non-Markovian nature of the environment into a low-dimensional effective reservoir. By initially employing a series of measurements as a 'dataset', we utilize machine learning techniques to learn the effective quantum dynamics more efficiently than traditional tomographic methods. Our methodology aims to demonstrates that by integrating reinforcement learning with model learning, it is possible to devise control policies and models that can counteract decoherence in a spin-boson system. This approach may not only mitigates the issues of model bias but also provides a more accurate representation of quantum dynamics, paving the way for more effective quantum control strategies.","sentences":["Reinforcement Learning (RL) techniques have been increasingly applied in optimizing control systems.","However, their application in quantum systems is hampered by the challenge of performing closed-loop control due to the difficulty in measuring these systems.","This often leads to reliance on assumed models, introducing model bias, a problem that is exacerbated in open quantum dynamics where Markovian approximations are not valid.","To address these challenges, we propose a novel approach that incorporates the non-Markovian nature of the environment into a low-dimensional effective reservoir.","By initially employing a series of measurements as a 'dataset', we utilize machine learning techniques to learn the effective quantum dynamics more efficiently than traditional tomographic methods.","Our methodology aims to demonstrates that by integrating reinforcement learning with model learning, it is possible to devise control policies and models that can counteract decoherence in a spin-boson system.","This approach may not only mitigates the issues of model bias but also provides a more accurate representation of quantum dynamics, paving the way for more effective quantum control strategies."],"url":"http://arxiv.org/abs/2402.05084v1","category":"quant-ph"}
{"created":"2024-02-07 18:22:41","title":"Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity","abstract":"We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\\rho$-cohypomonotonicity or admitting a solution to the $\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\\rho>0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate value of $\\rho$ no larger than $\\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\\rho < \\frac{1}{L}$. The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\\u{\\i}-Mann (KM) iterations. We also provide algorithms and complexity guarantees in the stochastic case with the same range on $\\rho$. Our main insight for the improvements in the convergence analyses is to harness the recently proposed \"conic nonexpansiveness\" property of operators. As byproducts, we provide a refined analysis for inexact Halpern iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator.","sentences":["We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\\rho$-cohypomonotonicity or admitting a solution to the $\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\\rho>0$ correspond to a greater degree of nonconvexity.","These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail.","It has been conjectured that first-order methods can tolerate value of $\\rho$ no larger than $\\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\\rho < \\frac{1}{L}$.","The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\\u{\\i}-Mann (KM) iterations.","We also provide algorithms and complexity guarantees in the stochastic case with the same range on $\\rho$. Our main insight for the improvements in the convergence analyses is to harness the recently proposed \"conic nonexpansiveness\" property of operators.","As byproducts, we provide a refined analysis for inexact Halpern iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator."],"url":"http://arxiv.org/abs/2402.05071v1","category":"math.OC"}
{"created":"2024-02-07 17:53:21","title":"Quantum circuit for multi-qubit Toffoli gate with optimal resource","abstract":"Resource consumption is an important issue in quantum information processing, particularly during the present NISQ era. In this paper, we investigate resource optimization of implementing multiple controlled operations, which are fundamental building blocks in the field of quantum computing and quantum simulation. We design new quantum circuits for the $n$-Toffoli gate and general multi-controlled unitary, which have only $O(\\log n)$-depth and $O(n)$-size, and only require $1$ ancillary qubit. To achieve these results, we explore the potential of ancillary qubits and discover a method to create new conditional clean qubits from existed ancillary qubits. These techniques can also be utilized to construct an efficient quantum circuit for incrementor, leading to an implementation of multi-qubit Toffoli gate with a depth of $O(\\log^2n)$ and size of $O(n)$ without any ancillary qubits. Furthermore, we explore the power of ancillary qubits from the perspective of resource theory. We demonstrate that without the assistance of ancillary qubit, any quantum circuit implementation of multi-qubit Toffoli gate must employ exponential precision gates. This finding indicates a significant disparity in computational power of quantum circuits between using and not using ancillary qubits. Additionally, we discuss the comparison of the power of ancillary qubits and extra energy levels in quantum circuit design.","sentences":["Resource consumption is an important issue in quantum information processing, particularly during the present NISQ era.","In this paper, we investigate resource optimization of implementing multiple controlled operations, which are fundamental building blocks in the field of quantum computing and quantum simulation.","We design new quantum circuits for the $n$-Toffoli gate and general multi-controlled unitary, which have only $O(\\log n)$-depth and $O(n)$-size, and only require $1$ ancillary qubit.","To achieve these results, we explore the potential of ancillary qubits and discover a method to create new conditional clean qubits from existed ancillary qubits.","These techniques can also be utilized to construct an efficient quantum circuit for incrementor, leading to an implementation of multi-qubit Toffoli gate with a depth of $O(\\log^2n)$ and size of $O(n)$ without any ancillary qubits.","Furthermore, we explore the power of ancillary qubits from the perspective of resource theory.","We demonstrate that without the assistance of ancillary qubit, any quantum circuit implementation of multi-qubit Toffoli gate must employ exponential precision gates.","This finding indicates a significant disparity in computational power of quantum circuits between using and not using ancillary qubits.","Additionally, we discuss the comparison of the power of ancillary qubits and extra energy levels in quantum circuit design."],"url":"http://arxiv.org/abs/2402.05053v1","category":"quant-ph"}
{"created":"2024-02-07 17:25:44","title":"Non-reversible lifts of reversible diffusion processes and relaxation times","abstract":"We propose a new concept of lifts of reversible diffusion processes and show that various well-known non-reversible Markov processes arising in applications are lifts in this sense of simple reversible diffusions. Furthermore, we introduce a concept of non-asymptotic relaxation times and show that these can at most be reduced by a square root through lifting, generalising a related result in discrete time. Finally, we demonstrate how the recently developed approach to quantitative hypocoercivity based on space-time Poincar\\'e inequalities can be rephrased and simplified in the language of lifts and how it can be applied to find optimal lifts.","sentences":["We propose a new concept of lifts of reversible diffusion processes and show that various well-known non-reversible Markov processes arising in applications are lifts in this sense of simple reversible diffusions.","Furthermore, we introduce a concept of non-asymptotic relaxation times and show that these can at most be reduced by a square root through lifting, generalising a related result in discrete time.","Finally, we demonstrate how the recently developed approach to quantitative hypocoercivity based on space-time Poincar\\'e inequalities can be rephrased and simplified in the language of lifts and how it can be applied to find optimal lifts."],"url":"http://arxiv.org/abs/2402.05041v1","category":"math.PR"}
{"created":"2024-02-07 17:17:25","title":"Smooth real-time motion planning based on a cascade dual-quaternion screw-geometry MPC","abstract":"This paper investigates the tracking problem of a smooth coordinate-invariant trajectory using dual quaternion algebra. The proposed architecture consists of a cascade structure in which the outer-loop MPC performs real-time smoothing of the manipulator's end-effector twist while an inner-loop kinematic controller ensures tracking of the instantaneous desired end-effector pose. Experiments on a $7$-DoF Franka Emika Panda robotic manipulator validate the proposed method demonstrating its application to constraint the robot twists, accelerations and jerks within prescribed bounds.","sentences":["This paper investigates the tracking problem of a smooth coordinate-invariant trajectory using dual quaternion algebra.","The proposed architecture consists of a cascade structure in which the outer-loop MPC performs real-time smoothing of the manipulator's end-effector twist while an inner-loop kinematic controller ensures tracking of the instantaneous desired end-effector pose.","Experiments on a $7$-DoF Franka Emika Panda robotic manipulator validate the proposed method demonstrating its application to constraint the robot twists, accelerations and jerks within prescribed bounds."],"url":"http://arxiv.org/abs/2402.05037v1","category":"cs.RO"}
{"created":"2024-02-07 17:07:41","title":"Simulated Overparameterization","abstract":"In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called \"majority kernels\", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and models, even outperforming strong baselines such as combinatorial optimization methods based on submodular optimization.","sentences":["In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP).","SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models.","SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference.","Building upon this framework, we present a novel, architecture agnostic algorithm called \"majority kernels\", which seamlessly integrates with predominant architectures, including Transformer models.","Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks.","Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time.","The proposed approach shows strong performance on a wide variety of datasets and models, even outperforming strong baselines such as combinatorial optimization methods based on submodular optimization."],"url":"http://arxiv.org/abs/2402.05033v1","category":"cs.LG"}
{"created":"2024-02-07 16:52:58","title":"Characterization and Optimization of Cryogenic Pure CsI Detector for CLOVERS Experiment","abstract":"In this study, we conducted a comprehensive characterization and optimization of a cryogenic pure CsI (pCsI) detector. Achieving a notable light yield of \\SI{35.2}{PE/\\keV_{ee}} and a world-leading energy resolution of \\SI{6.9}{\\%} at \\SI{60}{\\keV}, we utilized a \\SI{2}{\\centi\\metre} cubic crystal coupled with a HAMAMATSU R11065 photomultiplier tube (PMT). Additionally, we measured the scintillation decay time of pCsI, which proved to be significantly faster than that of CsI(Na) at room temperature. Furthermore, we investigated the impact of temperature, surface treatment, and crystal shape on the light yield. Notably, the light yield peaked at approximately \\SI{20}{\\K} and remained stable within the range of \\SI{70}-\\SI{100}{\\K}. We observed that the light yield of polished crystals was approximately 1.5 times greater than that of ground crystals, while the crystal shape exhibited minimal influence on the light yield. These results are crucial for the design of the \\SI{12}{\\kg} pCsI detector for the future CLOVERS (Coherent eLastic neutrinO(V)-nucleus scattERing at China Spallation Neutron Source (CSNS)) experiment.","sentences":["In this study, we conducted a comprehensive characterization and optimization of a cryogenic pure CsI (pCsI) detector.","Achieving a notable light yield of \\SI{35.2}{PE/\\keV_{ee}} and a world-leading energy resolution of \\SI{6.9}{\\%} at \\SI{60}{\\keV}, we utilized a \\SI{2}{\\centi\\metre} cubic crystal coupled with a HAMAMATSU R11065 photomultiplier tube (PMT).","Additionally, we measured the scintillation decay time of pCsI, which proved to be significantly faster than that of CsI(Na) at room temperature.","Furthermore, we investigated the impact of temperature, surface treatment, and crystal shape on the light yield.","Notably, the light yield peaked at approximately \\SI{20}{\\K} and remained stable within the range of \\SI{70}-\\SI{100}{\\K}.","We observed that the light yield of polished crystals was approximately 1.5 times greater than that of ground crystals, while the crystal shape exhibited minimal influence on the light yield.","These results are crucial for the design of the \\SI{12}{\\kg} pCsI detector for the future CLOVERS (Coherent eLastic neutrinO(V)-nucleus scattERing at China Spallation Neutron Source (CSNS))","experiment."],"url":"http://arxiv.org/abs/2402.05026v2","category":"physics.ins-det"}
{"created":"2024-02-07 16:34:10","title":"PhosNetVis: a web-based tool for kinase enrichment analysis and interactive 2D/3D network visualizations of phosphoproteomics data","abstract":"Protein phosphorylation is a vital process in cellular signaling that involves the reversible modification of a protein (substrate) residue by another protein (kinase). Advances in liquid chromatography-mass spectrometry have enabled the rapid generation of massive protein phosphorylation datasets across multiple conditions by many research groups. Researchers are then tasked with inferring kinases responsible for changes in phosphorylation sites of each substrate. Despite the recent explosion of tools to infer kinase-substrate interactions (KSIs) from such datasets, these are not optimized for the interactive exploration of the resulting large and complex KSI networks together with significant phosphorylation sites and states. There are also no dedicated tools that streamline kinase inferences together with interactive visualizations of the resulting networks. There is thus an unmet need for a tool that facilitates uster-intuitive analysis, interactive exploration, visualization, and communication of datasets from phosphoproteomics experiments. Here, we present PhosNetVis, a freely available web-based tool for researchers of all computational skill levels to easily infer, generate and interactively explore KSI networks in 2D or 3D by streamlining multiple phosphoproteomics data analysis steps within one single tool. PhostNetVis significantly lowers the barriers for researchers in rapidly generating high-quality visualizations to translate their rich phosphoproteomics datasets into biological and clinical insights.","sentences":["Protein phosphorylation is a vital process in cellular signaling that involves the reversible modification of a protein (substrate) residue by another protein (kinase).","Advances in liquid chromatography-mass spectrometry have enabled the rapid generation of massive protein phosphorylation datasets across multiple conditions by many research groups.","Researchers are then tasked with inferring kinases responsible for changes in phosphorylation sites of each substrate.","Despite the recent explosion of tools to infer kinase-substrate interactions (KSIs) from such datasets, these are not optimized for the interactive exploration of the resulting large and complex KSI networks together with significant phosphorylation sites and states.","There are also no dedicated tools that streamline kinase inferences together with interactive visualizations of the resulting networks.","There is thus an unmet need for a tool that facilitates uster-intuitive analysis, interactive exploration, visualization, and communication of datasets from phosphoproteomics experiments.","Here, we present PhosNetVis, a freely available web-based tool for researchers of all computational skill levels to easily infer, generate and interactively explore KSI networks in 2D or 3D by streamlining multiple phosphoproteomics data analysis steps within one single tool.","PhostNetVis significantly lowers the barriers for researchers in rapidly generating high-quality visualizations to translate their rich phosphoproteomics datasets into biological and clinical insights."],"url":"http://arxiv.org/abs/2402.05016v2","category":"q-bio.MN"}
{"created":"2024-02-07 16:30:05","title":"Exhaust Gas Optimization of Modern Scooters by Velocity Control","abstract":"This paper investigates the optimization of the exhaust gas composition by applying a velocity-controlled Throttle-by-Wire-System on modern 50 cc scooters (Euro 5). Nowadays combustion-powered scooters are still inefficiently restricted, resulting in an unreasonably high fuel consumption and unfavorable exhaust emissions. The velocity control prevents restriction by negatively shifting the ignition timing and regulates the throttle valve opening instead. Injection quantity, engine speed, ignition timing, cylinder wall temperature, exhaust gas temperature, oxygen sensor data, crankshaft position and in-cylinder pressure were acquired to measure engine parameters. At the same time, vehicle data on the CAN bus, such as throttle opening angle, the rider's acceleration command and vehicle velocity were recorded. For determination of the exhaust gas composition, five probes were sensing CO, CO2, NOx, O2 and HC in addition to the temperature and mass flow. A Peugeot Kisbee 50 4T (Euro 5) serves as test vehicle. The original and the optimized restriction were subjected to various gradients on a roller dynamometer at top speed. Thus, a statement can be made about all operating points of restriction. The resistance parameters required, were previously determined in a coast down test. When driving on level ground, a difference of 50% in the throttle opening leads to a 17% improvement in fuel economy. By measuring the engine parameters, optimum ignition timing could be proven with increasing internal cylinder pressure. Further, 17% reduction in exhaust gas flow was demonstrated. CO emissions decreased by a factor of 8.4, CO2 by 1.17 and HC by 2.1 while NOx increased by a factor of 3.","sentences":["This paper investigates the optimization of the exhaust gas composition by applying a velocity-controlled Throttle-by-Wire-System on modern 50 cc scooters (Euro 5).","Nowadays combustion-powered scooters are still inefficiently restricted, resulting in an unreasonably high fuel consumption and unfavorable exhaust emissions.","The velocity control prevents restriction by negatively shifting the ignition timing and regulates the throttle valve opening instead.","Injection quantity, engine speed, ignition timing, cylinder wall temperature, exhaust gas temperature, oxygen sensor data, crankshaft position and in-cylinder pressure were acquired to measure engine parameters.","At the same time, vehicle data on the CAN bus, such as throttle opening angle, the rider's acceleration command and vehicle velocity were recorded.","For determination of the exhaust gas composition, five probes were sensing CO, CO2, NOx, O2 and HC in addition to the temperature and mass flow.","A Peugeot Kisbee 50 4T (Euro 5) serves as test vehicle.","The original and the optimized restriction were subjected to various gradients on a roller dynamometer at top speed.","Thus, a statement can be made about all operating points of restriction.","The resistance parameters required, were previously determined in a coast down test.","When driving on level ground, a difference of 50% in the throttle opening leads to a 17% improvement in fuel economy.","By measuring the engine parameters, optimum ignition timing could be proven with increasing internal cylinder pressure.","Further, 17% reduction in exhaust gas flow was demonstrated.","CO emissions decreased by a factor of 8.4, CO2 by 1.17 and HC by 2.1 while NOx increased by a factor of 3."],"url":"http://arxiv.org/abs/2402.05010v1","category":"eess.SY"}
{"created":"2024-02-07 16:23:17","title":"Evidence and quantification of memory effects in competitive first passage events","abstract":"Splitting probabilities quantify the likelihood of a given outcome out of competitive events for general random processes. This key observable of random walk theory, historically introduced as the Gambler's ruin problem for a player in a casino, has a broad range of applications beyond mathematical finance in evolution genetics, physics and chemistry, such as allele fixation, polymer translocation, protein folding and more generally competitive reactions. The statistics of competitive events is well understood for memoryless (Markovian) processes. However, in complex systems such as polymer fluids, the motion of a particle should typically be described as a process with memory. Appart from scaling theories and perturbative approaches in one-dimension, the outcome of competitive events is much less characterized analytically for processes with memory. Here, we introduce an analytical approach that provides the splitting probabilities for general $d$-dimensional non-Markovian Gaussian processes. This analysis shows that splitting probabilities are critically controlled by the out of equilibrium statistics of reactive trajectories, observed after the first passage. This hallmark of non-Markovian dynamics and its quantitative impact on splitting probabilities are directly evidenced in a prototypical experimental reaction scheme in viscoelastic fluids. Altogether, these results reveal both experimentally and theoretically the importance of memory effects on competitive reactions.","sentences":["Splitting probabilities quantify the likelihood of a given outcome out of competitive events for general random processes.","This key observable of random walk theory, historically introduced as the Gambler's ruin problem for a player in a casino, has a broad range of applications beyond mathematical finance in evolution genetics, physics and chemistry, such as allele fixation, polymer translocation, protein folding and more generally competitive reactions.","The statistics of competitive events is well understood for memoryless (Markovian) processes.","However, in complex systems such as polymer fluids, the motion of a particle should typically be described as a process with memory.","Appart from scaling theories and perturbative approaches in one-dimension, the outcome of competitive events is much less characterized analytically for processes with memory.","Here, we introduce an analytical approach that provides the splitting probabilities for general $d$-dimensional non-Markovian Gaussian processes.","This analysis shows that splitting probabilities are critically controlled by the out of equilibrium statistics of reactive trajectories, observed after the first passage.","This hallmark of non-Markovian dynamics and its quantitative impact on splitting probabilities are directly evidenced in a prototypical experimental reaction scheme in viscoelastic fluids.","Altogether, these results reveal both experimentally and theoretically the importance of memory effects on competitive reactions."],"url":"http://arxiv.org/abs/2402.05005v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 16:22:02","title":"Near-Optimal Generalized Decoding of Polar-like Codes","abstract":"In this work, we present a framework that explores the tradeoff between the undetected error rate (UER) and block error rate (BLER) of polar-like codes. It relies on a novel approximation for what we call codebook probability, which assumes an auxiliary distribution mimicking the dynamics of decoding algorithms with successive cancellation (SC) decoding schedule. Simulation results demonstrates that, in the case of SC list decoding, the proposed framework outperforms the state-of-art approximations of Forney's generalized decoding rule for polar-like codes with dynamic frozen bits. In addition, the proposed generalized decoding outperforms the CRC-concatenated polar codes significantly in both BLER and UER. Finally, we briefly discuss two potential applications of the approximated codebook probability: coded pilot-free channel estimation and bitwise soft-output decoding.","sentences":["In this work, we present a framework that explores the tradeoff between the undetected error rate (UER) and block error rate (BLER) of polar-like codes.","It relies on a novel approximation for what we call codebook probability, which assumes an auxiliary distribution mimicking the dynamics of decoding algorithms with successive cancellation (SC) decoding schedule.","Simulation results demonstrates that, in the case of SC list decoding, the proposed framework outperforms the state-of-art approximations of Forney's generalized decoding rule for polar-like codes with dynamic frozen bits.","In addition, the proposed generalized decoding outperforms the CRC-concatenated polar codes significantly in both BLER and UER.","Finally, we briefly discuss two potential applications of the approximated codebook probability: coded pilot-free channel estimation and bitwise soft-output decoding."],"url":"http://arxiv.org/abs/2402.05004v1","category":"cs.IT"}
{"created":"2024-02-07 16:01:53","title":"Hovering Flight in Flapping Insects and Hummingbirds: A Natural Real-Time and Stable Extremum Seeking Feedback System","abstract":"In this paper, we show that the physical phenomenon of hovering flight is comprehensively characterized and captured if considered and treated as an extremum-seeking (ES) feedback system. Said novel characterization solves all the puzzle pieces of hovering flight that existed for decades in previous literature: it provides a simple model-free, real-time, stable feedback system. Consistent with natural observations and biological experiments, hovering via ES is simply achievable by the natural oscillations of the wing angles and measuring (sensing) altitude and/or power. We provide simulation trials to demonstrate the effectiveness of our results on dronefly, hawkmoth, bumblebee, fruitfly insects, and hummingbird.","sentences":["In this paper, we show that the physical phenomenon of hovering flight is comprehensively characterized and captured if considered and treated as an extremum-seeking (ES) feedback system.","Said novel characterization solves all the puzzle pieces of hovering flight that existed for decades in previous literature: it provides a simple model-free, real-time, stable feedback system.","Consistent with natural observations and biological experiments, hovering via ES is simply achievable by the natural oscillations of the wing angles and measuring (sensing) altitude and/or power.","We provide simulation trials to demonstrate the effectiveness of our results on dronefly, hawkmoth, bumblebee, fruitfly insects, and hummingbird."],"url":"http://arxiv.org/abs/2402.04985v1","category":"math.OC"}
{"created":"2024-02-07 15:59:53","title":"Broadband squeezed light field by magnetostriction in an opto-magnomechanical","abstract":"We present a novel mechanism for generating a wide bandwidth squeezed optical output field in an opto-magnomechanical system. In this system, the magnon (mechanical) mode in the yttrium-iron-garnet crystal is coupled to the microwave field (optical field) through magnetic dipole (radiation pressure) interaction. The magnetostrictive force induced by the yttrium-iron-garnet crystal causes a mechanical displacement and creates a quadrature squeezed magnon mode. Eventually, this quadrature squeezed mechanical mode is transferred to the output optical field through state-swap interaction. Our results demonstrate the optimal parameter range for obtaining a stable squeezed optical output field with a wide bandwidth. Moreover, the squeezed light field exhibits strong robustness to environmental temperature. The new scheme we propose has potential applications in quantum precision measurements, quantum wireless networks, quantum radar, etc.","sentences":["We present a novel mechanism for generating a wide bandwidth squeezed optical output field in an opto-magnomechanical system.","In this system, the magnon (mechanical) mode in the yttrium-iron-garnet crystal is coupled to the microwave field (optical field) through magnetic dipole (radiation pressure) interaction.","The magnetostrictive force induced by the yttrium-iron-garnet crystal causes a mechanical displacement and creates a quadrature squeezed magnon mode.","Eventually, this quadrature squeezed mechanical mode is transferred to the output optical field through state-swap interaction.","Our results demonstrate the optimal parameter range for obtaining a stable squeezed optical output field with a wide bandwidth.","Moreover, the squeezed light field exhibits strong robustness to environmental temperature.","The new scheme we propose has potential applications in quantum precision measurements, quantum wireless networks, quantum radar, etc."],"url":"http://arxiv.org/abs/2402.04983v1","category":"quant-ph"}
{"created":"2024-02-07 15:56:03","title":"Tuning HMC parameters with gradients","abstract":"We investigate the effectiveness of tuning HMC parameters using information from the gradients of the HMC acceptance probability with respect to the parameters. In particular, the optimization of the trajectory length and parameters for higher order integrators will be studied in the context of pure gauge and dynamical fermion actions.","sentences":["We investigate the effectiveness of tuning HMC parameters using information from the gradients of the HMC acceptance probability with respect to the parameters.","In particular, the optimization of the trajectory length and parameters for higher order integrators will be studied in the context of pure gauge and dynamical fermion actions."],"url":"http://arxiv.org/abs/2402.04976v1","category":"hep-lat"}
{"created":"2024-02-07 15:42:50","title":"Hidden non-equilibrium pathways towards crystalline perfection","abstract":"A central paradigm of non-equilibrium physics concerns the dynamics of heterogeneity and disorder, impacting processes ranging from the behavior of glasses1 to the emergent functionality of active matter. Understanding these complex mesoscopic systems requires probing the microscopic trajectories associated with irreversible processes, the role of fluctuations and entropy growth, and the timescales on which non-equilibrium responses are ultimately maintained. Approaches that illuminate these processes in model systems may enable a more general understanding of other heterogeneous non-equilibrium phenomena, and potentially define ultimate speed and energy cost limits for information processing technologies. Here, we apply ultrafast single shot x-ray photon correlation spectroscopy (XPCS) to resolve the non-equilibrium, heterogeneous, and irreversible mesoscale dynamics during a light-induced phase transition. This approach defines a new way of capturing the nucleation of the induced phase, the formation of transient mesoscale defects at the boundaries of the nuclei, and the eventual disappearance of these defects, even in systems with complex polarization topologies. A non-equilibrium, sub-diffusive response spanning >10 orders of magnitude in timescales is observed with multistep behavior similar to the plateaus observed in supercooled liquids or glasses. We show how the observed time-dependent long-time correlations can be understood in terms of the stochastic dynamics of domain walls, encoded in effective waiting-time distributions with power-law tails. This work defines new possibilities for probing the non-equilibrium dynamics of disordered and heterogeneous media.","sentences":["A central paradigm of non-equilibrium physics concerns the dynamics of heterogeneity and disorder, impacting processes ranging from the behavior of glasses1 to the emergent functionality of active matter.","Understanding these complex mesoscopic systems requires probing the microscopic trajectories associated with irreversible processes, the role of fluctuations and entropy growth, and the timescales on which non-equilibrium responses are ultimately maintained.","Approaches that illuminate these processes in model systems may enable a more general understanding of other heterogeneous non-equilibrium phenomena, and potentially define ultimate speed and energy cost limits for information processing technologies.","Here, we apply ultrafast single shot x-ray photon correlation spectroscopy (XPCS) to resolve the non-equilibrium, heterogeneous, and irreversible mesoscale dynamics during a light-induced phase transition.","This approach defines a new way of capturing the nucleation of the induced phase, the formation of transient mesoscale defects at the boundaries of the nuclei, and the eventual disappearance of these defects, even in systems with complex polarization topologies.","A non-equilibrium, sub-diffusive response spanning >10 orders of magnitude in timescales is observed with multistep behavior similar to the plateaus observed in supercooled liquids or glasses.","We show how the observed time-dependent long-time correlations can be understood in terms of the stochastic dynamics of domain walls, encoded in effective waiting-time distributions with power-law tails.","This work defines new possibilities for probing the non-equilibrium dynamics of disordered and heterogeneous media."],"url":"http://arxiv.org/abs/2402.04962v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 15:41:14","title":"Margin Propagation based XOR-SAT Solvers for Decoding of LDPC Codes","abstract":"Decoding of Low-Density Parity Check (LDPC) codes can be viewed as a special case of XOR-SAT problems, for which low-computational complexity bit-flipping algorithms have been proposed in the literature. However, a performance gap exists between the bit-flipping LDPC decoding algorithms and the benchmark LDPC decoding algorithms, such as the Sum-Product Algorithm (SPA). In this paper, we propose an XOR-SAT solver using log-sum-exponential functions and demonstrate its advantages for LDPC decoding. This is then approximated using the Margin Propagation formulation to attain a low-complexity LDPC decoder. The proposed algorithm uses soft information to decide the bit-flips that maximize the number of parity check constraints satisfied over an optimization function. The proposed solver can achieve results that are within $0.1$dB of the Sum-Product Algorithm for the same number of code iterations. It is also at least 10x lesser than other Gradient-Descent Bit Flipping decoding algorithms, which are also bit-flipping algorithms based on optimization functions. The approximation using the Margin Propagation formulation does not require any multipliers, resulting in significantly lower computational complexity than other soft-decision Bit-Flipping LDPC decoders.","sentences":["Decoding of Low-Density Parity Check (LDPC) codes can be viewed as a special case of XOR-SAT problems, for which low-computational complexity bit-flipping algorithms have been proposed in the literature.","However, a performance gap exists between the bit-flipping LDPC decoding algorithms and the benchmark LDPC decoding algorithms, such as the Sum-Product Algorithm (SPA).","In this paper, we propose an XOR-SAT solver using log-sum-exponential functions and demonstrate its advantages for LDPC decoding.","This is then approximated using the Margin Propagation formulation to attain a low-complexity LDPC decoder.","The proposed algorithm uses soft information to decide the bit-flips that maximize the number of parity check constraints satisfied over an optimization function.","The proposed solver can achieve results that are within $0.1$dB of the Sum-Product Algorithm for the same number of code iterations.","It is also at least 10x lesser than other Gradient-Descent Bit Flipping decoding algorithms, which are also bit-flipping algorithms based on optimization functions.","The approximation using the Margin Propagation formulation does not require any multipliers, resulting in significantly lower computational complexity than other soft-decision Bit-Flipping LDPC decoders."],"url":"http://arxiv.org/abs/2402.04959v1","category":"cs.IT"}
{"created":"2024-02-07 15:14:38","title":"Convergence of Approximate and Packet Routing Equilibria to Nash Flows Over Time","abstract":"We consider a dynamic model of traffic that has received a lot of attention in the past few years. Infinitesimally small agents aim to travel from a source to a destination as quickly as possible. Flow patterns vary over time, and congestion effects are modeled via queues, which form based on the deterministic queueing model whenever the inflow into a link exceeds its capacity. Are equilibria in this model meaningful as a prediction of traffic behavior? For this to be the case, a certain notion of stability under ongoing perturbations is needed. Real traffic consists of discrete, atomic ''packets'', rather than being a continuous flow of non-atomic agents. Users may not choose an absolutely quickest route available, if there are multiple routes with very similar travel times. We would hope that in both these situations -- a discrete packet model, with packet size going to 0, and $\\epsilon$-equilibria, with $\\epsilon$ going to 0 -- equilibria converge to dynamic equilibria in the flow over time model. No such convergence results were known. We show that such a convergence result does hold in single-commodity instances for both of these settings, in a unified way. More precisely, we introduce a notion of ''strict'' $\\epsilon$-equilibria, and show that these must converge to the exact dynamic equilibrium in the limit as $\\epsilon \\to 0$. We then show that results for the two settings mentioned can be deduced from this with only moderate further technical effort.","sentences":["We consider a dynamic model of traffic that has received a lot of attention in the past few years.","Infinitesimally small agents aim to travel from a source to a destination as quickly as possible.","Flow patterns vary over time, and congestion effects are modeled via queues, which form based on the deterministic queueing model whenever the inflow into a link exceeds its capacity.","Are equilibria in this model meaningful as a prediction of traffic behavior?","For this to be the case, a certain notion of stability under ongoing perturbations is needed.","Real traffic consists of discrete, atomic ''packets'', rather than being a continuous flow of non-atomic agents.","Users may not choose an absolutely quickest route available, if there are multiple routes with very similar travel times.","We would hope that in both these situations -- a discrete packet model, with packet size going to 0, and $\\epsilon$-equilibria, with $\\epsilon$ going to 0 -- equilibria converge to dynamic equilibria in the flow over time model.","No such convergence results were known.","We show that such a convergence result does hold in single-commodity instances for both of these settings, in a unified way.","More precisely, we introduce a notion of ''strict'' $\\epsilon$-equilibria, and show that these must converge to the exact dynamic equilibrium in the limit as $\\epsilon \\to 0$.","We then show that results for the two settings mentioned can be deduced from this with only moderate further technical effort."],"url":"http://arxiv.org/abs/2402.04935v1","category":"cs.GT"}
{"created":"2024-02-07 14:59:25","title":"Blue noise for diffusion models","abstract":"Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and quantitative evaluations on a variety of datasets using our method, achieving improvements on different tasks over existing deterministic diffusion models in terms of FID metric.","sentences":["Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network.","Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored.","In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account.","More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask.","Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only.","Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow.","We perform both qualitative and quantitative evaluations on a variety of datasets using our method, achieving improvements on different tasks over existing deterministic diffusion models in terms of FID metric."],"url":"http://arxiv.org/abs/2402.04930v1","category":"cs.CV"}
{"created":"2024-02-07 14:47:13","title":"Voronoi Candidates for Bayesian Optimization","abstract":"Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions. However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead. Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates. Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them. We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension. On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy.","sentences":["Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions.","However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead.","Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates.","Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them.","We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension.","On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy."],"url":"http://arxiv.org/abs/2402.04922v1","category":"stat.ML"}
{"created":"2024-02-07 14:43:48","title":"Maximal displacement of a time-inhomogeneous N(T)-particles branching Brownian motion","abstract":"The $N$-particles branching Brownian motion ($N$-BBM) is a branching Markov process which describes the evolution of a population of particles undergoing reproduction and selection. It shares many properties with the $N$-particles branching random walk ($N$-BRW), which itself is strongly related to physical $p$-spin models, or to Derrida's Random Energy Model. The $N$-BRW can also be seen as the realization of an optimization algorithm over hierarchical data, which is often called beam search. More precisely, the maximal displacement of the $N$-BRW (or $N$-BBM) can be seen as the output of the beam search algorithm; and the population size $N$ is the ``width'' of the beam, and (almost) matches the computational complexity of the algorithm.   In this paper, we investigate the maximal displacement at time $T$ of an $N$-BBM, where $N=N(T)$ is picked arbitrarily depending on $T$ and the diffusion of the process $\\sigma(\\cdot)$ is inhomogeneous in time. We prove the existence of a transition in the second order of the maximal displacement when $\\log N(T)$ is of order $T^{1/3}$. When $\\log N(T)\\ll T^{1/3}$, the maximal displacement behaves according to the Brunet-Derrida correction which has already been studied for $N$ a large constant and for $\\sigma$ constant. When $\\log N(T)\\gg T^{1/3}$, the output of the algorithm (i.e. the maximal displacement) is subject to two phenomena: on the one hand it begins to grow very slowly (logarithmically) in terms of the complexity $N$; and on the other hand its dependency in the time-inhomogeneity $\\sigma(\\cdot)$ becomes more intricate. The transition at $\\log N(T)\\approx T^{1/3}$ can be interpreted as an ``efficiency ceiling'' in the output of the beam search algorithm, which extends previous results from Addario-Berry and Maillard regarding an algorithm hardness threshold for optimization over the Continuous Random Energy Model.","sentences":["The $N$-particles branching Brownian motion ($N$-BBM) is a branching Markov process which describes the evolution of a population of particles undergoing reproduction and selection.","It shares many properties with the $N$-particles branching random walk ($N$-BRW), which itself is strongly related to physical $p$-spin models, or to Derrida's Random Energy Model.","The $N$-BRW can also be seen as the realization of an optimization algorithm over hierarchical data, which is often called beam search.","More precisely, the maximal displacement of the $N$-BRW (or $N$-BBM) can be seen as the output of the beam search algorithm; and the population size $N$ is the ``width'' of the beam, and (almost) matches the computational complexity of the algorithm.   ","In this paper, we investigate the maximal displacement at time $T$ of an $N$-BBM, where $N=N(T)$ is picked arbitrarily depending on $T$ and the diffusion of the process $\\sigma(\\cdot)$ is inhomogeneous in time.","We prove the existence of a transition in the second order of the maximal displacement when $\\log N(T)$ is of order $T^{1/3}$. When $\\log N(T)\\ll T^{1/3}$, the maximal displacement behaves according to the Brunet-Derrida correction which has already been studied for $N$ a large constant and for $\\sigma$ constant.","When $\\log N(T)\\gg T^{1/3}$, the output of the algorithm (i.e. the maximal displacement) is subject to two phenomena: on the one hand it begins to grow very slowly (logarithmically) in terms of the complexity $N$; and on the other hand its dependency in the time-inhomogeneity $\\sigma(\\cdot)$ becomes more intricate.","The transition at $\\log N(T)\\approx T^{1/3}$ can be interpreted as an ``efficiency ceiling'' in the output of the beam search algorithm, which extends previous results from Addario-Berry and Maillard regarding an algorithm hardness threshold for optimization over the Continuous Random Energy Model."],"url":"http://arxiv.org/abs/2402.04917v1","category":"math.PR"}
{"created":"2024-02-07 14:38:51","title":"Entanglement Definitions for Tethered Robots: Exploration and Analysis","abstract":"In this article we consider the problem of tether entanglement for tethered robots. In many applications, such as maintenance of underwater structures, aerial inspection, and underground exploration, tethered robots are often used in place of standalone (i.e., untethered) ones. However, the presence of a tether also introduces the risk for it to get entangled with obstacles present in the environment or with itself. To avoid these situations, a non-entanglement constraint can be considered in the motion planning problem for tethered robots. This constraint can be expressed either as a set of specific tether configurations that must be avoided, or as a quantitative measure of a `level of entanglement' that can be minimized. However, the literature lacks a generally accepted definition of entanglement, with existing definitions being limited and partial. Namely, the existing entanglement definitions either require a taut tether to come into contact with an obstacle or with another tether, or they require for the tether to do a full loop around an obstacle. In practice, this means that the existing definitions do not effectively cover all instances of tether entanglement. Our goal in this article is to bridge this gap and provide new definitions of entanglement, which, together with the existing ones, can be effectively used to qualify the entanglement state of a tethered robot in diverse situations. The new definitions find application mainly in motion planning for tethered robot systems, where they can be used to obtain more safe and robust entanglement-free trajectories. The present article focuses exclusively on the presentation and analysis of the entanglement definitions. The application of the definitions to the motion planning problem is left for future work.","sentences":["In this article we consider the problem of tether entanglement for tethered robots.","In many applications, such as maintenance of underwater structures, aerial inspection, and underground exploration, tethered robots are often used in place of standalone (i.e., untethered) ones.","However, the presence of a tether also introduces the risk for it to get entangled with obstacles present in the environment or with itself.","To avoid these situations, a non-entanglement constraint can be considered in the motion planning problem for tethered robots.","This constraint can be expressed either as a set of specific tether configurations that must be avoided, or as a quantitative measure of a `level of entanglement' that can be minimized.","However, the literature lacks a generally accepted definition of entanglement, with existing definitions being limited and partial.","Namely, the existing entanglement definitions either require a taut tether to come into contact with an obstacle or with another tether, or they require for the tether to do a full loop around an obstacle.","In practice, this means that the existing definitions do not effectively cover all instances of tether entanglement.","Our goal in this article is to bridge this gap and provide new definitions of entanglement, which, together with the existing ones, can be effectively used to qualify the entanglement state of a tethered robot in diverse situations.","The new definitions find application mainly in motion planning for tethered robot systems, where they can be used to obtain more safe and robust entanglement-free trajectories.","The present article focuses exclusively on the presentation and analysis of the entanglement definitions.","The application of the definitions to the motion planning problem is left for future work."],"url":"http://arxiv.org/abs/2402.04909v1","category":"cs.RO"}
{"created":"2024-02-07 14:25:20","title":"Dynamic Coalition Portfolio Selection with Recursive Utility","abstract":"In this paper, we consider a dynamic coalition portfolio selection problem, with each agent's objective given by an Epstein--Zin recursive utility. To find a Pareto optimum, the coalition's problem is formulated as an optimization problem evolved by a multi-dimensional forward-backward SDE. Since the evolution system has a forward-backward structure, the problem is intrinsically time-inconsistent. With the dynamic-game point of view, we rigorously develop an approach to finding the equilibrium Pareto investment-consumption strategy. We find that the relationship between risk aversion and EIS has more influence on the coalition's problem than that on the one-agent problem. More interestingly, we show that the equilibrium Pareto consumption strategy associated with the recursive utility is much more effective than that associated with the CRRA expected utility, which highlights the feature of recursive utilities that the marginal benefit of consumption can depend on the future consumption.","sentences":["In this paper, we consider a dynamic coalition portfolio selection problem, with each agent's objective given by an Epstein--Zin recursive utility.","To find a Pareto optimum, the coalition's problem is formulated as an optimization problem evolved by a multi-dimensional forward-backward SDE.","Since the evolution system has a forward-backward structure, the problem is intrinsically time-inconsistent.","With the dynamic-game point of view, we rigorously develop an approach to finding the equilibrium Pareto investment-consumption strategy.","We find that the relationship between risk aversion and EIS has more influence on the coalition's problem than that on the one-agent problem.","More interestingly, we show that the equilibrium Pareto consumption strategy associated with the recursive utility is much more effective than that associated with the CRRA expected utility, which highlights the feature of recursive utilities that the marginal benefit of consumption can depend on the future consumption."],"url":"http://arxiv.org/abs/2402.04895v1","category":"math.OC"}
{"created":"2024-02-07 14:21:26","title":"Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration","abstract":"Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization. Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements.","sentences":["Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space.","This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization.","Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms.","First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces.","Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis.","In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase.","The entire model training is accomplished through an end-to-end manner.","We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP.","Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements."],"url":"http://arxiv.org/abs/2402.04883v1","category":"cs.CV"}
{"created":"2024-02-07 14:07:47","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback","abstract":"In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval.","sentences":["In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users.","Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries.","However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images.","In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results.","We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process.","Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach.","Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement.","Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval."],"url":"http://arxiv.org/abs/2402.04867v1","category":"cs.IR"}
{"created":"2024-02-07 14:03:14","title":"Collaborative Computing in Non-Terrestrial Networks: A Multi-Time-Scale Deep Reinforcement Learning Approach","abstract":"Constructing earth-fixed cells with low-earth orbit (LEO) satellites in non-terrestrial networks (NTNs) has been the most promising paradigm to enable global coverage. The limited computing capabilities on LEO satellites however render tackling resource optimization within a short duration a critical challenge. Although the sufficient computing capabilities of the ground infrastructures can be utilized to assist the LEO satellite, different time-scale control cycles and coupling decisions between the space- and ground-segments still obstruct the joint optimization design for computing agents at different segments. To address the above challenges, in this paper, a multi-time-scale deep reinforcement learning (DRL) scheme is developed for achieving the radio resource optimization in NTNs, in which the LEO satellite and user equipment (UE) collaborate with each other to perform individual decision-making tasks with different control cycles. Specifically, the UE updates its policy toward improving value functions of both the satellite and UE, while the LEO satellite only performs finite-step rollout for decision-makings based on the reference decision trajectory provided by the UE. Most importantly, rigorous analysis to guarantee the performance convergence of the proposed scheme is provided. Comprehensive simulations are conducted to justify the effectiveness of the proposed scheme in balancing the transmission performance and computational complexity.","sentences":["Constructing earth-fixed cells with low-earth orbit (LEO) satellites in non-terrestrial networks (NTNs) has been the most promising paradigm to enable global coverage.","The limited computing capabilities on LEO satellites however render tackling resource optimization within a short duration a critical challenge.","Although the sufficient computing capabilities of the ground infrastructures can be utilized to assist the LEO satellite, different time-scale control cycles and coupling decisions between the space- and ground-segments still obstruct the joint optimization design for computing agents at different segments.","To address the above challenges, in this paper, a multi-time-scale deep reinforcement learning (DRL) scheme is developed for achieving the radio resource optimization in NTNs, in which the LEO satellite and user equipment (UE) collaborate with each other to perform individual decision-making tasks with different control cycles.","Specifically, the UE updates its policy toward improving value functions of both the satellite and UE, while the LEO satellite only performs finite-step rollout for decision-makings based on the reference decision trajectory provided by the UE.","Most importantly, rigorous analysis to guarantee the performance convergence of the proposed scheme is provided.","Comprehensive simulations are conducted to justify the effectiveness of the proposed scheme in balancing the transmission performance and computational complexity."],"url":"http://arxiv.org/abs/2402.04865v1","category":"eess.SP"}
{"created":"2024-02-07 13:52:11","title":"Leveraging LLMs for Unsupervised Dense Retriever Ranking","abstract":"This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus. Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus. The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set. The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable. Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge. Existing methodologies for ranking dense retrievers fall short in addressing these domain shift scenarios.   To tackle this, our method capitalizes on LLMs to create pseudo-relevant queries, labels, and reference lists by analyzing a subset of documents from the target corpus. This allows for the ranking of dense retrievers based on their performance with these pseudo-relevant signals. Significantly, this strategy is the first to depend exclusively on the target corpus data, removing the necessity for training data and test labels. We assessed the effectiveness of our approach by compiling a comprehensive pool of cutting-edge dense retrievers and comparing our method against traditional dense retriever selection benchmarks. The findings reveal that our proposed solution surpasses the existing benchmarks in both the selection and ranking of dense retrievers.","sentences":["This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus.","Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus.","The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set.","The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable.","Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge.","Existing methodologies for ranking dense retrievers fall short in addressing these domain shift scenarios.   ","To tackle this, our method capitalizes on LLMs to create pseudo-relevant queries, labels, and reference lists by analyzing a subset of documents from the target corpus.","This allows for the ranking of dense retrievers based on their performance with these pseudo-relevant signals.","Significantly, this strategy is the first to depend exclusively on the target corpus data, removing the necessity for training data and test labels.","We assessed the effectiveness of our approach by compiling a comprehensive pool of cutting-edge dense retrievers and comparing our method against traditional dense retriever selection benchmarks.","The findings reveal that our proposed solution surpasses the existing benchmarks in both the selection and ranking of dense retrievers."],"url":"http://arxiv.org/abs/2402.04853v1","category":"cs.IR"}
{"created":"2024-02-07 13:44:47","title":"AlphaFold Meets Flow Matching for Generating Protein Ensembles","abstract":"The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow.","sentences":["The biological functions of proteins often depend on dynamic structural ensembles.","In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins.","We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow.","When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling.","When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins.","Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations.","Code is available at https://github.com/bjing2016/alphaflow."],"url":"http://arxiv.org/abs/2402.04845v1","category":"q-bio.BM"}
{"created":"2024-02-07 13:43:18","title":"Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions","abstract":"The performance of optimization methods is often tied to the spectrum of the objective Hessian. Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements -- particularly not for non-convex problems. Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity. This allows to partition the class of non-convex problems into a nested chain of subclasses. Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses. As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade. Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian. Our theory is validated by numerical experiments executed on multiple practical machine learning problems.","sentences":["The performance of optimization methods is often tied to the spectrum of the objective Hessian.","Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements -- particularly not for non-convex problems.","Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity.","This allows to partition the class of non-convex problems into a nested chain of subclasses.","Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses.","As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade.","Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian.","Our theory is validated by numerical experiments executed on multiple practical machine learning problems."],"url":"http://arxiv.org/abs/2402.04843v1","category":"math.OC"}
{"created":"2024-02-07 13:41:45","title":"Efficient Estimation of a Gaussian Mean with Local Differential Privacy","abstract":"In this paper we study the problem of estimating the unknown mean $\\theta$ of a unit variance Gaussian distribution in a locally differentially private (LDP) way. In the high-privacy regime ($\\epsilon\\le 0.67$), we identify the exact optimal privacy mechanism that minimizes the variance of the estimator asymptotically. It turns out to be the extraordinarily simple sign mechanism that applies randomized response to the sign of $X_i-\\theta$. However, since this optimal mechanism depends on the unknown mean $\\theta$, we employ a two-stage LDP parameter estimation procedure which requires splitting agents into two groups. The first $n_1$ observations are used to consistently but not necessarily efficiently estimate the parameter $\\theta$ by $\\tilde{\\theta}_{n_1}$. Then this estimate is updated by applying the sign mechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining $n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown mean.","sentences":["In this paper we study the problem of estimating the unknown mean $\\theta$ of a unit variance Gaussian distribution in a locally differentially private (LDP) way.","In the high-privacy regime ($\\epsilon\\le 0.67$), we identify the exact optimal privacy mechanism that minimizes the variance of the estimator asymptotically.","It turns out to be the extraordinarily simple sign mechanism that applies randomized response to the sign of $X_i-\\theta$. However, since this optimal mechanism depends on the unknown mean $\\theta$, we employ a two-stage LDP parameter estimation procedure which requires splitting agents into two groups.","The first $n_1$ observations are used to consistently but not necessarily efficiently estimate the parameter $\\theta$ by $\\tilde{\\theta}_{n_1}$. Then this estimate is updated by applying the sign mechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining $n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown mean."],"url":"http://arxiv.org/abs/2402.04840v1","category":"math.ST"}
{"created":"2024-02-07 13:25:16","title":"NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering","abstract":"Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: https://nerfemitterpbir.github.io/.","sentences":["Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images.","Here lighting is an important part of achieving faithful light transport simulation.","While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering.","We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter.","By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering.","Project page and video: https://nerfemitterpbir.github.io/."],"url":"http://arxiv.org/abs/2402.04829v1","category":"cs.CV"}
{"created":"2024-02-07 13:20:23","title":"Kinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations","abstract":"Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot. The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers. We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas. We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK). Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs. We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects. We additionally compare our method against existing hand retargeting approaches. Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories.","sentences":["Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot.","The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers.","We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas.","We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK).","Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs.","We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects.","We additionally compare our method against existing hand retargeting approaches.","Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories."],"url":"http://arxiv.org/abs/2402.04820v1","category":"cs.GR"}
