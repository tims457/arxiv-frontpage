{"created":"2024-05-13 17:59:56","title":"MambaOut: Do We Really Need Mamba for Vision?","abstract":"Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks. The code is available at https://github.com/yuweihao/MambaOut","sentences":["Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks.","Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models.","In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics.","For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks.","To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM.","Experimental results strongly support our hypotheses.","Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task.","As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks.","The code is available at https://github.com/yuweihao/MambaOut"],"url":"http://arxiv.org/abs/2405.07992v2","category":"cs.CV"}
{"created":"2024-05-13 17:59:36","title":"SPIN: Simultaneous Perception, Interaction and Navigation","abstract":"While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge. Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments. While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together. Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together. This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination. In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment. Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move. This allows it to not only move around and interact with its environment but also, choose \"when\" to perceive \"what\" using an active visual system. We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps. Results visualizations and videos at https://spin-robot.github.io/","sentences":["While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge.","Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments.","While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together.","Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together.","This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination.","In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment.","Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move.","This allows it to not only move around and interact with its environment but also, choose \"when\" to perceive \"what\" using an active visual system.","We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps.","Results visualizations and videos at https://spin-robot.github.io/"],"url":"http://arxiv.org/abs/2405.07991v1","category":"cs.RO"}
{"created":"2024-05-13 17:59:22","title":"Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots","abstract":"The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly. To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs. We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries. For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4. This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities. Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images. Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation. The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code. With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction. We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs. All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code.","sentences":["The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts.","However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly.","To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs.","We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries.","For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4.","This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities.","Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images.","Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation.","The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code.","With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction.","We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs.","All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code."],"url":"http://arxiv.org/abs/2405.07990v1","category":"cs.CL"}
{"created":"2024-05-13 17:58:51","title":"A Generalist Learner for Multifaceted Medical Image Interpretation","abstract":"Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice. To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation. By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification. This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis. We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa. Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis. This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI-assisted clinical decision-making.","sentences":["Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice.","To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation.","By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification.","This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis.","We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa.","Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%.","MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis.","This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI-assisted clinical decision-making."],"url":"http://arxiv.org/abs/2405.07988v1","category":"cs.CV"}
{"created":"2024-05-13 17:58:30","title":"The Platonic Representation Hypothesis","abstract":"We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.","sentences":["We argue that representations in AI models, particularly deep networks, are converging.","First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned.","Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way.","We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality.","We term such a representation the platonic representation and discuss several possible selective pressures toward it.","Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis."],"url":"http://arxiv.org/abs/2405.07987v1","category":"cs.LG"}
{"created":"2024-05-13 17:52:25","title":"Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference","abstract":"Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.","sentences":["Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions.","In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design.","Our contributions are threefold.","First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs.","We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design.","Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization.","For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually.","For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering.","Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings.","Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners."],"url":"http://arxiv.org/abs/2405.07979v2","category":"stat.ME"}
{"created":"2024-05-13 17:49:20","title":"A Demographic-Conditioned Variational Autoencoder for fMRI Distribution Sampling and Removal of Confounds","abstract":"Objective: fMRI and derived measures such as functional connectivity (FC) have been used to predict brain age, general fluid intelligence, psychiatric disease status, and preclinical neurodegenerative disease. However, it is not always clear that all demographic confounds, such as age, sex, and race, have been removed from fMRI data. Additionally, many fMRI datasets are restricted to authorized researchers, making dissemination of these valuable data sources challenging. Methods: We create a variational autoencoder (VAE)-based model, DemoVAE, to decorrelate fMRI features from demographics and generate high-quality synthetic fMRI data based on user-supplied demographics. We train and validate our model using two large, widely used datasets, the Philadelphia Neurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP). Results: We find that DemoVAE recapitulates group differences in fMRI data while capturing the full breadth of individual variations. Significantly, we also find that most clinical and computerized battery fields that are correlated with fMRI data are not correlated with DemoVAE latents. An exception are several fields related to schizophrenia medication and symptom severity. Conclusion: Our model generates fMRI data that captures the full distribution of FC better than traditional VAE or GAN models. We also find that most prediction using fMRI data is dependent on correlation with, and prediction of, demographics. Significance: Our DemoVAE model allows for generation of high quality synthetic data conditioned on subject demographics as well as the removal of the confounding effects of demographics. We identify that FC-based prediction tasks are highly influenced by demographic confounds.","sentences":["Objective: fMRI and derived measures such as functional connectivity (FC) have been used to predict brain age, general fluid intelligence, psychiatric disease status, and preclinical neurodegenerative disease.","However, it is not always clear that all demographic confounds, such as age, sex, and race, have been removed from fMRI data.","Additionally, many fMRI datasets are restricted to authorized researchers, making dissemination of these valuable data sources challenging.","Methods: We create a variational autoencoder (VAE)-based model, DemoVAE, to decorrelate fMRI features from demographics and generate high-quality synthetic fMRI data based on user-supplied demographics.","We train and validate our model using two large, widely used datasets, the Philadelphia Neurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP).","Results:","We find that DemoVAE recapitulates group differences in fMRI data while capturing the full breadth of individual variations.","Significantly, we also find that most clinical and computerized battery fields that are correlated with fMRI data are not correlated with DemoVAE latents.","An exception are several fields related to schizophrenia medication and symptom severity.","Conclusion: Our model generates fMRI data that captures the full distribution of FC better than traditional VAE or GAN models.","We also find that most prediction using fMRI data is dependent on correlation with, and prediction of, demographics.","Significance: Our DemoVAE model allows for generation of high quality synthetic data conditioned on subject demographics as well as the removal of the confounding effects of demographics.","We identify that FC-based prediction tasks are highly influenced by demographic confounds."],"url":"http://arxiv.org/abs/2405.07977v1","category":"q-bio.QM"}
{"created":"2024-05-13 17:48:45","title":"Localized Adaptive Risk Control","abstract":"Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees. ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC. L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee. The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target. Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks.","sentences":["Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees.","ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions.","In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC.","L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee.","The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target.","Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks."],"url":"http://arxiv.org/abs/2405.07976v1","category":"stat.ML"}
{"created":"2024-05-13 17:48:20","title":"A Natural Formalized Proof Language","abstract":"Artificial intelligence assisted mathematical proof has become a highly focused area nowadays. One key problem in this field is to generate formal mathematical proofs from natural language proofs. Due to historical reasons, the formal proof languages adopted by traditional theorem provers were not intended to represent natural language proofs. Therefore, they are not well-suited for the aforementioned tasks and proof-checking work for educational purposes. In this paper, we design a proof language and its corresponding abstract syntax tree and implement a proof checking tool for it. This language can be easily converted from natural language, thus providing a rich corpus of formal proof. Additionally, it supports the handling of issues in informal proofs through static analysis, and enhances the expressive power of the language by introducing the structure of partial proofs. This design combines the expressiveness of natural language and the accuracy of formal language, resulting in an improved mathematical proof language.","sentences":["Artificial intelligence assisted mathematical proof has become a highly focused area nowadays.","One key problem in this field is to generate formal mathematical proofs from natural language proofs.","Due to historical reasons, the formal proof languages adopted by traditional theorem provers were not intended to represent natural language proofs.","Therefore, they are not well-suited for the aforementioned tasks and proof-checking work for educational purposes.","In this paper, we design a proof language and its corresponding abstract syntax tree and implement a proof checking tool for it.","This language can be easily converted from natural language, thus providing a rich corpus of formal proof.","Additionally, it supports the handling of issues in informal proofs through static analysis, and enhances the expressive power of the language by introducing the structure of partial proofs.","This design combines the expressiveness of natural language and the accuracy of formal language, resulting in an improved mathematical proof language."],"url":"http://arxiv.org/abs/2405.07973v1","category":"cs.PL"}
{"created":"2024-05-13 17:47:08","title":"Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation","abstract":"Zero-shot anomaly segmentation using pre-trained foundation models is a promising approach that enables effective algorithms without expensive, domain-specific training or fine-tuning. Ensuring that these methods work across various environmental conditions and are robust to distribution shifts is an open problem. We investigate the performance of WinCLIP [14] zero-shot anomaly segmentation algorithm by perturbing test data using three semantic transformations: bounded angular rotations, bounded saturation shifts, and hue shifts. We empirically measure a lower performance bound by aggregating across per-sample worst-case perturbations and find that average performance drops by up to 20% in area under the ROC curve and 40% in area under the per-region overlap curve. We find that performance is consistently lowered on three CLIP backbones, regardless of model architecture or learning objective, demonstrating a need for careful performance evaluation.","sentences":["Zero-shot anomaly segmentation using pre-trained foundation models is a promising approach that enables effective algorithms without expensive, domain-specific training or fine-tuning.","Ensuring that these methods work across various environmental conditions and are robust to distribution shifts is an open problem.","We investigate the performance of WinCLIP [14] zero-shot anomaly segmentation algorithm by perturbing test data using three semantic transformations: bounded angular rotations, bounded saturation shifts, and hue shifts.","We empirically measure a lower performance bound by aggregating across per-sample worst-case perturbations and find that average performance drops by up to 20% in area under the ROC curve and 40% in area under the per-region overlap curve.","We find that performance is consistently lowered on three CLIP backbones, regardless of model architecture or learning objective, demonstrating a need for careful performance evaluation."],"url":"http://arxiv.org/abs/2405.07969v1","category":"cs.CV"}
{"created":"2024-05-13 17:46:35","title":"OverlapMamba: Novel Shift State Space Model for LiDAR-based Place Recognition","abstract":"Place recognition is the foundation for enabling autonomous systems to achieve independent decision-making and safe operations. It is also crucial in tasks such as loop closure detection and global localization within SLAM. Previous methods utilize mundane point cloud representations as input and deep learning-based LiDAR-based Place Recognition (LPR) approaches employing different point cloud image inputs with convolutional neural networks (CNNs) or transformer architectures. However, the recently proposed Mamba deep learning model, combined with state space models (SSMs), holds great potential for long sequence modeling. Therefore, we developed OverlapMamba, a novel network for place recognition, which represents input range views (RVs) as sequences. In a novel way, we employ a stochastic reconstruction approach to build shift state space models, compressing the visual representation. Evaluated on three different public datasets, our method effectively detects loop closures, showing robustness even when traversing previously visited locations from different directions. Relying on raw range view inputs, it outperforms typical LiDAR and multi-view combination methods in time complexity and speed, indicating strong place recognition capabilities and real-time efficiency.","sentences":["Place recognition is the foundation for enabling autonomous systems to achieve independent decision-making and safe operations.","It is also crucial in tasks such as loop closure detection and global localization within SLAM.","Previous methods utilize mundane point cloud representations as input and deep learning-based LiDAR-based Place Recognition (LPR) approaches employing different point cloud image inputs with convolutional neural networks (CNNs) or transformer architectures.","However, the recently proposed Mamba deep learning model, combined with state space models (SSMs), holds great potential for long sequence modeling.","Therefore, we developed OverlapMamba, a novel network for place recognition, which represents input range views (RVs) as sequences.","In a novel way, we employ a stochastic reconstruction approach to build shift state space models, compressing the visual representation.","Evaluated on three different public datasets, our method effectively detects loop closures, showing robustness even when traversing previously visited locations from different directions.","Relying on raw range view inputs, it outperforms typical LiDAR and multi-view combination methods in time complexity and speed, indicating strong place recognition capabilities and real-time efficiency."],"url":"http://arxiv.org/abs/2405.07966v1","category":"cs.CV"}
{"created":"2024-05-13 17:38:53","title":"AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments","abstract":"Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at https://AgentClinic.github.io.","sentences":["Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it.","Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care.","However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work.","Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments.","In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection.","We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA.","We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents.","We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents.","Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA.","We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark.","We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents.","The code and data for this work is publicly available at https://AgentClinic.github.io."],"url":"http://arxiv.org/abs/2405.07960v1","category":"cs.HC"}
{"created":"2024-05-13 17:25:40","title":"Online Load and Graph Balancing for Random Order Inputs","abstract":"Online load balancing for heterogeneous machines aims to minimize the makespan (maximum machine workload) by scheduling arriving jobs with varying sizes on different machines. In the adversarial setting, where an adversary chooses not only the collection of job sizes but also their arrival order, the problem is well-understood and the optimal competitive ratio is known to be $\\Theta(\\log m)$ where $m$ is the number of machines. In the more realistic random arrival order model, the understanding is limited. Previously, the best lower bound on the competitive ratio was only $\\Omega(\\log \\log m)$.   We significantly improve this bound by showing an $\\Omega( \\sqrt {\\log m})$ lower bound, even for the restricted case where each job has a unit size on two machines and infinite size on the others. On the positive side, we propose an $O(\\log m/\\log \\log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model.","sentences":["Online load balancing for heterogeneous machines aims to minimize the makespan (maximum machine workload) by scheduling arriving jobs with varying sizes on different machines.","In the adversarial setting, where an adversary chooses not only the collection of job sizes but also their arrival order, the problem is well-understood and the optimal competitive ratio is known to be $\\Theta(\\log m)$ where $m$ is the number of machines.","In the more realistic random arrival order model, the understanding is limited.","Previously, the best lower bound on the competitive ratio was only $\\Omega(\\log \\log m)$.   ","We significantly improve this bound by showing an $\\Omega( \\sqrt {\\log m})$ lower bound, even for the restricted case where each job has a unit size on two machines and infinite size on the others.","On the positive side, we propose an $O(\\log m/\\log \\log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model."],"url":"http://arxiv.org/abs/2405.07949v1","category":"cs.DS"}
{"created":"2024-05-13 17:18:08","title":"Hierarchical Decision Mamba","abstract":"Recent advancements in imitation learning have been largely fueled by the integration of sequence models, which provide a structured flow of information to effectively mimic task behaviours. Currently, Decision Transformer (DT) and subsequently, the Hierarchical Decision Transformer (HDT), presented Transformer-based approaches to learn task policies. Recently, the Mamba architecture has shown to outperform Transformers across various task domains. In this work, we introduce two novel methods, Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the Transformer models. Through extensive experimentation across diverse environments such as OpenAI Gym and D4RL, leveraging varying demonstration data sets, we demonstrate the superiority of Mamba models over their Transformer counterparts in a majority of tasks. Results show that HDM outperforms other methods in most settings. The code can be found at https://github.com/meowatthemoon/HierarchicalDecisionMamba.","sentences":["Recent advancements in imitation learning have been largely fueled by the integration of sequence models, which provide a structured flow of information to effectively mimic task behaviours.","Currently, Decision Transformer (DT) and subsequently, the Hierarchical Decision Transformer (HDT), presented Transformer-based approaches to learn task policies.","Recently, the Mamba architecture has shown to outperform Transformers across various task domains.","In this work, we introduce two novel methods, Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the Transformer models.","Through extensive experimentation across diverse environments such as OpenAI Gym and D4RL, leveraging varying demonstration data sets, we demonstrate the superiority of Mamba models over their Transformer counterparts in a majority of tasks.","Results show that HDM outperforms other methods in most settings.","The code can be found at https://github.com/meowatthemoon/HierarchicalDecisionMamba."],"url":"http://arxiv.org/abs/2405.07943v1","category":"cs.LG"}
{"created":"2024-05-13 17:08:42","title":"PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition","abstract":"Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\"). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN.   We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset.","sentences":["Large language models (LLMs) have shown success in many natural language processing tasks.","Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models.","One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour.","A promising approach is to use the LLM itself as the safeguard.","Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy.","We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\").","In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs.","PARDEN neither requires finetuning nor white box access to the model.","We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2.","Code and data are available at https://github.com/Ed-Zh/PARDEN.   ","We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR).","For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset."],"url":"http://arxiv.org/abs/2405.07932v2","category":"cs.CL"}
{"created":"2024-05-13 16:57:48","title":"Stable Diffusion-based Data Augmentation for Federated Learning with Non-IID Data","abstract":"The proliferation of edge devices has brought Federated Learning (FL) to the forefront as a promising paradigm for decentralized and collaborative model training while preserving the privacy of clients' data. However, FL struggles with a significant performance reduction and poor convergence when confronted with Non-Independent and Identically Distributed (Non-IID) data distributions among participating clients. While previous efforts, such as client drift mitigation and advanced server-side model fusion techniques, have shown some success in addressing this challenge, they often overlook the root cause of the performance reduction - the absence of identical data accurately mirroring the global data distribution among clients. In this paper, we introduce Gen-FedSD, a novel approach that harnesses the powerful capability of state-of-the-art text-to-image foundation models to bridge the significant Non-IID performance gaps in FL. In Gen-FedSD, each client constructs textual prompts for each class label and leverages an off-the-shelf state-of-the-art pre-trained Stable Diffusion model to synthesize high-quality data samples. The generated synthetic data is tailored to each client's unique local data gaps and distribution disparities, effectively making the final augmented local data IID. Through extensive experimentation, we demonstrate that Gen-FedSD achieves state-of-the-art performance and significant communication cost savings across various datasets and Non-IID settings.","sentences":["The proliferation of edge devices has brought Federated Learning (FL) to the forefront as a promising paradigm for decentralized and collaborative model training while preserving the privacy of clients' data.","However, FL struggles with a significant performance reduction and poor convergence when confronted with Non-Independent and Identically Distributed (Non-IID) data distributions among participating clients.","While previous efforts, such as client drift mitigation and advanced server-side model fusion techniques, have shown some success in addressing this challenge, they often overlook the root cause of the performance reduction - the absence of identical data accurately mirroring the global data distribution among clients.","In this paper, we introduce Gen-FedSD, a novel approach that harnesses the powerful capability of state-of-the-art text-to-image foundation models to bridge the significant Non-IID performance gaps in FL.","In Gen-FedSD, each client constructs textual prompts for each class label and leverages an off-the-shelf state-of-the-art pre-trained Stable Diffusion model to synthesize high-quality data samples.","The generated synthetic data is tailored to each client's unique local data gaps and distribution disparities, effectively making the final augmented local data IID.","Through extensive experimentation, we demonstrate that Gen-FedSD achieves state-of-the-art performance and significant communication cost savings across various datasets and Non-IID settings."],"url":"http://arxiv.org/abs/2405.07925v1","category":"cs.LG"}
{"created":"2024-05-13 16:47:53","title":"IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data","abstract":"In this paper, we address two critical challenges in the domain of flood detection: the computational expense of large-scale time series change detection and the lack of interpretable decision-making processes on explainable AI (XAI). To overcome these challenges, we proposed an interpretable multi-stage approach to flood detection, IMAFD has been proposed. It provides an automatic, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into the decision-making process. The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation. It combines anomaly detection (at both image and pixel level) with semantic segmentation. The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making. Our contributions are two folder. First, we efficiently reduced the number of frames to be processed for dense change detection by providing a multi-stage holistic approach to flood detection. Second, the proposed semantic change detection method (stage 3) provides human users with an interpretable decision-making process, while most of the explainable AI (XAI) methods provide post hoc explanations. The evaluation of the proposed IMAFD framework was performed on three datasets, WorldFloods, RavAEn and MediaEval. For all the above datasets, the proposed framework demonstrates a competitive performance compared to other methods offering also interpretability and insight.","sentences":["In this paper, we address two critical challenges in the domain of flood detection: the computational expense of large-scale time series change detection and the lack of interpretable decision-making processes on explainable AI (XAI).","To overcome these challenges, we proposed an interpretable multi-stage approach to flood detection, IMAFD has been proposed.","It provides an automatic, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into the decision-making process.","The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation.","It combines anomaly detection (at both image and pixel level) with semantic segmentation.","The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making.","Our contributions are two folder.","First, we efficiently reduced the number of frames to be processed for dense change detection by providing a multi-stage holistic approach to flood detection.","Second, the proposed semantic change detection method (stage 3) provides human users with an interpretable decision-making process, while most of the explainable AI (XAI) methods provide post hoc explanations.","The evaluation of the proposed IMAFD framework was performed on three datasets, WorldFloods, RavAEn and MediaEval.","For all the above datasets, the proposed framework demonstrates a competitive performance compared to other methods offering also interpretability and insight."],"url":"http://arxiv.org/abs/2405.07916v1","category":"cs.CV"}
{"created":"2024-05-13 16:40:17","title":"PLUTO: Pathology-Universal Transformer","abstract":"Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease. Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites and extracts meaningful representations across multiple WSI scales that enable a large variety of downstream pathology tasks. In particular, we design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks which span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We compare PLUTO's performance to other state-of-the-art methods on a diverse set of external and internal benchmarks covering multiple biologically relevant tasks, tissue types, resolutions, stains, and scanners. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications.","sentences":["Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease.","Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions.","In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites and extracts meaningful representations across multiple WSI scales that enable a large variety of downstream pathology tasks.","In particular, we design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks which span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction.","We compare PLUTO's performance to other state-of-the-art methods on a diverse set of external and internal benchmarks covering multiple biologically relevant tasks, tissue types, resolutions, stains, and scanners.","We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO.","Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications."],"url":"http://arxiv.org/abs/2405.07905v1","category":"eess.IV"}
{"created":"2024-05-13 16:37:23","title":"Physically Consistent Online Inertial Adaptation for Humanoid Loco-manipulation","abstract":"The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots. Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous. External loads constitute a large model bias, which is typically unaccounted for. In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads. We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller. We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot.","sentences":["The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots.","Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous.","External loads constitute a large model bias, which is typically unaccounted for.","In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads.","We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller.","We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot."],"url":"http://arxiv.org/abs/2405.07901v1","category":"cs.RO"}
{"created":"2024-05-13 16:28:00","title":"Science based AI model certification for new operational environments with application in traffic state estimation","abstract":"The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training. Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data. However, interpreting the opaque nature of AI's black-box models remains a persistent challenge. Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in new operational environments. The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models. This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation. Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments. This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions.","sentences":["The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training.","Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data.","However, interpreting the opaque nature of AI's black-box models remains a persistent challenge.","Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in new operational environments.","The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models.","This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions.","The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation.","Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models.","By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments.","This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions."],"url":"http://arxiv.org/abs/2405.07893v1","category":"cs.AI"}
{"created":"2024-05-13 15:50:39","title":"RLHF Workflow: From Reward Modeling to Online RLHF","abstract":"We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information.","sentences":["We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature.","However, existing open-source RLHF projects are still largely confined to the offline learning setting.","In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF.","In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback.","Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation.","Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA.","We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets.","Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available.","Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information."],"url":"http://arxiv.org/abs/2405.07863v1","category":"cs.LG"}
{"created":"2024-05-13 15:42:46","title":"Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs","abstract":"The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs.","sentences":["The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields.","This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices.","However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept.","This phenomenon leads to instability and inefficiency when training poses are sparse.","In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals.","The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details.","We demonstrate that using residual connections between them seamlessly preserves their own inherent properties.","Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features.","We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs."],"url":"http://arxiv.org/abs/2405.07857v1","category":"cs.CV"}
{"created":"2024-05-13 15:25:03","title":"Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics","abstract":"Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an effective sampler for non-convex learning in large-scale datasets. However, the simulation may encounter stagnation issues when the high-temperature chain delves too deeply into the distribution tails. To tackle this issue, we propose reflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex exploration by utilizing reflection steps within a bounded domain. Theoretically, we observe that reducing the diameter of the domain enhances mixing rates, exhibiting a \\emph{quadratic} behavior. Empirically, we test its performance through extensive experiments, including identifying dynamical systems with physical constraints, simulations of constrained multi-modal distributions, and image classification tasks. The theoretical and empirical findings highlight the crucial role of constrained exploration in improving the simulation efficiency.","sentences":["Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an effective sampler for non-convex learning in large-scale datasets.","However, the simulation may encounter stagnation issues when the high-temperature chain delves too deeply into the distribution tails.","To tackle this issue, we propose reflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex exploration by utilizing reflection steps within a bounded domain.","Theoretically, we observe that reducing the diameter of the domain enhances mixing rates, exhibiting a \\emph{quadratic} behavior.","Empirically, we test its performance through extensive experiments, including identifying dynamical systems with physical constraints, simulations of constrained multi-modal distributions, and image classification tasks.","The theoretical and empirical findings highlight the crucial role of constrained exploration in improving the simulation efficiency."],"url":"http://arxiv.org/abs/2405.07839v1","category":"cs.LG"}
{"created":"2024-05-13 15:24:27","title":"Adaptive Exploration for Data-Efficient General Value Function Evaluations","abstract":"General Value Functions (GVFs) (Sutton et al, 2011) are an established way to represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique pseudo-reward. Multiple GVFs can be estimated in parallel using off-policy learning from a single stream of data, often sourced from a fixed behavior policy or pre-collected dataset. This leaves an open question: how can behavior policy be chosen for data-efficient GVF learning? To address this gap, we propose GVFExplorer, which aims at learning a behavior policy that efficiently gathers data for evaluating multiple GVFs in parallel. This behavior policy selects actions in proportion to the total variance in the return across all GVFs, reducing the number of environmental interactions. To enable accurate variance estimation, we use a recently proposed temporal-difference-style variance estimator. We prove that each behavior policy update reduces the mean squared error in the summed predictions over all GVFs. We empirically demonstrate our method's performance in both tabular representations and nonlinear function approximation.","sentences":["General Value Functions (GVFs) (Sutton et al, 2011) are an established way to represent predictive knowledge in reinforcement learning.","Each GVF computes the expected return for a given policy, based on a unique pseudo-reward.","Multiple GVFs can be estimated in parallel using off-policy learning from a single stream of data, often sourced from a fixed behavior policy or pre-collected dataset.","This leaves an open question: how can behavior policy be chosen for data-efficient GVF learning?","To address this gap, we propose GVFExplorer, which aims at learning a behavior policy that efficiently gathers data for evaluating multiple GVFs in parallel.","This behavior policy selects actions in proportion to the total variance in the return across all GVFs, reducing the number of environmental interactions.","To enable accurate variance estimation, we use a recently proposed temporal-difference-style variance estimator.","We prove that each behavior policy update reduces the mean squared error in the summed predictions over all GVFs.","We empirically demonstrate our method's performance in both tabular representations and nonlinear function approximation."],"url":"http://arxiv.org/abs/2405.07838v1","category":"cs.LG"}
{"created":"2024-05-13 15:20:21","title":"Subradiance and Superradiant Long Range Excitation Transport among Quantum Emitter Ensembles in a Waveguide","abstract":"In contrast to free space, in waveguides the dispersive and dissipative dipole-dipole interactions among quantum emitters exhibit a periodic behavior over remarkably long distances. We propose a novel setup exploiting this long-range periodicity in order to create highly excited subradiant states and facilitate fast controlled collective energy transport amongst far-apart ensembles coupled to a waveguide. For sufficiently large ensembles collective superradiant emission into the fiber modes dominates over its free space counterpart. We show that for a large number of emitters a fast transverse coherent pulse can create almost perfect subradiant states with up to $50\\%$ excitation. On the other hand, for a coherent excitation of one sub-ensemble above an overall excitation fraction of $50\\%$ we find a nearly lossless and fast energy transfer to the ground state sub-ensemble. This transport can be enhanced or suppressed by controlling the positions of the ensembles relative to each other, while it can also be realized with a random position distribution. In the optimally enhanced case this fast transfer appears as superradiant emission with subsequent superabsorption, yet, without a superradiant decay after the absorption. The highly excited subradiant states as well as the superradiant excitation transfer appear as suitable building blocks in applications like active atomic clocks, quantum batteries, quantum information protocols and quantum metrology procedures such as fiber-based Ramsey schemes.","sentences":["In contrast to free space, in waveguides the dispersive and dissipative dipole-dipole interactions among quantum emitters exhibit a periodic behavior over remarkably long distances.","We propose a novel setup exploiting this long-range periodicity in order to create highly excited subradiant states and facilitate fast controlled collective energy transport amongst far-apart ensembles coupled to a waveguide.","For sufficiently large ensembles collective superradiant emission into the fiber modes dominates over its free space counterpart.","We show that for a large number of emitters a fast transverse coherent pulse can create almost perfect subradiant states with up to $50\\%$ excitation.","On the other hand, for a coherent excitation of one sub-ensemble above an overall excitation fraction of $50\\%$ we find a nearly lossless and fast energy transfer to the ground state sub-ensemble.","This transport can be enhanced or suppressed by controlling the positions of the ensembles relative to each other, while it can also be realized with a random position distribution.","In the optimally enhanced case this fast transfer appears as superradiant emission with subsequent superabsorption, yet, without a superradiant decay after the absorption.","The highly excited subradiant states as well as the superradiant excitation transfer appear as suitable building blocks in applications like active atomic clocks, quantum batteries, quantum information protocols and quantum metrology procedures such as fiber-based Ramsey schemes."],"url":"http://arxiv.org/abs/2405.07833v1","category":"quant-ph"}
{"created":"2024-05-13 15:15:37","title":"Joint Precoding for RIS-Assisted Wideband THz Cell-Free Massive MIMO Systems","abstract":"Terahertz (THz) cell-free massive multiple-input-multiple-output (mMIMO) networks have been envisioned as a prospective technology for achieving higher system capacity, improved performance, and ultra-high reliability in 6G networks. However, due to severe attenuation and limited scattering in THz transmission, as well as high power consumption for increased number of access points (APs), further improvement of network capacity becomes challenging. Reconfigurable intelligent surface (RIS) has been introduced as a low-cost solution to reduce AP deployment and assist in data transmission. However, due to the ultra-wide bandwidth and frequency-dependent characteristics of RISs, beam split effect has become an unavoidable obstacle. To compensate the severe performance degradation caused by beam split effect, we introduce additional time delay (TD) layers at both access points (APs) and RISs. Accordingly, we propose a joint precoding framework at APs and RISs to fully unleash the potential of the considered network. Specifically, we first formulate the joint precoding as a non-convex optimization problem. Then, given the location of unchanged RISs, we adjust the time delays (TDs) of APs to align the generated beams towards RISs. After that, with knowledge of the optimal TDs of APs, we decouple the optimization problem into three subproblems of optimizing the baseband beamformers, RISs and TDs of RISs, respectively. Exploiting multidimensional complex quadratic transform, we transform the subproblems into convex forms and solve them under alternate optimizing framework. Numerical results verify that the proposed method can effectively mitigate beam split effect and significantly improve the achievable rate compared with conventional cell-free mMIMO networks.","sentences":["Terahertz (THz) cell-free massive multiple-input-multiple-output (mMIMO) networks have been envisioned as a prospective technology for achieving higher system capacity, improved performance, and ultra-high reliability in 6G networks.","However, due to severe attenuation and limited scattering in THz transmission, as well as high power consumption for increased number of access points (APs), further improvement of network capacity becomes challenging.","Reconfigurable intelligent surface (RIS) has been introduced as a low-cost solution to reduce AP deployment and assist in data transmission.","However, due to the ultra-wide bandwidth and frequency-dependent characteristics of RISs, beam split effect has become an unavoidable obstacle.","To compensate the severe performance degradation caused by beam split effect, we introduce additional time delay (TD) layers at both access points (APs) and RISs.","Accordingly, we propose a joint precoding framework at APs and RISs to fully unleash the potential of the considered network.","Specifically, we first formulate the joint precoding as a non-convex optimization problem.","Then, given the location of unchanged RISs, we adjust the time delays (TDs) of APs to align the generated beams towards RISs.","After that, with knowledge of the optimal TDs of APs, we decouple the optimization problem into three subproblems of optimizing the baseband beamformers, RISs and TDs of RISs, respectively.","Exploiting multidimensional complex quadratic transform, we transform the subproblems into convex forms and solve them under alternate optimizing framework.","Numerical results verify that the proposed method can effectively mitigate beam split effect and significantly improve the achievable rate compared with conventional cell-free mMIMO networks."],"url":"http://arxiv.org/abs/2405.07830v1","category":"eess.SP"}
{"created":"2024-05-13 15:12:21","title":"Automatic Recognition of Food Ingestion Environment from the AIM-2 Wearable Sensor","abstract":"Detecting an ingestion environment is an important aspect of monitoring dietary intake. It provides insightful information for dietary assessment. However, it is a challenging problem where human-based reviewing can be tedious, and algorithm-based review suffers from data imbalance and perceptual aliasing problems. To address these issues, we propose a neural network-based method with a two-stage training framework that tactfully combines fine-tuning and transfer learning techniques. Our method is evaluated on a newly collected dataset called ``UA Free Living Study\", which uses an egocentric wearable camera, AIM-2 sensor, to simulate food consumption in free-living conditions. The proposed training framework is applied to common neural network backbones, combined with approaches in the general imbalanced classification field. Experimental results on the collected dataset show that our proposed method for automatic ingestion environment recognition successfully addresses the challenging data imbalance problem in the dataset and achieves a promising overall classification accuracy of 96.63%.","sentences":["Detecting an ingestion environment is an important aspect of monitoring dietary intake.","It provides insightful information for dietary assessment.","However, it is a challenging problem where human-based reviewing can be tedious, and algorithm-based review suffers from data imbalance and perceptual aliasing problems.","To address these issues, we propose a neural network-based method with a two-stage training framework that tactfully combines fine-tuning and transfer learning techniques.","Our method is evaluated on a newly collected dataset called ``UA Free Living Study\", which uses an egocentric wearable camera, AIM-2 sensor, to simulate food consumption in free-living conditions.","The proposed training framework is applied to common neural network backbones, combined with approaches in the general imbalanced classification field.","Experimental results on the collected dataset show that our proposed method for automatic ingestion environment recognition successfully addresses the challenging data imbalance problem in the dataset and achieves a promising overall classification accuracy of 96.63%."],"url":"http://arxiv.org/abs/2405.07827v1","category":"cs.MM"}
{"created":"2024-05-13 15:08:02","title":"Integrating Multi-Physics Simulations and Machine Learning to Define the Spatter Mechanism and Process Window in Laser Powder Bed Fusion","abstract":"Laser powder bed fusion (LPBF) has shown promise for wide range of applications due to its ability to fabricate freeform geometries and generate a controlled microstructure. However, components generated by LPBF still possess sub-optimal mechanical properties due to the defects that are created during laser-material interactions. In this work, we investigate mechanism of spatter formation, using a high-fidelity modelling tool that was built to simulate the multi-physics phenomena in LPBF. The modelling tool have the capability to capture the 3D resolution of the meltpool and the spatter behavior. To understand spatter behavior and formation, we reveal its properties at ejection and evaluate its variation from the meltpool, the source where it is formed. The dataset of the spatter and the meltpool collected consist of 50 % spatter and 50 % melt pool samples, with features that include position components, velocity components, velocity magnitude, temperature, density and pressure. The relationship between the spatter and the meltpool were evaluated via correlation analysis and machine learning (ML) algorithms for classification tasks. Upon screening different ML algorithms on the dataset, a high accuracy was observed for all the ML models, with ExtraTrees having the highest at 96 % and KNN having the lowest at 94 %.","sentences":["Laser powder bed fusion (LPBF) has shown promise for wide range of applications due to its ability to fabricate freeform geometries and generate a controlled microstructure.","However, components generated by LPBF still possess sub-optimal mechanical properties due to the defects that are created during laser-material interactions.","In this work, we investigate mechanism of spatter formation, using a high-fidelity modelling tool that was built to simulate the multi-physics phenomena in LPBF.","The modelling tool have the capability to capture the 3D resolution of the meltpool and the spatter behavior.","To understand spatter behavior and formation, we reveal its properties at ejection and evaluate its variation from the meltpool, the source where it is formed.","The dataset of the spatter and the meltpool collected consist of 50 % spatter and 50 % melt pool samples, with features that include position components, velocity components, velocity magnitude, temperature, density and pressure.","The relationship between the spatter and the meltpool were evaluated via correlation analysis and machine learning (ML) algorithms for classification tasks.","Upon screening different ML algorithms on the dataset, a high accuracy was observed for all the ML models, with ExtraTrees having the highest at 96 % and KNN having the lowest at 94 %."],"url":"http://arxiv.org/abs/2405.07823v1","category":"cs.LG"}
{"created":"2024-05-13 15:07:52","title":"Synthetic Tabular Data Validation: A Divergence-Based Approach","abstract":"The ever-increasing use of generative models in various fields where tabular data is used highlights the need for robust and standardized validation metrics to assess the similarity between real and synthetic data. Current methods lack a unified framework and rely on diverse and often inconclusive statistical measures. Divergences, which quantify discrepancies between data distributions, offer a promising avenue for validation. However, traditional approaches calculate divergences independently for each feature due to the complexity of joint distribution modeling. This paper addresses this challenge by proposing a novel approach that uses divergence estimation to overcome the limitations of marginal comparisons. Our core contribution lies in applying a divergence estimator to build a validation metric considering the joint distribution of real and synthetic data. We leverage a probabilistic classifier to approximate the density ratio between datasets, allowing the capture of complex relationships. We specifically calculate two divergences: the well-known Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. KL divergence offers an established use in the field, while JS divergence is symmetric and bounded, providing a reliable metric. The efficacy of this approach is demonstrated through a series of experiments with varying distribution complexities. The initial phase involves comparing estimated divergences with analytical solutions for simple distributions, setting a benchmark for accuracy. Finally, we validate our method on a real-world dataset and its corresponding synthetic counterpart, showcasing its effectiveness in practical applications. This research offers a significant contribution with applicability beyond tabular data and the potential to improve synthetic data validation in various fields.","sentences":["The ever-increasing use of generative models in various fields where tabular data is used highlights the need for robust and standardized validation metrics to assess the similarity between real and synthetic data.","Current methods lack a unified framework and rely on diverse and often inconclusive statistical measures.","Divergences, which quantify discrepancies between data distributions, offer a promising avenue for validation.","However, traditional approaches calculate divergences independently for each feature due to the complexity of joint distribution modeling.","This paper addresses this challenge by proposing a novel approach that uses divergence estimation to overcome the limitations of marginal comparisons.","Our core contribution lies in applying a divergence estimator to build a validation metric considering the joint distribution of real and synthetic data.","We leverage a probabilistic classifier to approximate the density ratio between datasets, allowing the capture of complex relationships.","We specifically calculate two divergences: the well-known Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence.","KL divergence offers an established use in the field, while JS divergence is symmetric and bounded, providing a reliable metric.","The efficacy of this approach is demonstrated through a series of experiments with varying distribution complexities.","The initial phase involves comparing estimated divergences with analytical solutions for simple distributions, setting a benchmark for accuracy.","Finally, we validate our method on a real-world dataset and its corresponding synthetic counterpart, showcasing its effectiveness in practical applications.","This research offers a significant contribution with applicability beyond tabular data and the potential to improve synthetic data validation in various fields."],"url":"http://arxiv.org/abs/2405.07822v1","category":"cs.LG"}
{"created":"2024-05-13 14:59:44","title":"The Power of Combined Modalities in Interactive Robot Learning","abstract":"This study contributes to the evolving field of robot learning in interaction with humans, examining the impact of diverse input modalities on learning outcomes. It introduces the concept of \"meta-modalities\" which encapsulate additional forms of feedback beyond the traditional preference and scalar feedback mechanisms. Unlike prior research that focused on individual meta-modalities, this work evaluates their combined effect on learning outcomes. Through a study with human participants, we explore user preferences for these modalities and their impact on robot learning performance. Our findings reveal that while individual modalities are perceived differently, their combination significantly improves learning behavior and usability. This research not only provides valuable insights into the optimization of human-robot interactive task learning but also opens new avenues for enhancing the interactive freedom and scaffolding capabilities provided to users in such settings.","sentences":["This study contributes to the evolving field of robot learning in interaction with humans, examining the impact of diverse input modalities on learning outcomes.","It introduces the concept of \"meta-modalities\" which encapsulate additional forms of feedback beyond the traditional preference and scalar feedback mechanisms.","Unlike prior research that focused on individual meta-modalities, this work evaluates their combined effect on learning outcomes.","Through a study with human participants, we explore user preferences for these modalities and their impact on robot learning performance.","Our findings reveal that while individual modalities are perceived differently, their combination significantly improves learning behavior and usability.","This research not only provides valuable insights into the optimization of human-robot interactive task learning but also opens new avenues for enhancing the interactive freedom and scaffolding capabilities provided to users in such settings."],"url":"http://arxiv.org/abs/2405.07817v1","category":"cs.RO"}
{"created":"2024-05-13 14:58:57","title":"Quick and Accurate Affordance Learning","abstract":"Infants learn actively in their environments, shaping their own learning curricula. They learn about their environments' affordances, that is, how local circumstances determine how their behavior can affect the environment. Here we model this type of behavior by means of a deep learning architecture. The architecture mediates between global cognitive map exploration and local affordance learning. Inference processes actively move the simulated agent towards regions where they expect affordance-related knowledge gain. We contrast three measures of uncertainty to guide this exploration: predicted uncertainty of a model, standard deviation between the means of several models (SD), and the Jensen-Shannon Divergence (JSD) between several models. We show that the first measure gets fooled by aleatoric uncertainty inherent in the environment, while the two other measures focus learning on epistemic uncertainty. JSD exhibits the most balanced exploration strategy. From a computational perspective, our model suggests three key ingredients for coordinating the active generation of learning curricula: (1) Navigation behavior needs to be coordinated with local motor behavior for enabling active affordance learning. (2) Affordances need to be encoded locally for acquiring generalized knowledge. (3) Effective active affordance learning mechanisms should use density comparison techniques for estimating expected knowledge gain. Future work may seek collaborations with developmental psychology to model active play in children in more realistic scenarios.","sentences":["Infants learn actively in their environments, shaping their own learning curricula.","They learn about their environments' affordances, that is, how local circumstances determine how their behavior can affect the environment.","Here we model this type of behavior by means of a deep learning architecture.","The architecture mediates between global cognitive map exploration and local affordance learning.","Inference processes actively move the simulated agent towards regions where they expect affordance-related knowledge gain.","We contrast three measures of uncertainty to guide this exploration: predicted uncertainty of a model, standard deviation between the means of several models (SD), and the Jensen-Shannon Divergence (JSD) between several models.","We show that the first measure gets fooled by aleatoric uncertainty inherent in the environment, while the two other measures focus learning on epistemic uncertainty.","JSD exhibits the most balanced exploration strategy.","From a computational perspective, our model suggests three key ingredients for coordinating the active generation of learning curricula: (1) Navigation behavior needs to be coordinated with local motor behavior for enabling active affordance learning.","(2) Affordances need to be encoded locally for acquiring generalized knowledge.","(3) Effective active affordance learning mechanisms should use density comparison techniques for estimating expected knowledge gain.","Future work may seek collaborations with developmental psychology to model active play in children in more realistic scenarios."],"url":"http://arxiv.org/abs/2405.07816v1","category":"cs.AI"}
{"created":"2024-05-13 14:56:55","title":"NutritionVerse-Direct: Exploring Deep Neural Networks for Multitask Nutrition Prediction from Food Images","abstract":"Many aging individuals encounter challenges in effectively tracking their dietary intake, exacerbating their susceptibility to nutrition-related health complications. Self-reporting methods are often inaccurate and suffer from substantial bias; however, leveraging intelligent prediction methods can automate and enhance precision in this process. Recent work has explored using computer vision prediction systems to predict nutritional information from food images. Still, these methods are often tailored to specific situations, require other inputs in addition to a food image, or do not provide comprehensive nutritional information.   This paper aims to enhance the efficacy of dietary intake estimation by leveraging various neural network architectures to directly predict a meal's nutritional content from its image. Through comprehensive experimentation and evaluation, we present NutritionVerse-Direct, a model utilizing a vision transformer base architecture with three fully connected layers that lead to five regression heads predicting calories (kcal), mass (g), protein (g), fat (g), and carbohydrates (g) present in a meal. NutritionVerse-Direct yields a combined mean average error score on the NutritionVerse-Real dataset of 412.6, an improvement of 25.5% over the Inception-ResNet model, demonstrating its potential for improving dietary intake estimation accuracy.","sentences":["Many aging individuals encounter challenges in effectively tracking their dietary intake, exacerbating their susceptibility to nutrition-related health complications.","Self-reporting methods are often inaccurate and suffer from substantial bias; however, leveraging intelligent prediction methods can automate and enhance precision in this process.","Recent work has explored using computer vision prediction systems to predict nutritional information from food images.","Still, these methods are often tailored to specific situations, require other inputs in addition to a food image, or do not provide comprehensive nutritional information.   ","This paper aims to enhance the efficacy of dietary intake estimation by leveraging various neural network architectures to directly predict a meal's nutritional content from its image.","Through comprehensive experimentation and evaluation, we present NutritionVerse-Direct, a model utilizing a vision transformer base architecture with three fully connected layers that lead to five regression heads predicting calories (kcal), mass (g), protein (g), fat (g), and carbohydrates (g) present in a meal.","NutritionVerse-Direct yields a combined mean average error score on the NutritionVerse-Real dataset of 412.6, an improvement of 25.5% over the Inception-ResNet model, demonstrating its potential for improving dietary intake estimation accuracy."],"url":"http://arxiv.org/abs/2405.07814v1","category":"cs.CV"}
{"created":"2024-05-13 14:54:37","title":"Localizing Task Information for Improved Model Merging and Compression","abstract":"Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve >99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance.","sentences":["Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss.","Previous works have linked these drops to interference in the weight space and erasure of important task-specific features.","Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights.","We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve >99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints.","We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion.","For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches.","Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches.","Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."],"url":"http://arxiv.org/abs/2405.07813v1","category":"cs.LG"}
{"created":"2024-05-13 14:47:34","title":"A Decentralized and Self-Adaptive Approach for Monitoring Volatile Edge Environments","abstract":"Edge computing provides resources for IoT workloads at the network edge. Monitoring systems are vital for efficiently managing resources and application workloads by collecting, storing, and providing relevant information about the state of the resources. However, traditional monitoring systems have a centralized architecture for both data plane and control plane, which increases latency, creates a failure bottleneck, and faces challenges in providing quick and trustworthy data in volatile edge environments, especially where infrastructures are often built upon failure-prone, unsophisticated computing and network resources. Thus, we propose DEMon, a decentralized, self-adaptive monitoring system for edge. DEMon leverages the stochastic gossip communication protocol at its core. It develops efficient protocols for information dissemination, communication, and retrieval, avoiding a single point of failure and ensuring fast and trustworthy data access. Its decentralized control enables self-adaptive management of monitoring parameters, addressing the trade-offs between the quality of service of monitoring and resource consumption. We implement the proposed system as a lightweight and portable container-based system and evaluate it through experiments. We also present a use case demonstrating its feasibility. The results show that DEMon efficiently disseminates and retrieves the monitoring information, addressing the challenges of edge monitoring.","sentences":["Edge computing provides resources for IoT workloads at the network edge.","Monitoring systems are vital for efficiently managing resources and application workloads by collecting, storing, and providing relevant information about the state of the resources.","However, traditional monitoring systems have a centralized architecture for both data plane and control plane, which increases latency, creates a failure bottleneck, and faces challenges in providing quick and trustworthy data in volatile edge environments, especially where infrastructures are often built upon failure-prone, unsophisticated computing and network resources.","Thus, we propose DEMon, a decentralized, self-adaptive monitoring system for edge.","DEMon leverages the stochastic gossip communication protocol at its core.","It develops efficient protocols for information dissemination, communication, and retrieval, avoiding a single point of failure and ensuring fast and trustworthy data access.","Its decentralized control enables self-adaptive management of monitoring parameters, addressing the trade-offs between the quality of service of monitoring and resource consumption.","We implement the proposed system as a lightweight and portable container-based system and evaluate it through experiments.","We also present a use case demonstrating its feasibility.","The results show that DEMon efficiently disseminates and retrieves the monitoring information, addressing the challenges of edge monitoring."],"url":"http://arxiv.org/abs/2405.07806v1","category":"cs.DC"}
{"created":"2024-05-13 14:47:30","title":"The 25th anniversary for nuclear chirality","abstract":"The brief history for the prediction of the nuclear chirality is provided. The theoretical and experimental investigations of the nuclear chirality are reviewed, including the verification of chiral doublet bands, the chiral conundrum and its resolution, and the prediction and observation of the multiple chiral doublets (M$\\chi$D). Some recent theoretical progresses are highlighted, including the chiral collective Hamiltonian, the A-plot and the K-plot, the nuclear chirality-parity (ChP) violation, the chiral rotation induced by the pairing correlations, as well as the chiral dynamics. The possibly emerging area, challenges that lie ahead, and opportunities for progress in the context of the nuclear chirality are discussed.","sentences":["The brief history for the prediction of the nuclear chirality is provided.","The theoretical and experimental investigations of the nuclear chirality are reviewed, including the verification of chiral doublet bands, the chiral conundrum and its resolution, and the prediction and observation of the multiple chiral doublets (M$\\chi$D).","Some recent theoretical progresses are highlighted, including the chiral collective Hamiltonian, the A-plot and the K-plot, the nuclear chirality-parity (ChP) violation, the chiral rotation induced by the pairing correlations, as well as the chiral dynamics.","The possibly emerging area, challenges that lie ahead, and opportunities for progress in the context of the nuclear chirality are discussed."],"url":"http://arxiv.org/abs/2405.07805v1","category":"nucl-th"}
{"created":"2024-05-13 14:45:08","title":"Decoding Geometric Properties in Non-Random Data from First Information-Theoretic Principles","abstract":"Based on the principles of information theory, measure theory, and theoretical computer science, we introduce a univariate signal deconvolution method with a wide range of applications to coding theory, particularly in zero-knowledge one-way communication channels, such as in deciphering messages from unknown generating sources about which no prior knowledge is available and to which no return message can be sent. Our multidimensional space reconstruction method from an arbitrary received signal is proven to be agnostic vis-a-vis the encoding-decoding scheme, computation model, programming language, formal theory, the computable (or semi-computable) method of approximation to algorithmic complexity, and any arbitrarily chosen (computable) probability measure of the events. The method derives from the principles of an approach to Artificial General Intelligence capable of building a general-purpose model of models independent of any arbitrarily assumed prior probability distribution. We argue that this optimal and universal method of decoding non-random data has applications to signal processing, causal deconvolution, topological and geometric properties encoding, cryptography, and bio- and technosignature detection.","sentences":["Based on the principles of information theory, measure theory, and theoretical computer science, we introduce a univariate signal deconvolution method with a wide range of applications to coding theory, particularly in zero-knowledge one-way communication channels, such as in deciphering messages from unknown generating sources about which no prior knowledge is available and to which no return message can be sent.","Our multidimensional space reconstruction method from an arbitrary received signal is proven to be agnostic vis-a-vis the encoding-decoding scheme, computation model, programming language, formal theory, the computable (or semi-computable) method of approximation to algorithmic complexity, and any arbitrarily chosen (computable) probability measure of the events.","The method derives from the principles of an approach to Artificial General Intelligence capable of building a general-purpose model of models independent of any arbitrarily assumed prior probability distribution.","We argue that this optimal and universal method of decoding non-random data has applications to signal processing, causal deconvolution, topological and geometric properties encoding, cryptography, and bio- and technosignature detection."],"url":"http://arxiv.org/abs/2405.07803v1","category":"cs.IT"}
{"created":"2024-05-13 14:42:21","title":"Collective Decision-Making on Task Allocation Feasibility","abstract":"Robot swarms offer the potential to bring several advantages to the real-world applications but deploying them presents challenges in ensuring feasibility across diverse environments. Assessing the feasibility of new tasks for swarms is crucial to ensure the effective utilisation of resources, as well as to provide awareness of the suitability of a swarm solution for a particular task. In this paper, we introduce the concept of distributed feasibility, where the swarm collectively assesses the feasibility of task allocation based on local observations and interactions. We apply Direct Modulation of Majority-based Decisions as our collective decision-making strategy and show that, in a homogeneous setting, the swarm is able to collectively decide whether a given setup has a high or low feasibility as long as the robot-to-task ratio is not near one.","sentences":["Robot swarms offer the potential to bring several advantages to the real-world applications but deploying them presents challenges in ensuring feasibility across diverse environments.","Assessing the feasibility of new tasks for swarms is crucial to ensure the effective utilisation of resources, as well as to provide awareness of the suitability of a swarm solution for a particular task.","In this paper, we introduce the concept of distributed feasibility, where the swarm collectively assesses the feasibility of task allocation based on local observations and interactions.","We apply Direct Modulation of Majority-based Decisions as our collective decision-making strategy and show that, in a homogeneous setting, the swarm is able to collectively decide whether a given setup has a high or low feasibility as long as the robot-to-task ratio is not near one."],"url":"http://arxiv.org/abs/2405.07799v1","category":"cs.RO"}
{"created":"2024-05-13 14:42:13","title":"FreeVA: Offline MLLM as Training-Free Video Assistant","abstract":"This paper undertakes an empirical study to revisit the latest advancements in Multimodal Large Language Models (MLLMs): Video Assistant. This study, namely FreeVA, aims to extend existing image-based MLLM to the video domain in a training-free manner. The study provides an essential, yet must-know baseline, and reveals several surprising findings: 1) FreeVA, leveraging only offline image-based MLLM without additional training, excels in zero-shot video question-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even surpassing state-of-the-art methods that involve video instruction tuning. 2) While mainstream video-based MLLMs typically initialize with an image-based MLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study indicates that utilizing the widely adopted VideoInstruct-100K for video instruction tuning doesn't actually lead to better performance compared to not training at all. 3) The commonly used evaluation metrics in existing works are significantly influenced by changes in the GPT API version over time. If ignored, this could affect the fairness and uniformity of comparisons between different methods and impact the analysis and judgment of researchers in the field. The advancement of MLLMs is currently thriving, drawing numerous researchers into the field. We aim for this work to serve as a plug-and-play, simple yet effective baseline, encouraging the direct evaluation of existing MLLMs in video domain while also standardizing the field of video conversational models to a certain extent. Also, we encourage researchers to reconsider: Have current video MLLM methods truly acquired knowledge beyond image MLLM? Code is available at https://github.com/whwu95/FreeVA","sentences":["This paper undertakes an empirical study to revisit the latest advancements in Multimodal Large Language Models (MLLMs): Video Assistant.","This study, namely FreeVA, aims to extend existing image-based MLLM to the video domain in a training-free manner.","The study provides an essential, yet must-know baseline, and reveals several surprising findings: 1) FreeVA, leveraging only offline image-based MLLM without additional training, excels in zero-shot video question-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even surpassing state-of-the-art methods that involve video instruction tuning.","2) While mainstream video-based MLLMs typically initialize with an image-based MLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study indicates that utilizing the widely adopted VideoInstruct-100K for video instruction tuning doesn't actually lead to better performance compared to not training at all.","3) The commonly used evaluation metrics in existing works are significantly influenced by changes in the GPT API version over time.","If ignored, this could affect the fairness and uniformity of comparisons between different methods and impact the analysis and judgment of researchers in the field.","The advancement of MLLMs is currently thriving, drawing numerous researchers into the field.","We aim for this work to serve as a plug-and-play, simple yet effective baseline, encouraging the direct evaluation of existing MLLMs in video domain while also standardizing the field of video conversational models to a certain extent.","Also, we encourage researchers to reconsider: Have current video MLLM methods truly acquired knowledge beyond image MLLM?","Code is available at https://github.com/whwu95/FreeVA"],"url":"http://arxiv.org/abs/2405.07798v1","category":"cs.CV"}
{"created":"2024-05-13 14:24:56","title":"Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition","abstract":"This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, $\\mathsf{DirMixE}$, which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Theoretically, we show that our proposed objective benefits from enhanced generalization by virtue of the variance-based regularization. Comprehensive experiments across multiple benchmarks confirm the effectiveness of $\\mathsf{DirMixE}$. The code is available at \\url{https://github.com/scongl/DirMixE}.","sentences":["This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced.","We argue that the variation in these distributions can be broken down hierarchically into global and local levels.","The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor.","Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations.","However, the local variations are left unconsidered.","To address this issue, we propose a new MoE strategy, $\\mathsf{DirMixE}$, which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations.","Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations.","This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes.","Theoretically, we show that our proposed objective benefits from enhanced generalization by virtue of the variance-based regularization.","Comprehensive experiments across multiple benchmarks confirm the effectiveness of $\\mathsf{DirMixE}$.","The code is available at \\url{https://github.com/scongl/DirMixE}."],"url":"http://arxiv.org/abs/2405.07780v1","category":"cs.LG"}
{"created":"2024-05-13 14:23:37","title":"A Comprehensive Analysis of Static Word Embeddings for Turkish","abstract":"Word embeddings are fixed-length, dense and distributed word representations that are used in natural language processing (NLP) applications. There are basically two types of word embedding models which are non-contextual (static) models and contextual models. The former method generates a single embedding for a word regardless of its context, while the latter method produces distinct embeddings for a word based on the specific contexts in which it appears. There are plenty of works that compare contextual and non-contextual embedding models within their respective groups in different languages. However, the number of studies that compare the models in these two groups with each other is very few and there is no such study in Turkish. This process necessitates converting contextual embeddings into static embeddings. In this paper, we compare and evaluate the performance of several contextual and non-contextual models in both intrinsic and extrinsic evaluation settings for Turkish. We make a fine-grained comparison by analyzing the syntactic and semantic capabilities of the models separately. The results of the analyses provide insights about the suitability of different embedding models in different types of NLP tasks. We also build a Turkish word embedding repository comprising the embedding models used in this work, which may serve as a valuable resource for researchers and practitioners in the field of Turkish NLP. We make the word embeddings, scripts, and evaluation datasets publicly available.","sentences":["Word embeddings are fixed-length, dense and distributed word representations that are used in natural language processing (NLP) applications.","There are basically two types of word embedding models which are non-contextual (static) models and contextual models.","The former method generates a single embedding for a word regardless of its context, while the latter method produces distinct embeddings for a word based on the specific contexts in which it appears.","There are plenty of works that compare contextual and non-contextual embedding models within their respective groups in different languages.","However, the number of studies that compare the models in these two groups with each other is very few and there is no such study in Turkish.","This process necessitates converting contextual embeddings into static embeddings.","In this paper, we compare and evaluate the performance of several contextual and non-contextual models in both intrinsic and extrinsic evaluation settings for Turkish.","We make a fine-grained comparison by analyzing the syntactic and semantic capabilities of the models separately.","The results of the analyses provide insights about the suitability of different embedding models in different types of NLP tasks.","We also build a Turkish word embedding repository comprising the embedding models used in this work, which may serve as a valuable resource for researchers and practitioners in the field of Turkish NLP.","We make the word embeddings, scripts, and evaluation datasets publicly available."],"url":"http://arxiv.org/abs/2405.07778v1","category":"cs.CL"}
{"created":"2024-05-13 14:17:52","title":"Human-Modeling in Sequential Decision-Making: An Analysis through the Lens of Human-Aware AI","abstract":"\"Human-aware\" has become a popular keyword used to describe a particular class of AI systems that are designed to work and interact with humans. While there exists a surprising level of consistency among the works that use the label human-aware, the term itself mostly remains poorly understood. In this work, we retroactively try to provide an account of what constitutes a human-aware AI system. We see that human-aware AI is a design-oriented paradigm, one that focuses on the need for modeling the humans it may interact with. Additionally, we see that this paradigm offers us intuitive dimensions to understand and categorize the kinds of interactions these systems might have with humans. We show the pedagogical value of these dimensions by using them as a tool to understand and review the current landscape of work related to human-AI systems that purport some form of human modeling. To fit the scope of a workshop paper, we specifically narrowed our review to papers that deal with sequential decision-making and were published in a major AI conference in the last three years. Our analysis helps identify the space of potential research problems that are currently being overlooked. We perform additional analysis on the degree to which these works make explicit reference to results from social science and whether they actually perform user-studies to validate their systems. We also provide an accounting of the various AI methods used by these works.","sentences":["\"Human-aware\" has become a popular keyword used to describe a particular class of AI systems that are designed to work and interact with humans.","While there exists a surprising level of consistency among the works that use the label human-aware, the term itself mostly remains poorly understood.","In this work, we retroactively try to provide an account of what constitutes a human-aware AI system.","We see that human-aware AI is a design-oriented paradigm, one that focuses on the need for modeling the humans it may interact with.","Additionally, we see that this paradigm offers us intuitive dimensions to understand and categorize the kinds of interactions these systems might have with humans.","We show the pedagogical value of these dimensions by using them as a tool to understand and review the current landscape of work related to human-AI systems that purport some form of human modeling.","To fit the scope of a workshop paper, we specifically narrowed our review to papers that deal with sequential decision-making and were published in a major AI conference in the last three years.","Our analysis helps identify the space of potential research problems that are currently being overlooked.","We perform additional analysis on the degree to which these works make explicit reference to results from social science and whether they actually perform user-studies to validate their systems.","We also provide an accounting of the various AI methods used by these works."],"url":"http://arxiv.org/abs/2405.07773v1","category":"cs.RO"}
{"created":"2024-05-13 14:11:09","title":"Synthetic Test Collections for Retrieval Evaluation","abstract":"Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation.","sentences":["Test collections play a vital role in evaluation of information retrieval (IR) systems.","Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive.","Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications.","In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored.","Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems.","In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries.","In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models.","Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation."],"url":"http://arxiv.org/abs/2405.07767v1","category":"cs.IR"}
{"created":"2024-05-13 14:09:06","title":"Challenges and Opportunities of NLP for HR Applications: A Discussion Paper","abstract":"Over the course of the recent decade, tremendous progress has been made in the areas of machine learning and natural language processing, which opened up vast areas of potential application use cases, including hiring and human resource management. We review the use cases for text analytics in the realm of human resources/personnel management, including actually realized as well as potential but not yet implemented ones, and we analyze the opportunities and risks of these.","sentences":["Over the course of the recent decade, tremendous progress has been made in the areas of machine learning and natural language processing, which opened up vast areas of potential application use cases, including hiring and human resource management.","We review the use cases for text analytics in the realm of human resources/personnel management, including actually realized as well as potential but not yet implemented ones, and we analyze the opportunities and risks of these."],"url":"http://arxiv.org/abs/2405.07766v1","category":"cs.CL"}
{"created":"2024-05-13 14:07:15","title":"LGDE: Local Graph-based Dictionary Expansion","abstract":"Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection. Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary. At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association. We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities. We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary.","sentences":["Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection.","Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary.","At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords.","The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association.","We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities.","We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary."],"url":"http://arxiv.org/abs/2405.07764v1","category":"cs.CL"}
{"created":"2024-05-13 14:03:49","title":"LLM4ED: Large Language Models for Automatic Equation Discovery","abstract":"Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.","sentences":["Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain.","Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms.","In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data.","Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations.","In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively.","The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance.","The second strategy is to instruct LLMs to perform evolutionary operators for global search.","Experiments are extensively conducted on both partial differential equations and ordinary differential equations.","Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems.","Further comparisons are made with state-of-the-art models, demonstrating good stability and usability.","Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery."],"url":"http://arxiv.org/abs/2405.07761v1","category":"cs.LG"}
{"created":"2024-05-13 13:59:59","title":"MADRL-Based Rate Adaptation for 360$\\degree$ Video Streaming with Multi-Viewpoint Prediction","abstract":"Over the last few years, 360$\\degree$ video traffic on the network has grown significantly. A key challenge of 360$\\degree$ video playback is ensuring a high quality of experience (QoE) with limited network bandwidth. Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption. However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well. This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory. The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction. After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360$\\degree$ video streaming is proposed for maximizing different QoE objectives under various network conditions. We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem. The experimental results show that our proposed method improves the defined QoE metric by up to 85.5\\% compared to existing ABR methods.","sentences":["Over the last few years, 360$\\degree$ video traffic on the network has grown significantly.","A key challenge of 360$\\degree$ video playback is ensuring a high quality of experience (QoE) with limited network bandwidth.","Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption.","However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well.","This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory.","The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction.","After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360$\\degree$ video streaming is proposed for maximizing different QoE objectives under various network conditions.","We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem.","The experimental results show that our proposed method improves the defined QoE metric by up to 85.5\\% compared to existing ABR methods."],"url":"http://arxiv.org/abs/2405.07759v1","category":"cs.MM"}
{"created":"2024-05-13 13:47:15","title":"DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in Dynamically-Configured Systems","abstract":"Anomaly detection in distributed systems such as High-Performance Computing (HPC) clusters is vital for early fault detection, performance optimisation, security monitoring, reliability in general but also operational insights. Deep Neural Networks have seen successful use in detecting long-term anomalies in multidimensional data, originating for instance from industrial or medical systems, or weather prediction. A downside of such methods is that they require a static input size, or lose data through cropping, sampling, or other dimensionality reduction methods, making deployment on systems with variability on monitored data channels, such as computing clusters difficult. To address these problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-Based Anomaly Detection) which combines DBSCAN and learning-based anomaly detection. DBSCAN clustering is used to find point anomalies in time-series data, mitigating the risk of missing outliers through loss of information when reducing input data to a fixed number of channels. A deep learning-based time-series anomaly detection method is then applied to the reduced data in order to identify long-term outliers. This hybrid approach reduces the chances of missing anomalies that might be made indistinguishable from normal data by the reduction process, and likewise enables the algorithm to be scalable and tolerate partial system failures while retaining its detection capabilities. Using a subset of the well-known SMD dataset family, a modified variant of the Eclipse dataset, as well as an in-house dataset with a large variability in active data channels, made publicly available with this work, we furthermore analyse computational intensity, memory footprint, and activation counts. DeepHYDRA is shown to reliably detect different types of anomalies in both large and complex datasets.","sentences":["Anomaly detection in distributed systems such as High-Performance Computing (HPC) clusters is vital for early fault detection, performance optimisation, security monitoring, reliability in general but also operational insights.","Deep Neural Networks have seen successful use in detecting long-term anomalies in multidimensional data, originating for instance from industrial or medical systems, or weather prediction.","A downside of such methods is that they require a static input size, or lose data through cropping, sampling, or other dimensionality reduction methods, making deployment on systems with variability on monitored data channels, such as computing clusters difficult.","To address these problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-Based Anomaly Detection) which combines DBSCAN and learning-based anomaly detection.","DBSCAN clustering is used to find point anomalies in time-series data, mitigating the risk of missing outliers through loss of information when reducing input data to a fixed number of channels.","A deep learning-based time-series anomaly detection method is then applied to the reduced data in order to identify long-term outliers.","This hybrid approach reduces the chances of missing anomalies that might be made indistinguishable from normal data by the reduction process, and likewise enables the algorithm to be scalable and tolerate partial system failures while retaining its detection capabilities.","Using a subset of the well-known SMD dataset family, a modified variant of the Eclipse dataset, as well as an in-house dataset with a large variability in active data channels, made publicly available with this work, we furthermore analyse computational intensity, memory footprint, and activation counts.","DeepHYDRA is shown to reliably detect different types of anomalies in both large and complex datasets."],"url":"http://arxiv.org/abs/2405.07749v1","category":"cs.LG"}
{"created":"2024-05-13 13:41:59","title":"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language","abstract":"Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted.","sentences":["Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility.","The primary methods for representing these languages are monolingual and multilingual pretraining.","Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages.","This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages.","We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension.","The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks.","However, extending the vocabulary shows no substantial benefits.","Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted."],"url":"http://arxiv.org/abs/2405.07745v1","category":"cs.CL"}
{"created":"2024-05-13 13:36:53","title":"Search for the radiative transition $\u03c7_{c1}(3872)\\to\u03b3 \u03c8_2(3823)$","abstract":"Using 9.0 $\\rm fb^{-1}$ of $e^+e^-$ collision data collected at center-of-mass energies from 4.178 to 4.278 GeV with the BESIII detector at the BEPCII collider, we perform the first search for the radiative transition $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$. No $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$ signal is observed. The upper limit on the ratio of branching fractions $\\mathcal{B}(\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823), \\psi_2(3823)\\to\\gamma\\chi_{c1})/\\mathcal{B}(\\chi_{c1}(3872)\\to\\pi^+\\pi^- J/\\psi)$ is set as 0.075 at the 90\\% confidence level. Our result contradicts theoretical predictions under the assumption that the $\\chi_{c1}(3872)$ is the pure charmonium state $\\chi_{c1}(2P)$.","sentences":["Using 9.0 $\\rm fb^{-1}$ of $e^+e^-$ collision data collected at center-of-mass energies from 4.178 to 4.278 GeV with the BESIII detector at the BEPCII collider, we perform the first search for the radiative transition $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$.","No $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$ signal is observed.","The upper limit on the ratio of branching fractions $\\mathcal{B}(\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823), \\psi_2(3823)\\to\\gamma\\chi_{c1})/\\mathcal{B}(\\chi_{c1}(3872)\\to\\pi^+\\pi^- J/\\psi)$ is set as 0.075 at the 90\\% confidence level.","Our result contradicts theoretical predictions under the assumption that the $\\chi_{c1}(3872)$ is the pure charmonium state $\\chi_{c1}(2P)$."],"url":"http://arxiv.org/abs/2405.07741v1","category":"hep-ex"}
{"created":"2024-05-13 13:32:02","title":"Federated Hierarchical Tensor Networks: a Collaborative Learning Quantum AI-Driven Framework for Healthcare","abstract":"Healthcare industries frequently handle sensitive and proprietary data, and due to strict privacy regulations, they are often reluctant to share data directly. In today's context, Federated Learning (FL) stands out as a crucial remedy, facilitating the rapid advancement of distributed machine learning while effectively managing critical concerns regarding data privacy and governance. The fusion of federated learning and quantum computing represents a groundbreaking interdisciplinary approach with immense potential to revolutionize various industries, from healthcare to finance. In this work, we proposed a federated learning framework based on quantum tensor networks, which leverages the principles of many-body quantum physics. Currently, there are no known classical tensor networks implemented in federated settings. Furthermore, we investigated the effectiveness and feasibility of the proposed framework by conducting a differential privacy analysis to ensure the security of sensitive data across healthcare institutions. Experiments on popular medical image datasets show that the federated quantum tensor network model achieved a mean receiver-operator characteristic area under the curve (ROC-AUC) between 0.91-0.98. Experimental results demonstrate that the quantum federated global model, consisting of highly entangled tensor network structures, showed better generalization and robustness and achieved higher testing accuracy, surpassing the performance of locally trained clients under unbalanced data distributions among healthcare institutions.","sentences":["Healthcare industries frequently handle sensitive and proprietary data, and due to strict privacy regulations, they are often reluctant to share data directly.","In today's context, Federated Learning (FL) stands out as a crucial remedy, facilitating the rapid advancement of distributed machine learning while effectively managing critical concerns regarding data privacy and governance.","The fusion of federated learning and quantum computing represents a groundbreaking interdisciplinary approach with immense potential to revolutionize various industries, from healthcare to finance.","In this work, we proposed a federated learning framework based on quantum tensor networks, which leverages the principles of many-body quantum physics.","Currently, there are no known classical tensor networks implemented in federated settings.","Furthermore, we investigated the effectiveness and feasibility of the proposed framework by conducting a differential privacy analysis to ensure the security of sensitive data across healthcare institutions.","Experiments on popular medical image datasets show that the federated quantum tensor network model achieved a mean receiver-operator characteristic area under the curve (ROC-AUC) between 0.91-0.98.","Experimental results demonstrate that the quantum federated global model, consisting of highly entangled tensor network structures, showed better generalization and robustness and achieved higher testing accuracy, surpassing the performance of locally trained clients under unbalanced data distributions among healthcare institutions."],"url":"http://arxiv.org/abs/2405.07735v1","category":"quant-ph"}
{"created":"2024-05-13 13:08:02","title":"A Unified Sequence Parallelism Approach for Long Context Generative AI","abstract":"Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/expert/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 86\\% MFU on two 8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model. Our code is publicly available on \\url{https://github.com/feifeibear/long-context-attention}.","sentences":["Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models.","This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology.","This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/expert/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP.","We achieved 86\\% MFU on two 8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model.","Our code is publicly available on \\url{https://github.com/feifeibear/long-context-attention}."],"url":"http://arxiv.org/abs/2405.07719v2","category":"cs.LG"}
{"created":"2024-05-13 12:39:08","title":"FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction of Cancer Survival","abstract":"Integrating the different data modalities of cancer patients can significantly improve the predictive performance of patient survival. However, most existing methods ignore the simultaneous utilization of rich semantic features at different scales in pathology images. When collecting multimodal data and extracting features, there is a likelihood of encountering intra-modality missing data, introducing noise into the multimodal data. To address these challenges, this paper proposes a new end-to-end framework, FORESEE, for robustly predicting patient survival by mining multimodal information. Specifically, the cross-fusion transformer effectively utilizes features at the cellular level, tissue level, and tumor heterogeneity level to correlate prognosis through a cross-scale feature cross-fusion method. This enhances the ability of pathological image feature representation. Secondly, the hybrid attention encoder (HAE) uses the denoising contextual attention module to obtain the contextual relationship features and local detail features of the molecular data. HAE's channel attention module obtains global features of molecular data. Furthermore, to address the issue of missing information within modalities, we propose an asymmetrically masked triplet masked autoencoder to reconstruct lost information within modalities. Extensive experiments demonstrate the superiority of our method over state-of-the-art methods on four benchmark datasets in both complete and missing settings.","sentences":["Integrating the different data modalities of cancer patients can significantly improve the predictive performance of patient survival.","However, most existing methods ignore the simultaneous utilization of rich semantic features at different scales in pathology images.","When collecting multimodal data and extracting features, there is a likelihood of encountering intra-modality missing data, introducing noise into the multimodal data.","To address these challenges, this paper proposes a new end-to-end framework, FORESEE, for robustly predicting patient survival by mining multimodal information.","Specifically, the cross-fusion transformer effectively utilizes features at the cellular level, tissue level, and tumor heterogeneity level to correlate prognosis through a cross-scale feature cross-fusion method.","This enhances the ability of pathological image feature representation.","Secondly, the hybrid attention encoder (HAE) uses the denoising contextual attention module to obtain the contextual relationship features and local detail features of the molecular data.","HAE's channel attention module obtains global features of molecular data.","Furthermore, to address the issue of missing information within modalities, we propose an asymmetrically masked triplet masked autoencoder to reconstruct lost information within modalities.","Extensive experiments demonstrate the superiority of our method over state-of-the-art methods on four benchmark datasets in both complete and missing settings."],"url":"http://arxiv.org/abs/2405.07702v1","category":"cs.CV"}
{"created":"2024-05-13 12:20:05","title":"Comprehensive Analysis of Access Control Models in Edge Computing: Challenges, Solutions, and Future Directions","abstract":"Many contemporary applications, including smart homes and autonomous vehicles, rely on the Internet of Things technology. While cloud computing provides a multitude of valuable services for these applications, it generally imposes constraints on latency-sensitive applications due to the significant propagation delays. As a complementary technique to cloud computing, edge computing situates computing resources closer to the data sources, which reduces the latency and simultaneously alleviates the bandwidth pressure for the cloud and enhances data security. While edge computing offers significant advantages, it also presents significant challenges in access control -- a critical component for safeguarding data. For instance, it is crucial to implement access control mechanisms that are both effective and efficient on resource-constrained devices, ensuring high security without compromising the inherent low latency benefits of edge computing. These challenges drive the development of innovative access control solutions tailored to meet the unique requirements of edge computing environments. We classify related references from the perspectives of multiple data lifecycles (including data collection, storage, and usage), which thoroughly investigates the access control techniques and helps readers understand them systematically. Finally, we reflect on the classification and envisage future research directions.","sentences":["Many contemporary applications, including smart homes and autonomous vehicles, rely on the Internet of Things technology.","While cloud computing provides a multitude of valuable services for these applications, it generally imposes constraints on latency-sensitive applications due to the significant propagation delays.","As a complementary technique to cloud computing, edge computing situates computing resources closer to the data sources, which reduces the latency and simultaneously alleviates the bandwidth pressure for the cloud and enhances data security.","While edge computing offers significant advantages, it also presents significant challenges in access control -- a critical component for safeguarding data.","For instance, it is crucial to implement access control mechanisms that are both effective and efficient on resource-constrained devices, ensuring high security without compromising the inherent low latency benefits of edge computing.","These challenges drive the development of innovative access control solutions tailored to meet the unique requirements of edge computing environments.","We classify related references from the perspectives of multiple data lifecycles (including data collection, storage, and usage), which thoroughly investigates the access control techniques and helps readers understand them systematically.","Finally, we reflect on the classification and envisage future research directions."],"url":"http://arxiv.org/abs/2405.07685v1","category":"eess.SY"}
{"created":"2024-05-13 12:14:54","title":"FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation","abstract":"Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.","sentences":["Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems.","The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications.","In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments.","A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly.","With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation.","We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal.","By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times.","Audio samples and code are available at https://fastsag.github.io/."],"url":"http://arxiv.org/abs/2405.07682v1","category":"cs.SD"}
{"created":"2024-05-13 12:10:57","title":"Establishing a Unified Evaluation Framework for Human Motion Generation: A Comparative Analysis of Metrics","abstract":"The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework. This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings. We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons. Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data. We also conduct experimental analyses of three generative models using a publicly available dataset, offering insights into the interpretation of each metric in specific case scenarios. Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code.","sentences":["The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework.","This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings.","We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons.","Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data.","We also conduct experimental analyses of three generative models using a publicly available dataset, offering insights into the interpretation of each metric in specific case scenarios.","Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code."],"url":"http://arxiv.org/abs/2405.07680v1","category":"cs.CV"}
{"created":"2024-05-13 12:06:06","title":"On Minimum-Dispersion Control of Nonlinear Diffusion Processes","abstract":"This work collects some methodological insights for numerical solution of a \"minimum-dispersion\" control problem for nonlinear stochastic differential equations, a particular relaxation of the covariance steering task. The main ingredient of our approach is the theoretical foundation called $\\infty$-order variational analysis. This framework consists in establishing an exact representation of the increment ($\\infty$-order variation) of the objective functional using the duality, implied by the transformation of the nonlinear stochastic control problem to a linear deterministic control of the Fokker-Planck equation. The resulting formula for the cost increment analytically represents a \"law-feedback\" control for the diffusion process. This control mechanism enables us to learn time-dependent coefficients for a predefined Markovian control structure using Monte Carlo simulations with a modest population of samples. Numerical experiments prove the vitality of our approach.","sentences":["This work collects some methodological insights for numerical solution of a \"minimum-dispersion\" control problem for nonlinear stochastic differential equations, a particular relaxation of the covariance steering task.","The main ingredient of our approach is the theoretical foundation called $\\infty$-order variational analysis.","This framework consists in establishing an exact representation of the increment ($\\infty$-order variation) of the objective functional using the duality, implied by the transformation of the nonlinear stochastic control problem to a linear deterministic control of the Fokker-Planck equation.","The resulting formula for the cost increment analytically represents a \"law-feedback\" control for the diffusion process.","This control mechanism enables us to learn time-dependent coefficients for a predefined Markovian control structure using Monte Carlo simulations with a modest population of samples.","Numerical experiments prove the vitality of our approach."],"url":"http://arxiv.org/abs/2405.07676v1","category":"math.OC"}
{"created":"2024-05-13 11:54:03","title":"CrossCert: A Cross-Checking Detection Approach to Patch Robustness Certification for Deep Learning Models","abstract":"Patch robustness certification is an emerging kind of defense technique against adversarial patch attacks with provable guarantees. There are two research lines: certified recovery and certified detection. They aim to label malicious samples with provable guarantees correctly and issue warnings for malicious samples predicted to non-benign labels with provable guarantees, respectively. However, existing certified detection defenders suffer from protecting labels subject to manipulation, and existing certified recovery defenders cannot systematically warn samples about their labels. A certified defense that simultaneously offers robust labels and systematic warning protection against patch attacks is desirable. This paper proposes a novel certified defense technique called CrossCert. CrossCert formulates a novel approach by cross-checking two certified recovery defenders to provide unwavering certification and detection certification. Unwavering certification ensures that a certified sample, when subjected to a patched perturbation, will always be returned with a benign label without triggering any warnings with a provable guarantee. To our knowledge, CrossCert is the first certified detection technique to offer this guarantee. Our experiments show that, with a slightly lower performance than ViP and comparable performance with PatchCensor in terms of detection certification, CrossCert certifies a significant proportion of samples with the guarantee of unwavering certification.","sentences":["Patch robustness certification is an emerging kind of defense technique against adversarial patch attacks with provable guarantees.","There are two research lines: certified recovery and certified detection.","They aim to label malicious samples with provable guarantees correctly and issue warnings for malicious samples predicted to non-benign labels with provable guarantees, respectively.","However, existing certified detection defenders suffer from protecting labels subject to manipulation, and existing certified recovery defenders cannot systematically warn samples about their labels.","A certified defense that simultaneously offers robust labels and systematic warning protection against patch attacks is desirable.","This paper proposes a novel certified defense technique called CrossCert.","CrossCert formulates a novel approach by cross-checking two certified recovery defenders to provide unwavering certification and detection certification.","Unwavering certification ensures that a certified sample, when subjected to a patched perturbation, will always be returned with a benign label without triggering any warnings with a provable guarantee.","To our knowledge, CrossCert is the first certified detection technique to offer this guarantee.","Our experiments show that, with a slightly lower performance than ViP and comparable performance with PatchCensor in terms of detection certification, CrossCert certifies a significant proportion of samples with the guarantee of unwavering certification."],"url":"http://arxiv.org/abs/2405.07668v1","category":"cs.SE"}
{"created":"2024-05-13 11:45:22","title":"Geospatial Knowledge Graphs","abstract":"Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information. In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges. This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information. This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools. It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges. At the end, new research directions related to geospatial knowledge graphs are outlined.","sentences":["Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information.","In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges.","This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information.","This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools.","It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges.","At the end, new research directions related to geospatial knowledge graphs are outlined."],"url":"http://arxiv.org/abs/2405.07664v1","category":"cs.AI"}
{"created":"2024-05-13 11:43:38","title":"Squeezing Lemons with Hammers: An Evaluation of AutoML and Tabular Deep Learning for Data-Scarce Classification Applications","abstract":"Many industry verticals are confronted with small-sized tabular data. In this low-data regime, it is currently unclear whether the best performance can be expected from simple baselines, or more complex machine learning approaches that leverage meta-learning and ensembling. On 44 tabular classification datasets with sample sizes $\\leq$ 500, we find that L2-regularized logistic regression performs similar to state-of-the-art automated machine learning (AutoML) frameworks (AutoPrognosis, AutoGluon) and off-the-shelf deep neural networks (TabPFN, HyperFast) on the majority of the benchmark datasets. We therefore recommend to consider logistic regression as the first choice for data-scarce applications with tabular data and provide practitioners with best practices for further method selection.","sentences":["Many industry verticals are confronted with small-sized tabular data.","In this low-data regime, it is currently unclear whether the best performance can be expected from simple baselines, or more complex machine learning approaches that leverage meta-learning and ensembling.","On 44 tabular classification datasets with sample sizes $\\leq$ 500, we find that L2-regularized logistic regression performs similar to state-of-the-art automated machine learning (AutoML) frameworks (AutoPrognosis, AutoGluon) and off-the-shelf deep neural networks (TabPFN, HyperFast) on the majority of the benchmark datasets.","We therefore recommend to consider logistic regression as the first choice for data-scarce applications with tabular data and provide practitioners with best practices for further method selection."],"url":"http://arxiv.org/abs/2405.07662v1","category":"cs.LG"}
{"created":"2024-05-13 11:40:12","title":"Square-well model for superconducting pair-potential","abstract":"We study Andreev reflection in a one-dimensional square-well pair-potential. We discuss the history of the model. The current-phase relation is presented as a sum over Matsubara frequencies. How the current arises from bound and continuum levels is found by analytic continuation. We discuss two limiting cases of square-well model, the zero-length well and the infinite well. The model is quantitatively valid in some cases, but forms the basis for understanding a wide range of problems in inhomogeneous superconductivity and superfluidity.","sentences":["We study Andreev reflection in a one-dimensional square-well pair-potential.","We discuss the history of the model.","The current-phase relation is presented as a sum over Matsubara frequencies.","How the current arises from bound and continuum levels is found by analytic continuation.","We discuss two limiting cases of square-well model, the zero-length well and the infinite well.","The model is quantitatively valid in some cases, but forms the basis for understanding a wide range of problems in inhomogeneous superconductivity and superfluidity."],"url":"http://arxiv.org/abs/2405.07659v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 11:39:36","title":"Understanding Data Understanding: A Framework to Navigate the Intricacies of Data Analytics","abstract":"As organizations face the challenges of processing exponentially growing data volumes, their reliance on analytics to unlock value from this data has intensified. However, the intricacies of big data, such as its extensive feature sets, pose significant challenges. A crucial step in leveraging this data for insightful analysis is an in-depth understanding of both the data and its domain. Yet, existing literature presents a fragmented picture of what comprises an effective understanding of data and domain, varying significantly in depth and focus. To address this research gap, we conduct a systematic literature review, aiming to delineate the dimensions of data understanding. We identify five dimensions: Foundations, Collection & Selection, Contextualization & Integration, Exploration & Discovery, and Insights. These dimensions collectively form a comprehensive framework for data understanding, providing guidance for organizations seeking meaningful insights from complex datasets. This study synthesizes the current state of knowledge and lays the groundwork for further exploration.","sentences":["As organizations face the challenges of processing exponentially growing data volumes, their reliance on analytics to unlock value from this data has intensified.","However, the intricacies of big data, such as its extensive feature sets, pose significant challenges.","A crucial step in leveraging this data for insightful analysis is an in-depth understanding of both the data and its domain.","Yet, existing literature presents a fragmented picture of what comprises an effective understanding of data and domain, varying significantly in depth and focus.","To address this research gap, we conduct a systematic literature review, aiming to delineate the dimensions of data understanding.","We identify five dimensions: Foundations, Collection & Selection, Contextualization & Integration, Exploration & Discovery, and Insights.","These dimensions collectively form a comprehensive framework for data understanding, providing guidance for organizations seeking meaningful insights from complex datasets.","This study synthesizes the current state of knowledge and lays the groundwork for further exploration."],"url":"http://arxiv.org/abs/2405.07658v1","category":"cs.HC"}
{"created":"2024-05-13 11:37:50","title":"Beyond traditional Magnetic Resonance processing with Artificial Intelligence","abstract":"Smart signal processing approaches using Artificial Intelligence are gaining momentum in NMR applications. In this study, we demonstrate that AI offers new opportunities beyond tasks addressed by traditional techniques. We developed and trained several artificial neural networks in our new toolbox Magnetic Resonance with Artificial intelligence (MR-Ai) to solve three \"impossible\" problems: quadrature detection using only Echo (or Anti-Echo) modulation from the traditional Echo/Anti-Echo scheme; accessing uncertainty of signal intensity at each point in a spectrum processed by any given method; and defining a reference-free score for quantitative access of NMR spectrum quality. Our findings highlight the potential of AI techniques to revolutionize NMR processing and analysis.","sentences":["Smart signal processing approaches using Artificial Intelligence are gaining momentum in NMR applications.","In this study, we demonstrate that AI offers new opportunities beyond tasks addressed by traditional techniques.","We developed and trained several artificial neural networks in our new toolbox Magnetic Resonance with Artificial intelligence (MR-Ai) to solve three \"impossible\" problems: quadrature detection using only Echo (or Anti-Echo) modulation from the traditional Echo/Anti-Echo scheme; accessing uncertainty of signal intensity at each point in a spectrum processed by any given method; and defining a reference-free score for quantitative access of NMR spectrum quality.","Our findings highlight the potential of AI techniques to revolutionize NMR processing and analysis."],"url":"http://arxiv.org/abs/2405.07657v1","category":"physics.bio-ph"}
{"created":"2024-05-13 11:28:58","title":"Fast Training Data Acquisition for Object Detection and Segmentation using Black Screen Luminance Keying","abstract":"Deep Neural Networks (DNNs) require large amounts of annotated training data for a good performance. Often this data is generated using manual labeling (error-prone and time-consuming) or rendering (requiring geometry and material information). Both approaches make it difficult or uneconomic to apply them to many small-scale applications. A fast and straightforward approach of acquiring the necessary training data would allow the adoption of deep learning to even the smallest of applications. Chroma keying is the process of replacing a color (usually blue or green) with another background. Instead of chroma keying, we propose luminance keying for fast and straightforward training image acquisition. We deploy a black screen with high light absorption (99.99\\%) to record roughly 1-minute long videos of our target objects, circumventing typical problems of chroma keying, such as color bleeding or color overlap between background color and object color. Next we automatically mask our objects using simple brightness thresholding, saving the need for manual annotation. Finally, we automatically place the objects on random backgrounds and train a 2D object detector. We do extensive evaluation of the performance on the widely-used YCB-V object set and compare favourably to other conventional techniques such as rendering, without needing 3D meshes, materials or any other information of our target objects and in a fraction of the time needed for other approaches. Our work demonstrates highly accurate training data acquisition allowing to start training state-of-the-art networks within minutes.","sentences":["Deep Neural Networks (DNNs) require large amounts of annotated training data for a good performance.","Often this data is generated using manual labeling (error-prone and time-consuming) or rendering (requiring geometry and material information).","Both approaches make it difficult or uneconomic to apply them to many small-scale applications.","A fast and straightforward approach of acquiring the necessary training data would allow the adoption of deep learning to even the smallest of applications.","Chroma keying is the process of replacing a color (usually blue or green) with another background.","Instead of chroma keying, we propose luminance keying for fast and straightforward training image acquisition.","We deploy a black screen with high light absorption (99.99\\%) to record roughly 1-minute long videos of our target objects, circumventing typical problems of chroma keying, such as color bleeding or color overlap between background color and object color.","Next we automatically mask our objects using simple brightness thresholding, saving the need for manual annotation.","Finally, we automatically place the objects on random backgrounds and train a 2D object detector.","We do extensive evaluation of the performance on the widely-used YCB-V object set and compare favourably to other conventional techniques such as rendering, without needing 3D meshes, materials or any other information of our target objects and in a fraction of the time needed for other approaches.","Our work demonstrates highly accurate training data acquisition allowing to start training state-of-the-art networks within minutes."],"url":"http://arxiv.org/abs/2405.07653v1","category":"cs.CV"}
{"created":"2024-05-13 11:24:53","title":"G-VOILA: Gaze-Facilitated Information Querying in Daily Scenarios","abstract":"Modern information querying systems are progressively incorporating multimodal inputs like vision and audio. However, the integration of gaze -- a modality deeply linked to user intent and increasingly accessible via gaze-tracking wearables -- remains underexplored. This paper introduces a novel gaze-facilitated information querying paradigm, named G-VOILA, which synergizes users' gaze, visual field, and voice-based natural language queries to facilitate a more intuitive querying process. In a user-enactment study involving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed the ambiguity in users' query language and a gaze-voice coordination pattern in users' natural query behaviors with G-VOILA. Based on the quantitative and qualitative findings, we developed a design framework for the G-VOILA paradigm, which effectively integrates the gaze data with the in-situ querying context. Then we implemented a G-VOILA proof-of-concept using cutting-edge deep learning techniques. A follow-up user study (p = 16, scene = 2) demonstrates its effectiveness by achieving both higher objective score and subjective score, compared to a baseline without gaze data. We further conducted interviews and provided insights for future gaze-facilitated information querying systems.","sentences":["Modern information querying systems are progressively incorporating multimodal inputs like vision and audio.","However, the integration of gaze -- a modality deeply linked to user intent and increasingly accessible via gaze-tracking wearables -- remains underexplored.","This paper introduces a novel gaze-facilitated information querying paradigm, named G-VOILA, which synergizes users' gaze, visual field, and voice-based natural language queries to facilitate a more intuitive querying process.","In a user-enactment study involving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed the ambiguity in users' query language and a gaze-voice coordination pattern in users' natural query behaviors with G-VOILA.","Based on the quantitative and qualitative findings, we developed a design framework for the G-VOILA paradigm, which effectively integrates the gaze data with the in-situ querying context.","Then we implemented a G-VOILA proof-of-concept using cutting-edge deep learning techniques.","A follow-up user study (p = 16, scene = 2) demonstrates its effectiveness by achieving both higher objective score and subjective score, compared to a baseline without gaze data.","We further conducted interviews and provided insights for future gaze-facilitated information querying systems."],"url":"http://arxiv.org/abs/2405.07652v1","category":"cs.HC"}
{"created":"2024-05-13 11:00:27","title":"Evaluating Speech Enhancement Systems Through Listening Effort","abstract":"Understanding degraded speech is demanding, requiring increased listening effort (LE). Evaluating processed and unprocessed speech with respect to LE can objectively indicate if speech enhancement systems benefit listeners. However, existing methods for measuring LE are complex and not widely applicable. In this study, we propose a simple method to evaluate speech intelligibility and LE simultaneously without additional strain on subjects or operators. We assess this method using results from two independent studies in Norway and Denmark, testing 76 (50+26) subjects across 9 (6+3) processing conditions. Despite differences in evaluation setups, subject recruitment, and processing systems, trends are strikingly similar, demonstrating the proposed method's robustness and ease of implementation into existing practices.","sentences":["Understanding degraded speech is demanding, requiring increased listening effort (LE).","Evaluating processed and unprocessed speech with respect to LE can objectively indicate if speech enhancement systems benefit listeners.","However, existing methods for measuring LE are complex and not widely applicable.","In this study, we propose a simple method to evaluate speech intelligibility and LE simultaneously without additional strain on subjects or operators.","We assess this method using results from two independent studies in Norway and Denmark, testing 76 (50+26) subjects across 9 (6","+3) processing conditions.","Despite differences in evaluation setups, subject recruitment, and processing systems, trends are strikingly similar, demonstrating the proposed method's robustness and ease of implementation into existing practices."],"url":"http://arxiv.org/abs/2405.07641v1","category":"eess.AS"}
{"created":"2024-05-13 11:00:25","title":"Hyperparameter Importance Analysis for Multi-Objective AutoML","abstract":"Hyperparameter optimization plays a pivotal role in enhancing the predictive performance and generalization capabilities of ML models. However, in many applications, we do not only care about predictive performance but also about objectives such as inference time, memory, or energy consumption. In such MOO scenarios, determining the importance of hyperparameters poses a significant challenge due to the complex interplay between the conflicting objectives. In this paper, we propose the first method for assessing the importance of hyperparameters in the context of multi-objective hyperparameter optimization. Our approach leverages surrogate-based hyperparameter importance (HPI) measures, i.e. fANOVA and ablation paths, to provide insights into the impact of hyperparameters on the optimization objectives. Specifically, we compute the a-priori scalarization of the objectives and determine the importance of the hyperparameters for different objective tradeoffs. Through extensive empirical evaluations on diverse benchmark datasets with three different objectives paired with accuracy, namely time, demographic parity, and energy consumption, we demonstrate the effectiveness and robustness of our proposed method. Our findings not only offer valuable guidance for hyperparameter tuning in MOO tasks but also contribute to advancing the understanding of HPI in complex optimization scenarios.","sentences":["Hyperparameter optimization plays a pivotal role in enhancing the predictive performance and generalization capabilities of ML models.","However, in many applications, we do not only care about predictive performance but also about objectives such as inference time, memory, or energy consumption.","In such MOO scenarios, determining the importance of hyperparameters poses a significant challenge due to the complex interplay between the conflicting objectives.","In this paper, we propose the first method for assessing the importance of hyperparameters in the context of multi-objective hyperparameter optimization.","Our approach leverages surrogate-based hyperparameter importance (HPI) measures, i.e. fANOVA and ablation paths, to provide insights into the impact of hyperparameters on the optimization objectives.","Specifically, we compute the a-priori scalarization of the objectives and determine the importance of the hyperparameters for different objective tradeoffs.","Through extensive empirical evaluations on diverse benchmark datasets with three different objectives paired with accuracy, namely time, demographic parity, and energy consumption, we demonstrate the effectiveness and robustness of our proposed method.","Our findings not only offer valuable guidance for hyperparameter tuning in MOO tasks but also contribute to advancing the understanding of HPI in complex optimization scenarios."],"url":"http://arxiv.org/abs/2405.07640v1","category":"cs.LG"}
{"created":"2024-05-13 10:53:41","title":"DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS","abstract":"It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures. It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users. Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone. By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context. The representations are used to improve the DDoS detection performance. We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The tests have proven that DoLLM possesses strong detection capabilities. Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces.","sentences":["It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows.","This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area.","Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures.","It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users.","Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses.","We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone.","By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context.","The representations are used to improve the DDoS detection performance.","We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP.","The tests have proven that DoLLM possesses strong detection capabilities.","Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces."],"url":"http://arxiv.org/abs/2405.07638v1","category":"cs.NI"}
{"created":"2024-05-13 10:37:50","title":"AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models","abstract":"Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type. Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications. In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM. To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings. Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection. Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters.","sentences":["Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps.","With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type.","Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications.","In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM.","To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings.","Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection.","Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters."],"url":"http://arxiv.org/abs/2405.07626v1","category":"cs.LG"}
{"created":"2024-05-13 10:27:11","title":"Towards Adaptive IMFs -- Generalization of utility functions in Multi-Agent Frameworks","abstract":"Intent Management Function (IMF) is an integral part of future-generation networks. In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents. Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF. However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations. The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition. This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training. Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently. Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability.","sentences":["Intent Management Function (IMF) is an integral part of future-generation networks.","In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents.","Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF.","However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations.","The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition.","This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training.","Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently.","Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability."],"url":"http://arxiv.org/abs/2405.07621v2","category":"cs.LG"}
{"created":"2024-05-13 10:20:31","title":"NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition","abstract":"Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their theoretically achievable upper bound. We release NoiseBench to the research community.","sentences":["Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries.","Such label noise poses challenges for supervised learning and may significantly deteriorate model quality.","To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels.","These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted.","However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation.","To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors.","We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their theoretically achievable upper bound.","We release NoiseBench to the research community."],"url":"http://arxiv.org/abs/2405.07609v1","category":"cs.CL"}
{"created":"2024-05-13 10:07:36","title":"Reducing Risk for Assistive Reinforcement Learning Policies with Diffusion Models","abstract":"Care-giving and assistive robotics, driven by advancements in AI, offer promising solutions to meet the growing demand for care, particularly in the context of increasing numbers of individuals requiring assistance. This creates a pressing need for efficient and safe assistive devices, particularly in light of heightened demand due to war-related injuries. While cost has been a barrier to accessibility, technological progress is able to democratize these solutions. Safety remains a paramount concern, especially given the intricate interactions between assistive robots and humans. This study explores the application of reinforcement learning (RL) and imitation learning, in improving policy design for assistive robots. The proposed approach makes the risky policies safer without additional environmental interactions. Through experimentation using simulated environments, the enhancement of the conventional RL approaches in tasks related to assistive robotics is demonstrated.","sentences":["Care-giving and assistive robotics, driven by advancements in AI, offer promising solutions to meet the growing demand for care, particularly in the context of increasing numbers of individuals requiring assistance.","This creates a pressing need for efficient and safe assistive devices, particularly in light of heightened demand due to war-related injuries.","While cost has been a barrier to accessibility, technological progress is able to democratize these solutions.","Safety remains a paramount concern, especially given the intricate interactions between assistive robots and humans.","This study explores the application of reinforcement learning (RL) and imitation learning, in improving policy design for assistive robots.","The proposed approach makes the risky policies safer without additional environmental interactions.","Through experimentation using simulated environments, the enhancement of the conventional RL approaches in tasks related to assistive robotics is demonstrated."],"url":"http://arxiv.org/abs/2405.07603v1","category":"cs.RO"}
{"created":"2024-05-13 10:03:34","title":"On-device Online Learning and Semantic Management of TinyML Systems","abstract":"Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning. While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges. This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1) Embedded devices operate in dynamically changing conditions. Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects. However, static models may underperform in the real world due to evolving input data distributions. We propose online learning to enable training on constrained devices, adapting local models towards the latest field conditions. (2) Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices. We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning. This approach ensures optimal performance among distributed devices by knowledge sharing. (3) Moreover, TinyML's pivotal advantage is widespread adoption. Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures. Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up. We present semantic management for the joint management of models and devices at scale. We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection, confirming our approaches' effectiveness.","sentences":["Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning.","While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges.","This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1) Embedded devices operate in dynamically changing conditions.","Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects.","However, static models may underperform in the real world due to evolving input data distributions.","We propose online learning to enable training on constrained devices, adapting local models towards the latest field conditions.","(2) Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices.","We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning.","This approach ensures optimal performance among distributed devices by knowledge sharing.","(3) Moreover, TinyML's pivotal advantage is widespread adoption.","Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures.","Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up.","We present semantic management for the joint management of models and devices at scale.","We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection, confirming our approaches' effectiveness."],"url":"http://arxiv.org/abs/2405.07601v1","category":"cs.LG"}
{"created":"2024-05-13 09:56:57","title":"Environmental Matching Attack Against Unmanned Aerial Vehicles Object Detection","abstract":"Object detection techniques for Unmanned Aerial Vehicles (UAVs) rely on Deep Neural Networks (DNNs), which are vulnerable to adversarial attacks. Nonetheless, adversarial patches generated by existing algorithms in the UAV domain pay very little attention to the naturalness of adversarial patches. Moreover, imposing constraints directly on adversarial patches makes it difficult to generate patches that appear natural to the human eye while ensuring a high attack success rate. We notice that patches are natural looking when their overall color is consistent with the environment. Therefore, we propose a new method named Environmental Matching Attack(EMA) to address the issue of optimizing the adversarial patch under the constraints of color. To the best of our knowledge, this paper is the first to consider natural patches in the domain of UAVs. The EMA method exploits strong prior knowledge of a pretrained stable diffusion to guide the optimization direction of the adversarial patch, where the text guidance can restrict the color of the patch. To better match the environment, the contrast and brightness of the patch are appropriately adjusted. Instead of optimizing the adversarial patch itself, we optimize an adversarial perturbation patch which initializes to zero so that the model can better trade off attacking performance and naturalness. Experiments conducted on the DroneVehicle and Carpk datasets have shown that our work can reach nearly the same attack performance in the digital attack(no greater than 2 in mAP$\\%$), surpass the baseline method in the physical specific scenarios, and exhibit a significant advantage in terms of naturalness in visualization and color difference with the environment.","sentences":["Object detection techniques for Unmanned Aerial Vehicles (UAVs) rely on Deep Neural Networks (DNNs), which are vulnerable to adversarial attacks.","Nonetheless, adversarial patches generated by existing algorithms in the UAV domain pay very little attention to the naturalness of adversarial patches.","Moreover, imposing constraints directly on adversarial patches makes it difficult to generate patches that appear natural to the human eye while ensuring a high attack success rate.","We notice that patches are natural looking when their overall color is consistent with the environment.","Therefore, we propose a new method named Environmental Matching Attack(EMA) to address the issue of optimizing the adversarial patch under the constraints of color.","To the best of our knowledge, this paper is the first to consider natural patches in the domain of UAVs.","The EMA method exploits strong prior knowledge of a pretrained stable diffusion to guide the optimization direction of the adversarial patch, where the text guidance can restrict the color of the patch.","To better match the environment, the contrast and brightness of the patch are appropriately adjusted.","Instead of optimizing the adversarial patch itself, we optimize an adversarial perturbation patch which initializes to zero so that the model can better trade off attacking performance and naturalness.","Experiments conducted on the DroneVehicle and Carpk datasets have shown that our work can reach nearly the same attack performance in the digital attack(no greater than 2 in mAP$\\%$), surpass the baseline method in the physical specific scenarios, and exhibit a significant advantage in terms of naturalness in visualization and color difference with the environment."],"url":"http://arxiv.org/abs/2405.07595v1","category":"cs.CV"}
{"created":"2024-05-13 09:53:25","title":"Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data","abstract":"With the digitalization of health care systems, artificial intelligence becomes more present in medicine. Especially machine learning shows great potential for complex tasks such as time series classification, usually at the cost of transparency and comprehensibility. This leads to a lack of trust by humans and thus hinders its active usage. Explainable artificial intelligence tries to close this gap by providing insight into the decision-making process, the actual usefulness of its different methods is however unclear. This paper proposes a user study based evaluation of the explanation method Grad-CAM with application to a neural network for the classification of breaths in time series neonatal ventilation data. We present the perceived usefulness of the explainability method by different stakeholders, exposing the difficulty to achieve actual transparency and the wish for more in-depth explanations by many of the participants.","sentences":["With the digitalization of health care systems, artificial intelligence becomes more present in medicine.","Especially machine learning shows great potential for complex tasks such as time series classification, usually at the cost of transparency and comprehensibility.","This leads to a lack of trust by humans and thus hinders its active usage.","Explainable artificial intelligence tries to close this gap by providing insight into the decision-making process, the actual usefulness of its different methods is however unclear.","This paper proposes a user study based evaluation of the explanation method Grad-CAM with application to a neural network for the classification of breaths in time series neonatal ventilation data.","We present the perceived usefulness of the explainability method by different stakeholders, exposing the difficulty to achieve actual transparency and the wish for more in-depth explanations by many of the participants."],"url":"http://arxiv.org/abs/2405.07590v1","category":"cs.AI"}
{"created":"2024-05-13 09:38:49","title":"FRRffusion: Unveiling Authenticity with Diffusion-Based Face Retouching Reversal","abstract":"Unveiling the real appearance of retouched faces to prevent malicious users from deceptive advertising and economic fraud has been an increasing concern in the era of digital economics. This article makes the first attempt to investigate the face retouching reversal (FRR) problem. We first collect an FRR dataset, named deepFRR, which contains 50,000 StyleGAN-generated high-resolution (1024*1024) facial images and their corresponding retouched ones by a commercial online API. To our best knowledge, deepFRR is the first FRR dataset tailored for training the deep FRR models. Then, we propose a novel diffusion-based FRR approach (FRRffusion) for the FRR task. Our FRRffusion consists of a coarse-to-fine two-stage network: A diffusion-based Facial Morpho-Architectonic Restorer (FMAR) is constructed to generate the basic contours of low-resolution faces in the first stage, while a Transformer-based Hyperrealistic Facial Detail Generator (HFDG) is designed to create high-resolution facial details in the second stage. Tested on deepFRR, our FRRffusion surpasses the GP-UNIT and Stable Diffusion methods by a large margin in four widespread quantitative metrics. Especially, the de-retouched images by our FRRffusion are visually much closer to the raw face images than both the retouched face images and those restored by the GP-UNIT and Stable Diffusion methods in terms of qualitative evaluation with 85 subjects. These results sufficiently validate the efficacy of our work, bridging the recently-standing gap between the FRR and generic image restoration tasks. The dataset and code are available at https://github.com/GZHU-DVL/FRRffusion.","sentences":["Unveiling the real appearance of retouched faces to prevent malicious users from deceptive advertising and economic fraud has been an increasing concern in the era of digital economics.","This article makes the first attempt to investigate the face retouching reversal (FRR) problem.","We first collect an FRR dataset, named deepFRR, which contains 50,000 StyleGAN-generated high-resolution (1024*1024) facial images and their corresponding retouched ones by a commercial online API.","To our best knowledge, deepFRR is the first FRR dataset tailored for training the deep FRR models.","Then, we propose a novel diffusion-based FRR approach (FRRffusion) for the FRR task.","Our FRRffusion consists of a coarse-to-fine two-stage network:","A diffusion-based Facial Morpho-Architectonic Restorer (FMAR) is constructed to generate the basic contours of low-resolution faces in the first stage, while a Transformer-based Hyperrealistic Facial Detail Generator (HFDG) is designed to create high-resolution facial details in the second stage.","Tested on deepFRR, our FRRffusion surpasses the GP-UNIT and Stable Diffusion methods by a large margin in four widespread quantitative metrics.","Especially, the de-retouched images by our FRRffusion are visually much closer to the raw face images than both the retouched face images and those restored by the GP-UNIT and Stable Diffusion methods in terms of qualitative evaluation with 85 subjects.","These results sufficiently validate the efficacy of our work, bridging the recently-standing gap between the FRR and generic image restoration tasks.","The dataset and code are available at https://github.com/GZHU-DVL/FRRffusion."],"url":"http://arxiv.org/abs/2405.07582v1","category":"cs.CV"}
{"created":"2024-05-13 09:36:17","title":"DynLLM: When Large Language Models Meet Dynamic Graph Recommendation","abstract":"Last year has witnessed the considerable interest of Large Language Models (LLMs) for their potential applications in recommender systems, which may mitigate the persistent issue of data sparsity. Though large efforts have been made for user-item graph augmentation with better graph-based recommendation performance, they may fail to deal with the dynamic graph recommendation task, which involves both structural and temporal graph dynamics with inherent complexity in processing time-evolving data. To bridge this gap, in this paper, we propose a novel framework, called DynLLM, to deal with the dynamic graph recommendation task with LLMs. Specifically, DynLLM harnesses the power of LLMs to generate multi-faceted user profiles based on the rich textual features of historical purchase records, including crowd segments, personal interests, preferred categories, and favored brands, which in turn supplement and enrich the underlying relationships between users and items. Along this line, to fuse the multi-faceted profiles with temporal graph embedding, we engage LLMs to derive corresponding profile embeddings, and further employ a distilled attention mechanism to refine the LLM-generated profile embeddings for alleviating noisy signals, while also assessing and adjusting the relevance of each distilled facet embedding for seamless integration with temporal graph embedding from continuous time dynamic graphs (CTDGs). Extensive experiments on two real e-commerce datasets have validated the superior improvements of DynLLM over a wide range of state-of-the-art baseline methods.","sentences":["Last year has witnessed the considerable interest of Large Language Models (LLMs) for their potential applications in recommender systems, which may mitigate the persistent issue of data sparsity.","Though large efforts have been made for user-item graph augmentation with better graph-based recommendation performance, they may fail to deal with the dynamic graph recommendation task, which involves both structural and temporal graph dynamics with inherent complexity in processing time-evolving data.","To bridge this gap, in this paper, we propose a novel framework, called DynLLM, to deal with the dynamic graph recommendation task with LLMs.","Specifically, DynLLM harnesses the power of LLMs to generate multi-faceted user profiles based on the rich textual features of historical purchase records, including crowd segments, personal interests, preferred categories, and favored brands, which in turn supplement and enrich the underlying relationships between users and items.","Along this line, to fuse the multi-faceted profiles with temporal graph embedding, we engage LLMs to derive corresponding profile embeddings, and further employ a distilled attention mechanism to refine the LLM-generated profile embeddings for alleviating noisy signals, while also assessing and adjusting the relevance of each distilled facet embedding for seamless integration with temporal graph embedding from continuous time dynamic graphs (CTDGs).","Extensive experiments on two real e-commerce datasets have validated the superior improvements of DynLLM over a wide range of state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2405.07580v1","category":"cs.IR"}
{"created":"2024-05-13 09:24:15","title":"Is it getting harder to make a hit? Evidence from 65 years of US music chart history","abstract":"Since the creation of the Billboard Hot 100 music chart in 1958, the chart has been a window into the music consumption of Americans. Which songs succeed on the chart is decided by consumption volumes, which can be affected by consumer music taste, and other factors such as advertisement budgets, airplay time, the specifics of ranking algorithms, and more. Since its introduction, the chart has documented music consumerism through eras of globalization, economic growth, and the emergence of new technologies for music listening. In recent years, musicians and other hitmakers have voiced their worry that the music world is changing: Many claim that it is getting harder to make a hit but until now, the claims have not been backed using chart data. Here we show that the dynamics of the Billboard Hot 100 chart have changed significantly since the chart's founding in 1958, and in particular in the past 15 years. Whereas most songs spend less time on the chart now than songs did in the past, we show that top-1 songs have tripled their chart lifetime since the 1960s, the highest-ranked songs maintain their positions for far longer than previously, and the lowest-ranked songs are replaced more frequently than ever. At the same time, who occupies the chart has also changed over the years: In recent years, fewer new artists make it into the chart and more positions are occupied by established hit makers. Finally, investigating how song chart trajectories have changed over time, we show that historical song trajectories cluster into clear trajectory archetypes characteristic of the time period they were part of. The results are interesting in the context of collective attention: Whereas recent studies have documented that other cultural products such as books, news, and movies fade in popularity quicker in recent years, music hits seem to last longer now than in the past.","sentences":["Since the creation of the Billboard Hot 100 music chart in 1958, the chart has been a window into the music consumption of Americans.","Which songs succeed on the chart is decided by consumption volumes, which can be affected by consumer music taste, and other factors such as advertisement budgets, airplay time, the specifics of ranking algorithms, and more.","Since its introduction, the chart has documented music consumerism through eras of globalization, economic growth, and the emergence of new technologies for music listening.","In recent years, musicians and other hitmakers have voiced their worry that the music world is changing: Many claim that it is getting harder to make a hit but until now, the claims have not been backed using chart data.","Here we show that the dynamics of the Billboard Hot 100 chart have changed significantly since the chart's founding in 1958, and in particular in the past 15 years.","Whereas most songs spend less time on the chart now than songs did in the past, we show that top-1 songs have tripled their chart lifetime since the 1960s, the highest-ranked songs maintain their positions for far longer than previously, and the lowest-ranked songs are replaced more frequently than ever.","At the same time, who occupies the chart has also changed over the years: In recent years, fewer new artists make it into the chart and more positions are occupied by established hit makers.","Finally, investigating how song chart trajectories have changed over time, we show that historical song trajectories cluster into clear trajectory archetypes characteristic of the time period they were part of.","The results are interesting in the context of collective attention: Whereas recent studies have documented that other cultural products such as books, news, and movies fade in popularity quicker in recent years, music hits seem to last longer now than in the past."],"url":"http://arxiv.org/abs/2405.07574v1","category":"physics.soc-ph"}
{"created":"2024-05-13 09:17:42","title":"Gaze-Based Intention Recognition for Human-Robot Collaboration","abstract":"This work aims to tackle the intent recognition problem in Human-Robot Collaborative assembly scenarios. Precisely, we consider an interactive assembly of a wooden stool where the robot fetches the pieces in the correct order and the human builds the parts following the instruction manual. The intent recognition is limited to the idle state estimation and it is needed to ensure a better synchronization between the two agents. We carried out a comparison between two distinct solutions involving wearable sensors and eye tracking integrated into the perception pipeline of a flexible planning architecture based on Hierarchical Task Networks. At runtime, the wearable sensing module exploits the raw measurements from four 9-axis Inertial Measurement Units positioned on the wrists and hands of the user as an input for a Long Short-Term Memory Network. On the other hand, the eye tracking relies on a Head Mounted Display and Unreal Engine.   We tested the effectiveness of the two approaches with 10 participants, each of whom explored both options in alternate order. We collected explicit metrics about the attractiveness and efficiency of the two techniques through User Experience Questionnaires as well as implicit criteria regarding the classification time and the overall assembly time.   The results of our work show that the two methods can reach comparable performances both in terms of effectiveness and user preference. Future development could aim at joining the two approaches two allow the recognition of more complex activities and to anticipate the user actions.","sentences":["This work aims to tackle the intent recognition problem in Human-Robot Collaborative assembly scenarios.","Precisely, we consider an interactive assembly of a wooden stool where the robot fetches the pieces in the correct order and the human builds the parts following the instruction manual.","The intent recognition is limited to the idle state estimation and it is needed to ensure a better synchronization between the two agents.","We carried out a comparison between two distinct solutions involving wearable sensors and eye tracking integrated into the perception pipeline of a flexible planning architecture based on Hierarchical Task Networks.","At runtime, the wearable sensing module exploits the raw measurements from four 9-axis Inertial Measurement Units positioned on the wrists and hands of the user as an input for a Long Short-Term Memory Network.","On the other hand, the eye tracking relies on a Head Mounted Display and Unreal Engine.   ","We tested the effectiveness of the two approaches with 10 participants, each of whom explored both options in alternate order.","We collected explicit metrics about the attractiveness and efficiency of the two techniques through User Experience Questionnaires as well as implicit criteria regarding the classification time and the overall assembly time.   ","The results of our work show that the two methods can reach comparable performances both in terms of effectiveness and user preference.","Future development could aim at joining the two approaches two allow the recognition of more complex activities and to anticipate the user actions."],"url":"http://arxiv.org/abs/2405.07570v1","category":"cs.RO"}
{"created":"2024-05-13 08:52:04","title":"GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation","abstract":"While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.","sentences":["While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs).","In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks.","In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network.","We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker.","We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting."],"url":"http://arxiv.org/abs/2405.07562v1","category":"cs.LG"}
{"created":"2024-05-13 08:51:14","title":"Crystal Structure-Based Multioutput Property Prediction of Lithium Manganese Nickel Oxide using EfficientNet-B0","abstract":"Here, we present an EfficientNet-B0-based model to directly predict multiple properties of lithium manganese nickel oxides (LMNO) using their crystal structure images. The model is supposed to predict the energy above the convex hull, bandgap energy, crystal systems, and crystal space groups of LMNOs. In the last layer of the model, a linear function is used to predict the bandgap energy and energy above the convex hull, while a SoftMax function is used to classify the crystal systems and crystal space groups. In the test set, the percentages of coefficient of determination (R2) scores are 97.73% and 96.50% for the bandgap energy and energy above the convex hull predictions, respectively, while the percentages of accuracy are 99.45% and 99.27% for the crystal system and crystal space group classifications, respectively. The class saliency maps explain that the model pays more attention to the shape of the crystal lattices and gradients around the lattice region occupied by the larger ions. This work provides new insight into using an intelligent model to directly relate the crystal structures of LMNO materials with their properties.","sentences":["Here, we present an EfficientNet-B0-based model to directly predict multiple properties of lithium manganese nickel oxides (LMNO) using their crystal structure images.","The model is supposed to predict the energy above the convex hull, bandgap energy, crystal systems, and crystal space groups of LMNOs.","In the last layer of the model, a linear function is used to predict the bandgap energy and energy above the convex hull, while a SoftMax function is used to classify the crystal systems and crystal space groups.","In the test set, the percentages of coefficient of determination (R2) scores are 97.73% and 96.50% for the bandgap energy and energy above the convex hull predictions, respectively, while the percentages of accuracy are 99.45% and 99.27% for the crystal system and crystal space group classifications, respectively.","The class saliency maps explain that the model pays more attention to the shape of the crystal lattices and gradients around the lattice region occupied by the larger ions.","This work provides new insight into using an intelligent model to directly relate the crystal structures of LMNO materials with their properties."],"url":"http://arxiv.org/abs/2405.07561v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 08:36:06","title":"Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to Uncertain Human Behaving","abstract":"Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation. By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making. However, state-of-the-art HL-CACC technology still has a great limitation on driving safety for the lack of considering the leading human driver's uncertain behaving. In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC). It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV). The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficient for real-time implementation. The proposed controller is evaluated on a PreScan&Simulink simulation platform. Real vehicle trajectory data is collected for the calibration of simulation. Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brake; iii) is confirmed with string stability. The computation time is approximately 3 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU. This indicates the proposed controller is ready for real-time implementation.","sentences":["Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation.","By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making.","However, state-of-the-art HL-CACC technology still has a great limitation on driving safety for the lack of considering the leading human driver's uncertain behaving.","In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC).","It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV).","The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficient for real-time implementation.","The proposed controller is evaluated on a PreScan&Simulink simulation platform.","Real vehicle trajectory data is collected for the calibration of simulation.","Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brake; iii) is confirmed with string stability.","The computation time is approximately 3 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU.","This indicates the proposed controller is ready for real-time implementation."],"url":"http://arxiv.org/abs/2405.07556v1","category":"cs.RO"}
{"created":"2024-05-13 08:32:19","title":"MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning","abstract":"The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via multi-perspective data augmenting methods and then synthesize code-nested solutions to them. The open LLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the resulting models, MuMath-Code ($\\mu$-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code. Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods -- achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy. We release the proposed dataset along with the associated code for public use.","sentences":["The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data.","However, a great method to integrate the above two research paths and combine their advantages remains to be explored.","In this work, we firstly include new math questions via multi-perspective data augmenting methods and then synthesize code-nested solutions to them.","The open LLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the resulting models, MuMath-Code ($\\mu$-Math-Code).","During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results.","Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation.","To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.","Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods -- achieving 90.7% on GSM8K and 55.1% on MATH.","Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy.","We release the proposed dataset along with the associated code for public use."],"url":"http://arxiv.org/abs/2405.07551v1","category":"cs.CL"}
{"created":"2024-05-13 08:31:58","title":"Wild Berry image dataset collected in Finnish forests and peatlands using drones","abstract":"Berry picking has long-standing traditions in Finland, yet it is challenging and can potentially be dangerous. The integration of drones equipped with advanced imaging techniques represents a transformative leap forward, optimising harvests and promising sustainable practices. We propose WildBe, the first image dataset of wild berries captured in peatlands and under the canopy of Finnish forests using drones. Unlike previous and related datasets, WildBe includes new varieties of berries, such as bilberries, cloudberries, lingonberries, and crowberries, captured under severe light variations and in cluttered environments. WildBe features 3,516 images, including a total of 18,468 annotated bounding boxes. We carry out a comprehensive analysis of WildBe using six popular object detectors, assessing their effectiveness in berry detection across different forest regions and camera types. We will release WildBe publicly.","sentences":["Berry picking has long-standing traditions in Finland, yet it is challenging and can potentially be dangerous.","The integration of drones equipped with advanced imaging techniques represents a transformative leap forward, optimising harvests and promising sustainable practices.","We propose WildBe, the first image dataset of wild berries captured in peatlands and under the canopy of Finnish forests using drones.","Unlike previous and related datasets, WildBe includes new varieties of berries, such as bilberries, cloudberries, lingonberries, and crowberries, captured under severe light variations and in cluttered environments.","WildBe features 3,516 images, including a total of 18,468 annotated bounding boxes.","We carry out a comprehensive analysis of WildBe using six popular object detectors, assessing their effectiveness in berry detection across different forest regions and camera types.","We will release WildBe publicly."],"url":"http://arxiv.org/abs/2405.07550v1","category":"cs.CV"}
{"created":"2024-05-13 08:26:24","title":"Automatic Odometry-Less OpenDRIVE Generation From Sparse Point Clouds","abstract":"High-resolution road representations are a key factor for the success of (highly) automated driving functions. These representations, for example, high-definition (HD) maps, contain accurate information on a multitude of factors, among others: road geometry, lane information, and traffic signs. Through the growing complexity and functionality of automated driving functions, also the requirements on testing and evaluation grow continuously. This leads to an increasing interest in virtual test drives for evaluation purposes. As roads play a crucial role in traffic flow, accurate real-world representations are needed, especially when deriving realistic driving behavior data. This paper proposes a novel approach to generate realistic road representations based solely on point cloud information, independent of the LiDAR sensor, mounting position, and without the need for odometry data, multi-sensor fusion, machine learning, or highly-accurate calibration. As the primary use case is simulation, we use the OpenDRIVE format for evaluation.","sentences":["High-resolution road representations are a key factor for the success of (highly) automated driving functions.","These representations, for example, high-definition (HD) maps, contain accurate information on a multitude of factors, among others: road geometry, lane information, and traffic signs.","Through the growing complexity and functionality of automated driving functions, also the requirements on testing and evaluation grow continuously.","This leads to an increasing interest in virtual test drives for evaluation purposes.","As roads play a crucial role in traffic flow, accurate real-world representations are needed, especially when deriving realistic driving behavior data.","This paper proposes a novel approach to generate realistic road representations based solely on point cloud information, independent of the LiDAR sensor, mounting position, and without the need for odometry data, multi-sensor fusion, machine learning, or highly-accurate calibration.","As the primary use case is simulation, we use the OpenDRIVE format for evaluation."],"url":"http://arxiv.org/abs/2405.07544v1","category":"cs.RO"}
{"created":"2024-05-13 08:22:44","title":"Random walk model that universally generates inverse square L\u00e9vy walk by eliminating search cost minimization constraint","abstract":"The L\\'evy walk, a type of random walk characterized by linear step lengths that follow a power-law distribution, is observed in the migratory behaviors of various organisms, ranging from bacteria to humans. Notably, L\\'evy walks with power exponents close to two are frequently observed, though their underlying causes remain elusive. This study introduces a simplified, abstract random walk model designed to produce inverse square L\\'evy walks, also known as Cauchy walks and explores the conditions that facilitate these phenomena. In our model, agents move toward a randomly selected destination in multi-dimensional space, and their movement strategy is parameterized by the extent to which they pursue the shortest path. When the search cost is proportional to the distance traveled, this parameter effectively reflects the emphasis on minimizing search costs. Our findings reveal that strict adherence to this cost minimization constraint results in a Brownian walk pattern. However, removing this constraint transitions the movement to an inverse square L\\'evy walk. Therefore, by modulating the prioritization of search costs, our model can seamlessly alternate between Brownian and Cauchy walk dynamics. This model has the potential to be utilized for exploring the parameter space of an optimization problem.","sentences":["The L\\'evy walk, a type of random walk characterized by linear step lengths that follow a power-law distribution, is observed in the migratory behaviors of various organisms, ranging from bacteria to humans.","Notably, L\\'evy walks with power exponents close to two are frequently observed, though their underlying causes remain elusive.","This study introduces a simplified, abstract random walk model designed to produce inverse square L\\'evy walks, also known as Cauchy walks and explores the conditions that facilitate these phenomena.","In our model, agents move toward a randomly selected destination in multi-dimensional space, and their movement strategy is parameterized by the extent to which they pursue the shortest path.","When the search cost is proportional to the distance traveled, this parameter effectively reflects the emphasis on minimizing search costs.","Our findings reveal that strict adherence to this cost minimization constraint results in a Brownian walk pattern.","However, removing this constraint transitions the movement to an inverse square L\\'evy walk.","Therefore, by modulating the prioritization of search costs, our model can seamlessly alternate between Brownian and Cauchy walk dynamics.","This model has the potential to be utilized for exploring the parameter space of an optimization problem."],"url":"http://arxiv.org/abs/2405.07541v2","category":"cs.MA"}
{"created":"2024-05-13 08:20:59","title":"Theory of cell aggregates as interacting, spinning, active polar particles","abstract":"We discuss a generic description of the dynamics of cell aggregates. We describe cells as polar rotating objects which mechanically interact with each other and with the surrounding medium. We use the framework of non-equilibrium thermodynamics to derive generic constitutive equations for the interaction forces, torques and polarity dynamics. We apply our framework to discuss spontaneous motion of cell doublets. We find a rich phase diagram of possible collective motion, including steady rotation arising from flow-polarity coupling or coupling of polarity with cell position.","sentences":["We discuss a generic description of the dynamics of cell aggregates.","We describe cells as polar rotating objects which mechanically interact with each other and with the surrounding medium.","We use the framework of non-equilibrium thermodynamics to derive generic constitutive equations for the interaction forces, torques and polarity dynamics.","We apply our framework to discuss spontaneous motion of cell doublets.","We find a rich phase diagram of possible collective motion, including steady rotation arising from flow-polarity coupling or coupling of polarity with cell position."],"url":"http://arxiv.org/abs/2405.07540v1","category":"cond-mat.soft"}
{"created":"2024-05-13 08:19:27","title":"Intrinsic Langevin dynamics of rigid inclusions on curved surfaces","abstract":"The stochastic dynamics of a rigid inclusion constrained to move on a curved surface has many applications in biological and soft matter physics, ranging from the diffusion of passive or active membrane proteins to the motion of phoretic particles on liquid-liquid interfaces. Here we construct intrinsic Langevin equations for an oriented rigid inclusion on a curved surface using Cartan's method of moving frames. We first derive the Hamiltonian equations of motion for the translational and rotational momenta in the body frame. Surprisingly, surface curvature couples the linear and angular momenta of the inclusion. We then add to the Hamiltonian equations linear friction, white noise and arbitrary configuration-dependent forces and torques to obtain intrinsic Langevin equations of motion in phase space. We provide the integrability conditions, made non-trivial by surface curvature, for the forces and torques to admit a potential, thus distinguishing between passive and active stochastic motion. We derive the corresponding Fokker-Planck equation in geometric form and obtain fluctuation-dissipation relations that ensure Gibbsian equilibrium. We extract the overdamped equations of motion by adiabatically eliminating the momenta from the Fokker-Planck equation, showing how a peculiar cancellation leads to the naively expected Smoluchowski limit. The overdamped equations can be used for accurate and efficient intrinsic Brownian dynamics simulations of passive, driven and active diffusion processes on curved surfaces. Our work generalises to the collective dynamics of many inclusions on curved surfaces.","sentences":["The stochastic dynamics of a rigid inclusion constrained to move on a curved surface has many applications in biological and soft matter physics, ranging from the diffusion of passive or active membrane proteins to the motion of phoretic particles on liquid-liquid interfaces.","Here we construct intrinsic Langevin equations for an oriented rigid inclusion on a curved surface using Cartan's method of moving frames.","We first derive the Hamiltonian equations of motion for the translational and rotational momenta in the body frame.","Surprisingly, surface curvature couples the linear and angular momenta of the inclusion.","We then add to the Hamiltonian equations linear friction, white noise and arbitrary configuration-dependent forces and torques to obtain intrinsic Langevin equations of motion in phase space.","We provide the integrability conditions, made non-trivial by surface curvature, for the forces and torques to admit a potential, thus distinguishing between passive and active stochastic motion.","We derive the corresponding Fokker-Planck equation in geometric form and obtain fluctuation-dissipation relations that ensure Gibbsian equilibrium.","We extract the overdamped equations of motion by adiabatically eliminating the momenta from the Fokker-Planck equation, showing how a peculiar cancellation leads to the naively expected Smoluchowski limit.","The overdamped equations can be used for accurate and efficient intrinsic Brownian dynamics simulations of passive, driven and active diffusion processes on curved surfaces.","Our work generalises to the collective dynamics of many inclusions on curved surfaces."],"url":"http://arxiv.org/abs/2405.07539v1","category":"cond-mat.soft"}
{"created":"2024-05-13 07:56:15","title":"Prompt-based Code Completion via Multi-Retrieval Augmented Generation","abstract":"Automated code completion, aiming at generating subsequent tokens from unfinished code, has been significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms state-of-the-art code completion technique by 8.6% on our collected open-source benchmark suite and 10.1% on the private-domain benchmark suite collected from a billion-user e-commerce company in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned model.","sentences":["Automated code completion, aiming at generating subsequent tokens from unfinished code, has been significantly benefited from recent progress in pre-trained Large Language Models (LLMs).","However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data.","Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion.","However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics.","To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code.","ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives.","Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code.","Experimental results demonstrate that ProCC outperforms state-of-the-art code completion technique by 8.6% on our collected open-source benchmark suite and 10.1% on the private-domain benchmark suite collected from a billion-user e-commerce company in terms of Exact Match.","ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned model."],"url":"http://arxiv.org/abs/2405.07530v1","category":"cs.SE"}
{"created":"2024-05-13 07:46:48","title":"Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models","abstract":"Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines.","sentences":["Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training.","This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy.","Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability.","To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively.","Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones.","Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance.","Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines."],"url":"http://arxiv.org/abs/2405.07527v1","category":"cs.LG"}
{"created":"2024-05-13 07:41:28","title":"Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation","abstract":"Colonoscopy is a common and practical method for detecting and treating polyps. Segmenting polyps from colonoscopy image is useful for diagnosis and surgery progress. Nevertheless, achieving excellent segmentation performance is still difficult because of polyp characteristics like shape, color, condition, and obvious non-distinction from the surrounding context. This work presents a new novel architecture namely Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation (ADSNet), which modifies misclassified details and recovers weak features having the ability to vanish and not be detected at the final stage. The architecture consists of a complementary trilateral decoder to produce an early global map. A continuous attention module modifies semantics of high-level features to analyze two separate semantics of the early global map. The suggested method is experienced on polyp benchmarks in learning ability and generalization ability, experimental results demonstrate the great correction and recovery ability leading to better segmentation performance compared to the other state of the art in the polyp image segmentation task. Especially, the proposed architecture could be experimented flexibly for other CNN-based encoders, Transformer-based encoders, and decoder backbones.","sentences":["Colonoscopy is a common and practical method for detecting and treating polyps.","Segmenting polyps from colonoscopy image is useful for diagnosis and surgery progress.","Nevertheless, achieving excellent segmentation performance is still difficult because of polyp characteristics like shape, color, condition, and obvious non-distinction from the surrounding context.","This work presents a new novel architecture namely Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation (ADSNet), which modifies misclassified details and recovers weak features having the ability to vanish and not be detected at the final stage.","The architecture consists of a complementary trilateral decoder to produce an early global map.","A continuous attention module modifies semantics of high-level features to analyze two separate semantics of the early global map.","The suggested method is experienced on polyp benchmarks in learning ability and generalization ability, experimental results demonstrate the great correction and recovery ability leading to better segmentation performance compared to the other state of the art in the polyp image segmentation task.","Especially, the proposed architecture could be experimented flexibly for other CNN-based encoders, Transformer-based encoders, and decoder backbones."],"url":"http://arxiv.org/abs/2405.07523v1","category":"cs.CV"}
{"created":"2024-05-13 07:32:45","title":"SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts","abstract":"Monolithic large language models (LLMs) like GPT-4 have paved the way for modern generative AI applications. Training, serving, and maintaining monolithic LLMs at scale, however, remains prohibitively expensive and challenging. The disproportionate increase in compute-to-memory ratio of modern AI accelerators have created a memory wall, necessitating new methods to deploy AI. Composition of Experts (CoE) is an alternative modular approach that lowers the cost and complexity of training and serving. However, this approach presents two key challenges when using conventional hardware: (1) without fused operations, smaller models have lower operational intensity, which makes high utilization more challenging to achieve; and (2) hosting a large number of models can be either prohibitively expensive or slow when dynamically switching between them.   In this paper, we describe how combining CoE, streaming dataflow, and a three-tier memory system scales the AI memory wall. We describe Samba-CoE, a CoE system with 150 experts and a trillion total parameters. We deploy Samba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) - a commercial dataflow accelerator architecture that has been co-designed for enterprise inference and training applications. The chip introduces a new three-tier memory system with on-chip distributed SRAM, on-package HBM, and off-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out over multiple sockets. We demonstrate speedups ranging from 2x to 13x on various benchmarks running on eight RDU sockets compared with an unfused baseline. We show that for CoE inference deployments, the 8-socket RDU Node reduces machine footprint by up to 19x, speeds up model switching time by 15x to 31x, and achieves an overall speedup of 3.7x over a DGX H100 and 6.6x over a DGX A100.","sentences":["Monolithic large language models (LLMs) like GPT-4 have paved the way for modern generative AI applications.","Training, serving, and maintaining monolithic LLMs at scale, however, remains prohibitively expensive and challenging.","The disproportionate increase in compute-to-memory ratio of modern AI accelerators have created a memory wall, necessitating new methods to deploy AI.","Composition of Experts (CoE) is an alternative modular approach that lowers the cost and complexity of training and serving.","However, this approach presents two key challenges when using conventional hardware: (1) without fused operations, smaller models have lower operational intensity, which makes high utilization more challenging to achieve; and (2) hosting a large number of models can be either prohibitively expensive or slow when dynamically switching between them.   ","In this paper, we describe how combining CoE, streaming dataflow, and a three-tier memory system scales the AI memory wall.","We describe Samba-CoE, a CoE system with 150 experts and a trillion total parameters.","We deploy Samba-CoE on the SambaNova SN40L","Reconfigurable Dataflow Unit (RDU) - a commercial dataflow accelerator architecture that has been co-designed for enterprise inference and training applications.","The chip introduces a new three-tier memory system with on-chip distributed SRAM, on-package HBM, and off-package DDR DRAM.","A dedicated inter-RDU network enables scaling up and out over multiple sockets.","We demonstrate speedups ranging from 2x to 13x on various benchmarks running on eight RDU sockets compared with an unfused baseline.","We show that for CoE inference deployments, the 8-socket RDU Node reduces machine footprint by up to 19x, speeds up model switching time by 15x to 31x, and achieves an overall speedup of 3.7x over a DGX H100 and 6.6x over a DGX A100."],"url":"http://arxiv.org/abs/2405.07518v1","category":"cs.AR"}
{"created":"2024-05-13 07:22:50","title":"OpenBot-Fleet: A System for Collective Learning with Real Robots","abstract":"We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system for navigation. OpenBot-Fleet uses smartphones for sensing, local compute and communication, Google Firebase for secure cloud storage and off-board compute, and a robust yet low-cost wheeled robot toact in real-world environments. The robots collect task data and upload it to the cloud where navigation policies can be learned either offline or online and can then be sent back to the robot fleet. In our experiments we distribute 72 robots to a crowd of workers who operate them in homes, and show that OpenBot-Fleet can learn robust navigation policies that generalize to unseen homes with >80% success rate. OpenBot-Fleet represents a significant step forward in cloud robotics, making it possible to deploy large continually learning robot fleets in a cost-effective and scalable manner. All materials can be found at https://www.openbot.org. A video is available at https://youtu.be/wiv2oaDgDi8","sentences":["We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system for navigation.","OpenBot-Fleet uses smartphones for sensing, local compute and communication, Google Firebase for secure cloud storage and off-board compute, and a robust yet low-cost wheeled robot toact in real-world environments.","The robots collect task data and upload it to the cloud where navigation policies can be learned either offline or online and can then be sent back to the robot fleet.","In our experiments we distribute 72 robots to a crowd of workers who operate them in homes, and show that OpenBot-Fleet can learn robust navigation policies that generalize to unseen homes with >80% success rate.","OpenBot-Fleet represents a significant step forward in cloud robotics, making it possible to deploy large continually learning robot fleets in a cost-effective and scalable manner.","All materials can be found at https://www.openbot.org.","A video is available at https://youtu.be/wiv2oaDgDi8"],"url":"http://arxiv.org/abs/2405.07515v1","category":"cs.RO"}
{"created":"2024-05-13 07:10:35","title":"RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection","abstract":"Anomaly detection in time series data is crucial across various domains. The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods. These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets. To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture. This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data. RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies. Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets.","sentences":["Anomaly detection in time series data is crucial across various domains.","The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods.","These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets.","To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture.","This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data.","RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies.","Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets."],"url":"http://arxiv.org/abs/2405.07509v1","category":"cs.LG"}
{"created":"2024-05-13 06:53:42","title":"Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation","abstract":"Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy's learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as two real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy's quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps. Code and training details will be released publicly.","sentences":["Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints.","These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference.","In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control.","By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups.","A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy's learned trajectories.","We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as two real-world tasks where we demonstrate inference on a laptop GPU.","For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates.","We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy's quality, a useful result that helps practioners avoid extensive testing of the pretrained model.","Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps.","Code and training details will be released publicly."],"url":"http://arxiv.org/abs/2405.07503v1","category":"cs.RO"}
{"created":"2024-05-13 17:53:35","title":"Diagnosing and Predicting Autonomous Vehicle Operational Safety Using Multiple Simulation Modalities and a Virtual Environment","abstract":"Even as technology and performance gains are made in the sphere of automated driving, safety concerns remain. Vehicle simulation has long been seen as a tool to overcome the cost associated with a massive amount of on-road testing for development and discovery of safety critical \"edge-cases\". However, purely software-based vehicle models may leave a large realism gap between their real-world counterparts in terms of dynamic response, and highly realistic vehicle-in-the-loop (VIL) simulations that encapsulate a virtual world around a physical vehicle may still be quite expensive to produce and similarly time intensive as on-road testing. In this work, we demonstrate an AV simulation test bed that combines the realism of vehicle-in-the-loop (VIL) simulation with the ease of implementation of model-in-the-loop (MIL) simulation. The setup demonstrated in this work allows for response diagnosis for the VIL simulations. By observing causal links between virtual weather and lighting conditions that surround the virtual depiction of our vehicle, the vision-based perception model and controller of Openpilot, and the dynamic response of our physical vehicle under test, we can draw conclusions regarding how the perceived environment contributed to vehicle response. Conversely, we also demonstrate response prediction for the MIL setup, where the need for a physical vehicle is not required to draw richer conclusions around the impact of environmental conditions on AV performance than could be obtained with VIL simulation alone. These combine for a simulation setup with accurate real-world implications for edge-case discovery that is both cost effective and time efficient to implement.","sentences":["Even as technology and performance gains are made in the sphere of automated driving, safety concerns remain.","Vehicle simulation has long been seen as a tool to overcome the cost associated with a massive amount of on-road testing for development and discovery of safety critical \"edge-cases\".","However, purely software-based vehicle models may leave a large realism gap between their real-world counterparts in terms of dynamic response, and highly realistic vehicle-in-the-loop (VIL) simulations that encapsulate a virtual world around a physical vehicle may still be quite expensive to produce and similarly time intensive as on-road testing.","In this work, we demonstrate an AV simulation test bed that combines the realism of vehicle-in-the-loop (VIL) simulation with the ease of implementation of model-in-the-loop (MIL) simulation.","The setup demonstrated in this work allows for response diagnosis for the VIL simulations.","By observing causal links between virtual weather and lighting conditions that surround the virtual depiction of our vehicle, the vision-based perception model and controller of Openpilot, and the dynamic response of our physical vehicle under test, we can draw conclusions regarding how the perceived environment contributed to vehicle response.","Conversely, we also demonstrate response prediction for the MIL setup, where the need for a physical vehicle is not required to draw richer conclusions around the impact of environmental conditions on AV performance than could be obtained with VIL simulation alone.","These combine for a simulation setup with accurate real-world implications for edge-case discovery that is both cost effective and time efficient to implement."],"url":"http://arxiv.org/abs/2405.07981v1","category":"cs.RO"}
{"created":"2024-05-13 17:50:02","title":"Unveiling the Pockels Coefficient of Ferroelectric Nitride ScAlN","abstract":"Nitride ferroelectrics have recently emerged as promising alternatives to oxide ferroelectrics due to their compatibility with mainstream semiconductor processing. ScAlN, in particular, has exhibited remarkable piezoelectric coupling strength ($K^2$) comparable to that of lithium niobate (LN), making it a valuable choice for RF filters in wireless communications. Recently, ScAlN has sparked interest in its use for nanophotonic devices, chiefly due to its large bandgap facilitating operation in blue wavelengths coupled with promises of enhanced nonlinear optical properties such as a large second-order susceptibility ($\\chi^{(2)}$). It is still an open question whether ScAlN can outperform oxide ferroelectrics concerning the Pockels effect -- an electro-optic coupling extensively utilized in optical communications devices. In this paper, we present a comprehensive theoretical analysis and experimental demonstration of ScAlN's Pockels effect. Our findings reveal that the electro-optic coupling of ScAlN, despite being weak at low Sc concentration, may be significantly enhanced at high levels of Sc doping, which points the direction of continued research efforts to unlock the full potential of ScAlN.","sentences":["Nitride ferroelectrics have recently emerged as promising alternatives to oxide ferroelectrics due to their compatibility with mainstream semiconductor processing.","ScAlN, in particular, has exhibited remarkable piezoelectric coupling strength ($K^2$) comparable to that of lithium niobate (LN), making it a valuable choice for RF filters in wireless communications.","Recently, ScAlN has sparked interest in its use for nanophotonic devices, chiefly due to its large bandgap facilitating operation in blue wavelengths coupled with promises of enhanced nonlinear optical properties such as a large second-order susceptibility ($\\chi^{(2)}$).","It is still an open question whether ScAlN can outperform oxide ferroelectrics concerning the Pockels effect -- an electro-optic coupling extensively utilized in optical communications devices.","In this paper, we present a comprehensive theoretical analysis and experimental demonstration of ScAlN's Pockels effect.","Our findings reveal that the electro-optic coupling of ScAlN, despite being weak at low Sc concentration, may be significantly enhanced at high levels of Sc doping, which points the direction of continued research efforts to unlock the full potential of ScAlN."],"url":"http://arxiv.org/abs/2405.07978v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 17:46:38","title":"Massive gravity generalization of $T\\overline{T}$ deformations","abstract":"Motivated by the two-dimensional massive gravity description of $T\\overline{T}$ deformations, we propose a direct generalization in $d$ dimensions. Our methodology indicates that all terms up to order $d$ are present in the deformation. In two dimensions, $T\\overline{T}$ is enhanced by a linear and a constant term, and exhibits an interesting behaviour regarding the deformed spectrum and correlators. At certain limits, this deformation can reduce to $T\\overline{T}$ or $T\\overline{T}+\\Lambda_{2}$ consistently. Using the massive gravity method, we obtain the classically deformed action of a sigma model of bosons and fermions interacting with an arbitrary potential, extending previous results. As a consequence, a proposal regarding the deformation of higher-derivative theories is made. Moreover, a standard dimensional reduction procedure is presented, with the resulting operator matching the form of prior findings under certain assumptions. In $d\\geq2$, we provide the exact structure of the quadratic terms in agreement with previous approaches, as well as the structure of the linear and constant terms. All higher order contributions are not easily evaluated, yet we derive the complete answer for all cases up to seven dimensions. Under certain conditions, these terms vanish, resulting in a quadratic operator. The trace-flow equation for this family of deformations is also derived. Finally, we investigate the class of root-$T\\overline{T}$ operators in various dimensions within the scope of this formalism.","sentences":["Motivated by the two-dimensional massive gravity description of $T\\overline{T}$ deformations, we propose a direct generalization in $d$ dimensions.","Our methodology indicates that all terms up to order $d$ are present in the deformation.","In two dimensions, $T\\overline{T}$ is enhanced by a linear and a constant term, and exhibits an interesting behaviour regarding the deformed spectrum and correlators.","At certain limits, this deformation can reduce to $T\\overline{T}$ or $T\\overline{T}+\\Lambda_{2}$ consistently.","Using the massive gravity method, we obtain the classically deformed action of a sigma model of bosons and fermions interacting with an arbitrary potential, extending previous results.","As a consequence, a proposal regarding the deformation of higher-derivative theories is made.","Moreover, a standard dimensional reduction procedure is presented, with the resulting operator matching the form of prior findings under certain assumptions.","In $d\\geq2$, we provide the exact structure of the quadratic terms in agreement with previous approaches, as well as the structure of the linear and constant terms.","All higher order contributions are not easily evaluated, yet we derive the complete answer for all cases up to seven dimensions.","Under certain conditions, these terms vanish, resulting in a quadratic operator.","The trace-flow equation for this family of deformations is also derived.","Finally, we investigate the class of root-$T\\overline{T}$ operators in various dimensions within the scope of this formalism."],"url":"http://arxiv.org/abs/2405.07967v1","category":"hep-th"}
{"created":"2024-05-13 17:46:30","title":"Fast Computation of Superquantile-Constrained Optimization Through Implicit Scenario Reduction","abstract":"Superquantiles have recently gained significant interest as a risk-aware metric for addressing fairness and distribution shifts in statistical learning and decision making problems. This paper introduces a fast, scalable and robust second-order computational framework to solve large-scale optimization problems with superquantile-based constraints. Unlike empirical risk minimization, superquantile-based optimization requires ranking random functions evaluated across all scenarios to compute the tail conditional expectation. While this tail-based feature might seem computationally unfriendly, it provides an advantageous setting for a semismooth-Newton-based augmented Lagrangian method. The superquantile operator effectively reduces the dimensions of the Newton systems since the tail expectation involves considerably fewer scenarios. Notably, the extra cost of obtaining relevant second-order information and performing matrix inversions is often comparable to, and sometimes even less than, the effort required for gradient computation. Our developed solver is particularly effective when the number of scenarios substantially exceeds the number of decision variables. In synthetic problems with linear and convex diagonal quadratic objectives, numerical experiments demonstrate that our method outperforms existing approaches by a large margin: It achieves speeds more than 750 times faster for linear and quadratic objectives than the alternating direction method of multipliers as implemented by OSQP for computing low-accuracy solutions. Additionally, it is up to 25 times faster for linear objectives and 70 times faster for quadratic objectives than the commercial solver Gurobi, and 20 times faster for linear objectives and 30 times faster for quadratic objectives than the Portfolio Safeguard optimization suite for high-accuracy solution computations.","sentences":["Superquantiles have recently gained significant interest as a risk-aware metric for addressing fairness and distribution shifts in statistical learning and decision making problems.","This paper introduces a fast, scalable and robust second-order computational framework to solve large-scale optimization problems with superquantile-based constraints.","Unlike empirical risk minimization, superquantile-based optimization requires ranking random functions evaluated across all scenarios to compute the tail conditional expectation.","While this tail-based feature might seem computationally unfriendly, it provides an advantageous setting for a semismooth-Newton-based augmented Lagrangian method.","The superquantile operator effectively reduces the dimensions of the Newton systems since the tail expectation involves considerably fewer scenarios.","Notably, the extra cost of obtaining relevant second-order information and performing matrix inversions is often comparable to, and sometimes even less than, the effort required for gradient computation.","Our developed solver is particularly effective when the number of scenarios substantially exceeds the number of decision variables.","In synthetic problems with linear and convex diagonal quadratic objectives, numerical experiments demonstrate that our method outperforms existing approaches by a large margin: It achieves speeds more than 750 times faster for linear and quadratic objectives than the alternating direction method of multipliers as implemented by OSQP for computing low-accuracy solutions.","Additionally, it is up to 25 times faster for linear objectives and 70 times faster for quadratic objectives than the commercial solver Gurobi, and 20 times faster for linear objectives and 30 times faster for quadratic objectives than the Portfolio Safeguard optimization suite for high-accuracy solution computations."],"url":"http://arxiv.org/abs/2405.07965v1","category":"math.OC"}
{"created":"2024-05-13 17:27:50","title":"White paper on ($\u03b1$, n) neutron yields calculation","abstract":"Understanding the radiogenic neutron production rate through the (${\\alpha}$, n) reaction is essential in many fields of physics like dark matter searches, neutrino studies, nuclear astrophysics and medical physics. This white paper provides a review of the current landscape of (${\\alpha}$, n) yields, neutron spectra and correlated ${\\gamma}$-rays calculations, and describes the existing tools and the available cross-sections. The uncertainties that contribute to (${\\alpha}$, n) yield calculations are also discussed with plans for a program to improve the accuracy of these estimates. Novel ideas to measure (${\\alpha}$, n) cross-sections for a variety of materials of interest are presented. The goal of this study is to reduce the uncertainty in the expected sensitivity of next-generation physics experiments in the keV--MeV regime.","sentences":["Understanding the radiogenic neutron production rate through the (${\\alpha}$, n) reaction is essential in many fields of physics like dark matter searches, neutrino studies, nuclear astrophysics and medical physics.","This white paper provides a review of the current landscape of (${\\alpha}$, n) yields, neutron spectra and correlated ${\\gamma}$-rays calculations, and describes the existing tools and the available cross-sections.","The uncertainties that contribute to (${\\alpha}$, n) yield calculations are also discussed with plans for a program to improve the accuracy of these estimates.","Novel ideas to measure (${\\alpha}$, n) cross-sections for a variety of materials of interest are presented.","The goal of this study is to reduce the uncertainty in the expected sensitivity of next-generation physics experiments in the keV--MeV regime."],"url":"http://arxiv.org/abs/2405.07952v1","category":"nucl-ex"}
{"created":"2024-05-13 17:13:32","title":"Active Learning with Simple Questions","abstract":"We consider an active learning setting where a learner is presented with a pool S of n unlabeled examples belonging to a domain X and asks queries to find the underlying labeling that agrees with a target concept h^* \\in H.   In contrast to traditional active learning that queries a single example for its label, we study more general region queries that allow the learner to pick a subset of the domain T \\subset X and a target label y and ask a labeler whether h^*(x) = y for every example in the set T \\cap S.   Such more powerful queries allow us to bypass the limitations of traditional active learning and use significantly fewer rounds of interactions to learn but can potentially lead to a significantly more complex query language. Our main contribution is quantifying the trade-off between the number of queries and the complexity of the query language used by the learner.   We measure the complexity of the region queries via the VC dimension of the family of regions. We show that given any hypothesis class H with VC dimension d, one can design a region query family Q with VC dimension O(d) such that for every set of n examples S \\subset X and every h^* \\in H, a learner can submit O(d log n) queries from Q to a labeler and perfectly label S. We show a matching lower bound by designing a hypothesis class H with VC dimension d and a dataset S \\subset X of size n such that any learning algorithm using any query class with VC dimension O(d) must make poly(n) queries to label S perfectly.   Finally, we focus on well-studied hypothesis classes including unions of intervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain stronger results. In particular, we design learning algorithms that (i) are computationally efficient and (ii) work even when the queries are not answered based on the learner's pool of examples S but on some unknown superset L of S","sentences":["We consider an active learning setting where a learner is presented with a pool S of n unlabeled examples belonging to a domain X and asks queries to find the underlying labeling that agrees with a target concept h^*","\\in H.   In contrast to traditional active learning that queries a single example for its label, we study more general region queries that allow the learner to pick a subset of the domain T \\subset X and a target label y and ask a labeler whether h^*(x)","= y for every example in the set T \\cap S.   Such more powerful queries allow us to bypass the limitations of traditional active learning and use significantly fewer rounds of interactions to learn but can potentially lead to a significantly more complex query language.","Our main contribution is quantifying the trade-off between the number of queries and the complexity of the query language used by the learner.   ","We measure the complexity of the region queries via the VC dimension of the family of regions.","We show that given any hypothesis class H with VC dimension d, one can design a region query family Q with VC dimension O(d) such that for every set of n examples S \\subset X and every h^*","\\in H, a learner can submit O(d log n) queries from Q to a labeler and perfectly label S.","We show a matching lower bound by designing a hypothesis class H with VC dimension d and a dataset S \\subset X of size n such that any learning algorithm using any query class with VC dimension O(d) must make poly(n) queries to label S perfectly.   ","Finally, we focus on well-studied hypothesis classes including unions of intervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain stronger results.","In particular, we design learning algorithms that (i) are computationally efficient and (ii) work even when the queries are not answered based on the learner's pool of examples S","but on some unknown superset L of S"],"url":"http://arxiv.org/abs/2405.07937v1","category":"cs.LG"}
{"created":"2024-05-13 16:48:57","title":"High-level Stream Processing: A Complementary Analysis of Fault Recovery","abstract":"Parallel computing is very important to accelerate the performance of software systems. Additionally, considering that a recurring challenge is to process high data volumes continuously, stream processing emerged as a paradigm and software architectural style. Several software systems rely on stream processing to deliver scalable performance, whereas open-source frameworks provide coding abstraction and high-level parallel computing. Although stream processing's performance is being extensively studied, the measurement of fault tolerance--a key abstraction offered by stream processing frameworks--has still not been adequately measured with comprehensive testbeds. In this work, we extend the previous fault recovery measurements with an exploratory analysis of the configuration space, additional experimental measurements, and analysis of improvement opportunities. We focus on robust deployment setups inspired by requirements for near real-time analytics of a large cloud observability platform. The results indicate significant potential for improving fault recovery and performance. However, these improvements entail grappling with configuration complexities, particularly in identifying and selecting the configurations to be fine-tuned and determining the appropriate values for them. Therefore, new abstractions for transparent configuration tuning are also needed for large-scale industry setups. We believe that more software engineering efforts are needed to provide insights into potential abstractions and how to achieve them. The stream processing community and industry practitioners could also benefit from more interactions with the high-level parallel programming community, whose expertise and insights on making parallel programming more productive and efficient could be extended.","sentences":["Parallel computing is very important to accelerate the performance of software systems.","Additionally, considering that a recurring challenge is to process high data volumes continuously, stream processing emerged as a paradigm and software architectural style.","Several software systems rely on stream processing to deliver scalable performance, whereas open-source frameworks provide coding abstraction and high-level parallel computing.","Although stream processing's performance is being extensively studied, the measurement of fault tolerance--a key abstraction offered by stream processing frameworks--has still not been adequately measured with comprehensive testbeds.","In this work, we extend the previous fault recovery measurements with an exploratory analysis of the configuration space, additional experimental measurements, and analysis of improvement opportunities.","We focus on robust deployment setups inspired by requirements for near real-time analytics of a large cloud observability platform.","The results indicate significant potential for improving fault recovery and performance.","However, these improvements entail grappling with configuration complexities, particularly in identifying and selecting the configurations to be fine-tuned and determining the appropriate values for them.","Therefore, new abstractions for transparent configuration tuning are also needed for large-scale industry setups.","We believe that more software engineering efforts are needed to provide insights into potential abstractions and how to achieve them.","The stream processing community and industry practitioners could also benefit from more interactions with the high-level parallel programming community, whose expertise and insights on making parallel programming more productive and efficient could be extended."],"url":"http://arxiv.org/abs/2405.07917v1","category":"cs.DC"}
{"created":"2024-05-13 16:46:44","title":"CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control & Altering of T2I Models","abstract":"Text-to-image generative models have become a prominent and powerful tool that excels at generating high-resolution realistic images. However, guiding the generative process of these models to consider detailed forms of conditioning reflecting style and/or structure information remains an open problem. In this paper, we present LoRAdapter, an approach that unifies both style and structure conditioning under the same formulation using a novel conditional LoRA block that enables zero-shot control. LoRAdapter is an efficient, powerful, and architecture-agnostic approach to condition text-to-image diffusion models, which enables fine-grained control conditioning during generation and outperforms recent state-of-the-art approaches","sentences":["Text-to-image generative models have become a prominent and powerful tool that excels at generating high-resolution realistic images.","However, guiding the generative process of these models to consider detailed forms of conditioning reflecting style and/or structure information remains an open problem.","In this paper, we present LoRAdapter, an approach that unifies both style and structure conditioning under the same formulation using a novel conditional LoRA block that enables zero-shot control.","LoRAdapter is an efficient, powerful, and architecture-agnostic approach to condition text-to-image diffusion models, which enables fine-grained control conditioning during generation and outperforms recent state-of-the-art approaches"],"url":"http://arxiv.org/abs/2405.07913v1","category":"cs.CV"}
{"created":"2024-05-13 16:41:47","title":"Collaborative Planar Pushing of Polytopic Objects with Multiple Robots in Complex Scenes","abstract":"Pushing is a simple yet effective skill for robots to interact with and further change the environment. Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator. However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator. This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments. It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces. The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arch segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot. The proposed framework is complete under mild assumptions. Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments. Robustness to motion and actuation uncertainties is also demonstrated.","sentences":["Pushing is a simple yet effective skill for robots to interact with and further change the environment.","Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator.","However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator.","This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments.","It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces.","The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arch segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot.","The proposed framework is complete under mild assumptions.","Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments.","Robustness to motion and actuation uncertainties is also demonstrated."],"url":"http://arxiv.org/abs/2405.07908v1","category":"cs.RO"}
{"created":"2024-05-13 16:41:32","title":"Robust Quantum Sensing with Multiparameter Decorrelation","abstract":"The performance of a quantum sensor is fundamentally limited by noise. This noise is particularly damaging when it becomes correlated with the readout of a target signal, caused by fluctuations of the sensor's operating parameters. These uncertainties limit sensitivity in a way that can be understood with multiparameter estimation theory. We develop a new approach, adaptable to any quantum platform, for designing robust sensing protocols that leverages multiparameter estimation theory and machine learning to decorrelate a target signal from fluctuating off-target (``nuisance'') parameters. Central to our approach is the identification of information-theoretic goals that guide a machine learning agent through an otherwise intractably large space of potential sensing protocols. As an illustrative example, we apply our approach to a reconfigurable optical lattice to design an accelerometer whose sensitivity is decorrelated from lattice depth noise. We demonstrate the effect of decorrelation on outcomes and Bayesian inferencing through statistical analysis in parameter space, and discuss implications for future applications in quantum metrology and computing.","sentences":["The performance of a quantum sensor is fundamentally limited by noise.","This noise is particularly damaging when it becomes correlated with the readout of a target signal, caused by fluctuations of the sensor's operating parameters.","These uncertainties limit sensitivity in a way that can be understood with multiparameter estimation theory.","We develop a new approach, adaptable to any quantum platform, for designing robust sensing protocols that leverages multiparameter estimation theory and machine learning to decorrelate a target signal from fluctuating off-target (``nuisance'') parameters.","Central to our approach is the identification of information-theoretic goals that guide a machine learning agent through an otherwise intractably large space of potential sensing protocols.","As an illustrative example, we apply our approach to a reconfigurable optical lattice to design an accelerometer whose sensitivity is decorrelated from lattice depth noise.","We demonstrate the effect of decorrelation on outcomes and Bayesian inferencing through statistical analysis in parameter space, and discuss implications for future applications in quantum metrology and computing."],"url":"http://arxiv.org/abs/2405.07907v1","category":"quant-ph"}
{"created":"2024-05-13 17:58:14","title":"Improved LARS algorithm for adaptive LASSO in the linear regression model","abstract":"The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model. In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator. Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples.","sentences":["The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model.","In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator.","Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples."],"url":"http://arxiv.org/abs/2405.07985v1","category":"stat.ME"}
{"created":"2024-05-13 17:15:38","title":"Efficient and Universal Merkle Tree Inclusion Proofs via OR Aggregation","abstract":"Zero-knowledge proofs have emerged as a powerful tool for enhancing privacy and security in blockchain applications. However, the efficiency and scalability of proof systems remain a significant challenge, particularly in the context of Merkle tree inclusion proofs. Traditional proof aggregation techniques based on AND logic suffer from high verification complexity and data communication overhead, limiting their practicality for large-scale applications. In this paper, we propose a novel proof aggregation approach based on OR logic, which enables the generation of compact and universally verifiable proofs for Merkle tree inclusion. By aggregating proofs using OR logic, we achieve a proof size that is independent of the number of leaves in the tree, and verification can be performed using any single valid leaf hash. This represents a significant improvement over AND aggregation, which requires the verifier to process all leaf hashes. We formally define the OR aggregation logic, describe the process of generating universal proofs, and provide a comparative analysis demonstrating the advantages of our approach in terms of proof size, verification data, and universality. Furthermore, we discuss the potential of combining OR and AND aggregation logics to create complex acceptance functions, enabling the development of expressive and efficient proof systems for various blockchain applications. The proposed techniques have the potential to significantly enhance the scalability, efficiency, and flexibility of zero-knowledge proof systems, paving the way for more practical and adaptive solutions in the blockchain ecosystem.","sentences":["Zero-knowledge proofs have emerged as a powerful tool for enhancing privacy and security in blockchain applications.","However, the efficiency and scalability of proof systems remain a significant challenge, particularly in the context of Merkle tree inclusion proofs.","Traditional proof aggregation techniques based on AND logic suffer from high verification complexity and data communication overhead, limiting their practicality for large-scale applications.","In this paper, we propose a novel proof aggregation approach based on OR logic, which enables the generation of compact and universally verifiable proofs for Merkle tree inclusion.","By aggregating proofs using OR logic, we achieve a proof size that is independent of the number of leaves in the tree, and verification can be performed using any single valid leaf hash.","This represents a significant improvement over AND aggregation, which requires the verifier to process all leaf hashes.","We formally define the OR aggregation logic, describe the process of generating universal proofs, and provide a comparative analysis demonstrating the advantages of our approach in terms of proof size, verification data, and universality.","Furthermore, we discuss the potential of combining OR and AND aggregation logics to create complex acceptance functions, enabling the development of expressive and efficient proof systems for various blockchain applications.","The proposed techniques have the potential to significantly enhance the scalability, efficiency, and flexibility of zero-knowledge proof systems, paving the way for more practical and adaptive solutions in the blockchain ecosystem."],"url":"http://arxiv.org/abs/2405.07941v1","category":"cs.CR"}
{"created":"2024-05-13 17:09:03","title":"Authentic Hand Avatar from a Phone Scan via Universal Hand Model","abstract":"The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR. In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar. For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately. The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage. On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline. We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much. Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar.","sentences":["The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR.","In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar.","For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately.","The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage.","On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline.","We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much.","Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar."],"url":"http://arxiv.org/abs/2405.07933v1","category":"cs.CV"}
{"created":"2024-05-13 16:57:52","title":"Adaptive first-order methods with enhanced worst-case rates","abstract":"The Optimized Gradient Method (OGM), its strongly convex extension, the Information Theoretical Exact Method (ITEM), as well as the related Triple Momentum Method (TMM) have superior convergence guarantees when compared to the Fast Gradient Method but lack adaptivity and their derivation is incompatible with composite problems. In this work we introduce a slightly modified version of the estimate sequence that can be used to simultaneously derive OGM, ITEM and TMM while adding memory along with the ability to dynamically adjust the convergence guarantees at runtime. Our framework can be extended to the composite setup and we use it to construct an Enhanced Accelerated Composite Gradient Method equipped with fully-adaptive line-search.","sentences":["The Optimized Gradient Method (OGM), its strongly convex extension, the Information Theoretical Exact Method (ITEM), as well as the related Triple Momentum Method (TMM) have superior convergence guarantees when compared to the Fast Gradient Method but lack adaptivity and their derivation is incompatible with composite problems.","In this work we introduce a slightly modified version of the estimate sequence that can be used to simultaneously derive OGM, ITEM and TMM while adding memory along with the ability to dynamically adjust the convergence guarantees at runtime.","Our framework can be extended to the composite setup and we use it to construct an Enhanced Accelerated Composite Gradient Method equipped with fully-adaptive line-search."],"url":"http://arxiv.org/abs/2405.07926v1","category":"math.OC"}
{"created":"2024-05-13 16:52:17","title":"Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?","abstract":"Going beyond mere fine-tuning of vision-language models (VLMs), learnable prompt tuning has emerged as a promising, resource-efficient alternative. Despite their potential, effectively learning prompts faces the following challenges: (i) training in a low-shot scenario results in overfitting, limiting adaptability and yielding weaker performance on newer classes or datasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with decreased performance in large class spaces, signaling potential gaps in bridging image and class concepts. In this work, we ask the question if better text semantics can help address these concerns. In particular, we introduce a prompt-tuning method that leverages class descriptions obtained from large language models (LLMs). Our approach constructs part-level description-guided views of both image and text features, which are subsequently aligned to learn more generalizable prompts. Our comprehensive experiments, conducted across 11 benchmark datasets, outperform established methods, demonstrating substantial improvements.","sentences":["Going beyond mere fine-tuning of vision-language models (VLMs), learnable prompt tuning has emerged as a promising, resource-efficient alternative.","Despite their potential, effectively learning prompts faces the following challenges: (i) training in a low-shot scenario results in overfitting, limiting adaptability and yielding weaker performance on newer classes or datasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with decreased performance in large class spaces, signaling potential gaps in bridging image and class concepts.","In this work, we ask the question if better text semantics can help address these concerns.","In particular, we introduce a prompt-tuning method that leverages class descriptions obtained from large language models (LLMs).","Our approach constructs part-level description-guided views of both image and text features, which are subsequently aligned to learn more generalizable prompts.","Our comprehensive experiments, conducted across 11 benchmark datasets, outperform established methods, demonstrating substantial improvements."],"url":"http://arxiv.org/abs/2405.07921v1","category":"cs.CV"}
{"created":"2024-05-13 16:38:09","title":"Predicting State Transitions in Autonomous Nonlinear Bistable Systems with Hidden Stochasticity","abstract":"Bistable autonomous systems can be found inmany areas of science. When the intrinsic noise intensity is large, these systems exhibits stochastic transitions from onemetastable steady state to another. In electronic bistable memories, these transitions are failures, usually simulated in a Monte-Carlo fashion at a high CPU-time price. Existing closed-form formulas, relying on near-stable-steady-state approximations of the nonlinear system dynamics to estimate the mean transition time, have turned out inaccurate. Our contribution is twofold. From a unidimensional stochastic model of overdamped autonomous systems, we propose an extended Eyring-Kramers analytical formula accounting for both nonlinear drift and state-dependent white noise variance, rigorously derived from It\\^o stochastic calculus. We also adapt it to practical system engineering situations where the intrinsic noise sources are hidden and can only be inferred from the fluctuations of observables measured in steady states. First numerical trials on an industrial electronic case study suggest that our approximate prediction formula achieve remarkable accuracy, outperforming previous non-Monte-Carlo approaches.","sentences":["Bistable autonomous systems can be found inmany areas of science.","When the intrinsic noise intensity is large, these systems exhibits stochastic transitions from onemetastable steady state to another.","In electronic bistable memories, these transitions are failures, usually simulated in a Monte-Carlo fashion at a high CPU-time price.","Existing closed-form formulas, relying on near-stable-steady-state approximations of the nonlinear system dynamics to estimate the mean transition time, have turned out inaccurate.","Our contribution is twofold.","From a unidimensional stochastic model of overdamped autonomous systems, we propose an extended Eyring-Kramers analytical formula accounting for both nonlinear drift and state-dependent white noise variance, rigorously derived from It\\^o stochastic calculus.","We also adapt it to practical system engineering situations where the intrinsic noise sources are hidden and can only be inferred from the fluctuations of observables measured in steady states.","First numerical trials on an industrial electronic case study suggest that our approximate prediction formula achieve remarkable accuracy, outperforming previous non-Monte-Carlo approaches."],"url":"http://arxiv.org/abs/2405.07902v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-13 15:38:33","title":"Knowledge Graph Embedding in Intent-Based Networking","abstract":"This paper presents a novel approach to network management by integrating intent-based networking (IBN) with knowledge graphs (KGs), creating a more intuitive and efficient pipeline for service orchestration. By mapping high-level business intents onto network configurations using KGs, the system dynamically adapts to network changes and service demands, ensuring optimal performance and resource allocation. We utilize knowledge graph embedding (KGE) to acquire context information from the network and service providers. The KGE model is trained using a custom KG and Gaussian embedding model and maps intents to services via service prediction and intent validation processes. The proposed intent lifecycle enables intent translation and assurance by only deploying validated intents according to network and resource availability. We evaluate the trained model for its efficiency in service mapping and intent validation tasks using simulated environments and extensive experiments. The service prediction and intent verification accuracy greater than 80 percent is achieved for the trained KGE model on a custom service orchestration intent knowledge graph (IKG) based on TMForum's intent common model.","sentences":["This paper presents a novel approach to network management by integrating intent-based networking (IBN) with knowledge graphs (KGs), creating a more intuitive and efficient pipeline for service orchestration.","By mapping high-level business intents onto network configurations using KGs, the system dynamically adapts to network changes and service demands, ensuring optimal performance and resource allocation.","We utilize knowledge graph embedding (KGE) to acquire context information from the network and service providers.","The KGE model is trained using a custom KG and Gaussian embedding model and maps intents to services via service prediction and intent validation processes.","The proposed intent lifecycle enables intent translation and assurance by only deploying validated intents according to network and resource availability.","We evaluate the trained model for its efficiency in service mapping and intent validation tasks using simulated environments and extensive experiments.","The service prediction and intent verification accuracy greater than 80 percent is achieved for the trained KGE model on a custom service orchestration intent knowledge graph (IKG) based on TMForum's intent common model."],"url":"http://arxiv.org/abs/2405.07850v1","category":"cs.NI"}
{"created":"2024-05-13 15:20:31","title":"Adaptive Human-Swarm Interaction based on Workload Measurement using Functional Near-Infrared Spectroscopy","abstract":"One of the challenges of human-swarm interaction (HSI) is how to manage the operator's workload. In order to do this, we propose a novel neurofeedback technique for the real-time measurement of workload using functional near-infrared spectroscopy (fNIRS). The objective is to develop a baseline for workload measurement in human-swarm interaction using fNIRS and to develop an interface that dynamically adapts to the operator's workload. The proposed method consists of using fNIRS device to measure brain activity, process this through a machine learning algorithm, and pass it on to the HSI interface. By dynamically adapting the HSI interface, the swarm operator's workload could be reduced and the performance improved.","sentences":["One of the challenges of human-swarm interaction (HSI) is how to manage the operator's workload.","In order to do this, we propose a novel neurofeedback technique for the real-time measurement of workload using functional near-infrared spectroscopy (fNIRS).","The objective is to develop a baseline for workload measurement in human-swarm interaction using fNIRS and to develop an interface that dynamically adapts to the operator's workload.","The proposed method consists of using fNIRS device to measure brain activity, process this through a machine learning algorithm, and pass it on to the HSI interface.","By dynamically adapting the HSI interface, the swarm operator's workload could be reduced and the performance improved."],"url":"http://arxiv.org/abs/2405.07834v1","category":"cs.RO"}
{"created":"2024-05-13 14:46:39","title":"Multiple stochastic resonances and inverse stochastic resonances in asymmetric bistable system under the ultra-high frequency excitation","abstract":"Ultra-high frequency linear frequency modulation (UHF-LFM) signal, as a kind of typical non-stationary signal, has been widely used in microwave radar and other fields, with advantages such as long transmission distance, strong anti-interference ability, and wide bandwidth. Utilizing optimal dynamics response has unique advantages in weak feature identification under strong background noise. We propose a new stochastic resonance method in an asymmetric bistable system with the time-varying parameter to handle this special non-stationary signal. Interestingly, the nonlinear response exhibits multiple stochastic resonances (MSR) and inverse stochastic resonances (ISR) under UHF-LFM signal excitation, and some resonance regions may deviate or collapse due to the influence of system asymmetry. In addition, we analyze the responses of each resonance region and the mechanism and evolution law of each resonance region in detail. Finally, we significantly expand the resonance region within the parameter range by optimizing the time scale, which verifies the effectiveness of the proposed time-varying scale method. The mechanism and evolution law of MSR and ISR will provide references for researchers in related fields.","sentences":["Ultra-high frequency linear frequency modulation (UHF-LFM) signal, as a kind of typical non-stationary signal, has been widely used in microwave radar and other fields, with advantages such as long transmission distance, strong anti-interference ability, and wide bandwidth.","Utilizing optimal dynamics response has unique advantages in weak feature identification under strong background noise.","We propose a new stochastic resonance method in an asymmetric bistable system with the time-varying parameter to handle this special non-stationary signal.","Interestingly, the nonlinear response exhibits multiple stochastic resonances (MSR) and inverse stochastic resonances (ISR) under UHF-LFM signal excitation, and some resonance regions may deviate or collapse due to the influence of system asymmetry.","In addition, we analyze the responses of each resonance region and the mechanism and evolution law of each resonance region in detail.","Finally, we significantly expand the resonance region within the parameter range by optimizing the time scale, which verifies the effectiveness of the proposed time-varying scale method.","The mechanism and evolution law of MSR and ISR will provide references for researchers in related fields."],"url":"http://arxiv.org/abs/2405.07804v1","category":"nlin.AO"}
{"created":"2024-05-13 14:37:03","title":"Decentralized Kernel Ridge Regression Based on Data-dependent Random Feature","abstract":"Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR). Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical. However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs. To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes. The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\\% across six real-world data sets.","sentences":["Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR).","Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical.","However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs.","To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes.","The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\\% across six real-world data sets."],"url":"http://arxiv.org/abs/2405.07791v1","category":"cs.LG"}
{"created":"2024-05-13 14:21:18","title":"SAR Image Synthesis with Diffusion Models","abstract":"In recent years, diffusion models (DMs) have become a popular method for generating synthetic data. By achieving samples of higher quality, they quickly became superior to generative adversarial networks (GANs) and the current state-of-the-art method in generative modeling. However, their potential has not yet been exploited in radar, where the lack of available training data is a long-standing problem. In this work, a specific type of DMs, namely denoising diffusion probabilistic model (DDPM) is adapted to the SAR domain. We investigate the network choice and specific diffusion parameters for conditional and unconditional SAR image generation. In our experiments, we show that DDPM qualitatively and quantitatively outperforms state-of-the-art GAN-based methods for SAR image generation. Finally, we show that DDPM profits from pretraining on largescale clutter data, generating SAR images of even higher quality.","sentences":["In recent years, diffusion models (DMs) have become a popular method for generating synthetic data.","By achieving samples of higher quality, they quickly became superior to generative adversarial networks (GANs) and the current state-of-the-art method in generative modeling.","However, their potential has not yet been exploited in radar, where the lack of available training data is a long-standing problem.","In this work, a specific type of DMs, namely denoising diffusion probabilistic model (DDPM) is adapted to the SAR domain.","We investigate the network choice and specific diffusion parameters for conditional and unconditional SAR image generation.","In our experiments, we show that DDPM qualitatively and quantitatively outperforms state-of-the-art GAN-based methods for SAR image generation.","Finally, we show that DDPM profits from pretraining on largescale clutter data, generating SAR images of even higher quality."],"url":"http://arxiv.org/abs/2405.07776v1","category":"cs.CV"}
{"created":"2024-05-13 13:56:25","title":"Minimax rates in variance and covariance changepoint testing","abstract":"We study the detection of a change in the spatial covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\\log\\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data. Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account. In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues. In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult. As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change.","sentences":["We study the detection of a change in the spatial covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\\log\\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data.","Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account.","In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues.","In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult.","As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change."],"url":"http://arxiv.org/abs/2405.07757v1","category":"math.ST"}
{"created":"2024-05-13 13:40:55","title":"MoCo: Fuzzing Deep Learning Libraries via Assembling Code","abstract":"The rapidly developing deep learning (DL) techniques have been applied in software systems with various application scenarios. However, they could also pose new safety threats with potentially serious consequences, especially in safety-critical domains. DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems. Previous research on fuzzing DL libraries still has limitations in the diversity of test inputs, the construction of test oracles, and the precision of detection. In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code. MoCo first disassembles the seed code file to obtain the template and code blocks, and then employs code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate more new code blocks adapted to the template. By inserting context-appropriate code blocks into the template step by step, MoCo can generate a tree of code files with intergenerational relations. According to the derivation relations in this tree and the applied mutation operators, we construct the test oracle based on the execution state consistency. Since the granularity of code assembly and mutation is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions. We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor). During the experiment, MoCo detects 64 new bugs of four types in three DL libraries, where 51 bugs have been confirmed, and 13 bugs have been fixed by developers.","sentences":["The rapidly developing deep learning (DL) techniques have been applied in software systems with various application scenarios.","However, they could also pose new safety threats with potentially serious consequences, especially in safety-critical domains.","DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems.","Previous research on fuzzing DL libraries still has limitations in the diversity of test inputs, the construction of test oracles, and the precision of detection.","In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code.","MoCo first disassembles the seed code file to obtain the template and code blocks, and then employs code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate more new code blocks adapted to the template.","By inserting context-appropriate code blocks into the template step by step, MoCo can generate a tree of code files with intergenerational relations.","According to the derivation relations in this tree and the applied mutation operators, we construct the test oracle based on the execution state consistency.","Since the granularity of code assembly and mutation is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions.","We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor).","During the experiment, MoCo detects 64 new bugs of four types in three DL libraries, where 51 bugs have been confirmed, and 13 bugs have been fixed by developers."],"url":"http://arxiv.org/abs/2405.07744v1","category":"cs.SE"}
{"created":"2024-05-13 13:39:59","title":"The interaction of gravitational waves with matter","abstract":"It is well-known that gravitational waves (GWs) undergo no absorption or dissipation when traversing through a perfect fluid. However, in the presence of a viscous fluid, GWs transfer energy to the fluid medium. In this essay, we present a review of our recent series of results regarding the interaction between gravitational waves and surrounding matter. Additionally, we examine the impact of a viscous fluid shell on gravitational wave propagation, focusing particularly on GW damping and GW heating. Furthermore, we explore the significance of these effects in various astrophysical scenarios such as core-collapse Supernovae and primordial gravitational waves.","sentences":["It is well-known that gravitational waves (GWs) undergo no absorption or dissipation when traversing through a perfect fluid.","However, in the presence of a viscous fluid, GWs transfer energy to the fluid medium.","In this essay, we present a review of our recent series of results regarding the interaction between gravitational waves and surrounding matter.","Additionally, we examine the impact of a viscous fluid shell on gravitational wave propagation, focusing particularly on GW damping and GW heating.","Furthermore, we explore the significance of these effects in various astrophysical scenarios such as core-collapse Supernovae and primordial gravitational waves."],"url":"http://arxiv.org/abs/2405.07743v1","category":"gr-qc"}
{"created":"2024-05-13 13:24:17","title":"Does Dependency Locality Predict Non-canonical Word Order in Hindi?","abstract":"Previous work has shown that isolated non-canonical sentences with Object-before-Subject (OSV) order are initially harder to process than their canonical counterparts with Subject-before-Object (SOV) order. Although this difficulty diminishes with appropriate discourse context, the underlying cognitive factors responsible for alleviating processing challenges in OSV sentences remain a question. In this work, we test the hypothesis that dependency length minimization is a significant predictor of non-canonical (OSV) syntactic choices, especially when controlling for information status such as givenness and surprisal measures. We extract sentences from the Hindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects and objects, systematically permute the preverbal constituents of those sentences, and deploy a classifier to distinguish between original corpus sentences and artificially generated alternatives. The classifier leverages various discourse-based and cognitive features, including dependency length, surprisal, and information status, to inform its predictions. Our results suggest that, although there exists a preference for minimizing dependency length in non-canonical corpus sentences amidst the generated variants, this factor does not significantly contribute in identifying corpus sentences above and beyond surprisal and givenness measures. Notably, discourse predictability emerges as the primary determinant of constituent-order preferences. These findings are further supported by human evaluations involving 44 native Hindi speakers. Overall, this work sheds light on the role of expectation adaptation in word-ordering decisions. We conclude by situating our results within the theories of discourse production and information locality.","sentences":["Previous work has shown that isolated non-canonical sentences with Object-before-Subject (OSV) order are initially harder to process than their canonical counterparts with Subject-before-Object (SOV) order.","Although this difficulty diminishes with appropriate discourse context, the underlying cognitive factors responsible for alleviating processing challenges in OSV sentences remain a question.","In this work, we test the hypothesis that dependency length minimization is a significant predictor of non-canonical (OSV) syntactic choices, especially when controlling for information status such as givenness and surprisal measures.","We extract sentences from the Hindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects and objects, systematically permute the preverbal constituents of those sentences, and deploy a classifier to distinguish between original corpus sentences and artificially generated alternatives.","The classifier leverages various discourse-based and cognitive features, including dependency length, surprisal, and information status, to inform its predictions.","Our results suggest that, although there exists a preference for minimizing dependency length in non-canonical corpus sentences amidst the generated variants, this factor does not significantly contribute in identifying corpus sentences above and beyond surprisal and givenness measures.","Notably, discourse predictability emerges as the primary determinant of constituent-order preferences.","These findings are further supported by human evaluations involving 44 native Hindi speakers.","Overall, this work sheds light on the role of expectation adaptation in word-ordering decisions.","We conclude by situating our results within the theories of discourse production and information locality."],"url":"http://arxiv.org/abs/2405.07730v1","category":"cs.CL"}
{"created":"2024-05-13 12:57:03","title":"Valence Quark PDFs of the Proton from Two-Current Correlations in Lattice QCD","abstract":"Following previous works on that topic, we consider Euclidean hadronic matrix elements in position space of two spatially separated local currents on the lattice, in order to extract the $x$-dependence of parton distribution functions (PDFs). The corresponding approach is often referred to by the term lattice cross section (LCS). In this work we will consider valence quark PDFs of an unpolarized proton. We adapt the previously established formalism to our choice of operators. The calculation of the two-current matrix elements requires the evaluation of four-point functions. The corresponding calculation is carried out on a $n_f = 2+1$ gauge ensemble with lattice spacing $a = 0.0856~\\mathrm{fm}$ and pseudoscalar masses $m_\\pi = 355~\\mathrm{MeV}$, $m_K = 441~\\mathrm{MeV}$. The four-point functions have been evaluated in a previous project. The lattice data is converted to the $\\overline{\\mathrm{MS}}$-scheme at a scale $\\mu=2~\\mathrm{GeV}$ and improved w.r.t. lattice artifacts. We use a common model as fit ansatz for the lattice data in order to extract the PDFs.","sentences":["Following previous works on that topic, we consider Euclidean hadronic matrix elements in position space of two spatially separated local currents on the lattice, in order to extract the $x$-dependence of parton distribution functions (PDFs).","The corresponding approach is often referred to by the term lattice cross section (LCS).","In this work we will consider valence quark PDFs of an unpolarized proton.","We adapt the previously established formalism to our choice of operators.","The calculation of the two-current matrix elements requires the evaluation of four-point functions.","The corresponding calculation is carried out on a $n_f = 2+1$ gauge ensemble with lattice spacing $a = 0.0856~\\mathrm{fm}$ and pseudoscalar masses $m_\\pi = 355~\\mathrm{MeV}$, $m_K = 441~\\mathrm{MeV}$.","The four-point functions have been evaluated in a previous project.","The lattice data is converted to the $\\overline{\\mathrm{MS}}$-scheme at a scale $\\mu=2~\\mathrm{GeV}$ and improved w.r.t. lattice artifacts.","We use a common model as fit ansatz for the lattice data in order to extract the PDFs."],"url":"http://arxiv.org/abs/2405.07712v1","category":"hep-lat"}
{"created":"2024-05-13 12:52:58","title":"Secure Aggregation Meets Sparsification in Decentralized Learning","abstract":"Decentralized learning (DL) faces increased vulnerability to privacy breaches due to sophisticated attacks on machine learning (ML) models. Secure aggregation is a computationally efficient cryptographic technique that enables multiple parties to compute an aggregate of their private data while keeping their individual inputs concealed from each other and from any central aggregator. To enhance communication efficiency in DL, sparsification techniques are used, selectively sharing only the most crucial parameters or gradients in a model, thereby maintaining efficiency without notably compromising accuracy. However, applying secure aggregation to sparsified models in DL is challenging due to the transmission of disjoint parameter sets by distinct nodes, which can prevent masks from canceling out effectively. This paper introduces CESAR, a novel secure aggregation protocol for DL designed to be compatible with existing sparsification mechanisms. CESAR provably defends against honest-but-curious adversaries and can be formally adapted to counteract collusion between them. We provide a foundational understanding of the interaction between the sparsification carried out by the nodes and the proportion of the parameters shared under CESAR in both colluding and non-colluding environments, offering analytical insight into the working and applicability of the protocol. Experiments on a network with 48 nodes in a 3-regular topology show that with random subsampling, CESAR is always within 0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD), while adding only 11% of data overhead. Moreover, it surpasses the accuracy on TopK by up to 0.3% on independent and identically distributed (IID) data.","sentences":["Decentralized learning (DL) faces increased vulnerability to privacy breaches due to sophisticated attacks on machine learning (ML) models.","Secure aggregation is a computationally efficient cryptographic technique that enables multiple parties to compute an aggregate of their private data while keeping their individual inputs concealed from each other and from any central aggregator.","To enhance communication efficiency in DL, sparsification techniques are used, selectively sharing only the most crucial parameters or gradients in a model, thereby maintaining efficiency without notably compromising accuracy.","However, applying secure aggregation to sparsified models in DL is challenging due to the transmission of disjoint parameter sets by distinct nodes, which can prevent masks from canceling out effectively.","This paper introduces CESAR, a novel secure aggregation protocol for DL designed to be compatible with existing sparsification mechanisms.","CESAR provably defends against honest-but-curious adversaries and can be formally adapted to counteract collusion between them.","We provide a foundational understanding of the interaction between the sparsification carried out by the nodes and the proportion of the parameters shared under CESAR in both colluding and non-colluding environments, offering analytical insight into the working and applicability of the protocol.","Experiments on a network with 48 nodes in a 3-regular topology show that with random subsampling, CESAR is always within 0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD), while adding only 11% of data overhead.","Moreover, it surpasses the accuracy on TopK by up to 0.3% on independent and identically distributed (IID) data."],"url":"http://arxiv.org/abs/2405.07708v2","category":"cs.LG"}
{"created":"2024-05-13 12:32:45","title":"MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders","abstract":"Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed object occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains.","sentences":["Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image.","Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations.","We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space.","MonoMAE consists of two novel designs.","The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training.","It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information.","The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries.","With the proposed object occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects.","Additionally, MonoMAE learns generalizable representations that can work well in new domains."],"url":"http://arxiv.org/abs/2405.07696v1","category":"cs.CV"}
{"created":"2024-05-13 12:30:09","title":"High-energy neutrinos from late-time jets of gamma-ray bursts with cocoon photons","abstract":"In gamma-ray bursts (GRBs), $\\sim$ 100 - 1000 s after the prompt emission, afterglow observations have consistently shown X-ray excesses detected in the form of flares (XFs; in long GRBs) or extended emission (EEs; in short GRBs). These observations are interpreted as emissions from jets launched by late central engine activity. However, the characteristics of these late-time jets, particularly the dissipation radius ($r_{\\rm diss}$), Lorentz factor ($\\Gamma$), and cosmic-ray loading factor ($\\xi_p$), remain unknown despite their importance. Here, in order to understand the properties of the late-time jets with future multi-messenger observations, we estimate the detectability of neutrinos associated with late-time emissions for a wide range of $r_{\\rm diss}$ and $\\Gamma$, assuming $\\xi_p=10$. We take into account external seed photons from the cocoon around the jets, which can enhance the neutrino production through photohadronic interaction in the jet dissipation region. Our results are still consistent with the upper limit obtained by IceCube. Our calculations indicate a promising prospect for neutrino detection with IceCube-Gen2 through the stacking of $\\sim 1000-2000$ events, for a wide range of $r_{\\rm diss}$ and $\\Gamma$. We found that setting an optimal energy threshold of 10 TeV can significantly reduce noise without negatively affecting neutrino detection. Furthermore, even in the case of non-detection, we show that meaningful constraints on the characteristics of the late-time jets can be obtained.","sentences":["In gamma-ray bursts (GRBs), $\\sim$ 100 - 1000 s after the prompt emission, afterglow observations have consistently shown X-ray excesses detected in the form of flares (XFs; in long GRBs) or extended emission (EEs; in short GRBs).","These observations are interpreted as emissions from jets launched by late central engine activity.","However, the characteristics of these late-time jets, particularly the dissipation radius ($r_{\\rm diss}$), Lorentz factor ($\\Gamma$), and cosmic-ray loading factor ($\\xi_p$), remain unknown despite their importance.","Here, in order to understand the properties of the late-time jets with future multi-messenger observations, we estimate the detectability of neutrinos associated with late-time emissions for a wide range of $r_{\\rm diss}$ and $\\Gamma$, assuming $\\xi_p=10$. We take into account external seed photons from the cocoon around the jets, which can enhance the neutrino production through photohadronic interaction in the jet dissipation region.","Our results are still consistent with the upper limit obtained by IceCube.","Our calculations indicate a promising prospect for neutrino detection with IceCube-Gen2 through the stacking of $\\sim 1000-2000$ events, for a wide range of $r_{\\rm diss}$ and $\\Gamma$. We found that setting an optimal energy threshold of 10 TeV can significantly reduce noise without negatively affecting neutrino detection.","Furthermore, even in the case of non-detection, we show that meaningful constraints on the characteristics of the late-time jets can be obtained."],"url":"http://arxiv.org/abs/2405.07695v1","category":"astro-ph.HE"}
{"created":"2024-05-13 12:23:13","title":"Quality of Experience Optimization for Real-time XR Video Transmission with Energy Constraints","abstract":"Extended Reality (XR) is an important service in the 5G network and in future 6G networks. In contrast to traditional video on demand services, real-time XR video is transmitted frame-by-frame, requiring low latency and being highly sensitive to network fluctuations. In this paper, we model the quality of experience (QoE) for real-time XR video transmission on a frame-by-frame basis. Based on the proposed QoE model, we formulate an optimization problem that maximizes QoE with constraints on wireless resources and long-term energy consumption. We utilize Lyapunov optimization to transform the original problem into a single-frame optimization problem and then allocate wireless subchannels. We propose an adaptive XR video bitrate algorithm that employs a Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video bitrate selection. Through numerical results, we show that our proposed algorithm outperforms the baseline algorithms, with the average QoE improvements of 0.04 to 0.46. Specifically, compared to baseline algorithms, the proposed algorithm reduces average video quality variations by 29% to 50% and improves the frame transmission success rate by 5% to 48%.","sentences":["Extended Reality (XR) is an important service in the 5G network and in future 6G networks.","In contrast to traditional video on demand services, real-time XR video is transmitted frame-by-frame, requiring low latency and being highly sensitive to network fluctuations.","In this paper, we model the quality of experience (QoE) for real-time XR video transmission on a frame-by-frame basis.","Based on the proposed QoE model, we formulate an optimization problem that maximizes QoE with constraints on wireless resources and long-term energy consumption.","We utilize Lyapunov optimization to transform the original problem into a single-frame optimization problem and then allocate wireless subchannels.","We propose an adaptive XR video bitrate algorithm that employs a Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video bitrate selection.","Through numerical results, we show that our proposed algorithm outperforms the baseline algorithms, with the average QoE improvements of 0.04 to 0.46.","Specifically, compared to baseline algorithms, the proposed algorithm reduces average video quality variations by 29% to 50% and improves the frame transmission success rate by 5% to 48%."],"url":"http://arxiv.org/abs/2405.07689v1","category":"cs.MM"}
{"created":"2024-05-13 11:13:17","title":"CDFormer:When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution","abstract":"Existing Blind image Super-Resolution (BSR) methods focus on estimating either kernel or degradation information, but have long overlooked the essential content details. In this paper, we propose a novel BSR approach, Content-aware Degradation-driven Transformer (CDFormer), to capture both degradation and content representations. However, low-resolution images cannot provide enough content details, and thus we introduce a diffusion-based module $CDFormer_{diff}$ to first learn Content Degradation Prior (CDP) in both low- and high-resolution images, and then approximate the real distribution given only low-resolution information. Moreover, we apply an adaptive SR network $CDFormer_{SR}$ that effectively utilizes CDP to refine features. Compared to previous diffusion-based SR methods, we treat the diffusion model as an estimator that can overcome the limitations of expensive sampling time and excessive diversity. Experiments show that CDFormer can outperform existing methods, establishing a new state-of-the-art performance on various benchmarks under blind settings. Codes and models will be available at \\href{https://github.com/I2-Multimedia-Lab/CDFormer}{https://github.com/I2-Multimedia-Lab/CDFormer}.","sentences":["Existing Blind image Super-Resolution (BSR) methods focus on estimating either kernel or degradation information, but have long overlooked the essential content details.","In this paper, we propose a novel BSR approach, Content-aware Degradation-driven Transformer (CDFormer), to capture both degradation and content representations.","However, low-resolution images cannot provide enough content details, and thus we introduce a diffusion-based module $CDFormer_{diff}$ to first learn Content Degradation","Prior (CDP) in both low- and high-resolution images, and then approximate the real distribution given only low-resolution information.","Moreover, we apply an adaptive SR network $CDFormer_{SR}$ that effectively utilizes CDP to refine features.","Compared to previous diffusion-based SR methods, we treat the diffusion model as an estimator that can overcome the limitations of expensive sampling time and excessive diversity.","Experiments show that CDFormer can outperform existing methods, establishing a new state-of-the-art performance on various benchmarks under blind settings.","Codes and models will be available at \\href{https://github.com/I2-Multimedia-Lab/CDFormer}{https://github.com/I2-Multimedia-Lab/CDFormer}."],"url":"http://arxiv.org/abs/2405.07648v1","category":"cs.CV"}
{"created":"2024-05-13 10:26:53","title":"New Low-Dissipation Central-Upwind Schemes. Part II","abstract":"The low-dissipation central-upwind (LDCU) schemes have been recently introduced in [A. Kurganov and R. Xin, J. Sci. Comput., 96 (2023), Paper No. 56] as a modification of the central-upwind (CU) schemes from [{\\sc A. Kurganov and C. T. Lin, Commun. Comput. Phys., 2 (2007), pp. 141-163}]. The LDCU schemes achieve much higher resolution of contact waves and many (two-dimensional) structures resulting from complicated wave interaction. However, the LDCU schemes sometimes produce more oscillatory results compared with the CU schemes, especially near the computational domain boundaries.   In this paper, we propose a very simple -- yet systematic -- modification of the LDCU schemes, which completely eliminates the aforementioned oscillations almost without affecting the quality of the computed solution.","sentences":["The low-dissipation central-upwind (LDCU) schemes have been recently introduced in [A. Kurganov and R. Xin, J. Sci.","Comput., 96 (2023), Paper No. 56] as a modification of the central-upwind (CU) schemes from [{\\sc A. Kurganov and C. T. Lin, Commun.","Comput.","Phys., 2 (2007), pp. 141-163}].","The LDCU schemes achieve much higher resolution of contact waves and many (two-dimensional) structures resulting from complicated wave interaction.","However, the LDCU schemes sometimes produce more oscillatory results compared with the CU schemes, especially near the computational domain boundaries.   ","In this paper, we propose a very simple -- yet systematic -- modification of the LDCU schemes, which completely eliminates the aforementioned oscillations almost without affecting the quality of the computed solution."],"url":"http://arxiv.org/abs/2405.07620v1","category":"math.NA"}
{"created":"2024-05-13 10:06:25","title":"Robustness of Interferometric Power to Sudden Death","abstract":"We study the dissipative dynamics of interferometric power as a discordlike measure in Markovian environments, such as dephasing, depolarizing, and generalized amplitude damping. Moreover, we compare the dynamics of interferometric power and entanglement by choosing proper initial conditions. Our study shows that in all cases where the sudden death of entanglement appears, interferometric power decays asymptotically. Therefore, quantum metrology based on interferometric power is more robust than entanglement.","sentences":["We study the dissipative dynamics of interferometric power as a discordlike measure in Markovian environments, such as dephasing, depolarizing, and generalized amplitude damping.","Moreover, we compare the dynamics of interferometric power and entanglement by choosing proper initial conditions.","Our study shows that in all cases where the sudden death of entanglement appears, interferometric power decays asymptotically.","Therefore, quantum metrology based on interferometric power is more robust than entanglement."],"url":"http://arxiv.org/abs/2405.07602v1","category":"quant-ph"}
{"created":"2024-05-13 09:56:28","title":"RGBD-Glue: General Feature Combination for Robust RGB-D Point Cloud Registration","abstract":"Point cloud registration is a fundamental task for estimating rigid transformations between point clouds. Previous studies have used geometric information for extracting features, matching and estimating transformation. Recently, owing to the advancement of RGB-D sensors, researchers have attempted to utilize visual information to improve registration performance. However, these studies focused on extracting distinctive features by deep feature fusion, which cannot effectively solve the negative effects of each feature's weakness, and cannot sufficiently leverage the valid information. In this paper, we propose a new feature combination framework, which applies a looser but more effective fusion and can achieve better performance. An explicit filter based on transformation consistency is designed for the combination framework, which can overcome each feature's weakness. And an adaptive threshold determined by the error distribution is proposed to extract more valid information from the two types of features. Owing to the distinctive design, our proposed framework can estimate more accurate correspondences and is applicable to both hand-crafted and learning-based feature descriptors. Experiments on ScanNet show that our method achieves a state-of-the-art performance and the rotation accuracy of 99.1%.","sentences":["Point cloud registration is a fundamental task for estimating rigid transformations between point clouds.","Previous studies have used geometric information for extracting features, matching and estimating transformation.","Recently, owing to the advancement of RGB-D sensors, researchers have attempted to utilize visual information to improve registration performance.","However, these studies focused on extracting distinctive features by deep feature fusion, which cannot effectively solve the negative effects of each feature's weakness, and cannot sufficiently leverage the valid information.","In this paper, we propose a new feature combination framework, which applies a looser but more effective fusion and can achieve better performance.","An explicit filter based on transformation consistency is designed for the combination framework, which can overcome each feature's weakness.","And an adaptive threshold determined by the error distribution is proposed to extract more valid information from the two types of features.","Owing to the distinctive design, our proposed framework can estimate more accurate correspondences and is applicable to both hand-crafted and learning-based feature descriptors.","Experiments on ScanNet show that our method achieves a state-of-the-art performance and the rotation accuracy of 99.1%."],"url":"http://arxiv.org/abs/2405.07594v1","category":"cs.CV"}
{"created":"2024-05-13 08:33:00","title":"Space Domain based Ecological Cooperative and Adaptive Cruise Control on Rolling Terrain","abstract":"Ecological Cooperative and Adaptive Cruise Control (Eco-CACC) is widely focused to enhance sustainability of CACC. However, state-of-the-art Eco-CACC studies are still facing challenges in adopting on rolling terrain. Furthermore, they cannot ensure both ecology optimality and computational efficiency. Hence, this paper proposes a nonlinear optimal control based Eco-CACC controller. It has the following features: i) enhancing performance across rolling terrains by modeling in space domain; ii) enhancing fuel efficiency via globally optimizing all vehicle's fuel consumptions; iii) ensuring computational efficiency by developing a differential dynamic programming-based solving method for the non-linear optimal control problem; iv) ensuring string stability through theoretically proving and experimentally validating. The performance of the proposed Eco-CACC controller was evaluated. Results showed that the proposed Eco-CACC controller can improve average fuel saving by 37.67% at collector road and about 17.30% at major arterial.","sentences":["Ecological Cooperative and Adaptive Cruise Control (Eco-CACC) is widely focused to enhance sustainability of CACC.","However, state-of-the-art Eco-CACC studies are still facing challenges in adopting on rolling terrain.","Furthermore, they cannot ensure both ecology optimality and computational efficiency.","Hence, this paper proposes a nonlinear optimal control based Eco-CACC controller.","It has the following features: i) enhancing performance across rolling terrains by modeling in space domain; ii) enhancing fuel efficiency via globally optimizing all vehicle's fuel consumptions; iii) ensuring computational efficiency by developing a differential dynamic programming-based solving method for the non-linear optimal control problem; iv) ensuring string stability through theoretically proving and experimentally validating.","The performance of the proposed Eco-CACC controller was evaluated.","Results showed that the proposed Eco-CACC controller can improve average fuel saving by 37.67% at collector road and about 17.30% at major arterial."],"url":"http://arxiv.org/abs/2405.07553v1","category":"cs.RO"}
{"created":"2024-05-13 07:48:45","title":"Comparing Perceptions of Static and Adaptive Proactive Speech Agents","abstract":"A growing literature on speech interruptions describes how people interrupt one another with speech, but these behaviours have not yet been implemented in the design of artificial agents which interrupt. Perceptions of a prototype proactive speech agent which adapts its speech to both urgency and to the difficulty of the ongoing task it interrupts are compared against perceptions of a static proactive agent which does not. The study hypothesises that adaptive proactive speech modelled on human speech interruptions will lead to partner models which consider the proactive agent as a stronger conversational partner than a static agent, and that interruptions initiated by an adaptive agent will be judged as better timed and more appropriately asked. These hypotheses are all rejected however, as quantitative analysis reveals that participants view the adaptive agent as a poorer dialogue partner than the static agent and as less appropriate in the style it interrupts. Qualitative analysis sheds light on the source of this surprising finding, as participants see the adaptive agent as less socially appropriate and as less consistent in its interactions than the static agent.","sentences":["A growing literature on speech interruptions describes how people interrupt one another with speech, but these behaviours have not yet been implemented in the design of artificial agents which interrupt.","Perceptions of a prototype proactive speech agent which adapts its speech to both urgency and to the difficulty of the ongoing task it interrupts are compared against perceptions of a static proactive agent which does not.","The study hypothesises that adaptive proactive speech modelled on human speech interruptions will lead to partner models which consider the proactive agent as a stronger conversational partner than a static agent, and that interruptions initiated by an adaptive agent will be judged as better timed and more appropriately asked.","These hypotheses are all rejected however, as quantitative analysis reveals that participants view the adaptive agent as a poorer dialogue partner than the static agent and as less appropriate in the style it interrupts.","Qualitative analysis sheds light on the source of this surprising finding, as participants see the adaptive agent as less socially appropriate and as less consistent in its interactions than the static agent."],"url":"http://arxiv.org/abs/2405.07528v2","category":"cs.HC"}
{"created":"2024-05-13 07:20:21","title":"Fine-tuning the SwissBERT Encoder Model for Embedding Sentences and Documents","abstract":"Encoder models trained for the embedding of sentences or short documents have proven useful for tasks such as semantic search and topic modeling. In this paper, we present a version of the SwissBERT encoder model that we specifically fine-tuned for this purpose. SwissBERT contains language adapters for the four national languages of Switzerland -- German, French, Italian, and Romansh -- and has been pre-trained on a large number of news articles in those languages. Using contrastive learning based on a subset of these articles, we trained a fine-tuned version, which we call SentenceSwissBERT. Multilingual experiments on document retrieval and text classification in a Switzerland-specific setting show that SentenceSwissBERT surpasses the accuracy of the original SwissBERT model and of a comparable baseline. The model is openly available for research use.","sentences":["Encoder models trained for the embedding of sentences or short documents have proven useful for tasks such as semantic search and topic modeling.","In this paper, we present a version of the SwissBERT encoder model that we specifically fine-tuned for this purpose.","SwissBERT contains language adapters for the four national languages of Switzerland -- German, French, Italian, and Romansh -- and has been pre-trained on a large number of news articles in those languages.","Using contrastive learning based on a subset of these articles, we trained a fine-tuned version, which we call SentenceSwissBERT.","Multilingual experiments on document retrieval and text classification in a Switzerland-specific setting show that SentenceSwissBERT surpasses the accuracy of the original SwissBERT model and of a comparable baseline.","The model is openly available for research use."],"url":"http://arxiv.org/abs/2405.07513v1","category":"cs.CL"}
{"created":"2024-05-13 06:43:11","title":"Optimized Generation of Entanglement by Real-Time Ordering of Swapping Operations","abstract":"Long-distance quantum communication in quantum networks faces significant challenges due to the constraints imposed by the no-cloning theorem. Most existing quantum communication protocols rely on the a priori distribution of entanglement pairs (EPs), a process known to incur considerable latency due to its stochastic nature. In this work, we consider the problem of minimizing the latency of establishing an EP across a pair of nodes in a quantum network. While prior research has primarily focused on minimizing the expected generation latency by selecting {\\em static} entanglement routes and/or swapping trees in advance, our approach considers a real-time adaptive strategy -- wherein the order of entanglement-swapping operations (hence, the swapping tree used) is progressively determined at runtime based on the runtime success/failure of the stochastic events. In this context, we present a greedy algorithm that iteratively determines the best route and/or entanglement-swapping operation to perform at each stage based on the current network. We evaluate our schemes on randomly generated networks and observe a reduction in latency of up to 40% from the optimal offline approach.","sentences":["Long-distance quantum communication in quantum networks faces significant challenges due to the constraints imposed by the no-cloning theorem.","Most existing quantum communication protocols rely on the a priori distribution of entanglement pairs (EPs), a process known to incur considerable latency due to its stochastic nature.","In this work, we consider the problem of minimizing the latency of establishing an EP across a pair of nodes in a quantum network.","While prior research has primarily focused on minimizing the expected generation latency by selecting {\\em static} entanglement routes and/or swapping trees in advance, our approach considers a real-time adaptive strategy -- wherein the order of entanglement-swapping operations (hence, the swapping tree used) is progressively determined at runtime based on the runtime success/failure of the stochastic events.","In this context, we present a greedy algorithm that iteratively determines the best route and/or entanglement-swapping operation to perform at each stage based on the current network.","We evaluate our schemes on randomly generated networks and observe a reduction in latency of up to 40% from the optimal offline approach."],"url":"http://arxiv.org/abs/2405.07501v1","category":"quant-ph"}
{"created":"2024-05-13 06:32:57","title":"Oedipus: LLM-enchanced Reasoning CAPTCHA Solver","abstract":"CAPTCHAs have become a ubiquitous tool in safeguarding applications from automated bots. Over time, the arms race between CAPTCHA development and evasion techniques has led to increasingly sophisticated and diverse designs. The latest iteration, reasoning CAPTCHAs, exploits tasks that are intuitively simple for humans but challenging for conventional AI technologies, thereby enhancing security measures.   Driven by the evolving AI capabilities, particularly the advancements in Large Language Models (LLMs), we investigate the potential of multimodal LLMs to solve modern reasoning CAPTCHAs. Our empirical analysis reveals that, despite their advanced reasoning capabilities, LLMs struggle to solve these CAPTCHAs effectively. In response, we introduce Oedipus, an innovative end-to-end framework for automated reasoning CAPTCHA solving. Central to this framework is a novel strategy that dissects the complex and human-easy-AI-hard tasks into a sequence of simpler and AI-easy steps. This is achieved through the development of a Domain Specific Language (DSL) for CAPTCHAs that guides LLMs in generating actionable sub-steps for each CAPTCHA challenge. The DSL is customized to ensure that each unit operation is a highly solvable subtask revealed in our previous empirical study. These sub-steps are then tackled sequentially using the Chain-of-Thought (CoT) methodology.   Our evaluation shows that Oedipus effectively resolves the studied CAPTCHAs, achieving an average success rate of 63.5\\%. Remarkably, it also shows adaptability to the most recent CAPTCHA designs introduced in late 2023, which are not included in our initial study. This prompts a discussion on future strategies for designing reasoning CAPTCHAs that can effectively counter advanced AI solutions.","sentences":["CAPTCHAs have become a ubiquitous tool in safeguarding applications from automated bots.","Over time, the arms race between CAPTCHA development and evasion techniques has led to increasingly sophisticated and diverse designs.","The latest iteration, reasoning CAPTCHAs, exploits tasks that are intuitively simple for humans but challenging for conventional AI technologies, thereby enhancing security measures.   ","Driven by the evolving AI capabilities, particularly the advancements in Large Language Models (LLMs), we investigate the potential of multimodal LLMs to solve modern reasoning CAPTCHAs.","Our empirical analysis reveals that, despite their advanced reasoning capabilities, LLMs struggle to solve these CAPTCHAs effectively.","In response, we introduce Oedipus, an innovative end-to-end framework for automated reasoning CAPTCHA solving.","Central to this framework is a novel strategy that dissects the complex and human-easy-AI-hard tasks into a sequence of simpler and AI-easy steps.","This is achieved through the development of a Domain Specific Language (DSL) for CAPTCHAs that guides LLMs in generating actionable sub-steps for each CAPTCHA challenge.","The DSL is customized to ensure that each unit operation is a highly solvable subtask revealed in our previous empirical study.","These sub-steps are then tackled sequentially using the Chain-of-Thought (CoT) methodology.   ","Our evaluation shows that Oedipus effectively resolves the studied CAPTCHAs, achieving an average success rate of 63.5\\%.","Remarkably, it also shows adaptability to the most recent CAPTCHA designs introduced in late 2023, which are not included in our initial study.","This prompts a discussion on future strategies for designing reasoning CAPTCHAs that can effectively counter advanced AI solutions."],"url":"http://arxiv.org/abs/2405.07496v1","category":"cs.CR"}
{"created":"2024-05-13 06:16:15","title":"Variable-Length Secret Key Agreement via Random Stopping Time","abstract":"We consider a key agreement setting where two parties observe correlated random sources, and want to agree on a secret key via public discussions. In order to allow the key length to adapt to the realizations of the random sources, we allow the key to be of variable length, subject to a novel variable-length version of the uniformity constraint based on random stopping time. We propose simple, computationally efficient key agreement schemes under the new constraint. The proposed scheme can be considered as the key agreement analogue of variable-length source coding via Huffman coding, and the Knuth-Yao random number generator.","sentences":["We consider a key agreement setting where two parties observe correlated random sources, and want to agree on a secret key via public discussions.","In order to allow the key length to adapt to the realizations of the random sources, we allow the key to be of variable length, subject to a novel variable-length version of the uniformity constraint based on random stopping time.","We propose simple, computationally efficient key agreement schemes under the new constraint.","The proposed scheme can be considered as the key agreement analogue of variable-length source coding via Huffman coding, and the Knuth-Yao random number generator."],"url":"http://arxiv.org/abs/2405.07493v1","category":"cs.IT"}
{"created":"2024-05-13 05:48:35","title":"Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis","abstract":"Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.","sentences":["Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace.","Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one.","All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets.","In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently.","Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features.","To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment.","Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training.","In the case of full parameter fine-tuning, we can further improve layout analysis performance."],"url":"http://arxiv.org/abs/2405.07481v1","category":"cs.CV"}
{"created":"2024-05-13 05:41:55","title":"Enhancing 3D Object Detection by Using Neural Network with Self-adaptive Thresholding","abstract":"Robust 3D object detection remains a pivotal concern in the domain of autonomous field robotics. Despite notable enhancements in detection accuracy across standard datasets, real-world urban environments, characterized by their unstructured and dynamic nature, frequently precipitate an elevated incidence of false positives, thereby undermining the reliability of existing detection paradigms. In this context, our study introduces an advanced post-processing algorithm that modulates detection thresholds dynamically relative to the distance from the ego object. Traditional perception systems typically utilize a uniform threshold, which often leads to decreased efficacy in detecting distant objects. In contrast, our proposed methodology employs a Neural Network with a self-adaptive thresholding mechanism that significantly attenuates false negatives while concurrently diminishing false positives, particularly in complex urban settings. Empirical results substantiate that our algorithm not only augments the performance of 3D object detection models in diverse urban and adverse weather scenarios but also establishes a new benchmark for adaptive thresholding techniques in field robotics.","sentences":["Robust 3D object detection remains a pivotal concern in the domain of autonomous field robotics.","Despite notable enhancements in detection accuracy across standard datasets, real-world urban environments, characterized by their unstructured and dynamic nature, frequently precipitate an elevated incidence of false positives, thereby undermining the reliability of existing detection paradigms.","In this context, our study introduces an advanced post-processing algorithm that modulates detection thresholds dynamically relative to the distance from the ego object.","Traditional perception systems typically utilize a uniform threshold, which often leads to decreased efficacy in detecting distant objects.","In contrast, our proposed methodology employs a Neural Network with a self-adaptive thresholding mechanism that significantly attenuates false negatives while concurrently diminishing false positives, particularly in complex urban settings.","Empirical results substantiate that our algorithm not only augments the performance of 3D object detection models in diverse urban and adverse weather scenarios but also establishes a new benchmark for adaptive thresholding techniques in field robotics."],"url":"http://arxiv.org/abs/2405.07479v1","category":"cs.RO"}
{"created":"2024-05-13 05:23:48","title":"Integrating Intent Understanding and Optimal Behavior Planning for Behavior Tree Generation from Human Instructions","abstract":"Robots executing tasks following human instructions in domestic or industrial environments essentially require both adaptability and reliability. Behavior Tree (BT) emerges as an appropriate control architecture for these scenarios due to its modularity and reactivity. Existing BT generation methods, however, either do not involve interpreting natural language or cannot theoretically guarantee the BTs' success. This paper proposes a two-stage framework for BT generation, which first employs large language models (LLMs) to interpret goals from high-level instructions, then constructs an efficient goal-specific BT through the Optimal Behavior Tree Expansion Algorithm (OBTEA). We represent goals as well-formed formulas in first-order logic, effectively bridging intent understanding and optimal behavior planning. Experiments in the service robot validate the proficiency of LLMs in producing grammatically correct and accurately interpreted goals, demonstrate OBTEA's superiority over the baseline BT Expansion algorithm in various metrics, and finally confirm the practical deployability of our framework. The project website is https://dids-ei.github.io/Project/LLM-OBTEA/.","sentences":["Robots executing tasks following human instructions in domestic or industrial environments essentially require both adaptability and reliability.","Behavior Tree (BT) emerges as an appropriate control architecture for these scenarios due to its modularity and reactivity.","Existing BT generation methods, however, either do not involve interpreting natural language or cannot theoretically guarantee the BTs' success.","This paper proposes a two-stage framework for BT generation, which first employs large language models (LLMs) to interpret goals from high-level instructions, then constructs an efficient goal-specific BT through the Optimal Behavior Tree Expansion Algorithm (OBTEA).","We represent goals as well-formed formulas in first-order logic, effectively bridging intent understanding and optimal behavior planning.","Experiments in the service robot validate the proficiency of LLMs in producing grammatically correct and accurately interpreted goals, demonstrate OBTEA's superiority over the baseline BT Expansion algorithm in various metrics, and finally confirm the practical deployability of our framework.","The project website is https://dids-ei.github.io/Project/LLM-OBTEA/."],"url":"http://arxiv.org/abs/2405.07474v1","category":"cs.AI"}
{"created":"2024-05-13 05:18:07","title":"GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting","abstract":"The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing.","sentences":["The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON).","However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training.","Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON.","Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing.","In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON.","To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing.","To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues.","Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes.","Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing."],"url":"http://arxiv.org/abs/2405.07472v1","category":"cs.CV"}
{"created":"2024-05-13 04:21:00","title":"DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval","abstract":"Text-based person retrieval (TPR) aims to retrieve images of a person from an extensive array of candidates based on a given textual description. The core challenge lies in mapping visual and textual data into a unified latent space. While existing TPR methods concentrate on recognizing explicit and positive characteristics, they often neglect the critical influence of negative descriptors, resulting in potential false positives that fulfill positive criteria but could be excluded by negative descriptors. To alleviate these issues, we introduce DualFocus, a unified framework for integrating positive and negative descriptors to enhance the interpretative accuracy of vision-language foundational models regarding textual queries. DualFocus employs Dual (Positive/Negative) Attribute Prompt Learning (DAPL), which integrates Dual Image-Attribute Contrastive (DIAC) Learning and Sensitive Image-Attributes Matching (SIAM) Learning. This way DualFocus enhances the detection of unseen attributes, thereby boosting retrieval precision. To further achieve a balance between coarse and fine-grained alignment of visual and textual embeddings, we propose the Dynamic Tokenwise Similarity (DTS) loss, which refines the representation of both matching and non-matching descriptions, thereby enhancing the matching process through a detailed and adaptable similarity assessment. By focusing on token-level comparisons, DualFocus significantly outperforms existing techniques in both precision and robustness. The experiment results highlight DualFocus's superior performance on CUHK-PEDES, ICFG-PEDES, and RSTPReid.","sentences":["Text-based person retrieval (TPR) aims to retrieve images of a person from an extensive array of candidates based on a given textual description.","The core challenge lies in mapping visual and textual data into a unified latent space.","While existing TPR methods concentrate on recognizing explicit and positive characteristics, they often neglect the critical influence of negative descriptors, resulting in potential false positives that fulfill positive criteria but could be excluded by negative descriptors.","To alleviate these issues, we introduce DualFocus, a unified framework for integrating positive and negative descriptors to enhance the interpretative accuracy of vision-language foundational models regarding textual queries.","DualFocus employs Dual (Positive/Negative) Attribute Prompt Learning (DAPL), which integrates Dual Image-Attribute Contrastive (DIAC) Learning and Sensitive Image-Attributes Matching (SIAM) Learning.","This way DualFocus enhances the detection of unseen attributes, thereby boosting retrieval precision.","To further achieve a balance between coarse and fine-grained alignment of visual and textual embeddings, we propose the Dynamic Tokenwise Similarity (DTS) loss, which refines the representation of both matching and non-matching descriptions, thereby enhancing the matching process through a detailed and adaptable similarity assessment.","By focusing on token-level comparisons, DualFocus significantly outperforms existing techniques in both precision and robustness.","The experiment results highlight DualFocus's superior performance on CUHK-PEDES, ICFG-PEDES, and RSTPReid."],"url":"http://arxiv.org/abs/2405.07459v1","category":"cs.CV"}
{"created":"2024-05-13 02:07:51","title":"JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation","abstract":"Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras. However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced. In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose. Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter. JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively. The source code will be available at https://github.com/LuoXubo/JointLoc.","sentences":["Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras.","However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced.","In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose.","Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter.","JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively.","The source code will be available at https://github.com/LuoXubo/JointLoc."],"url":"http://arxiv.org/abs/2405.07429v1","category":"cs.RO"}
{"created":"2024-05-13 01:18:25","title":"MoVL:Exploring Fusion Strategies for the Domain-Adaptive Application of Pretrained Models in Medical Imaging Tasks","abstract":"Medical images are often more difficult to acquire than natural images due to the specialism of the equipment and technology, which leads to less medical image datasets. So it is hard to train a strong pretrained medical vision model. How to make the best of natural pretrained vision model and adapt in medical domain still pends. For image classification, a popular method is linear probe (LP). However, LP only considers the output after feature extraction. Yet, there exists a gap between input medical images and natural pretrained vision model. We introduce visual prompting (VP) to fill in the gap, and analyze the strategies of coupling between LP and VP. We design a joint learning loss function containing categorisation loss and discrepancy loss, which describe the variance of prompted and plain images, naming this joint training strategy MoVL (Mixture of Visual Prompting and Linear Probe). We experiment on 4 medical image classification datasets, with two mainstream architectures, ResNet and CLIP. Results shows that without changing the parameters and architecture of backbone model and with less parameters, there is potential for MoVL to achieve full finetune (FF) accuracy (on four medical datasets, average 90.91% for MoVL and 91.13% for FF). On out of distribution medical dataset, our method(90.33%) can outperform FF (85.15%) with absolute 5.18 % lead.","sentences":["Medical images are often more difficult to acquire than natural images due to the specialism of the equipment and technology, which leads to less medical image datasets.","So it is hard to train a strong pretrained medical vision model.","How to make the best of natural pretrained vision model and adapt in medical domain still pends.","For image classification, a popular method is linear probe (LP).","However, LP only considers the output after feature extraction.","Yet, there exists a gap between input medical images and natural pretrained vision model.","We introduce visual prompting (VP) to fill in the gap, and analyze the strategies of coupling between LP and VP.","We design a joint learning loss function containing categorisation loss and discrepancy loss, which describe the variance of prompted and plain images, naming this joint training strategy MoVL (Mixture of Visual Prompting and Linear Probe).","We experiment on 4 medical image classification datasets, with two mainstream architectures, ResNet and CLIP.","Results shows that without changing the parameters and architecture of backbone model and with less parameters, there is potential for MoVL to achieve full finetune (FF) accuracy (on four medical datasets, average 90.91% for MoVL and 91.13% for FF).","On out of distribution medical dataset, our method(90.33%) can outperform FF (85.15%) with absolute 5.18 % lead."],"url":"http://arxiv.org/abs/2405.07411v1","category":"cs.CV"}
{"created":"2024-05-12 23:34:06","title":"Semi-Supervised Weed Detection for Rapid Deployment and Enhanced Efficiency","abstract":"Weeds present a significant challenge in agriculture, causing yield loss and requiring expensive control measures. Automatic weed detection using computer vision and deep learning offers a promising solution. However, conventional deep learning methods often require large amounts of labelled training data, which can be costly and time-consuming to acquire. This paper introduces a novel method for semi-supervised weed detection, comprising two main components. Firstly, a multi-scale feature representation technique is employed to capture distinctive weed features across different scales. Secondly, we propose an adaptive pseudo-label assignment strategy, leveraging a small set of labelled images during training. This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data. Additionally, our approach integrates epoch-corresponding and mixed pseudo-labels to further enhance the learning process. Experimental results on the COCO dataset and five prominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap -- illustrate that our method achieves state-of-the-art performance in weed detection, even with significantly less labelled data compared to existing techniques. This approach holds the potential to alleviate the labelling burden and enhance the feasibility and deployment speed of deep learning for weed detection in real-world agricultural scenarios.","sentences":["Weeds present a significant challenge in agriculture, causing yield loss and requiring expensive control measures.","Automatic weed detection using computer vision and deep learning offers a promising solution.","However, conventional deep learning methods often require large amounts of labelled training data, which can be costly and time-consuming to acquire.","This paper introduces a novel method for semi-supervised weed detection, comprising two main components.","Firstly, a multi-scale feature representation technique is employed to capture distinctive weed features across different scales.","Secondly, we propose an adaptive pseudo-label assignment strategy, leveraging a small set of labelled images during training.","This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data.","Additionally, our approach integrates epoch-corresponding and mixed pseudo-labels to further enhance the learning process.","Experimental results on the COCO dataset and five prominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap -- illustrate that our method achieves state-of-the-art performance in weed detection, even with significantly less labelled data compared to existing techniques.","This approach holds the potential to alleviate the labelling burden and enhance the feasibility and deployment speed of deep learning for weed detection in real-world agricultural scenarios."],"url":"http://arxiv.org/abs/2405.07399v1","category":"cs.CV"}
{"created":"2024-05-12 23:32:31","title":"The Spike-and-Slab Quantile LASSO for Robust Variable Selection in Cancer Genomics Studies","abstract":"Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits. In the past decade, robust variable selection methods have emerged as powerful alternatives to the non-robust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models. In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD). The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Ro\\v{c}kov\\'a and George, 2018). Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with non-differentiable loss functions. We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods. The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA).","sentences":["Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits.","In the past decade, robust variable selection methods have emerged as powerful alternatives to the non-robust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models.","In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD).","The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Ro\\v{c}kov\\'a and George, 2018).","Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with non-differentiable loss functions.","We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods.","The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA)."],"url":"http://arxiv.org/abs/2405.07397v1","category":"stat.ME"}
{"created":"2024-05-12 20:45:52","title":"Advocating Feedback Control for Human-Earth System Applications","abstract":"This paper proposes a feedback control perspective for Human-Earth Systems (HESs) which essentially are complex systems that capture the interactions between humans and nature. Recent attention in HES research has been directed towards devising strategies for climate change mitigation and adaptation, aimed at achieving environmental and societal objectives. However, existing approaches heavily rely on HES models, which inherently suffer from inaccuracies due to the complexity of the system. Moreover, overly detailed models often prove impractical for optimization tasks. We propose a framework inheriting from feedback control strategies the robustness against model errors, because inaccuracies are mitigated using measurements retrieved from the field. The framework comprises two nested control loops. The outer loop computes the optimal inputs to the HES, which are then implemented by actuators controlled in the inner loop. Potential fields of applications are also identified.","sentences":["This paper proposes a feedback control perspective for Human-Earth Systems (HESs) which essentially are complex systems that capture the interactions between humans and nature.","Recent attention in HES research has been directed towards devising strategies for climate change mitigation and adaptation, aimed at achieving environmental and societal objectives.","However, existing approaches heavily rely on HES models, which inherently suffer from inaccuracies due to the complexity of the system.","Moreover, overly detailed models often prove impractical for optimization tasks.","We propose a framework inheriting from feedback control strategies the robustness against model errors, because inaccuracies are mitigated using measurements retrieved from the field.","The framework comprises two nested control loops.","The outer loop computes the optimal inputs to the HES, which are then implemented by actuators controlled in the inner loop.","Potential fields of applications are also identified."],"url":"http://arxiv.org/abs/2405.07376v1","category":"eess.SY"}
{"created":"2024-05-12 17:39:09","title":"AquaIntellect: A Semantic Self-learning Framework for Underwater Internet of Things Connectivity","abstract":"The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future. This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT. We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission. We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization. A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape. An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness. Simulation results show that the proposed solution outperforms conventional approaches.","sentences":["The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future.","This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT.","We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission.","We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization.","A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape.","An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness.","Simulation results show that the proposed solution outperforms conventional approaches."],"url":"http://arxiv.org/abs/2405.07342v1","category":"eess.SP"}
{"created":"2024-05-13 17:46:52","title":"Partial Causal Detectability of Linear Descriptor Systems and Existence of Functional ODE Estimators","abstract":"This paper studies the problem of state estimation for linear time-invariant descriptor systems in their most general form. The estimator is a system of ordinary differential equations (ODEs). We introduce the notion of partial causal detectability and characterize this concept by means of a simple rank criterion involving the system coefficient matrices. Also, several equivalent characterizations for partial causal detectability are established. In addition, we prove that partial causal detectability is equivalent to the existence of functional ODE estimators. A numerical example is given to validate the theoretical results.","sentences":["This paper studies the problem of state estimation for linear time-invariant descriptor systems in their most general form.","The estimator is a system of ordinary differential equations (ODEs).","We introduce the notion of partial causal detectability and characterize this concept by means of a simple rank criterion involving the system coefficient matrices.","Also, several equivalent characterizations for partial causal detectability are established.","In addition, we prove that partial causal detectability is equivalent to the existence of functional ODE estimators.","A numerical example is given to validate the theoretical results."],"url":"http://arxiv.org/abs/2405.07968v1","category":"math.OC"}
{"created":"2024-05-13 17:43:05","title":"KG-Planner: Knowledge-Informed Graph Neural Planning for Collaborative Manipulators","abstract":"This paper presents a novel knowledge-informed graph neural planner (KG-Planner) to address the challenge of efficiently planning collision-free motions for robots in high-dimensional spaces, considering both static and dynamic environments involving humans. Unlike traditional motion planners that struggle with finding a balance between efficiency and optimality, the KG-Planner takes a different approach. Instead of relying solely on a neural network or imitating the motions of an oracle planner, our KG-Planner integrates explicit physical knowledge from the workspace. The integration of knowledge has two key aspects: (1) we present an approach to design a graph that can comprehensively model the workspace's compositional structure. The designed graph explicitly incorporates critical elements such as robot joints, obstacles, and their interconnections. This representation allows us to capture the intricate relationships between these elements. (2) We train a Graph Neural Network (GNN) that excels at generating nearly optimal robot motions. In particular, the GNN employs a layer-wise propagation rule to facilitate the exchange and update of information among workspace elements based on their connections. This propagation emphasizes the influence of these elements throughout the planning process. To validate the efficacy and efficiency of our KG-Planner, we conduct extensive experiments in both static and dynamic environments. These experiments include scenarios with and without human workers. The results of our approach are compared against existing methods, showcasing the superior performance of the KG-Planner. A short video introduction of this work is available (video link provided in the paper).","sentences":["This paper presents a novel knowledge-informed graph neural planner (KG-Planner) to address the challenge of efficiently planning collision-free motions for robots in high-dimensional spaces, considering both static and dynamic environments involving humans.","Unlike traditional motion planners that struggle with finding a balance between efficiency and optimality, the KG-Planner takes a different approach.","Instead of relying solely on a neural network or imitating the motions of an oracle planner, our KG-Planner integrates explicit physical knowledge from the workspace.","The integration of knowledge has two key aspects: (1) we present an approach to design a graph that can comprehensively model the workspace's compositional structure.","The designed graph explicitly incorporates critical elements such as robot joints, obstacles, and their interconnections.","This representation allows us to capture the intricate relationships between these elements.","(2) We train a Graph Neural Network (GNN) that excels at generating nearly optimal robot motions.","In particular, the GNN employs a layer-wise propagation rule to facilitate the exchange and update of information among workspace elements based on their connections.","This propagation emphasizes the influence of these elements throughout the planning process.","To validate the efficacy and efficiency of our KG-Planner, we conduct extensive experiments in both static and dynamic environments.","These experiments include scenarios with and without human workers.","The results of our approach are compared against existing methods, showcasing the superior performance of the KG-Planner.","A short video introduction of this work is available (video link provided in the paper)."],"url":"http://arxiv.org/abs/2405.07962v1","category":"cs.RO"}
{"created":"2024-05-13 17:18:11","title":"Optimization Using Pathwise Algorithmic Derivatives of Electromagnetic Shower Simulations","abstract":"Among the well-known methods to approximate derivatives of expectancies computed by Monte-Carlo simulations, averages of pathwise derivatives are often the easiest one to apply. Computing them via algorithmic differentiation typically does not require major manual analysis and rewriting of the code, even for very complex programs like simulations of particle-detector interactions in high-energy physics. However, the pathwise derivative estimator can be biased if there are discontinuities in the program, which may diminish its value for applications.   This work integrates algorithmic differentiation into the electromagnetic shower simulation code HepEmShow based on G4HepEm, allowing us to study how well pathwise derivatives approximate derivatives of energy depositions in a sampling calorimeter with respect to parameters of the beam and geometry. We found that when multiple scattering is disabled in the simulation, means of pathwise derivatives converge quickly to their expected values, and these are close to the actual derivatives of the energy deposition. Additionally, we demonstrate the applicability of this novel gradient estimator for stochastic gradient-based optimization in a model example.","sentences":["Among the well-known methods to approximate derivatives of expectancies computed by Monte-Carlo simulations, averages of pathwise derivatives are often the easiest one to apply.","Computing them via algorithmic differentiation typically does not require major manual analysis and rewriting of the code, even for very complex programs like simulations of particle-detector interactions in high-energy physics.","However, the pathwise derivative estimator can be biased if there are discontinuities in the program, which may diminish its value for applications.   ","This work integrates algorithmic differentiation into the electromagnetic shower simulation code HepEmShow based on G4HepEm, allowing us to study how well pathwise derivatives approximate derivatives of energy depositions in a sampling calorimeter with respect to parameters of the beam and geometry.","We found that when multiple scattering is disabled in the simulation, means of pathwise derivatives converge quickly to their expected values, and these are close to the actual derivatives of the energy deposition.","Additionally, we demonstrate the applicability of this novel gradient estimator for stochastic gradient-based optimization in a model example."],"url":"http://arxiv.org/abs/2405.07944v1","category":"physics.comp-ph"}
{"created":"2024-05-13 17:14:06","title":"Compact moduli of Calabi-Yau cones and Sasaki-Einstein spaces","abstract":"We construct proper moduli algebraic spaces of K-polystable $\\mathbb{Q}$-Fano cones (a.k.a. Calabi-Yau cones) or equivalently their links i.e., Sasaki-Einstein manifolds with singularities.   As a byproduct, it gives alternative algebraic construction of proper K-moduli of $\\mathbb{Q}$-Fano varieties. In contrast to the previous algebraic proof of its properness ([BHLLX, LXZ]), we do not use the $\\delta$-invariants ([FO, BJ]) nor the $L^2$-normalized Donaldson-Futaki invariants. We use the local normalized volume of [Li] and the higher $\\Theta$-stable reduction instead.","sentences":["We construct proper moduli algebraic spaces of K-polystable $\\mathbb{Q}$-Fano cones (a.k.a. Calabi-Yau cones) or equivalently their links i.e., Sasaki-Einstein manifolds with singularities.   ","As a byproduct, it gives alternative algebraic construction of proper K-moduli of $\\mathbb{Q}$-Fano varieties.","In contrast to the previous algebraic proof of its properness ([BHLLX, LXZ]), we do not use the $\\delta$-invariants ([FO, BJ]) nor the $L^2$-normalized Donaldson-Futaki invariants.","We use the local normalized volume of [Li] and the higher $\\Theta$-stable reduction instead."],"url":"http://arxiv.org/abs/2405.07939v1","category":"math.AG"}
{"created":"2024-05-13 17:09:24","title":"Mordell-Tornheim zeta functions and functional equations for Herglotz-Zagier type functions","abstract":"The Mordell-Tornheim zeta function and the Herglotz-Zagier function $F(x)$ are two important functions in Mathematics. By generalizing a special case of the former, namely $\\Theta(z, x)$, we show that the theories of these functions are inextricably woven. We obtain a three-term functional equation for $\\Theta(z, x)$ as well as decompose it in terms of the Herglotz-Hurwitz function $\\Phi(z, x)$. This decomposition can be conceived as a two-term functional equation for $\\Phi(z, x)$. Through this result, we are not only able to get Zagier's identity relating $F(x)$ with $F(1/x)$ but also two-term functional equation for Ishibashi's generalization of $F(x)$, namely, $\\Phi_k(x)$ which has been sought after for over twenty years. We further generalize $\\Theta(z, x)$ by incorporating two Gauss sums, each associated to a Dirichlet character, and decompose it in terms of an interesting integral which involves the Fekete polynomial as well as the character polylogarithm. This result gives infinite families of functional equations of Herglotz-type integrals out of which only two, due to Kumar and Choie, were known so far. The first one among the two involves the integral $J(x)$ who special values have received a lot of attention, more recently, in the work of Muzzaffar and Williams, and in that of Radchenko and Zagier. Analytic continuation of our generalization of $\\Theta(z, x)$ is also accomplished which allows us to obtain transformations between certain double series and Herglotz-type integrals or their explicit evaluations.","sentences":["The Mordell-Tornheim zeta function and the Herglotz-Zagier function $F(x)$ are two important functions in Mathematics.","By generalizing a special case of the former, namely $\\Theta(z, x)$, we show that the theories of these functions are inextricably woven.","We obtain a three-term functional equation for $\\Theta(z, x)$ as well as decompose it in terms of the Herglotz-Hurwitz function $\\Phi(z, x)$.","This decomposition can be conceived as a two-term functional equation for $\\Phi(z, x)$.","Through this result, we are not only able to get Zagier's identity relating $F(x)$ with $F(1/x)$ but also two-term functional equation for Ishibashi's generalization of $F(x)$, namely, $\\Phi_k(x)$ which has been sought after for over twenty years.","We further generalize $\\Theta(z, x)$ by incorporating two Gauss sums, each associated to a Dirichlet character, and decompose it in terms of an interesting integral which involves the Fekete polynomial as well as the character polylogarithm.","This result gives infinite families of functional equations of Herglotz-type integrals out of which only two, due to Kumar and Choie, were known so far.","The first one among the two involves the integral $J(x)$ who special values have received a lot of attention, more recently, in the work of Muzzaffar and Williams, and in that of Radchenko and Zagier.","Analytic continuation of our generalization of $\\Theta(z, x)$ is also accomplished which allows us to obtain transformations between certain double series and Herglotz-type integrals or their explicit evaluations."],"url":"http://arxiv.org/abs/2405.07934v1","category":"math.NT"}
{"created":"2024-05-13 17:01:26","title":"On the Smooth Curve of Entire Vector Fields that Solves the Navier-Stokes Equation","abstract":"In this paper we prove the existence and smoothness of the Navier-Stokes Equation for viscosity large enough which after rescaling implies a solution for any positive viscosity, additionally, we show the existence of a curve of entire vector fields of order 2 that extends the solution to the complex domain for positive time.","sentences":["In this paper we prove the existence and smoothness of the Navier-Stokes Equation for viscosity large enough which after rescaling implies a solution for any positive viscosity, additionally, we show the existence of a curve of entire vector fields of order 2 that extends the solution to the complex domain for positive time."],"url":"http://arxiv.org/abs/2405.07929v1","category":"math.AP"}
{"created":"2024-05-13 16:50:42","title":"Exploring the Low-Pass Filtering Behavior in Image Super-Resolution","abstract":"Deep neural networks for image super-resolution have shown significant advantages over traditional approaches like interpolation. However, they are often criticized as `black boxes' compared to traditional approaches which have solid mathematical foundations. In this paper, we attempt to interpret the behavior of deep neural networks using theories from signal processing theories. We first report an intriguing phenomenon, referred to as `the sinc phenomenon,' which occurs when an impulse input is fed to a neural network. Building on this observation, we propose a method named Hybird Response Analysis (HyRA) to analyze the behavior of neural networks in image super-resolution tasks. In details, HyRA decomposes a neural network into a parallel connection of a linear system and a non-linear system, demonstrating that the linear system functions as a low-pass filter, while the non-linear system injects high-frequency information. Furthermore, to quantify the injected high-frequency information, we introduce a metric for image-to-image tasks called Frequency Spectrum Distribution Similarity (FSDS). FSDS reflects the distribution similarity of different frequency components, capturing nuances that traditional metrics may overlook. Code for this work can be found in: https://github.com/RisingEntropy/LPFInISR.","sentences":["Deep neural networks for image super-resolution have shown significant advantages over traditional approaches like interpolation.","However, they are often criticized as `black boxes' compared to traditional approaches which have solid mathematical foundations.","In this paper, we attempt to interpret the behavior of deep neural networks using theories from signal processing theories.","We first report an intriguing phenomenon, referred to as `the sinc phenomenon,' which occurs when an impulse input is fed to a neural network.","Building on this observation, we propose a method named Hybird Response Analysis (HyRA) to analyze the behavior of neural networks in image super-resolution tasks.","In details, HyRA decomposes a neural network into a parallel connection of a linear system and a non-linear system, demonstrating that the linear system functions as a low-pass filter, while the non-linear system injects high-frequency information.","Furthermore, to quantify the injected high-frequency information, we introduce a metric for image-to-image tasks called Frequency Spectrum Distribution Similarity (FSDS).","FSDS reflects the distribution similarity of different frequency components, capturing nuances that traditional metrics may overlook.","Code for this work can be found in: https://github.com/RisingEntropy/LPFInISR."],"url":"http://arxiv.org/abs/2405.07919v1","category":"cs.CV"}
{"created":"2024-05-13 16:47:06","title":"Discovery of highly anisotropic dielectric crystals with equivariant graph neural networks","abstract":"Anisotropy in crystals plays a pivotal role in many technological applications. For example, anisotropic electronic and thermal transport are thought to be beneficial for thermoelectric applications, while anisotropic mechanical properties are of interest for emerging metamaterials, and anisotropic dielectric materials have been suggested as a novel platform for dark matter detection. Understanding and tailoring anisotropy in crystals is therefore essential for the design of next-generation functional materials. To date, however, most data-driven approaches have focused on the prediction of scalar crystal properties, such as the spherically averaged dielectric tensor or the bulk and shear elastic moduli. Here, we adopt the latest approaches in equivariant graph neural networks to develop a model that can predict the full dielectric tensor of crystals. Our model, trained on the Materials Project dataset of c.a. 6,700 dielectric tensors, achieves state-of-the-art accuracy in scalar dielectric prediction in addition to capturing the directional response. We showcase the performance of the model by discovering crystals with almost isotropic connectivity but highly anisotropic dielectric tensors, thereby broadening our knowledge of the structure-property relationships in dielectric crystals.","sentences":["Anisotropy in crystals plays a pivotal role in many technological applications.","For example, anisotropic electronic and thermal transport are thought to be beneficial for thermoelectric applications, while anisotropic mechanical properties are of interest for emerging metamaterials, and anisotropic dielectric materials have been suggested as a novel platform for dark matter detection.","Understanding and tailoring anisotropy in crystals is therefore essential for the design of next-generation functional materials.","To date, however, most data-driven approaches have focused on the prediction of scalar crystal properties, such as the spherically averaged dielectric tensor or the bulk and shear elastic moduli.","Here, we adopt the latest approaches in equivariant graph neural networks to develop a model that can predict the full dielectric tensor of crystals.","Our model, trained on the Materials Project dataset of c.a. 6,700 dielectric tensors, achieves state-of-the-art accuracy in scalar dielectric prediction in addition to capturing the directional response.","We showcase the performance of the model by discovering crystals with almost isotropic connectivity but highly anisotropic dielectric tensors, thereby broadening our knowledge of the structure-property relationships in dielectric crystals."],"url":"http://arxiv.org/abs/2405.07915v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 16:45:06","title":"Slice closures of indexed languages and word equations with counting constraints","abstract":"Indexed languages are a classical notion in formal language theory. As the language equivalent of second-order pushdown automata, they have received considerable attention in higher-order model checking. Unfortunately, counting properties are notoriously difficult to decide for indexed languages: So far, all results about non-regular counting properties show undecidability.   In this paper, we initiate the study of slice closures of (Parikh images of) indexed languages. A slice is a set of vectors of natural numbers such that membership of $u,u+v,u+w$ implies membership of $u+v+w$. Our main result is that given an indexed language $L$, one can compute a semilinear representation of the smallest slice containing $L$'s Parikh image.   We present two applications. First, one can compute the set of all affine relations satisfied by the Parikh image of an indexed language. In particular, this answers affirmatively a question by Kobayashi: Is it decidable whether in a given indexed language, every word has the same number of $a$'s as $b$'s.   As a second application, we show decidability of (systems of) word equations with rational constraints and a class of counting constraints: These allow us to look for solutions where a counting function (defined by an automaton) is not zero. For example, one can decide whether a word equation with rational constraints has a solution where the number of occurrences of $a$ differs between variables $X$ and $Y$.","sentences":["Indexed languages are a classical notion in formal language theory.","As the language equivalent of second-order pushdown automata, they have received considerable attention in higher-order model checking.","Unfortunately, counting properties are notoriously difficult to decide for indexed languages: So far, all results about non-regular counting properties show undecidability.   ","In this paper, we initiate the study of slice closures of (Parikh images of) indexed languages.","A slice is a set of vectors of natural numbers such that membership of $u,u+v,u+w$ implies membership of $u+v+w$. Our main result is that given an indexed language $L$, one can compute a semilinear representation of the smallest slice containing $L$'s Parikh image.   ","We present two applications.","First, one can compute the set of all affine relations satisfied by the Parikh image of an indexed language.","In particular, this answers affirmatively a question by Kobayashi:","Is it decidable whether in a given indexed language, every word has the same number of $a$'s as $b$'s.   ","As a second application, we show decidability of (systems of) word equations with rational constraints and a class of counting constraints: These allow us to look for solutions where a counting function (defined by an automaton) is not zero.","For example, one can decide whether a word equation with rational constraints has a solution where the number of occurrences of $a$ differs between variables $X$ and $Y$."],"url":"http://arxiv.org/abs/2405.07911v1","category":"cs.FL"}
{"created":"2024-05-13 16:42:55","title":"A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability","abstract":"Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R.sq) in the regression of X against Xep may sometimes be sufficient to measure Pe.sq. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE can hold when EPC is controlled for, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment to ensure PE. This paper provides formal justifications for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and how to measure it.","sentences":["Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X).","When X is measured with error (Xep), two challenges arise in identifying AEE(X).","Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain.","To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts.","The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe).","Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms.","The coefficient of determination (R.sq) in the regression of X against Xep may sometimes be sufficient to measure Pe.sq.","The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms.","PE can hold when EPC is controlled for, which is weaker than NDEA.","The third, Emergent Confounding, describes when bias due to confounder measurement error arises.","Adjustment for E(P)C can be performed like confounding adjustment to ensure PE.","This paper provides formal justifications for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and how to measure it."],"url":"http://arxiv.org/abs/2405.07910v2","category":"math.ST"}
{"created":"2024-05-13 16:27:12","title":"All Nodes are created Not Equal: Node-Specific Layer Aggregation and Filtration for GNN","abstract":"The ever-designed Graph Neural Networks, though opening a promising path for the modeling of the graph-structure data, unfortunately introduce two daunting obstacles to their deployment on devices. (I) Most of existing GNNs are shallow, due mostly to the over-smoothing and gradient-vanish problem as they go deeper as convolutional architectures. (II) The vast majority of GNNs adhere to the homophily assumption, where the central node and its adjacent nodes share the same label. This assumption often poses challenges for many GNNs working with heterophilic graphs. Addressing the aforementioned issue has become a looming challenge in enhancing the robustness and scalability of GNN applications. In this paper, we take a comprehensive and systematic approach to overcoming the two aforementioned challenges for the first time. We propose a Node-Specific Layer Aggregation and Filtration architecture, termed NoSAF, a framework capable of filtering and processing information from each individual nodes. NoSAF introduces the concept of \"All Nodes are Created Not Equal\" into every layer of deep networks, aiming to provide a reliable information filter for each layer's nodes to sieve out information beneficial for the subsequent layer. By incorporating a dynamically updated codebank, NoSAF dynamically optimizes the optimal information outputted downwards at each layer. This effectively overcomes heterophilic issues and aids in deepening the network. To compensate for the information loss caused by the continuous filtering in NoSAF, we also propose NoSAF-D (Deep), which incorporates a compensation mechanism that replenishes information in every layer of the model, allowing NoSAF to perform meaningful computations even in very deep layers.","sentences":["The ever-designed Graph Neural Networks, though opening a promising path for the modeling of the graph-structure data, unfortunately introduce two daunting obstacles to their deployment on devices.","(I) Most of existing GNNs are shallow, due mostly to the over-smoothing and gradient-vanish problem as they go deeper as convolutional architectures.","(II)","The vast majority of GNNs adhere to the homophily assumption, where the central node and its adjacent nodes share the same label.","This assumption often poses challenges for many GNNs working with heterophilic graphs.","Addressing the aforementioned issue has become a looming challenge in enhancing the robustness and scalability of GNN applications.","In this paper, we take a comprehensive and systematic approach to overcoming the two aforementioned challenges for the first time.","We propose a Node-Specific Layer Aggregation and Filtration architecture, termed NoSAF, a framework capable of filtering and processing information from each individual nodes.","NoSAF introduces the concept of \"All Nodes are Created Not Equal\" into every layer of deep networks, aiming to provide a reliable information filter for each layer's nodes to sieve out information beneficial for the subsequent layer.","By incorporating a dynamically updated codebank, NoSAF dynamically optimizes the optimal information outputted downwards at each layer.","This effectively overcomes heterophilic issues and aids in deepening the network.","To compensate for the information loss caused by the continuous filtering in NoSAF, we also propose NoSAF-D (Deep), which incorporates a compensation mechanism that replenishes information in every layer of the model, allowing NoSAF to perform meaningful computations even in very deep layers."],"url":"http://arxiv.org/abs/2405.07892v1","category":"cs.LG"}
{"created":"2024-05-13 16:24:53","title":"The fermionic massless modular Hamiltonian","abstract":"We provide an explicit expression for the modular hamiltonian of the von Neumann algebras associated to the unit double cone for the (fermionic) quantum field theories of the 2-component Weyl (helicity 1/2) field, and of the 4-component massless Dirac and Majorana fields. To this end, we represent the one particle spaces of these theories in terms of solutions of the corresponding wave equations, and obtain the action of the modular group on them. As an application, we compute the relative entropy between the vacuum of the massless Majorana field and one particle states associated to waves with Cauchy data localized in the spatial unit ball.","sentences":["We provide an explicit expression for the modular hamiltonian of the von Neumann algebras associated to the unit double cone for the (fermionic) quantum field theories of the 2-component Weyl (helicity 1/2) field, and of the 4-component massless Dirac and Majorana fields.","To this end, we represent the one particle spaces of these theories in terms of solutions of the corresponding wave equations, and obtain the action of the modular group on them.","As an application, we compute the relative entropy between the vacuum of the massless Majorana field and one particle states associated to waves with Cauchy data localized in the spatial unit ball."],"url":"http://arxiv.org/abs/2405.07888v1","category":"math-ph"}
{"created":"2024-05-13 16:15:08","title":"On Hagedorn wavepackets associated with different Gaussians","abstract":"Hagedorn functions are carefully constructed generalizations of Hermite functions to the setting of many-dimensional squeezed and coupled harmonic systems. Wavepackets formed by superpositions of Hagedorn functions have been successfully used to solve the time-dependent Schr\\\"{o}dinger equation exactly in harmonic systems and variationally in anharmonic systems. For evaluating typical observables, such as position or kinetic energy, it is sufficient to consider orthonormal Hagedorn functions with a single Gaussian center. Here, we instead derive various relations between Hagedorn bases associated with different Gaussians, including their overlaps, which are necessary for evaluating quantities nonlocal in time, such as time correlation functions needed for computing spectra. First, we use the Bogoliubov transformation to obtain commutation relations between the ladder operators associated with different Gaussians. Then, instead of using numerical quadrature, we employ these commutation relations to derive exact recurrence relations for the overlap integrals between Hagedorn functions with different Gaussian centers. Finally, we present numerical experiments that demonstrate the accuracy and efficiency of our algebraic method as well as its suitability to treat problems in spectroscopy and chemical dynamics.","sentences":["Hagedorn functions are carefully constructed generalizations of Hermite functions to the setting of many-dimensional squeezed and coupled harmonic systems.","Wavepackets formed by superpositions of Hagedorn functions have been successfully used to solve the time-dependent Schr\\\"{o}dinger equation exactly in harmonic systems and variationally in anharmonic systems.","For evaluating typical observables, such as position or kinetic energy, it is sufficient to consider orthonormal Hagedorn functions with a single Gaussian center.","Here, we instead derive various relations between Hagedorn bases associated with different Gaussians, including their overlaps, which are necessary for evaluating quantities nonlocal in time, such as time correlation functions needed for computing spectra.","First, we use the Bogoliubov transformation to obtain commutation relations between the ladder operators associated with different Gaussians.","Then, instead of using numerical quadrature, we employ these commutation relations to derive exact recurrence relations for the overlap integrals between Hagedorn functions with different Gaussian centers.","Finally, we present numerical experiments that demonstrate the accuracy and efficiency of our algebraic method as well as its suitability to treat problems in spectroscopy and chemical dynamics."],"url":"http://arxiv.org/abs/2405.07880v1","category":"quant-ph"}
{"created":"2024-05-13 16:08:53","title":"Effective medium properties of stealthy hyperuniform photonic structures using multiscale physics-informed neural networks","abstract":"In this article, we employ multiscale physics-informed neural networks (MscalePINNs) for the inverse retrieval of the effective permittivity and homogenization of finite-size photonic media with stealthy hyperuniform (SHU) disordered geometries. Specifically, we show that MscalePINNs are capable of capturing the fast spatial variations of complex fields scattered by arrays of dielectric nanocylinders arranged according to isotropic SHU point patterns, thus enabling a systematic methodology to inverse retrieve their effective dielectric profiles. Our approach extends the recently developed high-frequency homogenization theory of hyperuniform media and retrieves more general permittivity profiles for applications-relevant finite-size SHU systems, unveiling unique features related to their isotropic nature. In particular, we demonstrate the existence of a transparency region beyond the long-wavelength approximation, enabling effective and isotropic homogenization even without disorder-averaging, in contrast to the case of uncorrelated Poisson random patterns. We believe that the multiscale network approach introduced here enables the efficient inverse design of general effective media and finite-size metamaterials with isotropic electromagnetic responses beyond the limitations of traditional homogenization theories.","sentences":["In this article, we employ multiscale physics-informed neural networks (MscalePINNs) for the inverse retrieval of the effective permittivity and homogenization of finite-size photonic media with stealthy hyperuniform (SHU) disordered geometries.","Specifically, we show that MscalePINNs are capable of capturing the fast spatial variations of complex fields scattered by arrays of dielectric nanocylinders arranged according to isotropic SHU point patterns, thus enabling a systematic methodology to inverse retrieve their effective dielectric profiles.","Our approach extends the recently developed high-frequency homogenization theory of hyperuniform media and retrieves more general permittivity profiles for applications-relevant finite-size SHU systems, unveiling unique features related to their isotropic nature.","In particular, we demonstrate the existence of a transparency region beyond the long-wavelength approximation, enabling effective and isotropic homogenization even without disorder-averaging, in contrast to the case of uncorrelated Poisson random patterns.","We believe that the multiscale network approach introduced here enables the efficient inverse design of general effective media and finite-size metamaterials with isotropic electromagnetic responses beyond the limitations of traditional homogenization theories."],"url":"http://arxiv.org/abs/2405.07878v1","category":"physics.optics"}
{"created":"2024-05-13 16:07:20","title":"Optimal accuracy for linear sets of equations with the graph Laplacian","abstract":"We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph -- those that achieve optimality asymptotically with the graph size and those that do not -- determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments.","sentences":["We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides.","Such sets of equations arise in PageRank and Markov chain theory.","We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations.","The set of optimally accurate systems can be separated into two groups for an undirected graph -- those that achieve optimality asymptotically with the graph size and those that do not -- determined by the angle between the right-hand side of the linear system and the vector of all ones.","We provide supporting numerical experiments."],"url":"http://arxiv.org/abs/2405.07877v1","category":"math.NA"}
{"created":"2024-05-13 15:58:20","title":"Conservative dielectric functions and electrical conductivities from the multicomponent Bhatnagar-Gross-Krook equation","abstract":"A considerable number of semi-empirical and first-principles models have been created to describe the dynamic response of a collisionally damped charged-particle system. However, known challenges persist for established dynamic structure factors (DSF), dielectric functions, and conductivities. For instance, the semi-empirical Drude-Smith conductivity [N.M. Smith, Phys. Rev. B 64, 155106 (2001)] lacks interpretability, and the first-principles Mermin dielectric function [N.D. Mermin, Phys. Rev. B, 1, 2362 (1970)] does not satisfy the frequency sum rule [G.S. Atwal and N.W. Ashcroft, Phys. Rev. B 65, 115109 (2002)]. In this work, starting from the multicomponent Bhatnagar-Gross-Krook (BGK) kinetic equation, we produce a multi-species susceptibility that conserves number and momentum, which we refer to as the ``completed Mermin'' susceptibility, and we explore its properties and uses. We show that the completed Mermin susceptibility satisfies the frequency sum (f-sum) rule. We compute the associated DSF and find that momentum conservation qualitatively impacts the DSF's shape for a carbon-contaminated deuterium and tritium plasma under NIF hot-spot conditions. In the appendices, we provide numerical implementations of the completed Mermin susceptibility, for the reader's convenience. Further, we produce a new non-Drude conductivity model, by taking the single-species limit and introducing free parameters in the terms that enforce number and momentum conservation. To illustrate how number and momentum conservation impact the dynamical conductivity shape, we apply our conductivity model to dynamical gold conductivity measurements [Z. Chen, et al., Nature communications, 12.1, 1638, (2021)]. Finally, comparing our model to the Drude-Smith conductivity model, we conclude that Smith's phenomenological parameter violates local number conservation.","sentences":["A considerable number of semi-empirical and first-principles models have been created to describe the dynamic response of a collisionally damped charged-particle system.","However, known challenges persist for established dynamic structure factors (DSF), dielectric functions, and conductivities.","For instance, the semi-empirical Drude-Smith conductivity [N.M. Smith, Phys.","Rev. B 64, 155106 (2001)] lacks interpretability, and the first-principles Mermin dielectric function [N.D. Mermin, Phys. Rev. B, 1, 2362 (1970)] does not satisfy the frequency sum rule [G.S. Atwal and N.W. Ashcroft, Phys.","Rev. B 65, 115109 (2002)].","In this work, starting from the multicomponent Bhatnagar-Gross-Krook (BGK) kinetic equation, we produce a multi-species susceptibility that conserves number and momentum, which we refer to as the ``completed Mermin'' susceptibility, and we explore its properties and uses.","We show that the completed Mermin susceptibility satisfies the frequency sum (f-sum) rule.","We compute the associated DSF and find that momentum conservation qualitatively impacts the DSF's shape for a carbon-contaminated deuterium and tritium plasma under NIF hot-spot conditions.","In the appendices, we provide numerical implementations of the completed Mermin susceptibility, for the reader's convenience.","Further, we produce a new non-Drude conductivity model, by taking the single-species limit and introducing free parameters in the terms that enforce number and momentum conservation.","To illustrate how number and momentum conservation impact the dynamical conductivity shape, we apply our conductivity model to dynamical gold conductivity measurements [Z. Chen, et al., Nature communications, 12.1, 1638, (2021)].","Finally, comparing our model to the Drude-Smith conductivity model, we conclude that Smith's phenomenological parameter violates local number conservation."],"url":"http://arxiv.org/abs/2405.07871v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 15:57:27","title":"Enhancing Clinically Significant Prostate Cancer Prediction in T2-weighted Images through Transfer Learning from Breast Cancer","abstract":"In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting in over 375,000 deaths. The accurate identification of clinically significant prostate cancer is crucial for delivering effective treatment to patients. Consequently, there has been a surge in research exploring the application of deep neural networks to predict clinical significance based on magnetic resonance images. However, these networks demand extensive datasets to attain optimal performance. Recently, transfer learning emerged as a technique that leverages acquired features from a domain with richer data to enhance the performance of a domain with limited data. In this paper, we investigate the improvement of clinically significant prostate cancer prediction in T2-weighted images through transfer learning from breast cancer. The results demonstrate a remarkable improvement of over 30% in leave-one-out cross-validation accuracy.","sentences":["In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting in over 375,000 deaths.","The accurate identification of clinically significant prostate cancer is crucial for delivering effective treatment to patients.","Consequently, there has been a surge in research exploring the application of deep neural networks to predict clinical significance based on magnetic resonance images.","However, these networks demand extensive datasets to attain optimal performance.","Recently, transfer learning emerged as a technique that leverages acquired features from a domain with richer data to enhance the performance of a domain with limited data.","In this paper, we investigate the improvement of clinically significant prostate cancer prediction in T2-weighted images through transfer learning from breast cancer.","The results demonstrate a remarkable improvement of over 30% in leave-one-out cross-validation accuracy."],"url":"http://arxiv.org/abs/2405.07869v1","category":"eess.IV"}
{"created":"2024-05-13 15:55:57","title":"Phonon Assisted Exciton Processes in Two-Dimensional Tungsten Monocarbide","abstract":"n this study, we utilize a rigorous ab initio-based finite momentum Bethe-Salpeter equation to investigate the photoluminescence emission in two-dimensional hexagonal tungsten carbide (h-WC). This thermodynamically stable monolayer exhibits an indirect optical gap, resulting in phonon-assisted emission. We observe that light absorption is a direct process centered around the direct quasiparticle gap, while light emission is indirect and requires modes between $\\Gamma$-$M$ in the phonon dispersion. The emission lines feature prominent phonon replicas at cryogenic temperatures, particularly near-infrared wavelengths (1.09 and 1.17 eV), and we observe exciton thermalization with the crystal beyond 25 K. Additionally, non-radiative recombination is a remarkably fast process, occurring at order of a few femtoseconds (4.8 fs at 0 K and 2.8 fs at 300 K) compared to radiative recombination (2.3 ps at 0 K and 214 ns at 300 K). These optical characteristics of 2D h-WC may facilitate the promise of photon-emitter devices for near-infrared signal communication.","sentences":["n this study, we utilize a rigorous ab initio-based finite momentum Bethe-Salpeter equation to investigate the photoluminescence emission in two-dimensional hexagonal tungsten carbide (h-WC).","This thermodynamically stable monolayer exhibits an indirect optical gap, resulting in phonon-assisted emission.","We observe that light absorption is a direct process centered around the direct quasiparticle gap, while light emission is indirect and requires modes between $\\Gamma$-$M$ in the phonon dispersion.","The emission lines feature prominent phonon replicas at cryogenic temperatures, particularly near-infrared wavelengths (1.09 and 1.17 eV), and we observe exciton thermalization with the crystal beyond 25 K. Additionally, non-radiative recombination is a remarkably fast process, occurring at order of a few femtoseconds (4.8 fs at 0 K and 2.8 fs at 300 K) compared to radiative recombination (2.3 ps at 0 K and 214 ns at 300 K).","These optical characteristics of 2D h-WC may facilitate the promise of photon-emitter devices for near-infrared signal communication."],"url":"http://arxiv.org/abs/2405.07867v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 15:52:30","title":"Sub-percent Characterization and Polarimetric Performance Analysis of Commercial Micro-polarizer Array Detectors","abstract":"Polarization imaging can yield crucial information in multiple applications of remote sensing, such as characterization of clouds, aerosols, and the Aurora Borealis. Some applications require sub-percent polarimetric sensitivity and accuracy in determining the Stokes parameters, which can be a challenge to attain. In 2018, Sony released a low-cost CMOS-based imaging chip with integrated micro-polarizer array for general polarization measurements. We implement the calibration steps required for these Sony chips to reach sub-percent polarimetric accuracies. To analyze their performances, we have compared the characteristics of four different detector packages by three manufacturers housing either the monochromatic version or the RGB color variant. We present a comprehensive overview of the effects that these characteristics have on the polarimetric performance of the camera. They include dark noise, behavior over different gain settings, detector/pixel artifacts, and polarimetric effects determined by polarizer extinction ratios, polarizer orientations, and accuracy of polarimetric zero points due to differential pixel gains. In addition to calibrations using unpolarized light and fully linearly polarized light, we assess the polarimetric sensitivity within a tilting and rotating glass-plate set-up. We discuss the benefits of adding a rotating half-wave plate as an additional temporal modulator to generically mitigate some of the detector effects, and achieve better polarimetric sensitivity/accuracy albeit at the expense of lower temporal resolution. We conclude by presenting and discussing the polarimetric limits to which we were able to calibrate the detector effects for practical purposes. By reaching a compound absolute polarimetric uncertainty of less than a percent, these very compact, low-cost detectors are enabled for a multitude of scientific goals.","sentences":["Polarization imaging can yield crucial information in multiple applications of remote sensing, such as characterization of clouds, aerosols, and the Aurora Borealis.","Some applications require sub-percent polarimetric sensitivity and accuracy in determining the Stokes parameters, which can be a challenge to attain.","In 2018, Sony released a low-cost CMOS-based imaging chip with integrated micro-polarizer array for general polarization measurements.","We implement the calibration steps required for these Sony chips to reach sub-percent polarimetric accuracies.","To analyze their performances, we have compared the characteristics of four different detector packages by three manufacturers housing either the monochromatic version or the RGB color variant.","We present a comprehensive overview of the effects that these characteristics have on the polarimetric performance of the camera.","They include dark noise, behavior over different gain settings, detector/pixel artifacts, and polarimetric effects determined by polarizer extinction ratios, polarizer orientations, and accuracy of polarimetric zero points due to differential pixel gains.","In addition to calibrations using unpolarized light and fully linearly polarized light, we assess the polarimetric sensitivity within a tilting and rotating glass-plate set-up.","We discuss the benefits of adding a rotating half-wave plate as an additional temporal modulator to generically mitigate some of the detector effects, and achieve better polarimetric sensitivity/accuracy albeit at the expense of lower temporal resolution.","We conclude by presenting and discussing the polarimetric limits to which we were able to calibrate the detector effects for practical purposes.","By reaching a compound absolute polarimetric uncertainty of less than a percent, these very compact, low-cost detectors are enabled for a multitude of scientific goals."],"url":"http://arxiv.org/abs/2405.07864v1","category":"astro-ph.IM"}
{"created":"2024-05-13 15:46:11","title":"Uniform Inference for Subsampled Moment Regression","abstract":"We propose a method for constructing a confidence region for the solution to a conditional moment equation. The method is built around a class of algorithms for nonparametric regression based on subsampled kernels. This class includes random forest regression. We bound the error in the confidence region's nominal coverage probability, under the restriction that the conditional moment equation of interest satisfies a local orthogonality condition. The method is applicable to the construction of confidence regions for conditional average treatment effects in randomized experiments, among many other similar problems encountered in applied economics and causal inference. As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional $U$-statistics.","sentences":["We propose a method for constructing a confidence region for the solution to a conditional moment equation.","The method is built around a class of algorithms for nonparametric regression based on subsampled kernels.","This class includes random forest regression.","We bound the error in the confidence region's nominal coverage probability, under the restriction that the conditional moment equation of interest satisfies a local orthogonality condition.","The method is applicable to the construction of confidence regions for conditional average treatment effects in randomized experiments, among many other similar problems encountered in applied economics and causal inference.","As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional $U$-statistics."],"url":"http://arxiv.org/abs/2405.07860v1","category":"econ.EM"}
{"created":"2024-05-13 15:40:42","title":"Riemannian radial distributions on Riemannian symmetric spaces: Optimal rates of convergence for parameter estimation","abstract":"Manifold data analysis is challenging due to the lack of parametric distributions on manifolds. To address this, we introduce a series of Riemannian radial distributions on Riemannian symmetric spaces. By utilizing the symmetry, we show that for many Riemannian radial distributions, the Riemannian $L^p$ center of mass is uniquely given by the location parameter, and the maximum likelihood estimator (MLE) of this parameter is given by an M-estimator. Therefore, these parametric distributions provide a promising tool for statistical modeling and algorithmic design.   In addition, our paper develops a novel theory for parameter estimation and minimax optimality by integrating statistics, Riemannian geometry, and Lie theory. We demonstrate that the MLE achieves a convergence rate of root-$n$ up to logarithmic terms, where the rate is quantified by both the hellinger distance between distributions and geodesic distance between parameters. Then we derive a root-$n$ minimax lower bound for the parameter estimation rate, demonstrating the optimality of the MLE. Our minimax analysis is limited to the case of simply connected Riemannian symmetric spaces for technical reasons, but is still applicable to numerous applications. Finally, we extend our studies to Riemannian radial distributions with an unknown temperature parameter, and establish the convergence rate of the MLE. We also derive the model complexity of von Mises-Fisher distributions on spheres and discuss the effects of geometry in statistical estimation.","sentences":["Manifold data analysis is challenging due to the lack of parametric distributions on manifolds.","To address this, we introduce a series of Riemannian radial distributions on Riemannian symmetric spaces.","By utilizing the symmetry, we show that for many Riemannian radial distributions, the Riemannian $L^p$ center of mass is uniquely given by the location parameter, and the maximum likelihood estimator (MLE) of this parameter is given by an M-estimator.","Therefore, these parametric distributions provide a promising tool for statistical modeling and algorithmic design.   ","In addition, our paper develops a novel theory for parameter estimation and minimax optimality by integrating statistics, Riemannian geometry, and Lie theory.","We demonstrate that the MLE achieves a convergence rate of root-$n$ up to logarithmic terms, where the rate is quantified by both the hellinger distance between distributions and geodesic distance between parameters.","Then we derive a root-$n$ minimax lower bound for the parameter estimation rate, demonstrating the optimality of the MLE.","Our minimax analysis is limited to the case of simply connected Riemannian symmetric spaces for technical reasons, but is still applicable to numerous applications.","Finally, we extend our studies to Riemannian radial distributions with an unknown temperature parameter, and establish the convergence rate of the MLE.","We also derive the model complexity of von Mises-Fisher distributions on spheres and discuss the effects of geometry in statistical estimation."],"url":"http://arxiv.org/abs/2405.07852v1","category":"math.ST"}
{"created":"2024-05-13 15:36:04","title":"SceneFactory: A Workflow-centric and Unified Framework for Incremental Scene Modeling","abstract":"We present SceneFactory, a workflow-centric and unified framework for incremental scene modeling, that supports conveniently a wide range of applications, such as (unposed and/or uncalibrated) multi-view depth estimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono//Depth-only reconstruction and SLAM. The workflow-centric design uses multiple blocks as the basis for building different production lines. The supported applications, i.e., productions avoid redundancy in their designs. Thus, the focus is on each block itself for independent expansion. To support all input combinations, our implementation consists of four building blocks in SceneFactory: (1) Mono-SLAM, (2) depth estimation, (3) flexion and (4) scene reconstruction. Furthermore, we propose an unposed & uncalibrated multi-view depth estimation model (U2-MVD) to estimate dense geometry. U2-MVD exploits dense bundle adjustment for solving for poses, intrinsics, and inverse depth. Then a semantic-awared ScaleCov step is introduced to complete the multi-view depth. Relying on U2-MVD, SceneFactory both supports user-friendly 3D creation (with just images) and bridges the applications of Dense RGB-D and Dense Mono. For high quality surface and color reconstruction, we propose due-purpose Multi-resolutional Neural Points (DM-NPs) for the first surface accessible Surface Color Field design, where we introduce Improved Point Rasterization (IPR) for point cloud based surface query.   We implement and experiment with SceneFactory to demonstrate its broad practicability and high flexibility. Its quality also competes or exceeds the tightly-coupled state of the art approaches in all tasks. We contribute the code to the community (https://jarrome.github.io/).","sentences":["We present SceneFactory, a workflow-centric and unified framework for incremental scene modeling, that supports conveniently a wide range of applications, such as (unposed and/or uncalibrated) multi-view depth estimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono//Depth-only reconstruction and SLAM.","The workflow-centric design uses multiple blocks as the basis for building different production lines.","The supported applications, i.e., productions avoid redundancy in their designs.","Thus, the focus is on each block itself for independent expansion.","To support all input combinations, our implementation consists of four building blocks in SceneFactory: (1) Mono-SLAM, (2) depth estimation, (3) flexion and (4) scene reconstruction.","Furthermore, we propose an unposed & uncalibrated multi-view depth estimation model (U2-MVD) to estimate dense geometry.","U2-MVD exploits dense bundle adjustment for solving for poses, intrinsics, and inverse depth.","Then a semantic-awared ScaleCov step is introduced to complete the multi-view depth.","Relying on U2-MVD, SceneFactory both supports user-friendly 3D creation (with just images) and bridges the applications of Dense RGB-D and Dense Mono.","For high quality surface and color reconstruction, we propose due-purpose Multi-resolutional Neural Points (DM-NPs) for the first surface accessible Surface Color Field design, where we introduce Improved Point Rasterization (IPR) for point cloud based surface query.   ","We implement and experiment with SceneFactory to demonstrate its broad practicability and high flexibility.","Its quality also competes or exceeds the tightly-coupled state of the art approaches in all tasks.","We contribute the code to the community (https://jarrome.github.io/)."],"url":"http://arxiv.org/abs/2405.07847v1","category":"cs.CV"}
{"created":"2024-05-13 15:34:27","title":"Time Evolution and Thermal Renormalization Group Flow in Cosmology","abstract":"Time-evolution of the Universe as described by the Friedmann equation can be coupled to equations of motion of matter fields. Quantum effects may be incorporated to improve these classical equations of motion by the renormalization group (RG) running of their couplings. Since temporal and thermal evolutions are linked to each other, astrophysical and cosmological treatements based on zero-temperature RG methods require the extension to finite-temperatures. We propose and explore a modification of the usual finite-temperature RG approach by relating the temperature parameter to the running RG scale as $T \\equiv k_T = \\tau k$ (in natural units), where $k_T$ is acting as a running cutoff for thermal fluctuations and the momentum $k$ can be used for the quantum fluctuations. In this approach, the temperature of the expanding Universe is related to the dimensionless quantity $\\tau$ (and not to $k_T$). We show that by this choice dimensionless RG flow equations have no explicit $k$-dependence, as it is convenient. We also discuss how this modified thermal RG is used to handle high-energy divergences of the RG running of the cosmological constant and to \"solve the triviality\" of the $\\phi^4$ model by a thermal phase transition in terms of $\\tau$ in $d=4$ Euclidean dimensions.","sentences":["Time-evolution of the Universe as described by the Friedmann equation can be coupled to equations of motion of matter fields.","Quantum effects may be incorporated to improve these classical equations of motion by the renormalization group (RG) running of their couplings.","Since temporal and thermal evolutions are linked to each other, astrophysical and cosmological treatements based on zero-temperature RG methods require the extension to finite-temperatures.","We propose and explore a modification of the usual finite-temperature RG approach by relating the temperature parameter to the running RG scale as $T \\equiv k_T = \\tau k$ (in natural units), where $k_T$ is acting as a running cutoff for thermal fluctuations and the momentum $k$ can be used for the quantum fluctuations.","In this approach, the temperature of the expanding Universe is related to the dimensionless quantity $\\tau$ (and not to $k_T$).","We show that by this choice dimensionless RG flow equations have no explicit $k$-dependence, as it is convenient.","We also discuss how this modified thermal RG is used to handle high-energy divergences of the RG running of the cosmological constant and to \"solve the triviality\" of the $\\phi^4$ model by a thermal phase transition in terms of $\\tau$ in $d=4$ Euclidean dimensions."],"url":"http://arxiv.org/abs/2405.07846v1","category":"hep-th"}
{"created":"2024-05-13 15:25:11","title":"Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM","abstract":"Decoding language information from brain signals represents a vital research area within brain-computer interfaces, particularly in the context of deciphering the semantic information from the fMRI signal. However, many existing efforts concentrate on decoding small vocabulary sets, leaving space for the exploration of open vocabulary continuous text decoding. In this paper, we introduce a novel method, the \\textbf{Brain Prompt GPT (BP-GPT)}. By using the brain representation that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode fMRI signals into stimulus text. Further, we introduce a text-to-text baseline and align the fMRI prompt to the text prompt. By introducing the text-to-text baseline, our BP-GPT can extract a more robust brain prompt and promote the decoding of pre-trained LLM. We evaluate our BP-GPT on the open-source auditory semantic decoding dataset and achieve a significant improvement up to $4.61\\%$ on METEOR and $2.43\\%$ on BERTScore across all the subjects compared to the state-of-the-art method. The experimental results demonstrate that using brain representation as a prompt to further drive LLM for auditory neural decoding is feasible and effective.","sentences":["Decoding language information from brain signals represents a vital research area within brain-computer interfaces, particularly in the context of deciphering the semantic information from the fMRI signal.","However, many existing efforts concentrate on decoding small vocabulary sets, leaving space for the exploration of open vocabulary continuous text decoding.","In this paper, we introduce a novel method, the \\textbf{Brain Prompt GPT (BP-GPT)}.","By using the brain representation that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode fMRI signals into stimulus text.","Further, we introduce a text-to-text baseline and align the fMRI prompt to the text prompt.","By introducing the text-to-text baseline, our BP-GPT can extract a more robust brain prompt and promote the decoding of pre-trained LLM.","We evaluate our BP-GPT on the open-source auditory semantic decoding dataset and achieve a significant improvement up to $4.61\\%$ on METEOR and $2.43\\%$ on BERTScore across all the subjects compared to the state-of-the-art method.","The experimental results demonstrate that using brain representation as a prompt to further drive LLM for auditory neural decoding is feasible and effective."],"url":"http://arxiv.org/abs/2405.07840v1","category":"cs.HC"}
{"created":"2024-05-13 15:24:11","title":"Why Decussate? Topological Constraints on 3D Wiring","abstract":"Many vertebrate motor and sensory systems decussate, or cross the midline to the opposite side of the body. The successful crossing of millions of axons during development requires a complex of tightly controlled regulatory processes. Because these processes have evolved in many distinct systems and organisms, it seems reasonable to presume that decussation confers a significant functional advantage. Yet if this is so, the nature of this advantage is not understood. In this article, we examine constraints imposed by topology on the ways that a three-dimensional processor and environment can be wired together in a continuous, somatotopic, way. We show that as the number of wiring connections grows, decussated arrangements become overwhelmingly more robust against wiring errors than seemingly simpler same-sided wiring schemes. These results provide a predictive approach for understanding how 3D networks must be wired if they are to be robust, and therefore have implications both for future large-scale computational networks and for complex bio-medical devices","sentences":["Many vertebrate motor and sensory systems decussate, or cross the midline to the opposite side of the body.","The successful crossing of millions of axons during development requires a complex of tightly controlled regulatory processes.","Because these processes have evolved in many distinct systems and organisms, it seems reasonable to presume that decussation confers a significant functional advantage.","Yet if this is so, the nature of this advantage is not understood.","In this article, we examine constraints imposed by topology on the ways that a three-dimensional processor and environment can be wired together in a continuous, somatotopic, way.","We show that as the number of wiring connections grows, decussated arrangements become overwhelmingly more robust against wiring errors than seemingly simpler same-sided wiring schemes.","These results provide a predictive approach for understanding how 3D networks must be wired if they are to be robust, and therefore have implications both for future large-scale computational networks and for complex bio-medical devices"],"url":"http://arxiv.org/abs/2405.07837v1","category":"q-bio.NC"}
{"created":"2024-05-13 15:09:45","title":"Stability of the Cosmological Dynamics of $O(D,D)$-complete Stringy Gravity","abstract":"The massless fields in the universal NS-NS sector of string theory form $O(D, D)$ multiplets of Double Field Theory, which is a theory that provides a T-duality covariant formulation of supergravity, leading to a stringy modification of General Relativity. In this framework, it is possible to write down the extensions of the Einstein field equations and the Friedmann equations in such a way that the coupling of gravitational and matter sectors is dictated by the $O(D, D)$ symmetry universally. In this paper, we obtain the autonomous form of the $O(D, D)$-complete Friedmann equations, find the critical points and perform their stability analysis. We also include the phase portraits of the system. Cosmologically interesting cases of scalar field, radiation, and matter are separately considered and compared with the Chameleon models in a similar setting. Accelerating phases and the conditions for their existence are also given for such cases.","sentences":["The massless fields in the universal NS-NS sector of string theory form $O(D, D)$ multiplets of Double Field Theory, which is a theory that provides a T-duality covariant formulation of supergravity, leading to a stringy modification of General Relativity.","In this framework, it is possible to write down the extensions of the Einstein field equations and the Friedmann equations in such a way that the coupling of gravitational and matter sectors is dictated by the $O(D, D)$ symmetry universally.","In this paper, we obtain the autonomous form of the $O(D, D)$-complete Friedmann equations, find the critical points and perform their stability analysis.","We also include the phase portraits of the system.","Cosmologically interesting cases of scalar field, radiation, and matter are separately considered and compared with the Chameleon models in a similar setting.","Accelerating phases and the conditions for their existence are also given for such cases."],"url":"http://arxiv.org/abs/2405.07825v1","category":"gr-qc"}
{"created":"2024-05-13 15:01:18","title":"Local Adjoints for Simultaneous Preaccumulations with Shared Inputs","abstract":"In shared-memory parallel automatic differentiation, shared inputs among simultaneous thread-local preaccumulations lead to data races if Jacobians are accumulated with a single, shared vector of adjoint variables. In this work, we discuss the benefits and tradeoffs of re-enabling such preaccumulations by a transition to suitable local adjoint variables. In particular, we assess the performance of mapped local adjoints in discrete adjoint computations in the multiphysics simulation suite SU2.","sentences":["In shared-memory parallel automatic differentiation, shared inputs among simultaneous thread-local preaccumulations lead to data races if Jacobians are accumulated with a single, shared vector of adjoint variables.","In this work, we discuss the benefits and tradeoffs of re-enabling such preaccumulations by a transition to suitable local adjoint variables.","In particular, we assess the performance of mapped local adjoints in discrete adjoint computations in the multiphysics simulation suite SU2."],"url":"http://arxiv.org/abs/2405.07819v1","category":"cs.MS"}
{"created":"2024-05-13 14:52:48","title":"On the quadratic stability of asymmetric Hermite basis and application to plasma physics","abstract":"We analyze why the discretization of linear transport with asymmetric Hermite basis functions can be instable in quadratic norm. The main reason is that the finite truncation of the infinite moment linear system looses the skew-symmetry property with respect to the Gram matrix. Then we propose an original closed formula for the scalar product of any pair of asymmetric basis functions. It makes possible the construction of two simple modifications of the linear systems which recover the skew-symmetry property. By construction the new methods are quadratically stable with respect to the natural $L^2$ norm. We explain how to generalize to other transport equations encountered in numerical plasma physics. Basic numerical tests illustrate the unconditional stability properties of our algorithms.","sentences":["We analyze why the discretization of linear transport with asymmetric Hermite basis functions can be instable in quadratic norm.","The main reason is that the finite truncation of the infinite moment linear system looses the skew-symmetry property with respect to the Gram matrix.","Then we propose an original closed formula for the scalar product of any pair of asymmetric basis functions.","It makes possible the construction of two simple modifications of the linear systems which recover the skew-symmetry property.","By construction the new methods are quadratically stable with respect to the natural $L^2$ norm.","We explain how to generalize to other transport equations encountered in numerical plasma physics.","Basic numerical tests illustrate the unconditional stability properties of our algorithms."],"url":"http://arxiv.org/abs/2405.07811v1","category":"math.NA"}
{"created":"2024-05-13 14:44:43","title":"Oscillations in the elastic high energy amplitude","abstract":"We discuss the oscillations in the elastic $pp$ differential cross section seen in the TOTEM data at $\\sqrt{s}=13$~TeV on the top of the usual smooth behaviour.","sentences":["We discuss the oscillations in the elastic $pp$ differential cross section seen in the TOTEM data at $\\sqrt{s}=13$~TeV on the top of the usual smooth behaviour."],"url":"http://arxiv.org/abs/2405.07802v1","category":"hep-ph"}
{"created":"2024-05-13 14:41:28","title":"Improved Bound for Robust Causal Bandits with Linear Models","abstract":"This paper investigates the robustness of causal bandits (CBs) in the face of temporal model fluctuations. This setting deviates from the existing literature's widely-adopted assumption of constant causal models. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown and subject to variations over time. The goal is to design a sequence of interventions that incur the smallest cumulative regret compared to an oracle aware of the entire causal model and its fluctuations. A robust CB algorithm is proposed, and its cumulative regret is analyzed by establishing both upper and lower bounds on the regret. It is shown that in a graph with maximum in-degree $d$, length of the largest causal path $L$, and an aggregate model deviation $C$, the regret is upper bounded by $\\tilde{\\mathcal{O}}(d^{L-\\frac{1}{2}}(\\sqrt{T} + C))$ and lower bounded by $\\Omega(d^{\\frac{L}{2}-2}\\max\\{\\sqrt{T}\\; ,\\; d^2C\\})$. The proposed algorithm achieves nearly optimal $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret when $C$ is $o(\\sqrt{T})$, maintaining sub-linear regret for a broad range of $C$.","sentences":["This paper investigates the robustness of causal bandits (CBs) in the face of temporal model fluctuations.","This setting deviates from the existing literature's widely-adopted assumption of constant causal models.","The focus is on causal systems with linear structural equation models (SEMs).","The SEMs and the time-varying pre- and post-interventional statistical models are all unknown and subject to variations over time.","The goal is to design a sequence of interventions that incur the smallest cumulative regret compared to an oracle aware of the entire causal model and its fluctuations.","A robust CB algorithm is proposed, and its cumulative regret is analyzed by establishing both upper and lower bounds on the regret.","It is shown that in a graph with maximum in-degree $d$, length of the largest causal path $L$, and an aggregate model deviation $C$, the regret is upper bounded by $\\tilde{\\mathcal{O}}(d^{L-\\frac{1}{2}}(\\sqrt{T} + C))$ and lower bounded by $\\Omega(d^{\\frac{L}{2}-2}\\max\\{\\sqrt{T}\\; ,\\; d^2C\\})$.","The proposed algorithm achieves nearly optimal $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret when $C$ is $o(\\sqrt{T})$, maintaining sub-linear regret for a broad range of $C$."],"url":"http://arxiv.org/abs/2405.07795v1","category":"stat.ML"}
{"created":"2024-05-13 14:36:22","title":"Hamiltonian-based Quantum Reinforcement Learning for Neural Combinatorial Optimization","abstract":"Advancements in Quantum Computing (QC) and Neural Combinatorial Optimization (NCO) represent promising steps in tackling complex computational challenges. On the one hand, Variational Quantum Algorithms such as QAOA can be used to solve a wide range of combinatorial optimization problems. On the other hand, the same class of problems can be solved by NCO, a method that has shown promising results, particularly since the introduction of Graph Neural Networks. Given recent advances in both research areas, we introduce Hamiltonian-based Quantum Reinforcement Learning (QRL), an approach at the intersection of QC and NCO. We model our ansatzes directly on the combinatorial optimization problem's Hamiltonian formulation, which allows us to apply our approach to a broad class of problems. Our ansatzes show favourable trainability properties when compared to the hardware efficient ansatzes, while also not being limited to graph-based problems, unlike previous works. In this work, we evaluate the performance of Hamiltonian-based QRL on a diverse set of combinatorial optimization problems to demonstrate the broad applicability of our approach and compare it to QAOA.","sentences":["Advancements in Quantum Computing (QC) and Neural Combinatorial Optimization (NCO) represent promising steps in tackling complex computational challenges.","On the one hand, Variational Quantum Algorithms such as QAOA can be used to solve a wide range of combinatorial optimization problems.","On the other hand, the same class of problems can be solved by NCO, a method that has shown promising results, particularly since the introduction of Graph Neural Networks.","Given recent advances in both research areas, we introduce Hamiltonian-based Quantum Reinforcement Learning (QRL), an approach at the intersection of QC and NCO.","We model our ansatzes directly on the combinatorial optimization problem's Hamiltonian formulation, which allows us to apply our approach to a broad class of problems.","Our ansatzes show favourable trainability properties when compared to the hardware efficient ansatzes, while also not being limited to graph-based problems, unlike previous works.","In this work, we evaluate the performance of Hamiltonian-based QRL on a diverse set of combinatorial optimization problems to demonstrate the broad applicability of our approach and compare it to QAOA."],"url":"http://arxiv.org/abs/2405.07790v1","category":"quant-ph"}
{"created":"2024-05-13 14:26:29","title":"Is Interpretable Machine Learning Effective at Feature Selection for Neural Learning-to-Rank?","abstract":"Neural ranking models have become increasingly popular for real-world search and recommendation systems in recent years. Unlike their tree-based counterparts, neural models are much less interpretable. That is, it is very difficult to understand their inner workings and answer questions like how do they make their ranking decisions? or what document features do they find important? This is particularly disadvantageous since interpretability is highly important for real-world systems. In this work, we explore feature selection for neural learning-to-rank (LTR). In particular, we investigate six widely-used methods from the field of interpretable machine learning (ML) and introduce our own modification, to select the input features that are most important to the ranking behavior. To understand whether these methods are useful for practitioners, we further study whether they contribute to efficiency enhancement. Our experimental results reveal a large feature redundancy in several LTR benchmarks: the local selection method TabNet can achieve optimal ranking performance with less than 10 features; the global methods, particularly our G-L2X, require slightly more selected features, but exhibit higher potential in improving efficiency. We hope that our analysis of these feature selection methods will bring the fields of interpretable ML and LTR closer together.","sentences":["Neural ranking models have become increasingly popular for real-world search and recommendation systems in recent years.","Unlike their tree-based counterparts, neural models are much less interpretable.","That is, it is very difficult to understand their inner workings and answer questions like how do they make their ranking decisions?","or what document features do they find important?","This is particularly disadvantageous since interpretability is highly important for real-world systems.","In this work, we explore feature selection for neural learning-to-rank (LTR).","In particular, we investigate six widely-used methods from the field of interpretable machine learning (ML) and introduce our own modification, to select the input features that are most important to the ranking behavior.","To understand whether these methods are useful for practitioners, we further study whether they contribute to efficiency enhancement.","Our experimental results reveal a large feature redundancy in several LTR benchmarks: the local selection method TabNet can achieve optimal ranking performance with less than 10 features; the global methods, particularly our G-L2X, require slightly more selected features, but exhibit higher potential in improving efficiency.","We hope that our analysis of these feature selection methods will bring the fields of interpretable ML and LTR closer together."],"url":"http://arxiv.org/abs/2405.07782v1","category":"cs.IR"}
{"created":"2024-05-13 14:17:35","title":"Characterizing virulence differences in a parasitoid wasp through comparative transcriptomic and proteomic","abstract":"Background: Two strains of the endoparasitoid Cotesia typhae present a differential parasitism success on the host, Sesamia nonagrioides. One is virulent on both permissive and resistant host populations, and the other only on the permissive host. This interaction provides a very interesting frame for studying virulence factors. Here, we used a combination of comparative transcriptomic and proteomic analyses to unravel the molecular basis underlying virulence differences between the strains.Results: First, we report that virulence genes are mostly expressed during the nymphal stage of the parasitoid. Especially, proviral genes are broadly up-regulated at this stage, while their expression is only expected in the host. Parasitoid gene expression in the host increases with time, indicating the production of more virulence factors. Secondly, comparison between strains reveals differences in venom composition, with 12 proteins showing differential abundance. Proviral expression in the host displays a strong temporal variability, along with differential patterns between strains. Notably, a subset of proviral genes including protein-tyrosine phosphatases is specifically over-expressed in the resistant host parasitized by the less virulent strain, 24 hours after parasitism. This result particularly hints at host modulation of proviral expression.Conclusions: This study sheds light on the temporal expression of virulence factors of Cotesia typhae, both in the host and in the parasitoid. It also identifies potential molecular candidates driving differences in parasitism success between two strains. Together, those findings provide a path for further exploration of virulence mechanisms in parasitoid wasps, and offer insights into host-parasitoid coevolution.","sentences":["Background: Two strains of the endoparasitoid Cotesia typhae present a differential parasitism success on the host, Sesamia nonagrioides.","One is virulent on both permissive and resistant host populations, and the other only on the permissive host.","This interaction provides a very interesting frame for studying virulence factors.","Here, we used a combination of comparative transcriptomic and proteomic analyses to unravel the molecular basis underlying virulence differences between the strains.","Results:","First, we report that virulence genes are mostly expressed during the nymphal stage of the parasitoid.","Especially, proviral genes are broadly up-regulated at this stage, while their expression is only expected in the host.","Parasitoid gene expression in the host increases with time, indicating the production of more virulence factors.","Secondly, comparison between strains reveals differences in venom composition, with 12 proteins showing differential abundance.","Proviral expression in the host displays a strong temporal variability, along with differential patterns between strains.","Notably, a subset of proviral genes including protein-tyrosine phosphatases is specifically over-expressed in the resistant host parasitized by the less virulent strain, 24 hours after parasitism.","This result particularly hints at host modulation of proviral expression.","Conclusions: This study sheds light on the temporal expression of virulence factors of Cotesia typhae, both in the host and in the parasitoid.","It also identifies potential molecular candidates driving differences in parasitism success between two strains.","Together, those findings provide a path for further exploration of virulence mechanisms in parasitoid wasps, and offer insights into host-parasitoid coevolution."],"url":"http://arxiv.org/abs/2405.07772v1","category":"q-bio.GN"}
{"created":"2024-05-13 13:46:02","title":"Neural Network Compression for Reinforcement Learning Tasks","abstract":"In real applications of Reinforcement Learning (RL), such as robotics, low latency and energy efficient inference is very desired. The use of sparsity and pruning for optimizing Neural Network inference, and particularly to improve energy and latency efficiency, is a standard technique. In this work, we perform a systematic investigation of applying these optimization techniques for different RL algorithms in different RL environments, yielding up to a 400-fold reduction in the size of neural networks.","sentences":["In real applications of Reinforcement Learning (RL), such as robotics, low latency and energy efficient inference is very desired.","The use of sparsity and pruning for optimizing Neural Network inference, and particularly to improve energy and latency efficiency, is a standard technique.","In this work, we perform a systematic investigation of applying these optimization techniques for different RL algorithms in different RL environments, yielding up to a 400-fold reduction in the size of neural networks."],"url":"http://arxiv.org/abs/2405.07748v1","category":"cs.LG"}
{"created":"2024-05-13 13:43:45","title":"Combination rule for Hammett $\u03c3$ constants in computational catalyst discovery","abstract":"The Hammett equation is popular for navigating chemical space by quantifying the effects of substituents on chemical properties and behavior. We study the applicability of the Hammett-inspired product (HIP) Ansatz to model relative substrate binding within homogenous organometallic catalysis, assigning $\\sigma$ and $\\rho$ to ligands and metals, respectively. Implementing an additive combination (c) rule for obtaining $\\sigma$ constants for any ligand pair combination results in a cHIP model that can be leveraged (i) as a baseline for $\\Delta$-machine learning (ML), and (ii) to identify novel catalyst candidates via volcano plots. After testing the combination rule on Hammett constants previously published in the literature, we have generated numerical evidence for the Suzuki-Miyaura (SM) C-C cross-coupling reaction using two synthetic datasets of metallic catalysts (including (10) and (11)-metals Ni, Pd, Pt, and Cu, Ag, Au as well as 96 ligands such as N-heterocyclic carbenes, phosphines, or pyridines). When used as a baseline, $\\Delta$-ML prediction errors of relative binding decrease systematically with training set size and reach chemical accuracy ($\\sim$1 kcal/mol) for 20k training instances. Employing the individual ligand constants obtained from cHIP, we report relative substrate binding for a novel dataset consisting of 720 catalysts (not part of training data), of which 145 fall into the most promising range on the volcano plot accounting for oxidative addition, transmetalation, and reductive elimination steps. Multiple Ni-based catalysts, e.g. Aphos-Ni-P($t$-Bu)$_3$, are included among these promising candidates, potentially offering dramatic cost savings in experimental applications.","sentences":["The Hammett equation is popular for navigating chemical space by quantifying the effects of substituents on chemical properties and behavior.","We study the applicability of the Hammett-inspired product (HIP) Ansatz to model relative substrate binding within homogenous organometallic catalysis, assigning $\\sigma$ and $\\rho$ to ligands and metals, respectively.","Implementing an additive combination (c) rule for obtaining $\\sigma$ constants for any ligand pair combination results in a cHIP model that can be leveraged (i) as a baseline for $\\Delta$-machine learning (ML), and (ii) to identify novel catalyst candidates via volcano plots.","After testing the combination rule on Hammett constants previously published in the literature, we have generated numerical evidence for the Suzuki-Miyaura (SM) C-C cross-coupling reaction using two synthetic datasets of metallic catalysts (including (10) and (11)-metals Ni, Pd, Pt, and Cu, Ag, Au as well as 96 ligands such as N-heterocyclic carbenes, phosphines, or pyridines).","When used as a baseline, $\\Delta$-ML prediction errors of relative binding decrease systematically with training set size and reach chemical accuracy ($\\sim$1 kcal/mol) for 20k training instances.","Employing the individual ligand constants obtained from cHIP, we report relative substrate binding for a novel dataset consisting of 720 catalysts (not part of training data), of which 145 fall into the most promising range on the volcano plot accounting for oxidative addition, transmetalation, and reductive elimination steps.","Multiple Ni-based catalysts, e.g. Aphos-Ni-P($t$-Bu)$_3$, are included among these promising candidates, potentially offering dramatic cost savings in experimental applications."],"url":"http://arxiv.org/abs/2405.07747v1","category":"physics.chem-ph"}
{"created":"2024-05-13 13:32:41","title":"Learning to Plan Maneuverable and Agile Flight Trajectory with Optimization Embedded Networks","abstract":"In recent times, an increasing number of researchers have been devoted to utilizing deep neural networks for end-to-end flight navigation. This approach has gained traction due to its ability to bridge the gap between perception and planning that exists in traditional methods, thereby eliminating delays between modules. However, the practice of replacing original modules with neural networks in a black-box manner diminishes the overall system's robustness and stability. It lacks principled explanations and often fails to consistently generate high-quality motion trajectories. Furthermore, such methods often struggle to rigorously account for the robot's kinematic constraints, resulting in the generation of trajectories that cannot be executed satisfactorily. In this work, we combine the advantages of traditional methods and neural networks by proposing an optimization-embedded neural network. This network can learn high-quality trajectories directly from visual inputs without the need of mapping, while ensuring dynamic feasibility. Here, the deep neural network is employed to directly extract environment safety regions from depth images. Subsequently, we employ a model-based approach to represent these regions as safety constraints in trajectory optimization. Leveraging the availability of highly efficient optimization algorithms, our method robustly converges to feasible and optimal solutions that satisfy various user-defined constraints. Moreover, we differentiate the optimization process, allowing it to be trained as a layer within the neural network. This approach facilitates the direct interaction between perception and planning, enabling the network to focus more on the spatial regions where optimal solutions exist. As a result, it further enhances the quality and stability of the generated trajectories.","sentences":["In recent times, an increasing number of researchers have been devoted to utilizing deep neural networks for end-to-end flight navigation.","This approach has gained traction due to its ability to bridge the gap between perception and planning that exists in traditional methods, thereby eliminating delays between modules.","However, the practice of replacing original modules with neural networks in a black-box manner diminishes the overall system's robustness and stability.","It lacks principled explanations and often fails to consistently generate high-quality motion trajectories.","Furthermore, such methods often struggle to rigorously account for the robot's kinematic constraints, resulting in the generation of trajectories that cannot be executed satisfactorily.","In this work, we combine the advantages of traditional methods and neural networks by proposing an optimization-embedded neural network.","This network can learn high-quality trajectories directly from visual inputs without the need of mapping, while ensuring dynamic feasibility.","Here, the deep neural network is employed to directly extract environment safety regions from depth images.","Subsequently, we employ a model-based approach to represent these regions as safety constraints in trajectory optimization.","Leveraging the availability of highly efficient optimization algorithms, our method robustly converges to feasible and optimal solutions that satisfy various user-defined constraints.","Moreover, we differentiate the optimization process, allowing it to be trained as a layer within the neural network.","This approach facilitates the direct interaction between perception and planning, enabling the network to focus more on the spatial regions where optimal solutions exist.","As a result, it further enhances the quality and stability of the generated trajectories."],"url":"http://arxiv.org/abs/2405.07736v1","category":"cs.RO"}
{"created":"2024-05-13 13:22:13","title":"Validated error bounds for pseudospectral approximation of delay differential equations: unstable manifolds","abstract":"Pseudospectral approximation provides a means to approximate the dynamics of delay differential equations (DDE) by ordinary differential equations (ODE). This article develops a computer-aided algorithm to determine the distance between the unstable manifold of a DDE and the unstable manifold of the approximating pseudospectral ODE. The algorithm is based upon the parametrization method. While a-priori the parametrization method for a vector-valued ODE involves computing a sequence of vector-valued Taylor coefficients, we show that for the pseudospectral ODE, due to its specific structure, the problem reduces to finding a sequence of scalars, which significantly simplifies the problem.","sentences":["Pseudospectral approximation provides a means to approximate the dynamics of delay differential equations (DDE) by ordinary differential equations (ODE).","This article develops a computer-aided algorithm to determine the distance between the unstable manifold of a DDE and the unstable manifold of the approximating pseudospectral ODE.","The algorithm is based upon the parametrization method.","While a-priori the parametrization method for a vector-valued ODE involves computing a sequence of vector-valued Taylor coefficients, we show that for the pseudospectral ODE, due to its specific structure, the problem reduces to finding a sequence of scalars, which significantly simplifies the problem."],"url":"http://arxiv.org/abs/2405.07727v1","category":"math.DS"}
{"created":"2024-05-13 12:25:56","title":"Holography of Higher Codimension Submanifolds: Riemannian and Conformal","abstract":"We provide a natural generalization to submanifolds of the holographic method used to extract higher-order local invariants of both Riemannian and conformal embeddings, some of which depend on a choice of parallelization of the normal bundle. Qualitatively new behavior is observed in the higher-codimension case, giving rise to new invariants that obstruct the order-by-order construction of unit defining maps. In the conformal setting, a novel invariant (that vanishes in codimension 1) is realized as the leading transverse-order term appearing in a holographically-constructed Willmore invariant. Using these same tools, we also investigate the formal solutions to extension problems off of an embedded submanifold.","sentences":["We provide a natural generalization to submanifolds of the holographic method used to extract higher-order local invariants of both Riemannian and conformal embeddings, some of which depend on a choice of parallelization of the normal bundle.","Qualitatively new behavior is observed in the higher-codimension case, giving rise to new invariants that obstruct the order-by-order construction of unit defining maps.","In the conformal setting, a novel invariant (that vanishes in codimension 1) is realized as the leading transverse-order term appearing in a holographically-constructed Willmore invariant.","Using these same tools, we also investigate the formal solutions to extension problems off of an embedded submanifold."],"url":"http://arxiv.org/abs/2405.07692v1","category":"math.DG"}
{"created":"2024-05-13 12:06:49","title":"Unveiling low-dimensional patterns induced by convex non-differentiable regularizers","abstract":"Popular regularizers with non-differentiable penalties, such as Lasso, Elastic Net, Generalized Lasso, or SLOPE, reduce the dimension of the parameter space by inducing sparsity or clustering in the estimators' coordinates. In this paper, we focus on linear regression and explore the asymptotic distributions of the resulting low-dimensional patterns when the number of regressors $p$ is fixed, the number of observations $n$ goes to infinity, and the penalty function increases at the rate of $\\sqrt{n}$. While the asymptotic distribution of the rescaled estimation error can be derived by relatively standard arguments, the convergence of the pattern does not simply follow from the convergence in distribution, and requires a careful and separate treatment. For this purpose, we use the Hausdorff distance as a suitable mode of convergence for subdifferentials, resulting in the desired pattern convergence. Furthermore, we derive the exact limiting probability of recovering the true model pattern. This probability goes to 1 if and only if the penalty scaling constant diverges to infinity and the regularizer-specific asymptotic irrepresentability condition is satisfied. We then propose simple two-step procedures that asymptotically recover the model patterns, irrespective whether the irrepresentability condition holds.   Interestingly, our theory shows that Fused Lasso cannot reliably recover its own clustering pattern, even for independent regressors. It also demonstrates how this problem can be resolved by ``concavifying'' the Fused Lasso penalty coefficients. Additionally, sampling from the asymptotic error distribution facilitates comparisons between different regularizers. We provide short simulation studies showcasing an illustrative comparison between the asymptotic properties of Lasso, Fused Lasso, and SLOPE.","sentences":["Popular regularizers with non-differentiable penalties, such as Lasso, Elastic Net, Generalized Lasso, or SLOPE, reduce the dimension of the parameter space by inducing sparsity or clustering in the estimators' coordinates.","In this paper, we focus on linear regression and explore the asymptotic distributions of the resulting low-dimensional patterns when the number of regressors $p$ is fixed, the number of observations $n$ goes to infinity, and the penalty function increases at the rate of $\\sqrt{n}$. While the asymptotic distribution of the rescaled estimation error can be derived by relatively standard arguments, the convergence of the pattern does not simply follow from the convergence in distribution, and requires a careful and separate treatment.","For this purpose, we use the Hausdorff distance as a suitable mode of convergence for subdifferentials, resulting in the desired pattern convergence.","Furthermore, we derive the exact limiting probability of recovering the true model pattern.","This probability goes to 1 if and only if the penalty scaling constant diverges to infinity and the regularizer-specific asymptotic irrepresentability condition is satisfied.","We then propose simple two-step procedures that asymptotically recover the model patterns, irrespective whether the irrepresentability condition holds.   ","Interestingly, our theory shows that Fused Lasso cannot reliably recover its own clustering pattern, even for independent regressors.","It also demonstrates how this problem can be resolved by ``concavifying'' the Fused Lasso penalty coefficients.","Additionally, sampling from the asymptotic error distribution facilitates comparisons between different regularizers.","We provide short simulation studies showcasing an illustrative comparison between the asymptotic properties of Lasso, Fused Lasso, and SLOPE."],"url":"http://arxiv.org/abs/2405.07677v1","category":"math.ST"}
{"created":"2024-05-13 12:01:54","title":"An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation","abstract":"Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages. In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise. To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation. This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes. We conduct both automatic and human evaluations. Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators. The dataset is publicly available at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval.","sentences":["Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages.","In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise.","To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation.","This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes.","We conduct both automatic and human evaluations.","Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators.","The dataset is publicly available at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval."],"url":"http://arxiv.org/abs/2405.07673v1","category":"cs.CL"}
{"created":"2024-05-13 11:59:20","title":"Impact of white Gaussian internal noise on analog echo-state neural networks","abstract":"In recent years, more and more works have appeared devoted to the analog (hardware) implementation of artificial neural networks, in which neurons and the connection between them are based not on computer calculations, but on physical principles. Such networks offer improved energy efficiency and, in some cases, scalability, but may be susceptible to internal noise. This paper studies the influence of noise on the functioning of recurrent networks using the example of trained echo state networks (ESNs). The most common reservoir connection matrices were chosen as various topologies of ESNs: random uniform and band matrices with different connectivity. White Gaussian noise was chosen as the influence, and according to the way of its introducing it was additive or multiplicative, as well as correlated or uncorrelated. In the paper, we show that the propagation of noise in reservoir is mainly controlled by the statistical properties of the output connection matrix, namely the mean and the mean square. Depending on these values, more correlated or uncorrelated noise accumulates in the network. We also show that there are conditions under which even noise with an intensity of $10^{-20}$ is already enough to completely lose the useful signal. In the article we show which types of noise are most critical for networks with different activation functions (hyperbolic tangent, sigmoid and linear) and if the network is self-closed.","sentences":["In recent years, more and more works have appeared devoted to the analog (hardware) implementation of artificial neural networks, in which neurons and the connection between them are based not on computer calculations, but on physical principles.","Such networks offer improved energy efficiency and, in some cases, scalability, but may be susceptible to internal noise.","This paper studies the influence of noise on the functioning of recurrent networks using the example of trained echo state networks (ESNs).","The most common reservoir connection matrices were chosen as various topologies of ESNs: random uniform and band matrices with different connectivity.","White Gaussian noise was chosen as the influence, and according to the way of its introducing it was additive or multiplicative, as well as correlated or uncorrelated.","In the paper, we show that the propagation of noise in reservoir is mainly controlled by the statistical properties of the output connection matrix, namely the mean and the mean square.","Depending on these values, more correlated or uncorrelated noise accumulates in the network.","We also show that there are conditions under which even noise with an intensity of $10^{-20}$ is already enough to completely lose the useful signal.","In the article we show which types of noise are most critical for networks with different activation functions (hyperbolic tangent, sigmoid and linear) and if the network is self-closed."],"url":"http://arxiv.org/abs/2405.07670v1","category":"cs.NE"}
{"created":"2024-05-13 17:48:22","title":"SignAvatar: Sign Language 3D Motion Reconstruction and Generation","abstract":"Achieving expressive 3D motion reconstruction and automatic generation for isolated sign words can be challenging, due to the lack of real-world 3D sign-word data, the complex nuances of signing motions, and the cross-modal understanding of sign language semantics. To address these challenges, we introduce SignAvatar, a framework capable of both word-level sign language reconstruction and generation. SignAvatar employs a transformer-based conditional variational autoencoder architecture, effectively establishing relationships across different semantic modalities. Additionally, this approach incorporates a curriculum learning strategy to enhance the model's robustness and generalization, resulting in more realistic motions. Furthermore, we contribute the ASL3DWord dataset, composed of 3D joint rotation data for the body, hands, and face, for unique sign words. We demonstrate the effectiveness of SignAvatar through extensive experiments, showcasing its superior reconstruction and automatic generation capabilities. The code and dataset are available on the project page.","sentences":["Achieving expressive 3D motion reconstruction and automatic generation for isolated sign words can be challenging, due to the lack of real-world 3D sign-word data, the complex nuances of signing motions, and the cross-modal understanding of sign language semantics.","To address these challenges, we introduce SignAvatar, a framework capable of both word-level sign language reconstruction and generation.","SignAvatar employs a transformer-based conditional variational autoencoder architecture, effectively establishing relationships across different semantic modalities.","Additionally, this approach incorporates a curriculum learning strategy to enhance the model's robustness and generalization, resulting in more realistic motions.","Furthermore, we contribute the ASL3DWord dataset, composed of 3D joint rotation data for the body, hands, and face, for unique sign words.","We demonstrate the effectiveness of SignAvatar through extensive experiments, showcasing its superior reconstruction and automatic generation capabilities.","The code and dataset are available on the project page."],"url":"http://arxiv.org/abs/2405.07974v1","category":"cs.CV"}
{"created":"2024-05-13 17:47:40","title":"Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits","abstract":"We propose an active sampling flow, with the use-case of simulating the impact of combined variations on analog circuits. In such a context, given the large number of parameters, it is difficult to fit a surrogate model and to efficiently explore the space of design features.   By combining a drastic dimension reduction using sensitivity analysis and Bayesian surrogate modeling, we obtain a flexible active sampling flow. On synthetic and real datasets, this flow outperforms the usual Monte-Carlo sampling which often forms the foundation of design space exploration.","sentences":["We propose an active sampling flow, with the use-case of simulating the impact of combined variations on analog circuits.","In such a context, given the large number of parameters, it is difficult to fit a surrogate model and to efficiently explore the space of design features.   ","By combining a drastic dimension reduction using sensitivity analysis and Bayesian surrogate modeling, we obtain a flexible active sampling flow.","On synthetic and real datasets, this flow outperforms the usual Monte-Carlo sampling which often forms the foundation of design space exploration."],"url":"http://arxiv.org/abs/2405.07971v1","category":"stat.ML"}
{"created":"2024-05-13 17:01:28","title":"Improving Multimodal Learning with Multi-Loss Gradient Modulation","abstract":"Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance. However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes. It has been observed that one modality can potentially dominate the learning process, hindering the effective utilization of information from other modalities and leading to sub-optimal model performance. To address this issue the vast majority of previous works suggest to assess the unimodal contributions and dynamically adjust the training to equalize them. We improve upon previous work by introducing a multi-loss objective and further refining the balancing process, allowing it to dynamically adjust the learning pace of each modality in both directions, acceleration and deceleration, with the ability to phase out balancing effects upon convergence. We achieve superior results across three audio-video datasets: on CREMA-D, models with ResNet backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer backbone models deliver improvements ranging from 2.8% to 14.1% across different fusion methods. On AVE, improvements range from 2.7% to 7.7%, while on UCF101, gains reach up to 6.1%.","sentences":["Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance.","However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes.","It has been observed that one modality can potentially dominate the learning process, hindering the effective utilization of information from other modalities and leading to sub-optimal model performance.","To address this issue the vast majority of previous works suggest to assess the unimodal contributions and dynamically adjust the training to equalize them.","We improve upon previous work by introducing a multi-loss objective and further refining the balancing process, allowing it to dynamically adjust the learning pace of each modality in both directions, acceleration and deceleration, with the ability to phase out balancing effects upon convergence.","We achieve superior results across three audio-video datasets: on CREMA-D, models with ResNet backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer backbone models deliver improvements ranging from 2.8% to 14.1% across different fusion methods.","On AVE, improvements range from 2.7% to 7.7%, while on UCF101, gains reach up to 6.1%."],"url":"http://arxiv.org/abs/2405.07930v1","category":"cs.MM"}
{"created":"2024-05-13 16:47:05","title":"Distribution Learning Meets Graph Structure Sampling","abstract":"This work establishes a novel link between the problem of PAC-learning high-dimensional graphical models and the task of (efficient) counting and sampling of graph structures, using an online learning framework.   We observe that if we apply the exponentially weighted average (EWA) or randomized weighted majority (RWM) forecasters on a sequence of samples from a distribution P using the log loss function, the average regret incurred by the forecaster's predictions can be used to bound the expected KL divergence between P and the predictions. Known regret bounds for EWA and RWM then yield new sample complexity bounds for learning Bayes nets. Moreover, these algorithms can be made computationally efficient for several interesting classes of Bayes nets. Specifically, we give a new sample-optimal and polynomial time learning algorithm with respect to trees of unknown structure and the first polynomial sample and time algorithm for learning with respect to Bayes nets over a given chordal skeleton.","sentences":["This work establishes a novel link between the problem of PAC-learning high-dimensional graphical models and the task of (efficient) counting and sampling of graph structures, using an online learning framework.   ","We observe that if we apply the exponentially weighted average (EWA) or randomized weighted majority (RWM) forecasters on a sequence of samples from a distribution P using the log loss function, the average regret incurred by the forecaster's predictions can be used to bound the expected KL divergence between P and the predictions.","Known regret bounds for EWA and RWM then yield new sample complexity bounds for learning Bayes nets.","Moreover, these algorithms can be made computationally efficient for several interesting classes of Bayes nets.","Specifically, we give a new sample-optimal and polynomial time learning algorithm with respect to trees of unknown structure and the first polynomial sample and time algorithm for learning with respect to Bayes nets over a given chordal skeleton."],"url":"http://arxiv.org/abs/2405.07914v1","category":"cs.LG"}
{"created":"2024-05-13 16:38:20","title":"A complete framework for cosmological emulation and inference with CosmoPower","abstract":"We present a coherent, re-usable python framework which further builds on the cosmological emulator code CosmoPower. In the current era of high-precision cosmology, we require high-accuracy calculations of cosmological observables with Einstein-Boltzmann codes. For detailed statistical analyses, such codes often incur high costs in terms of computing power, making parameter space exploration costly, especially for beyond-$\\Lambda$CDM analyses. Machine learning-enabled emulators of Einstein-Boltzmann codes have emerged as a solution to this problem and have become a common way to perform fast cosmological analyses. To enable generation, sharing and use of emulators for inference, we define standards for robustly describing, packaging and distributing them, and present software for easily performing these tasks in an automated and replicable manner. We provide examples and guidelines for generating your own sufficiently accurate emulators and wrappers for using them in popular cosmological inference codes. We demonstrate our framework by presenting a suite of high-accuracy emulators for the CAMB code's calculations of CMB $C_\\ell$, $P(k)$, background evolution, and derived parameter quantities. We show that these emulators are accurate enough for both $\\Lambda$CDM analysis and a set of single- and two-parameter extension models (including $N_{\\rm eff}$, $\\sum m_{\\nu}$ and $w_0 w_a$ cosmologies) with stage-IV observatories, recovering the original high-accuracy Einstein-Boltzmann spectra to tolerances well within the cosmic variance uncertainties across the full range of parameters considered. We also use our emulators to recover cosmological parameters in a simulated cosmic-variance limited experiment, finding results well within $0.1 \\sigma$ of the input cosmology, while requiring typically $\\lesssim1/50$ of the evaluation time than for the full Einstein-Boltzmann computation.","sentences":["We present a coherent, re-usable python framework which further builds on the cosmological emulator code CosmoPower.","In the current era of high-precision cosmology, we require high-accuracy calculations of cosmological observables with Einstein-Boltzmann codes.","For detailed statistical analyses, such codes often incur high costs in terms of computing power, making parameter space exploration costly, especially for beyond-$\\Lambda$CDM analyses.","Machine learning-enabled emulators of Einstein-Boltzmann codes have emerged as a solution to this problem and have become a common way to perform fast cosmological analyses.","To enable generation, sharing and use of emulators for inference, we define standards for robustly describing, packaging and distributing them, and present software for easily performing these tasks in an automated and replicable manner.","We provide examples and guidelines for generating your own sufficiently accurate emulators and wrappers for using them in popular cosmological inference codes.","We demonstrate our framework by presenting a suite of high-accuracy emulators for the CAMB code's calculations of CMB $C_\\ell$, $P(k)$, background evolution, and derived parameter quantities.","We show that these emulators are accurate enough for both $\\Lambda$CDM analysis and a set of single- and two-parameter extension models (including $N_{\\rm eff}$, $\\sum m_{\\nu}$ and $w_0 w_a$ cosmologies) with stage-IV observatories, recovering the original high-accuracy Einstein-Boltzmann spectra to tolerances well within the cosmic variance uncertainties across the full range of parameters considered.","We also use our emulators to recover cosmological parameters in a simulated cosmic-variance limited experiment, finding results well within $0.1 \\sigma$ of the input cosmology, while requiring typically $\\lesssim1/50$ of the evaluation time than for the full Einstein-Boltzmann computation."],"url":"http://arxiv.org/abs/2405.07903v1","category":"astro-ph.CO"}
{"created":"2024-05-13 16:17:57","title":"Lai Loss: A Novel Loss Integrating Regularization","abstract":"In the field of machine learning, traditional regularization methods generally tend to directly add regularization terms to the loss function. This paper introduces the \"Lai loss\", a novel loss design that integrates the regularization terms (gradient component) into the traditional loss function through a straightforward geometric ideation. This design innovatively penalizes the gradient vectors through the loss, effectively controlling the model's smoothness and offering the dual benefits of reducing overfitting and avoiding underfitting. Subsequently, we proposed a random sampling method that successfully addresses the challenges associated with its application under large sample conditions. We conducted preliminary experiments using publicly available datasets from Kaggle, demonstrating that the design of Lai loss can control the model's smoothness while ensuring maximum accuracy.","sentences":["In the field of machine learning, traditional regularization methods generally tend to directly add regularization terms to the loss function.","This paper introduces the \"Lai loss\", a novel loss design that integrates the regularization terms (gradient component) into the traditional loss function through a straightforward geometric ideation.","This design innovatively penalizes the gradient vectors through the loss, effectively controlling the model's smoothness and offering the dual benefits of reducing overfitting and avoiding underfitting.","Subsequently, we proposed a random sampling method that successfully addresses the challenges associated with its application under large sample conditions.","We conducted preliminary experiments using publicly available datasets from Kaggle, demonstrating that the design of Lai loss can control the model's smoothness while ensuring maximum accuracy."],"url":"http://arxiv.org/abs/2405.07884v1","category":"cs.LG"}
{"created":"2024-05-13 16:09:29","title":"On the Relation Between Autoencoders and Non-negative Matrix Factorization, and Their Application for Mutational Signature Extraction","abstract":"The aim of this study is to provide a foundation to understand the relationship between non-negative matrix factorization (NMF) and non-negative autoencoders enabling proper interpretation and understanding of autoencoder-based alternatives to NMF. Since its introduction, NMF has been a popular tool for extracting interpretable, low-dimensional representations of high-dimensional data. However, recently, several studies have proposed to replace NMF with autoencoders. This increasing popularity of autoencoders warrants an investigation on whether this replacement is in general valid and reasonable. Moreover, the exact relationship between non-negative autoencoders and NMF has not been thoroughly explored. Thus, a main aim of this study is to investigate in detail the relationship between non-negative autoencoders and NMF. We find that the connection between the two models can be established through convex NMF, which is a restricted case of NMF. In particular, convex NMF is a special case of an autoencoder. The performance of NMF and autoencoders is compared within the context of extraction of mutational signatures from cancer genomics data. We find that the reconstructions based on NMF are more accurate compared to autoencoders, while the signatures extracted using both methods show comparable consistencies and values when externally validated. These findings suggest that the non-negative autoencoders investigated in this article do not provide an improvement of NMF in the field of mutational signature extraction.","sentences":["The aim of this study is to provide a foundation to understand the relationship between non-negative matrix factorization (NMF) and non-negative autoencoders enabling proper interpretation and understanding of autoencoder-based alternatives to NMF.","Since its introduction, NMF has been a popular tool for extracting interpretable, low-dimensional representations of high-dimensional data.","However, recently, several studies have proposed to replace NMF with autoencoders.","This increasing popularity of autoencoders warrants an investigation on whether this replacement is in general valid and reasonable.","Moreover, the exact relationship between non-negative autoencoders and NMF has not been thoroughly explored.","Thus, a main aim of this study is to investigate in detail the relationship between non-negative autoencoders and NMF.","We find that the connection between the two models can be established through convex NMF, which is a restricted case of NMF.","In particular, convex NMF is a special case of an autoencoder.","The performance of NMF and autoencoders is compared within the context of extraction of mutational signatures from cancer genomics data.","We find that the reconstructions based on NMF are more accurate compared to autoencoders, while the signatures extracted using both methods show comparable consistencies and values when externally validated.","These findings suggest that the non-negative autoencoders investigated in this article do not provide an improvement of NMF in the field of mutational signature extraction."],"url":"http://arxiv.org/abs/2405.07879v1","category":"stat.AP"}
{"created":"2024-05-13 17:55:52","title":"Identifying the minimal sets of distance restraints for FRET-assisted protein structural modeling","abstract":"Proteins naturally occur in crowded cellular environments and interact with other proteins, nucleic acids, and organelles. Since most previous experimental protein structure determination techniques require that proteins occur in idealized, non-physiological environments, the effects of realistic cellular environments on protein structure are largely unexplored. Recently, F\\\"{o}rster resonance energy transfer (FRET) has been shown to be an effective experimental method for investigating protein structure in vivo. Inter-residue distances measured in vivo can be incorporated as restraints in molecular dynamics (MD) simulations to model protein structural dynamics in vivo. Since most FRET studies only obtain inter-residue separations for a small number of amino acid pairs, it is important to determine the minimum number of restraints in the MD simulations that are required to achieve a given root-mean-square deviation (RMSD) from the experimental structural ensemble. Further, what is the optimal method for selecting these inter-residue restraints? Here, we implement several methods for selecting the most important FRET pairs and determine the number of pairs $N_{r}$ that are needed to induce conformational changes in proteins between two experimentally determined structures. We find that enforcing only a small fraction of restraints, $N_{r}/N \\lesssim 0.08$, where $N$ is the number of amino acids, can induce the conformational changes. These results establish the efficacy of FRET-assisted MD simulations for atomic scale structural modeling of proteins in vivo.","sentences":["Proteins naturally occur in crowded cellular environments and interact with other proteins, nucleic acids, and organelles.","Since most previous experimental protein structure determination techniques require that proteins occur in idealized, non-physiological environments, the effects of realistic cellular environments on protein structure are largely unexplored.","Recently, F\\\"{o}rster resonance energy transfer (FRET) has been shown to be an effective experimental method for investigating protein structure in vivo.","Inter-residue distances measured in vivo can be incorporated as restraints in molecular dynamics (MD) simulations to model protein structural dynamics in vivo.","Since most FRET studies only obtain inter-residue separations for a small number of amino acid pairs, it is important to determine the minimum number of restraints in the MD simulations that are required to achieve a given root-mean-square deviation (RMSD) from the experimental structural ensemble.","Further, what is the optimal method for selecting these inter-residue restraints?","Here, we implement several methods for selecting the most important FRET pairs and determine the number of pairs $N_{r}$ that are needed to induce conformational changes in proteins between two experimentally determined structures.","We find that enforcing only a small fraction of restraints, $N_{r}/N \\lesssim 0.08$, where $N$ is the number of amino acids, can induce the conformational changes.","These results establish the efficacy of FRET-assisted MD simulations for atomic scale structural modeling of proteins in vivo."],"url":"http://arxiv.org/abs/2405.07983v1","category":"physics.bio-ph"}
{"created":"2024-05-13 17:53:51","title":"Enhancing Rover Mobility Monitoring: Autoencoder-driven Anomaly Detection for Curiosity","abstract":"Over eleven years into its mission, the Mars Science Laboratory remains vital to NASA's Mars exploration. Safeguarding the rover's long-term functionality is a top mission priority. In this study, we introduce and test undercomplete autoencoder models for detecting drive anomalies, using telemetry data from wheel actuators, the Rover Inertial Measurement Unit (RIMU), and the suspension system. Our approach enhances post-drive data analysis during tactical downlink sessions. We explore various model architectures and input features to understand their impact on performance. Evaluating the models involves testing them on unseen data to mimic real-world scenarios. Our experiments demonstrate the undercomplete autoencoder model's effectiveness in detecting drive anomalies within the Curiosity rover dataset. Remarkably, the model even identifies subtle anomalous telemetry patterns missed by human operators. Additionally, we provide insights into optimal design choices by comparing different model architectures and input features. The model's ability to capture inconspicuous anomalies, potentially indicating early-stage failures, holds promise for the field, by improving the reliability and safety of future planetary exploration missions through early anomaly detection and proactive maintenance.","sentences":["Over eleven years into its mission, the Mars Science Laboratory remains vital to NASA's Mars exploration.","Safeguarding the rover's long-term functionality is a top mission priority.","In this study, we introduce and test undercomplete autoencoder models for detecting drive anomalies, using telemetry data from wheel actuators, the Rover Inertial Measurement Unit (RIMU), and the suspension system.","Our approach enhances post-drive data analysis during tactical downlink sessions.","We explore various model architectures and input features to understand their impact on performance.","Evaluating the models involves testing them on unseen data to mimic real-world scenarios.","Our experiments demonstrate the undercomplete autoencoder model's effectiveness in detecting drive anomalies within the Curiosity rover dataset.","Remarkably, the model even identifies subtle anomalous telemetry patterns missed by human operators.","Additionally, we provide insights into optimal design choices by comparing different model architectures and input features.","The model's ability to capture inconspicuous anomalies, potentially indicating early-stage failures, holds promise for the field, by improving the reliability and safety of future planetary exploration missions through early anomaly detection and proactive maintenance."],"url":"http://arxiv.org/abs/2405.07982v1","category":"cs.RO"}
{"created":"2024-05-13 17:35:14","title":"An adjoint-based approach for the surgical correction of nasal septal deviations","abstract":"Deviations of the septal wall are widespread anatomic anomalies of the human nose; they vary significantly in shape and location, and often cause the obstruction of the nasal airways. When severe, septal deviations need to be surgically corrected by ear-nose-throat (ENT) specialists. Septoplasty, however, has a low success rate, owing to the lack of suitable standardized clinical tools for assessing type and severity of obstructions, and for surgery planning. Moreover, the restoration of a perfectly straight septal wall is often impossible and possibly unnecessary. This paper introduces a procedure, based on advanced patient-specific Computational Fluid Dynamics (CFD) simulations, to support ENT surgeons in septoplasty planning. The method hinges upon the theory of adjoint-based optimization, and minimizes a cost function that indirectly accounts for viscous losses. A sensitivity map is computed on the mucosal wall to provide the surgeon with a simple quantification of how much tissue removal at each location would contribute to easing the obstruction. The optimization procedure is applied to three representative nasal anatomies, reconstructed from CT scans of patients affected by complex septal deviations. The computed sensitivity consistently identifies all the anomalies correctly. Virtual surgery, i.e. morphing of the anatomies according to the computed sensitivity, confirms that the characteristics of the nasal airflow improve significantly after small anatomy changes derived from adjoint-based optimization.","sentences":["Deviations of the septal wall are widespread anatomic anomalies of the human nose; they vary significantly in shape and location, and often cause the obstruction of the nasal airways.","When severe, septal deviations need to be surgically corrected by ear-nose-throat (ENT) specialists.","Septoplasty, however, has a low success rate, owing to the lack of suitable standardized clinical tools for assessing type and severity of obstructions, and for surgery planning.","Moreover, the restoration of a perfectly straight septal wall is often impossible and possibly unnecessary.","This paper introduces a procedure, based on advanced patient-specific Computational Fluid Dynamics (CFD) simulations, to support ENT surgeons in septoplasty planning.","The method hinges upon the theory of adjoint-based optimization, and minimizes a cost function that indirectly accounts for viscous losses.","A sensitivity map is computed on the mucosal wall to provide the surgeon with a simple quantification of how much tissue removal at each location would contribute to easing the obstruction.","The optimization procedure is applied to three representative nasal anatomies, reconstructed from CT scans of patients affected by complex septal deviations.","The computed sensitivity consistently identifies all the anomalies correctly.","Virtual surgery, i.e. morphing of the anatomies according to the computed sensitivity, confirms that the characteristics of the nasal airflow improve significantly after small anatomy changes derived from adjoint-based optimization."],"url":"http://arxiv.org/abs/2405.07959v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 17:15:57","title":"Spatiotemporal control of structure and dynamics in a polar active fluid","abstract":"We apply optimal control theory to a model of a polar active fluid (the Toner-Tu model), with the objective of driving the system into particular emergent dynamical behaviors or programming switching between states on demand. We use the effective self-propulsion speed as the control parameter (i.e. the means of external actuation). We identify control protocols that achieve outcomes such as relocating asters to targeted positions, forcing propagating solitary waves to reorient to a particular direction, and switching between stationary asters and propagating fronts. We analyze the solutions to identify generic principles for controlling polar active fluids. Our findings have implications for achieving spatiotemporal control of active polar systems in experiments, particularly in vitro cytoskeletal systems. Additionally, this research paves the way for leveraging optimal control methods to engineer the structure and dynamics of active fluids more broadly.","sentences":["We apply optimal control theory to a model of a polar active fluid (the Toner-Tu model), with the objective of driving the system into particular emergent dynamical behaviors or programming switching between states on demand.","We use the effective self-propulsion speed as the control parameter (i.e. the means of external actuation).","We identify control protocols that achieve outcomes such as relocating asters to targeted positions, forcing propagating solitary waves to reorient to a particular direction, and switching between stationary asters and propagating fronts.","We analyze the solutions to identify generic principles for controlling polar active fluids.","Our findings have implications for achieving spatiotemporal control of active polar systems in experiments, particularly in vitro cytoskeletal systems.","Additionally, this research paves the way for leveraging optimal control methods to engineer the structure and dynamics of active fluids more broadly."],"url":"http://arxiv.org/abs/2405.07942v2","category":"cond-mat.soft"}
{"created":"2024-05-13 16:41:14","title":"Improved Downlink Channel Estimation in Time-Varying FDD Massive MIMO Systems","abstract":"In this work, we address the challenge of accurately obtaining channel state information at the transmitter (CSIT) for frequency division duplexing (FDD) multiple input multiple output systems. Although CSIT is vital for maximizing spatial multiplexing gains, traditional CSIT estimation methods often suffer from impracticality due to the substantial training and feedback overhead they require. To address this challenge, we leverage two sources of prior information simultaneously: the presence of limited local scatterers at the base station (BS) and the time-varying characteristics of the channel. The former results in a redundant angular sparsity of users' channels exceeding the spatial dimension (i.e., the number of BS antennas), while the latter provides a prior non-uniform distribution in the angular domain. We propose a weighted optimization framework that simultaneously reflects both of these features. The optimal weights are then obtained by minimizing the expected recovery error of the optimization problem. This establishes an analytical closed-form relationship between the optimal weights and the angular domain characteristics. Numerical experiments verify the effectiveness of our proposed approach in reducing the recovery error and consequently resulting in decreased training and feedback overhead.","sentences":["In this work, we address the challenge of accurately obtaining channel state information at the transmitter (CSIT) for frequency division duplexing (FDD)","multiple input multiple output systems.","Although CSIT is vital for maximizing spatial multiplexing gains, traditional CSIT estimation methods often suffer from impracticality due to the substantial training and feedback overhead they require.","To address this challenge, we leverage two sources of prior information simultaneously: the presence of limited local scatterers at the base station (BS) and the time-varying characteristics of the channel.","The former results in a redundant angular sparsity of users' channels exceeding the spatial dimension (i.e., the number of BS antennas), while the latter provides a prior non-uniform distribution in the angular domain.","We propose a weighted optimization framework that simultaneously reflects both of these features.","The optimal weights are then obtained by minimizing the expected recovery error of the optimization problem.","This establishes an analytical closed-form relationship between the optimal weights and the angular domain characteristics.","Numerical experiments verify the effectiveness of our proposed approach in reducing the recovery error and consequently resulting in decreased training and feedback overhead."],"url":"http://arxiv.org/abs/2405.07906v1","category":"eess.SP"}
{"created":"2024-05-13 16:31:53","title":"Optimal Transmitter Design and Pilot Spacing in MIMO Non-Stationary Aging Channels","abstract":"This work considers an uplink wireless communication system where multiple users with multiple antennas transmit data frames over dynamic channels. Previous studies have shown that multiple transmit and receive antennas can substantially enhance the sum-capacity of all users when the channel is known at the transmitter and in the case of uncorrelated transmit and receive antennas. However, spatial correlations stemming from close proximity of transmit antennas and channel variation between pilot and data time slots, known as channel aging, can substantially degrade the transmission rate if they are not properly into account. In this work, we provide an analytical framework to concurrently exploit both of these features. Specifically, we first propose a beamforming framework to capture spatial correlations. Then, based on random matrix theory tools, we introduce a deterministic expression that approximates the average sum-capacity of all users. Subsequently, we obtain the optimal values of pilot spacing and beamforming vectors upon maximizing this expression. Simulation results show the impacts of path loss, velocity of mobile users and Rician factor on the resulting sum-capacity and underscore the efficacy of our methodology compared to prior works.","sentences":["This work considers an uplink wireless communication system where multiple users with multiple antennas transmit data frames over dynamic channels.","Previous studies have shown that multiple transmit and receive antennas can substantially enhance the sum-capacity of all users when the channel is known at the transmitter and in the case of uncorrelated transmit and receive antennas.","However, spatial correlations stemming from close proximity of transmit antennas and channel variation between pilot and data time slots, known as channel aging, can substantially degrade the transmission rate if they are not properly into account.","In this work, we provide an analytical framework to concurrently exploit both of these features.","Specifically, we first propose a beamforming framework to capture spatial correlations.","Then, based on random matrix theory tools, we introduce a deterministic expression that approximates the average sum-capacity of all users.","Subsequently, we obtain the optimal values of pilot spacing and beamforming vectors upon maximizing this expression.","Simulation results show the impacts of path loss, velocity of mobile users and Rician factor on the resulting sum-capacity and underscore the efficacy of our methodology compared to prior works."],"url":"http://arxiv.org/abs/2405.07895v1","category":"eess.SP"}
{"created":"2024-05-13 16:25:43","title":"Subspace-Informed Matrix Completion","abstract":"In this work, we consider the matrix completion problem, where the objective is to reconstruct a low-rank matrix from a few observed entries. A commonly employed approach involves nuclear norm minimization. For this method to succeed, the number of observed entries needs to scale at least proportional to both the rank of the ground-truth matrix and the coherence parameter. While the only prior information is oftentimes the low-rank nature of the ground-truth matrix, in various real-world scenarios, additional knowledge about the ground-truth low-rank matrix is available. For instance, in collaborative filtering, Netflix problem, and dynamic channel estimation in wireless communications, we have partial or full knowledge about the signal subspace in advance. Specifically, we are aware of some subspaces that form multiple angles with the column and row spaces of the ground-truth matrix. Leveraging this valuable information has the potential to significantly reduce the required number of observations. To this end, we introduce a multi-weight nuclear norm optimization problem that concurrently promotes the low-rank property as well the information about the available subspaces. The proposed weights are tailored to penalize each angle corresponding to each basis of the prior subspace independently. We further propose an optimal weight selection strategy by minimizing the coherence parameter of the ground-truth matrix, which is equivalent to minimizing the required number of observations. Simulation results validate the advantages of incorporating multiple weights in the completion procedure. Specifically, our proposed multi-weight optimization problem demonstrates a substantial reduction in the required number of observations compared to the state-of-the-art methods.","sentences":["In this work, we consider the matrix completion problem, where the objective is to reconstruct a low-rank matrix from a few observed entries.","A commonly employed approach involves nuclear norm minimization.","For this method to succeed, the number of observed entries needs to scale at least proportional to both the rank of the ground-truth matrix and the coherence parameter.","While the only prior information is oftentimes the low-rank nature of the ground-truth matrix, in various real-world scenarios, additional knowledge about the ground-truth low-rank matrix is available.","For instance, in collaborative filtering, Netflix problem, and dynamic channel estimation in wireless communications, we have partial or full knowledge about the signal subspace in advance.","Specifically, we are aware of some subspaces that form multiple angles with the column and row spaces of the ground-truth matrix.","Leveraging this valuable information has the potential to significantly reduce the required number of observations.","To this end, we introduce a multi-weight nuclear norm optimization problem that concurrently promotes the low-rank property as well the information about the available subspaces.","The proposed weights are tailored to penalize each angle corresponding to each basis of the prior subspace independently.","We further propose an optimal weight selection strategy by minimizing the coherence parameter of the ground-truth matrix, which is equivalent to minimizing the required number of observations.","Simulation results validate the advantages of incorporating multiple weights in the completion procedure.","Specifically, our proposed multi-weight optimization problem demonstrates a substantial reduction in the required number of observations compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.07890v2","category":"eess.SP"}
{"created":"2024-05-13 16:19:31","title":"Quantum Computation Using Large Spin Qudits","abstract":"This dissertation explores quantum computation using qudits encoded into large spins, emphasizing the concept of quantum co-design to harness the unique capabilities of physical platforms for enhanced quantum information processing. First, we delve into the generation of high-fidelity universal gate sets for quantum computation with qudits. Leveraging principles from quantum optimal control, Rydberg physics, and the atomic structure of alkaline-earth atoms, we propose protocols for high-fidelity universal gate sets in the ground state of 87Sr with reasonable experimental parameters. Next, we analyze schemes to encode a qubit in the large spin qudits for fault-tolerant quantum computation (FTQC). By comprehending the most dominant noise in the physical system, we develop FTQC protocols that outperform the standard protocols. Finally, considering spin qudits for neutral atom quantum computation, we studied protocols for converting leakage errors to erasure errors resource efficiently. Also, we developed cooling methods for neutral atoms without destroying the quantum information.","sentences":["This dissertation explores quantum computation using qudits encoded into large spins, emphasizing the concept of quantum co-design to harness the unique capabilities of physical platforms for enhanced quantum information processing.","First, we delve into the generation of high-fidelity universal gate sets for quantum computation with qudits.","Leveraging principles from quantum optimal control, Rydberg physics, and the atomic structure of alkaline-earth atoms, we propose protocols for high-fidelity universal gate sets in the ground state of 87Sr with reasonable experimental parameters.","Next, we analyze schemes to encode a qubit in the large spin qudits for fault-tolerant quantum computation (FTQC).","By comprehending the most dominant noise in the physical system, we develop FTQC protocols that outperform the standard protocols.","Finally, considering spin qudits for neutral atom quantum computation, we studied protocols for converting leakage errors to erasure errors resource efficiently.","Also, we developed cooling methods for neutral atoms without destroying the quantum information."],"url":"http://arxiv.org/abs/2405.07885v1","category":"quant-ph"}
{"created":"2024-05-13 16:16:57","title":"Exploiting Spatial and Temporal Correlations in Massive MIMO Systems Over Non-Stationary Aging Channels","abstract":"This work investigates a multi-user, multi-antenna uplink wireless system, where multiple users transmit signals to a base station. Previous research has explored the potential for linear growth in spectral efficiency by employing multiple transmit and receive antennas. This gain depends on the quality of channel state information and uncorrelated antennas. However, spatial correlations, arising from closely-spaced antennas, and channel aging effects, stemming from the difference between the channel at pilot and data time instances, can substantially counteract these benefits and degrade the transmission rate, especially in non-stationary environments. To address these challenges, this work introduces a real-time beamforming framework to compensate for the spatial correlation effect. A channel estimation scheme is then developed, leveraging temporal channel correlations and considering mobile device velocity and antenna spacing. Subsequently, an expression approximating the average spectral efficiency is obtained, dependent on pilot spacing, pilot and data powers, and beamforming vectors. By maximizing this expression, optimal parameters are identified. Numerical results reveal the effectiveness of the proposed approach compared to prior works. Moreover, optimal pilot spacing remains unaffected by interference components such as path loss and the velocity of interference users. The impact of interference components also diminishes with an increasing number of transmit antennas.","sentences":["This work investigates a multi-user, multi-antenna uplink wireless system, where multiple users transmit signals to a base station.","Previous research has explored the potential for linear growth in spectral efficiency by employing multiple transmit and receive antennas.","This gain depends on the quality of channel state information and uncorrelated antennas.","However, spatial correlations, arising from closely-spaced antennas, and channel aging effects, stemming from the difference between the channel at pilot and data time instances, can substantially counteract these benefits and degrade the transmission rate, especially in non-stationary environments.","To address these challenges, this work introduces a real-time beamforming framework to compensate for the spatial correlation effect.","A channel estimation scheme is then developed, leveraging temporal channel correlations and considering mobile device velocity and antenna spacing.","Subsequently, an expression approximating the average spectral efficiency is obtained, dependent on pilot spacing, pilot and data powers, and beamforming vectors.","By maximizing this expression, optimal parameters are identified.","Numerical results reveal the effectiveness of the proposed approach compared to prior works.","Moreover, optimal pilot spacing remains unaffected by interference components such as path loss and the velocity of interference users.","The impact of interference components also diminishes with an increasing number of transmit antennas."],"url":"http://arxiv.org/abs/2405.07882v1","category":"eess.SP"}
{"created":"2024-05-13 15:57:19","title":"Boostlet.js: Image processing plugins for the web via JavaScript injection","abstract":"Can web-based image processing and visualization tools easily integrate into existing websites without significant time and effort? Our Boostlet.js library addresses this challenge by providing an open-source, JavaScript-based web framework to enable additional image processing functionalities. Boostlet examples include kernel filtering, image captioning, data visualization, segmentation, and web-optimized machine-learning models. To achieve this, Boostlet.js uses a browser bookmark to inject a user-friendly plugin selection tool called PowerBoost into any host website. Boostlet also provides on-site access to a standard API independent of any visualization framework for pixel data and scene manipulation. Web-based Boostlets provide a modular architecture and client-side processing capabilities to apply advanced image-processing techniques using consumer-level hardware. The code is open-source and available.","sentences":["Can web-based image processing and visualization tools easily integrate into existing websites without significant time and effort?","Our Boostlet.js library addresses this challenge by providing an open-source, JavaScript-based web framework to enable additional image processing functionalities.","Boostlet examples include kernel filtering, image captioning, data visualization, segmentation, and web-optimized machine-learning models.","To achieve this, Boostlet.js uses a browser bookmark to inject a user-friendly plugin selection tool called PowerBoost into any host website.","Boostlet also provides on-site access to a standard API independent of any visualization framework for pixel data and scene manipulation.","Web-based Boostlets provide a modular architecture and client-side processing capabilities to apply advanced image-processing techniques using consumer-level hardware.","The code is open-source and available."],"url":"http://arxiv.org/abs/2405.07868v1","category":"cs.CV"}
{"created":"2024-05-13 15:48:26","title":"Improving Breast Cancer Grade Prediction with Multiparametric MRI Created Using Optimized Synthetic Correlated Diffusion Imaging","abstract":"Breast cancer was diagnosed for over 7.8 million women between 2015 to 2020. Grading plays a vital role in breast cancer treatment planning. However, the current tumor grading method involves extracting tissue from patients, leading to stress, discomfort, and high medical costs. A recent paper leveraging volumetric deep radiomic features from synthetic correlated diffusion imaging (CDI$^s$) for breast cancer grade prediction showed immense promise for noninvasive methods for grading. Motivated by the impact of CDI$^s$ optimization for prostate cancer delineation, this paper examines using optimized CDI$^s$ to improve breast cancer grade prediction. We fuse the optimized CDI$^s$ signal with diffusion-weighted imaging (DWI) to create a multiparametric MRI for each patient. Using a larger patient cohort and training across all the layers of a pretrained MONAI model, we achieve a leave-one-out cross-validation accuracy of 95.79%, over 8% higher compared to that previously reported.","sentences":["Breast cancer was diagnosed for over 7.8 million women between 2015 to 2020.","Grading plays a vital role in breast cancer treatment planning.","However, the current tumor grading method involves extracting tissue from patients, leading to stress, discomfort, and high medical costs.","A recent paper leveraging volumetric deep radiomic features from synthetic correlated diffusion imaging (CDI$^s$) for breast cancer grade prediction showed immense promise for noninvasive methods for grading.","Motivated by the impact of CDI$^s$ optimization for prostate cancer delineation, this paper examines using optimized CDI$^s$ to improve breast cancer grade prediction.","We fuse the optimized CDI$^s$ signal with diffusion-weighted imaging (DWI) to create a multiparametric MRI for each patient.","Using a larger patient cohort and training across all the layers of a pretrained MONAI model, we achieve a leave-one-out cross-validation accuracy of 95.79%, over 8% higher compared to that previously reported."],"url":"http://arxiv.org/abs/2405.07861v1","category":"eess.IV"}
{"created":"2024-05-13 15:40:56","title":"Using Multiparametric MRI with Optimized Synthetic Correlated Diffusion Imaging to Enhance Breast Cancer Pathologic Complete Response Prediction","abstract":"In 2020, 685,000 deaths across the world were attributed to breast cancer, underscoring the critical need for innovative and effective breast cancer treatment. Neoadjuvant chemotherapy has recently gained popularity as a promising treatment strategy for breast cancer, attributed to its efficacy in shrinking large tumors and leading to pathologic complete response. However, the current process to recommend neoadjuvant chemotherapy relies on the subjective evaluation of medical experts which contain inherent biases and significant uncertainty. A recent study, utilizing volumetric deep radiomic features extracted from synthetic correlated diffusion imaging (CDI$^s$), demonstrated significant potential in noninvasive breast cancer pathologic complete response prediction. Inspired by the positive outcomes of optimizing CDI$^s$ for prostate cancer delineation, this research investigates the application of optimized CDI$^s$ to enhance breast cancer pathologic complete response prediction. Using multiparametric MRI that fuses optimized CDI$^s$ with diffusion-weighted imaging (DWI), we obtain a leave-one-out cross-validation accuracy of 93.28%, over 5.5% higher than that previously reported.","sentences":["In 2020, 685,000 deaths across the world were attributed to breast cancer, underscoring the critical need for innovative and effective breast cancer treatment.","Neoadjuvant chemotherapy has recently gained popularity as a promising treatment strategy for breast cancer, attributed to its efficacy in shrinking large tumors and leading to pathologic complete response.","However, the current process to recommend neoadjuvant chemotherapy relies on the subjective evaluation of medical experts which contain inherent biases and significant uncertainty.","A recent study, utilizing volumetric deep radiomic features extracted from synthetic correlated diffusion imaging (CDI$^s$), demonstrated significant potential in noninvasive breast cancer pathologic complete response prediction.","Inspired by the positive outcomes of optimizing CDI$^s$ for prostate cancer delineation, this research investigates the application of optimized CDI$^s$ to enhance breast cancer pathologic complete response prediction.","Using multiparametric MRI that fuses optimized CDI$^s$ with diffusion-weighted imaging (DWI), we obtain a leave-one-out cross-validation accuracy of 93.28%, over 5.5% higher than that previously reported."],"url":"http://arxiv.org/abs/2405.07854v1","category":"eess.IV"}
{"created":"2024-05-13 15:09:08","title":"Fixed Point Theory Analysis of a Lambda Policy Iteration with Randomization for the \u0106iri\u0107 Contraction Operator","abstract":"We apply methods of the fixed point theory to a Lambda policy iteration with a randomization algorithm for weak contractions mappings. This type of mappings covers a broader range than the strong contractions typically considered in the literature, such as \\'Ciri\\'c contraction. Specifically, we explore the characteristics of reinforcement learning procedures developed for feedback control within the context of fixed point theory. Under relatively general assumptions, we identify the sufficient conditions for convergence with a probability of one in infinite-dimensional policy spaces.","sentences":["We apply methods of the fixed point theory to a Lambda policy iteration with a randomization algorithm for weak contractions mappings.","This type of mappings covers a broader range than the strong contractions typically considered in the literature, such as \\'Ciri\\'c contraction.","Specifically, we explore the characteristics of reinforcement learning procedures developed for feedback control within the context of fixed point theory.","Under relatively general assumptions, we identify the sufficient conditions for convergence with a probability of one in infinite-dimensional policy spaces."],"url":"http://arxiv.org/abs/2405.07824v1","category":"math.OC"}
{"created":"2024-05-13 14:48:57","title":"Goal-oriented compression for $L_p$-norm-type goal functions: Application to power consumption scheduling","abstract":"Conventional data compression schemes aim at implementing a trade-off between the rate required to represent the compressed data and the resulting distortion between the original and reconstructed data. However, in more and more applications, what is desired is not reconstruction accuracy but the quality of the realization of a certain task by the receiver. In this paper, the receiver task is modeled by an optimization problem whose parameters have to be compressed by the transmitter. Motivated by applications such as the smart grid, this paper focuses on a goal function which is of $L_p$-norm-type. The aim is to design the precoding, quantization, and decoding stages such that the maximum of the goal function obtained with the compressed version of the parameters is as close as possible to the maximum obtained without compression. The numerical analysis, based on real smart grid signals, clearly shows the benefits of the proposed approach compared to the conventional distortion-based compression paradigm.","sentences":["Conventional data compression schemes aim at implementing a trade-off between the rate required to represent the compressed data and the resulting distortion between the original and reconstructed data.","However, in more and more applications, what is desired is not reconstruction accuracy but the quality of the realization of a certain task by the receiver.","In this paper, the receiver task is modeled by an optimization problem whose parameters have to be compressed by the transmitter.","Motivated by applications such as the smart grid, this paper focuses on a goal function which is of $L_p$-norm-type.","The aim is to design the precoding, quantization, and decoding stages such that the maximum of the goal function obtained with the compressed version of the parameters is as close as possible to the maximum obtained without compression.","The numerical analysis, based on real smart grid signals, clearly shows the benefits of the proposed approach compared to the conventional distortion-based compression paradigm."],"url":"http://arxiv.org/abs/2405.07808v1","category":"eess.SP"}
{"created":"2024-05-13 14:44:02","title":"Data Imputation by Pursuing Better Classification: A Supervised Kernel-Based Method","abstract":"Data imputation, the process of filling in missing feature elements for incomplete data sets, plays a crucial role in data-driven learning. A fundamental belief is that data imputation is helpful for learning performance, and it follows that the pursuit of better classification can guide the data imputation process. While some works consider using label information to assist in this task, their simplistic utilization of labels lacks flexibility and may rely on strict assumptions. In this paper, we propose a new framework that effectively leverages supervision information to complete missing data in a manner conducive to classification. Specifically, this framework operates in two stages. Firstly, it leverages labels to supervise the optimization of similarity relationships among data, represented by the kernel matrix, with the goal of enhancing classification accuracy. To mitigate overfitting that may occur during this process, a perturbation variable is introduced to improve the robustness of the framework. Secondly, the learned kernel matrix serves as additional supervision information to guide data imputation through regression, utilizing the block coordinate descent method. The superiority of the proposed method is evaluated on four real-world data sets by comparing it with state-of-the-art imputation methods. Remarkably, our algorithm significantly outperforms other methods when the data is missing more than 60\\% of the features","sentences":["Data imputation, the process of filling in missing feature elements for incomplete data sets, plays a crucial role in data-driven learning.","A fundamental belief is that data imputation is helpful for learning performance, and it follows that the pursuit of better classification can guide the data imputation process.","While some works consider using label information to assist in this task, their simplistic utilization of labels lacks flexibility and may rely on strict assumptions.","In this paper, we propose a new framework that effectively leverages supervision information to complete missing data in a manner conducive to classification.","Specifically, this framework operates in two stages.","Firstly, it leverages labels to supervise the optimization of similarity relationships among data, represented by the kernel matrix, with the goal of enhancing classification accuracy.","To mitigate overfitting that may occur during this process, a perturbation variable is introduced to improve the robustness of the framework.","Secondly, the learned kernel matrix serves as additional supervision information to guide data imputation through regression, utilizing the block coordinate descent method.","The superiority of the proposed method is evaluated on four real-world data sets by comparing it with state-of-the-art imputation methods.","Remarkably, our algorithm significantly outperforms other methods when the data is missing more than 60\\% of the features"],"url":"http://arxiv.org/abs/2405.07800v1","category":"cs.LG"}
{"created":"2024-05-13 14:42:00","title":"Measurement of the $^{14}$C spectrum with Silicon Drift Detectors: towards the study of forbidden $\u03b2$ transitions","abstract":"The ASPECT-BET (An sdd-SPECTrometer for BETa decay studies) project aims to develop a novel technique for the precise measurement of forbidden $\\beta$ spectra in the 10 keV - 1 MeV range. This technique uses a Silicon Drift Detector (SDD) as the main spectrometer, surrounded, if necessary, by a veto system to reject events with only partial energy deposition in the SDD. Accurate knowledge of the spectrometer's response to electrons is essential to reconstruct the theoretical shape of the $\\beta$ spectrum. To compute this response, GEANT4 simulations optimized for low-energy electron interactions are used. In this article, we present the performance of these simulations in reconstructing the electron spectra, measured with SDDs, of a $^{109}$Cd monochromatic source, both in vacuum and in air. The allowed $\\beta$ spectrum of a $^{14}$C source is also measured and analyzed, and it is shown that the experimental shape factor commonly used in the literature to reconstruct the measured spectrum is not necessary to explain the spectrum.","sentences":["The ASPECT-BET (An sdd-SPECTrometer for BETa decay studies) project aims to develop a novel technique for the precise measurement of forbidden $\\beta$ spectra in the 10 keV - 1 MeV range.","This technique uses a Silicon Drift Detector (SDD) as the main spectrometer, surrounded, if necessary, by a veto system to reject events with only partial energy deposition in the SDD.","Accurate knowledge of the spectrometer's response to electrons is essential to reconstruct the theoretical shape of the $\\beta$ spectrum.","To compute this response, GEANT4 simulations optimized for low-energy electron interactions are used.","In this article, we present the performance of these simulations in reconstructing the electron spectra, measured with SDDs, of a $^{109}$Cd monochromatic source, both in vacuum and in air.","The allowed $\\beta$ spectrum of a $^{14}$C source is also measured and analyzed, and it is shown that the experimental shape factor commonly used in the literature to reconstruct the measured spectrum is not necessary to explain the spectrum."],"url":"http://arxiv.org/abs/2405.07797v1","category":"physics.ins-det"}
{"created":"2024-05-13 14:38:35","title":"Optimal Matrix Sketching over Sliding Windows","abstract":"Matrix sketching, aimed at approximating a matrix $\\boldsymbol{A} \\in \\mathbb{R}^{N\\times d}$ consisting of vector streams of length $N$ with a smaller sketching matrix $\\boldsymbol{B} \\in \\mathbb{R}^{\\ell\\times d}, \\ell \\ll N$, has garnered increasing attention in fields such as large-scale data analytics and machine learning. A well-known deterministic matrix sketching method is the Frequent Directions algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound and provides a covariance error guarantee of $\\varepsilon = \\lVert \\boldsymbol{A}^\\top \\boldsymbol{A} - \\boldsymbol{B}^\\top \\boldsymbol{B} \\rVert_2/\\lVert \\boldsymbol{A} \\rVert_F^2$. The matrix sketching problem becomes particularly interesting in the context of sliding windows, where the goal is to approximate the matrix $\\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time units. However, despite recent efforts, whether achieving the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound on sliding windows is possible has remained an open question.   In this paper, we introduce the DS-FD algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound for matrix sketching over row-normalized, sequence-based sliding windows. We also present matching upper and lower space bounds for time-based and unnormalized sliding windows, demonstrating the generality and optimality of \\dsfd across various sliding window models. This conclusively answers the open question regarding the optimal space bound for matrix sketching over sliding windows. Furthermore, we conduct extensive experiments with both synthetic and real-world datasets, validating our theoretical claims and thus confirming the correctness and effectiveness of our algorithm, both theoretically and empirically.","sentences":["Matrix sketching, aimed at approximating a matrix $\\boldsymbol{A} \\in \\mathbb{R}^{N\\times d}$ consisting of vector streams of length $N$ with a smaller sketching matrix $\\boldsymbol{B} \\in \\mathbb{R}^{\\ell\\times d}, \\ell \\ll N$, has garnered increasing attention in fields such as large-scale data analytics and machine learning.","A well-known deterministic matrix sketching method is the Frequent Directions algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound and provides a covariance error guarantee of $\\varepsilon = \\lVert \\boldsymbol{A}^\\top \\boldsymbol{A} - \\boldsymbol{B}^\\top \\boldsymbol{B} \\rVert_2/\\lVert \\boldsymbol{A} \\rVert_F^2$. The matrix sketching problem becomes particularly interesting in the context of sliding windows, where the goal is to approximate the matrix $\\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time units.","However, despite recent efforts, whether achieving the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound on sliding windows is possible has remained an open question.   ","In this paper, we introduce the DS-FD algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound for matrix sketching over row-normalized, sequence-based sliding windows.","We also present matching upper and lower space bounds for time-based and unnormalized sliding windows, demonstrating the generality and optimality of \\dsfd across various sliding window models.","This conclusively answers the open question regarding the optimal space bound for matrix sketching over sliding windows.","Furthermore, we conduct extensive experiments with both synthetic and real-world datasets, validating our theoretical claims and thus confirming the correctness and effectiveness of our algorithm, both theoretically and empirically."],"url":"http://arxiv.org/abs/2405.07792v1","category":"cs.DB"}
{"created":"2024-05-13 14:14:12","title":"Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation","abstract":"Identifying optimal join orders (JOs) stands out as a key challenge in database research and engineering. Owing to the large search space, established classical methods rely on approximations and heuristics. Recent efforts have successfully explored reinforcement learning (RL) for JO. Likewise, quantum versions of RL have received considerable scientific attention. Yet, it is an open question if they can achieve sustainable, overall practical advantages with improved quantum processors.   In this paper, we present a novel approach that uses quantum reinforcement learning (QRL) for JO based on a hybrid variational quantum ansatz. It is able to handle general bushy join trees instead of resorting to simpler left-deep variants as compared to approaches based on quantum(-inspired) optimisation, yet requires multiple orders of magnitudes fewer qubits, which is a scarce resource even for post-NISQ systems.   Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities, which requires an evaluation by numerical simulations. While QRL may not significantly outperform classical approaches in solving the JO problem with respect to result quality (albeit we see parity), we find a drastic reduction in required trainable parameters. This benefits practically relevant aspects ranging from shorter training times compared to classical RL, less involved classical optimisation passes, or better use of available training data, and fits data-stream and low-latency processing scenarios. Our comprehensive evaluation and careful discussion delivers a balanced perspective on possible practical quantum advantage, provides insights for future systemic approaches, and allows for quantitatively assessing trade-offs of quantum approaches for one of the most crucial problems of database management systems.","sentences":["Identifying optimal join orders (JOs) stands out as a key challenge in database research and engineering.","Owing to the large search space, established classical methods rely on approximations and heuristics.","Recent efforts have successfully explored reinforcement learning (RL) for JO.","Likewise, quantum versions of RL have received considerable scientific attention.","Yet, it is an open question if they can achieve sustainable, overall practical advantages with improved quantum processors.   ","In this paper, we present a novel approach that uses quantum reinforcement learning (QRL) for JO based on a hybrid variational quantum ansatz.","It is able to handle general bushy join trees instead of resorting to simpler left-deep variants as compared to approaches based on quantum(-inspired) optimisation, yet requires multiple orders of magnitudes fewer qubits, which is a scarce resource even for post-NISQ systems.   ","Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities, which requires an evaluation by numerical simulations.","While QRL may not significantly outperform classical approaches in solving the JO problem with respect to result quality (albeit we see parity), we find a drastic reduction in required trainable parameters.","This benefits practically relevant aspects ranging from shorter training times compared to classical RL, less involved classical optimisation passes, or better use of available training data, and fits data-stream and low-latency processing scenarios.","Our comprehensive evaluation and careful discussion delivers a balanced perspective on possible practical quantum advantage, provides insights for future systemic approaches, and allows for quantitatively assessing trade-offs of quantum approaches for one of the most crucial problems of database management systems."],"url":"http://arxiv.org/abs/2405.07770v1","category":"quant-ph"}
{"created":"2024-05-13 14:00:02","title":"CAGES: Cost-Aware Gradient Entropy Search for Efficient Local Multi-Fidelity Bayesian Optimization","abstract":"Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions. An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality. One way to overcome this challenge is to focus on local BO methods that aim to efficiently learn gradients, which have shown strong empirical performance on a variety of high-dimensional problems including policy search in reinforcement learning (RL). However, current local BO methods assume access to only a single high-fidelity information source whereas, in many engineering and control problems, one has access to multiple cheaper approximations of the objective. We propose a novel algorithm, Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity black-box functions. CAGES makes no assumption about the relationship between different information sources, making it more flexible than other multi-fidelity methods. It also employs a new type of information-theoretic acquisition function, which enables systematic identification of samples that maximize the information gain about the unknown gradient per cost of the evaluation. We demonstrate CAGES can achieve significant performance improvements compared to other state-of-the-art methods on a variety of synthetic and benchmark RL problems.","sentences":["Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions.","An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality.","One way to overcome this challenge is to focus on local BO methods that aim to efficiently learn gradients, which have shown strong empirical performance on a variety of high-dimensional problems including policy search in reinforcement learning (RL).","However, current local BO methods assume access to only a single high-fidelity information source whereas, in many engineering and control problems, one has access to multiple cheaper approximations of the objective.","We propose a novel algorithm, Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity black-box functions.","CAGES makes no assumption about the relationship between different information sources, making it more flexible than other multi-fidelity methods.","It also employs a new type of information-theoretic acquisition function, which enables systematic identification of samples that maximize the information gain about the unknown gradient per cost of the evaluation.","We demonstrate CAGES can achieve significant performance improvements compared to other state-of-the-art methods on a variety of synthetic and benchmark RL problems."],"url":"http://arxiv.org/abs/2405.07760v1","category":"cs.LG"}
{"created":"2024-05-13 13:56:46","title":"Significant improvement in sensitivity of an anomalous Nernst heat flux sensor by composite structure","abstract":"Heat flux sensors (HFS) have attracted significant interest for their potential in managing waste heat efficiently. A recently proposed HFS, that works on the basis of the anomalous Nernst effect (ANE), offers several advantages in its simple structure leading to easy fabrication, low cost, and reduced thermal resistance. However, enhancing sensitivity through traditional material selection is now challenging due to a small number of materials satisfying the required coexistence of a large transverse Seebeck coefficient and low thermal conductivity. In this study, by utilizing composite structures and optimizing the device geometry, we have achieved a substantial improvement in the sensitivity of an ANE-based HFS. We developed composite structures comprised of a plastic substrate with an uneven surface and three-dimensional (3D) uneven TbCo films, fabricated using nanoimprint techniques and sputtering. This approach resulted in a sensitivity that is approximately four times greater than that observed in previous studies. Importantly, this method is independent of the material properties and can significantly enhance the sensitivity. Our findings could lead to the development of highly sensitive HFS devices and open new avenues for the fabrication of 3D devices.","sentences":["Heat flux sensors (HFS) have attracted significant interest for their potential in managing waste heat efficiently.","A recently proposed HFS, that works on the basis of the anomalous Nernst effect (ANE), offers several advantages in its simple structure leading to easy fabrication, low cost, and reduced thermal resistance.","However, enhancing sensitivity through traditional material selection is now challenging due to a small number of materials satisfying the required coexistence of a large transverse Seebeck coefficient and low thermal conductivity.","In this study, by utilizing composite structures and optimizing the device geometry, we have achieved a substantial improvement in the sensitivity of an ANE-based HFS.","We developed composite structures comprised of a plastic substrate with an uneven surface and three-dimensional (3D) uneven TbCo films, fabricated using nanoimprint techniques and sputtering.","This approach resulted in a sensitivity that is approximately four times greater than that observed in previous studies.","Importantly, this method is independent of the material properties and can significantly enhance the sensitivity.","Our findings could lead to the development of highly sensitive HFS devices and open new avenues for the fabrication of 3D devices."],"url":"http://arxiv.org/abs/2405.07758v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-13 13:55:19","title":"High-resolution dynamic consistency analysis of photonic time-delay reservoir computer","abstract":"We numerically investigate a time-delayed reservoir computer architecture based on a single mode laser diode with optical injection and optical feedback. Through a high-resolution parametric analysis, we reveal unforeseen regions of high dynamical consistency. We demonstrate furthermore that the best computing performance is not achieved at the edge of consistency as previously suggested in a coarser parametric analysis. This region of high consistency and optimal reservoir performances are highly sensitive to the data input modulation format","sentences":["We numerically investigate a time-delayed reservoir computer architecture based on a single mode laser diode with optical injection and optical feedback.","Through a high-resolution parametric analysis, we reveal unforeseen regions of high dynamical consistency.","We demonstrate furthermore that the best computing performance is not achieved at the edge of consistency as previously suggested in a coarser parametric analysis.","This region of high consistency and optimal reservoir performances are highly sensitive to the data input modulation format"],"url":"http://arxiv.org/abs/2405.07756v1","category":"physics.optics"}
{"created":"2024-05-13 13:38:40","title":"Optimal discrete Hardy-Rellich-Birman inequalities","abstract":"We prove sufficient conditions on a parameter sequence to determine optimal weights in inequalities for an integer power $\\ell$ of the discrete Laplacian on the half-line. By a concrete choice of the parameter sequence, we obtain explicit optimal discrete Rellich ($\\ell=2$) and Birman ($\\ell\\geq3$) weights. For $\\ell=1$, we rediscover the optimal Hardy weight of Keller-Pinchover-Pogorzelski. For $\\ell=2$, we improve upon the best known Rellich weights due to Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and Huang-Ye. For $\\ell\\geq3$, our main result proves a conjecture by Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and improves the discrete analogue of the classical Birman weight due to Huang-Ye to the optimal.","sentences":["We prove sufficient conditions on a parameter sequence to determine optimal weights in inequalities for an integer power $\\ell$ of the discrete Laplacian on the half-line.","By a concrete choice of the parameter sequence, we obtain explicit optimal discrete Rellich ($\\ell=2$) and Birman ($\\ell\\geq3$) weights.","For $\\ell=1$, we rediscover the optimal Hardy weight of Keller-Pinchover-Pogorzelski.","For $\\ell=2$, we improve upon the best known Rellich weights due to Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and Huang-Ye.","For $\\ell\\geq3$, our main result proves a conjecture by Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and improves the discrete analogue of the classical Birman weight due to Huang-Ye to the optimal."],"url":"http://arxiv.org/abs/2405.07742v1","category":"math.CA"}
{"created":"2024-05-13 13:34:42","title":"A Low-rank Projected Proximal Gradient Method for Spectral Compressed Sensing","abstract":"This paper presents a new approach to the recovery of a spectrally sparse signal (SSS) from partially observed entries, focusing on challenges posed by large-scale data and heavy noise environments. The SSS reconstruction can be formulated as a non-convex low-rank Hankel recovery problem. Traditional formulations for SSS recovery often suffer from reconstruction inaccuracies due to unequally weighted norms and over-relaxation of the Hankel structure in noisy conditions. Moreover, a critical limitation of standard proximal gradient (PG) methods for solving the optimization problem is their slow convergence. We overcome this by introducing a more accurate formulation and a Low-rank Projected Proximal Gradient (LPPG) method, designed to efficiently converge to stationary points through a two-step process. The first step involves a modified PG approach, allowing for a constant step size independent of signal size, which significantly accelerates the gradient descent phase. The second step employs a subspace projection strategy, optimizing within a low-rank matrix space to further decrease the objective function. Both steps of the LPPG method are meticulously tailored to exploit the intrinsic low-rank and Hankel structures of the problem, thereby enhancing computational efficiency. Our numerical simulations reveal a substantial improvement in both the efficiency and recovery accuracy of the LPPG method compared to existing benchmark algorithms. This performance gain is particularly pronounced in scenarios with significant noise, demonstrating the method's robustness and applicability to large-scale SSS recovery tasks.","sentences":["This paper presents a new approach to the recovery of a spectrally sparse signal (SSS) from partially observed entries, focusing on challenges posed by large-scale data and heavy noise environments.","The SSS reconstruction can be formulated as a non-convex low-rank Hankel recovery problem.","Traditional formulations for SSS recovery often suffer from reconstruction inaccuracies due to unequally weighted norms and over-relaxation of the Hankel structure in noisy conditions.","Moreover, a critical limitation of standard proximal gradient (PG) methods for solving the optimization problem is their slow convergence.","We overcome this by introducing a more accurate formulation and a Low-rank Projected Proximal Gradient (LPPG) method, designed to efficiently converge to stationary points through a two-step process.","The first step involves a modified PG approach, allowing for a constant step size independent of signal size, which significantly accelerates the gradient descent phase.","The second step employs a subspace projection strategy, optimizing within a low-rank matrix space to further decrease the objective function.","Both steps of the LPPG method are meticulously tailored to exploit the intrinsic low-rank and Hankel structures of the problem, thereby enhancing computational efficiency.","Our numerical simulations reveal a substantial improvement in both the efficiency and recovery accuracy of the LPPG method compared to existing benchmark algorithms.","This performance gain is particularly pronounced in scenarios with significant noise, demonstrating the method's robustness and applicability to large-scale SSS recovery tasks."],"url":"http://arxiv.org/abs/2405.07739v1","category":"eess.SP"}
{"created":"2024-05-13 13:30:43","title":"TOPress3D: 3D topology optimization with design-dependent pressure loads in MATLAB","abstract":"This paper introduces ``TOPress3D,\" a 3D topology optimization MATLAB code for structures subjected to design-dependent pressure loads. With a primary focus on pedagogical objectives, the code provides an easy learning experience, making it a valuable tool and practical gateway for newcomers, students, and researchers towards this topic. TOPress3D uses Darcy's law with a drainage term to link the given pressure load to design variables, which is converted to consistent nodal loads. Compliance minimization subjected to volume constraint optimization problems with pressure loads are solved. Load sensitivities arising due to design-dependent nature of the loads are evaluated using the adjoint-variable approach. The method of moving asymptotes is used to update the design variables. TOPress3D is constituted by six main parts. Each is described in detail. The code is also tailored to solve different problems. The robustness and success of the code are demonstrated while designing a few pressure load-bearing structures. The code is provided in Appendix B and is available with extensions in the supplementary material and publicly at \\url{https://github.com/PrabhatIn/TOPress3D}.","sentences":["This paper introduces ``TOPress3D,\" a 3D topology optimization MATLAB code for structures subjected to design-dependent pressure loads.","With a primary focus on pedagogical objectives, the code provides an easy learning experience, making it a valuable tool and practical gateway for newcomers, students, and researchers towards this topic.","TOPress3D uses Darcy's law with a drainage term to link the given pressure load to design variables, which is converted to consistent nodal loads.","Compliance minimization subjected to volume constraint optimization problems with pressure loads are solved.","Load sensitivities arising due to design-dependent nature of the loads are evaluated using the adjoint-variable approach.","The method of moving asymptotes is used to update the design variables.","TOPress3D is constituted by six main parts.","Each is described in detail.","The code is also tailored to solve different problems.","The robustness and success of the code are demonstrated while designing a few pressure load-bearing structures.","The code is provided in Appendix B and is available with extensions in the supplementary material and publicly at \\url{https://github.com/PrabhatIn/TOPress3D}."],"url":"http://arxiv.org/abs/2405.07733v1","category":"cs.CE"}
{"created":"2024-05-13 13:23:11","title":"Optimal bolometer transfer function deconvolution for CMB experiments through maximum likelihood mapmaking","abstract":"We revisit the impact of finite time responses of bolometric detectors used for deep observations of the cosmic microwave background (CMB). Until now, bolometer transfer functions have been accounted for through a two-step procedure by first deconvolving an estimate of their Fourier-space representation from the raw time-ordered data (TOD), and then averaging the deconvolved TOD into pixelized maps. However, for many experiments, including the Planck High Frequency Instrument (HFI), it is necessary to apply an additional low-pass filter to avoid an excessive noise boost, which leads to an asymmetric effective beam. In this paper we demonstrate that this effect can be avoided if the transfer function deconvolution and pixelization operations are performed simultaneously through integrated maximum likelihood mapmaking. The resulting algorithm is structurally identical to the artDeco algorithm introduced by Keih\\\"anen & Reinecke (2012) for beam deconvolution. We illustrate the relevance of this method with simulated Planck HFI 143 GHz data, and find that the resulting effective beam is both more symmetric than with the two-step procedure, resulting in a sky-averaged ellipticity that is 64 % lower, and an effective beam full-width-at-half-maximum (FWHM) that is 2.3 % smaller. Similar improvements are expected for any other bolometer-based CMB experiments with long time constants.","sentences":["We revisit the impact of finite time responses of bolometric detectors used for deep observations of the cosmic microwave background (CMB).","Until now, bolometer transfer functions have been accounted for through a two-step procedure by first deconvolving an estimate of their Fourier-space representation from the raw time-ordered data (TOD), and then averaging the deconvolved TOD into pixelized maps.","However, for many experiments, including the Planck High Frequency Instrument (HFI), it is necessary to apply an additional low-pass filter to avoid an excessive noise boost, which leads to an asymmetric effective beam.","In this paper we demonstrate that this effect can be avoided if the transfer function deconvolution and pixelization operations are performed simultaneously through integrated maximum likelihood mapmaking.","The resulting algorithm is structurally identical to the artDeco algorithm introduced by Keih\\\"anen & Reinecke (2012) for beam deconvolution.","We illustrate the relevance of this method with simulated Planck HFI 143 GHz data, and find that the resulting effective beam is both more symmetric than with the two-step procedure, resulting in a sky-averaged ellipticity that is 64 % lower, and an effective beam full-width-at-half-maximum (FWHM) that is 2.3 % smaller.","Similar improvements are expected for any other bolometer-based CMB experiments with long time constants."],"url":"http://arxiv.org/abs/2405.07729v1","category":"astro-ph.CO"}
{"created":"2024-05-13 13:21:35","title":"Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing","abstract":"Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with all persona statements. Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation. This paper presents a pioneering exploration to quantify PRP faithfulness as a fine-grained and explainable criterion, which also serves as a reliable reference for optimization. Our criterion first discriminates persona statements into active and passive constraints by identifying the query-statement relevance. Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints. We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of natural language inference (NLI) scores weighted by relevance scores. In practice, we build the APC scoring system by symbolically distilling small discriminators from GPT-4 for efficiency. We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation. We further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations. We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques. We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion.","sentences":["Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with all persona statements.","Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation.","This paper presents a pioneering exploration to quantify PRP faithfulness as a fine-grained and explainable criterion, which also serves as a reliable reference for optimization.","Our criterion first discriminates persona statements into active and passive constraints by identifying the query-statement relevance.","Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints.","We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of natural language inference (NLI) scores weighted by relevance scores.","In practice, we build the APC scoring system by symbolically distilling small discriminators from GPT-4 for efficiency.","We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation.","We further leverage it as a reward system in direct preference optimization (DPO) for better AI characters.","Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations.","We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques.","We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion."],"url":"http://arxiv.org/abs/2405.07726v1","category":"cs.CL"}
{"created":"2024-05-13 13:15:20","title":"High-frequency Optimally Windowed Chirp rheometry for rapidly evolving viscoelastic materials: application to a crosslinking thermoset","abstract":"Abstract   Knowledge of the evolution of mechanical properties of the curing matrix is of great importance in composite parts or structure fabrication. Conventional rheometry, based on small amplitude oscillatory shear is limited by long interrogation times. In rapidly evolving materials, time sweeps can provide a meaningful measurement albeit at a single frequency. To overcome this constraint we utilize a combined frequency and amplitude-modulated chirped strain waveform in conjunction with a home-made sliding plate piezo-operated (PZR) and a dual-head commercial rotational rheometer (Anton Paar MCR 702) to probe the linear viscoelasticity of these time-evolving materials. The direct controllability of the PZR resulting from the absence of any kind of firmware and the microsecond actuator-sensor response renders this device ideal for exploring the advantages of this technique. The high frequency capability allows us to extend the upper limits of the accessible linear viscoelastic spectrum and most importantly, to shorten the length of the interrogating strain signal (OWCh-PZR) to sub-second scales, while retaining a high time-bandwidth product. This short duration ensures that the mutation number (NMu) is kept sufficiently low, even in fast curing resins. The method is validated via calibration tests in both instruments and the corresponding limitations are discussed. As a proof of concept the technique is applied to a curing vinylester resin. The linear viscoelastic (LVE) spectrum is assessed every 20 seconds to monitor the rapid evolution of the time- and frequency-dependence of the complex modulus. Finally, FTIR spectroscopy is utilized to gain insights on the evolution of the chemical network while the gap-dependence of the evolving material properties in these heterogeneous systems is also investigated.","sentences":["Abstract   Knowledge of the evolution of mechanical properties of the curing matrix is of great importance in composite parts or structure fabrication.","Conventional rheometry, based on small amplitude oscillatory shear is limited by long interrogation times.","In rapidly evolving materials, time sweeps can provide a meaningful measurement albeit at a single frequency.","To overcome this constraint we utilize a combined frequency and amplitude-modulated chirped strain waveform in conjunction with a home-made sliding plate piezo-operated (PZR) and a dual-head commercial rotational rheometer (Anton Paar MCR 702) to probe the linear viscoelasticity of these time-evolving materials.","The direct controllability of the PZR resulting from the absence of any kind of firmware and the microsecond actuator-sensor response renders this device ideal for exploring the advantages of this technique.","The high frequency capability allows us to extend the upper limits of the accessible linear viscoelastic spectrum and most importantly, to shorten the length of the interrogating strain signal (OWCh-PZR) to sub-second scales, while retaining a high time-bandwidth product.","This short duration ensures that the mutation number (NMu) is kept sufficiently low, even in fast curing resins.","The method is validated via calibration tests in both instruments and the corresponding limitations are discussed.","As a proof of concept the technique is applied to a curing vinylester resin.","The linear viscoelastic (LVE) spectrum is assessed every 20 seconds to monitor the rapid evolution of the time- and frequency-dependence of the complex modulus.","Finally, FTIR spectroscopy is utilized to gain insights on the evolution of the chemical network while the gap-dependence of the evolving material properties in these heterogeneous systems is also investigated."],"url":"http://arxiv.org/abs/2405.07721v1","category":"cond-mat.soft"}
{"created":"2024-05-13 13:14:01","title":"Symmetric Clifford twirling for cost-optimal quantum error mitigation in early FTQC regime","abstract":"Twirling noise affecting quantum gates is essential in understanding and controlling errors, but applicable operations to noise are usually restricted by symmetries inherent in quantum gates. In this Letter, we propose symmetric Clifford twirling, a Clifford twirling utilizing only symmetric Clifford operators that commute with certain Pauli subgroups. We fully characterize how each Pauli noise is converted through the twirling and show that certain Pauli noise can be scrambled to a noise exponentially close to the global white noise. We further demonstrate that the effective noise of some highly structured circuits, such as Trotterized Hamiltonian simulation circuits, is scrambled to global white noise, and even a single use of CNOT gate can significantly accelerate the scrambling. These findings enable us to mitigate errors in non-Clifford operations with minimal sampling overhead in the early stages of fault-tolerant quantum computing, where executing non-Clifford operations is expected to be significantly more challenging than Clifford operations. Furthermore, they offer new insights into various fields of physics where randomness and symmetry play crucial roles.","sentences":["Twirling noise affecting quantum gates is essential in understanding and controlling errors, but applicable operations to noise are usually restricted by symmetries inherent in quantum gates.","In this Letter, we propose symmetric Clifford twirling, a Clifford twirling utilizing only symmetric Clifford operators that commute with certain Pauli subgroups.","We fully characterize how each Pauli noise is converted through the twirling and show that certain Pauli noise can be scrambled to a noise exponentially close to the global white noise.","We further demonstrate that the effective noise of some highly structured circuits, such as Trotterized Hamiltonian simulation circuits, is scrambled to global white noise, and even a single use of CNOT gate can significantly accelerate the scrambling.","These findings enable us to mitigate errors in non-Clifford operations with minimal sampling overhead in the early stages of fault-tolerant quantum computing, where executing non-Clifford operations is expected to be significantly more challenging than Clifford operations.","Furthermore, they offer new insights into various fields of physics where randomness and symmetry play crucial roles."],"url":"http://arxiv.org/abs/2405.07720v1","category":"quant-ph"}
{"created":"2024-05-13 13:02:50","title":"Joint Robotic Aerial Base Station Deployment and Wireless Backhauling in 6G Multi-hop Networks","abstract":"Due to their ability to anchor into tall urban landforms, such as lampposts or street lights, robotic aerial base stations (RABSs) can create a hyper-flexible wireless multi-hop heterogeneous network to meet the forthcoming green, densified, and dynamic network deployment to support, inter alia, high data rates. In this work, we propose a network infrastructure that can concurrently support the wireless backhaul link capacity and access link traffic demand in the millimeter-wave (mmWave) frequency band. The RABSs grasping locations, resource blocks (RBs) assignment, and route flow control are simultaneously optimized to maximize the served traffic demands. Robotic base stations capitalize on the fact that traffic distribution varies considerably across both time and space within a given geographical area. Hence, they are able to relocate to suitable locations, i.e., 'follow' the traffic demand as it unfolds to increase the overall network efficiency. To tackle the curse of dimensionality of the proposed mixed-integer linear problem, we propose a greedy algorithm to obtain a competitive solution with low computational complexity. Compared to baseline models, which are heterogeneous networks with randomly deployed fixed small cells and pre-allocated RBs for wireless access and backhaul links, a wide set of numerical investigations reveals that robotic base stations could improve the served traffic demand. Specifically, the proposed mode serves at most 65\\% more traffic demand compared to an equal number of deployed fixed small cells.","sentences":["Due to their ability to anchor into tall urban landforms, such as lampposts or street lights, robotic aerial base stations (RABSs) can create a hyper-flexible wireless multi-hop heterogeneous network to meet the forthcoming green, densified, and dynamic network deployment to support, inter alia, high data rates.","In this work, we propose a network infrastructure that can concurrently support the wireless backhaul link capacity and access link traffic demand in the millimeter-wave (mmWave) frequency band.","The RABSs grasping locations, resource blocks (RBs) assignment, and route flow control are simultaneously optimized to maximize the served traffic demands.","Robotic base stations capitalize on the fact that traffic distribution varies considerably across both time and space within a given geographical area.","Hence, they are able to relocate to suitable locations, i.e., 'follow' the traffic demand as it unfolds to increase the overall network efficiency.","To tackle the curse of dimensionality of the proposed mixed-integer linear problem, we propose a greedy algorithm to obtain a competitive solution with low computational complexity.","Compared to baseline models, which are heterogeneous networks with randomly deployed fixed small cells and pre-allocated RBs for wireless access and backhaul links, a wide set of numerical investigations reveals that robotic base stations could improve the served traffic demand.","Specifically, the proposed mode serves at most 65\\% more traffic demand compared to an equal number of deployed fixed small cells."],"url":"http://arxiv.org/abs/2405.07714v1","category":"cs.NI"}
{"created":"2024-05-13 12:55:56","title":"Waste Factor and Waste Figure: A Unified Theory for Modeling and Analyzing Wasted Power in Radio Access Networks for Improved Sustainability","abstract":"This paper introduces Waste Factor (W), also denoted as Waste Figure (WF) in dB, a promising new metric for quantifying energy efficiency in a wide range of circuits and systems applications, including data centers and RANs. Also, the networks used to connect data centers and AI computing engines with users for ML applications must become more power efficient. This paper illustrates the limitations of existing energy efficiency metrics that inadequately capture the intricate energy dynamics of RAN components. We delineate the methodology for applying W across various network configurations, including MISO, SIMO, and MIMO systems, and demonstrate the effectiveness of W in identifying energy optimization opportunities. Our findings reveal that W not only offers nuanced insights into the energy performance of RANs but also facilitates informed decision-making for network design and operational efficiency. Furthermore, we show how W can be integrated with other KPIs to guide the development of optimal strategies for enhancing network energy efficiency under different operational conditions. Additionally, we present simulation results for a distributed multi-user MIMO system at 3.5, 17, and 28 GHz, demonstrating overall network power efficiency on a per square kilometer basis, and show how overall W decreases with an increasing number of base stations and increasing carrier frequency. This paper shows that adopting W as a figure of merit can significantly contribute to the sustainability and energy optimization of next-generation wireless communication networks, paving the way for greener and more sustainable, energy-efficient 5G and 6G technologies.","sentences":["This paper introduces Waste Factor (W), also denoted as Waste Figure (WF) in dB, a promising new metric for quantifying energy efficiency in a wide range of circuits and systems applications, including data centers and RANs.","Also, the networks used to connect data centers and AI computing engines with users for ML applications must become more power efficient.","This paper illustrates the limitations of existing energy efficiency metrics that inadequately capture the intricate energy dynamics of RAN components.","We delineate the methodology for applying W across various network configurations, including MISO, SIMO, and MIMO systems, and demonstrate the effectiveness of W in identifying energy optimization opportunities.","Our findings reveal that W not only offers nuanced insights into the energy performance of RANs but also facilitates informed decision-making for network design and operational efficiency.","Furthermore, we show how W can be integrated with other KPIs to guide the development of optimal strategies for enhancing network energy efficiency under different operational conditions.","Additionally, we present simulation results for a distributed multi-user MIMO system at 3.5, 17, and 28 GHz, demonstrating overall network power efficiency on a per square kilometer basis, and show how overall W decreases with an increasing number of base stations and increasing carrier frequency.","This paper shows that adopting W as a figure of merit can significantly contribute to the sustainability and energy optimization of next-generation wireless communication networks, paving the way for greener and more sustainable, energy-efficient 5G and 6G technologies."],"url":"http://arxiv.org/abs/2405.07710v1","category":"cs.NI"}
{"created":"2024-05-13 12:55:03","title":"Ultrafast Structured Spin-Manipulation of Relativistic Lepton Beams","abstract":"Relativistic spin-polarized (SP) lepton beams are important for investigating spin-dependent interaction processes. In particular, spatially structured spin-polarized (SSP) lepton beams may find new applications in material, atomic, nuclear, high-energy physics and new physics beyond the Standard Model. However, realizing ultrafast generation and spin-manipulation of relativistic SSP lepton beams pose significant challenges. Here, we put forward a novel method of ultrafast (picosecond-timescale) generation of a relativistic SSP lepton beam via employing a moderate terahertz (THz) wave in a dielectric-lined waveguide (DWL). We first find that lepton beams with customizable spin-polarization structures can be generated by utilizing different electromagnetic modes, and optimizing the lepton velocity and THz phase velocity can improve efficiency of spin-manipulation and visibility of the SP structure. These SSP beams play a profound role in studying magnetic effects in material physics, chiral-selective chemistry, generation of structured $\\gamma$-rays, etc., and open a new avenue for research on relativistic SP particles.","sentences":["Relativistic spin-polarized (SP) lepton beams are important for investigating spin-dependent interaction processes.","In particular, spatially structured spin-polarized (SSP) lepton beams may find new applications in material, atomic, nuclear, high-energy physics and new physics beyond the Standard Model.","However, realizing ultrafast generation and spin-manipulation of relativistic SSP lepton beams pose significant challenges.","Here, we put forward a novel method of ultrafast (picosecond-timescale) generation of a relativistic SSP lepton beam via employing a moderate terahertz (THz) wave in a dielectric-lined waveguide (DWL).","We first find that lepton beams with customizable spin-polarization structures can be generated by utilizing different electromagnetic modes, and optimizing the lepton velocity and THz phase velocity can improve efficiency of spin-manipulation and visibility of the SP structure.","These SSP beams play a profound role in studying magnetic effects in material physics, chiral-selective chemistry, generation of structured $\\gamma$-rays, etc., and open a new avenue for research on relativistic SP particles."],"url":"http://arxiv.org/abs/2405.07709v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 12:21:59","title":"Highly Efficient Observation Process based on FFT Filtering for Robot Swarm Collaborative Navigation in Unknown Environments","abstract":"Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem. This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm. This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues. We treat sensors' environmental observations as a digital sampling process. Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data. The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision. The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level. The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations.","sentences":["Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem.","This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm.","This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues.","We treat sensors' environmental observations as a digital sampling process.","Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data.","The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision.","The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level.","The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations."],"url":"http://arxiv.org/abs/2405.07687v1","category":"cs.RO"}
{"created":"2024-05-13 12:21:36","title":"Pole trajectories of the $\u039b(1405)$ helps establish its dynamical nature","abstract":"Chiral trajectories of dynamically generated resonances are connected to the SU(3) breaking pattern and their nature. From an analysis of a recent LQCD simulation on the $\\pi\\Sigma-\\bar{K}N$ scattering for $I=0$, and the study of the quark mass dependence of the octet baryons, we determine for the first time the trajectory of the two poles associated to the $\\Lambda(1405)$ towards the symmetric point $(\\mathrm{Tr}[M]=\\mathrm{cte})$ accurately. Our result at unphysical pion mass is consistent with the lattice simulation at $m_\\pi\\simeq 200$ MeV and the extrapolation to the physical point, based on the NLO chiral lagrangian, agrees perfectly well with previous analyses of experimental data. Contrary to other works, we predict qualitatively similar trajectories at LO and up to NLO, being consistent with the dominance of the LO interaction. At the SU(3) symmetric point up to NLO, we obtain that the lower pole is located at $E^{(1)}=1595\\pm8$ MeV, being a singlet representation, while the higher pole belongs to the octet with a mass $E^{(8)}=1600\\pm4$ MeV. This can be tested in the future LQCD simulations.","sentences":["Chiral trajectories of dynamically generated resonances are connected to the SU(3) breaking pattern and their nature.","From an analysis of a recent LQCD simulation on the $\\pi\\Sigma-\\bar{K}N$ scattering for $I=0$, and the study of the quark mass dependence of the octet baryons, we determine for the first time the trajectory of the two poles associated to the $\\Lambda(1405)$ towards the symmetric point $(\\mathrm{Tr}[M]=\\mathrm{cte})$ accurately.","Our result at unphysical pion mass is consistent with the lattice simulation at $m_\\pi\\simeq 200$ MeV and the extrapolation to the physical point, based on the NLO chiral lagrangian, agrees perfectly well with previous analyses of experimental data.","Contrary to other works, we predict qualitatively similar trajectories at LO and up to NLO, being consistent with the dominance of the LO interaction.","At the SU(3) symmetric point up to NLO, we obtain that the lower pole is located at $E^{(1)}=1595\\pm8$ MeV, being a singlet representation, while the higher pole belongs to the octet with a mass $E^{(8)}=1600\\pm4$ MeV. This can be tested in the future LQCD simulations."],"url":"http://arxiv.org/abs/2405.07686v1","category":"hep-ph"}
{"created":"2024-05-13 12:19:12","title":"Constructive reachability for linear control problems under conic constraints","abstract":"Motivated by applications requiring sparse or nonnegative controls, we investigate reachability properties of linear infinite-dimensional control problems under conic constraints. Relaxing the problem to convex constraints if the initial cone is not already convex, we provide a constructive approach based on minimising a properly defined dual functional, which covers both the approximate and exact reachability problems. Our main results heavily rely on convex analysis, Fenchel duality and the Fenchel-Rockafellar theorem. As a byproduct, we uncover new sufficient conditions for approximate and exact reachability under convex conic constraints. We also prove that these conditions are in fact necessary. When the constraints are nonconvex, our method leads to sufficient conditions ensuring that the constructed controls fulfill the original constraints, which is in the flavour of bang-bang type properties. We show that our approach encompasses and generalises several works, and we obtain new results for different types of conic constraints and control systems.","sentences":["Motivated by applications requiring sparse or nonnegative controls, we investigate reachability properties of linear infinite-dimensional control problems under conic constraints.","Relaxing the problem to convex constraints if the initial cone is not already convex, we provide a constructive approach based on minimising a properly defined dual functional, which covers both the approximate and exact reachability problems.","Our main results heavily rely on convex analysis, Fenchel duality and the Fenchel-Rockafellar theorem.","As a byproduct, we uncover new sufficient conditions for approximate and exact reachability under convex conic constraints.","We also prove that these conditions are in fact necessary.","When the constraints are nonconvex, our method leads to sufficient conditions ensuring that the constructed controls fulfill the original constraints, which is in the flavour of bang-bang type properties.","We show that our approach encompasses and generalises several works, and we obtain new results for different types of conic constraints and control systems."],"url":"http://arxiv.org/abs/2405.07684v1","category":"math.OC"}
{"created":"2024-05-13 11:59:44","title":"Duality-based single-level reformulations of bilevel optimization problems","abstract":"Usually, bilevel optimization problems need to be transformed into single-level ones in order to derive optimality conditions and solution algorithms. Among the available approaches, the replacement of the lower-level problem by means of duality relations became popular quite recently. We revisit three realizations of this idea which are based on the lower-level Lagrange, Wolfe, and Mond--Weir dual problem. The resulting single-level surrogate problems are equivalent to the original bilevel optimization problem from the viewpoint of global minimizers under mild assumptions. However, all these reformulations suffer from the appearance of so-called implicit variables, i.e., surrogate variables which do not enter the objective function but appear in the feasible set for modeling purposes. Treating implicit variables as explicit ones has been shown to be problematic when locally optimal solutions, stationary points, and applicable constraint qualifications are compared to the original problem. Indeed, we illustrate that the same difficulties have to be faced when using these duality-based reformulations. Furthermore, we show that the Mangasarian-Fromovitz constraint qualification is likely to be violated at each feasible point of these reformulations, contrasting assertions in some recently published papers.","sentences":["Usually, bilevel optimization problems need to be transformed into single-level ones in order to derive optimality conditions and solution algorithms.","Among the available approaches, the replacement of the lower-level problem by means of duality relations became popular quite recently.","We revisit three realizations of this idea which are based on the lower-level Lagrange, Wolfe, and Mond--Weir dual problem.","The resulting single-level surrogate problems are equivalent to the original bilevel optimization problem from the viewpoint of global minimizers under mild assumptions.","However, all these reformulations suffer from the appearance of so-called implicit variables, i.e., surrogate variables which do not enter the objective function but appear in the feasible set for modeling purposes.","Treating implicit variables as explicit ones has been shown to be problematic when locally optimal solutions, stationary points, and applicable constraint qualifications are compared to the original problem.","Indeed, we illustrate that the same difficulties have to be faced when using these duality-based reformulations.","Furthermore, we show that the Mangasarian-Fromovitz constraint qualification is likely to be violated at each feasible point of these reformulations, contrasting assertions in some recently published papers."],"url":"http://arxiv.org/abs/2405.07672v1","category":"math.OC"}
{"created":"2024-05-13 11:45:58","title":"Partial information decomposition as information bottleneck","abstract":"The partial information decomposition (PID) aims to quantify the amount of redundant information that a set of sources provide about a target. Here we show that this goal can be formulated as a type of information bottleneck (IB) problem, which we term the \"redundancy bottleneck\" (RB). The RB formalizes a tradeoff between prediction and compression: it extracts information from the sources that predicts the target, without revealing which source provided the information. It can be understood as a generalization \"Blackwell redundancy\", which we previously proposed as a principled measure of PID redundancy. The \"RB curve\" quantifies the prediction/compression tradeoff at multiple scales. This curve can also be quantified for individual sources, allowing subsets of redundant sources to be identified without combinatorial optimization. We provide an efficient iterative algorithm for computing the RB curve.","sentences":["The partial information decomposition (PID) aims to quantify the amount of redundant information that a set of sources provide about a target.","Here we show that this goal can be formulated as a type of information bottleneck (IB) problem, which we term the \"redundancy bottleneck\" (RB).","The RB formalizes a tradeoff between prediction and compression: it extracts information from the sources that predicts the target, without revealing which source provided the information.","It can be understood as a generalization \"Blackwell redundancy\", which we previously proposed as a principled measure of PID redundancy.","The \"RB curve\" quantifies the prediction/compression tradeoff at multiple scales.","This curve can also be quantified for individual sources, allowing subsets of redundant sources to be identified without combinatorial optimization.","We provide an efficient iterative algorithm for computing the RB curve."],"url":"http://arxiv.org/abs/2405.07665v1","category":"cs.IT"}
{"created":"2024-05-13 11:14:40","title":"Arrow of Time in Estimation and Control: Duality Theory Beyond the Linear Gaussian Model","abstract":"Duality between estimation and control is a foundational concept in Control Theory. Most students learn about the elementary duality -- between observability and controllability -- in their first graduate course in linear systems theory. Therefore, it comes as a surprise that for a more general class of nonlinear stochastic systems (hidden Markov models or HMMs), duality is incomplete.   Our objective in writing this article is two-fold: (i) To describe the difficulty in extending duality to HMMs; and (ii) To discuss its recent resolution by the authors. A key message is that the main difficulty in extending duality comes from time reversal in going from estimation to control. The reason for time reversal is explained with the aid of the familiar linear deterministic and linear Gaussian models. The explanation is used to motivate the difference between the linear and the nonlinear models. Once the difference is understood, duality for HMMs is described based on our recent work. The article also includes a comparison and discussion of the different types of duality considered in literature.","sentences":["Duality between estimation and control is a foundational concept in Control Theory.","Most students learn about the elementary duality -- between observability and controllability -- in their first graduate course in linear systems theory.","Therefore, it comes as a surprise that for a more general class of nonlinear stochastic systems (hidden Markov models or HMMs), duality is incomplete.   ","Our objective in writing this article is two-fold: (i) To describe the difficulty in extending duality to HMMs; and (ii) To discuss its recent resolution by the authors.","A key message is that the main difficulty in extending duality comes from time reversal in going from estimation to control.","The reason for time reversal is explained with the aid of the familiar linear deterministic and linear Gaussian models.","The explanation is used to motivate the difference between the linear and the nonlinear models.","Once the difference is understood, duality for HMMs is described based on our recent work.","The article also includes a comparison and discussion of the different types of duality considered in literature."],"url":"http://arxiv.org/abs/2405.07650v1","category":"math.OC"}
{"created":"2024-05-13 10:51:01","title":"Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback","abstract":"In many real-world applications, it is hard to provide a reward signal in each step of a Reinforcement Learning (RL) process and more natural to give feedback when an episode ends. To this end, we study the recently proposed model of RL with Aggregate Bandit Feedback (RL-ABF), where the agent only observes the sum of rewards at the end of an episode instead of each reward individually. Prior work studied RL-ABF only in tabular settings, where the number of states is assumed to be small. In this paper, we extend ABF to linear function approximation and develop two efficient algorithms with near-optimal regret guarantees: a value-based optimistic algorithm built on a new randomization technique with a Q-functions ensemble, and a policy optimization algorithm that uses a novel hedging scheme over the ensemble.","sentences":["In many real-world applications, it is hard to provide a reward signal in each step of a Reinforcement Learning (RL) process and more natural to give feedback when an episode ends.","To this end, we study the recently proposed model of RL with Aggregate Bandit Feedback (RL-ABF), where the agent only observes the sum of rewards at the end of an episode instead of each reward individually.","Prior work studied RL-ABF only in tabular settings, where the number of states is assumed to be small.","In this paper, we extend ABF to linear function approximation and develop two efficient algorithms with near-optimal regret guarantees: a value-based optimistic algorithm built on a new randomization technique with a Q-functions ensemble, and a policy optimization algorithm that uses a novel hedging scheme over the ensemble."],"url":"http://arxiv.org/abs/2405.07637v2","category":"cs.LG"}
{"created":"2024-05-13 10:48:03","title":"Nonlinear Network Identifiability with Full Excitations","abstract":"We derive conditions for the identifiability of nonlinear networks characterized by additive dynamics at the level of the edges when all the nodes are excited. In contrast to linear systems, we show that the measurement of all sinks is necessary and sufficient for the identifiability of directed acyclic graphs, under the assumption that dynamics are described by analytic functions without constant terms (i.e., $f(0)=0$). But if constant terms are present, then the identifiability is impossible as soon as one node has more than one in-neighbor. In the case of general digraphs where cycles can exist, we consider additively separable functions for the analysis of the identifiability, and we show that the measurement of one node of all the sinks of the condensation digraph is necessary and sufficient. Several examples are added to illustrate the results.","sentences":["We derive conditions for the identifiability of nonlinear networks characterized by additive dynamics at the level of the edges when all the nodes are excited.","In contrast to linear systems, we show that the measurement of all sinks is necessary and sufficient for the identifiability of directed acyclic graphs, under the assumption that dynamics are described by analytic functions without constant terms (i.e., $f(0)=0$).","But if constant terms are present, then the identifiability is impossible as soon as one node has more than one in-neighbor.","In the case of general digraphs where cycles can exist, we consider additively separable functions for the analysis of the identifiability, and we show that the measurement of one node of all the sinks of the condensation digraph is necessary and sufficient.","Several examples are added to illustrate the results."],"url":"http://arxiv.org/abs/2405.07636v1","category":"math.OC"}
{"created":"2024-05-13 10:47:31","title":"Piecewise omnigenous stellarators","abstract":"In omnigeneous magnetic fields, charged particles are perfectly confined in the absence of collisions and turbulence. For this reason, the magnetic configuration is optimized to be close to omnigenity in any candidate for a stellarator fusion reactor. However, approaching omnigenity imposes severe constraints on the spatial variation of the magnetic field. In particular, the topology of the contours of constant magnetic-field-strength on each magnetic surface must be such that there are no particles transitioning between different types of wells. This, in turn, usually leads to complicated plasma shapes and coils. This Letter presents a new family of optimized fields that display tokamak-like collisional energy transport while having transitioning particles. This result radically broadens the space of accessible reactor-relevant configurations.","sentences":["In omnigeneous magnetic fields, charged particles are perfectly confined in the absence of collisions and turbulence.","For this reason, the magnetic configuration is optimized to be close to omnigenity in any candidate for a stellarator fusion reactor.","However, approaching omnigenity imposes severe constraints on the spatial variation of the magnetic field.","In particular, the topology of the contours of constant magnetic-field-strength on each magnetic surface must be such that there are no particles transitioning between different types of wells.","This, in turn, usually leads to complicated plasma shapes and coils.","This Letter presents a new family of optimized fields that display tokamak-like collisional energy transport while having transitioning particles.","This result radically broadens the space of accessible reactor-relevant configurations."],"url":"http://arxiv.org/abs/2405.07634v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 10:38:05","title":"Substitutability, equilibrium transport, and matching models","abstract":"This chapter explores the role of substitutability in economic models, particularly in the context of optimal transport and matching models. In equilibrium models with substitutability, market-clearing prices can often be recovered using coordinate update methods such as Jacobi's algorithm. We provide a detailed mathematical analysis of models with substitutability through the lens of Z- and M-functions, in particular regarding their role in ensuring the convergence of Jacobi's algorithm. The chapter proceeds by studying matching models using substitutability, first focusing on models with (imperfectly) transferable utility, and then on models with non-transferable utility. In both cases, the text reviews theoretical implications as well as computational approaches (Sinkhorn, Gale--Shapley), and highlights a practical economic application.","sentences":["This chapter explores the role of substitutability in economic models, particularly in the context of optimal transport and matching models.","In equilibrium models with substitutability, market-clearing prices can often be recovered using coordinate update methods such as Jacobi's algorithm.","We provide a detailed mathematical analysis of models with substitutability through the lens of Z- and M-functions, in particular regarding their role in ensuring the convergence of Jacobi's algorithm.","The chapter proceeds by studying matching models using substitutability, first focusing on models with (imperfectly) transferable utility, and then on models with non-transferable utility.","In both cases, the text reviews theoretical implications as well as computational approaches (Sinkhorn, Gale--Shapley), and highlights a practical economic application."],"url":"http://arxiv.org/abs/2405.07628v1","category":"econ.TH"}
{"created":"2024-05-13 10:35:50","title":"Analytical lower bound on the number of queries to a black-box unitary operation in deterministic exact transformations of unknown unitary operations","abstract":"Several counter-intuitive go-theorems have recently been shown for transformations of unknown unitary operations; deterministic and exact complex conjugation, inversion, and transposition of a general $d$-dimensional unknown unitary operation are implementable with a finite number of queries of the black-box unitary operation. However, the minimum numbers of the required queries are not known except for $d=2$ unitary inversion and unitary transposition (numerical) and unitary conjugation (analytic). In this work, we derive complementary no-go theorems for deterministic and exact implementations of inversion and transposition of a $d$-dimensional unknown unitary operation under certain numbers of queries. The obtained no-go theorem indicates that the analytical lower bound of the number of queries for unitary inversion is $d^2$ and that for unitary transposition is $4$ for $d=2$ and $d+3$ for $d \\geq 3$. We have developed a new framework that utilizes differentiation to obtain the analytical lower bounds on the number of queries to the black-box unitary operation required to implement a transformation given by a general differentiable function mapping a unitary operation to another unitary operation, which reproduces the lower bound of the number of queries for unitary complex conjugation $d-1$. As a corollary, we show the relationship between the tightness of the lower bounds and the existence of optimal catalytic transformations, which is a new aspect recently identified in the study of deterministic and exact unitary inversion. Furthermore, we extend our framework to the probabilistic setting where the transformation is required to succeed with a certain probability, thereby showing a possible tradeoff relation between query numbers and the required success probability.","sentences":["Several counter-intuitive go-theorems have recently been shown for transformations of unknown unitary operations; deterministic and exact complex conjugation, inversion, and transposition of a general $d$-dimensional unknown unitary operation are implementable with a finite number of queries of the black-box unitary operation.","However, the minimum numbers of the required queries are not known except for $d=2$ unitary inversion and unitary transposition (numerical) and unitary conjugation (analytic).","In this work, we derive complementary no-go theorems for deterministic and exact implementations of inversion and transposition of a $d$-dimensional unknown unitary operation under certain numbers of queries.","The obtained no-go theorem indicates that the analytical lower bound of the number of queries for unitary inversion is $d^2$ and that for unitary transposition is $4$ for $d=2$ and $d+3$ for $d \\geq 3$.","We have developed a new framework that utilizes differentiation to obtain the analytical lower bounds on the number of queries to the black-box unitary operation required to implement a transformation given by a general differentiable function mapping a unitary operation to another unitary operation, which reproduces the lower bound of the number of queries for unitary complex conjugation $d-1$. As a corollary, we show the relationship between the tightness of the lower bounds and the existence of optimal catalytic transformations, which is a new aspect recently identified in the study of deterministic and exact unitary inversion.","Furthermore, we extend our framework to the probabilistic setting where the transformation is required to succeed with a certain probability, thereby showing a possible tradeoff relation between query numbers and the required success probability."],"url":"http://arxiv.org/abs/2405.07625v1","category":"quant-ph"}
{"created":"2024-05-13 10:35:23","title":"Towards Robust Benchmarking of Quantum Optimization Algorithms","abstract":"Benchmarking the performance of quantum optimization algorithms is crucial for identifying utility for industry-relevant use cases. Benchmarking processes vary between optimization applications and depend on user-specified goals. The heuristic nature of quantum algorithms poses challenges, especially when comparing to classical counterparts. A key problem in existing benchmarking frameworks is the lack of equal effort in optimizing for the best quantum and, respectively, classical approaches. This paper presents a comprehensive set of guidelines comprising universal steps towards fair benchmarks. We discuss (1) application-specific algorithm choice, ensuring every solver is provided with the most fitting mathematical formulation of a problem; (2) the selection of benchmark data, including hard instances and real-world samples; (3) the choice of a suitable holistic figure of merit, like time-to-solution or solution quality within time constraints; and (4) equitable hyperparameter training to eliminate bias towards a particular method. The proposed guidelines are tested across three benchmarking scenarios, utilizing the Max-Cut (MC) and Travelling Salesperson Problem (TSP). The benchmarks employ classical mathematical algorithms, such as Branch-and-Cut (BNC) solvers, classical heuristics, Quantum Annealing (QA), and the Quantum Approximate Optimization Algorithm (QAOA).","sentences":["Benchmarking the performance of quantum optimization algorithms is crucial for identifying utility for industry-relevant use cases.","Benchmarking processes vary between optimization applications and depend on user-specified goals.","The heuristic nature of quantum algorithms poses challenges, especially when comparing to classical counterparts.","A key problem in existing benchmarking frameworks is the lack of equal effort in optimizing for the best quantum and, respectively, classical approaches.","This paper presents a comprehensive set of guidelines comprising universal steps towards fair benchmarks.","We discuss (1) application-specific algorithm choice, ensuring every solver is provided with the most fitting mathematical formulation of a problem; (2) the selection of benchmark data, including hard instances and real-world samples; (3) the choice of a suitable holistic figure of merit, like time-to-solution or solution quality within time constraints; and (4) equitable hyperparameter training to eliminate bias towards a particular method.","The proposed guidelines are tested across three benchmarking scenarios, utilizing the Max-Cut (MC) and Travelling Salesperson Problem (TSP).","The benchmarks employ classical mathematical algorithms, such as Branch-and-Cut (BNC) solvers, classical heuristics, Quantum Annealing (QA), and the Quantum Approximate Optimization Algorithm (QAOA)."],"url":"http://arxiv.org/abs/2405.07624v1","category":"quant-ph"}
{"created":"2024-05-13 10:30:33","title":"COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via Nonlinear Integer Programming","abstract":"For language model classification, would you prefer having only one workable class or having every class working? The latter makes more practical uses. Especially for large language models (LLMs), the fact that they achieve a fair overall accuracy by in-context learning (ICL) obscures a large difference in individual class accuracies. In this work, we uncover and tackle language models' imbalance in per-class prediction accuracy by reconceptualizing it as the Contextual Oddity Bias (COBias), and we are the first to engage nonlinear integer programming (NIP) to debias it. Briefly, COBias refers to the difference in accuracy by a class A compared to its ''odd'' class, which holds the majority wrong predictions of class A. With the COBias metric, we reveal that LLMs of varied scales and families exhibit large per-class accuracy differences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class probabilities for lower bias and higher overall accuracy. Our optimization objective is directly based on the evaluation scores by COBias and accuracy metrics, solved by simulated annealing. Evaluations on three LLMs across seven NLP classification tasks show that DNIP simultaneously achieves significant COBias reduction ($-27\\%$) and accuracy improvement ($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise class accuracy differences is a direction in pushing forward more accurate, more reliable LLM predictions.","sentences":["For language model classification, would you prefer having only one workable class or having every class working?","The latter makes more practical uses.","Especially for large language models (LLMs), the fact that they achieve a fair overall accuracy by in-context learning (ICL) obscures a large difference in individual class accuracies.","In this work, we uncover and tackle language models' imbalance in per-class prediction accuracy by reconceptualizing it as the Contextual Oddity Bias (COBias), and we are the first to engage nonlinear integer programming (NIP) to debias it.","Briefly, COBias refers to the difference in accuracy by a class A compared to its ''odd'' class, which holds the majority wrong predictions of class A. With the COBias metric, we reveal that LLMs of varied scales and families exhibit large per-class accuracy differences.","Then we propose Debiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class probabilities for lower bias and higher overall accuracy.","Our optimization objective is directly based on the evaluation scores by COBias and accuracy metrics, solved by simulated annealing.","Evaluations on three LLMs across seven NLP classification tasks show that DNIP simultaneously achieves significant COBias reduction ($-27\\%$) and accuracy improvement ($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise class accuracy differences is a direction in pushing forward more accurate, more reliable LLM predictions."],"url":"http://arxiv.org/abs/2405.07623v1","category":"cs.CL"}
{"created":"2024-05-13 10:19:25","title":"FNCC: Fast Notification Congestion Control in Data Center Networks","abstract":"Congestion control plays a pivotal role in large-scale data centers, facilitating ultra-low latency, high bandwidth, and optimal utilization. Even with the deployment of data center congestion control mechanisms such as DCQCN and HPCC, these algorithms often respond to congestion sluggishly. This sluggishness is primarily due to the slow notification of congestion. It takes almost one round-trip time (RTT) for the congestion information to reach the sender. In this paper, we introduce the Fast Notification Congestion Control (FNCC) mechanism, which achieves sub-RTT notification. FNCC leverages the acknowledgment packet (ACK) from the return path to carry in-network telemetry (INT) information of the request path, offering the sender more timely and accurate INT. To further accelerate the responsiveness of last-hop congestion control, we propose that the receiver notifies the sender of the number of concurrent congested flows, which can be used to adjust the congested flows to a fair rate quickly. Our experimental results demonstrate that FNCC reduces flow completion time by 27.4% and 88.9% compared to HPCC and DCQCN, respectively. Moreover, FNCC triggers minimal pause frames and maintains high utilization even at 400Gbps.","sentences":["Congestion control plays a pivotal role in large-scale data centers, facilitating ultra-low latency, high bandwidth, and optimal utilization.","Even with the deployment of data center congestion control mechanisms such as DCQCN and HPCC, these algorithms often respond to congestion sluggishly.","This sluggishness is primarily due to the slow notification of congestion.","It takes almost one round-trip time (RTT) for the congestion information to reach the sender.","In this paper, we introduce the Fast Notification Congestion Control (FNCC) mechanism, which achieves sub-RTT notification.","FNCC leverages the acknowledgment packet (ACK) from the return path to carry in-network telemetry (INT) information of the request path, offering the sender more timely and accurate INT.","To further accelerate the responsiveness of last-hop congestion control, we propose that the receiver notifies the sender of the number of concurrent congested flows, which can be used to adjust the congested flows to a fair rate quickly.","Our experimental results demonstrate that FNCC reduces flow completion time by 27.4% and 88.9% compared to HPCC and DCQCN, respectively.","Moreover, FNCC triggers minimal pause frames and maintains high utilization even at 400Gbps."],"url":"http://arxiv.org/abs/2405.07608v1","category":"cs.NI"}
{"created":"2024-05-13 09:59:59","title":"Transferable Neural Wavefunctions for Solids","abstract":"Deep-Learning-based Variational Monte Carlo (DL-VMC) has recently emerged as a highly accurate approach for finding approximate solutions to the many-electron Schr\\\"odinger equation. Despite its favorable scaling with the number of electrons, $\\mathcal{O}(n_\\text{el}^{4})$, the practical value of DL-VMC is limited by the high cost of optimizing the neural network weights for every system studied. To mitigate this problem, recent research has proposed optimizing a single neural network across multiple systems, reducing the cost per system. Here we extend this approach to solids, where similar but distinct calculations using different geometries, boundary conditions, and supercell sizes are often required. We show how to optimize a single ansatz across all of these variations, reducing the required number of optimization steps by an order of magnitude. Furthermore, we exploit the transfer capabilities of a pre-trained network. We successfully transfer a network, pre-trained on 2x2x2 supercells of LiH, to 3x3x3 supercells. This reduces the number of optimization steps required to simulate the large system by a factor of 50 compared to previous work.","sentences":["Deep-Learning-based Variational Monte Carlo (DL-VMC) has recently emerged as a highly accurate approach for finding approximate solutions to the many-electron Schr\\\"odinger equation.","Despite its favorable scaling with the number of electrons, $\\mathcal{O}(n_\\text{el}^{4})$, the practical value of DL-VMC is limited by the high cost of optimizing the neural network weights for every system studied.","To mitigate this problem, recent research has proposed optimizing a single neural network across multiple systems, reducing the cost per system.","Here we extend this approach to solids, where similar but distinct calculations using different geometries, boundary conditions, and supercell sizes are often required.","We show how to optimize a single ansatz across all of these variations, reducing the required number of optimization steps by an order of magnitude.","Furthermore, we exploit the transfer capabilities of a pre-trained network.","We successfully transfer a network, pre-trained on 2x2x2 supercells of LiH, to 3x3x3 supercells.","This reduces the number of optimization steps required to simulate the large system by a factor of 50 compared to previous work."],"url":"http://arxiv.org/abs/2405.07599v1","category":"physics.comp-ph"}
