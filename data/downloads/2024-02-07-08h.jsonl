{"created":"2024-02-06 18:59:57","title":"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls","abstract":"We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.","sentences":["We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries.","We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries.","AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable.","AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules.","We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.","By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench.","Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.","For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench.","Code will be available at https://github.com/dyabel/AnyTool."],"url":"http://arxiv.org/abs/2402.04253v1","category":"cs.CL"}
{"created":"2024-02-06 18:59:30","title":"Linear-time Minimum Bayes Risk Decoding with Reference Aggregation","abstract":"Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr","sentences":["Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used.","Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity.","In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations.","This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding.","We release our source code at https://github.com/ZurichNLP/mbr"],"url":"http://arxiv.org/abs/2402.04251v1","category":"cs.CL"}
{"created":"2024-02-06 18:59:08","title":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal","abstract":"Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.","sentences":["Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods.","To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming.","We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria.","Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights.","We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses.","We open source HarmBench at https://github.com/centerforaisafety/HarmBench."],"url":"http://arxiv.org/abs/2402.04249v1","category":"cs.LG"}
{"created":"2024-02-06 18:54:07","title":"Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science","abstract":"Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.","sentences":["Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines.","While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety.","However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities.","This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures.","We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment.","Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works.","Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks.","Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively."],"url":"http://arxiv.org/abs/2402.04247v1","category":"cs.CY"}
{"created":"2024-02-06 18:53:58","title":"Theory of Supervibronic Transitions via Casimir Polaritons","abstract":"A remote energy transfer pathway from electronic to vibrational degrees of freedom is identified inside an infrared optical cavity under vibrational strong coupling conditions. This mechanism relies on the dynamical Casimir effect, whereby real infrared photons are generated due to a sudden electronic transition of molecules. Moreover, the formation of vibrational polaritons enables the excited photon energy to be transferred to the vibrational degrees of freedom before any dissipation occurs. Both analytic solutions and numerical simulations reveal that the magnitude of this electronic to vibrational energy transfer depends quadratically on the number of molecules and resonantly on the vibration-cavity detuning. During this \"supervibronic\" transition process, because the vibrational energy gain per molecule can be meaningful in the macroscopic limit, this process may potentially be observed using conventional vibrational strong coupling devices at room temperature.","sentences":["A remote energy transfer pathway from electronic to vibrational degrees of freedom is identified inside an infrared optical cavity under vibrational strong coupling conditions.","This mechanism relies on the dynamical Casimir effect, whereby real infrared photons are generated due to a sudden electronic transition of molecules.","Moreover, the formation of vibrational polaritons enables the excited photon energy to be transferred to the vibrational degrees of freedom before any dissipation occurs.","Both analytic solutions and numerical simulations reveal that the magnitude of this electronic to vibrational energy transfer depends quadratically on the number of molecules and resonantly on the vibration-cavity detuning.","During this \"supervibronic\" transition process, because the vibrational energy gain per molecule can be meaningful in the macroscopic limit, this process may potentially be observed using conventional vibrational strong coupling devices at room temperature."],"url":"http://arxiv.org/abs/2402.04246v1","category":"quant-ph"}
{"created":"2024-02-06 18:52:40","title":"The spectrum of excisive functors","abstract":"We prove a thick subcategory theorem for the category of $d$-excisive functors from finite spectra to spectra. This generalizes the Hopkins-Smith thick subcategory theorem (the $d=1$ case) and the $C_2$-equivariant thick subcategory theorem (the $d=2$ case). We obtain our classification theorem by completely computing the Balmer spectrum of compact $d$-excisive functors. A key ingredient is a non-abelian blueshift theorem for the generalized Tate construction associated to the family of non-transitive subgroups of products of symmetric groups. Also important are the techniques of tensor triangular geometry and striking analogies between functor calculus and equivariant homotopy theory. In particular, we introduce a functor calculus analogue of the Burnside ring and describe its Zariski spectrum \\`{a} la Dress. The analogy with equivariant homotopy theory is strengthened further through two applications: We explain the effect of changing coefficients from spectra to ${\\mathrm{H}\\mathbb{Z}}$-modules and we establish a functor calculus analogue of transchromatic Smith-Floyd theory as developed by Kuhn-Lloyd. Our work offers a new perspective on functor calculus which builds upon the previous approaches of Arone-Ching and Glasman.","sentences":["We prove a thick subcategory theorem for the category of $d$-excisive functors from finite spectra to spectra.","This generalizes the Hopkins-Smith thick subcategory theorem (the $d=1$ case) and the $C_2$-equivariant thick subcategory theorem (the $d=2$ case).","We obtain our classification theorem by completely computing the Balmer spectrum of compact $d$-excisive functors.","A key ingredient is a non-abelian blueshift theorem for the generalized Tate construction associated to the family of non-transitive subgroups of products of symmetric groups.","Also important are the techniques of tensor triangular geometry and striking analogies between functor calculus and equivariant homotopy theory.","In particular, we introduce a functor calculus analogue of the Burnside ring and describe its Zariski spectrum \\`{a} la Dress.","The analogy with equivariant homotopy theory is strengthened further through two applications: We explain the effect of changing coefficients from spectra to ${\\mathrm{H}\\mathbb{Z}}$-modules and we establish a functor calculus analogue of transchromatic Smith-Floyd theory as developed by Kuhn-Lloyd.","Our work offers a new perspective on functor calculus which builds upon the previous approaches of Arone-Ching and Glasman."],"url":"http://arxiv.org/abs/2402.04244v1","category":"math.AT"}
{"created":"2024-02-06 18:51:41","title":"Exact weights and path metrics for triangulated categories and the derived category of persistence modules","abstract":"We define exact weights on a triangulated category to be nonnegative functions on objects satisfying a subadditivity condition with respect to exact triangles. Such weights induce a metric on objects in the triangulated category, which we call a path metric. Our exact weights generalize the rank functions of J.\\ Chuang and A.\\ Lazarev and are analogous to the exact weights for an exact category given by the first author and J.\\ Scott and D.\\ Stanley. We show that cohomological functors from a triangulated category to an abelian category with an additive weight induce an exact weight on the triangulated category. We prove that triangle equivalences induce an isometry for the path metrics induced by cohomological functors. In the perfectly generated or compactly generated case, we use Brown representability to express the exact weight on the triangulated category. We give three characterizations of exactness for a weight on a triangulated category and show that they are equivalent. We also define Wasserstein distances for triangulated categories. Finally, we apply our work to derived categories of persistence modules and to representations of continuous quivers of type $\\mathbb{A}$.","sentences":["We define exact weights on a triangulated category to be nonnegative functions on objects satisfying a subadditivity condition with respect to exact triangles.","Such weights induce a metric on objects in the triangulated category, which we call a path metric.","Our exact weights generalize the rank functions of J.\\ Chuang and A.\\ Lazarev and are analogous to the exact weights for an exact category given by the first author and J.\\ Scott and D.\\ Stanley.","We show that cohomological functors from a triangulated category to an abelian category with an additive weight induce an exact weight on the triangulated category.","We prove that triangle equivalences induce an isometry for the path metrics induced by cohomological functors.","In the perfectly generated or compactly generated case, we use Brown representability to express the exact weight on the triangulated category.","We give three characterizations of exactness for a weight on a triangulated category and show that they are equivalent.","We also define Wasserstein distances for triangulated categories.","Finally, we apply our work to derived categories of persistence modules and to representations of continuous quivers of type $\\mathbb{A}$."],"url":"http://arxiv.org/abs/2402.04242v1","category":"math.CT"}
{"created":"2024-02-06 18:47:52","title":"CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers","abstract":"The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster. We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers.","sentences":["The Transformer architecture has shown to be a powerful tool for a wide range of tasks.","It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers.","In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers.","CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries.","The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence.","CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster.","We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers."],"url":"http://arxiv.org/abs/2402.04239v1","category":"cs.LG"}
{"created":"2024-02-06 18:43:48","title":"CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations","abstract":"Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at https://github.com/THUDM/CogCoM.","sentences":["Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers.","However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses.","In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in).","This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths.","We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism.","Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance.","The code and data are publicly available at https://github.com/THUDM/CogCoM."],"url":"http://arxiv.org/abs/2402.04236v1","category":"cs.CV"}
{"created":"2024-02-06 18:41:31","title":"Role of spontaneously generated coherence (SGC) in laser cooling of atoms","abstract":"The well-known sub-Doppler polarization gradient cooling in type-I transition ($F_e=F_g+1$) is caused by red-detuned lasers. On the other hand, in type-II transition ($F_e\\le F_g$), sub-Doppler cooling takes place through blue-detuned lasers. This opposite behavior for the two types of transitions is due to SGC. In the absence of SGC, both types of transitions show blue-detuned cooling. In this work, we experimentally and theoretically demonstrate blue-detuned cooling for both types of transitions in $^{\\textrm{87}}$Rb. For completeness, we compare the temperatures in various configurations.","sentences":["The well-known sub-Doppler polarization gradient cooling in type-I transition ($F_e=F_g+1$) is caused by red-detuned lasers.","On the other hand, in type-II transition ($F_e\\le F_g$), sub-Doppler cooling takes place through blue-detuned lasers.","This opposite behavior for the two types of transitions is due to SGC.","In the absence of SGC, both types of transitions show blue-detuned cooling.","In this work, we experimentally and theoretically demonstrate blue-detuned cooling for both types of transitions in $^{\\textrm{87}}$Rb.","For completeness, we compare the temperatures in various configurations."],"url":"http://arxiv.org/abs/2402.04234v1","category":"physics.atom-ph"}
{"created":"2024-02-06 18:39:43","title":"Can Generative Agents Predict Emotion?","abstract":"Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.","sentences":["Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans.","In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories.","Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation.","First, the agent perceives new experiences as time series text data.","After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm.","Through this comparison we can analyse how the agent reacts to the new experience in context.","The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event.","Finally, the new experience is then added to the agents memory to be used in the creation of future norms.","By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios.","The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary.","We hope that this paper is another step towards the alignment of generative agents."],"url":"http://arxiv.org/abs/2402.04232v1","category":"cs.AI"}
{"created":"2024-02-06 18:39:25","title":"Further Constructions of AMUBs for Non-prime power Composite Dimensions","abstract":"Construction of a large class of Mutually Unbiased Bases (MUBs) for non-prime power composite dimensions ($d = k\\times s$) is a long standing open problem, which leads to different construction methods for the class Approximate MUBs (AMUBs) by relaxing the criterion that the absolute value of the dot product between two vectors chosen from different bases should be $\\leq \\frac{\\beta}{\\sqrt{d}}$. In this chapter, we consider a more general class of AMUBs (ARMUBs, considering the real ones too), compared to our earlier work in [Cryptography and Communications, 14(3): 527--549, 2022]. We note that the quality of AMUBs (ARMUBs) constructed using RBD$(X,A)$ with $|X|= d$, critically depends on the parameters, $|s-k|$, $\\mu$ (maximum number of elements common between any pair of blocks), and the set of block sizes. We present the construction of $\\mathcal{O}(\\sqrt{d})$ many $\\beta$-AMUBs for composite $d$ when $|s-k|< \\sqrt{d}$, using RBDs having block sizes approximately $\\sqrt{d}$, such that $|\\braket{\\psi^l_i|\\psi^m_j}| \\leq \\frac{\\beta}{\\sqrt{d}}$ where $\\beta = 1 + \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1}) \\leq 2$. Moreover, if real Hadamard matrix of order $k$ or $s$ exists, then one can construct at least $N(k)+1$ (or $N(s)+1$) many $\\beta$-ARMUBs for dimension $d$, with $\\beta \\leq 2 - \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1})< 2$, where $N(w)$ is the number of MOLS$(w)$. This improves and generalizes some of our previous results for ARMUBs from two points, viz., the real cases are now extended to complex ones too. The earlier efforts use some existing RBDs, whereas here we consider new instances of RBDs that provide better results. Similar to the earlier cases, the AMUBs (ARMUBs) constructed using RBDs are in general very sparse, where the sparsity $(\\epsilon)$ is $1 - \\mathcal{O}(d^{-\\frac{1}{2}})$.","sentences":["Construction of a large class of Mutually Unbiased Bases (MUBs) for non-prime power composite dimensions ($d = k\\times s$) is a long standing open problem, which leads to different construction methods for the class Approximate MUBs (AMUBs) by relaxing the criterion that the absolute value of the dot product between two vectors chosen from different bases should be $\\leq \\frac{\\beta}{\\sqrt{d}}$.","In this chapter, we consider a more general class of AMUBs (ARMUBs, considering the real ones too), compared to our earlier work in [Cryptography and Communications, 14(3): 527--549, 2022].","We note that the quality of AMUBs (ARMUBs) constructed using RBD$(X,A)$ with $|X|= d$, critically depends on the parameters, $|s-k|$, $\\mu$ (maximum number of elements common between any pair of blocks), and the set of block sizes.","We present the construction of $\\mathcal{O}(\\sqrt{d})$ many $\\beta$-AMUBs for composite $d$ when $|s-k|< \\sqrt{d}$, using RBDs having block sizes approximately $\\sqrt{d}$, such that $|\\braket{\\psi^l_i|\\psi^m_j}| \\leq \\frac{\\beta}{\\sqrt{d}}$ where $\\beta = 1 + \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1})","\\leq 2$.","Moreover, if real Hadamard matrix of order $k$ or $s$ exists, then one can construct at least $N(k)+1$ (or $N(s)+1$) many $\\beta$-ARMUBs for dimension $d$, with $\\beta \\leq 2 - \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1})< 2$, where $N(w)$ is the number of MOLS$(w)$. This improves and generalizes some of our previous results for ARMUBs from two points, viz., the real cases are now extended to complex ones too.","The earlier efforts use some existing RBDs, whereas here we consider new instances of RBDs that provide better results.","Similar to the earlier cases, the AMUBs (ARMUBs) constructed using RBDs are in general very sparse, where the sparsity $(\\epsilon)$ is $1 - \\mathcal{O}(d^{-\\frac{1}{2}})$."],"url":"http://arxiv.org/abs/2402.04231v1","category":"cs.DM"}
{"created":"2024-02-06 18:37:14","title":"Medium Resolution 0.97-5.3 micron spectra of Very Young Benchmark Brown Dwarfs with NIRSpec onboard the James Webb Space Telescope","abstract":"Spectra of young benchmark brown dwarfs with well-known ages are vital to characterize other brown dwarfs, for which ages are in general not known. These spectra are also crucial to test atmospheric models which have the potential to provide detailed information about the atmospheres of these objects. However, to optimally test atmospheric models, medium-resolution, long-wavelength coverage spectra with well-understood uncertainties are ideal, such as the spectra provided by the NIRSpec instrument onboard the James Webb Space Telescope. In this paper, we present the medium-resolution JWST/NIRSpec spectra of two young brown dwarfs, TWA 28 (M9.0) and TWA 27A (M9.0), and one planetary-mass object, TWA 27B (L6.0), members of the TW Hydrae Association (~10 Myr). We show the richness of the atomic lines and molecular bands present in the spectra. All objects show signs of a circumstellar disk, via near-infrared excess and/or via emission lines. We matched a set of cloudless atmospheric spectra (ATMO), and cloudy atmospheric spectra (BT-Settl) to our NIRSpec spectra, and analyzed which wavelength ranges and spectral features both models reproduce best. Both models derive consistent parameters for the three sources, and predict the existence of CH4 at 3.35 microns in TWA 27B. Nonetheless, in contrast to other slightly older objects with similar spectral type, like PSO 318.5-22 and VHS 1256b, this feature is not present in the spectrum of TWA 27B. The lack of the CH4 feature might suggest that the L/T transition of very young dwarfs starts at later spectral types than for older brown dwarfs.","sentences":["Spectra of young benchmark brown dwarfs with well-known ages are vital to characterize other brown dwarfs, for which ages are in general not known.","These spectra are also crucial to test atmospheric models which have the potential to provide detailed information about the atmospheres of these objects.","However, to optimally test atmospheric models, medium-resolution, long-wavelength coverage spectra with well-understood uncertainties are ideal, such as the spectra provided by the NIRSpec instrument onboard the James Webb Space Telescope.","In this paper, we present the medium-resolution JWST/NIRSpec spectra of two young brown dwarfs, TWA 28 (M9.0) and TWA 27A (M9.0), and one planetary-mass object, TWA 27B (L6.0), members of the TW Hydrae Association (~10 Myr).","We show the richness of the atomic lines and molecular bands present in the spectra.","All objects show signs of a circumstellar disk, via near-infrared excess and/or via emission lines.","We matched a set of cloudless atmospheric spectra (ATMO), and cloudy atmospheric spectra (BT-Settl) to our NIRSpec spectra, and analyzed which wavelength ranges and spectral features both models reproduce best.","Both models derive consistent parameters for the three sources, and predict the existence of CH4 at 3.35 microns in TWA 27B.","Nonetheless, in contrast to other slightly older objects with similar spectral type, like PSO 318.5-22 and VHS 1256b, this feature is not present in the spectrum of TWA 27B.","The lack of the CH4 feature might suggest that the L/T transition of very young dwarfs starts at later spectral types than for older brown dwarfs."],"url":"http://arxiv.org/abs/2402.04230v1","category":"astro-ph.SR"}
{"created":"2024-02-06 18:36:52","title":"MusicRL: Aligning Music Generation to Human Preferences","abstract":"We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.","sentences":["We propose MusicRL, the first music generation system finetuned from human feedback.","Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat).","Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning.","MusicRL is a pretrained autoregressive MusicLM","(Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards.","We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences.","Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale.","Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline.","Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters.","Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it.","This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models."],"url":"http://arxiv.org/abs/2402.04229v1","category":"cs.LG"}
{"created":"2024-02-06 18:36:44","title":"Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models","abstract":"Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator. The school behavior of fish is usually a good inspiration to design control architecture for swarm robots. In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots. In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces. Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment. Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments.","sentences":["Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator.","The school behavior of fish is usually a good inspiration to design control architecture for swarm robots.","In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots.","In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces.","Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment.","Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats.","Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments."],"url":"http://arxiv.org/abs/2402.04228v1","category":"cs.RO"}
{"created":"2024-02-06 18:36:11","title":"A short proof of Frobenius for generic fibrations","abstract":"We give a simple diagrammatic proof of the Frobenius property for generic fibrations, that does not depend on any additional structure on the interval object such as connections.","sentences":["We give a simple diagrammatic proof of the Frobenius property for generic fibrations, that does not depend on any additional structure on the interval object such as connections."],"url":"http://arxiv.org/abs/2402.04227v1","category":"math.CT"}
{"created":"2024-02-06 18:34:34","title":"Performance of entanglement purification including maximally entangled mixed states","abstract":"Entanglement between distant quantum systems is a critical resource for implementing quantum communication. This property is affected by external agents and can be restored by employing efficient entanglement purification protocols. In this work, we propose an entanglement purification protocol based on two entangling two-qubit operations that replace the usual controlled-NOT (CNOT) gate. These operations arise from a generalized quantum measurement and can be understood as measurement operators in a positive operator-valued measure (POVM). Furthermore, two variants of the core protocol are introduced and shown to be more practical in certain scenarios. The performance of the protocols is studied in terms of the overall success probability of reaching a Bell state and the number of purifiable states. Based on rank-two states, we can obtain analytical expressions for the success probability that we extend and refine using numerical calculations to the case of maximally entangled states (MEMS). We also consider more general rank-three states to show that our procedure is in general more convenient compared to purification protocols based on Bell diagonal states. Finally, we test the protocols using initial random states. In all cases, we find a larger performance and larger amount of purifiable states using our schemes compared to the CNOT-based purification protocol.","sentences":["Entanglement between distant quantum systems is a critical resource for implementing quantum communication.","This property is affected by external agents and can be restored by employing efficient entanglement purification protocols.","In this work, we propose an entanglement purification protocol based on two entangling two-qubit operations that replace the usual controlled-NOT (CNOT) gate.","These operations arise from a generalized quantum measurement and can be understood as measurement operators in a positive operator-valued measure (POVM).","Furthermore, two variants of the core protocol are introduced and shown to be more practical in certain scenarios.","The performance of the protocols is studied in terms of the overall success probability of reaching a Bell state and the number of purifiable states.","Based on rank-two states, we can obtain analytical expressions for the success probability that we extend and refine using numerical calculations to the case of maximally entangled states (MEMS).","We also consider more general rank-three states to show that our procedure is in general more convenient compared to purification protocols based on Bell diagonal states.","Finally, we test the protocols using initial random states.","In all cases, we find a larger performance and larger amount of purifiable states using our schemes compared to the CNOT-based purification protocol."],"url":"http://arxiv.org/abs/2402.04226v1","category":"quant-ph"}
{"created":"2024-02-06 18:34:19","title":"Growth rate of self-sustained QED cascades induced by intense lasers","abstract":"It was suggested [A. R. Bell & J. G. Kirk, PRL 101, 200403 (2008)] that an avalanche of electron-positron pairs can be triggered in the laboratory by a standing wave generated by intense laser fields. Here, we present a general solution to the long-standing problem of the avalanche growth rate calculation. We provide a simple formula that we apply to the case of the standing wave created by two circularly polarized lasers and demonstrate that it allows to predict the particle yield for the full range of intensity able to generate an avalanche. We account for the damping of the growth rate due to pair migration from the region of prolific generation and show that above a threshold in intensity, this effect is negligible. The growth rate calculation allows us to predict when abundant pair production will induce a back-reaction on the generating field due to plasma collective effects and screening. Our model shows excellent agreement with self-consistent PIC simulations and can be applied to study the generation of electron-positron pair avalanches in realistic field configurations to plan future experiments at ultra-high-intensity laser facilities.","sentences":["It was suggested [A. R. Bell & J. G. Kirk, PRL 101, 200403 (2008)] that an avalanche of electron-positron pairs can be triggered in the laboratory by a standing wave generated by intense laser fields.","Here, we present a general solution to the long-standing problem of the avalanche growth rate calculation.","We provide a simple formula that we apply to the case of the standing wave created by two circularly polarized lasers and demonstrate that it allows to predict the particle yield for the full range of intensity able to generate an avalanche.","We account for the damping of the growth rate due to pair migration from the region of prolific generation and show that above a threshold in intensity, this effect is negligible.","The growth rate calculation allows us to predict when abundant pair production will induce a back-reaction on the generating field due to plasma collective effects and screening.","Our model shows excellent agreement with self-consistent PIC simulations and can be applied to study the generation of electron-positron pair avalanches in realistic field configurations to plan future experiments at ultra-high-intensity laser facilities."],"url":"http://arxiv.org/abs/2402.04225v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 18:29:39","title":"What is 'Typological Diversity' in NLP?","abstract":"The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of 'typological diversity' that empirically justifies the diversity of language samples.","sentences":["The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP.","However, these improvements only apply to a small subset of the world's languages.","Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages.","To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages.","These selections are often described as being 'typologically diverse'.","In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'.","We find there are no set definitions or criteria for such claims.","We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers.","Furthermore, we show that skewed language selection can lead to overestimated multilingual performance.","We recommend future work to include an operationalization of 'typological diversity' that empirically justifies the diversity of language samples."],"url":"http://arxiv.org/abs/2402.04222v1","category":"cs.CL"}
{"created":"2024-02-06 18:20:47","title":"Geometric theory of (extended) time-reversal symmetries in stochastic processes -- Part I: finite dimension","abstract":"In this article, we analyze three classes of time-reversal of a Markov process with Gaussian noise on a manifold. We first unveil a commutativity constraint for the most general of these time-reversals to be well defined. Then we give a triad of necessary and sufficient conditions for the stochastic process to be time-reversible. While most reversibility conditions in the literature require knowledge of the stationary probability, our conditions do not, and therefore can be analytically checked in a systematic way. We then show that the mathematical objects whose cancellation is required by our reversibility conditions play the role of independent sources of entropy production. Furthermore, we give a geometric interpretation of the so-called irreversible cycle-affinity as the vorticity of a certain vector field for a Riemannian geometry given by the diffusion tensor. We also discuss the relation between the time-reversability of the stochastic process and that of an associated deterministic dynamics: its Stratonovitch average. Finally, we show that a suitable choice of a reference measure - that can be considered as a prior or a gauge, depending on the context - allows to study a stochastic process in a way that is both coordinate-free and independent of the prescription used to define stochastic integrals. When this reference measure plays the role of a gauge choice, we interpret our previous results through the lens of gauge theory and prove them to be gauge-invariant.","sentences":["In this article, we analyze three classes of time-reversal of a Markov process with Gaussian noise on a manifold.","We first unveil a commutativity constraint for the most general of these time-reversals to be well defined.","Then we give a triad of necessary and sufficient conditions for the stochastic process to be time-reversible.","While most reversibility conditions in the literature require knowledge of the stationary probability, our conditions do not, and therefore can be analytically checked in a systematic way.","We then show that the mathematical objects whose cancellation is required by our reversibility conditions play the role of independent sources of entropy production.","Furthermore, we give a geometric interpretation of the so-called irreversible cycle-affinity as the vorticity of a certain vector field for a Riemannian geometry given by the diffusion tensor.","We also discuss the relation between the time-reversability of the stochastic process and that of an associated deterministic dynamics: its Stratonovitch average.","Finally, we show that a suitable choice of a reference measure - that can be considered as a prior or a gauge, depending on the context - allows to study a stochastic process in a way that is both coordinate-free and independent of the prescription used to define stochastic integrals.","When this reference measure plays the role of a gauge choice, we interpret our previous results through the lens of gauge theory and prove them to be gauge-invariant."],"url":"http://arxiv.org/abs/2402.04217v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-06 18:14:25","title":"Universal distribution of the number of minima for random walks and L\u00e9vy flights","abstract":"We compute exactly the full distribution of the number $m$ of local minima in a one-dimensional landscape generated by a random walk or a L\\'evy flight. We consider two different ensembles of landscapes, one with a fixed number of steps $N$ and the other till the first-passage time of the random walk to the origin. We show that the distribution of $m$ is drastically different in the two ensembles (Gaussian in the former case, while having a power-law tail in the latter $m^{-3/2}$ in the latter case). However, the most striking aspect of our results is that, in each case, the distribution is completely universal for all $m$ (and not just for large $m$), i.e., independent of the jump distribution in the random walk. This means that the distributions are exactly identical for L\\'evy flights and random walks with finite jump variance. Our analytical results are in excellent agreement with our numerical simulations.","sentences":["We compute exactly the full distribution of the number $m$ of local minima in a one-dimensional landscape generated by a random walk or a L\\'evy flight.","We consider two different ensembles of landscapes, one with a fixed number of steps $N$ and the other till the first-passage time of the random walk to the origin.","We show that the distribution of $m$ is drastically different in the two ensembles (Gaussian in the former case, while having a power-law tail in the latter $m^{-3/2}$ in the latter case).","However, the most striking aspect of our results is that, in each case, the distribution is completely universal for all $m$ (and not just for large $m$), i.e., independent of the jump distribution in the random walk.","This means that the distributions are exactly identical for L\\'evy flights and random walks with finite jump variance.","Our analytical results are in excellent agreement with our numerical simulations."],"url":"http://arxiv.org/abs/2402.04215v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-06 18:13:18","title":"Symmetry shapes thermodynamics of macroscopic quantum systems","abstract":"We derive a systematic approach to the thermodynamics of quantum systems based on the underlying symmetry groups. We show that the entropy of a system can be described in terms of group-theoretical quantities that are largely independent of the details of its density matrix. We apply our technique to generic $N$ identical interacting $d$-level quantum systems. Using permutation invariance, we find that, for large $N$, entropy displays a universal large deviation behavior with a rate function $s(\\boldsymbol{x})$ that is completely independent of the microscopic details of the model, but depends only on the size of the irreducible representations of the permutation group $\\text{S}_N$. In turn, the partition function is shown to satisfy a large deviation principle with a free energy $f(\\boldsymbol{x})=e(\\boldsymbol{x})-\\beta^{-1}s(\\boldsymbol{x})$, where $e(\\boldsymbol{x})$ is a rate function that only depends on the ground state energy of particular subspaces determined by group representation theory. We apply our theory to the transverse-field Curie-Weiss model, a minimal model of phase transition exhibiting an interplay of thermal and quantum fluctuations.","sentences":["We derive a systematic approach to the thermodynamics of quantum systems based on the underlying symmetry groups.","We show that the entropy of a system can be described in terms of group-theoretical quantities that are largely independent of the details of its density matrix.","We apply our technique to generic $N$ identical interacting $d$-level quantum systems.","Using permutation invariance, we find that, for large $N$, entropy displays a universal large deviation behavior with a rate function $s(\\boldsymbol{x})$ that is completely independent of the microscopic details of the model, but depends only on the size of the irreducible representations of the permutation group $\\text{S}_N$. In turn, the partition function is shown to satisfy a large deviation principle with a free energy $f(\\boldsymbol{x})=e(\\boldsymbol{x})-\\beta^{-1}s(\\boldsymbol{x})$, where $e(\\boldsymbol{x})$ is a rate function that only depends on the ground state energy of particular subspaces determined by group representation theory.","We apply our theory to the transverse-field Curie-Weiss model, a minimal model of phase transition exhibiting an interplay of thermal and quantum fluctuations."],"url":"http://arxiv.org/abs/2402.04214v1","category":"quant-ph"}
{"created":"2024-02-06 18:10:52","title":"Quantifying information flow in quantum processes","abstract":"We present a framework for quantifying information flow within general quantum processes. For this purpose, we introduce the signaling power of quantum channels and discuss its relevant operational properties. This function supports extensions to higher order maps, enabling the evaluation of information flow in general quantum causal networks and also processes with indefinite causal order. Furthermore, our results offer a rigorous approach to information dynamics in open systems that applies also in the presence of initial system-environment correlations, and allows for the distinction between classical and quantum information backflow.","sentences":["We present a framework for quantifying information flow within general quantum processes.","For this purpose, we introduce the signaling power of quantum channels and discuss its relevant operational properties.","This function supports extensions to higher order maps, enabling the evaluation of information flow in general quantum causal networks and also processes with indefinite causal order.","Furthermore, our results offer a rigorous approach to information dynamics in open systems that applies also in the presence of initial system-environment correlations, and allows for the distinction between classical and quantum information backflow."],"url":"http://arxiv.org/abs/2402.04213v1","category":"quant-ph"}
{"created":"2024-02-06 18:09:51","title":"Preparing general mixed quantum states on quantum computers","abstract":"The preparation of quantum states serves as a pivotal subroutine across various domains, including quantum communication protocols, quantum computing, and the exploration of quantum correlations and other resources within physical systems. Building upon the protocols introduced in previous works [M. B. Pozzobom and J. Maziero, Quantum Inf. Process. 18, 142 (2019)] and [E. R. G\\aa rding et al., Entropy 23, 797 (2021)], the authors of [F. Shahbeigi, M. Karimi and V. Karimipour, Phys. Scr. 97, 025101 (2022)] demonstrated the capability to prepare mixed two-qubit X-real states on quantum computers by extending the methodology initially devised for mixed two-qubit Bell-diagonal states. In this article, we delve into an overlooked pattern within these quantum circuits, allowing us to generalize the approach to encompass a broader scope. Presenting an algorithm tailored for the preparation of $d$-dimensional mixed quantum states using quantum information processors, we offer a significant advancement in mixed state preparation methodologies. To validate the efficacy of our algorithm, we conducted comprehensive tests utilizing both X and non-X mixed two-qubit states, as well as arbitrary random density matrices spanning one, two, and three qubits.","sentences":["The preparation of quantum states serves as a pivotal subroutine across various domains, including quantum communication protocols, quantum computing, and the exploration of quantum correlations and other resources within physical systems.","Building upon the protocols introduced in previous works [M. B. Pozzobom and J. Maziero, Quantum Inf.","Process.","18, 142 (2019)]","and [E. R. G\\aa rding et al., Entropy 23, 797 (2021)], the authors of [F. Shahbeigi, M. Karimi and V. Karimipour, Phys.","Scr. 97, 025101 (2022)] demonstrated the capability to prepare mixed two-qubit X-real states on quantum computers by extending the methodology initially devised for mixed two-qubit Bell-diagonal states.","In this article, we delve into an overlooked pattern within these quantum circuits, allowing us to generalize the approach to encompass a broader scope.","Presenting an algorithm tailored for the preparation of $d$-dimensional mixed quantum states using quantum information processors, we offer a significant advancement in mixed state preparation methodologies.","To validate the efficacy of our algorithm, we conducted comprehensive tests utilizing both X and non-X mixed two-qubit states, as well as arbitrary random density matrices spanning one, two, and three qubits."],"url":"http://arxiv.org/abs/2402.04212v1","category":"quant-ph"}
{"created":"2024-02-06 18:09:05","title":"Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification","abstract":"Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture. Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance.","sentences":["Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes.","Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations.","We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass.","Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations.","Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture.","Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance."],"url":"http://arxiv.org/abs/2402.04211v1","category":"cs.LG"}
{"created":"2024-02-06 18:07:43","title":"\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors","abstract":"Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.","sentences":["Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences.","Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback.","In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met.","Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations).","However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving.","This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos?","To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies.","Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes.","Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement.","The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic."],"url":"http://arxiv.org/abs/2402.04210v1","category":"cs.AI"}
{"created":"2024-02-06 18:05:30","title":"Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study","abstract":"Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the receiver operating curve values (AUROC) for the UFH test cohort ranged between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort. UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval [CI] [0.80, 0.83]) for UFH and 0.82 (95% CI [0.81,0.84]) for UPMC test cohorts; an area under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06]) for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts. Kinetic estimated glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen remained the top three features with the highest influence across the models and health centers. Conclusion: Locally developed models displayed marginally reduced discrimination when tested on another institution, while the top set of influencing features remained the same across the models and sites.","sentences":["Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions.","Progression of AKI may lead to irreversible kidney damage.","Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202).","We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours.","We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model).","We internally and externally validated the models on each site and performed subgroup analyses across sex and race.","Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively.","Area under the receiver operating curve values (AUROC) for the UFH test cohort ranged between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort.","UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval","[CI]","[0.80, 0.83]) for UFH and 0.82 (95% CI","[0.81,0.84]) for UPMC test cohorts; an area under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06]) for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts.","Kinetic estimated glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen remained the top three features with the highest influence across the models and health centers.","Conclusion: Locally developed models displayed marginally reduced discrimination when tested on another institution, while the top set of influencing features remained the same across the models and sites."],"url":"http://arxiv.org/abs/2402.04209v1","category":"cs.LG"}
{"created":"2024-02-06 18:01:29","title":"Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models","abstract":"This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI). Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area. The work described in this paper aims to take advantage of the capabilities of Large Language Models (LLMs) in performing natural language processing tasks. This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems. In addition, this work also presents a formalization of the proposed explanation system. It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition. Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users. The results obtained during the experiment highlight the potential utility of LLMs in achieving explanatory capabilities in robots.","sentences":["This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI).","Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area.","The work described in this paper aims to take advantage of the capabilities of Large Language Models (LLMs) in performing natural language processing tasks.","This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems.","In addition, this work also presents a formalization of the proposed explanation system.","It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition.","Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users.","The results obtained during the experiment highlight the potential utility of LLMs in achieving explanatory capabilities in robots."],"url":"http://arxiv.org/abs/2402.04206v1","category":"cs.RO"}
{"created":"2024-02-06 18:00:27","title":"Maximal regularity and optimal control for a non-local Cahn-Hilliard tumour growth model","abstract":"We consider a non-local tumour growth model of phase-field type, describing the evolution of tumour cells through proliferation in presence of a nutrient. The model consists of a coupled system, incorporating a non-local Cahn-Hilliard equation for the tumour phase variable and a reaction-diffusion equation for the nutrient. Non-local cell-to-cell adhesion effects are included through a convolution operator with appropriate spatial kernels. First, we establish novel regularity results for such model, by applying maximal regularity theory in weighted $L^p$ spaces. Such technique enables us to prove local existence and uniqueness of a regular solution in a quite general framework, which also includes chemotaxis effects. Then, by leveraging time-regularisation properties of the weighted spaces and global boundedness estimates, we further extend the solution to a global one, under some additional assumptions. These results provide the foundation for addressing an optimal distributed control problem, aimed at identifying a suitable therapy, capable of guiding the evolution of the tumour towards a predefined target. Specifically, we prove the existence of an optimal therapy and then, by studying the Fr\\'echet-differentiability of the control-to-state operator and introducing the adjoint system, we derive first-order necessary optimality conditions.","sentences":["We consider a non-local tumour growth model of phase-field type, describing the evolution of tumour cells through proliferation in presence of a nutrient.","The model consists of a coupled system, incorporating a non-local Cahn-Hilliard equation for the tumour phase variable and a reaction-diffusion equation for the nutrient.","Non-local cell-to-cell adhesion effects are included through a convolution operator with appropriate spatial kernels.","First, we establish novel regularity results for such model, by applying maximal regularity theory in weighted $L^p$ spaces.","Such technique enables us to prove local existence and uniqueness of a regular solution in a quite general framework, which also includes chemotaxis effects.","Then, by leveraging time-regularisation properties of the weighted spaces and global boundedness estimates, we further extend the solution to a global one, under some additional assumptions.","These results provide the foundation for addressing an optimal distributed control problem, aimed at identifying a suitable therapy, capable of guiding the evolution of the tumour towards a predefined target.","Specifically, we prove the existence of an optimal therapy and then, by studying the Fr\\'echet-differentiability of the control-to-state operator and introducing the adjoint system, we derive first-order necessary optimality conditions."],"url":"http://arxiv.org/abs/2402.04204v1","category":"math.AP"}
{"created":"2024-02-06 17:59:46","title":"Human-Like Geometric Abstraction in Large Pre-trained Neural Networks","abstract":"Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.","sentences":["Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry.","Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations.","However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data.","In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations.","We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing."],"url":"http://arxiv.org/abs/2402.04203v1","category":"cs.AI"}
{"created":"2024-02-06 17:52:41","title":"From zero-mode intermittency to hidden symmetry in random scalar advection","abstract":"The statistical behavior of scalars passively advected by random flows exhibits intermittency in the form of anomalous multiscaling, in many ways similar to the patterns commonly observed in incompressible high-Reynolds fluids. This similarity suggests a generic dynamical mechanism underlying intermittency, though its specific nature remains unclear. Scalar turbulence is framed in a linear setting that points towards a zero-mode scenario connecting anomalous scaling to the presence of statistical conservation laws; the duality is fully substantiated within Kraichnan theory of random flows. However, extending the zero-mode scenario to nonlinear settings faces formidable technical challenges. Here, we revisit the scalar problem in the light of a hidden symmetry scenario introduced in recent deterministic turbulence studies addressing the Sabra shell model and the Navier-Stokes equations. Hidden symmetry uses a rescaling strategy based entirely on symmetry considerations, transforming the original dynamics into a rescaled (hidden) system; It ultimately identifies the scaling exponents as the eigenvalues of a Perron-Frobenius operator acting on invariant measures of the rescaled equations. Considering a minimal shell model of scalar advection of the Kraichnan type that was previously studied by Biferale & Wirth, the present work extends the hidden symmetry approach to a stochastic setting, in order to explicitly contrast it with the zero-mode scenario. Our study indicates that the zero-mode scenario represents only one facet of intermittency, here prescribing the scaling exponents of even-order correlators. Besides, we argue that hidden symmetry provides a more generic mechanism, fully prescribing intermittency in terms of scaling anomalies, but also in terms of its multiplicative random nature and fusion rules required to explicitly compute zero-modes from first principles.","sentences":["The statistical behavior of scalars passively advected by random flows exhibits intermittency in the form of anomalous multiscaling, in many ways similar to the patterns commonly observed in incompressible high-Reynolds fluids.","This similarity suggests a generic dynamical mechanism underlying intermittency, though its specific nature remains unclear.","Scalar turbulence is framed in a linear setting that points towards a zero-mode scenario connecting anomalous scaling to the presence of statistical conservation laws; the duality is fully substantiated within Kraichnan theory of random flows.","However, extending the zero-mode scenario to nonlinear settings faces formidable technical challenges.","Here, we revisit the scalar problem in the light of a hidden symmetry scenario introduced in recent deterministic turbulence studies addressing the Sabra shell model and the Navier-Stokes equations.","Hidden symmetry uses a rescaling strategy based entirely on symmetry considerations, transforming the original dynamics into a rescaled (hidden) system; It ultimately identifies the scaling exponents as the eigenvalues of a Perron-Frobenius operator acting on invariant measures of the rescaled equations.","Considering a minimal shell model of scalar advection of the Kraichnan type that was previously studied by Biferale & Wirth, the present work extends the hidden symmetry approach to a stochastic setting, in order to explicitly contrast it with the zero-mode scenario.","Our study indicates that the zero-mode scenario represents only one facet of intermittency, here prescribing the scaling exponents of even-order correlators.","Besides, we argue that hidden symmetry provides a more generic mechanism, fully prescribing intermittency in terms of scaling anomalies, but also in terms of its multiplicative random nature and fusion rules required to explicitly compute zero-modes from first principles."],"url":"http://arxiv.org/abs/2402.04198v1","category":"physics.flu-dyn"}
{"created":"2024-02-06 17:47:03","title":"Arrow of time and gravitational entropy in collapse","abstract":"We investigate the status of the gravitational arrow of time in the case of a spherical collapse of a fluid that conducts heat and radiates energy. In particular, we examine the results obtained by W. B. Bonnor in his 1985 paper where he found that the gravitational arrow of time was opposite to the thermodynamic arrow of time. The measure of gravitational epoch function $P$ used by Bonnor was given by the ratio of the Weyl square to the Ricci square. In this paper, we have assumed the measure of gravitational entropy $P_{1}$ to be given by the ratio of the Weyl scalar to the Kretschmann scalar. Our analysis indicates that Bonnor's result seems to be validated, i.e., the gravitational arrow and the thermodynamic arrow of time point in opposite directions. This strengthens the opinion that the Weyl proposal of gravitational entropy applies only to the universe as a whole (provided that we exclude the white wholes).","sentences":["We investigate the status of the gravitational arrow of time in the case of a spherical collapse of a fluid that conducts heat and radiates energy.","In particular, we examine the results obtained by W. B. Bonnor in his 1985 paper where he found that the gravitational arrow of time was opposite to the thermodynamic arrow of time.","The measure of gravitational epoch function $P$ used by Bonnor was given by the ratio of the Weyl square to the Ricci square.","In this paper, we have assumed the measure of gravitational entropy $P_{1}$ to be given by the ratio of the Weyl scalar to the Kretschmann scalar.","Our analysis indicates that Bonnor's result seems to be validated, i.e., the gravitational arrow and the thermodynamic arrow of time point in opposite directions.","This strengthens the opinion that the Weyl proposal of gravitational entropy applies only to the universe as a whole (provided that we exclude the white wholes)."],"url":"http://arxiv.org/abs/2402.04188v1","category":"gr-qc"}
{"created":"2024-02-06 17:46:41","title":"Start Stop Bit Method for Efficient Data Communication in 6G Mobile Radio Systems","abstract":"In this article, a novel approach for mobile radio communications is proposed and analysed, which is promising for future 6G cooperative distributed MIMO systems. The fundamental idea is a new mechanism namely start stop bit method, which transmits bit sequences as the start/stop bits of a synchronized counter instead of transmitting the full encoded bit sequence itself. In that way, theoretically, we can transmit infinitely long data messages with only one bit for starting and one bit for stopping the counter. The value of the counter, as identified by the stop bit, is then used to reconstruct and remap the one and unique transmitted bit sequence. The start stop bit method is characterized by a high signal sparsity as only two bits are transmitted, independently of the bit sequence length for the message. Among the benefits of the start stop bit method are energy efficient data transmission, and effective distributed MIMO systems, which exploit the sparse inter cooperation area interference as well as the low processing complexity for the sparse precoder calculation. Moreover, for the next mobile wireless generation, we propose an advanced scheme of the start stop bit method which enhances its resource usage. We call the resulting method a sparse dMIMO system.","sentences":["In this article, a novel approach for mobile radio communications is proposed and analysed, which is promising for future 6G cooperative distributed MIMO systems.","The fundamental idea is a new mechanism namely start stop bit method, which transmits bit sequences as the start/stop bits of a synchronized counter instead of transmitting the full encoded bit sequence itself.","In that way, theoretically, we can transmit infinitely long data messages with only one bit for starting and one bit for stopping the counter.","The value of the counter, as identified by the stop bit, is then used to reconstruct and remap the one and unique transmitted bit sequence.","The start stop bit method is characterized by a high signal sparsity as only two bits are transmitted, independently of the bit sequence length for the message.","Among the benefits of the start stop bit method are energy efficient data transmission, and effective distributed MIMO systems, which exploit the sparse inter cooperation area interference as well as the low processing complexity for the sparse precoder calculation.","Moreover, for the next mobile wireless generation, we propose an advanced scheme of the start stop bit method which enhances its resource usage.","We call the resulting method a sparse dMIMO system."],"url":"http://arxiv.org/abs/2402.04187v1","category":"cs.IT"}
{"created":"2024-02-06 17:45:16","title":"Tropical Geometry of Rado Matroids","abstract":"In this note, we characterize the products of simplicial generators for the Chow ring of a loopless matroid, extending a result of Backman, Eur, and Simpson. We prove that the stable intersection of a collection of tropical hyperplanes centered at the origin with the Bergman fan of a matroid is the Bergman fan of the dual of a certain Rado matroid.","sentences":["In this note, we characterize the products of simplicial generators for the Chow ring of a loopless matroid, extending a result of Backman, Eur, and Simpson.","We prove that the stable intersection of a collection of tropical hyperplanes centered at the origin with the Bergman fan of a matroid is the Bergman fan of the dual of a certain Rado matroid."],"url":"http://arxiv.org/abs/2402.04186v1","category":"math.CO"}
{"created":"2024-02-06 17:44:09","title":"Exciting Prospects for Dark Matter at Large-Volume Neutrino Detectors","abstract":"We propose a new approach to search for light dark matter (DM) in the range of keV-GeV via inelastic nucleus scattering at large-volume neutrino detectors such as Borexino, DUNE, JUNO, and Hyper-K. The approach uses inelastic nuclear scattering of cosmic-ray boosted DM, enabling a low-background search for DM in these experiments. The large neutrino detectors with higher threshold can be used since the nuclear deexcitation lines are $O(10)$ MeV. Using a hadrophilic dark-gauge-boson-portal model as a benchmark, we show that the nuclear inelastic channels generally provide better sensitivity than the elastic scattering for a large region of light DM parameter space.","sentences":["We propose a new approach to search for light dark matter (DM) in the range of keV-GeV via inelastic nucleus scattering at large-volume neutrino detectors such as Borexino, DUNE, JUNO, and Hyper-K. The approach uses inelastic nuclear scattering of cosmic-ray boosted DM, enabling a low-background search for DM in these experiments.","The large neutrino detectors with higher threshold can be used since the nuclear deexcitation lines are $O(10)$ MeV. Using a hadrophilic dark-gauge-boson-portal model as a benchmark, we show that the nuclear inelastic channels generally provide better sensitivity than the elastic scattering for a large region of light DM parameter space."],"url":"http://arxiv.org/abs/2402.04184v1","category":"hep-ph"}
{"created":"2024-02-06 17:42:39","title":"Reinforcement Learning with Ensemble Model Predictive Safety Certification","abstract":"Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.","sentences":["Reinforcement learning algorithms need exploration to learn.","However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment.","In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning.","Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller.","Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods."],"url":"http://arxiv.org/abs/2402.04182v1","category":"cs.LG"}
{"created":"2024-02-06 17:40:54","title":"Longevity Studies of CSC Prototypes Operating with Ar+CO$_{2}$ Gas Mixture and Different Fractions of CF4","abstract":"Studies of Cathode Strip Chamber longevity, comparing Ar+CO2 gas mixtures with fractions of 5%, 2%, and 0% CF4, were performed using several small cathode strip prototype chambers. In each trial, a localized source of radiation was used to irradiate up to an accumulated charge of about 300 mC/cm. Additionally, longevity of a uniformly irradiated prototype operating with 2% CF4 was studied at the CERN Gamma Irradiation Facility GIF++. Post-hoc analysis of the chamber electrodes using spectroscopy techniques was also done.","sentences":["Studies of Cathode Strip Chamber longevity, comparing Ar+CO2 gas mixtures with fractions of 5%, 2%, and 0% CF4, were performed using several small cathode strip prototype chambers.","In each trial, a localized source of radiation was used to irradiate up to an accumulated charge of about 300 mC/cm.","Additionally, longevity of a uniformly irradiated prototype operating with 2% CF4 was studied at the CERN Gamma Irradiation Facility GIF++.","Post-hoc analysis of the chamber electrodes using spectroscopy techniques was also done."],"url":"http://arxiv.org/abs/2402.04181v1","category":"physics.ins-det"}
{"created":"2024-02-06 17:38:41","title":"Deep-Learning Estimation of Weight Distribution Using Joint Kinematics for Lower-Limb Exoskeleton Control","abstract":"In the control of lower-limb exoskeletons with feet, the phase in the gait cycle can be identified by monitoring the weight distribution at the feet. This phase information can be used in the exoskeleton's controller to compensate the dynamics of the exoskeleton and to assign impedance parameters. Typically the weight distribution is calculated using data from sensors such as treadmill force plates or insole force sensors. However, these solutions increase both the setup complexity and cost. For this reason, we propose a deep-learning approach that uses a short time window of joint kinematics to predict the weight distribution of an exoskeleton in real time. The model was trained on treadmill walking data from six users wearing a four-degree-of-freedom exoskeleton and tested in real time on three different users wearing the same device. This test set includes two users not present in the training set to demonstrate the model's ability to generalize across individuals. Results show that the proposed method is able to fit the actual weight distribution with R2=0.9 and is suitable for real-time control with prediction times less than 1 ms. Experiments in closed-loop exoskeleton control show that deep-learning-based weight distribution estimation can be used to replace force sensors in overground and treadmill walking.","sentences":["In the control of lower-limb exoskeletons with feet, the phase in the gait cycle can be identified by monitoring the weight distribution at the feet.","This phase information can be used in the exoskeleton's controller to compensate the dynamics of the exoskeleton and to assign impedance parameters.","Typically the weight distribution is calculated using data from sensors such as treadmill force plates or insole force sensors.","However, these solutions increase both the setup complexity and cost.","For this reason, we propose a deep-learning approach that uses a short time window of joint kinematics to predict the weight distribution of an exoskeleton in real time.","The model was trained on treadmill walking data from six users wearing a four-degree-of-freedom exoskeleton and tested in real time on three different users wearing the same device.","This test set includes two users not present in the training set to demonstrate the model's ability to generalize across individuals.","Results show that the proposed method is able to fit the actual weight distribution with R2=0.9 and is suitable for real-time control with prediction times less than 1 ms.","Experiments in closed-loop exoskeleton control show that deep-learning-based weight distribution estimation can be used to replace force sensors in overground and treadmill walking."],"url":"http://arxiv.org/abs/2402.04180v1","category":"cs.RO"}
{"created":"2024-02-06 17:31:36","title":"SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models","abstract":"Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability. However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored. In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks. For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask). For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities. Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings. The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection. Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining. Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT. The project is available at https$:$//github.com/laiyingxin2/SHIELD","sentences":["Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability.","However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored.","In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection.","Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks.","For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask).","For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities.","Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings.","The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection.","Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining.","Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT.","The project is available at https$:$//github.com/laiyingxin2/SHIELD"],"url":"http://arxiv.org/abs/2402.04178v1","category":"cs.CV"}
{"created":"2024-02-06 17:31:05","title":"Low energy effective theories of composite dark matter with real representations","abstract":"We consider pseudo Nambu-Goldstone bosons arising from Dirac fermions transforming in real representations of a confining gauge group as dark matter candidates. We consider a special case of two Dirac fermions and couple the resulting dark sector to the Standard Model using a vector mediator. Within this construction, we develop a consistent low energy effective theory, with special attention to Wess-Zumino-Witten term given the topologically non-trivial coset space. We furthermore include the heavier spin-0 flavour singlet state and the spin-1 vector meson multiplet, by using the Hidden Local Symmetry Lagrangian for the latter. Although we concentrate on special case of two flavours, our results are generic and can be applied to a wider variety of theories featuring real representations. We apply our formalism and comment on the effect of the flavour singlet for dark matter phenomenology. Finally, we also comment on generalisation of our formalism for higher representations and provide potential consequences of discrete symmetry breaking.","sentences":["We consider pseudo Nambu-Goldstone bosons arising from Dirac fermions transforming in real representations of a confining gauge group as dark matter candidates.","We consider a special case of two Dirac fermions and couple the resulting dark sector to the Standard Model using a vector mediator.","Within this construction, we develop a consistent low energy effective theory, with special attention to Wess-Zumino-Witten term given the topologically non-trivial coset space.","We furthermore include the heavier spin-0 flavour singlet state and the spin-1 vector meson multiplet, by using the Hidden Local Symmetry Lagrangian for the latter.","Although we concentrate on special case of two flavours, our results are generic and can be applied to a wider variety of theories featuring real representations.","We apply our formalism and comment on the effect of the flavour singlet for dark matter phenomenology.","Finally, we also comment on generalisation of our formalism for higher representations and provide potential consequences of discrete symmetry breaking."],"url":"http://arxiv.org/abs/2402.04176v1","category":"hep-ph"}
{"created":"2024-02-06 17:27:12","title":"COPS: A Compact On-device Pipeline for real-time Smishing detection","abstract":"Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping. However, with the increased usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in particular, have observed a significant upsurge in recent years. This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours. This renders the standard practice of keeping a database of malicious URLs ineffective. To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time. COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets. We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices.","sentences":["Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping.","However, with the increased usage, cybercrime aimed at mobile devices is rocketing.","Smishing attacks, in particular, have observed a significant upsurge in recent years.","This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours.","This renders the standard practice of keeping a database of malicious URLs ineffective.","To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time.","COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets.","We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices."],"url":"http://arxiv.org/abs/2402.04173v1","category":"cs.CR"}
{"created":"2024-02-06 17:27:09","title":"An overview of existing and new nuclear and astrophysical constraints on the equation of state of neutron-rich dense matter","abstract":"Through continuous progress in nuclear theory and experiment and an increasing number of neutron-star observations, a multitude of information about the equation of state (EOS) for matter at extreme densities is available. Here, we apply these different pieces of data individually to a broad set of physics-agnostic candidate EOSs and analyze the resulting constraints. Specifically, we make use of information from chiral effective field theory, perturbative quantum chromodynamics, as well as data from heavy-ion collisions and the PREX-II and CREX experiments. We also investigate the impact of current mass and radius measurements of neutron stars, such as radio timing measurements of heavy pulsars, NICER data, and other X-ray observations. We augment these by reanalyses of the gravitational-wave (GW) signal GW170817, its associated kilonova AT2017gfo and gamma-ray burst afterglow, the GW signal GW190425, and the GRB211211A afterglow, where we use improved models for the tidal waveform and kilonova light curves. Additionally, we consider the postmerger fate of GW170817 and its consequences for the EOS. This large and diverse set of constraints is eventually combined in numerous ways to explore limits on quantities such as the typical neutron-star radius, the maximum neutron-star mass, the nuclear symmetry-energy parameters, and the speed of sound. Based on the priors from our EOS candidate set, we find the radius of the canonical 1.4 M$_\\odot$ neutron star to be $R_{1.4}= 12.27_{-0.94}^{+0.83}$ km and the TOV mass $M_{\\rm TOV}= 2.26_{-0.22}^{+0.45}$ M$_\\odot$ at 95% credibility, when including those constraints where systematic uncertainties are deemed small. A less conservative approach, combining all the presented constraints, similarly yields $R_{1.4}= 12.20_{-0.50}^{+0.53}$ km and $M_{\\rm TOV}= 2.31_{-0.20}^{+0.08}$ M$_\\odot$.","sentences":["Through continuous progress in nuclear theory and experiment and an increasing number of neutron-star observations, a multitude of information about the equation of state (EOS) for matter at extreme densities is available.","Here, we apply these different pieces of data individually to a broad set of physics-agnostic candidate EOSs and analyze the resulting constraints.","Specifically, we make use of information from chiral effective field theory, perturbative quantum chromodynamics, as well as data from heavy-ion collisions and the PREX-II and CREX experiments.","We also investigate the impact of current mass and radius measurements of neutron stars, such as radio timing measurements of heavy pulsars, NICER data, and other X-ray observations.","We augment these by reanalyses of the gravitational-wave (GW) signal GW170817, its associated kilonova AT2017gfo and gamma-ray burst afterglow, the GW signal GW190425, and the GRB211211A afterglow, where we use improved models for the tidal waveform and kilonova light curves.","Additionally, we consider the postmerger fate of GW170817 and its consequences for the EOS.","This large and diverse set of constraints is eventually combined in numerous ways to explore limits on quantities such as the typical neutron-star radius, the maximum neutron-star mass, the nuclear symmetry-energy parameters, and the speed of sound.","Based on the priors from our EOS candidate set, we find the radius of the canonical 1.4 M$_\\odot$ neutron star to be $R_{1.4}= 12.27_{-0.94}^{+0.83}$ km and the TOV mass $M_{\\rm TOV}=","2.26_{-0.22}^{+0.45}$ M$_\\odot$ at 95% credibility, when including those constraints where systematic uncertainties are deemed small.","A less conservative approach, combining all the presented constraints, similarly yields $R_{1.4}= 12.20_{-0.50}^{+0.53}$ km and $M_{\\rm TOV}= 2.31_{-0.20}^{+0.08}$ M$_\\odot$."],"url":"http://arxiv.org/abs/2402.04172v1","category":"astro-ph.HE"}
{"created":"2024-02-06 17:22:45","title":"Mind the Gap: Securely modeling cyber risk based on security deviations from a peer group","abstract":"There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share. Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed. As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group. This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations. We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data. We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers.","sentences":["There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers?","Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share.","Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed.","As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group.","This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations.","We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data.","We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers."],"url":"http://arxiv.org/abs/2402.04166v1","category":"cs.CR"}
{"created":"2024-02-06 17:21:26","title":"A note on the persistence of multiplicity of eigenvalues of fractional Laplacian under perturbations","abstract":"We consider the eigenvalues problem for the the fractional Laplacian in a bounded domain Omega with Dirichlet boundary condition. A recent result by Fall, Ghimenti, Micheletti and Pistoia (CVPDE (2023)) states that under generic small perturbations of the coefficient of the equation or of the domain Omega all the eigenvalues are simple. In this paper we give a condition for which a perturbation of the coefficient or of the domain preserves the multiplicity of a given eigenvalue. Also, in the case of an eigenvalue of multiplicity 2 we prove that the set of perturbations of the coefficients which preserve the multiplicity is a smooth manifold of codimension $2$ in C^1(Omega).","sentences":["We consider the eigenvalues problem for the the fractional Laplacian in a bounded domain Omega with Dirichlet boundary condition.","A recent result by Fall, Ghimenti, Micheletti and Pistoia (CVPDE (2023)) states that under generic small perturbations of the coefficient of the equation or of the domain Omega all the eigenvalues are simple.","In this paper we give a condition for which a perturbation of the coefficient or of the domain preserves the multiplicity of a given eigenvalue.","Also, in the case of an eigenvalue of multiplicity 2 we prove that the set of perturbations of the coefficients which preserve the multiplicity is a smooth manifold of codimension $2$ in C^1(Omega)."],"url":"http://arxiv.org/abs/2402.04164v1","category":"math.AP"}
{"created":"2024-02-06 17:21:06","title":"Tempered Calculus for ML: Application to Hyperbolic Model Embedding","abstract":"Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the metricity, hyperbolicity, and encoding. We show how to apply it to a problem that has recently gained traction in ML: hyperbolic embeddings with a \"cheap\" and accurate encoding along the hyperbolic vs Euclidean scale. We unveil a new application for which the Poincar\\'e disk model has very appealing features, and our theory comes in handy: \\textit{model} embeddings for boosted combinations of decision trees, trained using the log-loss (trees) and logistic loss (combinations).","sentences":["Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc.","In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements.","We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics.","Notably, this recovers Volterra's product integral as a special case.","We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative.","This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the metricity, hyperbolicity, and encoding.","We show how to apply it to a problem that has recently gained traction in ML: hyperbolic embeddings with a \"cheap\" and accurate encoding along the hyperbolic vs Euclidean scale.","We unveil a new application for which the Poincar\\'e disk model has very appealing features, and our theory comes in handy: \\textit{model} embeddings for boosted combinations of decision trees, trained using the log-loss (trees) and logistic loss (combinations)."],"url":"http://arxiv.org/abs/2402.04163v1","category":"cs.LG"}
{"created":"2024-02-06 17:18:59","title":"Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains","abstract":"In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena. Code is available at \\url{https://github.com/Bond1995/Markov}.","sentences":["In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages.","A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner.","To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains.","Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance.","In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture.","Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results.","We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena.","Code is available at \\url{https://github.com/Bond1995/Markov}."],"url":"http://arxiv.org/abs/2402.04161v1","category":"cs.LG"}
{"created":"2024-02-06 17:18:25","title":"Harnessing the Plug-and-Play Controller by Prompting","abstract":"Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation. Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.","sentences":["Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications.","Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner.","However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation.","Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model.","This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs).","The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs.","The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute.","To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process.","The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference.","Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency."],"url":"http://arxiv.org/abs/2402.04160v1","category":"cs.CL"}
{"created":"2024-02-06 17:15:33","title":"Black holes and gravitational waves from slow phase transitions","abstract":"Slow first-order phase transitions generate large inhomogeneities that can lead to the formation of primordial black holes (PBHs). We show that the gravitational wave (GW) spectrum then consists of a primary component sourced by bubble collisions and a secondary one induced by large perturbations. The latter gives the dominant peak if $\\beta/H_0 < 10$, impacting, in particular, the interpretation of the recent PTA data. The GW signal associated with a particular PBH population is stronger than in typical scenarios because of a negative non-Gaussianity of the perturbations and it has a distinguishable shape with two peaks.","sentences":["Slow first-order phase transitions generate large inhomogeneities that can lead to the formation of primordial black holes (PBHs).","We show that the gravitational wave (GW) spectrum then consists of a primary component sourced by bubble collisions and a secondary one induced by large perturbations.","The latter gives the dominant peak if $\\beta/H_0 < 10$, impacting, in particular, the interpretation of the recent PTA data.","The GW signal associated with a particular PBH population is stronger than in typical scenarios because of a negative non-Gaussianity of the perturbations and it has a distinguishable shape with two peaks."],"url":"http://arxiv.org/abs/2402.04158v1","category":"astro-ph.CO"}
{"created":"2024-02-06 17:09:25","title":"Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction","abstract":"Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.","sentences":["Developing a generalist agent is a longstanding objective in artificial intelligence.","Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.","However, these works encounter challenges in extending their capabilities to new tasks.","Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.","However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.","This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability.","Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.","Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities."],"url":"http://arxiv.org/abs/2402.04154v1","category":"cs.AI"}
{"created":"2024-02-06 17:01:10","title":"Dynamic Realization Games in Newsvendor Inventory Centralization","abstract":"Consider a set N of n (>1) stores with single-item and single-period nondeterministic demands like in a classic newsvendor setting with holding and penalty costs only. Assume a risk-pooling single-warehouse centralized inventory ordering option. Allocation of costs in the centralized inventory ordering corresponds to modelling it as a cooperative cost game whose players are the stores. It has been shown that when holding and penalty costs are identical for all subsets of stores, the game based on optimal expected costs has a non empty core (Hartman et. al., 2000, Muller \\textit{et. al.}, 2002). In this paper we examine a related inventory centralization game based on demand realizations that has, in general, an empty core even with identical penalty and holding costs (Hartman and Dror, 2005). We propose a repeated cost allocation scheme for dynamic realization games based on allocation processes introduced by Lehrer (2002a). We prove that the cost subsequences of the dynamic realization game process, based on Lehrer's rules, converge almost surely to either a least square value or the core of the expected game. We extend the above results to more general dynamic cost games and relax the independence hypothesis of the sequence of players' demands at different stages.","sentences":["Consider a set N of n (>1) stores with single-item and single-period nondeterministic demands like in a classic newsvendor setting with holding and penalty costs only.","Assume a risk-pooling single-warehouse centralized inventory ordering option.","Allocation of costs in the centralized inventory ordering corresponds to modelling it as a cooperative cost game whose players are the stores.","It has been shown that when holding and penalty costs are identical for all subsets of stores, the game based on optimal expected costs has a non empty core (Hartman et.","al., 2000, Muller \\textit{et.","al.}, 2002).","In this paper we examine a related inventory centralization game based on demand realizations that has, in general, an empty core even with identical penalty and holding costs (Hartman and Dror, 2005).","We propose a repeated cost allocation scheme for dynamic realization games based on allocation processes introduced by Lehrer (2002a).","We prove that the cost subsequences of the dynamic realization game process, based on Lehrer's rules, converge almost surely to either a least square value or the core of the expected game.","We extend the above results to more general dynamic cost games and relax the independence hypothesis of the sequence of players' demands at different stages."],"url":"http://arxiv.org/abs/2402.04149v1","category":"cs.GT"}
{"created":"2024-02-06 16:56:42","title":"Hyperoctahedral group characters and a type-BC analog of graph coloring","abstract":"We state combinatorial formulas for hyperoctahedral group ($\\mathfrak B_n$) character evaluations of the form $\\chi( {{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))$, where ${{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1) \\in \\Bbb Z[\\mathfrak B_n]$ is a type-BC Kazhdan-Lusztig basis element, with $w \\in \\mathfrak B_n$ corresponding to simultaneously smooth type-B and C Schubert varieties. We also extend the definition of symmetric group codominance to elements of $\\mathfrak B_n$ and show that for each element $w \\in \\mathfrak B_n$ above, there exists a BC-codominant element $v \\in \\mathfrak B_n$ satisfying $\\chi( {{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1)) = \\chi( {{\\widetilde C}_v}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))$ for all $\\mathfrak B_n$-characters $\\chi$. Combinatorial structures and maps appearing in these formulas are type-BC extensions of planar networks, unit interval orders, indifference graphs, poset tableaux, and colorings. Using the ring of type-BC symmetric functions, we introduce natural generating functions $Y( {{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))$ for the above evaluations. These provide a new type-BC analog of Stanley's chromatic symmetric functions [Adv. Math. 111 (1995) pp. 166-194].","sentences":["We state combinatorial formulas for hyperoctahedral group ($\\mathfrak B_n$) character evaluations of the form $\\chi( {{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))$, where ${{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1) \\in \\Bbb Z[\\mathfrak B_n]$ is a type-BC Kazhdan-Lusztig basis element, with $w \\in \\mathfrak B_n$ corresponding to simultaneously smooth type-B and C Schubert varieties.","We also extend the definition of symmetric group codominance to elements of $\\mathfrak B_n$ and show that for each element $w \\in \\mathfrak B_n$ above, there exists a BC-codominant element $v \\in \\mathfrak B_n$ satisfying $\\chi( {{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))","= \\chi( {{\\widetilde C}_v}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))$ for all $\\mathfrak B_n$-characters $\\chi$. Combinatorial structures and maps appearing in these formulas are type-BC extensions of planar networks, unit interval orders, indifference graphs, poset tableaux, and colorings.","Using the ring of type-BC symmetric functions, we introduce natural generating functions $Y( {{\\widetilde C}_w}^{\\negthickspace\\negthickspace BC}\\negthickspace(1))$ for the above evaluations.","These provide a new type-BC analog of Stanley's chromatic symmetric functions [Adv. Math. 111 (1995) pp.","166-194]."],"url":"http://arxiv.org/abs/2402.04148v1","category":"math.CO"}
{"created":"2024-02-06 16:55:48","title":"Finite temperature fermionic condensate and energy-momentum tensor in compactified cosmic string spacetime","abstract":"Here we analyze the expectation value of the fermionic condensate and the energy-momentum tensor associated with a massive charged fermionic quantum field with a nonzero chemical potential propagating in a magnetic-flux-carrying cosmic string with compactified dimension in thermal equilibrium at finite temperature $T$. We assume that, in addition to the magnetic field flux running along the string's core, a magnetic flux enclosed by the compact dimension is presented, and the field interacts with both. The expectation values of the fermionic condensate and the energy-momentum tensor are expressed as the sum of vacuum expectation values and the finite temperature contributions coming from the particles and antiparticles excitation. As consequence of the compactification, the thermal corrections of these observable are decomposed in a part induced by pure cosmic string spacetime, plus a contribution induced by the compactification. The thermal expectations values of the fermionic condensate and the energy-momentum tensor are even periodic functions of the magnetic flux with period being the quantum flux, and also even functions of the chemical potential. Because the analyses of vacuum expectation of the fermionic condensate and energy-momentum tensor have been developed in literature, here we are mainly interested in the investigation of the thermal corrections. In this way we explicitly study how these observable behaves in the limits of low and high temperatures, and also for points near the string. Besides the analytical discussions, we included some graphs that exhibit the behavior of these observable for different values of the physical parameters of the model.","sentences":["Here we analyze the expectation value of the fermionic condensate and the energy-momentum tensor associated with a massive charged fermionic quantum field with a nonzero chemical potential propagating in a magnetic-flux-carrying cosmic string with compactified dimension in thermal equilibrium at finite temperature $T$. We assume that, in addition to the magnetic field flux running along the string's core, a magnetic flux enclosed by the compact dimension is presented, and the field interacts with both.","The expectation values of the fermionic condensate and the energy-momentum tensor are expressed as the sum of vacuum expectation values and the finite temperature contributions coming from the particles and antiparticles excitation.","As consequence of the compactification, the thermal corrections of these observable are decomposed in a part induced by pure cosmic string spacetime, plus a contribution induced by the compactification.","The thermal expectations values of the fermionic condensate and the energy-momentum tensor are even periodic functions of the magnetic flux with period being the quantum flux, and also even functions of the chemical potential.","Because the analyses of vacuum expectation of the fermionic condensate and energy-momentum tensor have been developed in literature, here we are mainly interested in the investigation of the thermal corrections.","In this way we explicitly study how these observable behaves in the limits of low and high temperatures, and also for points near the string.","Besides the analytical discussions, we included some graphs that exhibit the behavior of these observable for different values of the physical parameters of the model."],"url":"http://arxiv.org/abs/2402.04147v1","category":"hep-th"}
{"created":"2024-02-06 16:54:59","title":"Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process","abstract":"With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic categorical variable that are mapped into a physically interpretable latent space, allowing the development of source-aware data fusion modeling. Additionally, a dissimilarity metric based on the latent variables of LVGP is introduced to study and understand the differences in the sources of data. The proposed approach is demonstrated on and analyzed through two mathematical (representative parabola problem, 2D Ackley function) and two materials science (design of FeCrAl and SmCoFe alloys) case studies. From the case studies, it is observed that compared to using single-source and source unaware ML models, the proposed multi-source data fusion framework can provide better predictions for sparse-data problems, interpretability regarding the sources, and enhanced modeling capabilities by taking advantage of the correlations and relationships among different sources.","sentences":["With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data).","The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities.","A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources.","However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization.","Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed.","The individual data sources are tagged as a characteristic categorical variable that are mapped into a physically interpretable latent space, allowing the development of source-aware data fusion modeling.","Additionally, a dissimilarity metric based on the latent variables of LVGP is introduced to study and understand the differences in the sources of data.","The proposed approach is demonstrated on and analyzed through two mathematical (representative parabola problem, 2D Ackley function) and two materials science (design of FeCrAl and SmCoFe alloys) case studies.","From the case studies, it is observed that compared to using single-source and source unaware ML models, the proposed multi-source data fusion framework can provide better predictions for sparse-data problems, interpretability regarding the sources, and enhanced modeling capabilities by taking advantage of the correlations and relationships among different sources."],"url":"http://arxiv.org/abs/2402.04146v1","category":"stat.ML"}
{"created":"2024-02-06 16:48:50","title":"Multi-line AI-assisted Code Authoring","abstract":"CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions. Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%. Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions.","sentences":["CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta.","In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions.","This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   ","First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   ","Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users.","These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   ","Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions.","Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%.","Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions."],"url":"http://arxiv.org/abs/2402.04141v1","category":"cs.SE"}
{"created":"2024-02-06 16:47:34","title":"Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)","abstract":"This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA. A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration. This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis.","sentences":["This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong.","This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions.","By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law.","SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions.","SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections.","Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA.","A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration.","This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis."],"url":"http://arxiv.org/abs/2402.04140v1","category":"cs.AI"}
{"created":"2024-02-06 16:40:31","title":"A gravitational metrological triangle","abstract":"Motivated by the similarity of the mathematical structure of Einstein's General Relativity in its weak field limit and of Maxwell's theory of electrodynamics it is shown that there are gravitational analogues of the Josephson effect and the quantum Hall effect. These effects can be combined to derive a gravitational analogue of the quantum/electric metrological triangle. The gravitational metrological triangle may have applications in metrology and could be used to investigate the relation of the Planck constant to fundamental particle masses. This allows for quantum tests of the Weak Equivalence Principle. Moreover, the similarity of the gravitational and the quantum/electrical metrological triangle can be used to test the universality of quantum mechanics.","sentences":["Motivated by the similarity of the mathematical structure of Einstein's General Relativity in its weak field limit and of Maxwell's theory of electrodynamics it is shown that there are gravitational analogues of the Josephson effect and the quantum Hall effect.","These effects can be combined to derive a gravitational analogue of the quantum/electric metrological triangle.","The gravitational metrological triangle may have applications in metrology and could be used to investigate the relation of the Planck constant to fundamental particle masses.","This allows for quantum tests of the Weak Equivalence Principle.","Moreover, the similarity of the gravitational and the quantum/electrical metrological triangle can be used to test the universality of quantum mechanics."],"url":"http://arxiv.org/abs/2402.04135v1","category":"gr-qc"}
{"created":"2024-02-06 16:35:49","title":"Spectroscopic performance of Low-Gain Avalanche Diodes for different types of radiation","abstract":"Low-Gain Avalanche Diodes are a type of silicon Avalanche Photo-Diodes originally developed for the fast detection of minimum ionizing particles in high-energy physics experiments. Thanks to their fast timing performance, the Low-Gain Avalanche Diode paradigm enables detectors to accurately measure minimum ionizing particles with a timing resolution of a few tens of picoseconds. Such a performance is due to a thin substrate and the presence of a moderate signal gain. This internal gain of a few tens is enough to compensate for the reduced charge deposition in the thinner substrate and the noise of fast read-out systems. While Low-Gain Avalanche Diodes are optimized for the detection of minimum ionizing particles for high-energy particle detectors, it is critical to study their performance for the detection of different types of particle, such as X-rays, gamma-rays, or alphas. In this paper, we evaluate the gain of three types of Low-Gain Avalanche Diodes: two devices with different geometries and doping profiles fabricated by Brookhaven National Laboratory, and one fabricated by Hamamatsu Photonics with a different process.   Since the gain in LGADs depends on the bias voltage applied to the sensor, pulse-height spectra have been acquired for bias voltages spanning from the depletion voltage up to breakdown voltage. The signal-to-noise ratio of the generated signals and the shape of their spectra allow us to probe the underlying physics of the multiplication process.","sentences":["Low-Gain Avalanche Diodes are a type of silicon Avalanche Photo-Diodes originally developed for the fast detection of minimum ionizing particles in high-energy physics experiments.","Thanks to their fast timing performance, the Low-Gain Avalanche Diode paradigm enables detectors to accurately measure minimum ionizing particles with a timing resolution of a few tens of picoseconds.","Such a performance is due to a thin substrate and the presence of a moderate signal gain.","This internal gain of a few tens is enough to compensate for the reduced charge deposition in the thinner substrate and the noise of fast read-out systems.","While Low-Gain Avalanche Diodes are optimized for the detection of minimum ionizing particles for high-energy particle detectors, it is critical to study their performance for the detection of different types of particle, such as X-rays, gamma-rays, or alphas.","In this paper, we evaluate the gain of three types of Low-Gain Avalanche Diodes: two devices with different geometries and doping profiles fabricated by Brookhaven National Laboratory, and one fabricated by Hamamatsu Photonics with a different process.   ","Since the gain in LGADs depends on the bias voltage applied to the sensor, pulse-height spectra have been acquired for bias voltages spanning from the depletion voltage up to breakdown voltage.","The signal-to-noise ratio of the generated signals and the shape of their spectra allow us to probe the underlying physics of the multiplication process."],"url":"http://arxiv.org/abs/2402.04132v1","category":"physics.ins-det"}
{"created":"2024-02-06 16:25:59","title":"Role of seeding in the generation of polarization squeezed light by atomic Kerr medium","abstract":"Quantum state production and characterization are fundamental elements for many quantum technological applications. In this work, we studied the generation of polarization quantum states by interacting light with a Kerr medium and the dependency of the outcome on orthogonal polarization seedlings. Starting from %Our experimental apparatus is based on coherent states produced by Ti:Sapphire laser, interaction with a $^{87}$Rb warm vapor cell led to noise compression of $-5.2\\pm 0.5$ dB ($6.4\\pm 0.6$ dB after correction of the detection quantum efficiency).","sentences":["Quantum state production and characterization are fundamental elements for many quantum technological applications.","In this work, we studied the generation of polarization quantum states by interacting light with a Kerr medium and the dependency of the outcome on orthogonal polarization seedlings.","Starting from %Our experimental apparatus is based on coherent states produced by Ti:Sapphire laser, interaction with a $^{87}$Rb warm vapor cell led to noise compression of $-5.2\\pm 0.5$ dB ($6.4\\pm 0.6$ dB after correction of the detection quantum efficiency)."],"url":"http://arxiv.org/abs/2402.04127v1","category":"quant-ph"}
{"created":"2024-02-06 16:22:25","title":"Balanced Quantum Hall Resistor","abstract":"The quantum anomalous Hall effect in magnetic topological insulators has been recognized as a promising platform for applications in quantum metrology. The primary reason for this is the electronic conductance quantization at zero external magnetic field, which allows to combine it with the quantum standard of voltage. Here we demonstrate a measurement scheme that increases the robustness of the zero magnetic field quantum anomalous Hall resistor, allowing for higher operational currents. This is achieved by simultaneous current injection into the two disconnected perimeters of a multi-terminal Corbino device to balance the electrochemical potential between the edges, screening the electric field that drives back-scattering through the bulk, and thus improving the stability of the quantization at increased currents. This approach is not only applicable to devices based on the quantum anomalous Hall effect, but more generally can also be applied to existing quantum resistance standards that rely on the integer quantum Hall effect.","sentences":["The quantum anomalous Hall effect in magnetic topological insulators has been recognized as a promising platform for applications in quantum metrology.","The primary reason for this is the electronic conductance quantization at zero external magnetic field, which allows to combine it with the quantum standard of voltage.","Here we demonstrate a measurement scheme that increases the robustness of the zero magnetic field quantum anomalous Hall resistor, allowing for higher operational currents.","This is achieved by simultaneous current injection into the two disconnected perimeters of a multi-terminal Corbino device to balance the electrochemical potential between the edges, screening the electric field that drives back-scattering through the bulk, and thus improving the stability of the quantization at increased currents.","This approach is not only applicable to devices based on the quantum anomalous Hall effect, but more generally can also be applied to existing quantum resistance standards that rely on the integer quantum Hall effect."],"url":"http://arxiv.org/abs/2402.04126v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 16:19:57","title":"Long time stability for cubic nonlinear Schr\u00f6dinger equations on non-rectangular flat tori","abstract":"We consider nonlinear Schr\\\"odinger equations on flat tori satisfying a simple and explicit Diophantine non-degeneracy condition. Provided that the nonlinearity contains a cubic term, we prove the almost global existence and stability of most of the small solutions in high regularity Sobolev spaces. To this end, we develop a normal form approach designed to handle general resonant Hamiltonian partial differential equations for which it is possible to modulate the frequencies by using the initial data.","sentences":["We consider nonlinear Schr\\\"odinger equations on flat tori satisfying a simple and explicit Diophantine non-degeneracy condition.","Provided that the nonlinearity contains a cubic term, we prove the almost global existence and stability of most of the small solutions in high regularity Sobolev spaces.","To this end, we develop a normal form approach designed to handle general resonant Hamiltonian partial differential equations for which it is possible to modulate the frequencies by using the initial data."],"url":"http://arxiv.org/abs/2402.04122v1","category":"math.AP"}
{"created":"2024-02-06 16:17:58","title":"Multivariable generalizations of bivariate means via invariance","abstract":"For a given $p$-variable mean $M \\colon I^p \\to I$ ($I$ is a subinterval of $\\mathbb{R}$), following (Horwitz, 2002) and (Lawson and Lim, 2008), we can define (under certain assumption) its $(p+1)$-variable $\\beta$-invariant extension as the unique solution $K \\colon I^{p+1} \\to I$ of the functional equation \\begin{align*} K\\big(M(x_2,\\dots,x_{p+1})&,M(x_1,x_3,\\dots,x_{p+1}),\\dots,M(x_1,\\dots,x_p)\\big)\\\\ &=K(x_1,\\dots,x_{p+1}), \\text{ for all }x_1,\\dots,x_{p+1} \\in I \\end{align*} in the family of means.   Applying this procedure iteratively we can obtain a mean which is defined for vectors of arbitrary lengths starting from the bivariate one. The aim of this paper is to study the properties of such extensions.","sentences":["For a given $p$-variable mean $M \\colon I^p \\to I$ ($I$ is a subinterval of $\\mathbb{R}$), following (Horwitz, 2002) and (Lawson and Lim, 2008), we can define (under certain assumption) its $(p+1)$-variable $\\beta$-invariant extension as the unique solution $K \\colon I^{p+1} \\to I$ of the functional equation \\begin{align*} K\\big(M(x_2,\\dots,x_{p+1})&,M(x_1,x_3,\\dots,x_{p+1}),\\dots,M(x_1,\\dots,x_p)\\big)\\\\ &=K(x_1,\\dots,x_{p+1}), \\text{ for all }x_1,\\dots,x_{p+1} \\in I \\end{align*} in the family of means.   ","Applying this procedure iteratively we can obtain a mean which is defined for vectors of arbitrary lengths starting from the bivariate one.","The aim of this paper is to study the properties of such extensions."],"url":"http://arxiv.org/abs/2402.04121v1","category":"math.DS"}
{"created":"2024-02-06 16:12:44","title":"Feynman rules and loop structure of Carrollian amplitude","abstract":"In this paper, we derive the Carrollian amplitude in the framework of bulk reduction. The Carrollian amplitude is shown to relate to the scattering amplitude by a Fourier transform in this method. We propose Feynman rules to calculate the Carrollian amplitude where the Fourier transforms emerge as the integral representation of the external lines in the Carrollian space. Then we study the four-point Carrollian amplitude at loop level in massless $\\Phi^4$ theory. As a consequence of Poincar\\'e invariance, the four-point Carrollian amplitude can be transformed to the amplitude that only depends on the cross ratio $z$ of the celestial sphere and a variable $\\chi$ invariant under translation. The four-point Carrollian amplitude is a polynomial of the two-point Carrollian amplitude whose argument is replaced with $\\chi$. The coefficients of the polynomial have branch cuts in the complex $z$ plane. We also show that the renormalized Carrollian amplitude obeys the Callan-Symanzik equation. Moreover, we initiate a generalized $\\Phi^4$ theory by designing the Feynman rules for more general Carrollian amplitude.","sentences":["In this paper, we derive the Carrollian amplitude in the framework of bulk reduction.","The Carrollian amplitude is shown to relate to the scattering amplitude by a Fourier transform in this method.","We propose Feynman rules to calculate the Carrollian amplitude where the Fourier transforms emerge as the integral representation of the external lines in the Carrollian space.","Then we study the four-point Carrollian amplitude at loop level in massless $\\Phi^4$ theory.","As a consequence of Poincar\\'e invariance, the four-point Carrollian amplitude can be transformed to the amplitude that only depends on the cross ratio $z$ of the celestial sphere and a variable $\\chi$ invariant under translation.","The four-point Carrollian amplitude is a polynomial of the two-point Carrollian amplitude whose argument is replaced with $\\chi$. The coefficients of the polynomial have branch cuts in the complex $z$ plane.","We also show that the renormalized Carrollian amplitude obeys the Callan-Symanzik equation.","Moreover, we initiate a generalized $\\Phi^4$ theory by designing the Feynman rules for more general Carrollian amplitude."],"url":"http://arxiv.org/abs/2402.04120v1","category":"hep-th"}
{"created":"2024-02-06 16:05:18","title":"Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors","abstract":"Approximate message passing (AMP) algorithms are devised under the Gaussianity assumption of the measurement noise vector. In this work, we relax this assumption within the vector AMP (VAMP) framework to arbitrary independent and identically distributed (i.i.d.) noise priors. We do so by rederiving the linear minimum mean square error (LMMSE) to accommodate both the noise and signal estimations within the message passing steps of VAMP. Numerical results demonstrate how our proposed algorithm handles non-Gaussian noise models as compared to VAMP. This extension to general noise priors enables the use of AMP algorithms in a wider range of engineering applications where non-Gaussian noise models are more appropriate.","sentences":["Approximate message passing (AMP) algorithms are devised under the Gaussianity assumption of the measurement noise vector.","In this work, we relax this assumption within the vector AMP (VAMP) framework to arbitrary independent and identically distributed (i.i.d.) noise priors.","We do so by rederiving the linear minimum mean square error (LMMSE) to accommodate both the noise and signal estimations within the message passing steps of VAMP.","Numerical results demonstrate how our proposed algorithm handles non-Gaussian noise models as compared to VAMP.","This extension to general noise priors enables the use of AMP algorithms in a wider range of engineering applications where non-Gaussian noise models are more appropriate."],"url":"http://arxiv.org/abs/2402.04111v1","category":"cs.IT"}
{"created":"2024-02-06 16:03:57","title":"Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs","abstract":"ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in information withholding. This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5. Apparently, in this case an increased data exposure correlates with a greater belief in the control of information. An assignment of extreme political affiliations increased the belief in conspiracy theories. Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory.","sentences":["ChatGPT is notorious for its intransparent behavior.","This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4.","Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale.","The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4.","For traits that have shown to be interdependent in human studies, correlations were considered.","Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses.","Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4.","However, GPT-4 showed a pronounced tendency to believe in information withholding.","This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5.","Apparently, in this case an increased data exposure correlates with a greater belief in the control of information.","An assignment of extreme political affiliations increased the belief in conspiracy theories.","Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory."],"url":"http://arxiv.org/abs/2402.04110v1","category":"cs.CL"}
{"created":"2024-02-06 16:02:17","title":"Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems","abstract":"EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.","sentences":["EU directives stipulate a systematic follow-up of train delays.","In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code.","However, this delay attribution code is assigned manually, which is a complex task.","In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated.","The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration.","Further, the problem is modeled as both a hierarchical and flat approach.","The results indicate that a hierarchical approach performs better than a flat approach.","Both approaches perform better than the random uniform classifier but perform worse than the manual classification."],"url":"http://arxiv.org/abs/2402.04108v1","category":"cs.LG"}
{"created":"2024-02-06 15:58:14","title":"An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market","abstract":"Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH). The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80.","sentences":["Recently, peoples awareness of online purchases has significantly risen.","This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour.","Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation.","Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability.","In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry.","To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository.","The retail dataset consists of 541,909 customer records and eight features.","Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values.","Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH).","The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80."],"url":"http://arxiv.org/abs/2402.04103v1","category":"cs.LG"}
{"created":"2024-02-06 15:57:08","title":"Use of Multi-CNNs for Section Analysis in Static Malware Detection","abstract":"Existing research on malware detection focuses almost exclusively on the detection rate. However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst. In this aim, we propose a new model to analyze Portable Executable files. Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section. Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score.","sentences":["Existing research on malware detection focuses almost exclusively on the detection rate.","However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst.","In this aim, we propose a new model to analyze Portable Executable files.","Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section.","Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score."],"url":"http://arxiv.org/abs/2402.04102v1","category":"cs.CR"}
{"created":"2024-02-06 15:55:46","title":"VRMM: A Volumetric Relightable Morphable Head Model","abstract":"In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.","sentences":["In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling.","While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions.","Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations.","This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice.","The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions.","We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation.","Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM.","Such an approach enables accurate 3D face reconstruction from even a single portrait input.","Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling."],"url":"http://arxiv.org/abs/2402.04101v1","category":"cs.CV"}
{"created":"2024-02-06 15:54:45","title":"Bayesian sensitivity of binary pulsars to ultra-light dark matter","abstract":"Ultra-light dark matter perturbs the orbital motion of binary pulsars, in particular by causing peculiar time variations of a binary's orbital parameters, which then induce variations in the pulses' times-of-arrival. Binary pulsars have therefore been shown to be promising detectors of ultra-light dark matter. To date, the sensitivity of binary pulsars to ultra-light dark matter has only been studied for dark matter masses in a narrow resonance band around a multiple of the binary pulsar orbital frequency. In this study we devise a two-step, bayesian method that enables us to compute semi-analytically the sensitivity for all masses, also away from the resonance, and to combine several observed binaries into one global sensitivity curve. We then apply our method to the case of a universal, linearly-coupled, scalar ultra-light dark matter. We find that with next-generation radio observatories the sensitivity to the ultra-light dark matter coupling will surpass that of solar-system constraints for a decade in mass around $m\\sim10^{-21}$ $\\text{eV}$, even beyond resonance.","sentences":["Ultra-light dark matter perturbs the orbital motion of binary pulsars, in particular by causing peculiar time variations of a binary's orbital parameters, which then induce variations in the pulses' times-of-arrival.","Binary pulsars have therefore been shown to be promising detectors of ultra-light dark matter.","To date, the sensitivity of binary pulsars to ultra-light dark matter has only been studied for dark matter masses in a narrow resonance band around a multiple of the binary pulsar orbital frequency.","In this study we devise a two-step, bayesian method that enables us to compute semi-analytically the sensitivity for all masses, also away from the resonance, and to combine several observed binaries into one global sensitivity curve.","We then apply our method to the case of a universal, linearly-coupled, scalar ultra-light dark matter.","We find that with next-generation radio observatories the sensitivity to the ultra-light dark matter coupling will surpass that of solar-system constraints for a decade in mass around $m\\sim10^{-21}$ $\\text{eV}$, even beyond resonance."],"url":"http://arxiv.org/abs/2402.04099v1","category":"astro-ph.HE"}
{"created":"2024-02-06 15:53:31","title":"Random L\u00e9vy Looptrees and L\u00e9vy Maps","abstract":"Motivated by scaling limits of random planar maps in random geometry, we introduce and study L\\'evy looptrees and L\\'evy maps. They are defined using excursions of general L\\'evy processes with no negative jump and extend the known stable looptrees and stable maps, associated with stable processes. We compute in particular their fractal dimensions in terms of the upper and lower Blumenthal-Getoor exponents of the coding L\\'evy process. In a second part, we consider excursions of stable processes with a drift and prove that the corresponding looptrees interpolate continuously as the drift varies from $-\\infty$ to $\\infty$, or as the self-similarity index varies from $1$ to $2$, between a circle and the Brownian tree, whereas the corresponding maps interpolate between the Brownian tree and the Brownian sphere.","sentences":["Motivated by scaling limits of random planar maps in random geometry, we introduce and study L\\'evy looptrees and L\\'evy maps.","They are defined using excursions of general L\\'evy processes with no negative jump and extend the known stable looptrees and stable maps, associated with stable processes.","We compute in particular their fractal dimensions in terms of the upper and lower Blumenthal-Getoor exponents of the coding L\\'evy process.","In a second part, we consider excursions of stable processes with a drift and prove that the corresponding looptrees interpolate continuously as the drift varies from $-\\infty$ to $\\infty$, or as the self-similarity index varies from $1$ to $2$, between a circle and the Brownian tree, whereas the corresponding maps interpolate between the Brownian tree and the Brownian sphere."],"url":"http://arxiv.org/abs/2402.04098v1","category":"math.PR"}
{"created":"2024-02-06 15:51:39","title":"Two-step growth mechanism of the solid electrolyte interphase in argyrodyte/Li-metal contacts","abstract":"The structure and growth of the Solid Electrolyte Interphase (SEI) region between an electrolyte and an electrode is one of the most fundamental, yet less-well understood phenomena in solid-state batteries. We present a parameter-free atomistic simulation of the SEI growth for one of the currently promising solid electrolytes (Li$_6$PS$_5$Cl), based on \\textit{ab initio} trained machine learning (ML) interatomic potentials, for over 30,000 atoms during 10 ns, well-beyond the capabilities of conventional MD. This unveils a two-step growth mechanism: Li-argyrodite chemical reaction leading to the formation of an amorphous phase, followed by a kinetically slower crystallization of the reaction products into a 5Li$_2$SLi$_3$PLiCl solid solution. The simulation results support the recent, experimentally founded hypothesis of an indirect pathway of electrolyte reduction. These findings shed light on the intricate processes governing SEI evolution, providing a valuable foundation for the design and optimization of next-generation solid-state batteries.","sentences":["The structure and growth of the Solid Electrolyte Interphase (SEI) region between an electrolyte and an electrode is one of the most fundamental, yet less-well understood phenomena in solid-state batteries.","We present a parameter-free atomistic simulation of the SEI growth for one of the currently promising solid electrolytes (Li$_6$PS$_5$Cl), based on \\textit{ab initio} trained machine learning (ML) interatomic potentials, for over 30,000 atoms during 10 ns, well-beyond the capabilities of conventional MD.","This unveils a two-step growth mechanism: Li-argyrodite chemical reaction leading to the formation of an amorphous phase, followed by a kinetically slower crystallization of the reaction products into a 5Li$_2$SLi$_3$PLiCl solid solution.","The simulation results support the recent, experimentally founded hypothesis of an indirect pathway of electrolyte reduction.","These findings shed light on the intricate processes governing SEI evolution, providing a valuable foundation for the design and optimization of next-generation solid-state batteries."],"url":"http://arxiv.org/abs/2402.04095v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 15:51:10","title":"Stochastic theta methods for free stochastic differential equations","abstract":"We introduce free probability analogues of the stochastic theta methods for free stochastic differential equations, which generalize the free Euler-Maruyama method introduced by Schl\\\"{u}chtermann and Wibmer [27]. Under some mild conditions, we prove the strong convergence and exponential stability in mean square of the numerical solution. The free stochastic theta method with $\\theta=1$ can inherit the exponential stability of original equations for any given step size. Our method can offer better stability and efficiency than the free Euler-Maruyama method. Moreover, numerical results are reported to confirm these theoretical findings.","sentences":["We introduce free probability analogues of the stochastic theta methods for free stochastic differential equations, which generalize the free Euler-Maruyama method introduced by Schl\\\"{u}chtermann and Wibmer [27].","Under some mild conditions, we prove the strong convergence and exponential stability in mean square of the numerical solution.","The free stochastic theta method with $\\theta=1$ can inherit the exponential stability of original equations for any given step size.","Our method can offer better stability and efficiency than the free Euler-Maruyama method.","Moreover, numerical results are reported to confirm these theoretical findings."],"url":"http://arxiv.org/abs/2402.04094v1","category":"math.NA"}
{"created":"2024-02-06 15:48:24","title":"Macroscopic Fluctuation Theory versus large-deviation-induced GENERIC","abstract":"Recent developments in Macroscopic Fluctuation Theory show that many interacting particle systems behave macroscopically as a combination of a gradient flow with Hamiltonian dynamics. This observation leads to the natural question how these structures compare to the GENERIC framework. This paper serves as a brief survey of both fields and a comparison between them, including a number of example models to which the comparison results are applied.","sentences":["Recent developments in Macroscopic Fluctuation Theory show that many interacting particle systems behave macroscopically as a combination of a gradient flow with Hamiltonian dynamics.","This observation leads to the natural question how these structures compare to the GENERIC framework.","This paper serves as a brief survey of both fields and a comparison between them, including a number of example models to which the comparison results are applied."],"url":"http://arxiv.org/abs/2402.04092v1","category":"math-ph"}
{"created":"2024-02-06 15:48:08","title":"Sudden cosmological singularities in Aether scalar-tensor theories","abstract":"In this work we analyze the possibility of sudden cosmological singularities, also known as type-II singularities, in the background of a Friedmann-Lema\\^itre-Robertson-Walker (FLRW) geometry in an extension of General Relativity (GR) known as Aether scalar-tensor theories (AeST). Similarly to several scalar-tensor theories, we observe that sudden singularities may occur in certain AeST models at the level of the second-order time derivative of the scale factor. These singularities can either be induced by AeST's scalar field itself in the absence of a fluid matter component, or by a divergence of the pressure component of the fluid. In the latter case, one observes that the second-order time derivative of the scalar field $Q$ is also divergent at the instant the sudden singularity happens. We show that the sudden singularities can be prevented by an appropriate choice of the action, for which a divergence in the scalar field compensates the divergence in the pressure component of the matter fluid, thus preserving the regularity of the scale factor and all its time derivatives. For the models featuring a sudden singularity in the second-order time derivative of the scale factor, an analysis of the cosmographic parameters, namely the Hubble and the deceleration parameters, indicates that cosmological models featuring sudden singularities are allowed by the current cosmological measurements. Furthermore, an analysis of the jerk parameter favours cosmological models that attain a sudden singularity at a faster rate, up to a time of at most $t_s\\sim 1.2 t_0$, where $t_0$ is the current age of the universe, and with negative values for the cosmological snap parameter.","sentences":["In this work we analyze the possibility of sudden cosmological singularities, also known as type-II singularities, in the background of a Friedmann-Lema\\^itre-Robertson-Walker (FLRW) geometry in an extension of General Relativity (GR) known as Aether scalar-tensor theories (AeST).","Similarly to several scalar-tensor theories, we observe that sudden singularities may occur in certain AeST models at the level of the second-order time derivative of the scale factor.","These singularities can either be induced by AeST's scalar field itself in the absence of a fluid matter component, or by a divergence of the pressure component of the fluid.","In the latter case, one observes that the second-order time derivative of the scalar field $Q$ is also divergent at the instant the sudden singularity happens.","We show that the sudden singularities can be prevented by an appropriate choice of the action, for which a divergence in the scalar field compensates the divergence in the pressure component of the matter fluid, thus preserving the regularity of the scale factor and all its time derivatives.","For the models featuring a sudden singularity in the second-order time derivative of the scale factor, an analysis of the cosmographic parameters, namely the Hubble and the deceleration parameters, indicates that cosmological models featuring sudden singularities are allowed by the current cosmological measurements.","Furthermore, an analysis of the jerk parameter favours cosmological models that attain a sudden singularity at a faster rate, up to a time of at most $t_s\\sim 1.2 t_0$, where $t_0$ is the current age of the universe, and with negative values for the cosmological snap parameter."],"url":"http://arxiv.org/abs/2402.04091v1","category":"gr-qc"}
{"created":"2024-02-06 15:46:31","title":"The Use of a Large Language Model for Cyberbullying Detection","abstract":"The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter). Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models.","sentences":["The dominance of social media has added to the channels of bullying for perpetrators.","Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens.","This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society.","Several machine learning (ML) algorithms have been proposed for this purpose.","However, their performances are not consistent due to high class imbalance and generalisation issues.","In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks.","Unfortunately, the LLMs have not been applied extensively for CB detection.","In our paper, we explored the use of these models for cyberbullying (CB) detection.","We have prepared a new dataset (D2) from existing studies (Formspring and Twitter).","Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models."],"url":"http://arxiv.org/abs/2402.04088v1","category":"cs.CL"}
{"created":"2024-02-06 15:45:27","title":"A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation","abstract":"Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}.","sentences":["Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity.","Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks.","However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources.","In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP.","Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance.","By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training.","To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP.","Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization.","In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches.","Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}."],"url":"http://arxiv.org/abs/2402.04087v1","category":"cs.CV"}
{"created":"2024-02-06 15:36:06","title":"An Optimal House Price Prediction Algorithm: XGBoost","abstract":"An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price prediction.","sentences":["An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending.","It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood.","Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers.","To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables.","We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction.","Afterwards, we identified the key factors that influence housing costs.","Our results show that XGBoost is the best performing model for house price prediction."],"url":"http://arxiv.org/abs/2402.04082v1","category":"cs.LG"}
{"created":"2024-02-06 15:34:44","title":"Improved Generalization of Weight Space Networks via Augmentations","abstract":"Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification.","sentences":["Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks.","Unfortunately, weight space models tend to suffer from substantial overfitting.","We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets.","While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object.","To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces.","We demonstrate the effectiveness of these methods in two setups.","In classification, they improve performance similarly to having up to 10 times more data.","In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification."],"url":"http://arxiv.org/abs/2402.04081v1","category":"cs.LG"}
{"created":"2024-02-06 15:34:00","title":"Design and implementation of a real-time onboard system for a stratospheric balloon mission using commercial off-the-self components and a model-based approach","abstract":"Stratospheric balloons have emerged as an affordable and flexible alternative to traditional spacecrafts as they are implemented using commercial off-the-shelf (COTS) equipment without following strict methodologies. HERCCULES is a stratospheric balloon mission that aims to characterize the convective heat and radiative environment in the stratosphere. The purpose of this article is to present the HERCCULES onboard software (OBSW) whose design and complexity is comparable to that of satellite systems, since it must control about sixty COTS equipment using a single Raspberry Pi 4B as onboard computer and ensure the real-time requirements. Compared to similar systems, novel contributions are presented as the OBSW is developed following modelbased and component-based approaches using the TASTE toolchain from the European Space Agency (ESA) for automatic code generation. Besides, the OBSW is verified and validated following the ESA standards and the results obtained demonstrate the suitability and efficiency of the solution and the selected methodologies.","sentences":["Stratospheric balloons have emerged as an affordable and flexible alternative to traditional spacecrafts as they are implemented using commercial off-the-shelf (COTS) equipment without following strict methodologies.","HERCCULES is a stratospheric balloon mission that aims to characterize the convective heat and radiative environment in the stratosphere.","The purpose of this article is to present the HERCCULES onboard software (OBSW) whose design and complexity is comparable to that of satellite systems, since it must control about sixty COTS equipment using a single Raspberry Pi 4B as onboard computer and ensure the real-time requirements.","Compared to similar systems, novel contributions are presented as the OBSW is developed following modelbased and component-based approaches using the TASTE toolchain from the European Space Agency (ESA) for automatic code generation.","Besides, the OBSW is verified and validated following the ESA standards and the results obtained demonstrate the suitability and efficiency of the solution and the selected methodologies."],"url":"http://arxiv.org/abs/2402.04079v1","category":"eess.SY"}
{"created":"2024-02-06 15:27:32","title":"Synthetic aperture holographic third harmonic generation microscopy","abstract":"Third harmonic generation (THG) provides a valuable, label-free approach to imaging biological systems. To date, THG microscopy has been performed using point scanning methods that rely on intensity measurements lacking phase information of the complex field. We report the first demonstration of THG holographic microscopy and the reconstruction of the complex THG signal field with spatial synthetic aperture imaging. Phase distortions arising from measurement-to-measurement fluctuations and imaging components cause optical aberrations in the reconstructed THG field. We have developed an aberration-correction algorithm that estimates and corrects for these phase distortions to reconstruct the spatial synthetic aperture THG field without optical aberrations.","sentences":["Third harmonic generation (THG) provides a valuable, label-free approach to imaging biological systems.","To date, THG microscopy has been performed using point scanning methods that rely on intensity measurements lacking phase information of the complex field.","We report the first demonstration of THG holographic microscopy and the reconstruction of the complex THG signal field with spatial synthetic aperture imaging.","Phase distortions arising from measurement-to-measurement fluctuations and imaging components cause optical aberrations in the reconstructed THG field.","We have developed an aberration-correction algorithm that estimates and corrects for these phase distortions to reconstruct the spatial synthetic aperture THG field without optical aberrations."],"url":"http://arxiv.org/abs/2402.04077v1","category":"physics.optics"}
{"created":"2024-02-06 15:22:09","title":"$D^+ \\to K_s^0 \u03c0^+ \u03b7$ reaction and $a_0(980)^+$","abstract":"We study the $D^+ \\to \\bar K^0 \\pi^+ \\eta$ reaction where the $a_0(980)$ excitation plays a dominant role. We consider mechanisms of external and internal emission at the quark level, hadronize the $q \\bar q$ components into two mesons and allow these mesons to undergo final state interaction where the $a_0(980)$ state is generated. While the $a_0(980)$ production is the dominant term, we also find other terms in the reaction that interfere with this production mode and, through interference with it, lead to a shape of the $a_0(980)$ significantly different from the one observed in other experiments, with an apparently much larger width.","sentences":["We study the $D^+ \\to \\bar K^0 \\pi^+ \\eta$ reaction where the $a_0(980)$ excitation plays a dominant role.","We consider mechanisms of external and internal emission at the quark level, hadronize the $q \\bar q$ components into two mesons and allow these mesons to undergo final state interaction where the $a_0(980)$ state is generated.","While the $a_0(980)$ production is the dominant term, we also find other terms in the reaction that interfere with this production mode and, through interference with it, lead to a shape of the $a_0(980)$ significantly different from the one observed in other experiments, with an apparently much larger width."],"url":"http://arxiv.org/abs/2402.04073v1","category":"hep-ph"}
{"created":"2024-02-06 15:12:34","title":"On the Modelling of Ship Wakes in S-Band SAR Images and an Application to Ship Identification","abstract":"We present a novel ship wake simulation system for generating S-band Synthetic Aperture Radar (SAR) images, and demonstrate the use of such imagery for the classification of ships based on their wake signatures via a deep learning approach. Ship wakes are modeled through the linear superposition of wind-induced sea elevation and the Kelvin wakes model of a moving ship. Our SAR imaging simulation takes into account frequency-dependent radar parameters, i.e., the complex dielectric constant ($\\varepsilon$) and the relaxation rate ($\\mu$) of seawater. The former was determined through the Debye model while the latter was estimated for S-band SAR based on pre-existing values for the L, C, and X-bands. The results show good agreement between simulated and real imagery upon visual inspection. The results of implementing different training strategies are also reported, showcasing a notable improvement in accuracy of classifier achieved by integrating real and simulated SAR images during the training.","sentences":["We present a novel ship wake simulation system for generating S-band Synthetic Aperture Radar (SAR) images, and demonstrate the use of such imagery for the classification of ships based on their wake signatures via a deep learning approach.","Ship wakes are modeled through the linear superposition of wind-induced sea elevation and the Kelvin wakes model of a moving ship.","Our SAR imaging simulation takes into account frequency-dependent radar parameters, i.e., the complex dielectric constant ($\\varepsilon$) and the relaxation rate ($\\mu$) of seawater.","The former was determined through the Debye model while the latter was estimated for S-band SAR based on pre-existing values for the L, C, and X-bands.","The results show good agreement between simulated and real imagery upon visual inspection.","The results of implementing different training strategies are also reported, showcasing a notable improvement in accuracy of classifier achieved by integrating real and simulated SAR images during the training."],"url":"http://arxiv.org/abs/2402.04066v1","category":"eess.IV"}
{"created":"2024-02-06 15:10:26","title":"Mice with Woodin cardinals from a Reinhardt","abstract":"Suppose there is a Reinhardt cardinal. Then (1) $M_n(X)$ exists and is fully iterable (above $X$) for every transitive set $X$ and every $n<\\omega$ (here $M_n(X)$ denotes the canonical minimal proper class inner model containing $X$ and having $n$ Woodin cardinals above the rank of $X$); and (2) Projective Determinacy holds in every set generic extension.","sentences":["Suppose there is a Reinhardt cardinal.","Then (1) $M_n(X)$ exists and is fully iterable (above $X$) for every transitive set $X$ and every $n<\\omega$ (here $M_n(X)$ denotes the canonical minimal proper class inner model containing $X$ and having $n$ Woodin cardinals above the rank of $X$); and (2) Projective Determinacy holds in every set generic extension."],"url":"http://arxiv.org/abs/2402.04065v1","category":"math.LO"}
{"created":"2024-02-06 15:09:50","title":"Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing","abstract":"Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.","sentences":["Road pavement detection and segmentation are critical for developing autonomous road repair systems.","However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes.","We propose a novel end-to-end method for multi-class road defect detection and segmentation.","The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions.","Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned.","To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes.","The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods."],"url":"http://arxiv.org/abs/2402.04064v1","category":"cs.CV"}
{"created":"2024-02-06 15:05:40","title":"Link Prediction with Relational Hypergraphs","abstract":"Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction. Our study therefore unlocks applications of graph neural networks to fully relational structures.","sentences":["Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications.","Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs.","The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$).","In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms.","Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks.","The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction.","Our study therefore unlocks applications of graph neural networks to fully relational structures."],"url":"http://arxiv.org/abs/2402.04062v1","category":"cs.LG"}
{"created":"2024-02-06 15:03:53","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","abstract":"The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}.","sentences":["The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis.","Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks.","In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods.","First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations.","We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks.","Finally, the open issues for future research on multivariate time series imputation are pointed out.","All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}."],"url":"http://arxiv.org/abs/2402.04059v1","category":"cs.LG"}
{"created":"2024-02-06 15:02:54","title":"Latitudinal Variation of Clouds' Structure Responsible for Venus' Cold Collar","abstract":"Global Climate Models (GCM) are very useful tools to study theoretically the general dynamics and specific phenomena in planetary atmospheres. In the case of Venus, several GCMs succeeded in reproducing the atmosphere's superrotation and the global temperature field. However, the highly variable polar temperature and the permanent cold collar have not been reproduced satisfactorily yet. Here we improve the radiative transfer scheme of the Institut Pierre Simon Laplace Venus GCM in order to numerically simulate the polar thermal features in Venus atmosphere. The main difference with the previous model is that we now take into account the latitudinal variation of the cloud structure. Both solar heating rates and infrared cooling rates have been modified to consider the cloud top's altitude decrease toward the poles and the variation in latitude of the different particle modes' abundances. A new structure that closely resembles the observed cold collar appears in the average temperature field at $2\\times10^{4} - 4\\times10^{3}$~Pa ($\\sim62 - 66$~km) altitude range and $60^{\\circ} - 90^{\\circ}$ latitude band. It is not isolated from the pole as in the observation-based maps, but the obtained temperature values (220~K) are in good agreement with observed values. Temperature polar maps across this region show an inner warm region where the polar vortex is observed, but the obtained 230~K average value is colder than the observed mean value and the simulated horizontal structure does not show the fine-scale features present within the vortex. Our study shows that the cloud structure is essential in the cold collar formation. Although our analysis focuses on the improvement of the radiative forcing and the variations it causes in the thermal structure, polar dynamics is definitely affected by this modified environment and a noteworthy upwelling motion is found in the cold collar area.","sentences":["Global Climate Models (GCM) are very useful tools to study theoretically the general dynamics and specific phenomena in planetary atmospheres.","In the case of Venus, several GCMs succeeded in reproducing the atmosphere's superrotation and the global temperature field.","However, the highly variable polar temperature and the permanent cold collar have not been reproduced satisfactorily yet.","Here we improve the radiative transfer scheme of the Institut Pierre Simon Laplace Venus GCM in order to numerically simulate the polar thermal features in Venus atmosphere.","The main difference with the previous model is that we now take into account the latitudinal variation of the cloud structure.","Both solar heating rates and infrared cooling rates have been modified to consider the cloud top's altitude decrease toward the poles and the variation in latitude of the different particle modes' abundances.","A new structure that closely resembles the observed cold collar appears in the average temperature field at $2\\times10^{4} - 4\\times10^{3}$~Pa ($\\sim62 - 66$~km) altitude range and $60^{\\circ} - 90^{\\circ}$ latitude band.","It is not isolated from the pole as in the observation-based maps, but the obtained temperature values (220~K) are in good agreement with observed values.","Temperature polar maps across this region show an inner warm region where the polar vortex is observed, but the obtained 230~K average value is colder than the observed mean value and the simulated horizontal structure does not show the fine-scale features present within the vortex.","Our study shows that the cloud structure is essential in the cold collar formation.","Although our analysis focuses on the improvement of the radiative forcing and the variations it causes in the thermal structure, polar dynamics is definitely affected by this modified environment and a noteworthy upwelling motion is found in the cold collar area."],"url":"http://arxiv.org/abs/2402.04057v1","category":"astro-ph.EP"}
{"created":"2024-02-06 15:00:08","title":"More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms","abstract":"We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory. Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. For previous approaches, this could only happen indirectly, by means of learning prior distributions over models. In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks. The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms. Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms.","sentences":["We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory.","Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized.","For previous approaches, this could only happen indirectly, by means of learning prior distributions over models.","In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks.","The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms.","Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms."],"url":"http://arxiv.org/abs/2402.04054v1","category":"cs.LG"}
{"created":"2024-02-06 14:56:52","title":"Ramification filtration via deformations, II","abstract":"Let $\\mathcal K$ be a field of formal Laurent series with coefficients in a finite field of characteristic $p$. For $M\\in\\mathbb{N}$, let $\\mathcal G_{<p,M}$ be the maximal quotient of the Galois group of $\\mathcal K$ of period $p^M$ and nilpotent class $<p$ and $\\{\\mathcal G_{<p,M}^{(v)}\\}_{v\\geqslant 0}$ -- the filtration by ramification subgroups in upper numbering. Let $\\mathcal G_{<p,M}=G(\\mathcal L)$ be the identification of nilpotent Artin-Schreier theory: here $G(\\mathcal L)$ is the group obtained from a profinite Lie $\\mathbb{Z}/p^M$-algebra $\\mathcal L$ via the Campbell-Hausdorff composition law. We obtain a \"geometrical\" construction of the ideals $\\mathcal L^{(v)}$ such that $G(\\mathcal L^{(v)})=\\mathcal G_{<p,M}^{(v)}$. Given $v_0\\geqslant 1$, we construct a decreasing central filtration $\\mathcal L(w)$, $1\\leqslant w\\leqslant p$, on $\\mathcal L$, an epimorphism of Lie $\\mathbb{Z}/p^M$-algebras $\\bar{\\mathcal V}: \\bar{\\mathcal L}^{\\dag }\\to \\bar{\\mathcal L}:=\\mathcal L/\\mathcal L(p)$ and unipotent action $\\Omega $ of $\\mathbb{Z} /p^M$, on $\\bar{\\mathcal L}^{\\dag }$. Suppose $d\\Omega =B^{\\dag }$ and $\\bar{\\mathcal L}^{\\dag [v_0]}$ is the ideal of $\\bar{\\mathcal L}^{\\dag }$ generated by $B^{\\dag }(\\bar{\\mathcal L}^{\\dag })$. The main result states that $\\mathcal L^{(v_0)}$ appears as the preimage of the ideal in $\\bar{\\mathcal L}$ generated by $\\bar{\\mathcal V}B^{\\dag }(\\bar{\\mathcal L}^{\\dag [v_0]})$. The results of this paper generalise (and also considerably simplify) the author results in the context of the maximal quotient $\\mathcal{G}_{<p,1}$ of period $p$.","sentences":["Let $\\mathcal K$ be a field of formal Laurent series with coefficients in a finite field of characteristic $p$. For $M\\in\\mathbb{N}$, let $\\mathcal G_{<p,M}$ be the maximal quotient of the Galois group of $\\mathcal K$ of period $p^M$ and nilpotent class $<p$ and $\\{\\mathcal G_{<p,M}^{(v)}\\}_{v\\geqslant 0}$ -- the filtration by ramification subgroups in upper numbering.","Let $\\mathcal G_{<p,M}=G(\\mathcal L)$ be the identification of nilpotent Artin-Schreier theory: here $G(\\mathcal L)$ is the group obtained from a profinite Lie $\\mathbb{Z}/p^M$-algebra $\\mathcal L$ via the Campbell-Hausdorff composition law.","We obtain a \"geometrical\" construction of the ideals $\\mathcal L^{(v)}$ such that $G(\\mathcal L^{(v)})=\\mathcal G_{<p,M}^{(v)}$. Given $v_0\\geqslant 1$, we construct a decreasing central filtration $\\mathcal L(w)$, $1\\leqslant w\\leqslant p$, on $\\mathcal L$, an epimorphism of Lie $\\mathbb{Z}/p^M$-algebras $\\bar{\\mathcal V}: \\bar{\\mathcal L}^{\\dag }\\to \\bar{\\mathcal L}:=\\mathcal L/\\mathcal L(p)$ and unipotent action $\\Omega $ of $\\mathbb{Z} /p^M$, on $\\bar{\\mathcal L}^{\\dag }$.","Suppose $d\\Omega =B^{\\dag }$ and $\\bar{\\mathcal L}^{\\dag","[v_0]}$ is the ideal of $\\bar{\\mathcal L}^{\\dag }$ generated by $B^{\\dag }(\\bar{\\mathcal L}^{\\dag })$.","The main result states that $\\mathcal L^{(v_0)}$ appears as the preimage of the ideal in $\\bar{\\mathcal L}$ generated by $\\bar{\\mathcal V}B^{\\dag }(\\bar{\\mathcal L}^{\\dag","[v_0]})$. The results of this paper generalise (and also considerably simplify) the author results in the context of the maximal quotient $\\mathcal{G}_{<p,1}$ of period $p$."],"url":"http://arxiv.org/abs/2402.04053v1","category":"math.NT"}
{"created":"2024-02-06 14:53:19","title":"Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models","abstract":"With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method.","sentences":["With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks.","Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership.","This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model.","CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style.","Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules.","These modules are optimized by a novel collaborative training algorithm.","Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT.","The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries.","Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method."],"url":"http://arxiv.org/abs/2402.04050v1","category":"cs.LG"}
{"created":"2024-02-06 14:51:55","title":"Systematic Biases in LLM Simulations of Debates","abstract":"Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.","sentences":["Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately.","However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors.","In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates.","Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives.","This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans.","We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases.","These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations."],"url":"http://arxiv.org/abs/2402.04049v1","category":"cs.CL"}
{"created":"2024-02-06 14:48:34","title":"Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes","abstract":"Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are crucial. Additionally, we introduce a new synthetic dataset that incorporates edge values. Furthermore, we propose a novel application that greatly benefits from the method due to its nature: the generation of traffic scenes represented as graphs. Our method outperforms other graph generation methods, demonstrating a significant advantage in edge-related measures.","sentences":["Graph generation is integral to various engineering and scientific disciplines.","Nevertheless, existing methodologies tend to overlook the generation of edge attributes.","However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts.","Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components.","To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components.","Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process.","We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are crucial.","Additionally, we introduce a new synthetic dataset that incorporates edge values.","Furthermore, we propose a novel application that greatly benefits from the method due to its nature: the generation of traffic scenes represented as graphs.","Our method outperforms other graph generation methods, demonstrating a significant advantage in edge-related measures."],"url":"http://arxiv.org/abs/2402.04046v1","category":"cs.SI"}
{"created":"2024-02-06 14:41:15","title":"Universal entanglement spectrum in gapless symmetry protected topological states","abstract":"Quantum entanglement marks a definitive feature of topological states. However, the entanglement spectrum remains insufficiently explored for topological states without a bulk energy gap. Using a combination of field theory and numerical techniques, we accurately calculate and analyze the entanglement spectrum of gapless symmetry protected topological states in one dimension. We highlight that the universal entanglement spectrum not only encodes the nontrivial edge degeneracy, generalizing the Li-Haldane conjecture to gapless topological states, but also contains the operator content of the underlying boundary conformal field theory. This implies that the bulk wave function can act as a fingerprint of both quantum criticality and topology in gapless symmetry protected topological states. We also identify a symmetry enriched conformal boundary condition that goes beyond the conventional conformal boundary condition.","sentences":["Quantum entanglement marks a definitive feature of topological states.","However, the entanglement spectrum remains insufficiently explored for topological states without a bulk energy gap.","Using a combination of field theory and numerical techniques, we accurately calculate and analyze the entanglement spectrum of gapless symmetry protected topological states in one dimension.","We highlight that the universal entanglement spectrum not only encodes the nontrivial edge degeneracy, generalizing the Li-Haldane conjecture to gapless topological states, but also contains the operator content of the underlying boundary conformal field theory.","This implies that the bulk wave function can act as a fingerprint of both quantum criticality and topology in gapless symmetry protected topological states.","We also identify a symmetry enriched conformal boundary condition that goes beyond the conventional conformal boundary condition."],"url":"http://arxiv.org/abs/2402.04042v1","category":"cond-mat.str-el"}
{"created":"2024-02-06 14:40:32","title":"A discrete model of competing species sharing a parasite","abstract":"In this work we develop a discrete model of competing species affected by a common parasite. We analyze the influence of the fast development of the shared disease on the community dynamics. The model is presented under the form of a two time scales discrete system with four variables. Thus, it becomes analytically tractable with the help of the appropriate reduction method. The 2-dimensional reduced system, that has the same the asymptotic behaviour of the full model, is a generalization of the Leslie-Gower competition model. It has the unfrequent property in this kind of models of including multiple equilibrium attractors of mixed type. The analysis of the reduced system shows that parasites can completely alter the outcome of competition depending on the parasite's basic reproductive number R0. In some cases, initial conditions decide among several exclusion or coexistence scenarios.","sentences":["In this work we develop a discrete model of competing species affected by a common parasite.","We analyze the influence of the fast development of the shared disease on the community dynamics.","The model is presented under the form of a two time scales discrete system with four variables.","Thus, it becomes analytically tractable with the help of the appropriate reduction method.","The 2-dimensional reduced system, that has the same the asymptotic behaviour of the full model, is a generalization of the Leslie-Gower competition model.","It has the unfrequent property in this kind of models of including multiple equilibrium attractors of mixed type.","The analysis of the reduced system shows that parasites can completely alter the outcome of competition depending on the parasite's basic reproductive number R0.","In some cases, initial conditions decide among several exclusion or coexistence scenarios."],"url":"http://arxiv.org/abs/2402.04041v1","category":"math.DS"}
{"created":"2024-02-06 14:34:17","title":"PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network","abstract":"Graph neural networks (GNNs) have gained popularity for various graph-related tasks. However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks. Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. As corollaries, we derive better PAC-Bayesian robust generalization bounds for GCN in the standard setting, which improve the bounds in (Liao et al., 2020) by avoiding exponential dependence on the maximum node degree.","sentences":["Graph neural networks (GNNs) have gained popularity for various graph-related tasks.","However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks.","Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks.","In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework.","Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models.","Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree.","As corollaries, we derive better PAC-Bayesian robust generalization bounds for GCN in the standard setting, which improve the bounds in (Liao et al., 2020) by avoiding exponential dependence on the maximum node degree."],"url":"http://arxiv.org/abs/2402.04038v1","category":"stat.ML"}
{"created":"2024-02-06 14:26:22","title":"HEAM : Hashed Embedding Acceleration using Processing-In-Memory","abstract":"In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables. The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table. This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity. This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline.","sentences":["In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations.","Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth.","However, these solutions fall short when dealing with the expanding size of personalized recommendation systems.","Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers.","Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources.","This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables.","The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table.","This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity.","This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline."],"url":"http://arxiv.org/abs/2402.04032v1","category":"cs.AR"}
{"created":"2024-02-06 14:26:02","title":"Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation","abstract":"This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm.","sentences":["This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps.","Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images.","By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data).","Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models.","The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm."],"url":"http://arxiv.org/abs/2402.04031v1","category":"cs.CV"}
{"created":"2024-02-06 14:24:28","title":"AlbNews: A Corpus of Headlines for Topic Modeling in Albanian","abstract":"The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.","sentences":["The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks.","This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian.","The data can be freely used for conducting topic modeling research.","We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples.","These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments."],"url":"http://arxiv.org/abs/2402.04028v1","category":"cs.CL"}
{"created":"2024-02-06 14:12:46","title":"A General Theory for Kernel Packets: from state space model to compactly supported basis","abstract":"It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \\leq t_1$, 0 $\\leq j \\leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\\phi^{(j)}(t)=0$ for any $t\\not\\in(t_0,t_{2m})$ and $j=0,\\cdots,m-1$. KPs further reduces the prediction time of GP to O(log n) or even O(1) and can be applied to more general problems involving the derivative of GPs.","sentences":["It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points.","We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \\leq t_1$, 0","$\\leq j \\leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\\phi^{(j)}(t)=0$ for any $t\\not\\in(t_0,t_{2m})$ and $j=0,\\cdots,m-1$. KPs further reduces the prediction time of GP to O(log n) or even O(1) and can be applied to more general problems involving the derivative of GPs."],"url":"http://arxiv.org/abs/2402.04022v1","category":"stat.ML"}
{"created":"2024-02-06 14:11:18","title":"ALE spaces and nodal curves","abstract":"We consider the twistor theory approach to Kronheimer's ALE metrics on resolutions of the quotient of C^2 by a finite subgroup of SU(2). The circle action on the 4-manifold induces a C^* action on a compactification of the twistor space and we identify the orbit of a generic twistor line as a nodal rational curve in a particular cohomology class of a projective rational surface. Using the results of N.Honda et al we identify this surface with the minitwistor space for the Einstein-Weyl structure on the 3-dimensional quotient of the ALE space by the circle action.","sentences":["We consider the twistor theory approach to Kronheimer's ALE metrics on resolutions of the quotient of C^2 by a finite subgroup of SU(2).","The circle action on the 4-manifold induces a C^* action on a compactification of the twistor space and we identify the orbit of a generic twistor line as a nodal rational curve in a particular cohomology class of a projective rational surface.","Using the results of N.Honda et al we identify this surface with the minitwistor space for the Einstein-Weyl structure on the 3-dimensional quotient of the ALE space by the circle action."],"url":"http://arxiv.org/abs/2402.04021v1","category":"math.DG"}
{"created":"2024-02-06 14:09:13","title":"Colliding null matter with a specific stress tensor","abstract":"The accretion disks around black holes consist of infalling matter boosted almost to the speed of light making collisions with opposite counterpart. This is the rough picture occurring near black holes or other strongly gravitating centers that produce observed phenomena such as astrophysical jets. A toy model that can be considered imitating such a process is colliding null sources in general relativity. We present such a simple model projected into the plane of null coordinates that takes into account only neutral sources. We show that even at such a simplified model, uncharged and non-rotating, it is possible to obtain jet-like ejections albeit they lie below the horizon. In the present study the spacetime consists of either one of i) a cloud of strings, ii) a global monopole, iii) a particular model of bumblebee gravity, all described by a similar class of stress-energy tensor. There are gravitational waves accompanying the null sources and naturally collision of gravitational waves is also taken into account. After the collision, the spacetime contains both null and non-null sources, followed by trailing gravitational radiations. Locally the interaction region of the colliding null-sources and gravitational waves is isometric to the static background spacetime.","sentences":["The accretion disks around black holes consist of infalling matter boosted almost to the speed of light making collisions with opposite counterpart.","This is the rough picture occurring near black holes or other strongly gravitating centers that produce observed phenomena such as astrophysical jets.","A toy model that can be considered imitating such a process is colliding null sources in general relativity.","We present such a simple model projected into the plane of null coordinates that takes into account only neutral sources.","We show that even at such a simplified model, uncharged and non-rotating, it is possible to obtain jet-like ejections albeit they lie below the horizon.","In the present study the spacetime consists of either one of i) a cloud of strings, ii) a global monopole, iii) a particular model of bumblebee gravity, all described by a similar class of stress-energy tensor.","There are gravitational waves accompanying the null sources and naturally collision of gravitational waves is also taken into account.","After the collision, the spacetime contains both null and non-null sources, followed by trailing gravitational radiations.","Locally the interaction region of the colliding null-sources and gravitational waves is isometric to the static background spacetime."],"url":"http://arxiv.org/abs/2402.04017v1","category":"gr-qc"}
{"created":"2024-02-06 14:05:05","title":"Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously","abstract":"Availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release. Ideally, the obtained unlearnability prevents algorithms from training usable models. When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection. Through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection. Different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications.","sentences":["Availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release.","Ideally, the obtained unlearnability prevents algorithms from training usable models.","When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection.","Through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection.","Different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL.","Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications."],"url":"http://arxiv.org/abs/2402.04010v1","category":"cs.LG"}
{"created":"2024-02-06 14:03:15","title":"Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning","abstract":"In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters. LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30\\% of GPU memory footprint and 60\\% of training time compared to existing PEFT methods, but achieves significantly higher accuracy.","sentences":["In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed.","Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning.","We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network.","LAST trains a side-network composed of only low-rank self-attention modules.","By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge.","We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters.","LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30\\% of GPU memory footprint and 60\\% of training time compared to existing PEFT methods, but achieves significantly higher accuracy."],"url":"http://arxiv.org/abs/2402.04009v1","category":"cs.CV"}
{"created":"2024-02-06 14:03:08","title":"Understanding spectral artefacts in SKA-LOW 21-cm cosmology experiments: the impact of cable reflections","abstract":"The Cosmic Dawn marks the first star formations and preceded the Epoch-of-Reionization, when the Universe underwent a fundamental transformation propelled by the radiation from these first stars and galaxies. Interferometric 21-cm experiments aim to probe redshifted neutral hydrogen signals from these periods, constraining the conditions of the early Universe. The SKA-LOW instrument of the Square Kilometre Array telescope is envisaged to be the largest and most sensitive radio telescope at m and cm wavelengths. The latest Aperture Array Verification Systems feature 7m coaxial transmission lines connecting the Low Noise Amplifiers to optical transmitters at the front of the analogue-receiving chain. An impedance mismatch between these components results in a partially reflected electromagnetic signal, which introduces chromatic aberrations in the instrument bandpass. This causes power from the foreground signals to appear at higher delays, potentially contaminating the EoR window, a region at which the 21-cm signal should be detectable. We present an end-to-end simulation pipeline for SKA-LOW using a composite sky model combining radio foregrounds from The GLEAM Survey, Haslam $408$MHz, and a $1.5$cGpc 21-cm brightness temperature cube generated with the 21cmSPACE simulator. Iterating a parametric approach, we derive a model for the scattering parameters of a coaxial transmission line in terms of its specifications and bulk material properties. Assuming identical cables of length $\\leq 15.0$m with impedance mismatch $\\leq 10\\Omega$ confines the reflection to k-modes below the EoR window. However, we demonstrate that even a $0.1$% length tolerance introduces contamination with an RMSE of $\\sim 10$% across all accessible k-modes.","sentences":["The Cosmic Dawn marks the first star formations and preceded the Epoch-of-Reionization, when the Universe underwent a fundamental transformation propelled by the radiation from these first stars and galaxies.","Interferometric 21-cm experiments aim to probe redshifted neutral hydrogen signals from these periods, constraining the conditions of the early Universe.","The SKA-LOW instrument of the Square Kilometre Array telescope is envisaged to be the largest and most sensitive radio telescope at m and cm wavelengths.","The latest Aperture Array Verification Systems feature 7m coaxial transmission lines connecting the Low Noise Amplifiers to optical transmitters at the front of the analogue-receiving chain.","An impedance mismatch between these components results in a partially reflected electromagnetic signal, which introduces chromatic aberrations in the instrument bandpass.","This causes power from the foreground signals to appear at higher delays, potentially contaminating the EoR window, a region at which the 21-cm signal should be detectable.","We present an end-to-end simulation pipeline for SKA-LOW using a composite sky model combining radio foregrounds from The GLEAM Survey, Haslam $408$MHz, and a $1.5$cGpc 21-cm brightness temperature cube generated with the 21cmSPACE simulator.","Iterating a parametric approach, we derive a model for the scattering parameters of a coaxial transmission line in terms of its specifications and bulk material properties.","Assuming identical cables of length $\\leq 15.0$m with impedance mismatch $\\leq 10\\Omega$ confines the reflection to k-modes below the EoR window.","However, we demonstrate that even a $0.1$% length tolerance introduces contamination with an RMSE of $\\sim 10$% across all accessible k-modes."],"url":"http://arxiv.org/abs/2402.04008v1","category":"astro-ph.CO"}
{"created":"2024-02-06 13:59:56","title":"Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought","abstract":"During both pretraining and fine-tuning, Large Language Models (\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \\textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \\textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \\textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained models both prompted and fine-tuned on noised datasets with varying levels of dataset contamination and intensity. We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise. In contrast, few-shot prompted models appear more sensitive to even static noise. We conclude with a discussion of how our findings impact noise filtering best-practices, in particular emphasizing the importance of removing samples containing destructive dynamic noise with global errors.","sentences":["During both pretraining and fine-tuning, Large Language Models (\\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality.","Both phases of training typically involve heuristically filtering out ``low-quality'' or \\textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance.","In this work, we study how noise in chain of thought (\\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks.","First, we develop the Traced Integer (\\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers.","We then define two types of noise: \\textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \\textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed.","We then evaluate the test performance of pretrained models both prompted and fine-tuned on noised datasets with varying levels of dataset contamination and intensity.","We find fine-tuned models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise.","In contrast, few-shot prompted models appear more sensitive to even static noise.","We conclude with a discussion of how our findings impact noise filtering best-practices, in particular emphasizing the importance of removing samples containing destructive dynamic noise with global errors."],"url":"http://arxiv.org/abs/2402.04004v1","category":"cs.LG"}
{"created":"2024-02-06 13:58:53","title":"Generalized Ces\u00e0ro operators in weighted Banach spaces of analytic functions with sup-norms","abstract":"An investigation is made of the generalized Ces\\`aro operators $C_t$, for $t\\in [0,1]$, when they act on the space $H(\\mathbb{D})$ of holomorphic functions on the open unit disc $\\mathbb{D}$, on the Banach space $H^\\infty$ of bounded analytic functions and on the weighted Banach spaces $H_v^\\infty$ and $H_v^0$ with their sup-norms. Of particular interest are the continuity, compactness, spectrum and point spectrum of $C_t$ as well as their linear dynamics and mean ergodicity.","sentences":["An investigation is made of the generalized Ces\\`aro operators $C_t$, for $t\\in [0,1]$, when they act on the space $H(\\mathbb{D})$ of holomorphic functions on the open unit disc $\\mathbb{D}$, on the Banach space $H^\\infty$ of bounded analytic functions and on the weighted Banach spaces $H_v^\\infty$ and $H_v^0$ with their sup-norms.","Of particular interest are the continuity, compactness, spectrum and point spectrum of $C_t$ as well as their linear dynamics and mean ergodicity."],"url":"http://arxiv.org/abs/2402.04003v1","category":"math.FA"}
{"created":"2024-02-06 13:52:21","title":"Probing 95 GeV Higgs in the 2HDM Type-III","abstract":"The recent results reported by the CMS collaboration, indicating \"bumps\" in the $\\gamma\\gamma$ and $\\tau\\tau$ channels at $m_\\phi\\approx 95$ GeV, provide interesting hints for new physics. We find that the lightest Higgs state of the general 2HDM (2HDM Type-III) can perfectly and simultaneously accommodate the two excesses alongside with the LEP long-standing anomaly observed in the $b\\bar{b}$ channel while meeting all theoretical and experimental requirements. Furthermore, the study predicts an enhanced production process for the SM-like Higgs in $pp\\to t\\bar t H_{\\rm SM}$, offering a testable hypothesis for future experiments.","sentences":["The recent results reported by the CMS collaboration, indicating \"bumps\" in the $\\gamma\\gamma$ and $\\tau\\tau$ channels at $m_\\phi\\approx 95$ GeV, provide interesting hints for new physics.","We find that the lightest Higgs state of the general 2HDM (2HDM Type-III) can perfectly and simultaneously accommodate the two excesses alongside with the LEP long-standing anomaly observed in the $b\\bar{b}$ channel while meeting all theoretical and experimental requirements.","Furthermore, the study predicts an enhanced production process for the SM-like Higgs in $pp\\to t\\bar t H_{\\rm SM}$, offering a testable hypothesis for future experiments."],"url":"http://arxiv.org/abs/2402.03998v1","category":"hep-ph"}
{"created":"2024-02-06 13:50:52","title":"Generalized almost-K\u00e4hler-Ricci solitons","abstract":"We generalize K\\\"ahler-Ricci solitons to the almost-K\\\"ahler setting as the zeros of Inoue's moment map \\cite{MR4017922}, and show that their existence is an obstruction to the existence of first-Chern-Einstein almost-K\\\"ahler metrics on compact symplectic Fano manifolds. We prove deformation results of such metrics in the $4$-dimensional case. Moreover, we study the Lie algebra of holomorphic vector fields on $2n$-dimensional compact symplectic Fano manifolds admitting generalized almost-K\\\"ahler-Ricci solitons. In particular, we partially extend Matsushima's theorem \\cite{MR0094478} to compact first-Chern-Einstein almost-K\\\"ahler manifolds.","sentences":["We generalize K\\\"ahler-Ricci solitons to the almost-K\\\"ahler setting as the zeros of Inoue's moment map \\cite{MR4017922}, and show that their existence is an obstruction to the existence of first-Chern-Einstein almost-K\\\"ahler metrics on compact symplectic Fano manifolds.","We prove deformation results of such metrics in the $4$-dimensional case.","Moreover, we study the Lie algebra of holomorphic vector fields on $2n$-dimensional compact symplectic Fano manifolds admitting generalized almost-K\\\"ahler-Ricci solitons.","In particular, we partially extend Matsushima's theorem \\cite{MR0094478} to compact first-Chern-Einstein almost-K\\\"ahler manifolds."],"url":"http://arxiv.org/abs/2402.03996v1","category":"math.DG"}
{"created":"2024-02-06 13:50:06","title":"$\\mathrm{ku}$-theoretic spectral decompositions for spheres and projective spaces","abstract":"Ben-Zvi--Sakellaridis--Venkatesh described a conjectural extension of the geometric Satake equivalence to spherical varieties, whose spectral decomposition is described by Hamiltonian varieties. The goal of this article is to study their conjecture, especially in the case of spherical varieties of relative rank 1, using tools from homotopy theory. Our discussion relates their conjecture to classical topics in homotopy theory such as the EHP sequence and Hopf fibrations, as well as more modern topics such as Hochschild (co)homology. We will also study an analogue of the derived geometric Satake equivalence and of the Ben-Zvi--Sakellaridis--Venkatesh conjecture with coefficients in connective complex K-theory. In this generalized setting, the dual *group* (a la Langlands, Gaitsgory--Nadler, Sakellaridis--Venkatesh, Knop--Schalke) remains unchanged, but the specific dual \"representation\" of the dual group changes. On the spectral/Langlands dual side, we expect that the appropriate replacement of Hamiltonian varieties are given by what we term \"ku-Hamiltonian varieties\"; this is a notion interpolating between Hamiltonian and quasi-Hamiltonian varieties (a la Alekseev--Malkin--Meinrenken). Finally, we suggest possible generalizations to more exotic cohomology theories such as complex cobordism.","sentences":["Ben-Zvi--Sakellaridis--Venkatesh described a conjectural extension of the geometric Satake equivalence to spherical varieties, whose spectral decomposition is described by Hamiltonian varieties.","The goal of this article is to study their conjecture, especially in the case of spherical varieties of relative rank 1, using tools from homotopy theory.","Our discussion relates their conjecture to classical topics in homotopy theory such as the EHP sequence and Hopf fibrations, as well as more modern topics such as Hochschild (co)homology.","We will also study an analogue of the derived geometric Satake equivalence and of the Ben-Zvi--Sakellaridis--Venkatesh conjecture with coefficients in connective complex K-theory.","In this generalized setting, the dual *group* (a la Langlands, Gaitsgory--Nadler, Sakellaridis--Venkatesh, Knop--Schalke) remains unchanged, but the specific dual \"representation\" of the dual group changes.","On the spectral/Langlands dual side, we expect that the appropriate replacement of Hamiltonian varieties are given by what we term \"ku-Hamiltonian varieties\"; this is a notion interpolating between Hamiltonian and quasi-Hamiltonian varieties (a la Alekseev--Malkin--Meinrenken).","Finally, we suggest possible generalizations to more exotic cohomology theories such as complex cobordism."],"url":"http://arxiv.org/abs/2402.03995v1","category":"math.AT"}
{"created":"2024-02-06 13:45:19","title":"Bitangents of real algebraic curves: signed count and constructions","abstract":"We study real bitangents of real algebraic plane curves from two perspectives. We first show that there exists a signed count of such bitangents that only depends on the real topological type of the curve. From this follows that a generic real algebraic curve of even degree $d$ has at least $\\frac{d(d-2)}{2}$ real bitangents. Next we explain how to locate (real) bitangents of a (real) perturbation of a multiple (real) conic in $\\mathbb{C}P^2$. As main applications, we exhibit a real sextic with a total of $318$ real bitangents and 6 complex ones, and perform asymptotical constructions that give the best, to our knowledge, number of real bitangents of real algebraic plane curves of a given degree.","sentences":["We study real bitangents of real algebraic plane curves from two perspectives.","We first show that there exists a signed count of such bitangents that only depends on the real topological type of the curve.","From this follows that a generic real algebraic curve of even degree $d$ has at least $\\frac{d(d-2)}{2}$ real bitangents.","Next we explain how to locate (real) bitangents of a (real) perturbation of a multiple (real) conic in $\\mathbb{C}P^2$. As main applications, we exhibit a real sextic with a total of $318$ real bitangents and 6 complex ones, and perform asymptotical constructions that give the best, to our knowledge, number of real bitangents of real algebraic plane curves of a given degree."],"url":"http://arxiv.org/abs/2402.03993v1","category":"math.AG"}
{"created":"2024-02-06 13:45:01","title":"Space Group Constrained Crystal Generation","abstract":"Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account. Experiments on several popular datasets verify the benefit of the involvement of the space group constraint, and show that our DiffCSP++ achieves promising performance on crystal structure prediction, ab initio crystal generation and controllable generation with customized space groups.","sentences":["Crystals are the foundation of numerous scientific and industrial applications.","While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties.","However, considering space group constraint is challenging owing to its diverse and nontrivial forms.","In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process.","In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates.","Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account.","Experiments on several popular datasets verify the benefit of the involvement of the space group constraint, and show that our DiffCSP++ achieves promising performance on crystal structure prediction, ab initio crystal generation and controllable generation with customized space groups."],"url":"http://arxiv.org/abs/2402.03992v1","category":"cs.LG"}
{"created":"2024-02-06 13:44:39","title":"Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias","abstract":"Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. Our theoretical findings are supported by a range of experimental evaluations illustrating the phenomenon.","sentences":["Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance.","However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks.","In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers.","Our theoretical findings are supported by a range of experimental evaluations illustrating the phenomenon."],"url":"http://arxiv.org/abs/2402.03991v1","category":"cs.LG"}
{"created":"2024-02-06 13:31:45","title":"YOLOPoint Joint Keypoint and Object Detection","abstract":"Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.","sentences":["Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings.","Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry.","To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate.","By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks."],"url":"http://arxiv.org/abs/2402.03989v1","category":"cs.CV"}
{"created":"2024-02-06 13:20:46","title":"A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets","abstract":"Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.","sentences":["Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation.","These benefits have clear empirical support, but the theoretical understanding of them is currently very light.","We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets.","Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score.","We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors.","The results follow our theory, showing that our insights are also practically relevant."],"url":"http://arxiv.org/abs/2402.03985v1","category":"cs.LG"}
{"created":"2024-02-06 13:20:05","title":"Angular correlation and deformed Hellings-Downs curve by spin-2 ultralight dark matter","abstract":"The pulsar timings are sensitive to both the nanohertz gravitational-wave background and the oscillation of ultralight dark matter. The Hellings-Downs angular correlation curve provides a criterion to search for stochastic gravitational-wave backgrounds at nanohertz via pulsar timing arrays. We study the angular correlation of the timing residuals induced by the spin-2 ultralight dark matter, which is different from the usual Hellings-Downs correlation. At a typical frequency, we show that the spin-2 ultralight dark matter can give rise to the deformation of the Hellings-Downs correlation curve induced by the stochastic gravitational wave background.","sentences":["The pulsar timings are sensitive to both the nanohertz gravitational-wave background and the oscillation of ultralight dark matter.","The Hellings-Downs angular correlation curve provides a criterion to search for stochastic gravitational-wave backgrounds at nanohertz via pulsar timing arrays.","We study the angular correlation of the timing residuals induced by the spin-2 ultralight dark matter, which is different from the usual Hellings-Downs correlation.","At a typical frequency, we show that the spin-2 ultralight dark matter can give rise to the deformation of the Hellings-Downs correlation curve induced by the stochastic gravitational wave background."],"url":"http://arxiv.org/abs/2402.03984v1","category":"gr-qc"}
{"created":"2024-02-06 13:19:26","title":"On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions","abstract":"The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\\mathcal{O}(\\text{poly}(\\log T)/\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions.","sentences":["The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks.","Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise.","In this paper, we study vanilla Adam under these challenging conditions.","We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise.","We show that Adam can find a stationary point with a $\\mathcal{O}(\\text{poly}(\\log T)/\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors.","More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions.","We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions."],"url":"http://arxiv.org/abs/2402.03982v1","category":"math.OC"}
{"created":"2024-02-06 13:16:54","title":"Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting","abstract":"In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities. Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints. However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision. These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints. To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories. To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left. Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories. Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings.","sentences":["In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities.","Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents' stochastic behavior bounded by scene constraints.","However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision.","These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints.","To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a Transformer-based conditional denoising diffusion model to guide the prediction of future trajectories.","To ensure multimodality, we incorporate behavioral tokens to direct the trajectory's modes, such as going straight, turning right or left.","Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories.","Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings."],"url":"http://arxiv.org/abs/2402.03981v1","category":"cs.CV"}
{"created":"2024-02-06 13:07:28","title":"Smoothed analysis of deterministic discounted and mean-payoff games","abstract":"We devise a policy-iteration algorithm for deterministic two-player discounted and mean-payoff games, that runs in polynomial time with high probability, on any input where each payoff is chosen independently from a sufficiently random distribution.   This includes the case where an arbitrary set of payoffs has been perturbed by a Gaussian, showing for the first time that deterministic two-player games can be solved efficiently, in the sense of smoothed analysis.   More generally, we devise a condition number for deterministic discounted and mean-payoff games, and show that our algorithm runs in time polynomial in this condition number.   Our result confirms a previous conjecture of Boros et al., which was claimed as a theorem and later retracted. It stands in contrast with a recent counter-example by Christ and Yannakakis, showing that Howard's policy-iteration algorithm does not run in smoothed polynomial time on stochastic single-player mean-payoff games.   Our approach is inspired by the analysis of random optimal assignment instances by Frieze and Sorkin, and the analysis of bias-induced policies for mean-payoff games by Akian, Gaubert and Hochart.","sentences":["We devise a policy-iteration algorithm for deterministic two-player discounted and mean-payoff games, that runs in polynomial time with high probability, on any input where each payoff is chosen independently from a sufficiently random distribution.   ","This includes the case where an arbitrary set of payoffs has been perturbed by a Gaussian, showing for the first time that deterministic two-player games can be solved efficiently, in the sense of smoothed analysis.   ","More generally, we devise a condition number for deterministic discounted and mean-payoff games, and show that our algorithm runs in time polynomial in this condition number.   ","Our result confirms a previous conjecture of Boros et al., which was claimed as a theorem and later retracted.","It stands in contrast with a recent counter-example by Christ and Yannakakis, showing that Howard's policy-iteration algorithm does not run in smoothed polynomial time on stochastic single-player mean-payoff games.   ","Our approach is inspired by the analysis of random optimal assignment instances by Frieze and Sorkin, and the analysis of bias-induced policies for mean-payoff games by Akian, Gaubert and Hochart."],"url":"http://arxiv.org/abs/2402.03975v1","category":"cs.GT"}
{"created":"2024-02-06 13:06:40","title":"Hankel transforms of general monotone functions","abstract":"We show that the Hankel transform of a general monotone function converges uniformly if and only if the limit function is bounded. To this end, we rely on an Abel-Olivier test for real-valued functions. Analogous results for cosine series are derived as well. We also show that our statements do not hold without the general monotonicity assumption in the case of cosine integrals and series.","sentences":["We show that the Hankel transform of a general monotone function converges uniformly if and only if the limit function is bounded.","To this end, we rely on an Abel-Olivier test for real-valued functions.","Analogous results for cosine series are derived as well.","We also show that our statements do not hold without the general monotonicity assumption in the case of cosine integrals and series."],"url":"http://arxiv.org/abs/2402.03974v1","category":"math.CA"}
{"created":"2024-02-06 13:02:00","title":"Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning","abstract":"Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to reveal shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks. Results show that joint exploration is crucial for solving tasks where the optimal strategy requires a high level of coordination.","sentences":["Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards.","This challenge becomes even more pronounced when coordination among agents is necessary.","As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder.","In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one.","In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors.","We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm.","JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments.","We demonstrate the strengths of this approach both in a synthetic environment designed to reveal shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks.","Results show that joint exploration is crucial for solving tasks where the optimal strategy requires a high level of coordination."],"url":"http://arxiv.org/abs/2402.03972v1","category":"cs.MA"}
{"created":"2024-02-06 12:59:02","title":"Tabular Data: Is Attention All You Need?","abstract":"Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.","sentences":["Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data.","Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data.","In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections.","In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees.","Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets.","As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications."],"url":"http://arxiv.org/abs/2402.03970v1","category":"cs.LG"}
{"created":"2024-02-06 12:42:21","title":"Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims","abstract":"Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.","sentences":["Humans have a tendency to see 'human'-like qualities in objects around them.","We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do.","This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs).","In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs.","In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome.","Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes."],"url":"http://arxiv.org/abs/2402.03962v1","category":"cs.AI"}
{"created":"2024-02-06 12:41:39","title":"Self-Reproduction and Evolution in Cellular Automata: 25 Years after Evoloops","abstract":"The year of 2024 marks the 25th anniversary of the publication of evoloops, an evolutionary variant of Chris Langton's self-reproducing loops which proved that Darwinian evolution of self-reproducing organisms by variation and natural selection is possible within deterministic cellular automata. Over the last few decades, this line of Artificial Life research has since undergone several important developments. Although it experienced a relative dormancy of activities for a while, the recent rise of interest in open-ended evolution and the success of continuous cellular automata models have brought researchers' attention back to how to make spatio-temporal patterns self-reproduce and evolve within spatially distributed computational media. This article provides a review of the relevant literature on this topic over the past 25 years and highlights the major accomplishments made so far, the challenges being faced, and promising future research directions.","sentences":["The year of 2024 marks the 25th anniversary of the publication of evoloops, an evolutionary variant of Chris Langton's self-reproducing loops which proved that Darwinian evolution of self-reproducing organisms by variation and natural selection is possible within deterministic cellular automata.","Over the last few decades, this line of Artificial Life research has since undergone several important developments.","Although it experienced a relative dormancy of activities for a while, the recent rise of interest in open-ended evolution and the success of continuous cellular automata models have brought researchers' attention back to how to make spatio-temporal patterns self-reproduce and evolve within spatially distributed computational media.","This article provides a review of the relevant literature on this topic over the past 25 years and highlights the major accomplishments made so far, the challenges being faced, and promising future research directions."],"url":"http://arxiv.org/abs/2402.03961v1","category":"nlin.CG"}
{"created":"2024-02-06 12:39:17","title":"Lattice simulation of $SU(2)$ dark glueball with machine learning","abstract":"We study the mass and scattering cross section of $SU(2)$ glueballs as dark matter candidates using lattice simulations. We employ both naive and improved $SU(2)$ gauge actions in $3+1$ dimensions with several $\\beta$ values, and adopt both the tranditional Monte Carlo method and the flow-based model based on machine learning techniques to generate lattice configurations. The mass of the scalar glueball with $J^{PC}=0^{++}$ and the NBS wave function are calculated. Using the Runge-Kutta method, we extract the glueball interaction potential and scattering cross section. From the observational constraints, we obtain the lower bound of the mass of scalar glueball candidates as potential components of dark matter.","sentences":["We study the mass and scattering cross section of $SU(2)$ glueballs as dark matter candidates using lattice simulations.","We employ both naive and improved $SU(2)$ gauge actions in $3+1$ dimensions with several $\\beta$ values, and adopt both the tranditional Monte Carlo method and the flow-based model based on machine learning techniques to generate lattice configurations.","The mass of the scalar glueball with $J^{PC}=0^{++}$ and the NBS wave function are calculated.","Using the Runge-Kutta method, we extract the glueball interaction potential and scattering cross section.","From the observational constraints, we obtain the lower bound of the mass of scalar glueball candidates as potential components of dark matter."],"url":"http://arxiv.org/abs/2402.03959v1","category":"hep-lat"}
{"created":"2024-02-06 12:38:18","title":"Discrete epidemic models with two time scales","abstract":"The main aim of the work is to present a general class of two time scales discrete-time epidemic models. In the proposed framework the disease dynamics is considered to act on a slower time scale than a second different process that could represent movements between spatial locations, changes of individual activities or behaviours, or others. To include a sufficiently general disease model, we first build up from first principles a discrete-time Susceptible-Exposed-Infectious-Recovered-Susceptible (SEIRS) model and characterize the eradication or endemicity of the disease with the help of its basic reproduction number R0. Then, we propose a general full model that includes sequentially the two processes at different time scales, and proceed to its analysis through a reduced model. The basic reproduction number R0 of the reduced system gives a good approximation of the R0 of the full model since it serves at analyzing its asymptotic behaviour. As an illustration of the proposed general framework, it is shown that there exist conditions under which a locally endemic disease, considering isolated patches in a metapopulation, can be eradicated globally by establishing the appropriate movements between patches.","sentences":["The main aim of the work is to present a general class of two time scales discrete-time epidemic models.","In the proposed framework the disease dynamics is considered to act on a slower time scale than a second different process that could represent movements between spatial locations, changes of individual activities or behaviours, or others.","To include a sufficiently general disease model, we first build up from first principles a discrete-time Susceptible-Exposed-Infectious-Recovered-Susceptible (SEIRS) model and characterize the eradication or endemicity of the disease with the help of its basic reproduction number R0.","Then, we propose a general full model that includes sequentially the two processes at different time scales, and proceed to its analysis through a reduced model.","The basic reproduction number R0 of the reduced system gives a good approximation of the R0 of the full model since it serves at analyzing its asymptotic behaviour.","As an illustration of the proposed general framework, it is shown that there exist conditions under which a locally endemic disease, considering isolated patches in a metapopulation, can be eradicated globally by establishing the appropriate movements between patches."],"url":"http://arxiv.org/abs/2402.03958v1","category":"math.DS"}
{"created":"2024-02-06 12:30:46","title":"Terahertz ratchet in graphene 2D metamaterial formed by a patterned gate with an antidot arrayd","abstract":"We report the observation of the terahertz-induced ratchet effect in graphene-based two-dimensional (2D) metamaterials. The metamaterial consists of a graphite gate patterned with an array of triangular antidots placed under a graphene monolayer. We show that the ratchet current appears due to the noncentrosymmetry of the periodic structure unit cell. The ratchet current is generated owing to the combined action of a spatially periodic in-plane electrostatic potential and a periodically modulated radiation electric field caused by near-field diffraction. The magnitude and direction of the ratchet current are shown to be controlled by voltages applied to both back and patterned gates, which change the lateral asymmetry, carrier type and density. The phenomenological and microscopic theories of ratchet effects in graphene-based 2D metamaterials are developed. The experimental data are discussed in the light of the theory based on the solution of the Boltzmann kinetic equation and the calculated electrostatic potential profile. The theory describes well all the experimental results and shows that the observed ratchet current consists of the Seebeck thermoratchet contribution as well as the linear contribution, which is sensitive to the orientation of the radiation electric field vector with respect to the triangles.","sentences":["We report the observation of the terahertz-induced ratchet effect in graphene-based two-dimensional (2D) metamaterials.","The metamaterial consists of a graphite gate patterned with an array of triangular antidots placed under a graphene monolayer.","We show that the ratchet current appears due to the noncentrosymmetry of the periodic structure unit cell.","The ratchet current is generated owing to the combined action of a spatially periodic in-plane electrostatic potential and a periodically modulated radiation electric field caused by near-field diffraction.","The magnitude and direction of the ratchet current are shown to be controlled by voltages applied to both back and patterned gates, which change the lateral asymmetry, carrier type and density.","The phenomenological and microscopic theories of ratchet effects in graphene-based 2D metamaterials are developed.","The experimental data are discussed in the light of the theory based on the solution of the Boltzmann kinetic equation and the calculated electrostatic potential profile.","The theory describes well all the experimental results and shows that the observed ratchet current consists of the Seebeck thermoratchet contribution as well as the linear contribution, which is sensitive to the orientation of the radiation electric field vector with respect to the triangles."],"url":"http://arxiv.org/abs/2402.03956v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 12:23:14","title":"Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping","abstract":"Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.","sentences":["Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems.","To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed.","However, they show poor performances in attacking systems having different model genera from the surrogate model.","In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack.","Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input.","To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy.","Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition.","Code is made available at https://github.com/LinQinLiang/DeCoWA."],"url":"http://arxiv.org/abs/2402.03951v1","category":"cs.CV"}
{"created":"2024-02-06 12:20:10","title":"BioNet-XR: Biological Network Visualization Framework for Virtual Reality and Mixed Reality Environments","abstract":"Protein-protein interaction networks (PPIN) enable the study of cellular processes in organisms. Visualizing PPINs in extended reality (XR), including virtual reality (VR) and mixed reality (MR), is crucial for exploring subnetworks, evaluating protein positions, and collaboratively analyzing and discussing on networks with the help of recent technological advancements. Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in VR and MR environments. BioNet-XR was developed with the Unity3D game engine. Our framework provides state-of-the-art methods and visualization features including teleportation between nodes, general and first-person view to explore the network, subnetwork construction via PageRank, Steiner tree, and all-pair shortest path algorithms for a given set of initial nodes. We used usability tests to gather feedback from both specialists (bioinformaticians) and generalists (multidisciplinary groups), addressing the need for usability evaluations of visualization tools. In the MR version of BioNet-XR, users can seamlessly transition to real-world environments and interact with protein interaction networks. BioNet-XR is highly modular and adaptable for visualization of other biological networks, such as metabolic and regulatory networks, and extension with additional network methods.","sentences":["Protein-protein interaction networks (PPIN) enable the study of cellular processes in organisms.","Visualizing PPINs in extended reality (XR), including virtual reality (VR) and mixed reality (MR), is crucial for exploring subnetworks, evaluating protein positions, and collaboratively analyzing and discussing on networks with the help of recent technological advancements.","Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in VR and MR environments.","BioNet-XR was developed with the Unity3D game engine.","Our framework provides state-of-the-art methods and visualization features including teleportation between nodes, general and first-person view to explore the network, subnetwork construction via PageRank, Steiner tree, and all-pair shortest path algorithms for a given set of initial nodes.","We used usability tests to gather feedback from both specialists (bioinformaticians) and generalists (multidisciplinary groups), addressing the need for usability evaluations of visualization tools.","In the MR version of BioNet-XR, users can seamlessly transition to real-world environments and interact with protein interaction networks.","BioNet-XR is highly modular and adaptable for visualization of other biological networks, such as metabolic and regulatory networks, and extension with additional network methods."],"url":"http://arxiv.org/abs/2402.03946v1","category":"cs.MM"}
{"created":"2024-02-06 12:19:44","title":"Long-term dust dynamics in Didymos and Dimorphos system: production, stability, and transport","abstract":"Target of NASA's DART mission, the system of Didymos and Dimorphos will once again be visited by a space mission -- ESA's Hera mission, scheduled to be launch in 2024. Hera will arrive in the system approximately 4 years after the DART impact, a long period compared to Dimorphos' orbital period (about 12 hours). It is therefore imperative to understand the dynamics of material in this environment on a long timescale. Here, we explore the long-term dynamics of the binary system (65038) Didymos, in the context of the perturbed, planar, circular and restricted 3-body problem. We design an analytical description for a symmetrical top-shaped object, the shape assumed for the Didymos, while the Dimorphos is considered an ellipsoid. In the absence of external effects, we identify seven stable equatorial regions where particles persist for more than a decade. However, in the presence of the solar radiation effect, the lifetime of small particles (<mm) is in the order of days, being unlikely that Hera spacecraft will encounter clusters of millimetre and sub-millimetre particles in stable equatorial orbits. Nonetheless, large objects may reside in the region for some years, particularly in quasi-satellite orbits, the most stable orbits in the system. Additionally, interplanetary dust impacts onto Didymos populate the region, extending up to a distance of approximately 1500 meters from the primary center, with young dust. These impacts are responsible for a transfer of dust mainly from Didymos to Dimorphos. If the interplanetary dust impacts generate metric-sized boulders, they may persist in the system for years, in first sort orbits around Didymos.","sentences":["Target of NASA's DART mission, the system of Didymos and Dimorphos will once again be visited by a space mission -- ESA's Hera mission, scheduled to be launch in 2024.","Hera will arrive in the system approximately 4 years after the DART impact, a long period compared to Dimorphos' orbital period (about 12 hours).","It is therefore imperative to understand the dynamics of material in this environment on a long timescale.","Here, we explore the long-term dynamics of the binary system (65038)","Didymos, in the context of the perturbed, planar, circular and restricted 3-body problem.","We design an analytical description for a symmetrical top-shaped object, the shape assumed for the Didymos, while the Dimorphos is considered an ellipsoid.","In the absence of external effects, we identify seven stable equatorial regions where particles persist for more than a decade.","However, in the presence of the solar radiation effect, the lifetime of small particles (<mm) is in the order of days, being unlikely that Hera spacecraft will encounter clusters of millimetre and sub-millimetre particles in stable equatorial orbits.","Nonetheless, large objects may reside in the region for some years, particularly in quasi-satellite orbits, the most stable orbits in the system.","Additionally, interplanetary dust impacts onto Didymos populate the region, extending up to a distance of approximately 1500 meters from the primary center, with young dust.","These impacts are responsible for a transfer of dust mainly from Didymos to Dimorphos.","If the interplanetary dust impacts generate metric-sized boulders, they may persist in the system for years, in first sort orbits around Didymos."],"url":"http://arxiv.org/abs/2402.03943v1","category":"astro-ph.EP"}
{"created":"2024-02-06 12:18:54","title":"Discovery of the Hidden World with Large Language Models","abstract":"Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs. We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis.","sentences":["Science originates with discovering new causal knowledge from a combination of known facts and observations.","Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations.","However, the causal variables are usually unavailable in a wide range of real-world applications.","The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data.","Therefore, we introduce COAT:","Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data.","Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data.","The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by LLMs.","We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis."],"url":"http://arxiv.org/abs/2402.03941v1","category":"cs.LG"}
{"created":"2024-02-06 12:13:28","title":"On the Nonequilibrium Dynamics of Gravitational Algebras","abstract":"We explore nonequilibrium features of certain operator algebras which appear in quantum gravity. The algebra of observables in a black hole background is a Type $\\mathrm{II}_\\infty$ von Neumann algebra. We discuss how this algebra can be coupled to the algebra of observable of an infinite reservoir within the canonical ensemble, aiming to induce nonequilibrium dynamics. The resulting dynamics can lead the system towards a nonequilibrium steady state which can be characterized through modular theory. Within this framework we address the definition of entropy production and its relationship to relative entropy, alongside exploring other applications.","sentences":["We explore nonequilibrium features of certain operator algebras which appear in quantum gravity.","The algebra of observables in a black hole background is a Type $\\mathrm{II}_\\infty$ von Neumann algebra.","We discuss how this algebra can be coupled to the algebra of observable of an infinite reservoir within the canonical ensemble, aiming to induce nonequilibrium dynamics.","The resulting dynamics can lead the system towards a nonequilibrium steady state which can be characterized through modular theory.","Within this framework we address the definition of entropy production and its relationship to relative entropy, alongside exploring other applications."],"url":"http://arxiv.org/abs/2402.03939v1","category":"hep-th"}
{"created":"2024-02-06 12:11:22","title":"Apparent Distance and a Notion of BCH Multivariate Codes","abstract":"This paper is devoted to studying two main problems: 1) computing the apparent distance of an Abelian code and 2) giving a notion of Bose, Ray-Chaudhuri, Hocquenghem (BCH) multivariate code. To do this, we first strengthen the notion of an apparent distance by introducing the notion of a strong apparent distance; then, we present an algorithm to compute the strong apparent distance of an Abelian code, based on some manipulations of hypermatrices associated with its generating idempotent. Our method uses less computations than those given by Camion and Sabin; furthermore, in the bivariate case, the order of computation complexity is reduced from exponential to linear. Then, we use our techniques to develop a notion of a BCH code in the multivariate case, and we extend most of the classical results on cyclic BCH codes. Finally, we apply our method to the design of Abelian codes with maximum dimension with respect to a fixed apparent distance and a fixed length.","sentences":["This paper is devoted to studying two main problems: 1) computing the apparent distance of an Abelian code and 2) giving a notion of Bose, Ray-Chaudhuri, Hocquenghem (BCH) multivariate code.","To do this, we first strengthen the notion of an apparent distance by introducing the notion of a strong apparent distance; then, we present an algorithm to compute the strong apparent distance of an Abelian code, based on some manipulations of hypermatrices associated with its generating idempotent.","Our method uses less computations than those given by Camion and Sabin; furthermore, in the bivariate case, the order of computation complexity is reduced from exponential to linear.","Then, we use our techniques to develop a notion of a BCH code in the multivariate case, and we extend most of the classical results on cyclic BCH codes.","Finally, we apply our method to the design of Abelian codes with maximum dimension with respect to a fixed apparent distance and a fixed length."],"url":"http://arxiv.org/abs/2402.03938v1","category":"cs.IT"}
{"created":"2024-02-06 12:09:21","title":"Commutativity and orthogonality of similarity orbits in Banach algebras","abstract":"For a semisimple unital Banach algebra $A$ over $\\mathbb{C}$, and elements $a,b\\in A,$ we show that the similarity orbits, $\\mathrm{orb}(a)$ and $\\mathrm{orb}(b)$, over the principal component of the invertible group of $A$ commute precisely when there is at least one nonzero complex number not belonging to the spectrum of any product $a^\\prime b^\\prime$ -- where $(a^\\prime,b^\\prime)\\in\\mathrm{orb}(a)\\times\\mathrm{orb}(b)$. In this case, the polynomially convex hull of the spectra of the $a^\\prime b^\\prime$ is constant. When $\\mathrm{orb}(a)=\\mathrm{orb}(b)$, then $a$ is central under the aforementioned assumption -- and the result then generalizes part of an old theorem due to J. Zem\\'anek. We show further that the two classical characterizations of commutative Banach algebras via the spectral radius can be algebraically localized in the sense of `local' implies `global'. Thereafter, in Section 3, we give a (somewhat weaker) localization of the above situation involving spectral perturbation on small neighborhoods in a similarity orbit. Finally, we apply the above results to algebraic elements and idempotents in particular, so that orthogonality of similarity orbits of two idempotents is equivalent to a pair of spectral radius properties. To conclude with, a couple of localization theorems specific to idempotents and algebraic elements are presented. Similar statements to all of the above hold if $ a^\\prime b^\\prime $ is replaced by $ a^\\prime + b^\\prime $, $ a^\\prime - b^\\prime $, or $ a^\\prime + b^\\prime-a^\\prime b^\\prime $.","sentences":["For a semisimple unital Banach algebra $A$ over $\\mathbb{C}$, and elements $a,b\\in A,$ we show that the similarity orbits, $\\mathrm{orb}(a)$ and $\\mathrm{orb}(b)$, over the principal component of the invertible group of $A$ commute precisely when there is at least one nonzero complex number not belonging to the spectrum of any product $a^\\prime b^\\prime$ -- where $(a^\\prime,b^\\prime)\\in\\mathrm{orb}(a)\\times\\mathrm{orb}(b)$. In this case, the polynomially convex hull of the spectra of the $a^\\prime b^\\prime$ is constant.","When $\\mathrm{orb}(a)=\\mathrm{orb}(b)$, then $a$ is central under the aforementioned assumption -- and the result then generalizes part of an old theorem due to J. Zem\\'anek.","We show further that the two classical characterizations of commutative Banach algebras via the spectral radius can be algebraically localized in the sense of `local' implies `global'.","Thereafter, in Section 3, we give a (somewhat weaker) localization of the above situation involving spectral perturbation on small neighborhoods in a similarity orbit.","Finally, we apply the above results to algebraic elements and idempotents in particular, so that orthogonality of similarity orbits of two idempotents is equivalent to a pair of spectral radius properties.","To conclude with, a couple of localization theorems specific to idempotents and algebraic elements are presented.","Similar statements to all of the above hold if $ a^\\prime b^\\prime $ is replaced by $ a^\\prime + b^\\prime $, $ a^\\prime - b^\\prime $, or $ a^\\prime + b^\\prime-a^\\prime b^\\prime $."],"url":"http://arxiv.org/abs/2402.03936v1","category":"math.FA"}
{"created":"2024-02-06 12:07:29","title":"Confronting primordial black holes with LIGO-Virgo-KAGRA and the Einstein Telescope","abstract":"The detection of gravitational waves (GWs) from binary black hole (BBH) coalescences by the LIGO-Virgo-KAGRA (LVK) Collaboration has raised fundamental questions about the genesis of these events. In this chapter, we explore the possibility that PBHs, proposed candidates for dark matter, may serve as the progenitors of the BBHs observed by LVK. Employing a Bayesian analysis, we constrain the PBH model using the LVK third GW Transient Catalog (GWTC-3), revealing that stellar-mass PBHs cannot dominate cold dark matter. Considering a mixed population of astrophysical black holes (ABHs) and PBHs, we determine that approximately $1/4$ of the detectable events in the GWTC-3 can be attributed to PBH binaries. We also forecast detectable event rate distributions for PBH and ABH binaries by the third-generation ground-based GW detectors, such as the Einstein Telescope, offering a potential avenue to distinguish PBHs from ABHs based on their distinct redshift evolutions.","sentences":["The detection of gravitational waves (GWs) from binary black hole (BBH) coalescences by the LIGO-Virgo-KAGRA (LVK) Collaboration has raised fundamental questions about the genesis of these events.","In this chapter, we explore the possibility that PBHs, proposed candidates for dark matter, may serve as the progenitors of the BBHs observed by LVK.","Employing a Bayesian analysis, we constrain the PBH model using the LVK third GW Transient Catalog (GWTC-3), revealing that stellar-mass PBHs cannot dominate cold dark matter.","Considering a mixed population of astrophysical black holes (ABHs) and PBHs, we determine that approximately $1/4$ of the detectable events in the GWTC-3 can be attributed to PBH binaries.","We also forecast detectable event rate distributions for PBH and ABH binaries by the third-generation ground-based GW detectors, such as the Einstein Telescope, offering a potential avenue to distinguish PBHs from ABHs based on their distinct redshift evolutions."],"url":"http://arxiv.org/abs/2402.03934v1","category":"astro-ph.CO"}
{"created":"2024-02-06 11:54:59","title":"Viscous regularization of the MHD equations","abstract":"Nonlinear conservation laws such as the system of ideal magnetohydrodynamics (MHD) equations may develop singularities over time. In these situations, viscous regularization is a common approach to regain regularity of the solution. In this paper, we present a new viscous flux to regularize the MHD equations which holds many attractive properties. In particular, we prove that the proposed viscous flux preserves positivity of density and internal energy, satisfies the minimum entropy principle, is consistent with all generalized entropies, and is Galilean and rotationally invariant. We also provide a variation of the viscous flux that conserves angular momentum. To make the analysis more useful for numerical schemes, the divergence of the magnetic field is not assumed to be zero. Using continuous finite elements, we show several numerical experiments including contact waves and magnetic reconnection.","sentences":["Nonlinear conservation laws such as the system of ideal magnetohydrodynamics (MHD) equations may develop singularities over time.","In these situations, viscous regularization is a common approach to regain regularity of the solution.","In this paper, we present a new viscous flux to regularize the MHD equations which holds many attractive properties.","In particular, we prove that the proposed viscous flux preserves positivity of density and internal energy, satisfies the minimum entropy principle, is consistent with all generalized entropies, and is Galilean and rotationally invariant.","We also provide a variation of the viscous flux that conserves angular momentum.","To make the analysis more useful for numerical schemes, the divergence of the magnetic field is not assumed to be zero.","Using continuous finite elements, we show several numerical experiments including contact waves and magnetic reconnection."],"url":"http://arxiv.org/abs/2402.03929v1","category":"math.NA"}
{"created":"2024-02-06 11:54:48","title":"Approximating the Core via Iterative Coalition Sampling","abstract":"The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition. However, it has long been known that the core (and approximations, such as the least-core) are hard to compute. This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as explainable AI (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models. We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs. As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks. We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications.","sentences":["The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition.","However, it has long been known that the core (and approximations, such as the least-core) are hard to compute.","This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as explainable AI (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models.","We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs.","As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks.","We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications."],"url":"http://arxiv.org/abs/2402.03928v1","category":"cs.GT"}
{"created":"2024-02-06 11:54:23","title":"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs","abstract":"Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.","sentences":["Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source.","The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers.","Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error.","Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users.","In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination.","By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release.","We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks.","At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues.","We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts."],"url":"http://arxiv.org/abs/2402.03927v1","category":"cs.CL"}
{"created":"2024-02-06 11:54:21","title":"A model of scattered thermal radiation for Venus from 3 to 5 $\u03bc$ m","abstract":"Thermal radiation becomes a prominent feature in the continuum spectrum of Venus longwards of $\\sim$3 $\\mu$m. The emission is traceable to the upper cloud and haze layers in the planet's mesosphere. Venus' thermal radiation spectrum is punctuated by CO$_2$ bands of various strengths probing into different atmospheric depths. It is thus possible to invert measured spectra of thermal radiation to infer atmospheric temperature profiles and offer some insight into the cloud and haze structure. In practice, the retrieval becomes complicated by the fact that the outgoing radiation is multiply scattered by the ubiquitous aerosol particles before leaving the atmosphere. We numerically investigate the radiative transfer problem of thermal radiation from the Venus night side between 3 and 5 $\\mu$m with a purpose-built model of Venus' mesosphere. Special emphasis is laid on the significance of scattering. The simulations explore the space of model parameters, which includes the atmospheric temperature, cloud opacity, and the aerosols' size and chemical composition. We confirm that aerosol scattering must be taken into account in a prospective temperature retrieval, which means an additional complication to the already ill-posed retrieval problem. We briefly touch upon the degeneracy in the spectrum's shape associated with parameterization of the Venus clouds. Reasonable perturbations in the chemical composition and size of aerosols do not significantly impact the model simulations. Although the experiments are specific to the technical characteristics of the Visual and Infrared Thermal Imaging Spectrometer on the Venus Express spacecraft, the conclusions are generally valid.","sentences":["Thermal radiation becomes a prominent feature in the continuum spectrum of Venus longwards of $\\sim$3 $\\mu$m.","The emission is traceable to the upper cloud and haze layers in the planet's mesosphere.","Venus' thermal radiation spectrum is punctuated by CO$_2$ bands of various strengths probing into different atmospheric depths.","It is thus possible to invert measured spectra of thermal radiation to infer atmospheric temperature profiles and offer some insight into the cloud and haze structure.","In practice, the retrieval becomes complicated by the fact that the outgoing radiation is multiply scattered by the ubiquitous aerosol particles before leaving the atmosphere.","We numerically investigate the radiative transfer problem of thermal radiation from the Venus night side between 3 and 5 $\\mu$m with a purpose-built model of Venus' mesosphere.","Special emphasis is laid on the significance of scattering.","The simulations explore the space of model parameters, which includes the atmospheric temperature, cloud opacity, and the aerosols' size and chemical composition.","We confirm that aerosol scattering must be taken into account in a prospective temperature retrieval, which means an additional complication to the already ill-posed retrieval problem.","We briefly touch upon the degeneracy in the spectrum's shape associated with parameterization of the Venus clouds.","Reasonable perturbations in the chemical composition and size of aerosols do not significantly impact the model simulations.","Although the experiments are specific to the technical characteristics of the Visual and Infrared Thermal Imaging Spectrometer on the Venus Express spacecraft, the conclusions are generally valid."],"url":"http://arxiv.org/abs/2402.03926v1","category":"astro-ph.EP"}
{"created":"2024-02-06 11:46:47","title":"Return-Aligned Decision Transformer","abstract":"Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, states, and actions, to enhance the relationships between returns and states, as well as returns and actions. Extensive experiments show that RADT reduces the discrepancies between the actual return and the target return of DT-based methods.","sentences":["Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return.","However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance.","Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return.","Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT.","In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return.","Our model decouples returns from the conventional input sequence, which typically consists of returns, states, and actions, to enhance the relationships between returns and states, as well as returns and actions.","Extensive experiments show that RADT reduces the discrepancies between the actual return and the target return of DT-based methods."],"url":"http://arxiv.org/abs/2402.03923v1","category":"cs.LG"}
{"created":"2024-02-06 11:44:06","title":"Large Language Models to Enhance Bayesian Optimization","abstract":"Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.","sentences":["Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions.","Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation.","While there has been substantial progress in BO methods, striking this balance still remains a delicate process.","In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO.","At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations.","More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO.","Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse.","Our approach is performed in context and does not require LLM finetuning.","Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method.","We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks."],"url":"http://arxiv.org/abs/2402.03921v1","category":"cs.LG"}
{"created":"2024-02-06 11:40:53","title":"The QISG suite: high-performance codes for studying Quantum Ising Spin Glasses","abstract":"We release a set of GPU programs for the study of the Quantum ($S=1/2$) Spin Glass on a square lattice, with binary couplings. The library contains two main codes: MCQSG (that carries out Monte Carlo simulations using both the Metropolis and the Parallel Tempering algorithms, for the problem formulated in the Trotter-Suzuki approximation), and EDQSG (that obtains the extremal eigenvalues of the Transfer Matrix using the Lanczos algorithm). EDQSG has allowed us to diagonalize transfer matrices with size up to $2^{36}\\times2^{36}$. From its side, MCQSG running on four NVIDIA A100 cards delivers a sub-picosecond time per spin-update, a performance that is competitive with dedicated hardware. We include as well in our library GPU programs for the analysis of the spin configurations generated by MCQSG. Finally, we provide two auxiliary codes: the first generates the lookup tables employed by the random number generator of MCQSG; the second one simplifies the execution of multiple runs using different input data.","sentences":["We release a set of GPU programs for the study of the Quantum ($S=1/2$) Spin Glass on a square lattice, with binary couplings.","The library contains two main codes: MCQSG (that carries out Monte Carlo simulations using both the Metropolis and the Parallel Tempering algorithms, for the problem formulated in the Trotter-Suzuki approximation), and EDQSG (that obtains the extremal eigenvalues of the Transfer Matrix using the Lanczos algorithm).","EDQSG has allowed us to diagonalize transfer matrices with size up to $2^{36}\\times2^{36}$. From its side, MCQSG running on four NVIDIA A100 cards delivers a sub-picosecond time per spin-update, a performance that is competitive with dedicated hardware.","We include as well in our library GPU programs for the analysis of the spin configurations generated by MCQSG.","Finally, we provide two auxiliary codes: the first generates the lookup tables employed by the random number generator of MCQSG; the second one simplifies the execution of multiple runs using different input data."],"url":"http://arxiv.org/abs/2402.03920v1","category":"physics.comp-ph"}
{"created":"2024-02-06 11:38:23","title":"Dynastic Potential Crossover Operator","abstract":"An optimal recombination operator for two parent solutions provides the best solution among those that take the value for each variable from one of the parents (gene transmission property). If the solutions are bit strings, the offspring of an optimal recombination operator is optimal in the smallest hyperplane containing the two parent solutions. Exploring this hyperplane is computationally costly, in general, requiring exponential time in the worst case. However, when the variable interaction graph of the objective function is sparse, exploration can be done in polynomial time.   In this paper, we present a recombination operator, called Dynastic Potential Crossover (DPX), that runs in polynomial time and behaves like an optimal recombination operator for low-epistasis combinatorial problems. We compare this operator, both theoretically and experimentally, with traditional crossover operators, like uniform crossover and network crossover, and with two recently defined efficient recombination operators: partition crossover and articulation points partition crossover. The empirical comparison uses NKQ Landscapes and MAX-SAT instances. DPX outperforms the other crossover operators in terms of quality of the offspring and provides better results included in a trajectory and a population-based metaheuristic, but it requires more time and memory to compute the offspring.","sentences":["An optimal recombination operator for two parent solutions provides the best solution among those that take the value for each variable from one of the parents (gene transmission property).","If the solutions are bit strings, the offspring of an optimal recombination operator is optimal in the smallest hyperplane containing the two parent solutions.","Exploring this hyperplane is computationally costly, in general, requiring exponential time in the worst case.","However, when the variable interaction graph of the objective function is sparse, exploration can be done in polynomial time.   ","In this paper, we present a recombination operator, called Dynastic Potential Crossover (DPX), that runs in polynomial time and behaves like an optimal recombination operator for low-epistasis combinatorial problems.","We compare this operator, both theoretically and experimentally, with traditional crossover operators, like uniform crossover and network crossover, and with two recently defined efficient recombination operators: partition crossover and articulation points partition crossover.","The empirical comparison uses NKQ Landscapes and MAX-SAT instances.","DPX outperforms the other crossover operators in terms of quality of the offspring and provides better results included in a trajectory and a population-based metaheuristic, but it requires more time and memory to compute the offspring."],"url":"http://arxiv.org/abs/2402.03918v1","category":"cs.NE"}
{"created":"2024-02-06 11:28:51","title":"Twin Stars in General Relativity and Extended Theories of Gravity","abstract":"We explore gravity-independent equations of state for neutron stars, particularly focusing on twin stars. Examining four categories, we emphasize their behavior in both General Relativity and Palatini gravity. Additionally, we discuss a subcategory of type I, which, in the context of General Relativity, does not exhibit twin star phenomena, yet demonstrates this phenomenon in modified gravity. Furthermore, we briefly address challenges associated with the negative trace of the energy-momentum tensor, prevalent in both theories.","sentences":["We explore gravity-independent equations of state for neutron stars, particularly focusing on twin stars.","Examining four categories, we emphasize their behavior in both General Relativity and Palatini gravity.","Additionally, we discuss a subcategory of type I, which, in the context of General Relativity, does not exhibit twin star phenomena, yet demonstrates this phenomenon in modified gravity.","Furthermore, we briefly address challenges associated with the negative trace of the energy-momentum tensor, prevalent in both theories."],"url":"http://arxiv.org/abs/2402.03914v1","category":"gr-qc"}
{"created":"2024-02-06 11:23:03","title":"Stability of Rotating, Charged Fluids: Generalization of the Hoiland Conditions in Newtonian Non-conductive Case","abstract":"We study the conditions for stability of electrically charged, non-conductive perfect fluid tori with respect to linear perturbations. To this end we employ Lagrangian perturbation formalism and we assume a system where the fluid orbits a central body. Gravitational field of the latter is described in the Newtonian framework. We first formulate the criteria valid for a general, non-axisymmetric situation, and then we concentrate on the axisymmetric model in more detail. In the latter case we generalize the H{\\o}iland criterion of stability to non-vanishing electric charge and classify special examples. Toroidal structures with constant angular momentum distribution are found to be linearly stable. Subsequently, like in the uncharged case, rotating charged fluids are found to be unstable with respect to non-axisymmetric perturbations.","sentences":["We study the conditions for stability of electrically charged, non-conductive perfect fluid tori with respect to linear perturbations.","To this end we employ Lagrangian perturbation formalism and we assume a system where the fluid orbits a central body.","Gravitational field of the latter is described in the Newtonian framework.","We first formulate the criteria valid for a general, non-axisymmetric situation, and then we concentrate on the axisymmetric model in more detail.","In the latter case we generalize the H{\\o}iland criterion of stability to non-vanishing electric charge and classify special examples.","Toroidal structures with constant angular momentum distribution are found to be linearly stable.","Subsequently, like in the uncharged case, rotating charged fluids are found to be unstable with respect to non-axisymmetric perturbations."],"url":"http://arxiv.org/abs/2402.03911v1","category":"gr-qc"}
{"created":"2024-02-06 11:21:58","title":"EscherNet: A Generative Model for Scalable View Synthesis","abstract":"We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \\url{https://kxhit.github.io/EscherNet}.","sentences":["We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis.","EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views.","EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views.","As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework.","Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem.","This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision.","Project page: \\url{https://kxhit.github.io/EscherNet}."],"url":"http://arxiv.org/abs/2402.03908v1","category":"cs.CV"}
{"created":"2024-02-06 11:19:40","title":"Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy","abstract":"Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions. While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated. In summary, despite some challenges, embedding LLMs into XR is a promising and novel research area with several opportunities.","sentences":["Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive.","While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques.","In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes.","We argue that such inclusion will facilitate diversity for XR use.","In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life.","Lastly, we speculate that combining the information provided to LLM-powered environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions.","While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated.","In summary, despite some challenges, embedding LLMs into XR is a promising and novel research area with several opportunities."],"url":"http://arxiv.org/abs/2402.03907v1","category":"cs.HC"}
{"created":"2024-02-06 11:18:13","title":"SU-8 meta phenylenediamine conjugated thin film for temperature sensing","abstract":"Polymers, demonstrating distinctive optical properties alongside facile and mastered fabrication methods, have become increasingly important platforms for realizing a variety of nanophotonic devices. Enhancing these materials with additional functions might expand their range of multidisciplinary applications. Here, we demonstrate the temperature sensing potential of SU8-Phenylenediamine (SU8-mPD), which was produced by epoxy amination of the SU-8 polymer. The SU8-mPD properties were examined through a series of molecular structural techniques and optical methods. Thin layers have demonstrated optical emission and absorption in the visible range around 420 and 520 nm respectively alongside a strong thermal responsivity, characterized by the 18 ppm\\cdotK-1 expansion coefficient. A photonic chip, comprising a thin 5-10 {\\mu}m SU8-mPD layer, encased between parallel silver and/or gold thin film mirrors, has been fabricated. This assembly, when pumped by an external light source, generates a pronounced fluorescent signal which is superimposed with the Fabry-P\\'erot (FP) resonant response. The chip undergoes mechanical deformation in response to temperature changes, thereby shifting the FP resonance and encoding temperature information into the fluorescence output spectrum. The time response of the device was estimated to be below 500 msec opening a new avenue for optical sensing using SU8-based polymers. Thermoresponsive resonant structures, encompassing strong tunable fluorescent properties, can further enrich the functionalities of nanophotonic polymer-based platforms.","sentences":["Polymers, demonstrating distinctive optical properties alongside facile and mastered fabrication methods, have become increasingly important platforms for realizing a variety of nanophotonic devices.","Enhancing these materials with additional functions might expand their range of multidisciplinary applications.","Here, we demonstrate the temperature sensing potential of SU8-Phenylenediamine (SU8-mPD), which was produced by epoxy amination of the SU-8 polymer.","The SU8-mPD properties were examined through a series of molecular structural techniques and optical methods.","Thin layers have demonstrated optical emission and absorption in the visible range around 420 and 520 nm respectively alongside a strong thermal responsivity, characterized by the 18 ppm\\cdotK-1 expansion coefficient.","A photonic chip, comprising a thin 5-10 {\\mu}m SU8-mPD layer, encased between parallel silver and/or gold thin film mirrors, has been fabricated.","This assembly, when pumped by an external light source, generates a pronounced fluorescent signal which is superimposed with the Fabry-P\\'erot (FP) resonant response.","The chip undergoes mechanical deformation in response to temperature changes, thereby shifting the FP resonance and encoding temperature information into the fluorescence output spectrum.","The time response of the device was estimated to be below 500 msec opening a new avenue for optical sensing using SU8-based polymers.","Thermoresponsive resonant structures, encompassing strong tunable fluorescent properties, can further enrich the functionalities of nanophotonic polymer-based platforms."],"url":"http://arxiv.org/abs/2402.03906v1","category":"physics.app-ph"}
{"created":"2024-02-06 11:16:18","title":"Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching","abstract":"We propose a novel constraint called Multiple Spectral filter Operators Preservation (MSFOR) to compute functional maps and based on it, develop an efficient deep functional map architecture called Deep MSFOP for shape matching. The core idea is that, instead of using the general descriptor preservation constraint, we require our maps to preserve multiple spectral filter operators. This allows us to incorporate more informative geometrical information, contained in different frequency bands of functions, into the functional map computing. This can be confirmed by that some previous techniques like wavelet preservation and LBO commutativity are actually our special cases. Moreover, we also develop a very efficient way to compute the maps with MSFOP constraint, which can be conveniently embedded into the deep learning, especially having learnable filter operators. Utilizing the above results, we finally design our Deep MSFOP pipeline, equipped with a suitable unsupervised loss jointly penalizing the functional map and the underlying pointwise map. Our deep functional map has notable advantages, including that the functional map is more geometrically informative and guaranteed to be proper, and the computing is numerically stable. Extensive experimental results on different datasets demonstrate that our approach outperforms the existing state-of-the-art methods, especially in challenging settings like non-isometric and inconsistent topology datasets.","sentences":["We propose a novel constraint called Multiple Spectral filter Operators Preservation (MSFOR) to compute functional maps and based on it, develop an efficient deep functional map architecture called Deep MSFOP for shape matching.","The core idea is that, instead of using the general descriptor preservation constraint, we require our maps to preserve multiple spectral filter operators.","This allows us to incorporate more informative geometrical information, contained in different frequency bands of functions, into the functional map computing.","This can be confirmed by that some previous techniques like wavelet preservation and LBO commutativity are actually our special cases.","Moreover, we also develop a very efficient way to compute the maps with MSFOP constraint, which can be conveniently embedded into the deep learning, especially having learnable filter operators.","Utilizing the above results, we finally design our Deep MSFOP pipeline, equipped with a suitable unsupervised loss jointly penalizing the functional map and the underlying pointwise map.","Our deep functional map has notable advantages, including that the functional map is more geometrically informative and guaranteed to be proper, and the computing is numerically stable.","Extensive experimental results on different datasets demonstrate that our approach outperforms the existing state-of-the-art methods, especially in challenging settings like non-isometric and inconsistent topology datasets."],"url":"http://arxiv.org/abs/2402.03904v1","category":"cs.CV"}
{"created":"2024-02-06 11:13:57","title":"Compound Returns Reduce Variance in Reinforcement Learning","abstract":"Multistep returns, such as $n$-step returns and $\\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that two-bootstrap returns can improve the sample efficiency of $n$-step deep RL agents, with little additional computational cost.","sentences":["Multistep returns, such as $n$-step returns and $\\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods.","The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning.","In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance.","We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance.","We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation.","Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay.","We conduct experiments showing that two-bootstrap returns can improve the sample efficiency of $n$-step deep RL agents, with little additional computational cost."],"url":"http://arxiv.org/abs/2402.03903v1","category":"cs.LG"}
{"created":"2024-02-06 11:13:26","title":"Batch Universal Prediction","abstract":"Large language models (LLMs) have recently gained much popularity due to their surprising ability at generating human-like English sentences. LLMs are essentially predictors, estimating the probability of a sequence of words given the past. Therefore, it is natural to evaluate their performance from a universal prediction perspective. In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources.","sentences":["Large language models (LLMs) have recently gained much popularity due to their surprising ability at generating human-like English sentences.","LLMs are essentially predictors, estimating the probability of a sequence of words given the past.","Therefore, it is natural to evaluate their performance from a universal prediction perspective.","In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources."],"url":"http://arxiv.org/abs/2402.03901v1","category":"cs.IT"}
{"created":"2024-02-06 11:10:35","title":"DistiLLM: Towards Streamlined Distillation for Large Language Models","abstract":"Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods.","sentences":["Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities.","However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function.","Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs.","To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models.","DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs.","Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods."],"url":"http://arxiv.org/abs/2402.03898v1","category":"cs.CL"}
{"created":"2024-02-06 11:07:05","title":"Convincing Rationales for Visual Question Answering Reasoning","abstract":"Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image. It requires deep understanding of both the textual question and visual image. Prior works directly evaluate the answering models by simply calculating the accuracy of the predicted answers. However, the inner reasoning behind the prediction is disregarded in such a \"black box\" system, and we do not even know if one can trust the predictions. In some cases, the models still get the correct answers even when they focus on irrelevant visual regions or textual tokens, which makes the models unreliable and illogical. To generate both visual and textual rationales next to the predicted answer to the given image/question pair, we propose Convincing Rationales for VQA, CRVQA. Considering the extra annotations brought by the new outputs, {CRVQA} is trained and evaluated by samples converted from some existing VQA datasets and their visual labels. The extensive experiments demonstrate that the visual and textual rationales support the prediction of the answers, and further improve the accuracy. Furthermore, {CRVQA} achieves competitive performance on generic VQA datatsets in the zero-shot evaluation setting. The dataset and source code will be released under https://github.com/lik1996/CRVQA2024.","sentences":["Visual Question Answering (VQA) is a challenging task of predicting the answer to a question about the content of an image.","It requires deep understanding of both the textual question and visual image.","Prior works directly evaluate the answering models by simply calculating the accuracy of the predicted answers.","However, the inner reasoning behind the prediction is disregarded in such a \"black box\" system, and we do not even know if one can trust the predictions.","In some cases, the models still get the correct answers even when they focus on irrelevant visual regions or textual tokens, which makes the models unreliable and illogical.","To generate both visual and textual rationales next to the predicted answer to the given image/question pair, we propose Convincing Rationales for VQA, CRVQA.","Considering the extra annotations brought by the new outputs, {CRVQA} is trained and evaluated by samples converted from some existing VQA datasets and their visual labels.","The extensive experiments demonstrate that the visual and textual rationales support the prediction of the answers, and further improve the accuracy.","Furthermore, {CRVQA} achieves competitive performance on generic VQA datatsets in the zero-shot evaluation setting.","The dataset and source code will be released under https://github.com/lik1996/CRVQA2024."],"url":"http://arxiv.org/abs/2402.03896v1","category":"cs.CV"}
{"created":"2024-02-06 11:04:43","title":"Extended Doubled Structures of Algebroids for Gauged Double Field Theory","abstract":"We study an analogue of the Drinfel'd double for algebroids associated with the $O(D,D+n)$ gauged double field theory (DFT). We show that algebroids defined by the twisted C-bracket in the gauged DFT are built out of a direct sum of three (twisted) Lie algebroids. They exhibit a \"tripled\", which we call the extended double, rather than the \"doubled\" structure appeared in (ungauged) DFT. We find that the compatibilities of the extended doubled structure result not only in the strong constraint but also the additional condition in the gauged DFT. We establish a geometrical implementation of these structures in a $(2D+n)$-dimensional product manifold and examine the relations to the generalized geometry for heterotic string theories and non-Abelian gauge symmetries in DFT.","sentences":["We study an analogue of the Drinfel'd double for algebroids associated with the $O(D,D+n)$ gauged double field theory (DFT).","We show that algebroids defined by the twisted C-bracket in the gauged DFT are built out of a direct sum of three (twisted) Lie algebroids.","They exhibit a \"tripled\", which we call the extended double, rather than the \"doubled\" structure appeared in (ungauged) DFT.","We find that the compatibilities of the extended doubled structure result not only in the strong constraint but also the additional condition in the gauged DFT.","We establish a geometrical implementation of these structures in a $(2D+n)$-dimensional product manifold and examine the relations to the generalized geometry for heterotic string theories and non-Abelian gauge symmetries in DFT."],"url":"http://arxiv.org/abs/2402.03895v1","category":"hep-th"}
{"created":"2024-02-06 11:02:01","title":"Interpersonal trust: Asymptotic analysis of a stochastic coordination game with multi-agent learning","abstract":"We study the interpersonal trust of a population of agents, asking whether chance may decide if a population ends up in a high trust or low trust state. We model this by a discrete time, random matching stochastic coordination game. Agents are endowed with an exponential smoothing learning rule about the behaviour of their neighbours. We find that, with probability one in the long run the whole population either always cooperates or always defects. By simulation we study the impact of the distributions of the payoffs in the game and of the exponential smoothing learning (memory of the agents). We find, that as the agent memory increases or as the size of the population increases, the actual dynamics start to resemble the expectation of the process. We conclude that it is indeed possible that different populations may converge upon high or low trust between its citizens simply by chance, though the game parameters (context of the society) may be quite telling.","sentences":["We study the interpersonal trust of a population of agents, asking whether chance may decide if a population ends up in a high trust or low trust state.","We model this by a discrete time, random matching stochastic coordination game.","Agents are endowed with an exponential smoothing learning rule about the behaviour of their neighbours.","We find that, with probability one in the long run the whole population either always cooperates or always defects.","By simulation we study the impact of the distributions of the payoffs in the game and of the exponential smoothing learning (memory of the agents).","We find, that as the agent memory increases or as the size of the population increases, the actual dynamics start to resemble the expectation of the process.","We conclude that it is indeed possible that different populations may converge upon high or low trust between its citizens simply by chance, though the game parameters (context of the society) may be quite telling."],"url":"http://arxiv.org/abs/2402.03894v1","category":"physics.soc-ph"}
{"created":"2024-02-06 10:58:13","title":"Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency","abstract":"Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort. We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians.","sentences":["Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance.","However, the relationship between the time horizon associated with these predictions and AV performance remains unclear.","Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods.","Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency.","Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds.","Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs.","Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort.","We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians."],"url":"http://arxiv.org/abs/2402.03893v1","category":"cs.RO"}
{"created":"2024-02-06 10:53:31","title":"Stochastic matrix metapopulation models with fast migration: re-scaling survival to the fast scale","abstract":"In this work we address the analysis of discrete-time models of structured metapopulations subject to environmental stochasticity. Previous works on these models made use of the fact that migrations between the patches can be considered fast with respect to demography (maturation, survival, reproduction) in the population. It was assumed that, within each time step of the model, there are many fast migration steps followed by one slow demographic event. This assumption allowed one to apply approximate reduction techniques that eased the model analysis. It is however a questionable issue in some cases since, in particular, individuals can die at any moment of the time step. We propose new non-equivalent models in which we re-scale survival to consider its effect on the fast scale. We propose a more general formulation of the approximate reduction techniques so that they also apply to the proposed new models. We prove that the main asymptotic elements in this kind of stochastic models, the Stochastic Growth Rate (SGR) and the Scaled Logarithmic Variance (SLV), can be related between the original and the reduced systems, so that the analysis of the latter allows us to ascertain the population fate in the first. Then we go on to considering some cases where we illustrate the reduction technique and show the differences between both modelling options. In some cases using one option represents exponential growth, whereas the other yields extinction.","sentences":["In this work we address the analysis of discrete-time models of structured metapopulations subject to environmental stochasticity.","Previous works on these models made use of the fact that migrations between the patches can be considered fast with respect to demography (maturation, survival, reproduction) in the population.","It was assumed that, within each time step of the model, there are many fast migration steps followed by one slow demographic event.","This assumption allowed one to apply approximate reduction techniques that eased the model analysis.","It is however a questionable issue in some cases since, in particular, individuals can die at any moment of the time step.","We propose new non-equivalent models in which we re-scale survival to consider its effect on the fast scale.","We propose a more general formulation of the approximate reduction techniques so that they also apply to the proposed new models.","We prove that the main asymptotic elements in this kind of stochastic models, the Stochastic Growth Rate (SGR) and the Scaled Logarithmic Variance (SLV), can be related between the original and the reduced systems, so that the analysis of the latter allows us to ascertain the population fate in the first.","Then we go on to considering some cases where we illustrate the reduction technique and show the differences between both modelling options.","In some cases using one option represents exponential growth, whereas the other yields extinction."],"url":"http://arxiv.org/abs/2402.03888v1","category":"q-bio.PE"}
{"created":"2024-02-06 10:48:46","title":"MOMENT: A Family of Open Time-series Foundation Models","abstract":"We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.","sentences":["We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis.","Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous.","Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages.","To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training.","Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings.","Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning.","Finally, we present several interesting empirical observations about large pre-trained time-series models.","Our code is available anonymously at anonymous.4open.science/r/BETT-773F/."],"url":"http://arxiv.org/abs/2402.03885v1","category":"cs.LG"}
{"created":"2024-02-06 10:45:51","title":"A Framework for Bilevel Optimization on Riemannian Manifolds","abstract":"Bilevel optimization has seen an increasing presence in various domains of applications. In this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on Riemannian manifolds. We provide several hypergradient estimation strategies on manifolds and study their estimation error. We provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds. We also extend the developments to stochastic bilevel optimization and to the use of general retraction. We showcase the utility of the proposed framework on various applications.","sentences":["Bilevel optimization has seen an increasing presence in various domains of applications.","In this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on Riemannian manifolds.","We provide several hypergradient estimation strategies on manifolds and study their estimation error.","We provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds.","We also extend the developments to stochastic bilevel optimization and to the use of general retraction.","We showcase the utility of the proposed framework on various applications."],"url":"http://arxiv.org/abs/2402.03883v1","category":"math.OC"}
{"created":"2024-02-06 10:45:49","title":"Demonstration of Metaplectic Geometrical Optics for Reduced Modeling of Plasma Waves","abstract":"The WKB approximation of geometrical optics is widely used in plasma physics, quantum mechanics and reduced wave modeling in general. However, it is well-known that the approximation breaks down at focal and turning points. In this work we present the first unsupervised numerical implementation of the recently developed metaplectic geometrical optics framework, which extends the applicability beyond the limitations of WKB, such that the wave field remains finite at caustics. The implementation is in 1D and uses a combination of Gauss-Freud quadrature and barycentric rational function inter- and extrapolation to perform an inverse metaplectic transform numerically. The capabilities of the numerical implementation are demonstrated on Airy's and Weber's equation, which both have exact solutions to compare with. Finally, the implementation is applied to the plasma physics problem of linear conversion of X-mode to electron Bernstein waves at the upper hybrid layer and a comparison is made with results from fully kinetic particle-in-cell simulations. In all three applications we find good agreement between the exact results and the new reduced wave modeling paradigm of metaplectic geometrical optics.","sentences":["The WKB approximation of geometrical optics is widely used in plasma physics, quantum mechanics and reduced wave modeling in general.","However, it is well-known that the approximation breaks down at focal and turning points.","In this work we present the first unsupervised numerical implementation of the recently developed metaplectic geometrical optics framework, which extends the applicability beyond the limitations of WKB, such that the wave field remains finite at caustics.","The implementation is in 1D and uses a combination of Gauss-Freud quadrature and barycentric rational function inter- and extrapolation to perform an inverse metaplectic transform numerically.","The capabilities of the numerical implementation are demonstrated on Airy's and Weber's equation, which both have exact solutions to compare with.","Finally, the implementation is applied to the plasma physics problem of linear conversion of X-mode to electron Bernstein waves at the upper hybrid layer and a comparison is made with results from fully kinetic particle-in-cell simulations.","In all three applications we find good agreement between the exact results and the new reduced wave modeling paradigm of metaplectic geometrical optics."],"url":"http://arxiv.org/abs/2402.03882v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 10:40:40","title":"DEthna: Accurate Ethereum Network Topology Discovery with Marked Transactions","abstract":"In Ethereum, the ledger exchanges messages along an underlying Peer-to-Peer (P2P) network to reach consistency. Understanding the underlying network topology of Ethereum is crucial for network optimization, security and scalability. However, the accurate discovery of Ethereum network topology is non-trivial due to its deliberately designed security mechanism. Consequently, existing measuring schemes cannot accurately infer the Ethereum network topology with a low cost. To address this challenge, we propose the Distributed Ethereum Network Analyzer (DEthna) tool, which can accurately and efficiently measure the Ethereum network topology. In DEthna, a novel parallel measurement model is proposed that can generate marked transactions to infer link connections based on the transaction replacement and propagation mechanism in Ethereum. Moreover, a workload offloading scheme is designed so that DEthna can be deployed on multiple distributed probing nodes so as to measure a large-scale Ethereum network at a low cost. We run DEthna on Goerli (the most popular Ethereum test network) to evaluate its capability in discovering network topology. The experimental results demonstrate that DEthna significantly outperforms the state-of-the-art baselines. Based on DEthna, we further analyze characteristics of the Ethereum network revealing that there exist more than 50% low-degree Ethereum nodes that weaken the network robustness.","sentences":["In Ethereum, the ledger exchanges messages along an underlying Peer-to-Peer (P2P) network to reach consistency.","Understanding the underlying network topology of Ethereum is crucial for network optimization, security and scalability.","However, the accurate discovery of Ethereum network topology is non-trivial due to its deliberately designed security mechanism.","Consequently, existing measuring schemes cannot accurately infer the Ethereum network topology with a low cost.","To address this challenge, we propose the Distributed Ethereum Network Analyzer (DEthna) tool, which can accurately and efficiently measure the Ethereum network topology.","In DEthna, a novel parallel measurement model is proposed that can generate marked transactions to infer link connections based on the transaction replacement and propagation mechanism in Ethereum.","Moreover, a workload offloading scheme is designed so that DEthna can be deployed on multiple distributed probing nodes so as to measure a large-scale Ethereum network at a low cost.","We run DEthna on Goerli (the most popular Ethereum test network) to evaluate its capability in discovering network topology.","The experimental results demonstrate that DEthna significantly outperforms the state-of-the-art baselines.","Based on DEthna, we further analyze characteristics of the Ethereum network revealing that there exist more than 50% low-degree Ethereum nodes that weaken the network robustness."],"url":"http://arxiv.org/abs/2402.03881v1","category":"cs.NI"}
{"created":"2024-02-06 10:40:06","title":"Aperiodic two-layer energy management system for community microgrids based on blockchain strategy","abstract":"This work proposes a geographically-based split of the community microgrids into clusters of members that tend to have similar consumption and generation profiles. Assuming a community microgrid divided into clusters, a two-layer architecture is developed to facilitate the greater penetration of distributed energy resources in an efficient way. The first layer, referred as the market layer, is responsible for creating local energy markets with the aim of maximising the economic benefits for community microgrid members. The second layer is responsible for the network reconfiguration, which is based on the energy balance within each cluster. This layer complies with the IEC 61850 communication standard, in order to control commercial sectionalizing and tie switches. This allows the community microgrid network to be reconfigured to minimise energy exchanges with the main grid. To implement this two-layer energy management strategy, an aperiodic market approach based on Blockchain technology, and the additional functionality offered by Smart Contracts is adopted. This embraces the concept of energy communities since it decentralizes the control and eliminates intermediaries. The use of aperiodic control techniques helps to overcome the challenges of using Blockchain technology in terms of storage, computational requirements and member privacy. The scalability and modularity of the Smart Contract-based system allow each cluster of members to be designed by tailoring the system to their specific needs. The implementation of this strategy is based on low-cost off-the-shelf devices, such as Raspberry Pi 4 Model B boards, which operate as Blockchain nodes of community microgrid members. Finally, the strategy has been validated by emulating two use cases based on the IEEE 123-node system network model highlighting the benefits of the proposal.","sentences":["This work proposes a geographically-based split of the community microgrids into clusters of members that tend to have similar consumption and generation profiles.","Assuming a community microgrid divided into clusters, a two-layer architecture is developed to facilitate the greater penetration of distributed energy resources in an efficient way.","The first layer, referred as the market layer, is responsible for creating local energy markets with the aim of maximising the economic benefits for community microgrid members.","The second layer is responsible for the network reconfiguration, which is based on the energy balance within each cluster.","This layer complies with the IEC 61850 communication standard, in order to control commercial sectionalizing and tie switches.","This allows the community microgrid network to be reconfigured to minimise energy exchanges with the main grid.","To implement this two-layer energy management strategy, an aperiodic market approach based on Blockchain technology, and the additional functionality offered by Smart Contracts is adopted.","This embraces the concept of energy communities since it decentralizes the control and eliminates intermediaries.","The use of aperiodic control techniques helps to overcome the challenges of using Blockchain technology in terms of storage, computational requirements and member privacy.","The scalability and modularity of the Smart Contract-based system allow each cluster of members to be designed by tailoring the system to their specific needs.","The implementation of this strategy is based on low-cost off-the-shelf devices, such as Raspberry Pi 4 Model B boards, which operate as Blockchain nodes of community microgrid members.","Finally, the strategy has been validated by emulating two use cases based on the IEEE 123-node system network model highlighting the benefits of the proposal."],"url":"http://arxiv.org/abs/2402.03880v1","category":"eess.SY"}
{"created":"2024-02-06 10:39:32","title":"The generalizations of Hamiltonian in oriented graphs","abstract":"An oriented graph is an orientation of a simple graph. In 2009, Keevash, K\\\"{u}hn and Osthus proved that every sufficiently large oriented graph $D$ of order $n$ with $(3n-4)/8$ is Hamiltonian. Later, Kelly, K\\\"{u}hn and Osthus showed that it is also pancyclic. Inspired by this, we show that for any given constant $t$ and positive integer partition $n = n_1 + \\cdots + n_t$, if $D$ is an oriented graph on $n$ vertices with minimum semidegree at least $(3n-4)/8$, then it contains $t$ disjoint cycles of lengths $n_1,\\ldots , n_t$. Also, we determine the bounds on the semidegree of sufficiently large oriented graphs that are strongly Hamiltonian-connected, $k$-ordered Hamiltonian and spanning $k$-linked.","sentences":["An oriented graph is an orientation of a simple graph.","In 2009, Keevash, K\\\"{u}hn and Osthus proved that every sufficiently large oriented graph $D$ of order $n$ with $(3n-4)/8$ is Hamiltonian.","Later, Kelly, K\\\"{u}hn and Osthus showed that it is also pancyclic.","Inspired by this, we show that for any given constant $t$ and positive integer partition $n = n_1 + \\cdots + n_t$, if $D$ is an oriented graph on $n$ vertices with minimum semidegree at least $(3n-4)/8$, then it contains $t$ disjoint cycles of lengths $n_1,\\ldots , n_t$. Also, we determine the bounds on the semidegree of sufficiently large oriented graphs that are strongly Hamiltonian-connected, $k$-ordered Hamiltonian and spanning $k$-linked."],"url":"http://arxiv.org/abs/2402.03878v1","category":"math.CO"}
{"created":"2024-02-06 10:37:21","title":"Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models","abstract":"Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.","sentences":["Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored.","We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning.","Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas.","LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements.","To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue.","This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations."],"url":"http://arxiv.org/abs/2402.03877v1","category":"cs.CL"}
{"created":"2024-02-06 10:33:49","title":"Holomorphic forms and non-tautological cycles on moduli spaces of curves","abstract":"We prove, for infinitely many values of $g$ and $n$, the existence of non-tautological algebraic cohomology classes on the moduli space $\\mathcal{M}_{g,n}$ of smooth, genus-$g$, $n$-pointed curves. In particular, when $n=0$, our results show that there exist non-tautological algebraic cohomology classes on $\\mathcal{M}_g$ for $g=12$ and all $g \\geq 16$. These results generalize the work of Graber--Pandharipande and van Zelm, who proved that the classes of particular loci of bielliptic curves are non-tautological and thereby exhibited the only previously-known non-tautological class on any $\\mathcal{M}_g$: the bielliptic cycle on $\\mathcal{M}_{12}$. We extend their work by using the existence of holomorphic forms on certain moduli spaces $\\overline{\\mathcal{M}}_{g,n}$ to produce non-tautological classes with nontrivial restriction to the interior, via which we conclude that the classes of many new double-cover loci are non-tautological.","sentences":["We prove, for infinitely many values of $g$ and $n$, the existence of non-tautological algebraic cohomology classes on the moduli space $\\mathcal{M}_{g,n}$ of smooth, genus-$g$, $n$-pointed curves.","In particular, when $n=0$, our results show that there exist non-tautological algebraic cohomology classes on $\\mathcal{M}_g$ for $g=12$ and all $g \\geq 16$.","These results generalize the work of Graber--Pandharipande and van Zelm, who proved that the classes of particular loci of bielliptic curves are non-tautological and thereby exhibited the only previously-known non-tautological class on any $\\mathcal{M}_g$: the bielliptic cycle on $\\mathcal{M}_{12}$. We extend their work by using the existence of holomorphic forms on certain moduli spaces $\\overline{\\mathcal{M}}_{g,n}$ to produce non-tautological classes with nontrivial restriction to the interior, via which we conclude that the classes of many new double-cover loci are non-tautological."],"url":"http://arxiv.org/abs/2402.03874v1","category":"math.AG"}
{"created":"2024-02-06 10:32:51","title":"Upper deviation probabilities for level sets of a supercritical branching random walk","abstract":"Given a supercritical branching random walk $\\{Z_n\\}_{n\\geq 0}$ on $\\mathbb{R}$, let $Z_n([y,\\infty))$ be the number of particles located in $[y,\\infty)\\subset\\mathbb{R}$ at generation $n$. Let $m$ be the mean of the offspring law of $\\{Z_n\\}_{n\\geq 0}$ and $I(x)$ be the large deviation rate function of the underlying random walk of $\\{Z_n\\}_{n\\geq 0}$. It is known from [6] that under some mild conditions, for $x\\in(0,x^*)$, $n^{-1}\\log Z_n([nx,\\infty))$ converges almost surely to $\\log m- I(x)$ on the event of nonextinction as $n\\to\\infty$, where $x^*$ is the speed of maximal position of the branching random walk. In this work, we investigate its upper deviation probabilities, in other words, the convergence rates of \\[\\mathbb{P}(Z_n([xn,\\infty))\\geq e^{an})\\] as $n\\to\\infty$, where $x>0$ and $a>(\\log m- I(x))^+$. This paper is a counterpart work of the lower deviation probabilities [28] and also completes those results in [1] for the branching Brownian motion.","sentences":["Given a supercritical branching random walk $\\{Z_n\\}_{n\\geq 0}$ on $\\mathbb{R}$, let $Z_n([y,\\infty))$ be the number of particles located in $[y,\\infty)\\subset\\mathbb{R}$ at generation $n$. Let $m$ be the mean of the offspring law of $\\{Z_n\\}_{n\\geq 0}$ and $I(x)$ be the large deviation rate function of the underlying random walk of $\\{Z_n\\}_{n\\geq 0}$.","It is known from [6] that under some mild conditions, for $x\\in(0,x^*)$, $n^{-1}\\log Z_n([nx,\\infty))$ converges almost surely to $\\log m-","I(x)$ on the event of nonextinction as $n\\to\\infty$, where $x^*$ is the speed of maximal position of the branching random walk.","In this work, we investigate its upper deviation probabilities, in other words, the convergence rates of \\[\\mathbb{P}(Z_n([xn,\\infty))\\geq e^{an})\\] as $n\\to\\infty$, where $x>0$ and $a>(\\log m-","I(x))^+$.","This paper is a counterpart work of the lower deviation probabilities [28] and also completes those results in [1] for the branching Brownian motion."],"url":"http://arxiv.org/abs/2402.03872v1","category":"math.PR"}
{"created":"2024-02-06 10:32:34","title":"Less than one percent of words would be affected by gender-inclusive language in German press texts","abstract":"Research on gender and language is tightly knitted to social debates on gender equality and non-discriminatory language use. Psycholinguistic scholars have made significant contributions in this field. However, corpus-based studies that investigate these matters within the context of language use are still rare. In our study, we address the question of how much textual material would actually have to be changed if non-gender-inclusive texts were rewritten to be gender-inclusive. This quantitative measure is an important empirical insight, as a recurring argument against the use of gender-inclusive German is that it supposedly makes written texts too long and complicated. It is also argued that gender-inclusive language has negative effects on language learners. However, such effects are only likely if gender-inclusive texts are very different from those that are not gender-inclusive. In our corpus-linguistic study, we manually annotated German press texts to identify the parts that would have to be changed. Our results show that, on average, less than 1% of all tokens would be affected by gender-inclusive language. This small proportion calls into question whether gender-inclusive German presents a substantial barrier to understanding and learning the language, particularly when we take into account the potential complexities of interpreting masculine generics.","sentences":["Research on gender and language is tightly knitted to social debates on gender equality and non-discriminatory language use.","Psycholinguistic scholars have made significant contributions in this field.","However, corpus-based studies that investigate these matters within the context of language use are still rare.","In our study, we address the question of how much textual material would actually have to be changed if non-gender-inclusive texts were rewritten to be gender-inclusive.","This quantitative measure is an important empirical insight, as a recurring argument against the use of gender-inclusive German is that it supposedly makes written texts too long and complicated.","It is also argued that gender-inclusive language has negative effects on language learners.","However, such effects are only likely if gender-inclusive texts are very different from those that are not gender-inclusive.","In our corpus-linguistic study, we manually annotated German press texts to identify the parts that would have to be changed.","Our results show that, on average, less than 1% of all tokens would be affected by gender-inclusive language.","This small proportion calls into question whether gender-inclusive German presents a substantial barrier to understanding and learning the language, particularly when we take into account the potential complexities of interpreting masculine generics."],"url":"http://arxiv.org/abs/2402.03870v1","category":"cs.CL"}
{"created":"2024-02-06 10:27:34","title":"Patterns in temporal networks with higher-order egocentric structures","abstract":"The analysis of complex and time-evolving interactions like social dynamics represents a current challenge for the science of complex systems. Temporal networks stand as a suitable tool to schematise such systems, encoding all the appearing interactions between pairs of individuals in discrete time. Over the years, network science has developed many measures to analyse and compare temporal networks. Some of them imply a decomposition of the network into small pieces of interactions, i.e. only involving a few nodes for a short time range. Along this line, a possible way to decompose a network is to assume an egocentric perspective, i.e. to consider for each node the time evolution of its neighbourhood. This has been proposed by Longa et al. by defining the \"egocentric temporal neighbourhood\", which has proven a useful tool to characterise temporal networks relative to social interactions. However, this definition neglects group interactions (quite common in social domains) as they are always decomposed into pairwise connections.A more general framework that allows us to consider also larger interactions is represented by higher-order networks. Here, we generalise the description of social interactions by making use of hypergraphs, consequently, we generalise its decomposition into \"hyper egocentric temporal neighbourhoods\". This will allow us to analyse social interactions, to compare different datasets or different nodes inside a dataset, by taking into account the intrinsic complexity represented by higher-order interactions. Even if we limit the order of interactions to the second (triplets of nodes), our results reveal the importance of a higher-order representation. In fact, our analyses show second-order structures are responsible for the majority of the variability at all scales: between datasets, amongst nodes and over time.","sentences":["The analysis of complex and time-evolving interactions like social dynamics represents a current challenge for the science of complex systems.","Temporal networks stand as a suitable tool to schematise such systems, encoding all the appearing interactions between pairs of individuals in discrete time.","Over the years, network science has developed many measures to analyse and compare temporal networks.","Some of them imply a decomposition of the network into small pieces of interactions, i.e. only involving a few nodes for a short time range.","Along this line, a possible way to decompose a network is to assume an egocentric perspective, i.e. to consider for each node the time evolution of its neighbourhood.","This has been proposed by Longa et al.","by defining the \"egocentric temporal neighbourhood\", which has proven a useful tool to characterise temporal networks relative to social interactions.","However, this definition neglects group interactions (quite common in social domains) as they are always decomposed into pairwise connections.","A more general framework that allows us to consider also larger interactions is represented by higher-order networks.","Here, we generalise the description of social interactions by making use of hypergraphs, consequently, we generalise its decomposition into \"hyper egocentric temporal neighbourhoods\".","This will allow us to analyse social interactions, to compare different datasets or different nodes inside a dataset, by taking into account the intrinsic complexity represented by higher-order interactions.","Even if we limit the order of interactions to the second (triplets of nodes), our results reveal the importance of a higher-order representation.","In fact, our analyses show second-order structures are responsible for the majority of the variability at all scales: between datasets, amongst nodes and over time."],"url":"http://arxiv.org/abs/2402.03866v1","category":"physics.soc-ph"}
{"created":"2024-02-06 10:22:19","title":"Discounted Stochastic Games for the Smart Grid with Prospect Prosumers","abstract":"As a crucial component of the demand side in the smart grid, prosumers who are equipped with renewable resource generators can produce and consume energy. This paper considers the discounted criterion of nonzero-sum stochastic games with prospect prosumers. The uncertainty is from the renewable energy. The subjective behavior of prosumers is described by the prospect theory (PT). Compared to the average criterion of stochastic games under PT studied firstly in 2018, we are concerned with the time value of utility, i.e., the utility should be discounted in the future. The state transition probability is nonstationary, which makes the generated energy of prosumers more in line with the actual situation. Since PT distorts the probability, the optimality equation that plays a significant role in proving the existence of equilibrium does not exist. On the other hand, the games change into Markov decision processes (MDPs) with nonstationary payoff function when fixing others' stationary Markov strategies, then the occupation measure and the linear programming of stationary MDPs are no longer suitable. Therefore, we explore a new technique by constructing the marginal distribution on the state-action pairs at any time, and establish the existence of Nash equilibrium. When the initial state is given, there exists a Markov Nash equilibrium. Furthermore, this novel technique can be extended to the finite horizon criterion. Finally, we present an algorithm to find a Markov $\\varepsilon$-equilibrium and give some simulation results.","sentences":["As a crucial component of the demand side in the smart grid, prosumers who are equipped with renewable resource generators can produce and consume energy.","This paper considers the discounted criterion of nonzero-sum stochastic games with prospect prosumers.","The uncertainty is from the renewable energy.","The subjective behavior of prosumers is described by the prospect theory (PT).","Compared to the average criterion of stochastic games under PT studied firstly in 2018, we are concerned with the time value of utility, i.e., the utility should be discounted in the future.","The state transition probability is nonstationary, which makes the generated energy of prosumers more in line with the actual situation.","Since PT distorts the probability, the optimality equation that plays a significant role in proving the existence of equilibrium does not exist.","On the other hand, the games change into Markov decision processes (MDPs) with nonstationary payoff function when fixing others' stationary Markov strategies, then the occupation measure and the linear programming of stationary MDPs are no longer suitable.","Therefore, we explore a new technique by constructing the marginal distribution on the state-action pairs at any time, and establish the existence of Nash equilibrium.","When the initial state is given, there exists a Markov Nash equilibrium.","Furthermore, this novel technique can be extended to the finite horizon criterion.","Finally, we present an algorithm to find a Markov $\\varepsilon$-equilibrium and give some simulation results."],"url":"http://arxiv.org/abs/2402.03862v1","category":"math.OC"}
{"created":"2024-02-06 10:14:00","title":"General boundary conditions for a Boussinesq model with varying bathymetry","abstract":"This paper is devoted to the theoretical and numerical investigation of the initial boundary value problem for a system of equations used for the description of waves in coastal areas, namely, the Boussinesq-Abbott system in the presence of topography. We propose a procedure that allows one to handle very general linear or nonlinear boundary conditions. It consists in reducing the problem to a system of conservation laws with nonlocal fluxes and coupled to an ODE. This reformulation is used to propose two hybrid finite volumes/finite differences schemes of first and second order respectively. The possibility to use many kinds of boundary conditions is used to investigate numerically the asymptotic stability of the boundary conditions, which is an issue of practical relevance in coastal oceanography since asymptotically stable boundary conditions would allow one to reconstruct a wave field from the knowledge of the boundary data only, even if the initial data is not known.","sentences":["This paper is devoted to the theoretical and numerical investigation of the initial boundary value problem for a system of equations used for the description of waves in coastal areas, namely, the Boussinesq-Abbott system in the presence of topography.","We propose a procedure that allows one to handle very general linear or nonlinear boundary conditions.","It consists in reducing the problem to a system of conservation laws with nonlocal fluxes and coupled to an ODE.","This reformulation is used to propose two hybrid finite volumes/finite differences schemes of first and second order respectively.","The possibility to use many kinds of boundary conditions is used to investigate numerically the asymptotic stability of the boundary conditions, which is an issue of practical relevance in coastal oceanography since asymptotically stable boundary conditions would allow one to reconstruct a wave field from the knowledge of the boundary data only, even if the initial data is not known."],"url":"http://arxiv.org/abs/2402.03859v1","category":"math.AP"}
{"created":"2024-02-06 10:06:13","title":"Position Paper: Toward New Frameworks for Studying Model Representations","abstract":"Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.","sentences":["Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn.","Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned.","However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis.","We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations.","With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations."],"url":"http://arxiv.org/abs/2402.03855v1","category":"cs.LG"}
{"created":"2024-02-06 09:50:15","title":"Global certification via perfect hashing","abstract":"In this work, we provide an upper bound for global certification of graph homomorphism, a generalization of graph coloring. In certification, the nodes of a network should decide if the network satisfies a given property, thanks to small pieces of information called certificates. Here, there is only one global certificate which is shared by all the nodes, and the property we want to certify is the existence of a graph homomorphism to a given graph.   For bipartiteness, a special case of graph homomorphism, Feuilloley and Hirvonen proved in~\\cite{FeuilloleyH18} some upper and lower bounds on the size of the optimal certificate, and made the conjecture that their lower bound could be improved to match their upper bound. We prove that this conjecture is false: their lower bound was in fact optimal, and we prove it by providing the matching upper bound using a known result of perfect hashing.","sentences":["In this work, we provide an upper bound for global certification of graph homomorphism, a generalization of graph coloring.","In certification, the nodes of a network should decide if the network satisfies a given property, thanks to small pieces of information called certificates.","Here, there is only one global certificate which is shared by all the nodes, and the property we want to certify is the existence of a graph homomorphism to a given graph.   ","For bipartiteness, a special case of graph homomorphism, Feuilloley and Hirvonen proved in~\\cite{FeuilloleyH18} some upper and lower bounds on the size of the optimal certificate, and made the conjecture that their lower bound could be improved to match their upper bound.","We prove that this conjecture is false: their lower bound was in fact optimal, and we prove it by providing the matching upper bound using a known result of perfect hashing."],"url":"http://arxiv.org/abs/2402.03849v1","category":"cs.DC"}
{"created":"2024-02-06 09:50:08","title":"ANLS* -- A Universal Document Processing Metric for Generative Large Language Models","abstract":"Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores. An evaluation of 7 different datasets and 3 different GLLMs using the ANLS* metric is also provided, demonstrating the importance of the proposed metric. We also benchmark a novel approach to generate prompts for documents, called SFT, against other prompting techniques such as LATIN. In 15 out of 21 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $15$ percentage points.   Sources are available at https://github.com/deepopinion/anls_star_metric","sentences":["Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction.","These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score.","However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning.","However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs.","This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks.","The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores.","An evaluation of 7 different datasets and 3 different GLLMs using the ANLS* metric is also provided, demonstrating the importance of the proposed metric.","We also benchmark a novel approach to generate prompts for documents, called SFT, against other prompting techniques such as LATIN.","In 15 out of 21 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $15$ percentage points.   ","Sources are available at https://github.com/deepopinion/anls_star_metric"],"url":"http://arxiv.org/abs/2402.03848v1","category":"cs.CL"}
{"created":"2024-02-06 09:48:33","title":"Efficient Generation of Hidden Outliers for Improved Outlier Detection","abstract":"Outlier generation is a popular technique used for solving important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the 'multiple views' property of outliers in high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose BISECT, a new outlier generation method that creates realistic outliers mimicking said property. To do so, BISECT employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current methodology for recreating 'multiple views'. We use the synthetic outliers generated by BISECT to effectively enhance outlier detection in diverse datasets, for multiple use cases. For instance, oversampling with BISECT reduced the error by up to 3 times when compared with the baselines.","sentences":["Outlier generation is a popular technique used for solving important outlier detection tasks.","Generating outliers with realistic behavior is challenging.","Popular existing methods tend to disregard the 'multiple views' property of outliers in high-dimensional spaces.","The only existing method accounting for this property falls short in efficiency and effectiveness.","We propose BISECT, a new outlier generation method that creates realistic outliers mimicking said property.","To do so, BISECT employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers.","Our method has better guarantees and complexity than the current methodology for recreating 'multiple views'.","We use the synthetic outliers generated by BISECT to effectively enhance outlier detection in diverse datasets, for multiple use cases.","For instance, oversampling with BISECT reduced the error by up to 3 times when compared with the baselines."],"url":"http://arxiv.org/abs/2402.03846v1","category":"cs.LG"}
{"created":"2024-02-06 09:41:43","title":"On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models","abstract":"Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions. They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model. In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process). Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative). Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical results. Here, we provide three analytical results regarding the extent of the modeling freedom of this vector field. {Firstly, we propose a novel decomposition of vector fields into a conservative component and an orthogonal component which satisfies a given (gauge) freedom. Secondly, from this orthogonal decomposition, we show that exact density estimation and exact sampling is achieved when the conservative component is exactly equals to the true score and therefore conservativity is neither necessary nor sufficient to obtain exact density estimation and exact sampling. Finally, we show that when it comes to inferring local information of the data manifold, constraining the vector field to be conservative is desirable.","sentences":["Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions.","They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model.","In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process).","Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative).","Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical results.","Here, we provide three analytical results regarding the extent of the modeling freedom of this vector field.","{Firstly, we propose a novel decomposition of vector fields into a conservative component and an orthogonal component which satisfies a given (gauge) freedom.","Secondly, from this orthogonal decomposition, we show that exact density estimation and exact sampling is achieved when the conservative component is exactly equals to the true score and therefore conservativity is neither necessary nor sufficient to obtain exact density estimation and exact sampling.","Finally, we show that when it comes to inferring local information of the data manifold, constraining the vector field to be conservative is desirable."],"url":"http://arxiv.org/abs/2402.03845v1","category":"cs.LG"}
{"created":"2024-02-06 09:39:05","title":"A new method for optical steel rope non-destructive damage detection","abstract":"This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the highest accuracy achieved by the detection model reached 0.975, and the maximum F-measure achieved by the segmentation model reached 0.948.","sentences":["This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway).","The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds.","This model is equipped with the capability to process and combine color and depth information through the proposed CMA module.","Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes.","It integrates the VovNet architecture with a DBB module to enhance performance.","Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model.","Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models.","Experiments demonstrate a significant improvement over baseline models.","On the proposed dataset, the highest accuracy achieved by the detection model reached 0.975, and the maximum F-measure achieved by the segmentation model reached 0.948."],"url":"http://arxiv.org/abs/2402.03843v1","category":"cs.CV"}
{"created":"2024-02-06 09:38:21","title":"Momentum-space Langevin dynamics of holographic Wilsonian RG flow: self-interacting massive scalar field theory","abstract":"We explore holographic Wilsonian renormalization group(HWRG) and stochastic quantization(SQ) motivated by the similarity of the monotonicity in RG flow and Langevin dynamics of non-equilibrium thermodynamics with the scalar field theory in AdS space with its generic mass, self-interaction, and boundary deformation in the momentum space. Identifying the stochastic time $t$ and radial coordinate $r$ in AdS, we establish maps between the fictitious time evolution of stochastic $n$-point correlation function and the radial evolution of multi-trace deformation, which respectively, express the relaxation process of Langevin dynamics and holographic RG flow. We especially consider marginal multi-trace deformation on the AdS boundary which is successfully captured by a Langevin dynamics of SQ.","sentences":["We explore holographic Wilsonian renormalization group(HWRG) and stochastic quantization(SQ) motivated by the similarity of the monotonicity in RG flow and Langevin dynamics of non-equilibrium thermodynamics with the scalar field theory in AdS space with its generic mass, self-interaction, and boundary deformation in the momentum space.","Identifying the stochastic time $t$ and radial coordinate $r$ in AdS, we establish maps between the fictitious time evolution of stochastic $n$-point correlation function and the radial evolution of multi-trace deformation, which respectively, express the relaxation process of Langevin dynamics and holographic RG flow.","We especially consider marginal multi-trace deformation on the AdS boundary which is successfully captured by a Langevin dynamics of SQ."],"url":"http://arxiv.org/abs/2402.03841v1","category":"hep-th"}
{"created":"2024-02-06 09:37:42","title":"Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation","abstract":"In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of \\textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for the task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects.","sentences":["In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information.","We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission.","We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data.","A novel Graph Convolutional Neural Network (GCN) model is developed, to learn CECI from a repository of 3D scene graphs.","As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces.","The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method.","We establish the novel concept of \\textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations.","This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level reasoning for the task planning and optimization of a variety of robotics missions.","The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects."],"url":"http://arxiv.org/abs/2402.03840v1","category":"cs.RO"}
{"created":"2024-02-06 09:33:07","title":"Expressivity of Geometric Inhomogeneous Random Graphs -- Metric and Non-Metric","abstract":"Recently there has been increased interest in fitting generative graph models to real-world networks. In particular, Bl\\\"asius et al. have proposed a framework for systematic evaluation of the expressivity of random graph models. We extend this framework to Geometric Inhomogeneous Random Graphs (GIRGs). This includes a family of graphs induced by non-metric distance functions which allow capturing more complex models of partial similarity between nodes as a basis of connection - as well as homogeneous and non-homogeneous feature spaces. As part of the extension, we develop schemes for estimating the multiplicative constant and the long-range parameter in the connection probability. Moreover, we devise an algorithm for sampling Minimum-Component-Distance GIRGs whose runtime is linear both in the number of vertices and in the dimension of the underlying geometric space. Our results provide evidence that GIRGs are more realistic candidates with respect to various graph features such as closeness centrality, betweenness centrality, local clustering coefficient, and graph effective diameter, while they face difficulties to replicate higher variance and more extreme values of graph statistics observed in real-world networks.","sentences":["Recently there has been increased interest in fitting generative graph models to real-world networks.","In particular, Bl\\\"asius et al. have proposed a framework for systematic evaluation of the expressivity of random graph models.","We extend this framework to Geometric Inhomogeneous Random Graphs (GIRGs).","This includes a family of graphs induced by non-metric distance functions which allow capturing more complex models of partial similarity between nodes as a basis of connection - as well as homogeneous and non-homogeneous feature spaces.","As part of the extension, we develop schemes for estimating the multiplicative constant and the long-range parameter in the connection probability.","Moreover, we devise an algorithm for sampling Minimum-Component-Distance GIRGs whose runtime is linear both in the number of vertices and in the dimension of the underlying geometric space.","Our results provide evidence that GIRGs are more realistic candidates with respect to various graph features such as closeness centrality, betweenness centrality, local clustering coefficient, and graph effective diameter, while they face difficulties to replicate higher variance and more extreme values of graph statistics observed in real-world networks."],"url":"http://arxiv.org/abs/2402.03837v1","category":"cs.SI"}
{"created":"2024-02-06 09:26:46","title":"Enhanced Security and Efficiency in Blockchain with Aggregated Zero-Knowledge Proof Mechanisms","abstract":"Blockchain technology has emerged as a revolutionary tool in ensuring data integrity and security in digital transactions. However, the current approaches to data verification in blockchain systems, particularly in Ethereum, face challenges in terms of efficiency and computational overhead. The traditional use of Merkle Trees and cryptographic hash functions, while effective, leads to significant resource consumption, especially for large datasets. This highlights a gap in existing research: the need for more efficient methods of data verification in blockchain networks. Our study addresses this gap by proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the structure of Merkle Trees. We develop a system that significantly reduces the size of the proof and the computational resources needed for its generation and verification. Our approach represents a paradigm shift in blockchain data verification, balancing security with efficiency. We conducted extensive experimental evaluations using real Ethereum block data to validate the effectiveness of our proposed scheme. The results demonstrate a drastic reduction in proof size and computational requirements compared to traditional methods, making the verification process more efficient and economically viable. Our contribution fills a critical research void, offering a scalable and secure solution for blockchain data verification. The implications of our work are far-reaching, enhancing the overall performance and adaptability of blockchain technology in various applications, from financial transactions to supply chain management.","sentences":["Blockchain technology has emerged as a revolutionary tool in ensuring data integrity and security in digital transactions.","However, the current approaches to data verification in blockchain systems, particularly in Ethereum, face challenges in terms of efficiency and computational overhead.","The traditional use of Merkle Trees and cryptographic hash functions, while effective, leads to significant resource consumption, especially for large datasets.","This highlights a gap in existing research: the need for more efficient methods of data verification in blockchain networks.","Our study addresses this gap by proposing an innovative aggregation scheme for Zero-Knowledge Proofs within the structure of Merkle Trees.","We develop a system that significantly reduces the size of the proof and the computational resources needed for its generation and verification.","Our approach represents a paradigm shift in blockchain data verification, balancing security with efficiency.","We conducted extensive experimental evaluations using real Ethereum block data to validate the effectiveness of our proposed scheme.","The results demonstrate a drastic reduction in proof size and computational requirements compared to traditional methods, making the verification process more efficient and economically viable.","Our contribution fills a critical research void, offering a scalable and secure solution for blockchain data verification.","The implications of our work are far-reaching, enhancing the overall performance and adaptability of blockchain technology in various applications, from financial transactions to supply chain management."],"url":"http://arxiv.org/abs/2402.03834v1","category":"cs.CR"}
{"created":"2024-02-06 09:24:53","title":"An SVD-free Approach to Nonlinear Dictionary Learning based on RVFL","abstract":"This paper presents a novel nonlinear dictionary learning algorithm leveraging the theory of a feed-forward neural network called Random Vector Functional Link (RVFL). The proposed RVFL-based nonlinear Dictionary Learning (RVFLDL) learns a dictionary as a sparse-to-dense feature map from nonlinear sparse coefficients to the dense input features. Kernel-based nonlinear dictionary learning methods operate in a feature space obtained by an implicit feature map, and they are not independent of computationally expensive operations like Singular Value Decomposition (SVD). Training the RVFL-based dictionary is free from SVD computation as RVFL generates weights from the input to the output layer analytically. Sparsity-inducing Horse-shoe prior is assumed on the coefficients to generate a sparse coefficient matrix w.r.t an initial random dictionary. Higher-order dependencies between the input sparse coefficients and the dictionary atoms are incorporated into the training process by nonlinearly transforming the sparse coefficients and adding them as enhanced features. Thus the method projects sparse coefficients to a higher dimensional space while inducing nonlinearities into the dictionary. For classification using RVFL-net, a classifier matrix is learned as a transform that maps nonlinear sparse coefficients to the labels. The performance of the method illustrated in image classification and reconstruction applications is comparable to that of other nonlinear dictionary learning methods. Experiments show that RVFLDL is scalable and provides a solution better than those obtained using other nonlinear dictionary learning methods.","sentences":["This paper presents a novel nonlinear dictionary learning algorithm leveraging the theory of a feed-forward neural network called Random Vector Functional Link (RVFL).","The proposed RVFL-based nonlinear Dictionary Learning (RVFLDL) learns a dictionary as a sparse-to-dense feature map from nonlinear sparse coefficients to the dense input features.","Kernel-based nonlinear dictionary learning methods operate in a feature space obtained by an implicit feature map, and they are not independent of computationally expensive operations like Singular Value Decomposition (SVD).","Training the RVFL-based dictionary is free from SVD computation as RVFL generates weights from the input to the output layer analytically.","Sparsity-inducing Horse-shoe prior is assumed on the coefficients to generate a sparse coefficient matrix w.r.t an initial random dictionary.","Higher-order dependencies between the input sparse coefficients and the dictionary atoms are incorporated into the training process by nonlinearly transforming the sparse coefficients and adding them as enhanced features.","Thus the method projects sparse coefficients to a higher dimensional space while inducing nonlinearities into the dictionary.","For classification using RVFL-net, a classifier matrix is learned as a transform that maps nonlinear sparse coefficients to the labels.","The performance of the method illustrated in image classification and reconstruction applications is comparable to that of other nonlinear dictionary learning methods.","Experiments show that RVFLDL is scalable and provides a solution better than those obtained using other nonlinear dictionary learning methods."],"url":"http://arxiv.org/abs/2402.03833v1","category":"cs.CV"}
{"created":"2024-02-06 09:19:44","title":"OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving","abstract":"With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency. The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years. Data plays a core role in the algorithm closed-loop system. However, collecting real-world data is expensive, time-consuming, and unsafe. With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering. It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology. (2) Trajectory editing of the ego vehicle and participating vehicles. (3) Rich vehicle model library that can be freely selected and inserted into the scene. (4) Rich sensors model library where you can select specified sensors to generate data. (5) A highly customizable data generation system can generate data according to user needs. We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition. Code is available at https://github.com/PJLab-ADG/OASim.","sentences":["With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency.","The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years.","Data plays a core role in the algorithm closed-loop system.","However, collecting real-world data is expensive, time-consuming, and unsafe.","With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering.","It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology.","(2) Trajectory editing of the ego vehicle and participating vehicles.","(3) Rich vehicle model library that can be freely selected and inserted into the scene.","(4) Rich sensors model library where you can select specified sensors to generate data.","(5) A highly customizable data generation system can generate data according to user needs.","We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition.","Code is available at https://github.com/PJLab-ADG/OASim."],"url":"http://arxiv.org/abs/2402.03830v1","category":"cs.CV"}
{"created":"2024-02-06 09:17:07","title":"Estimating Barycenters of Distributions with Neural Optimal Transport","abstract":"Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.","sentences":["Given a collection of probability measures, a practitioner sometimes needs to find an \"average\" distribution which adequately aggregates reference distributions.","A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work.","By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem.","Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions.","These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost.","We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups."],"url":"http://arxiv.org/abs/2402.03828v1","category":"cs.LG"}
{"created":"2024-02-06 09:16:29","title":"Conditions for growth and extinction in matrix models with environmental stochasticity","abstract":"In this kind of model, the main characteristic that determines population viability in the long term is the stochastic growth rate (SGR) denoted $\\lambda_S$. When $\\lambda_S$ is larger than one, the population grows exponentially with probability one and when it is smaller than one, the population goes extinct with probability one. However, even in very simple situations it is not possible to calculate the SGR analytically. The literature offers some approximations for the case in which environmental variability is low, and there are also some lower and upper bounds, but there is no study of the practical situations in which they would be tight. Some new bounds for the SGR are built and the conditions under which each bound works best are analyzed. These bounds are used to give some necessary and some sufficient conditions for population explosion and extinction that are easy to check in practice. The general results are applied to several cases, amongst them a population structured as juveniles and adults living in an environment switching randomly between \"rich\" and \"poor\".","sentences":["In this kind of model, the main characteristic that determines population viability in the long term is the stochastic growth rate (SGR) denoted $\\lambda_S$. When $\\lambda_S$ is larger than one, the population grows exponentially with probability one and when it is smaller than one, the population goes extinct with probability one.","However, even in very simple situations it is not possible to calculate the SGR analytically.","The literature offers some approximations for the case in which environmental variability is low, and there are also some lower and upper bounds, but there is no study of the practical situations in which they would be tight.","Some new bounds for the SGR are built and the conditions under which each bound works best are analyzed.","These bounds are used to give some necessary and some sufficient conditions for population explosion and extinction that are easy to check in practice.","The general results are applied to several cases, amongst them a population structured as juveniles and adults living in an environment switching randomly between \"rich\" and \"poor\"."],"url":"http://arxiv.org/abs/2402.03827v1","category":"math.DS"}
{"created":"2024-02-06 09:11:20","title":"A call for embodied AI","abstract":"We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.","sentences":["We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models.","We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning.","By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent.","This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development.","Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist.","Our discussion lays down a foundational guideline for future Embodied AI research.","Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI."],"url":"http://arxiv.org/abs/2402.03824v1","category":"cs.AI"}
{"created":"2024-02-06 09:10:35","title":"RevOrder: A Novel Method for Enhanced Arithmetic in Language Models","abstract":"This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.","sentences":["This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks.","Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation complexity.","Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle.","Implementation of RevOrder is cost-effective for both training and inference phases.","Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4."],"url":"http://arxiv.org/abs/2402.03822v1","category":"cs.AI"}
{"created":"2024-02-06 09:07:26","title":"Asymptotic generalization error of a single-layer graph convolutional network","abstract":"While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.","sentences":["While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks.","In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit.","Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM.","We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases."],"url":"http://arxiv.org/abs/2402.03818v1","category":"cs.LG"}
{"created":"2024-02-06 08:56:22","title":"Using Perspective-n-Point Algorithms for a Local Positioning System Based on LEDs and a QADA Receiver","abstract":"The research interest on location-based services has increased during the last years ever since 3D centimetre accuracy inside intelligent environments could be confronted with. This work proposes an indoor local positioning system based on LED lighting, transmitted from a set of beacons to a receiver.The receiver is based on a quadrant photodiode angular diversity aperture (QADA) plus an aperture placed over it.This configuration can be modelled as a perspective camera, where the image position of the transmitters can be used to recover the receiver's 3D pose. This process is known as the perspective-n-point (PnP) problem, which is well known in computer vision and photogrammetry. This work investigates the use of different state-of-the-art PnP algorithms to localize the receiver in a large space based on four co-planar transmitters and with a distance from transmitters to receiver of 3.4 m. Encoding techniques are used to permit the simultaneous emission of all the transmitted signals and their processing in the receiver. In addition, correlation techniques are used to determine the image points projected from each emitter on the QADA. This work uses Monte Carlo simulations to characterize the absolute errors for a grid of test points under noisy measurements, as well as the robustness of the system when varying the 3D location of one transmitter. The IPPE algorithm obtained the best performance in this configuration. The proposal has also been experimentally evaluated in a real setup. The estimation of the receiver's position at three points using the IPPE algorithm achieves average absolute errors of 4.33cm, 3.51cm and 28.90cm in the coordinates x, y and z, respectively. These positioning results are in line with those obtained in previous work using triangulation techniques but with the addition that the complete pose of the receiver is obtained in this proposal.","sentences":["The research interest on location-based services has increased during the last years ever since 3D centimetre accuracy inside intelligent environments could be confronted with.","This work proposes an indoor local positioning system based on LED lighting, transmitted from a set of beacons to a receiver.","The receiver is based on a quadrant photodiode angular diversity aperture (QADA) plus an aperture placed over it.","This configuration can be modelled as a perspective camera, where the image position of the transmitters can be used to recover the receiver's 3D pose.","This process is known as the perspective-n-point (PnP) problem, which is well known in computer vision and photogrammetry.","This work investigates the use of different state-of-the-art PnP algorithms to localize the receiver in a large space based on four co-planar transmitters and with a distance from transmitters to receiver of 3.4 m. Encoding techniques are used to permit the simultaneous emission of all the transmitted signals and their processing in the receiver.","In addition, correlation techniques are used to determine the image points projected from each emitter on the QADA.","This work uses Monte Carlo simulations to characterize the absolute errors for a grid of test points under noisy measurements, as well as the robustness of the system when varying the 3D location of one transmitter.","The IPPE algorithm obtained the best performance in this configuration.","The proposal has also been experimentally evaluated in a real setup.","The estimation of the receiver's position at three points using the IPPE algorithm achieves average absolute errors of 4.33cm, 3.51cm and 28.90cm in the coordinates x, y and z, respectively.","These positioning results are in line with those obtained in previous work using triangulation techniques but with the addition that the complete pose of the receiver is obtained in this proposal."],"url":"http://arxiv.org/abs/2402.03811v1","category":"eess.SP"}
{"created":"2024-02-06 08:49:27","title":"Combining additivity and active subspaces for high-dimensional Gaussian process modeling","abstract":"Gaussian processes are a widely embraced technique for regression and classification due to their good prediction accuracy, analytical tractability and built-in capabilities for uncertainty quantification. However, they suffer from the curse of dimensionality whenever the number of variables increases. This challenge is generally addressed by assuming additional structure in theproblem, the preferred options being either additivity or low intrinsic dimensionality. Our contribution for high-dimensional Gaussian process modeling is to combine them with a multi-fidelity strategy, showcasing the advantages through experiments on synthetic functions and datasets.","sentences":["Gaussian processes are a widely embraced technique for regression and classification due to their good prediction accuracy, analytical tractability and built-in capabilities for uncertainty quantification.","However, they suffer from the curse of dimensionality whenever the number of variables increases.","This challenge is generally addressed by assuming additional structure in theproblem, the preferred options being either additivity or low intrinsic dimensionality.","Our contribution for high-dimensional Gaussian process modeling is to combine them with a multi-fidelity strategy, showcasing the advantages through experiments on synthetic functions and datasets."],"url":"http://arxiv.org/abs/2402.03809v1","category":"math.OC"}
{"created":"2024-02-06 08:48:39","title":"SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising","abstract":"Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart. Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference). Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data. In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising. To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database. The experiment result indicates that SDEMG outperformed comparative methods and produced high-quality sEMG samples. The source code of SDEMG the framework is available at: https://github.com/tonyliu0910/SDEMG","sentences":["Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart.","Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG","(sEMG with ECG interference).","Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data.","In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising.","To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database.","The experiment result indicates that SDEMG outperformed comparative methods and produced high-quality sEMG samples.","The source code of SDEMG the framework is available at: https://github.com/tonyliu0910/SDEMG"],"url":"http://arxiv.org/abs/2402.03808v1","category":"eess.SP"}
{"created":"2024-02-06 08:48:01","title":"SEABO: A Simple Search-Based Method for Offline Imitation Learning","abstract":"Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks. Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations. Our code is publicly available at https://github.com/dmksjfl/SEABO.","sentences":["Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment.","Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels.","In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient.","To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data.","To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO.","SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner.","Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL algorithms with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and offline IL methods across many tasks.","Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations.","Our code is publicly available at https://github.com/dmksjfl/SEABO."],"url":"http://arxiv.org/abs/2402.03807v1","category":"cs.LG"}
{"created":"2024-02-06 08:47:16","title":"Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering","abstract":"This paper explores the integration of Explainable Automated Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making. The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems. The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while Explainable AI (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models' decision-making processes. This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems. The findings underscore the potential of explainable AutoML in improving the transparency and accountability of AI-driven financial decisions, aligning with regulatory requirements and ethical considerations.","sentences":["This paper explores the integration of Explainable Automated Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making.","The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems.","The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while Explainable AI (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models' decision-making processes.","This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems.","The findings underscore the potential of explainable AutoML in improving the transparency and accountability of AI-driven financial decisions, aligning with regulatory requirements and ethical considerations."],"url":"http://arxiv.org/abs/2402.03806v1","category":"q-fin.RM"}
{"created":"2024-02-06 08:46:14","title":"Automated Description Generation for Software Patches","abstract":"Software patches are pivotal in refining and evolving codebases, addressing bugs, vulnerabilities, and optimizations. Patch descriptions provide detailed accounts of changes, aiding comprehension and collaboration among developers. However, manual description creation poses challenges in terms of time consumption and variations in quality and detail. In this paper, we propose PATCHEXPLAINER, an approach that addresses these challenges by framing patch description generation as a machine translation task. In PATCHEXPLAINER, we leverage explicit representations of critical elements, historical context, and syntactic conventions. Moreover, the translation model in PATCHEXPLAINER is designed with an awareness of description similarity. Particularly, the model is explicitly trained to recognize and incorporate similarities present in patch descriptions clustered into groups, improving its ability to generate accurate and consistent descriptions across similar patches. The dual objectives maximize similarity and accurately predict affiliating groups. Our experimental results on a large dataset of real-world software patches show that PATCHEXPLAINER consistently outperforms existing methods, with improvements up to 189% in BLEU, 5.7X in Exact Match rate, and 154% in Semantic Similarity, affirming its effectiveness in generating software patch descriptions.","sentences":["Software patches are pivotal in refining and evolving codebases, addressing bugs, vulnerabilities, and optimizations.","Patch descriptions provide detailed accounts of changes, aiding comprehension and collaboration among developers.","However, manual description creation poses challenges in terms of time consumption and variations in quality and detail.","In this paper, we propose PATCHEXPLAINER, an approach that addresses these challenges by framing patch description generation as a machine translation task.","In PATCHEXPLAINER, we leverage explicit representations of critical elements, historical context, and syntactic conventions.","Moreover, the translation model in PATCHEXPLAINER is designed with an awareness of description similarity.","Particularly, the model is explicitly trained to recognize and incorporate similarities present in patch descriptions clustered into groups, improving its ability to generate accurate and consistent descriptions across similar patches.","The dual objectives maximize similarity and accurately predict affiliating groups.","Our experimental results on a large dataset of real-world software patches show that PATCHEXPLAINER consistently outperforms existing methods, with improvements up to 189% in BLEU, 5.7X in Exact Match rate, and 154% in Semantic Similarity, affirming its effectiveness in generating software patch descriptions."],"url":"http://arxiv.org/abs/2402.03805v1","category":"cs.SE"}
{"created":"2024-02-06 08:45:51","title":"ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs","abstract":"Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs. We will release the code to facilitate future research.","sentences":["Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons.","While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values.","We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation.","To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity.","We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$.","The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.","We will release the code to facilitate future research."],"url":"http://arxiv.org/abs/2402.03804v1","category":"cs.LG"}
{"created":"2024-02-06 08:37:19","title":"Effects of carbon pricing and other climate policies on CO2 emissions","abstract":"We provide ex-post empirical analysis of the effects of climate policies on carbon dioxide emissions at the aggregate national level. Our results are based on a comprehensive database of 121 countries. As climate policies we examine carbon taxes and emissions trading systems (ETS), as well as the overall stringency of climate policies. We use dynamic panel regressions, controlling for macroeconomic factors such as economic development, GDP growth, urbanisation, as well as the energy mix. We find that higher carbon taxes and prices of permits in ETS reduce carbon emissions. An increase in carbon taxes by $10 per ton of CO2 reduces CO2 emissions per capita by 1.3% in the short run and by 4.6% in the long run.","sentences":["We provide ex-post empirical analysis of the effects of climate policies on carbon dioxide emissions at the aggregate national level.","Our results are based on a comprehensive database of 121 countries.","As climate policies we examine carbon taxes and emissions trading systems (ETS), as well as the overall stringency of climate policies.","We use dynamic panel regressions, controlling for macroeconomic factors such as economic development, GDP growth, urbanisation, as well as the energy mix.","We find that higher carbon taxes and prices of permits in ETS reduce carbon emissions.","An increase in carbon taxes by $10 per ton of CO2 reduces CO2 emissions per capita by 1.3% in the short run and by 4.6% in the long run."],"url":"http://arxiv.org/abs/2402.03800v1","category":"econ.GN"}
{"created":"2024-02-06 08:30:26","title":"Saturation of fishbone instability through zonal flows driven by energetic particle transport in tokamak plasmas","abstract":"Gyrokinetic and kinetic-MHD simulations are performed for the fishbone instability in the DIII-D discharge #178631, chosen for validation of first-principles simulations to predict the energetic particle (EP) transport in an ITER prefusion baseline scenario. Fishbone modes are found to generate zonal flows, which dominate the fishbone saturation. The underlying mechanisms of the two-way fishbone-zonal flows nonlinear interplay are discussed in details. Numerical and analytical analyses identify the fishbone-induced EP redistribution as the dominant generation mechanism for zonal flows. The zonal flows modify the nonlinear dynamics of phase space zonal structures, which reduces the amount of EPs able to resonate with the mode, leading to an early fishbone saturation. Simulation results including zonal flows agree quantitatively with DIII-D experimental measurements of the fishbone saturation amplitude and EP transport, supporting this novel saturation mechanism by self-generated zonal flows. Moreover, the wave-particle mode-locking mechanism is shown to determine quantitatively the fishbone frequency down-chirping, as evident in GTC simulation results in agreement with predictions from analytical theory. Finally, the fishbone-induced zonal flows are possibly responsible for the formation of an ion-ITB in the DIII-D discharge. Based on the low EP transport and the large zonal flow shearing rates associated with the fishbone instability in gyrokinetic simulations of the ITER scenario, it is conjectured that high performance scenarios could be designed in ITER burning plasmas through fishbone-induced ITBs.","sentences":["Gyrokinetic and kinetic-MHD simulations are performed for the fishbone instability in the DIII-D discharge #178631, chosen for validation of first-principles simulations to predict the energetic particle (EP) transport in an ITER prefusion baseline scenario.","Fishbone modes are found to generate zonal flows, which dominate the fishbone saturation.","The underlying mechanisms of the two-way fishbone-zonal flows nonlinear interplay are discussed in details.","Numerical and analytical analyses identify the fishbone-induced EP redistribution as the dominant generation mechanism for zonal flows.","The zonal flows modify the nonlinear dynamics of phase space zonal structures, which reduces the amount of EPs able to resonate with the mode, leading to an early fishbone saturation.","Simulation results including zonal flows agree quantitatively with DIII-D experimental measurements of the fishbone saturation amplitude and EP transport, supporting this novel saturation mechanism by self-generated zonal flows.","Moreover, the wave-particle mode-locking mechanism is shown to determine quantitatively the fishbone frequency down-chirping, as evident in GTC simulation results in agreement with predictions from analytical theory.","Finally, the fishbone-induced zonal flows are possibly responsible for the formation of an ion-ITB in the DIII-D discharge.","Based on the low EP transport and the large zonal flow shearing rates associated with the fishbone instability in gyrokinetic simulations of the ITER scenario, it is conjectured that high performance scenarios could be designed in ITER burning plasmas through fishbone-induced ITBs."],"url":"http://arxiv.org/abs/2402.03797v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 08:29:39","title":"Face Detection: Present State and Research Directions","abstract":"The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.","sentences":["The majority of computer vision applications that handle images featuring humans use face detection as a core component.","Face detection still has issues, despite much research on the topic.","Face detection's accuracy and speed might yet be increased.","This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled.","The paper provides research directions that can be taken up as research projects in the field of face detection."],"url":"http://arxiv.org/abs/2402.03796v1","category":"cs.CV"}
{"created":"2024-02-06 08:26:00","title":"Quantum Heisenberg Enveloping Algebra","abstract":"In this article, the two-parameter quantum Heisenberg enveloping algebra, which serves as a model for certain quantum generalized Heisenberg algebras, have been studied at roots of unity. In this context, the quantum Heisenberg enveloping algebra becomes a polynomial identity algebra, and the dimension of simple modules is bounded by its PI degree. The PI degree, center, and complete classification of simple modules up to isomorphism are explicitly presented. We work over a field of arbitrary characteristic, although our results concerning the representations require that it is algebraically closed.","sentences":["In this article, the two-parameter quantum Heisenberg enveloping algebra, which serves as a model for certain quantum generalized Heisenberg algebras, have been studied at roots of unity.","In this context, the quantum Heisenberg enveloping algebra becomes a polynomial identity algebra, and the dimension of simple modules is bounded by its PI degree.","The PI degree, center, and complete classification of simple modules up to isomorphism are explicitly presented.","We work over a field of arbitrary characteristic, although our results concerning the representations require that it is algebraically closed."],"url":"http://arxiv.org/abs/2402.03793v1","category":"math.RT"}
{"created":"2024-02-06 08:18:14","title":"No-Regret Reinforcement Learning in Smooth MDPs","abstract":"Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \\textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \\textsc{Legendre-LSVI}, runs in polynomial time, although for a smaller class of problems. After analyzing their regret properties, we compare our results with state-of-the-art ones from RL theory, showing that our algorithms achieve the best guarantees.","sentences":["Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field.","Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved.","In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs).","To face this challenging scenario, we propose two algorithms for regret minimization in $\\nu-$smooth MDPs.","Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials.","The first algorithm, \\textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \\textsc{Legendre-LSVI}, runs in polynomial time, although for a smaller class of problems.","After analyzing their regret properties, we compare our results with state-of-the-art ones from RL theory, showing that our algorithms achieve the best guarantees."],"url":"http://arxiv.org/abs/2402.03792v1","category":"cs.LG"}
{"created":"2024-02-06 08:10:02","title":"Scalable Parallel Algorithm for Graph Neural Network Interatomic Potentials in Molecular Dynamics Simulations","abstract":"Message-passing graph neural network interatomic potentials (GNN-IPs), particularly those with equivariant representations such as NequIP, are attracting significant attention due to their data efficiency and high accuracy. However, parallelizing GNN-IPs poses challenges because multiple message-passing layers complicate data communication within the spatial decomposition method, which is preferred by many molecular dynamics (MD) packages. In this article, we propose an efficient parallelization scheme compatible with GNN-IPs and develop a package, SevenNet (Scalable EquiVariance-Enabled Neural NETwork), based on the NequIP architecture. For MD simulations, SevenNet interfaces with the LAMMPS package. Through benchmark tests on a 32-GPU cluster with examples of SiO$_2$, SevenNet achieves over 80% parallel efficiency in weak-scaling scenarios and exhibits nearly ideal strong-scaling performance as long as GPUs are fully utilized. However, the strong-scaling performance significantly declines with suboptimal GPU utilization, particularly affecting parallel efficiency in cases involving lightweight models or simulations with small numbers of atoms. We also pre-train SevenNet with a vast dataset from the Materials Project (dubbed `SevenNet-0') and assess its performance on generating amorphous Si$_3$N$_4$ containing more than 100,000 atoms. By developing scalable GNN-IPs, this work aims to bridge the gap between advanced machine learning models and large-scale MD simulations, offering researchers a powerful tool to explore complex material systems with high accuracy and efficiency.","sentences":["Message-passing graph neural network interatomic potentials (GNN-IPs), particularly those with equivariant representations such as NequIP, are attracting significant attention due to their data efficiency and high accuracy.","However, parallelizing GNN-IPs poses challenges because multiple message-passing layers complicate data communication within the spatial decomposition method, which is preferred by many molecular dynamics (MD) packages.","In this article, we propose an efficient parallelization scheme compatible with GNN-IPs and develop a package, SevenNet (Scalable EquiVariance-Enabled Neural NETwork), based on the NequIP architecture.","For MD simulations, SevenNet interfaces with the LAMMPS package.","Through benchmark tests on a 32-GPU cluster with examples of SiO$_2$, SevenNet achieves over 80% parallel efficiency in weak-scaling scenarios and exhibits nearly ideal strong-scaling performance as long as GPUs are fully utilized.","However, the strong-scaling performance significantly declines with suboptimal GPU utilization, particularly affecting parallel efficiency in cases involving lightweight models or simulations with small numbers of atoms.","We also pre-train SevenNet with a vast dataset from the Materials Project (dubbed `SevenNet-0') and assess its performance on generating amorphous Si$_3$N$_4$ containing more than 100,000 atoms.","By developing scalable GNN-IPs, this work aims to bridge the gap between advanced machine learning models and large-scale MD simulations, offering researchers a powerful tool to explore complex material systems with high accuracy and efficiency."],"url":"http://arxiv.org/abs/2402.03789v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 08:08:34","title":"Symmetry reductions of a generalized Kuramoto-Sivashinsky equation via equivalence transformations","abstract":"In this paper we consider a generalized Kuramoto-Sivashinsky equation. The equivalence group of the class under consideration has been constructed. This group allows us to perform a comprehensive study and a clear and concise formulation of the results. We have constructed the optimal system of subalgebras of the projections of the equivalence algebra on the space formed by the dependent variable and the arbitrary functions. By using this optimal system, all nonequivalent equations admitting an extension by one of the principal Lie algebra of the class under consideration can be determined. Taking into account the additional symmetries obtained we reduce some partial differential equations belonging to the class into ordinary differential equations. We derive some exact solutions of these equations.","sentences":["In this paper we consider a generalized Kuramoto-Sivashinsky equation.","The equivalence group of the class under consideration has been constructed.","This group allows us to perform a comprehensive study and a clear and concise formulation of the results.","We have constructed the optimal system of subalgebras of the projections of the equivalence algebra on the space formed by the dependent variable and the arbitrary functions.","By using this optimal system, all nonequivalent equations admitting an extension by one of the principal Lie algebra of the class under consideration can be determined.","Taking into account the additional symmetries obtained we reduce some partial differential equations belonging to the class into ordinary differential equations.","We derive some exact solutions of these equations."],"url":"http://arxiv.org/abs/2402.03788v1","category":"math.AP"}
{"created":"2024-02-06 18:49:39","title":"Novel IMU-based Adaptive Estimator of the Center of Rotation of Joints for Movement Analysis","abstract":"The location of the center of rotation (COR) of joints is a key parameter in multiple applications of human motion analysis. The aim of this work was to propose a novel real-time estimator of the center of fixed joints using an inertial measurement unit (IMU). Since the distance to this center commonly varies during the joint motion due to soft tissue artifacts (STA), our approach is aimed at adapting to these small variations when the COR is fixed. Our proposal, called ArVEd, to the best of our knowledge, is the first real-time estimator of the IMU-joint center vector based on one IMU. Previous works are off-line and require a complete measurement batch to be solved and most of them are not tested on the real scenario. The algorithm is based on an Extended Kalman Filter (EKF) that provides an adaptive vector to STA motion variations at each time instant, without requiring a pre-processing stage to reduce the level of noise. ArVEd has been tested through different experiments, including synthetic and real data. The synthetic data are obtained from a simulated spherical pendulum whose COR is fixed, considering both a constant and a variable IMU-joint vector, that simulates translational IMU motions due to STA. The results prove that ArVEd is adapted to obtain a vector per sample with an accuracy of 6.8$\\pm$3.9 on the synthetic data, that means an error lower than 3.5% of the simulated IMU-joint vector. Its accuracy is also tested on the real scenario estimating the COR of the hip of 5 volunteers using as reference the results from an optical system. In this case, ArVEd gets an average error of 9.5% of the real vector value. In all the experiments, ArVEd outperforms the published results of the reference algorithms.","sentences":["The location of the center of rotation (COR) of joints is a key parameter in multiple applications of human motion analysis.","The aim of this work was to propose a novel real-time estimator of the center of fixed joints using an inertial measurement unit (IMU).","Since the distance to this center commonly varies during the joint motion due to soft tissue artifacts (STA), our approach is aimed at adapting to these small variations when the COR is fixed.","Our proposal, called ArVEd, to the best of our knowledge, is the first real-time estimator of the IMU-joint center vector based on one IMU.","Previous works are off-line and require a complete measurement batch to be solved and most of them are not tested on the real scenario.","The algorithm is based on an Extended Kalman Filter (EKF) that provides an adaptive vector to STA motion variations at each time instant, without requiring a pre-processing stage to reduce the level of noise.","ArVEd has been tested through different experiments, including synthetic and real data.","The synthetic data are obtained from a simulated spherical pendulum whose COR is fixed, considering both a constant and a variable IMU-joint vector, that simulates translational IMU motions due to STA.","The results prove that ArVEd is adapted to obtain a vector per sample with an accuracy of 6.8$\\pm$3.9 on the synthetic data, that means an error lower than 3.5% of the simulated IMU-joint vector.","Its accuracy is also tested on the real scenario estimating the COR of the hip of 5 volunteers using as reference the results from an optical system.","In this case, ArVEd gets an average error of 9.5% of the real vector value.","In all the experiments, ArVEd outperforms the published results of the reference algorithms."],"url":"http://arxiv.org/abs/2402.04240v1","category":"eess.SP"}
{"created":"2024-02-06 17:43:27","title":"Incivility in Open Source Projects: A Comprehensive Annotated Dataset of Locked GitHub Issue Threads","abstract":"In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations. This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects. We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility. We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset. The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively. This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior.","sentences":["In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations.","This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects.","We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility.","We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset.","The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively.","This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior."],"url":"http://arxiv.org/abs/2402.04183v1","category":"cs.SE"}
{"created":"2024-02-06 15:16:27","title":"Hermitian stochastic methodology for X-ray superfluorescence","abstract":"A recently introduced theoretical framework for modeling the dynamics of X-ray amplified spontaneous emission is based on stochastic sampling of the density matrix of quantum emitters and the radiation field, similarly to other phase-space sampling techniques. While based on first principles and providing valuable theoretical insights, the original stochastic differential equations exhibit divergences and numerical instabilities. Here, we resolve this issue by accounting the stochastic components perturbatively. The refined formalism accurately reproduces the properties of spontaneous emission and proves universally applicable for describing all stages of collective X-ray emission in paraxial geometry, including spontaneous emission, amplified spontaneous emission, and the non-linear regime. Through numerical examples, we analyze key features of superfluorescence in one-dimensional approximation. Importantly, single realizations of the underlying stochastic equations can be fully interpreted as individual experimental observations of superfluorescence.","sentences":["A recently introduced theoretical framework for modeling the dynamics of X-ray amplified spontaneous emission is based on stochastic sampling of the density matrix of quantum emitters and the radiation field, similarly to other phase-space sampling techniques.","While based on first principles and providing valuable theoretical insights, the original stochastic differential equations exhibit divergences and numerical instabilities.","Here, we resolve this issue by accounting the stochastic components perturbatively.","The refined formalism accurately reproduces the properties of spontaneous emission and proves universally applicable for describing all stages of collective X-ray emission in paraxial geometry, including spontaneous emission, amplified spontaneous emission, and the non-linear regime.","Through numerical examples, we analyze key features of superfluorescence in one-dimensional approximation.","Importantly, single realizations of the underlying stochastic equations can be fully interpreted as individual experimental observations of superfluorescence."],"url":"http://arxiv.org/abs/2402.04069v1","category":"physics.optics"}
{"created":"2024-02-06 14:43:31","title":"Mission Planning and Safety Assessment for Pipeline Inspection Using Autonomous Underwater Vehicles: A Framework based on Behavior Trees","abstract":"The recent advance in autonomous underwater robotics facilitates autonomous inspection tasks of offshore infrastructure. However, current inspection missions rely on predefined plans created offline, hampering the flexibility and autonomy of the inspection vehicle and the mission's success in case of unexpected events. In this work, we address these challenges by proposing a framework encompassing the modeling and verification of mission plans through Behavior Trees (BTs). This framework leverages the modularity of BTs to model onboard reactive behaviors, thus enabling autonomous plan executions, and uses BehaVerify to verify the mission's safety. Moreover, as a use case of this framework, we present a novel AI-enabled algorithm that aims for efficient, autonomous pipeline camera data collection. In a simulated environment, we demonstrate the framework's application to our proposed pipeline inspection algorithm. Our framework marks a significant step forward in the field of autonomous underwater robotics, promising to enhance the safety and success of underwater missions in practical, real-world applications. https://github.com/remaro-network/pipe_inspection_mission","sentences":["The recent advance in autonomous underwater robotics facilitates autonomous inspection tasks of offshore infrastructure.","However, current inspection missions rely on predefined plans created offline, hampering the flexibility and autonomy of the inspection vehicle and the mission's success in case of unexpected events.","In this work, we address these challenges by proposing a framework encompassing the modeling and verification of mission plans through Behavior Trees (BTs).","This framework leverages the modularity of BTs to model onboard reactive behaviors, thus enabling autonomous plan executions, and uses BehaVerify to verify the mission's safety.","Moreover, as a use case of this framework, we present a novel AI-enabled algorithm that aims for efficient, autonomous pipeline camera data collection.","In a simulated environment, we demonstrate the framework's application to our proposed pipeline inspection algorithm.","Our framework marks a significant step forward in the field of autonomous underwater robotics, promising to enhance the safety and success of underwater missions in practical, real-world applications.","https://github.com/remaro-network/pipe_inspection_mission"],"url":"http://arxiv.org/abs/2402.04045v1","category":"cs.RO"}
{"created":"2024-02-06 13:24:36","title":"Tail-Erasure-Correcting Codes","abstract":"The increasing demand for data storage has prompted the exploration of new techniques, with molecular data storage being a promising alternative. In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays. Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows. We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios. We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions.","sentences":["The increasing demand for data storage has prompted the exploration of new techniques, with molecular data storage being a promising alternative.","In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays.","Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows.","We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios.","We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions."],"url":"http://arxiv.org/abs/2402.03987v1","category":"cs.IT"}
{"created":"2024-02-06 12:55:25","title":"Almost Perfect Mutually Unbiased Bases that are Sparse","abstract":"In dimension $d$, Mutually Unbiased Bases (MUBs) are a collection of orthonormal bases over $\\mathbb{C}^d$ such that for any two vectors $v_1, v_2$ belonging to different bases, the dot or scalar product $|\\braket{v_1|v_2}| = \\frac{1}{\\sqrt{d}}$. The upper bound on the number of such bases is $d+1$. Construction methods to achieve this bound are known for cases when $d$ is some power of prime. The situation is more restrictive in other cases and also when we consider the results over real rather than complex. Thus, certain relaxations of this model are considered in literature and consequently Approximate MUBs (AMUB) are studied. This enables one to construct potentially large number of such objects for $\\mathbb{C}^d$ as well as in $\\mathbb{R}^d$. In this regard, we propose the concept of Almost Perfect MUBs (APMUB), where we restrict the absolute value of inner product $|\\braket{v_1|v_2}|$ to be two-valued, one being 0 and the other $ \\leq \\frac{1+\\mathcal{O}(d^{-\\lambda})}{\\sqrt{d}}$, such that $\\lambda > 0$ and the numerator $1 + \\mathcal{O}(d^{-\\lambda}) \\leq 2$. Each such vector constructed, has an important feature that large number of its components are zero and the non-zero components are of equal magnitude. Our techniques are based on combinatorial structures related to Resolvable Block Designs (RBDs). We show that for several composite dimensions $d$, one can construct $\\mathcal{O}(\\sqrt{d})$ many APMUBs, in which cases the number of MUBs are significantly small. To be specific, this result works for $d$ of the form $(q-e)(q+f), \\ q, e, f \\in \\mathbb{N}$, with the conditions $0 \\leq f \\leq e$ for constant $e, f$ and $q$ some power of prime. We also show that such APMUBs provide sets of Bi-angular vectors which are of the order of $\\mathcal{O}(d^{3/2})$ in numbers, having high angular distances among them.","sentences":["In dimension $d$, Mutually Unbiased Bases (MUBs) are a collection of orthonormal bases over $\\mathbb{C}^d$ such that for any two vectors $v_1, v_2$ belonging to different bases, the dot or scalar product $|\\braket{v_1|v_2}| = \\frac{1}{\\sqrt{d}}$. The upper bound on the number of such bases is $d+1$. Construction methods to achieve this bound are known for cases when $d$ is some power of prime.","The situation is more restrictive in other cases and also when we consider the results over real rather than complex.","Thus, certain relaxations of this model are considered in literature and consequently Approximate MUBs (AMUB) are studied.","This enables one to construct potentially large number of such objects for $\\mathbb{C}^d$ as well as in $\\mathbb{R}^d$.","In this regard, we propose the concept of Almost Perfect MUBs (APMUB), where we restrict the absolute value of inner product $|\\braket{v_1|v_2}|$ to be two-valued, one being 0 and the other $ \\leq \\frac{1+\\mathcal{O}(d^{-\\lambda})}{\\sqrt{d}}$, such that $\\lambda > 0$ and the numerator $1 + \\mathcal{O}(d^{-\\lambda})","\\leq 2$.","Each such vector constructed, has an important feature that large number of its components are zero and the non-zero components are of equal magnitude.","Our techniques are based on combinatorial structures related to Resolvable Block Designs (RBDs).","We show that for several composite dimensions $d$, one can construct $\\mathcal{O}(\\sqrt{d})$ many APMUBs, in which cases the number of MUBs are significantly small.","To be specific, this result works for $d$ of the form $(q-e)(q+f), \\ q, e, f \\in \\mathbb{N}$, with the conditions $0","\\leq f \\leq e$ for constant $e, f$ and $q$ some power of prime.","We also show that such APMUBs provide sets of Bi-angular vectors which are of the order of $\\mathcal{O}(d^{3/2})$ in numbers, having high angular distances among them."],"url":"http://arxiv.org/abs/2402.03964v1","category":"cs.DM"}
{"created":"2024-02-06 12:19:46","title":"Using metaheuristics for the location of bicycle stations","abstract":"In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles. To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization. The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station). We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages. We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically. We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application. We have compared our results with the implemented solution in Malaga. Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations.","sentences":["In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles.","To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization.","The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station).","We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages.","We use irace to parameterize each algorithm automatically, to contribute with a methodology to fine-tune algorithms automatically.","We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application.","We have compared our results with the implemented solution in Malaga.","Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations."],"url":"http://arxiv.org/abs/2402.03945v1","category":"cs.NE"}
{"created":"2024-02-06 11:31:04","title":"Learning Metrics that Maximise Power for Accelerated A/B-Tests","abstract":"Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.","sentences":["Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies.","A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior.","North Star metrics are typically delayed and insensitive.","As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   ","We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star.","We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments.","We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.","Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star.","Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation."],"url":"http://arxiv.org/abs/2402.03915v1","category":"cs.LG"}
{"created":"2024-02-06 11:10:32","title":"Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis","abstract":"Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow. However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control. In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances. Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique. Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs. Simulations validate the effectiveness of the proposed method in mitigating traffic waves with better robustness.","sentences":["Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow.","However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control.","In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances.","Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique.","Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs.","Simulations validate the effectiveness of the proposed method in mitigating traffic waves with better robustness."],"url":"http://arxiv.org/abs/2402.03897v1","category":"cs.SY"}
{"created":"2024-02-06 10:55:17","title":"The Emergence of Cooperation in the well-mixed Prisoner's Dilemma: Memory Couples Individual and Group Strategies","abstract":"Exploration of mechanisms underlying the emergence of collective cooperation remains a focal point in field of evolution of cooperation. Prevailing studies often neglect historical information, relying on the latest rewards as the primary criterion for individual decision-making-a method incongruent with human cognition and decision-making modes. This limitation impedes a comprehensive understanding of the spontaneous emergence of cooperation. Integrating memory factors into evolutionary game models to formulate decision criteria with delayed effects has shown potential in unraveling cooperation mechanisms. However, this comes at the significant cost of heightened computational complexity. In this paper, we propose an experiential decision-making method based on reinforcement learning. Utilizing this method, we construct a multi-agent system to engage in the evolutionary Prisoner's Dilemma game. Simulation results indicate that memory establishes a coupling relationship between individual and group strategies, fostering periodic oscillation between cooperation and defection in a well-mixed group. Specifically, defection loses its payoff advantage over cooperation as the group cooperation rate decreases. Conversely, the cooperative behavior gains reinforcement with an increase in the group cooperation rate, overcoming defection as the dominant strategy for individuals. This coupling between individual and group strategies fundamentally bridges the gap between individual and group interests, integrating a multitude of known factors and elucidating the fundamental mechanism of cooperation emergence in the face of social dilemmas.","sentences":["Exploration of mechanisms underlying the emergence of collective cooperation remains a focal point in field of evolution of cooperation.","Prevailing studies often neglect historical information, relying on the latest rewards as the primary criterion for individual decision-making-a method incongruent with human cognition and decision-making modes.","This limitation impedes a comprehensive understanding of the spontaneous emergence of cooperation.","Integrating memory factors into evolutionary game models to formulate decision criteria with delayed effects has shown potential in unraveling cooperation mechanisms.","However, this comes at the significant cost of heightened computational complexity.","In this paper, we propose an experiential decision-making method based on reinforcement learning.","Utilizing this method, we construct a multi-agent system to engage in the evolutionary Prisoner's Dilemma game.","Simulation results indicate that memory establishes a coupling relationship between individual and group strategies, fostering periodic oscillation between cooperation and defection in a well-mixed group.","Specifically, defection loses its payoff advantage over cooperation as the group cooperation rate decreases.","Conversely, the cooperative behavior gains reinforcement with an increase in the group cooperation rate, overcoming defection as the dominant strategy for individuals.","This coupling between individual and group strategies fundamentally bridges the gap between individual and group interests, integrating a multitude of known factors and elucidating the fundamental mechanism of cooperation emergence in the face of social dilemmas."],"url":"http://arxiv.org/abs/2402.03890v1","category":"physics.soc-ph"}
{"created":"2024-02-06 09:17:32","title":"Precise Measurement of Born Cross Sections for $e^+e^-\\to D\\bar{D}$ and Observation of One Structure between $\\sqrt{s} = 3.80-4.95$ GeV","abstract":"Using data samples collected with the BESIII detector at the BEPCII collider at center-of-mass energies ranging from 3.80 to 4.95 GeV, corresponding to an integrated luminosity of 20 fb$^{-1}$, a measurement of Born cross sections for the $e^+e^-\\to D^{0}\\bar{D}^{0}$ and $D^{+}D^{-}$ processes is presented with unprecedented precision. By performing a simultaneous fit to the dressed cross sections for both processes, one possible new structure around 3.9 GeV/$c^2$ is observed for the first time, in addition to seven known resonances $\\psi(3770)$, $\\psi(4040)$, $\\psi(4160)$, $Y(4230)$, $Y(4360)$, $\\psi(4415)$, and $Y(4660)$. These results offer crucial experimental insights into the nature of hadron production in the open charm region.","sentences":["Using data samples collected with the BESIII detector at the BEPCII collider at center-of-mass energies ranging from 3.80 to 4.95 GeV, corresponding to an integrated luminosity of 20 fb$^{-1}$, a measurement of Born cross sections for the $e^+e^-\\to D^{0}\\bar{D}^{0}$ and $D^{+}D^{-}$ processes is presented with unprecedented precision.","By performing a simultaneous fit to the dressed cross sections for both processes, one possible new structure around 3.9 GeV/$c^2$ is observed for the first time, in addition to seven known resonances $\\psi(3770)$, $\\psi(4040)$, $\\psi(4160)$, $Y(4230)$, $Y(4360)$, $\\psi(4415)$, and $Y(4660)$. These results offer crucial experimental insights into the nature of hadron production in the open charm region."],"url":"http://arxiv.org/abs/2402.03829v1","category":"hep-ex"}
{"created":"2024-02-06 08:53:57","title":"On Erd\u0151s covering systems in global function fields","abstract":"A covering system of the integers is a finite collection of arithmetic progressions whose union is the set of integers. A well-known problem on covering systems is the minimum modulus problem posed by Erd\\H{o}s in 1950, who asked whether the minimum modulus in such systems with distinct moduli is arbitrarily large. This problem was resolved by Hough in 2015, showing that the minimum modulus is at most $10^{16}$. In 2022, Balister, Bollob\\'as, Morris, Sahasrabudhe and Tiba reduced Hough's bound to $616,000$ by developing Hough's method. They call it the distortion method. In this paper, by applying this method, we mainly prove that there does not exist any covering system of multiplicity $s$ in any global function field of genus $g$ over $\\mathbb{F}_q$ for $q\\geq (1.14+0.16g)e^{6.5+0.97g}s^2$. In particular, there is no covering system of $\\mathbb{F}_q[x]$ of distinct moduli for $q\\geq 759$.","sentences":["A covering system of the integers is a finite collection of arithmetic progressions whose union is the set of integers.","A well-known problem on covering systems is the minimum modulus problem posed by Erd\\H{o}s in 1950, who asked whether the minimum modulus in such systems with distinct moduli is arbitrarily large.","This problem was resolved by Hough in 2015, showing that the minimum modulus is at most $10^{16}$. In 2022, Balister, Bollob\\'as, Morris, Sahasrabudhe and Tiba reduced Hough's bound to $616,000$ by developing Hough's method.","They call it the distortion method.","In this paper, by applying this method, we mainly prove that there does not exist any covering system of multiplicity $s$ in any global function field of genus $g$ over $\\mathbb{F}_q$ for $q\\geq (1.14+0.16g)e^{6.5+0.97g}s^2$. In particular, there is no covering system of $\\mathbb{F}_q[x]$ of distinct moduli for $q\\geq 759$."],"url":"http://arxiv.org/abs/2402.03810v1","category":"math.NT"}
{"created":"2024-02-06 07:55:54","title":"AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction","abstract":"Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships within the air quality data. Experiments on two real-world benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art models for different testing scenarios including different lead time (24h, 48h, 72h), sparse data and sudden change prediction, achieving reduction in prediction errors up to 10%. Moreover, a case study further validates that our model captures underlying physical processes of particle movement and generates accurate predictions with real physical meaning.","sentences":["Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions.","Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions.","To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet).","Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks.","Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships within the air quality data.","Experiments on two real-world benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art models for different testing scenarios including different lead time (24h, 48h, 72h), sparse data and sudden change prediction, achieving reduction in prediction errors up to 10%.","Moreover, a case study further validates that our model captures underlying physical processes of particle movement and generates accurate predictions with real physical meaning."],"url":"http://arxiv.org/abs/2402.03784v1","category":"cs.LG"}
{"created":"2024-02-06 07:52:30","title":"Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More","abstract":"Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.","sentences":["Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters.","This paper investigates the potential of SPT for cross-lingual transfer.","Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt.","This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages.","Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance."],"url":"http://arxiv.org/abs/2402.03782v1","category":"cs.CL"}
{"created":"2024-02-06 07:51:56","title":"MolTC: Towards Molecular Relational Modeling In Language Models","abstract":"Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolTC innovatively develops a dynamic parameter-sharing strategy for cross-dataset information exchange, and introduces a Multi-hierarchical CoT principle to refine training paradigm. Our experiments, conducted across twelve varied datasets involving over 4,000,000 molecular pairs, demonstrate the superiority of our method over current GNN and LLM-based baselines. On the top of that, a comprehensive Molecular Interactive Instructions dataset is constructed for the development of biochemical LLM, including our MolTC. Code is available at https://github.com/MangoKiller/MolTC.","sentences":["Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research.","Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL.","Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs.","Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets.","To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs.","For achieving a unified MRL, MolTC innovatively develops a dynamic parameter-sharing strategy for cross-dataset information exchange, and introduces a Multi-hierarchical CoT principle to refine training paradigm.","Our experiments, conducted across twelve varied datasets involving over 4,000,000 molecular pairs, demonstrate the superiority of our method over current GNN and LLM-based baselines.","On the top of that, a comprehensive Molecular Interactive Instructions dataset is constructed for the development of biochemical LLM, including our MolTC.","Code is available at https://github.com/MangoKiller/MolTC."],"url":"http://arxiv.org/abs/2402.03781v1","category":"q-bio.QM"}
{"created":"2024-02-06 07:51:54","title":"Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification","abstract":"This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   Keywords: Propaganda, Fake News, Explainability, AI alignment, Vagueness, Subjectivity, Exaggeration, Stylistic analysis","sentences":["This paper investigates the language of propaganda and its stylistic features.","It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies.","A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels.","The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels.","We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification.","They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   ","Keywords: Propaganda, Fake News, Explainability, AI alignment, Vagueness, Subjectivity, Exaggeration, Stylistic analysis"],"url":"http://arxiv.org/abs/2402.03780v1","category":"cs.CL"}
{"created":"2024-02-06 07:48:22","title":"Improving Automated Code Reviews: Learning from Experience","abstract":"Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.","sentences":["Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments.","This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers.","To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would.","A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus.","However, such techniques did not fully utilise quality reviews amongst the training data.","Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others.","In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique.","Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data.","The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies.","This work sheds light on resource-efficient ways to boost automated code review models."],"url":"http://arxiv.org/abs/2402.03777v1","category":"cs.SE"}
{"created":"2024-02-06 07:43:07","title":"Large Language Models As MOOCs Graders","abstract":"Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics. Our results show that Zero-shot-CoT, when integrated with instructor-provided answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading. However, the History and Philosophy of Astronomy course proves to be more challenging in terms of grading as opposed to other courses. Finally, our study reveals a promising direction for automating grading systems for MOOCs, especially in subjects with well-defined rubrics.","sentences":["Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet.","Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment.","As a result, peer grading, often guided by a straightforward rubric, is the method of choice.","While convenient, peer grading often falls short in terms of reliability and validity.","In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs.","Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy.","To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics.","Our results show that Zero-shot-CoT, when integrated with instructor-provided answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading.","However, the History and Philosophy of Astronomy course proves to be more challenging in terms of grading as opposed to other courses.","Finally, our study reveals a promising direction for automating grading systems for MOOCs, especially in subjects with well-defined rubrics."],"url":"http://arxiv.org/abs/2402.03776v1","category":"cs.CL"}
{"created":"2024-02-06 07:40:53","title":"Learning a Decision Tree Algorithm with Transformers","abstract":"Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.","sentences":["Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data.","Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree.","However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization.","To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification.","Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets.","We then train MetaTree to produce the trees that achieve strong generalization performance.","This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance."],"url":"http://arxiv.org/abs/2402.03774v1","category":"cs.LG"}
{"created":"2024-02-06 07:32:02","title":"Fundamental Limits of Two-Hop MIMO Channels: An Asymptotic Approach","abstract":"Multi-antenna relays and intelligent reflecting surfaces (IRSs) have been utilized to construct favorable channels to improve the performance of wireless systems. A common feature between relay systems and IRS-aided systems is the two-hop multiple-input multiple-output (MIMO) channel. As a result, the mutual information (MI) of two-hop MIMO channels has been widely investigated with very engaging results. However, a rigorous investigation on the fundamental limits of two-hop MIMO channels, i.e., the first and second-order analysis, is not yet available in the literature, due to the difficulties caused by the two-hop (product) channel and the noise introduced by the relay (active IRS). In this paper, we employ large-scale random matrix theory (RMT), specifically Gaussian tools, to derive the closed-form deterministic approximation for the mean and variance of the MI. Additionally, we determine the convergence rate for the mean, variance and the characteristic function of the MI, and prove the asymptotic Gaussianity. Furthermore, we also investigate the analytical properties of the fundamental equations that describe the closed-form approximation and prove the existence and uniqueness of the solution. An iterative algorithm is then proposed to obtain the solution for the fundamental equations. Numerical results validate the accuracy of the theoretical analysis.","sentences":["Multi-antenna relays and intelligent reflecting surfaces (IRSs) have been utilized to construct favorable channels to improve the performance of wireless systems.","A common feature between relay systems and IRS-aided systems is the two-hop multiple-input multiple-output (MIMO) channel.","As a result, the mutual information (MI) of two-hop MIMO channels has been widely investigated with very engaging results.","However, a rigorous investigation on the fundamental limits of two-hop MIMO channels, i.e., the first and second-order analysis, is not yet available in the literature, due to the difficulties caused by the two-hop (product) channel and the noise introduced by the relay (active IRS).","In this paper, we employ large-scale random matrix theory (RMT), specifically Gaussian tools, to derive the closed-form deterministic approximation for the mean and variance of the MI.","Additionally, we determine the convergence rate for the mean, variance and the characteristic function of the MI, and prove the asymptotic Gaussianity.","Furthermore, we also investigate the analytical properties of the fundamental equations that describe the closed-form approximation and prove the existence and uniqueness of the solution.","An iterative algorithm is then proposed to obtain the solution for the fundamental equations.","Numerical results validate the accuracy of the theoretical analysis."],"url":"http://arxiv.org/abs/2402.03772v1","category":"cs.IT"}
{"created":"2024-02-06 07:25:21","title":"Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes","abstract":"In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the model utility) subject to the budget for communication. We further demonstrate that Fed-CVLC is indeed a general compression design that bridges quantization and sparsification, with greater flexibility. Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving model utility by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%.","sentences":["In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients.","FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck.","Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates.","In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL.","We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates.","We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the model utility) subject to the budget for communication.","We further demonstrate that Fed-CVLC is indeed a general compression design that bridges quantization and sparsification, with greater flexibility.","Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving model utility by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%."],"url":"http://arxiv.org/abs/2402.03770v1","category":"cs.LG"}
{"created":"2024-02-06 07:16:36","title":"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model","abstract":"We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .","sentences":["We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance.","Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale.","Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale.","Our models will be released at https://github.com/Meituan-AutoML/MobileVLM ."],"url":"http://arxiv.org/abs/2402.03766v1","category":"cs.CV"}
{"created":"2024-02-06 06:47:14","title":"QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model","abstract":"Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts.","sentences":["Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.","However, tailoring these agents for specialized domains like quantitative investment remains a formidable task.","The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process.","This paper introduces a principled framework to address this challenge, comprising a two-layer loop.","In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.","We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.","Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent.","Empirical results showcase QuantAgent's capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts."],"url":"http://arxiv.org/abs/2402.03755v1","category":"cs.AI"}
{"created":"2024-02-06 06:42:51","title":"Enhanced sampling of robust molecular datasets with uncertainty-based collective variables","abstract":"Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP). However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge. Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations. In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain. This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations. The effectiveness of our approach in overcoming energy barriers and exploring unseen energy minima, thereby enhancing the data set in an active learning framework, is demonstrated on the alanine dipeptide benchmark system.","sentences":["Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP).","However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge.","Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations.","In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain.","This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations.","The effectiveness of our approach in overcoming energy barriers and exploring unseen energy minima, thereby enhancing the data set in an active learning framework, is demonstrated on the alanine dipeptide benchmark system."],"url":"http://arxiv.org/abs/2402.03753v1","category":"cs.LG"}
{"created":"2024-02-06 06:37:43","title":"Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach","abstract":"With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) framework to learn node profiles on a mobility network DT model. Extensive experiments have been conducted upon three real-world datasets. Experimental results demonstrate the effectiveness of DTMP.","sentences":["With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system.","Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications.","However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges.","Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour.","In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions.","We propose a digital twin mobility profiling (DTMP) framework to learn node profiles on a mobility network DT model.","Extensive experiments have been conducted upon three real-world datasets.","Experimental results demonstrate the effectiveness of DTMP."],"url":"http://arxiv.org/abs/2402.03750v1","category":"cs.LG"}
{"created":"2024-02-06 06:18:16","title":"SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems","abstract":"Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests the sharing of transitions among subpolicies to improve the exploitative ability of attackers. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.","sentences":["Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement.","However, potential security threats during the MARL deployment need more attention and thorough investigation.","Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks.","For example, reducing the winning rate of a superhuman-level Go AI to around 20%.","They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   ","In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments.","Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests the sharing of transitions among subpolicies to improve the exploitative ability of attackers.","Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations.","Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks.","Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments."],"url":"http://arxiv.org/abs/2402.03741v1","category":"cs.LG"}
{"created":"2024-02-06 06:12:03","title":"AoSRNet: All-in-One Scene Recovery Networks via Multi-knowledge Integration","abstract":"Scattering and attenuation of light in no-homogeneous imaging media or inconsistent light intensity will cause insufficient contrast and color distortion in the collected images, which limits the developments such as vision-driven smart urban, autonomous vehicles, and intelligent robots. In this paper, we propose an all-in-one scene recovery network via multi-knowledge integration (termed AoSRNet) to improve the visibility of imaging devices in typical low-visibility imaging scenes (e.g., haze, sand dust, and low light). It combines gamma correction (GC) and optimized linear stretching (OLS) to create the detail enhancement module (DEM) and color restoration module (CRM). Additionally, we suggest a multi-receptive field extraction module (MEM) to attenuate the loss of image texture details caused by GC nonlinear and OLS linear transformations. Finally, we refine the coarse features generated by DEM, CRM, and MEM through Encoder-Decoder to generate the final restored image. Comprehensive experimental results demonstrate the effectiveness and stability of AoSRNet compared to other state-of-the-art methods. The source code is available at \\url{https://github.com/LouisYuxuLu/AoSRNet}.","sentences":["Scattering and attenuation of light in no-homogeneous imaging media or inconsistent light intensity will cause insufficient contrast and color distortion in the collected images, which limits the developments such as vision-driven smart urban, autonomous vehicles, and intelligent robots.","In this paper, we propose an all-in-one scene recovery network via multi-knowledge integration (termed AoSRNet) to improve the visibility of imaging devices in typical low-visibility imaging scenes (e.g., haze, sand dust, and low light).","It combines gamma correction (GC) and optimized linear stretching (OLS) to create the detail enhancement module (DEM) and color restoration module (CRM).","Additionally, we suggest a multi-receptive field extraction module (MEM) to attenuate the loss of image texture details caused by GC nonlinear and OLS linear transformations.","Finally, we refine the coarse features generated by DEM, CRM, and MEM through Encoder-Decoder to generate the final restored image.","Comprehensive experimental results demonstrate the effectiveness and stability of AoSRNet compared to other state-of-the-art methods.","The source code is available at \\url{https://github.com/LouisYuxuLu/AoSRNet}."],"url":"http://arxiv.org/abs/2402.03738v1","category":"cs.CV"}
{"created":"2024-02-06 05:58:15","title":"Deep Outdated Fact Detection in Knowledge Graphs","abstract":"Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.","sentences":["Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains.","However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves.","Existing solutions for outdated fact detection often rely on manual recognition.","In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs.","DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations.","To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities.","Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2402.03732v1","category":"cs.AI"}
{"created":"2024-02-06 05:50:04","title":"Consistent Joint Decision-Making with Heterogeneous Learning Models","abstract":"This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.","sentences":["This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge.","Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy.","Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets."],"url":"http://arxiv.org/abs/2402.03728v1","category":"cs.AI"}
{"created":"2024-02-06 05:49:24","title":"A Blowup Solution of Multispeed Klein-Gordon System in Space Dimension Two with Small Initial Data","abstract":"We find an example to illustrate that the first nondegeneracy condition of (1.2) is actually needed in proving the global exsitence of 2D multispeed Klein-Gordon system with small initial data (See [3]). We construct a collection of special Klein-Gordon dispersive relations and by iterating the corresponding profiles we can find a blowup in finite time.","sentences":["We find an example to illustrate that the first nondegeneracy condition of (1.2) is actually needed in proving the global exsitence of 2D multispeed Klein-Gordon system with small initial data (See [3]).","We construct a collection of special Klein-Gordon dispersive relations and by iterating the corresponding profiles we can find a blowup in finite time."],"url":"http://arxiv.org/abs/2402.03727v1","category":"math.AP"}
{"created":"2024-02-06 05:29:05","title":"Similarity-based Neighbor Selection for Graph LLMs","abstract":"Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning practices, demonstrate that SNS, through simple prompt interactions with LLMs, consistently outperforms vanilla GNNs and achieves state-of-the-art results on datasets like PubMed in node classification, showcasing LLMs' potential in graph structure understanding. Our research further underscores the significance of graph structure integration in LLM applications and identifies key factors for their success in node classification. Code is available at https://github.com/ruili33/SNS.","sentences":["Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs.","Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs.","To address these challenges, we introduce Similarity-based Neighbor Selection (SNS).","Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily.","Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods.","Our comprehensive experiments, adhering to standard dataset partitioning practices, demonstrate that SNS, through simple prompt interactions with LLMs, consistently outperforms vanilla GNNs and achieves state-of-the-art results on datasets like PubMed in node classification, showcasing LLMs' potential in graph structure understanding.","Our research further underscores the significance of graph structure integration in LLM applications and identifies key factors for their success in node classification.","Code is available at https://github.com/ruili33/SNS."],"url":"http://arxiv.org/abs/2402.03720v1","category":"cs.LG"}
{"created":"2024-02-06 05:24:16","title":"Empowering Language Models with Active Inquiry for Deeper Understanding","abstract":"The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50.9%, outperforming other leading question-answering frameworks. Moreover, in scenarios involving human participants, LaMAI consistently generates responses that are superior or comparable to baseline methods in more than 82% of the cases. The applicability of LaMAI is further evidenced by its successful integration with various LLMs, highlighting its potential for the future of interactive language models.","sentences":["The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language.","However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses.","In natural human interactions, clarification is sought through targeted questioning to uncover obscure information.","Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement.","LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue.","This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations.","Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI.","The method improves answer accuracy from 31.9% to 50.9%, outperforming other leading question-answering frameworks.","Moreover, in scenarios involving human participants, LaMAI consistently generates responses that are superior or comparable to baseline methods in more than 82% of the cases.","The applicability of LaMAI is further evidenced by its successful integration with various LLMs, highlighting its potential for the future of interactive language models."],"url":"http://arxiv.org/abs/2402.03719v1","category":"cs.CL"}
{"created":"2024-02-06 05:11:38","title":"Clarify: Improving Model Robustness With Natural Language Corrections","abstract":"In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data. Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 17.1% in two datasets. Additionally, we use Clarify to find and rectify 31 novel hard subpopulations in the ImageNet dataset, improving minority-split accuracy from 21.1% to 28.7%.","sentences":["In supervised learning, models are trained to extract correlations from a static dataset.","This often leads to models that rely on high-level misconceptions.","To prevent such misconceptions, we must necessarily provide additional information beyond the training data.","Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution.","Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data.","We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision.","We introduce Clarify, a novel interface and method for interactively correcting model misconceptions.","Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns.","Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data.","Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 17.1% in two datasets.","Additionally, we use Clarify to find and rectify 31 novel hard subpopulations in the ImageNet dataset, improving minority-split accuracy from 21.1% to 28.7%."],"url":"http://arxiv.org/abs/2402.03715v1","category":"cs.LG"}
{"created":"2024-02-06 05:10:00","title":"Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices","abstract":"Wearable sensors have permeated into people's lives, ushering impactful applications in interactive systems and activity recognition. However, practitioners face significant obstacles when dealing with sensing heterogeneities, requiring custom models for different platforms. In this paper, we conduct a comprehensive evaluation of the generalizability of motion models across sensor locations. Our analysis highlights this challenge and identifies key on-body locations for building location-invariant models that can be integrated on any device. For this, we introduce the largest multi-location activity dataset (N=50, 200 cumulative hours), which we make publicly available. We also present deployable on-device motion models reaching 91.41% frame-level F1-score from a single model irrespective of sensor placements. Lastly, we investigate cross-location data synthesis, aiming to alleviate the laborious data collection tasks by synthesizing data in one location given data from another. These contributions advance our vision of low-barrier, location-invariant activity recognition systems, catalyzing research in HCI and ubiquitous computing.","sentences":["Wearable sensors have permeated into people's lives, ushering impactful applications in interactive systems and activity recognition.","However, practitioners face significant obstacles when dealing with sensing heterogeneities, requiring custom models for different platforms.","In this paper, we conduct a comprehensive evaluation of the generalizability of motion models across sensor locations.","Our analysis highlights this challenge and identifies key on-body locations for building location-invariant models that can be integrated on any device.","For this, we introduce the largest multi-location activity dataset (N=50, 200 cumulative hours), which we make publicly available.","We also present deployable on-device motion models reaching 91.41% frame-level F1-score from a single model irrespective of sensor placements.","Lastly, we investigate cross-location data synthesis, aiming to alleviate the laborious data collection tasks by synthesizing data in one location given data from another.","These contributions advance our vision of low-barrier, location-invariant activity recognition systems, catalyzing research in HCI and ubiquitous computing."],"url":"http://arxiv.org/abs/2402.03714v1","category":"cs.HC"}
{"created":"2024-02-06 05:09:25","title":"Measurement of $CP$ asymmetries in $B^0\\to\u03b7'K^0_s$ decays at Belle II","abstract":"We describe a measurement of charge-parity ($CP$) violation asymmetries in $B^0\\to\\eta'K^0_S$ decays using Belle II data. We consider $\\eta'\\to\\eta(\\to\\gamma\\gamma)\\pi^+\\pi^-$ and $\\eta'\\to\\rho(\\to\\pi^+\\pi^-)\\gamma$ decays. The data were collected at the SuperKEKB asymmetric-energy $e^+e^-$ collider between the years 2019 and 2022, and contain $(387\\pm 6) \\times 10^6$ bottom-antibottom meson pairs. We reconstruct $829\\pm35$ signal decays and extract the $CP$ violating parameters from a fit to the distribution of the proper-decay-time difference between the two $B$ mesons. The measured direct and mixing-induced $CP$ asymmetries are $\\text{C}_{\\eta'K^0_S} = -0.19 \\pm 0.08 \\pm 0.03 $ and $\\text{S}_{\\eta'K^0_S} = +0.67 \\pm 0.10 \\pm 0.04 $, respectively, where the first uncertainties are statistical and the second are systematic. These results are in agreement with current world averages and standard model predictions.","sentences":["We describe a measurement of charge-parity ($CP$) violation asymmetries in $B^0\\to\\eta'K^0_S$ decays using Belle II data.","We consider $\\eta'\\to\\eta(\\to\\gamma\\gamma)\\pi^+\\pi^-$ and $\\eta'\\to\\rho(\\to\\pi^+\\pi^-)\\gamma$ decays.","The data were collected at the SuperKEKB asymmetric-energy $e^+e^-$ collider between the years 2019 and 2022, and contain $(387\\pm 6) \\times 10^6$ bottom-antibottom meson pairs.","We reconstruct $829\\pm35$ signal decays and extract the $CP$ violating parameters from a fit to the distribution of the proper-decay-time difference between the two $B$ mesons.","The measured direct and mixing-induced $CP$ asymmetries are $\\text{C}_{\\eta'K^0_S} = -0.19 \\pm 0.08 \\pm 0.03 $ and $\\text{S}_{\\eta'K^0_S} = +0.67 \\pm 0.10 \\pm 0.04 $, respectively, where the first uncertainties are statistical and the second are systematic.","These results are in agreement with current world averages and standard model predictions."],"url":"http://arxiv.org/abs/2402.03713v1","category":"hep-ex"}
{"created":"2024-02-06 05:02:33","title":"SISP: A Benchmark Dataset for Fine-grained Ship Instance Segmentation in Panchromatic Satellite Images","abstract":"Fine-grained ship instance segmentation in satellite images holds considerable significance for monitoring maritime activities at sea. However, existing datasets often suffer from the scarcity of fine-grained information or pixel-wise localization annotations, as well as the insufficient image diversity and variations, thus limiting the research of this task. To this end, we propose a benchmark dataset for fine-grained Ship Instance Segmentation in Panchromatic satellite images, namely SISP, which contains 56,693 well-annotated ship instances with four fine-grained categories across 10,000 sliced images, and all the images are collected from SuperView-1 satellite with the resolution of 0.5m. Targets in the proposed SISP dataset have characteristics that are consistent with real satellite scenes, such as high class imbalance, various scenes, large variations in target densities and scales, and high inter-class similarity and intra-class diversity, all of which make the SISP dataset more suitable for real-world applications. In addition, we introduce a Dynamic Feature Refinement-assist Instance segmentation network, namely DFRInst, as the benchmark method for ship instance segmentation in satellite images, which can fortify the explicit representation of crucial features, thus improving the performance of ship instance segmentation. Experiments and analysis are performed on the proposed SISP dataset to evaluate the benchmark method and several state-of-the-art methods to establish baselines for facilitating future research. The proposed dataset and source codes will be available at: https://github.com/Justlovesmile/SISP.","sentences":["Fine-grained ship instance segmentation in satellite images holds considerable significance for monitoring maritime activities at sea.","However, existing datasets often suffer from the scarcity of fine-grained information or pixel-wise localization annotations, as well as the insufficient image diversity and variations, thus limiting the research of this task.","To this end, we propose a benchmark dataset for fine-grained Ship Instance Segmentation in Panchromatic satellite images, namely SISP, which contains 56,693 well-annotated ship instances with four fine-grained categories across 10,000 sliced images, and all the images are collected from SuperView-1 satellite with the resolution of 0.5m. Targets in the proposed SISP dataset have characteristics that are consistent with real satellite scenes, such as high class imbalance, various scenes, large variations in target densities and scales, and high inter-class similarity and intra-class diversity, all of which make the SISP dataset more suitable for real-world applications.","In addition, we introduce a Dynamic Feature Refinement-assist Instance segmentation network, namely DFRInst, as the benchmark method for ship instance segmentation in satellite images, which can fortify the explicit representation of crucial features, thus improving the performance of ship instance segmentation.","Experiments and analysis are performed on the proposed SISP dataset to evaluate the benchmark method and several state-of-the-art methods to establish baselines for facilitating future research.","The proposed dataset and source codes will be available at: https://github.com/Justlovesmile/SISP."],"url":"http://arxiv.org/abs/2402.03708v1","category":"cs.CV"}
{"created":"2024-02-06 04:57:07","title":"MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats","abstract":"In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD.","sentences":["In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset.","MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation.","MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays.","It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB.","Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets.","Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions.","Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools.","Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds.","This approach enhances the dataset's applicability, capturing the exact challenges faced during proximate vehicular operations.","It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond.","Our dataset, codes, and designs will be available in https://github.com/ntu-aris/MMAUD."],"url":"http://arxiv.org/abs/2402.03706v1","category":"cs.RO"}
{"created":"2024-02-06 04:44:36","title":"On Learning Spatial Provenance in Privacy-Constrained Wireless Networks","abstract":"In Vehicle-to-Everything networks that involve multi-hop communication, the Road Side Units (RSUs) typically aim to collect location information from the participating vehicles to provide security and network diagnostics features. While the vehicles commonly use the Global Positioning System (GPS) for navigation, they may refrain from sharing their precise GPS coordinates with the RSUs due to privacy concerns. Therefore, to jointly address the high localization requirements by the RSUs as well as the vehicles' privacy, we present a novel spatial-provenance framework wherein each vehicle uses Bloom filters to embed their partial location information when forwarding the packets. In this framework, the RSUs and the vehicles agree upon fragmenting the coverage area into several smaller regions so that the vehicles can embed the identity of their regions through Bloom filters. Given the probabilistic nature of Bloom filters, we derive an analytical expression on the error-rates in provenance recovery and then pose an optimization problem to choose the underlying parameters. With the help of extensive simulation results, we show that our method offers near-optimal Bloom filter parameters in learning spatial provenance. Some interesting trade-offs between the communication-overhead, spatial privacy of the vehicles and the error rates in provenance recovery are also discussed.","sentences":["In Vehicle-to-Everything networks that involve multi-hop communication, the Road Side Units (RSUs) typically aim to collect location information from the participating vehicles to provide security and network diagnostics features.","While the vehicles commonly use the Global Positioning System (GPS) for navigation, they may refrain from sharing their precise GPS coordinates with the RSUs due to privacy concerns.","Therefore, to jointly address the high localization requirements by the RSUs as well as the vehicles' privacy, we present a novel spatial-provenance framework wherein each vehicle uses Bloom filters to embed their partial location information when forwarding the packets.","In this framework, the RSUs and the vehicles agree upon fragmenting the coverage area into several smaller regions so that the vehicles can embed the identity of their regions through Bloom filters.","Given the probabilistic nature of Bloom filters, we derive an analytical expression on the error-rates in provenance recovery and then pose an optimization problem to choose the underlying parameters.","With the help of extensive simulation results, we show that our method offers near-optimal Bloom filter parameters in learning spatial provenance.","Some interesting trade-offs between the communication-overhead, spatial privacy of the vehicles and the error rates in provenance recovery are also discussed."],"url":"http://arxiv.org/abs/2402.03702v1","category":"cs.IT"}
{"created":"2024-02-06 04:41:06","title":"GenLens: A Systematic Evaluation of Visual GenAI Model Outputs","abstract":"The rapid development of generative AI (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and fairness. Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development. Current practices often depend on developers' subjective visual assessments, which may lack scalability and generalizability. This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting. Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development. GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration. A user study with model developers reveals that GenLens effectively enhances their workflow, evidenced by high satisfaction rates and a strong intent to integrate it into their practices. This research underscores the importance of robust early-stage evaluation tools in GenAI development, contributing to the advancement of fair and high-quality GenAI models.","sentences":["The rapid development of generative AI (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and fairness.","Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development.","Current practices often depend on developers' subjective visual assessments, which may lack scalability and generalizability.","This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting.","Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development.","GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration.","A user study with model developers reveals that GenLens effectively enhances their workflow, evidenced by high satisfaction rates and a strong intent to integrate it into their practices.","This research underscores the importance of robust early-stage evaluation tools in GenAI development, contributing to the advancement of fair and high-quality GenAI models."],"url":"http://arxiv.org/abs/2402.03700v1","category":"cs.HC"}
{"created":"2024-02-06 04:28:33","title":"ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis","abstract":"Network traffic analysis increasingly uses complex machine learning models as the internet consolidates and traffic gets more encrypted. However, over high-bandwidth networks, flows can easily arrive faster than model inference rates. The temporal nature of network flows limits simple scale-out approaches leveraged in other high-traffic machine learning applications. Accordingly, this paper presents ServeFlow, a solution for machine-learning model serving aimed at network traffic analysis tasks, which carefully selects the number of packets to collect and the models to apply for individual flows to achieve a balance between minimal latency, high service rate, and high accuracy. We identify that on the same task, inference time across models can differ by 2.7x-136.3x, while the median inter-packet waiting time is often 6-8 orders of magnitude higher than the inference time! ServeFlow is able to make inferences on 76.3% flows in under 16ms, which is a speed-up of 40.5x on the median end-to-end serving latency while increasing the service rate and maintaining similar accuracy. Even with thousands of features per flow, it achieves a service rate of over 48.5k new flows per second on a 16-core CPU commodity server, which matches the order of magnitude of flow rates observed on city-level network backbones.","sentences":["Network traffic analysis increasingly uses complex machine learning models as the internet consolidates and traffic gets more encrypted.","However, over high-bandwidth networks, flows can easily arrive faster than model inference rates.","The temporal nature of network flows limits simple scale-out approaches leveraged in other high-traffic machine learning applications.","Accordingly, this paper presents ServeFlow, a solution for machine-learning model serving aimed at network traffic analysis tasks, which carefully selects the number of packets to collect and the models to apply for individual flows to achieve a balance between minimal latency, high service rate, and high accuracy.","We identify that on the same task, inference time across models can differ by 2.7x-136.3x, while the median inter-packet waiting time is often 6-8 orders of magnitude higher than the inference time!","ServeFlow is able to make inferences on 76.3% flows in under 16ms, which is a speed-up of 40.5x on the median end-to-end serving latency while increasing the service rate and maintaining similar accuracy.","Even with thousands of features per flow, it achieves a service rate of over 48.5k new flows per second on a 16-core CPU commodity server, which matches the order of magnitude of flow rates observed on city-level network backbones."],"url":"http://arxiv.org/abs/2402.03694v1","category":"cs.NI"}
{"created":"2024-02-06 04:22:44","title":"A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective","abstract":"Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models. Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats. In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL. We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions. Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures. This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life cycle.","sentences":["Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models.","Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats.","In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL.","We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions.","Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures.","This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life cycle."],"url":"http://arxiv.org/abs/2402.03688v1","category":"cs.CR"}
{"created":"2024-02-06 04:14:09","title":"Minds versus Machines: Rethinking Entailment Verification with Language Models","abstract":"Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application, we showcase the efficacy of our finetuned model in enhancing self-consistency in model-generated explanations, resulting in a 6% performance boost on average across three multiple-choice question-answering datasets.","sentences":["Humans make numerous inferences in text comprehension to understand discourse.","This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs).","Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories.","Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances.","Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning.","Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification.","As a practical application, we showcase the efficacy of our finetuned model in enhancing self-consistency in model-generated explanations, resulting in a 6% performance boost on average across three multiple-choice question-answering datasets."],"url":"http://arxiv.org/abs/2402.03686v1","category":"cs.CL"}
{"created":"2024-02-06 04:06:06","title":"RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback","abstract":"Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions.","sentences":["Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions.","In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs).","The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent.","We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions."],"url":"http://arxiv.org/abs/2402.03681v1","category":"cs.RO"}
{"created":"2024-02-06 04:00:21","title":"Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents","abstract":"Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshold performance on complex sequential decision-making problems compared to state-of-the-art RM and Automaton-guided RL baselines, such as Q-Learning for Reward Machines and Compositional RL from logical Specifications (DIRL). Moreover, we demonstrate that our method outperforms RM and Automaton-guided RL baselines in terms of sample-efficiency, both in a partially observable robotic task and in a continuous control robotic manipulation task.","sentences":["Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors.","However, learning an effective policy often requires a large number of environment interactions.","To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent.","In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions.","Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies.","We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshold performance on complex sequential decision-making problems compared to state-of-the-art RM and Automaton-guided RL baselines, such as Q-Learning for Reward Machines and Compositional RL from logical Specifications (DIRL).","Moreover, we demonstrate that our method outperforms RM and Automaton-guided RL baselines in terms of sample-efficiency, both in a partially observable robotic task and in a continuous control robotic manipulation task."],"url":"http://arxiv.org/abs/2402.03678v1","category":"cs.AI"}
{"created":"2024-02-06 03:57:06","title":"Effective Protein-Protein Interaction Exploration with PPIretrieval","abstract":"Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes.","sentences":["Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense.","As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs.","Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces.","When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes."],"url":"http://arxiv.org/abs/2402.03675v1","category":"q-bio.BM"}
{"created":"2024-02-06 03:41:12","title":"Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning","abstract":"Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs. The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.","sentences":["Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning.","However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR.","Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof.","Specifically, our methodology comprises two steps.","Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs.","Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process.","Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs.","The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods.","Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy."],"url":"http://arxiv.org/abs/2402.03667v1","category":"cs.CL"}
{"created":"2024-02-06 03:33:50","title":"Symbol Correctness in Deep Neural Networks Containing Symbolic Layers","abstract":"To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs faced by NS-DNN training algorithms. In doing so, we both identify significant points of ambiguity in prior work, and provide a framework to support further NS-DNN developments.","sentences":["To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference.","We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data.","We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for).","Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs faced by NS-DNN training algorithms.","In doing so, we both identify significant points of ambiguity in prior work, and provide a framework to support further NS-DNN developments."],"url":"http://arxiv.org/abs/2402.03663v1","category":"cs.LG"}
{"created":"2024-02-06 03:31:28","title":"Transductive Reward Inference on Graph","abstract":"In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in offline reinforcement learning tasks.","sentences":["In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning.","Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics.","Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data.","We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards.","Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data.","Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum.","Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach.","The application of our inferred rewards improves the performance in offline reinforcement learning tasks."],"url":"http://arxiv.org/abs/2402.03661v1","category":"cs.LG"}
{"created":"2024-02-06 03:28:36","title":"Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm","abstract":"The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, particularly by translating operations from the parameter space to the feature space. Furthermore, we delve deeper into the underlying factors for the emergence of CTL, emphasizing the impact of pretraining.","sentences":["The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning.","In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL).","Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer.","Such cross-task linearity has not been noted in peer literature.","We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint.","We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space.","Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, particularly by translating operations from the parameter space to the feature space.","Furthermore, we delve deeper into the underlying factors for the emergence of CTL, emphasizing the impact of pretraining."],"url":"http://arxiv.org/abs/2402.03660v1","category":"cs.LG"}
{"created":"2024-02-06 02:47:16","title":"CAMBranch: Contrastive Learning with Augmented MILPs for Branching","abstract":"Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \\textbf{C}ontrastive Learning with \\textbf{A}ugmented \\textbf{M}ILPs for \\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental results demonstrate that CAMBranch, trained with only 10\\% of the complete dataset, exhibits superior performance. Ablation studies further validate the effectiveness of our method.","sentences":["Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\\&B) branching policies for solving Mixed Integer Linear Programming (MILP).","These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance.","However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor.","To address this challenge, we propose \\textbf{C}ontrastive Learning with \\textbf{A}ugmented \\textbf{M}ILPs for \\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs.","This approach enables the acquisition of a considerable number of labeled expert samples.","CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions.","Experimental results demonstrate that CAMBranch, trained with only 10\\% of the complete dataset, exhibits superior performance.","Ablation studies further validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.03647v1","category":"cs.LG"}
{"created":"2024-02-06 02:33:00","title":"torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability Problem","abstract":"The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms. Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process. In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT). Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation. This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm. Additionally, we leverage the computational power of GPUs to accelerate these computations. Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT solvers, and is on par with another in terms of solution cost, without necessitating any training or access to an underlying SAT solver. Given that numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves the way for a new generation of solvers poised to benefit from neural network GPU acceleration.","sentences":["The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms.","Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process.","In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT).","Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation.","This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm.","Additionally, we leverage the computational power of GPUs to accelerate these computations.","Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT solvers, and is on par with another in terms of solution cost, without necessitating any training or access to an underlying SAT solver.","Given that numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves the way for a new generation of solvers poised to benefit from neural network GPU acceleration."],"url":"http://arxiv.org/abs/2402.03640v1","category":"cs.AI"}
{"created":"2024-02-06 01:59:41","title":"Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context","abstract":"Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information available in IDEs to enhance the capabilities of LLMs of repository-level code completion. We conducted preliminary experiments to validate the performance of IDECoder and observed that this synergy represents a promising trend for future exploration.","sentences":["Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot.","Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files.","However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information.","Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs.","In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion.","We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement.","IDECoder utilizes the rich cross-context information available in IDEs to enhance the capabilities of LLMs of repository-level code completion.","We conducted preliminary experiments to validate the performance of IDECoder and observed that this synergy represents a promising trend for future exploration."],"url":"http://arxiv.org/abs/2402.03630v1","category":"cs.SE"}
{"created":"2024-02-06 01:48:53","title":"Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies","abstract":"The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.","sentences":["The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities.","This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies.","We posit that PAgents can reshape professional services through continuously developed expertise.","Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer.","This paper aims to spur discourse on promising real-world applications of LLMs.","We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence."],"url":"http://arxiv.org/abs/2402.03628v1","category":"cs.CL"}
{"created":"2024-02-06 01:44:38","title":"Partially Recentralization Softmax Loss for Vision-Language Models Robustness","abstract":"As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after this paper is accepted","sentences":["As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular.","However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input.","While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored.","In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs.","Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks.","Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions.","Our code will be available after this paper is accepted"],"url":"http://arxiv.org/abs/2402.03627v1","category":"cs.CL"}
{"created":"2024-02-06 01:15:06","title":"Neural Network Approximators for Marginal MAP in Probabilistic Circuits","abstract":"Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires only linear time to output a solution. We evaluate our new approach on several benchmark datasets and show that it outperforms three competing linear time approximations, max-product inference, max-marginal inference and sequential estimation, which are used in practice to solve MMAP tasks in PCs.","sentences":["Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions.","They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network.","Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models.","Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs.","The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function.","The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires only linear time to output a solution.","We evaluate our new approach on several benchmark datasets and show that it outperforms three competing linear time approximations, max-product inference, max-marginal inference and sequential estimation, which are used in practice to solve MMAP tasks in PCs."],"url":"http://arxiv.org/abs/2402.03621v1","category":"cs.LG"}
{"created":"2024-02-06 01:13:53","title":"Self-Discover: Large Language Models Self-Compose Reasoning Structures","abstract":"We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.","sentences":["We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods.","Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding.","SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT).","Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute.","Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns."],"url":"http://arxiv.org/abs/2402.03620v1","category":"cs.AI"}
{"created":"2024-02-06 01:07:56","title":"Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction","abstract":"Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.","sentences":["Humans extract useful abstractions of the world from noisy sensory data.","Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions.","Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language.","To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa.","We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's.","This suggests human visual and linguistic representations are more dissociable than those of GPT-4."],"url":"http://arxiv.org/abs/2402.03618v1","category":"cs.AI"}
{"created":"2024-02-06 01:05:14","title":"Leveraging Large Language Models for Hybrid Workplace Decision Support","abstract":"Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influenced by the LLM's suggestions and explanations. The participants in our study find the system to be convenient, regardless of whether reasons are provided or not. Our results show that employees can benefit from the LLM-empowered system for their workspace selection in hybrid workplace.","sentences":["Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions.","In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans.","In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience.","In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs.","We first examine LLM's capability of making suitable workspace suggestions.","We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces.","We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system.","We observe that a worker's decision could be influenced by the LLM's suggestions and explanations.","The participants in our study find the system to be convenient, regardless of whether reasons are provided or not.","Our results show that employees can benefit from the LLM-empowered system for their workspace selection in hybrid workplace."],"url":"http://arxiv.org/abs/2402.03616v1","category":"cs.CL"}
{"created":"2024-02-06 00:55:06","title":"Privacy risk in GeoData: A survey","abstract":"With the ubiquitous use of location-based services, large-scale individual-level location data has been widely collected through location-awareness devices. The exposure of location data constitutes a significant privacy risk to users as it can lead to de-anonymisation, the inference of sensitive information, and even physical threats. Geoprivacy concerns arise on the issues of user identity de-anonymisation and location exposure. In this survey, we analyse different geomasking techniques that have been proposed to protect the privacy of individuals in geodata. We present a taxonomy to characterise these techniques along different dimensions, and conduct a survey of geomasking techniques. We then highlight shortcomings of current techniques and discuss avenues for future research.","sentences":["With the ubiquitous use of location-based services, large-scale individual-level location data has been widely collected through location-awareness devices.","The exposure of location data constitutes a significant privacy risk to users as it can lead to de-anonymisation, the inference of sensitive information, and even physical threats.","Geoprivacy concerns arise on the issues of user identity de-anonymisation and location exposure.","In this survey, we analyse different geomasking techniques that have been proposed to protect the privacy of individuals in geodata.","We present a taxonomy to characterise these techniques along different dimensions, and conduct a survey of geomasking techniques.","We then highlight shortcomings of current techniques and discuss avenues for future research."],"url":"http://arxiv.org/abs/2402.03612v1","category":"cs.CR"}
{"created":"2024-02-06 00:53:27","title":"RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents","abstract":"Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.","sentences":["Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration.","However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges.","Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities.","RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks.","Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks.","These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications."],"url":"http://arxiv.org/abs/2402.03610v1","category":"cs.LG"}
{"created":"2024-02-06 00:51:27","title":"Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning","abstract":"The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.","sentences":["The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online.","However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships.","Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues.","In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns.","While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory."],"url":"http://arxiv.org/abs/2402.03607v1","category":"cs.AI"}
{"created":"2024-02-06 00:16:01","title":"A Review on Internet of Things for Defense and Public Safety","abstract":"The Internet of Things (IoT) is undeniably transforming the way that organizations communicate and organize everyday businesses and industrial procedures. Its adoption has proven well suited for sectors that manage a large number of assets and coordinate complex and distributed processes. This survey analyzes the great potential for applying IoT technologies (i.e., data-driven applications or embedded automation and intelligent adaptive systems) to revolutionize modern warfare and provide benefits similar to those in industry. It identifies scenarios where Defense and Public Safety (PS) could leverage better commercial IoT capabilities to deliver greater survivability to the warfighter or first responders, while reducing costs and increasing operation efficiency and effectiveness. This article reviews the main tactical requirements and the architecture, examining gaps and shortcomings in existing IoT systems across the military field and mission-critical scenarios. The review characterizes the open challenges for a broad deployment and presents a research roadmap for enabling an affordable IoT for defense and PS.","sentences":["The Internet of Things (IoT) is undeniably transforming the way that organizations communicate and organize everyday businesses and industrial procedures.","Its adoption has proven well suited for sectors that manage a large number of assets and coordinate complex and distributed processes.","This survey analyzes the great potential for applying IoT technologies (i.e., data-driven applications or embedded automation and intelligent adaptive systems) to revolutionize modern warfare and provide benefits similar to those in industry.","It identifies scenarios where Defense and Public Safety (PS) could leverage better commercial IoT capabilities to deliver greater survivability to the warfighter or first responders, while reducing costs and increasing operation efficiency and effectiveness.","This article reviews the main tactical requirements and the architecture, examining gaps and shortcomings in existing IoT systems across the military field and mission-critical scenarios.","The review characterizes the open challenges for a broad deployment and presents a research roadmap for enabling an affordable IoT for defense and PS."],"url":"http://arxiv.org/abs/2402.03599v1","category":"eess.SY"}
{"created":"2024-02-05 23:55:46","title":"Reverse Engineering and Security Evaluation of Commercial Tags for RFID-Based IoT Applications","abstract":"The Internet of Things (IoT) is a distributed system of physical objects that requires the seamless integration of hardware (e.g., sensors, actuators, electronics) and network communications in order to collect and exchange data. IoT smart objects need to be somehow identified to determine the origin of the data and to automatically detect the elements around us. One of the best positioned technologies to perform identification is RFID (Radio Frequency Identification), which in the last years has gained a lot of popularity in applications like access control, payment cards or logistics. Despite its popularity, RFID security has not been properly handled in numerous applications. To foster security in such applications, this article includes three main contributions. First, in order to establish the basics, a detailed review of the most common flaws found in RFID-based IoT systems is provided, including the latest attacks described in the literature. Second, a novel methodology that eases the detection and mitigation of such flaws is presented. Third, the latest RFID security tools are analyzed and the methodology proposed is applied through one of them (Proxmark 3) to validate it. Thus, the methodology is tested in different scenarios where tags are commonly used for identification. In such systems it was possible to clone transponders, extract information, and even emulate both tags and readers. Therefore, it is shown that the methodology proposed is useful for auditing security and reverse engineering RFID communications in IoT applications. It must be noted that, although this paper is aimed at fostering RFID communications security in IoT applications, the methodology can be applied to any RFID communications protocol.","sentences":["The Internet of Things (IoT) is a distributed system of physical objects that requires the seamless integration of hardware (e.g., sensors, actuators, electronics) and network communications in order to collect and exchange data.","IoT smart objects need to be somehow identified to determine the origin of the data and to automatically detect the elements around us.","One of the best positioned technologies to perform identification is RFID (Radio Frequency Identification), which in the last years has gained a lot of popularity in applications like access control, payment cards or logistics.","Despite its popularity, RFID security has not been properly handled in numerous applications.","To foster security in such applications, this article includes three main contributions.","First, in order to establish the basics, a detailed review of the most common flaws found in RFID-based IoT systems is provided, including the latest attacks described in the literature.","Second, a novel methodology that eases the detection and mitigation of such flaws is presented.","Third, the latest RFID security tools are analyzed and the methodology proposed is applied through one of them (Proxmark 3) to validate it.","Thus, the methodology is tested in different scenarios where tags are commonly used for identification.","In such systems it was possible to clone transponders, extract information, and even emulate both tags and readers.","Therefore, it is shown that the methodology proposed is useful for auditing security and reverse engineering RFID communications in IoT applications.","It must be noted that, although this paper is aimed at fostering RFID communications security in IoT applications, the methodology can be applied to any RFID communications protocol."],"url":"http://arxiv.org/abs/2402.03591v1","category":"eess.SY"}
{"created":"2024-02-05 23:50:55","title":"Assessing the Impact of Distribution Shift on Reinforcement Learning Performance","abstract":"Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dynamic environments allow us to make stronger assumptions to justify the measurement of causal impact in our evaluations. We then apply these tools to single-agent and multi-agent environments to show the impact of introducing distribution shifts during test time. We present this methodology as a first step toward rigorous RL evaluation in the presence of distribution shifts.","sentences":["Research in machine learning is making progress in fixing its own reproducibility crisis.","Reinforcement learning (RL), in particular, faces its own set of unique challenges.","Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup.","Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations.","We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts.","The tools presented here argue for the need to account for performance over time while the agent is acting in its environment.","In particular, we recommend time series analysis as a method of observational RL evaluation.","We also show that the unique properties of RL and simulated dynamic environments allow us to make stronger assumptions to justify the measurement of causal impact in our evaluations.","We then apply these tools to single-agent and multi-agent environments to show the impact of introducing distribution shifts during test time.","We present this methodology as a first step toward rigorous RL evaluation in the presence of distribution shifts."],"url":"http://arxiv.org/abs/2402.03590v1","category":"cs.LG"}
{"created":"2024-02-05 23:46:42","title":"A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System","abstract":"Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles. Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations. Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators. As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems. This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles. We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework. This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle departures are synchronized. A comprehensive simulator under the first-arrive-first-serve rule is then developed to facilitate the learning process by computing immediate rewards under diverse demand scenarios. To estimate the value function and learn the rebalancing policy, various Deep Q-Network configurations are tested, minimizing the lost demand. Experiments are carried out on various datasets generated from historical data, affected by both temporal and weather factors. The proposed algorithms outperform benchmarks, including a multi-period Mixed-Integer Programming model, in terms of lost demand. Once trained, it yields immediate decisions, making it suitable for real-time applications. Our work offers practical insights for operators and enriches the integration of reinforcement learning into dynamic rebalancing problems, paving the way for more intelligent and robust urban mobility solutions.","sentences":["Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles.","Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations.","Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators.","As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems.","This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles.","We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework.","This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle departures are synchronized.","A comprehensive simulator under the first-arrive-first-serve rule is then developed to facilitate the learning process by computing immediate rewards under diverse demand scenarios.","To estimate the value function and learn the rebalancing policy, various Deep Q-Network configurations are tested, minimizing the lost demand.","Experiments are carried out on various datasets generated from historical data, affected by both temporal and weather factors.","The proposed algorithms outperform benchmarks, including a multi-period Mixed-Integer Programming model, in terms of lost demand.","Once trained, it yields immediate decisions, making it suitable for real-time applications.","Our work offers practical insights for operators and enriches the integration of reinforcement learning into dynamic rebalancing problems, paving the way for more intelligent and robust urban mobility solutions."],"url":"http://arxiv.org/abs/2402.03589v1","category":"cs.LG"}
{"created":"2024-02-05 23:46:03","title":"Continual Domain Adversarial Adaptation via Double-Head Discriminators","abstract":"Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\\%$ improvement on all categories of target domain adaptation task while significantly mitigating the forgetting on source domain.","sentences":["Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data.","Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches.","This limitation arises from the erroneous empirical estimation of $\\gH$-divergence with few source domain samples.","To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase.","We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\\gH$-divergence related adversarial loss is reduced from the source domain side.","Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\\%$ improvement on all categories of target domain adaptation task while significantly mitigating the forgetting on source domain."],"url":"http://arxiv.org/abs/2402.03588v1","category":"cs.LG"}
{"created":"2024-02-05 23:20:05","title":"MQuinE: a cure for \"Z-paradox'' in knowledge graph embedding models","abstract":"Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \\emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \\emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.","sentences":["Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval.","Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \\emph{Z-paradox}.","Motivated by the existence of Z-paradox, we propose a new KGE model called \\emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification.","Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\\% accuracy drop on some challenging test samples.","Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks."],"url":"http://arxiv.org/abs/2402.03583v1","category":"cs.SI"}
{"created":"2024-02-05 23:06:42","title":"LLM Multi-Agent Systems: Challenges and Open Problems","abstract":"This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.","sentences":["This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed.","By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration.","We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems.","We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems."],"url":"http://arxiv.org/abs/2402.03578v1","category":"cs.MA"}
{"created":"2024-02-05 22:55:33","title":"Toward Human-AI Alignment in Large-Scale Multi-Player Games","abstract":"Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.","sentences":["Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay.","We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies.","Our approach has three components.","First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space.","This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent.","Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior.","Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast.","This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity.","Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns.","These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications.","Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming."],"url":"http://arxiv.org/abs/2402.03575v1","category":"cs.AI"}
{"created":"2024-02-05 22:46:38","title":"Entanglement-enhanced quantum metrology: from standard quantum limit to Heisenberg limit","abstract":"Entanglement-enhanced quantum metrology explores the utilization of quantum entanglement to enhance measurement precision. When particles in a probe are prepared into a quantum entangled state, they collectively accumulate information about the physical quantity to be measured, leading to an improvement in measurement precision beyond the standard quantum limit and approaching the Heisenberg limit. The rapid advancement of techniques for quantum manipulation and detection has enabled the generation, manipulation, and detection of multi-particle entangled states in synthetic quantum systems such as cold atoms and trapped ions. This article aims to review and illustrate the fundamental principles and experimental progresses that demonstrate multi-particle entanglement for quantum metrology, as well as discuss the potential applications of entanglement-enhanced quantum sensors.","sentences":["Entanglement-enhanced quantum metrology explores the utilization of quantum entanglement to enhance measurement precision.","When particles in a probe are prepared into a quantum entangled state, they collectively accumulate information about the physical quantity to be measured, leading to an improvement in measurement precision beyond the standard quantum limit and approaching the Heisenberg limit.","The rapid advancement of techniques for quantum manipulation and detection has enabled the generation, manipulation, and detection of multi-particle entangled states in synthetic quantum systems such as cold atoms and trapped ions.","This article aims to review and illustrate the fundamental principles and experimental progresses that demonstrate multi-particle entanglement for quantum metrology, as well as discuss the potential applications of entanglement-enhanced quantum sensors."],"url":"http://arxiv.org/abs/2402.03572v1","category":"quant-ph"}
{"created":"2024-02-05 22:43:57","title":"Diffusion World Model","abstract":"We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\\%$ performance gain, and achieves state-of-the-art performance.","sentences":["We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently.","As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires.","We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM.","In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling.","Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data.","Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation.","In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\\%$ performance gain, and achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.03570v1","category":"cs.LG"}
{"created":"2024-02-05 22:34:04","title":"Exploring freeze out and flow using exact solutions of conformal hydrodynamics","abstract":"Exact solutions to the equations of hydrodynamics provide valuable benchmark tests for numerical hydrodynamic codes and also provide useful insights into the nature of hydrodynamic flow. In this paper, we introduce two novel, closely related exact solutions with non-trivial rapidity dependence which are generalizations of the well-known Gubser flow solution to conformal hydrodynamics. We then use one of our solutions to explore the consequences of choosing between two different criteria for implementing the freeze out process in fluid dynamical simulations of nuclear collisions: freeze out at constant temperature vs. freeze out at constant Knudsen number. We find that, employing our exact solution, the differences between these freeze out criteria are heavily influenced by the presence of strong collective flow. Our results highlight the importance of accurately describing the freeze out process in collisions with large flow gradients, particularly in small systems.","sentences":["Exact solutions to the equations of hydrodynamics provide valuable benchmark tests for numerical hydrodynamic codes and also provide useful insights into the nature of hydrodynamic flow.","In this paper, we introduce two novel, closely related exact solutions with non-trivial rapidity dependence which are generalizations of the well-known Gubser flow solution to conformal hydrodynamics.","We then use one of our solutions to explore the consequences of choosing between two different criteria for implementing the freeze out process in fluid dynamical simulations of nuclear collisions: freeze out at constant temperature vs. freeze out at constant Knudsen number.","We find that, employing our exact solution, the differences between these freeze out criteria are heavily influenced by the presence of strong collective flow.","Our results highlight the importance of accurately describing the freeze out process in collisions with large flow gradients, particularly in small systems."],"url":"http://arxiv.org/abs/2402.03568v1","category":"nucl-th"}
{"created":"2024-02-05 22:22:49","title":"Distinguishing the Knowable from the Unknowable with Language Models","abstract":"We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.","sentences":["We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text.","In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth.","We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others.","Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task.","Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings."],"url":"http://arxiv.org/abs/2402.03563v1","category":"cs.LG"}
{"created":"2024-02-05 22:20:19","title":"VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation","abstract":"Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.","sentences":["Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions.","The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data.","To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance.","VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models.","We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks:","Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations.","The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset.","Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset."],"url":"http://arxiv.org/abs/2402.03561v1","category":"cs.CV"}
{"created":"2024-02-05 22:18:16","title":"Projected Generative Diffusion Models for Constraint Satisfaction","abstract":"Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generation, trajectory optimization in path planning, and morphometric properties adherence in material science.","sentences":["Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process.","However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges.","This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem.","This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles.","This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations.","These capabilities are demonstrated by physics-informed motion in video generation, trajectory optimization in path planning, and morphometric properties adherence in material science."],"url":"http://arxiv.org/abs/2402.03559v1","category":"cs.LG"}
{"created":"2024-02-05 21:51:36","title":"Extended Version of: On the Structural Hardness of Answer Set Programming: Can Structure Efficiently Confine the Power of Disjunctions?","abstract":"Answer Set Programming (ASP) is a generic problem modeling and solving framework with a strong focus on knowledge representation and a rapid growth of industrial applications. So far, the study of complexity resulted in characterizing hardness and determining their sources, fine-grained insights in the form of dichotomy-style results, as well as detailed parameterized complexity landscapes. Unfortunately, for the well-known parameter treewidth disjunctive programs require double-exponential runtime under reasonable complexity assumptions. This quickly becomes out of reach. We deal with the classification of structural parameters for disjunctive ASP on the program's rule structure (incidence graph).   First, we provide a polynomial kernel to obtain single-exponential runtime in terms of vertex cover size, despite subset-minimization being not represented in the program's structure. Then we turn our attention to strictly better structural parameters between vertex cover size and treewidth. Here, we provide double-exponential lower bounds for the most prominent parameters in that range: treedepth, feedback vertex size, and cliquewidth. Based on this, we argue that unfortunately our options beyond vertex cover size are limited. Our results provide an in-depth hardness study, relying on a novel reduction from normal to disjunctive programs, trading the increase of complexity for an exponential parameter compression.","sentences":["Answer Set Programming (ASP) is a generic problem modeling and solving framework with a strong focus on knowledge representation and a rapid growth of industrial applications.","So far, the study of complexity resulted in characterizing hardness and determining their sources, fine-grained insights in the form of dichotomy-style results, as well as detailed parameterized complexity landscapes.","Unfortunately, for the well-known parameter treewidth disjunctive programs require double-exponential runtime under reasonable complexity assumptions.","This quickly becomes out of reach.","We deal with the classification of structural parameters for disjunctive ASP on the program's rule structure (incidence graph).   ","First, we provide a polynomial kernel to obtain single-exponential runtime in terms of vertex cover size, despite subset-minimization being not represented in the program's structure.","Then we turn our attention to strictly better structural parameters between vertex cover size and treewidth.","Here, we provide double-exponential lower bounds for the most prominent parameters in that range: treedepth, feedback vertex size, and cliquewidth.","Based on this, we argue that unfortunately our options beyond vertex cover size are limited.","Our results provide an in-depth hardness study, relying on a novel reduction from normal to disjunctive programs, trading the increase of complexity for an exponential parameter compression."],"url":"http://arxiv.org/abs/2402.03539v1","category":"cs.AI"}
{"created":"2024-02-05 21:44:19","title":"Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model","abstract":"The availability of large, public, multi-modal astronomical datasets presents an opportunity to execute novel research that straddles the line between science of AI and science of astronomy. Photometric redshift estimation is a well-established subfield of astronomy. Prior works show that computer vision models typically outperform catalog-based models, but these models face additional complexities when incorporating images from more than one instrument or sensor. In this report, we detail our progress creating Mantis Shrimp, a multi-survey computer vision model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. We use deep learning interpretability diagnostics to measure how the model leverages information from the different inputs. We reason about the behavior of the CNNs from the interpretability metrics, specifically framing the result in terms of physically-grounded knowledge of galaxy properties.","sentences":["The availability of large, public, multi-modal astronomical datasets presents an opportunity to execute novel research that straddles the line between science of AI and science of astronomy.","Photometric redshift estimation is a well-established subfield of astronomy.","Prior works show that computer vision models typically outperform catalog-based models, but these models face additional complexities when incorporating images from more than one instrument or sensor.","In this report, we detail our progress creating Mantis Shrimp, a multi-survey computer vision model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery.","We use deep learning interpretability diagnostics to measure how the model leverages information from the different inputs.","We reason about the behavior of the CNNs from the interpretability metrics, specifically framing the result in terms of physically-grounded knowledge of galaxy properties."],"url":"http://arxiv.org/abs/2402.03535v1","category":"astro-ph.IM"}
{"created":"2024-02-05 21:38:18","title":"ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing","abstract":"Peer review is a cornerstone of science. Research communities conduct peer reviews to assess contributions and to improve the overall quality of science work. Every year, new community members are recruited as peer reviewers for the first time. How could technology help novices adhere to their community's practices and standards for peer reviewing? To better understand peer review practices and challenges, we conducted a formative study with 10 novices and 10 experts. We found that many experts adopt a workflow of annotating, note-taking, and synthesizing notes into well-justified reviews that align with community standards. Novices lack timely guidance on how to read and assess submissions and how to structure paper reviews. To support the peer review process, we developed ReviewFlow -- an AI-driven workflow that scaffolds novices with contextual reflections to critique and annotate submissions, in-situ knowledge support to assess novelty, and notes-to-outline synthesis to help align peer reviews with community expectations. In a within-subjects experiment, 16 inexperienced reviewers wrote reviews using ReviewFlow and a baseline environment with minimal guidance. Participants produced more comprehensive reviews using ReviewFlow than the baseline, calling out more pros and cons, but they still struggled to provide actionable suggestions to address the weaknesses. While participants appreciated the streamlined process support from ReviewFlow, they also expressed concerns about using AI as part of the scientific review process. We discuss the implications of using AI to scaffold peer review process on scientific work and beyond.","sentences":["Peer review is a cornerstone of science.","Research communities conduct peer reviews to assess contributions and to improve the overall quality of science work.","Every year, new community members are recruited as peer reviewers for the first time.","How could technology help novices adhere to their community's practices and standards for peer reviewing?","To better understand peer review practices and challenges, we conducted a formative study with 10 novices and 10 experts.","We found that many experts adopt a workflow of annotating, note-taking, and synthesizing notes into well-justified reviews that align with community standards.","Novices lack timely guidance on how to read and assess submissions and how to structure paper reviews.","To support the peer review process, we developed ReviewFlow -- an AI-driven workflow that scaffolds novices with contextual reflections to critique and annotate submissions, in-situ knowledge support to assess novelty, and notes-to-outline synthesis to help align peer reviews with community expectations.","In a within-subjects experiment, 16 inexperienced reviewers wrote reviews using ReviewFlow and a baseline environment with minimal guidance.","Participants produced more comprehensive reviews using ReviewFlow than the baseline, calling out more pros and cons, but they still struggled to provide actionable suggestions to address the weaknesses.","While participants appreciated the streamlined process support from ReviewFlow, they also expressed concerns about using AI as part of the scientific review process.","We discuss the implications of using AI to scaffold peer review process on scientific work and beyond."],"url":"http://arxiv.org/abs/2402.03530v1","category":"cs.HC"}
{"created":"2024-02-05 21:25:45","title":"Deep Reinforcement Learning for Picker Routing Problem in Warehousing","abstract":"Order Picker Routing is a critical issue in Warehouse Operations Management. Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice. However, Reinforcement Learning offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy. We introduce an attention based neural network for modeling picker tours, which is trained using Reinforcement Learning. Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy. A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes.","sentences":["Order Picker Routing is a critical issue in Warehouse Operations Management.","Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice.","However, Reinforcement Learning offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy.","We introduce an attention based neural network for modeling picker tours, which is trained using Reinforcement Learning.","Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy.","A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes."],"url":"http://arxiv.org/abs/2402.03525v1","category":"cs.LG"}
{"created":"2024-02-05 21:05:35","title":"Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration","abstract":"Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benefits from our proposed system.","sentences":["Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks.","Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions.","In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process.","Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets.","Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency.","Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benefits from our proposed system."],"url":"http://arxiv.org/abs/2402.03519v1","category":"cs.CL"}
{"created":"2024-02-05 20:51:11","title":"Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains","abstract":"Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate summarization, beyond news articles. The dataset can be downloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains","sentences":["Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries.","However, this prior work has focussed almost exclusively on evaluating news article summarization.","How do zero-shot summarizers perform in other (potentially more specialized) domains?","In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference).","We focus especially on the factuality of outputs.","We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.","We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain.","We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate summarization, beyond news articles.","The dataset can be downloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains"],"url":"http://arxiv.org/abs/2402.03509v1","category":"cs.CL"}
{"created":"2024-02-05 20:48:57","title":"Neural networks for abstraction and reasoning: Towards broad generalization in machines","abstract":"For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining models to achieve better results than any system alone. Finally, we publish the arckit Python library to make future research on ARC easier.","sentences":["For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy.","While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.","In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization.","Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all.","We revisit whether recent advances in neural networks allow progress on this task.   ","First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.","DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition.","We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.","We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks.","LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches.","We perform an ensemble analysis, combining models to achieve better results than any system alone.","Finally, we publish the arckit Python library to make future research on ARC easier."],"url":"http://arxiv.org/abs/2402.03507v1","category":"cs.AI"}
{"created":"2024-02-05 20:34:32","title":"An Inpainting-Infused Pipeline for Attire and Background Replacement","abstract":"In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains. In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation. The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline. Experiments conducted in this study underscore the methodology's efficacy, highlighting its potential to produce visually captivating content. The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background based on specific prompts without manually input inpainting masks, effectively placing the subjects within the vast landscape of creative imagination.","sentences":["In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains.","In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation.","The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline.","Experiments conducted in this study underscore the methodology's efficacy, highlighting its potential to produce visually captivating content.","The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background based on specific prompts without manually input inpainting masks, effectively placing the subjects within the vast landscape of creative imagination."],"url":"http://arxiv.org/abs/2402.03501v1","category":"cs.CV"}
{"created":"2024-02-05 20:33:00","title":"Curriculum reinforcement learning for quantum architecture search under hardware errors","abstract":"The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.","sentences":["The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations.","Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop.","However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture.","Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically.","In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study.","However, the effects of noise on the architecture search, which could be just as critical, are poorly understood.","This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS)","algorithm designed to tackle challenges in realistic VQA deployment.","The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence.","To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis.","Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments."],"url":"http://arxiv.org/abs/2402.03500v1","category":"quant-ph"}
{"created":"2024-02-05 20:25:59","title":"The TESS-Keck Survey. XIX. A Warm Transiting Sub-Saturn Mass Planet and a non-Transiting Saturn Mass Planet Orbiting a Solar Analog","abstract":"The Transiting Exoplanet Survey Satellite (TESS) continues to dramatically increase the number of known transiting exoplanets, and is optimal for monitoring bright stars amenable to radial velocity (RV) and atmospheric follow-up observations. TOI-1386 is a solar-type (G5V) star that was detected via TESS photometry to exhibit transit signatures in three sectors with a period of 25.84 days. We conducted follow-up RV observations using Keck/HIRES as part of the TESS-Keck Survey (TKS), collecting 64 RV measurements of TOI-1386 with the HIRES spectrograph over 2.5 years. Our combined fit of the TOI-1386 photometry and RV data confirm the planetary nature of the detected TESS signal, and provide a mass and radius for planet b of $0.148\\pm0.019$ $M_J$ and $0.540\\pm0.017$ $R_J$, respectively, marking TOI-1386 b as a warm sub-Saturn planet. Our RV data further reveal an additional outer companion, TOI-1386 c, with an estimated orbital period of 227.6 days and a minimum mass of $0.309\\pm0.038$ $M_J$. The dynamical modeling of the system shows that the measured system architecture is long-term stable, although there may be substantial eccentricity oscillations of the inner planet due to the dynamical influence of the outer planet.","sentences":["The Transiting Exoplanet Survey Satellite (TESS) continues to dramatically increase the number of known transiting exoplanets, and is optimal for monitoring bright stars amenable to radial velocity (RV) and atmospheric follow-up observations.","TOI-1386 is a solar-type (G5V) star that was detected via TESS photometry to exhibit transit signatures in three sectors with a period of 25.84 days.","We conducted follow-up RV observations using Keck/HIRES as part of the TESS-Keck Survey (TKS), collecting 64 RV measurements of TOI-1386 with the HIRES spectrograph over 2.5 years.","Our combined fit of the TOI-1386 photometry and RV data confirm the planetary nature of the detected TESS signal, and provide a mass and radius for planet b of $0.148\\pm0.019$ $M_J$ and $0.540\\pm0.017$ $R_J$, respectively, marking TOI-1386 b as a warm sub-Saturn planet.","Our RV data further reveal an additional outer companion, TOI-1386 c, with an estimated orbital period of 227.6 days and a minimum mass of $0.309\\pm0.038$ $M_J$. The dynamical modeling of the system shows that the measured system architecture is long-term stable, although there may be substantial eccentricity oscillations of the inner planet due to the dynamical influence of the outer planet."],"url":"http://arxiv.org/abs/2402.03498v1","category":"astro-ph.EP"}
{"created":"2024-02-05 20:11:56","title":"Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues","abstract":"This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present \"Beyond Text\"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach not only achieves a 70.26% winning rate, outperforming existing LLMs by 48.30%, but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate. \"Beyond Text\" marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.","sentences":["This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications.","While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems.","We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses.","These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said.","We present \"Beyond Text\"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations.","This approach not only achieves a 70.26% winning rate, outperforming existing LLMs by 48.30%, but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate.","\"Beyond Text\" marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models."],"url":"http://arxiv.org/abs/2402.03494v1","category":"cs.AI"}
{"created":"2024-02-05 19:58:40","title":"Early prediction of onset of sepsis in Clinical Setting","abstract":"This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The F1 scores were 80.8\\% and 67.1\\% respectively for the test data and the prospective data for the same threshold, highlighting its potential to be integrated into clinical decision-making processes effectively. These results bear testament to the model's robust predictive capabilities and its potential to substantially impact clinical decision-making processes.","sentences":["This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA.","A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\\% of the train dataset, encompassing 107 features (including the original and derived features).","Subsequently, the model was evaluated on the remaining 20\\% of the test data.","The model was validated on prospective data that was entirely unseen during the training phase.","To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper.","Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised.","The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3.","The F1 scores were 80.8\\% and 67.1\\% respectively for the test data and the prospective data for the same threshold, highlighting its potential to be integrated into clinical decision-making processes effectively.","These results bear testament to the model's robust predictive capabilities and its potential to substantially impact clinical decision-making processes."],"url":"http://arxiv.org/abs/2402.03486v1","category":"cs.LG"}
{"created":"2024-02-05 19:55:06","title":"SWAG: Storytelling With Action Guidance","abstract":"Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best \"action\" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.","sentences":["Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content.","We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs.","Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best \"action\" to steer the story's future direction.","Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo."],"url":"http://arxiv.org/abs/2402.03483v1","category":"cs.CL"}
{"created":"2024-02-05 19:48:31","title":"Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision","abstract":"Deep learning methods are transforming research, enabling new techniques, and ultimately leading to new discoveries. As the demand for more capable AI models continues to grow, we are now entering an era of Trillion Parameter Models (TPM), or models with more than a trillion parameters -- such as Huawei's PanGu-$\\Sigma$. We describe a vision for the ecosystem of TPM users and providers that caters to the specific needs of the scientific community. We then outline the significant technical challenges and open problems in system design for serving TPMs to enable scientific research and discovery. Specifically, we describe the requirements of a comprehensive software stack and interfaces to support the diverse and flexible requirements of researchers.","sentences":["Deep learning methods are transforming research, enabling new techniques, and ultimately leading to new discoveries.","As the demand for more capable AI models continues to grow, we are now entering an era of Trillion Parameter Models (TPM), or models with more than a trillion parameters -- such as Huawei's PanGu-$\\Sigma$.","We describe a vision for the ecosystem of TPM users and providers that caters to the specific needs of the scientific community.","We then outline the significant technical challenges and open problems in system design for serving TPMs to enable scientific research and discovery.","Specifically, we describe the requirements of a comprehensive software stack and interfaces to support the diverse and flexible requirements of researchers."],"url":"http://arxiv.org/abs/2402.03480v1","category":"cs.LG"}
{"created":"2024-02-05 19:47:45","title":"ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design","abstract":"Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce in-context environment design (ICED). ICED generates levels using a variational autoencoder trained over an initial set of level parameters, reducing distributional shift, and achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods.","sentences":["Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training.","In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents.","We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data.","This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies.","We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism.","We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance.","To prevent both overfitting and distributional shift, we introduce in-context environment design (ICED).","ICED generates levels using a variational autoencoder trained over an initial set of level parameters, reducing distributional shift, and achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods."],"url":"http://arxiv.org/abs/2402.03479v1","category":"cs.LG"}
{"created":"2024-02-05 19:19:39","title":"Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband Signal","abstract":"Spectrum has become an extremely scarce and congested resource. As a consequence, spectrum sensing enables the coexistence of different wireless technologies in shared spectrum bands. Most existing work requires spectrograms to classify signals. Ultimately, this implies that images need to be continuously created from I/Q samples, thus creating unacceptable latency for real-time operations. In addition, spectrogram-based approaches do not achieve sufficient granularity level as they are based on object detection performed on pixels and are based on rectangular bounding boxes. For this reason, we propose a completely novel approach based on semantic spectrum segmentation, where multiple signals are simultaneously classified and localized in both time and frequency at the I/Q level. Conversely from the state-of-the-art computer vision algorithm, we add non-local blocks to combine the spatial features of signals, and thus achieve better performance. In addition, we propose a novel data generation approach where a limited set of easy-to-collect real-world wireless signals are ``stitched together'' to generate large-scale, wideband, and diverse datasets. Experimental results obtained on multiple testbeds (including the Arena testbed) using multiple antennas, multiple sampling frequencies, and multiple radios over the course of 3 days show that our approach classifies and localizes signals with a mean intersection over union (IOU) of 96.70% across 5 wireless protocols while performing in real-time with a latency of 2.6 ms. Moreover, we demonstrate that our approach based on non-local blocks achieves 7% more accuracy when segmenting the most challenging signals with respect to the state-of-the-art U-Net algorithm. We will release our 17 GB dataset and code.","sentences":["Spectrum has become an extremely scarce and congested resource.","As a consequence, spectrum sensing enables the coexistence of different wireless technologies in shared spectrum bands.","Most existing work requires spectrograms to classify signals.","Ultimately, this implies that images need to be continuously created from I/Q samples, thus creating unacceptable latency for real-time operations.","In addition, spectrogram-based approaches do not achieve sufficient granularity level as they are based on object detection performed on pixels and are based on rectangular bounding boxes.","For this reason, we propose a completely novel approach based on semantic spectrum segmentation, where multiple signals are simultaneously classified and localized in both time and frequency at the I/Q level.","Conversely from the state-of-the-art computer vision algorithm, we add non-local blocks to combine the spatial features of signals, and thus achieve better performance.","In addition, we propose a novel data generation approach where a limited set of easy-to-collect real-world wireless signals are ``stitched together'' to generate large-scale, wideband, and diverse datasets.","Experimental results obtained on multiple testbeds (including the Arena testbed) using multiple antennas, multiple sampling frequencies, and multiple radios over the course of 3 days show that our approach classifies and localizes signals with a mean intersection over union (IOU) of 96.70% across 5 wireless protocols while performing in real-time with a latency of 2.6 ms.","Moreover, we demonstrate that our approach based on non-local blocks achieves 7% more accuracy when segmenting the most challenging signals with respect to the state-of-the-art U-Net algorithm.","We will release our 17 GB dataset and code."],"url":"http://arxiv.org/abs/2402.03465v1","category":"cs.NI"}
{"created":"2024-02-05 19:09:16","title":"Median and Small Parsimony Problems on RNA trees","abstract":"Motivation: Non-coding RNAs (ncRNAs) express their functions by adopting molecular structures. Specifically, RNA secondary structures serve as a relatively stable intermediate step before tertiary structures, offering a reliable signature of molecular function. Consequently, within an RNA functional family, secondary structures are generally more evolutionarily conserved than sequences. Conversely, homologous RNA families grouped within an RNA clan share ancestors but typically exhibit structural differences. Inferring the evolution of RNA structures within RNA families and clans is crucial for gaining insights into functional adaptations over time and providing clues about the Ancient RNA World Hypothesis. Results: We introduce the median problem and the small parsimony problem for ncRNA families, where secondary structures are represented as leaf-labelled trees. We utilize the Robinson-Foulds (RF) tree distance, which corresponds to a specific edit distance between RNA trees, and a new metric called the Internal-Leafset (IL) distance. While the RF tree distance compares sets of leaves descending from internal nodes of two RNA trees, the IL distance compares the collection of leaf-children of internal nodes. The latter is better at capturing differences in structural elements of RNAs than the RF distance, which is more focused on base pairs. We also consider a more general tree edit distance that allows the mapping of base pairs that are not perfectly aligned. We study the theoretical complexity of the median problem and the small parsimony problem under the three distance metrics and various biologically-relevant constraints, and we present polynomial-time maximum parsimony algorithms for solving some versions of the problems. Our algorithms are applied to ncRNA families from the RFAM database, illustrating their practical utility","sentences":["Motivation: Non-coding RNAs (ncRNAs) express their functions by adopting molecular structures.","Specifically, RNA secondary structures serve as a relatively stable intermediate step before tertiary structures, offering a reliable signature of molecular function.","Consequently, within an RNA functional family, secondary structures are generally more evolutionarily conserved than sequences.","Conversely, homologous RNA families grouped within an RNA clan share ancestors but typically exhibit structural differences.","Inferring the evolution of RNA structures within RNA families and clans is crucial for gaining insights into functional adaptations over time and providing clues about the Ancient RNA World Hypothesis.","Results: We introduce the median problem and the small parsimony problem for ncRNA families, where secondary structures are represented as leaf-labelled trees.","We utilize the Robinson-Foulds (RF) tree distance, which corresponds to a specific edit distance between RNA trees, and a new metric called the Internal-Leafset (IL) distance.","While the RF tree distance compares sets of leaves descending from internal nodes of two RNA trees, the IL distance compares the collection of leaf-children of internal nodes.","The latter is better at capturing differences in structural elements of RNAs than the RF distance, which is more focused on base pairs.","We also consider a more general tree edit distance that allows the mapping of base pairs that are not perfectly aligned.","We study the theoretical complexity of the median problem and the small parsimony problem under the three distance metrics and various biologically-relevant constraints, and we present polynomial-time maximum parsimony algorithms for solving some versions of the problems.","Our algorithms are applied to ncRNA families from the RFAM database, illustrating their practical utility"],"url":"http://arxiv.org/abs/2402.03455v1","category":"cs.DS"}
{"created":"2024-02-05 19:00:02","title":"Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach","abstract":"This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of \"open-source\" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.","sentences":["This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level.","The work is circumscribed to the use of \"open-source\" LLMs that can be run locally, thereby enhancing data privacy.","Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets.","The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion.","Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach.","This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task."],"url":"http://arxiv.org/abs/2402.03435v1","category":"cs.CL"}
{"created":"2024-02-05 18:59:41","title":"HASSOD: Hierarchical Adaptive Self-Supervised Object Detection","abstract":"The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.","sentences":["The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects.","Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision.","HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image.","Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures.","This additional self-supervised learning task leads to improved detection performance and enhanced interpretability.","Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process.","Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection.","Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io."],"url":"http://arxiv.org/abs/2402.03311v1","category":"cs.CV"}
{"created":"2024-02-05 18:59:36","title":"V-IRL: Grounding Virtual Intelligence in Real Life","abstract":"There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.","sentences":["There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created.","To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds.","How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control?","Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment.","Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe."],"url":"http://arxiv.org/abs/2402.03310v1","category":"cs.AI"}
{"created":"2024-02-05 18:58:38","title":"Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?","abstract":"Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location. Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes.","sentences":["Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows.","These outputs indicate the ability to perform compositional generalization, but how do the models do so?","We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions.","Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance.","En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold.","Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location.","Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks.","These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes."],"url":"http://arxiv.org/abs/2402.03305v1","category":"cs.LG"}
{"created":"2024-02-05 18:58:19","title":"Nevermind: Instruction Override and Moderation in Large Language Models","abstract":"Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines. Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the LLM itself.","sentences":["Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides.","These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak.","Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault.","When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities.","Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines.","Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the LLM itself."],"url":"http://arxiv.org/abs/2402.03303v1","category":"cs.CL"}
{"created":"2024-02-05 18:55:32","title":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models","abstract":"Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.","sentences":["Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.","In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data.","DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.","Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.","The mathematical reasoning capability of DeepSeekMath is attributed to two key factors:","First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline.","Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO."],"url":"http://arxiv.org/abs/2402.03300v2","category":"cs.CL"}
{"created":"2024-02-05 18:54:43","title":"GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models","abstract":"The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.","sentences":["The discovery of \"jailbreaks\" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures.","One major safety measure is to proactively test the LLMs with jailbreaks prior to the release.","Therefore, such testing will require a method that can generate jailbreaks massively and efficiently.","In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation.","We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks.","Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence.","We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve.","Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses.","In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly.","We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics).","We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT).","Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities."],"url":"http://arxiv.org/abs/2402.03299v1","category":"cs.LG"}
{"created":"2024-02-05 18:51:17","title":"Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks","abstract":"Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code is publicly available.","sentences":["Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices.","Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning.","The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix.","These requirements are infeasible even with state-of-the-art hardware.","In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix.","Our method enjoys efficient linear memory and time complexity for each iteration.","Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate.","We provide the convergence result of Ginger for non-convex objectives.","Our experiments on different tasks with different model architectures verify the effectiveness of our method.","Our code is publicly available."],"url":"http://arxiv.org/abs/2402.03295v1","category":"cs.LG"}
{"created":"2024-02-05 18:50:39","title":"Flora: Low-Rank Adapters Are Secretly Gradient Compressors","abstract":"Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.","sentences":["Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training.","To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters.","However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance.","In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection.","Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states.","We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.03293v1","category":"cs.LG"}
{"created":"2024-02-05 18:49:17","title":"InstanceDiffusion: Instance-level Control for Image Generation","abstract":"Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, and 25.4% IoU for mask inputs.","sentences":["Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image.","We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models.","InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof.","We propose three major changes to text-to-image models that enable precise instance-level control.","Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances.","InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition.","Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, and 25.4% IoU for mask inputs."],"url":"http://arxiv.org/abs/2402.03290v1","category":"cs.CV"}
{"created":"2024-02-05 18:47:04","title":"Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS","abstract":"Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.","sentences":["Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency.","This is due to the lack of PPA awareness in conventional transformer decoding algorithms.","In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code.","Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models.","For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product."],"url":"http://arxiv.org/abs/2402.03289v1","category":"cs.LG"}
{"created":"2024-02-05 18:42:34","title":"Training-Free Consistent Text-to-Image Generation","abstract":"Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.","sentences":["Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language.","However, using these models to consistently portray the same subject across diverse prompts remains challenging.","Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model.","These methods require lengthy per-subject optimization or large-scale pre-training.","Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects.","Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model.","We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images.","Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency.","We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step.","Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects."],"url":"http://arxiv.org/abs/2402.03286v1","category":"cs.CV"}
{"created":"2024-02-05 18:41:46","title":"Estimating position-dependent and anisotropic diffusivity tensors from molecular dynamics trajectories: Existing methods and future outlook","abstract":"Confinement can substantially alter the physicochemical properties of materials by breaking translational isotropy and rendering all physical properties position-dependent. Molecular dynamics (MD) simulations have proven instrumental in characterizing such spatial heterogeneities and probing the impact of confinement on materials' properties. For static properties, this is a straightforward task and can be achieved via simple spatial binning. Such an approach, however, cannot be readily applied to transport coefficients due to lack of natural extensions of autocorrelations used for their calculation in the bulk. The prime example of this challenge is diffusivity, which, in the bulk, can be readily estimated from the particles' mobility statistics, which satisfy the Fokker-Planck equation. Under confinement, however, such statistics will follow the Smoluchowski equation, which lacks a closed-form analytical solution. This brief review explores the rich history of estimating profiles of the diffusivity tensor from MD simulations and discusses various approximate methods and algorithms developed for this purpose. Beside discussing heuristic extensions of bulk methods, we overview more rigorous algorithms, including kernel-based methods, Bayesian approaches, and operator discretization techniques. Additionally, we outline methods based on applying biasing potentials or imposing constraints on tracer particles. Finally, we discuss approaches that estimate diffusivity from mean first passage time or committor probability profiles, a conceptual framework originally developed in the context of collective variable spaces describing rare events in computational chemistry and biology. In summary, this paper offers a concise survey of diverse approaches for estimating diffusivity from MD trajectories, highlighting challenges and opportunities in this area.","sentences":["Confinement can substantially alter the physicochemical properties of materials by breaking translational isotropy and rendering all physical properties position-dependent.","Molecular dynamics (MD) simulations have proven instrumental in characterizing such spatial heterogeneities and probing the impact of confinement on materials' properties.","For static properties, this is a straightforward task and can be achieved via simple spatial binning.","Such an approach, however, cannot be readily applied to transport coefficients due to lack of natural extensions of autocorrelations used for their calculation in the bulk.","The prime example of this challenge is diffusivity, which, in the bulk, can be readily estimated from the particles' mobility statistics, which satisfy the Fokker-Planck equation.","Under confinement, however, such statistics will follow the Smoluchowski equation, which lacks a closed-form analytical solution.","This brief review explores the rich history of estimating profiles of the diffusivity tensor from MD simulations and discusses various approximate methods and algorithms developed for this purpose.","Beside discussing heuristic extensions of bulk methods, we overview more rigorous algorithms, including kernel-based methods, Bayesian approaches, and operator discretization techniques.","Additionally, we outline methods based on applying biasing potentials or imposing constraints on tracer particles.","Finally, we discuss approaches that estimate diffusivity from mean first passage time or committor probability profiles, a conceptual framework originally developed in the context of collective variable spaces describing rare events in computational chemistry and biology.","In summary, this paper offers a concise survey of diverse approaches for estimating diffusivity from MD trajectories, highlighting challenges and opportunities in this area."],"url":"http://arxiv.org/abs/2402.03285v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 18:39:47","title":"Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models","abstract":"Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing \"conversation forecasting\" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.","sentences":["Effective interlocutors account for the uncertain goals, beliefs, and emotions of others.","But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue.","How well can language models represent inherent uncertainty in conversations?","We propose FortUne Dial, an expansion of the long-standing \"conversation forecasting\" task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances.","We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations.","Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size."],"url":"http://arxiv.org/abs/2402.03284v1","category":"cs.CL"}
{"created":"2024-02-05 18:38:55","title":"A Framework for Partially Observed Reward-States in RLHF","abstract":"The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed \"internal states.\" Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. We show that our models and guarantees in both settings generalize and extend existing ones. Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes.","sentences":["The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs.","Neuroscience research shows that human responses to stimuli are known to depend on partially-observed \"internal states.\"","Unfortunately current models of RLHF do not take take this into consideration.","Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment.","To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL).","We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL.","For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI.","For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret.","We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret.","We show that our models and guarantees in both settings generalize and extend existing ones.","Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes."],"url":"http://arxiv.org/abs/2402.03282v1","category":"cs.LG"}
{"created":"2024-02-05 18:28:44","title":"Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models","abstract":"In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).","sentences":["In the face of uncertainty, the ability to seek information is of fundamental importance.","In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms).","In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions.","UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward.","In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task)."],"url":"http://arxiv.org/abs/2402.03271v1","category":"cs.CL"}
{"created":"2024-02-05 18:25:51","title":"Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation","abstract":"Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.","sentences":["Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning.","To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time.","We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs).","More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs.","Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason.","Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance."],"url":"http://arxiv.org/abs/2402.03268v1","category":"cs.LG"}
{"created":"2024-02-05 18:21:34","title":"Small area estimation of forest biomass via a two-stage model for continuous zero-inflated data","abstract":"The U.S. Forest Inventory & Analysis Program (FIA) collects data on and monitors the trends of forests in the United States. FIA is increasingly interested in monitoring forest attributes such as biomass at fine geographic and temporal scales, resulting in a need for assessment and development of small area estimation techniques applied to forest inventory contexts. We implement a small area estimator and parametric bootstrap estimator that account for zero-inflation in biomass data via a two-stage model-based approach. We compare the performance of this estimator to the post-stratified estimator and to the unit and area-level empirical best linear unbiased prediction (EBLUP) estimators. We conduct a simulation study with counties in Nevada, U.S. based on actual sampled plot data and remote sensing data products. Results show that the zero-inflation estimator has the lowest relative bias and the smallest empirical root mean square error. Moreover, the 95% confidence interval coverages of the zero-inflation estimator and the unit-level EBLUP are more accurate than the other two estimators. To further illustrate the practical utility, we employ a data application across the 2019 measurement year in Nevada. We introduce the R package, saeczi, which efficiently implements the zero-inflation estimator and its mean square error estimator.","sentences":["The U.S. Forest Inventory & Analysis Program (FIA) collects data on and monitors the trends of forests in the United States.","FIA is increasingly interested in monitoring forest attributes such as biomass at fine geographic and temporal scales, resulting in a need for assessment and development of small area estimation techniques applied to forest inventory contexts.","We implement a small area estimator and parametric bootstrap estimator that account for zero-inflation in biomass data via a two-stage model-based approach.","We compare the performance of this estimator to the post-stratified estimator and to the unit and area-level empirical best linear unbiased prediction (EBLUP) estimators.","We conduct a simulation study with counties in Nevada, U.S. based on actual sampled plot data and remote sensing data products.","Results show that the zero-inflation estimator has the lowest relative bias and the smallest empirical root mean square error.","Moreover, the 95% confidence interval coverages of the zero-inflation estimator and the unit-level EBLUP are more accurate than the other two estimators.","To further illustrate the practical utility, we employ a data application across the 2019 measurement year in Nevada.","We introduce the R package, saeczi, which efficiently implements the zero-inflation estimator and its mean square error estimator."],"url":"http://arxiv.org/abs/2402.03263v1","category":"stat.AP"}
{"created":"2024-02-05 18:09:33","title":"CLIP Can Understand Depth","abstract":"Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.","sentences":["Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts.","In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment.","By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth.","With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin.","Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework.","Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way.","This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging.","We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2402.03251v1","category":"cs.CV"}
{"created":"2024-02-05 18:05:34","title":"HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference","abstract":"Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs significantly reduces the crosstalk effects, thereby increasing the wavelength parallelism in HEANA. Moreover, HEANA employs our invented balanced photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal accumulations to eliminate the need to use reduction networks in HEANA, relieving it from related latency and energy overheads. Our evaluation for the inference of four modern CNNs indicates that HEANA provides improvements of atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency), respectively, for equal-area comparisons, on gmean over two MRR-based analog CNN accelerators from prior work.","sentences":["Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts.","However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations.","These shortcomings collectively hamper the performance and energy efficiency of prior accelerators.","To tackle these shortcomings, we present a novel","Hybrid timE","Amplitude aNalog optical Accelerator, called HEANA.","HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows.","A spectrally hitless arrangement of TAOMs significantly reduces the crosstalk effects, thereby increasing the wavelength parallelism in HEANA.","Moreover, HEANA employs our invented balanced photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal accumulations to eliminate the need to use reduction networks in HEANA, relieving it from related latency and energy overheads.","Our evaluation for the inference of four modern CNNs indicates that HEANA provides improvements of atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency), respectively, for equal-area comparisons, on gmean over two MRR-based analog CNN accelerators from prior work."],"url":"http://arxiv.org/abs/2402.03247v1","category":"cs.AR"}
{"created":"2024-02-05 18:03:53","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","abstract":"Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.","sentences":["Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation.","Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations.","Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions.","Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality.","Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability."],"url":"http://arxiv.org/abs/2402.03246v1","category":"cs.CV"}
{"created":"2024-02-05 17:38:49","title":"IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images","abstract":"In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks. Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD. Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities.","sentences":["In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses.","Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites.","In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization.","IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy.","During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization.","Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites.","The assessments included the transformation of MR images with traveling subjects, the preservation of pairwise distances between MR images within domains, the evolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease (AD), and the performance in age regression and patient classification tasks.","Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR images and is more suitable for maintaining and reinforcing variabilities related to age and AD.","Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different image modalities."],"url":"http://arxiv.org/abs/2402.03227v1","category":"cs.CV"}
{"created":"2024-02-05 17:37:46","title":"FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion","abstract":"As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.","sentences":["As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples.","Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance.","We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function.","Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories.","Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks.","The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks."],"url":"http://arxiv.org/abs/2402.03226v1","category":"cs.LG"}
{"created":"2024-02-05 17:26:49","title":"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation","abstract":"In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.","sentences":["In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity.","It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks.","It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications.","It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.","The effective training of M3-Embedding involves the following technical contributions.","We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality.","We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings.","To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility.","The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding."],"url":"http://arxiv.org/abs/2402.03216v1","category":"cs.CL"}
{"created":"2024-02-05 17:25:04","title":"Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?","abstract":"The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.","sentences":["The advent of generative AI images has completely disrupted the art world.","Identifying AI generated images from human art is a challenging problem whose impact is growing over time.","The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery.","This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse.","There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques.","In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings.","We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI).","Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives).","We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness."],"url":"http://arxiv.org/abs/2402.03214v1","category":"cs.CV"}
{"created":"2024-02-05 17:16:02","title":"Bad Science Matrices","abstract":"Inspired by the bad scientist who keeps repeating an experiment 20 times to get a single outcome with $p < 0.05$, we consider matrices $A \\in \\mathbb{R}^{n \\times n}$ whose columns are normalized in $\\ell^2$ and for which $2^{-n}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}}$ is large. They correspond to affine transformations of the discrete unit cube to points with, on average, at least one large coordinate. Such matrices can be seen as a collection of fair tests on a fair coin where at least one outcome is typically atypical. We prove that, as $n \\rightarrow \\infty$, the quantity can scale as   $$ \\max_{A \\in \\mathbb{R}^{n \\times n}} \\frac{1}{2^{n}}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}} = (1+o(1)) \\cdot \\sqrt{2\\log{n}}.$$ We also present candidate maximizers up to dimension $n \\leq 8$ which appear to be highly structured and have nice closed-form solutions.","sentences":["Inspired by the bad scientist who keeps repeating an experiment 20 times to get a single outcome with $p < 0.05$, we consider matrices $A \\in \\mathbb{R}^{n \\times n}$ whose columns are normalized in $\\ell^2$ and for which $2^{-n}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}}$ is large.","They correspond to affine transformations of the discrete unit cube to points with, on average, at least one large coordinate.","Such matrices can be seen as a collection of fair tests on a fair coin where at least one outcome is typically atypical.","We prove that, as $n \\rightarrow \\infty$, the quantity can scale as   $$ \\max_{A \\in \\mathbb{R}^{n \\times n}} \\frac{1}{2^{n}}\\sum_{x \\in \\left\\{-1,1\\right\\}^n} \\|Ax\\|_{\\ell^{\\infty}} = (1+o(1))","\\cdot \\sqrt{2\\log{n}}.$$","We also present candidate maximizers up to dimension $n \\leq 8$ which appear to be highly structured and have nice closed-form solutions."],"url":"http://arxiv.org/abs/2402.03205v1","category":"math.FA"}
{"created":"2024-02-05 17:15:00","title":"Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems","abstract":"We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor policy reduces power consumption by approximately 8.7% during low-traffic hours and improves energy efficiency by approximately 19% during high-traffic hours, respectively.","sentences":["We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs.","The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference.","A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy.","To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed.","Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies.","Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor policy reduces power consumption by approximately 8.7% during low-traffic hours and improves energy efficiency by approximately 19% during high-traffic hours, respectively."],"url":"http://arxiv.org/abs/2402.03204v1","category":"cs.IT"}
{"created":"2024-02-05 17:13:12","title":"Leveraging IRS Induced Time Delay for Enhanced Physical Layer Security in VLC Systems","abstract":"Indoor visible light communication (VLC) is considered secure against attackers outside the confined area where the light propagates, but it is still susceptible to interception from inside the coverage area. A new technology, intelligent reflecting surfaces (IRS), has been recently introduced, offering a way to enhance physical layer security (PLS). Most research on IRS-assisted VLC assumes the same time of arrival from all reflecting elements and overlooks the effect of time delay and the associated intersymbol interference. This paper tackles, for the first time, the effect of time delay on the secrecy rate in VLC systems. Our results show that, at a fixed light-emitting diode (LED) power of 3W, the secrecy rate can be enhanced by up to 253\\% at random positions for the legitimate user when the eavesdropper is located within a 1-meter radius of the LED. Our results also show that careful allocation of the IRS elements can lead to enhanced PLS even when the eavesdropper has a more favourable position and, thus, a better channel gain than the legitimate user.","sentences":["Indoor visible light communication (VLC) is considered secure against attackers outside the confined area where the light propagates, but it is still susceptible to interception from inside the coverage area.","A new technology, intelligent reflecting surfaces (IRS), has been recently introduced, offering a way to enhance physical layer security (PLS).","Most research on IRS-assisted VLC assumes the same time of arrival from all reflecting elements and overlooks the effect of time delay and the associated intersymbol interference.","This paper tackles, for the first time, the effect of time delay on the secrecy rate in VLC systems.","Our results show that, at a fixed light-emitting diode (LED) power of 3W, the secrecy rate can be enhanced by up to 253\\% at random positions for the legitimate user when the eavesdropper is located within a 1-meter radius of the LED.","Our results also show that careful allocation of the IRS elements can lead to enhanced PLS even when the eavesdropper has a more favourable position and, thus, a better channel gain than the legitimate user."],"url":"http://arxiv.org/abs/2402.03202v1","category":"cs.IT"}
{"created":"2024-02-05 16:56:11","title":"Unified Hallucination Detection for Multimodal Large Language Models","abstract":"Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.","sentences":["Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination.","The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment.","Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity.","In response to these challenges, our work expands the investigative horizons of hallucination detection.","We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods.","Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly.","We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis.","We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations."],"url":"http://arxiv.org/abs/2402.03190v1","category":"cs.CL"}
{"created":"2024-02-05 16:47:13","title":"Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning","abstract":"Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating the contributions among meta environments in the meta-model built, which fits better with the characteristic of configuration data that is known to dramatically differ between different environments. Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup. All code and data can be found at our repository: https://github.com/ideas-labo/SeMPL.","sentences":["Learning and predicting the performance of given software configurations are of high importance to many software engineering activities.","While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments.","In this paper, we target configuration performance learning under multiple environments.","We do so by designing SeMPL - a meta-learning framework that learns the common understanding from configurations measured in distinct (meta) environments and generalizes them to the unforeseen, target environment.","What makes it unique is that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train the meta environments in parallel, we train them sequentially, one at a time.","The order of training naturally allows discriminating the contributions among meta environments in the meta-model built, which fits better with the characteristic of configuration data that is known to dramatically differ between different environments.","Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup.","All code and data can be found at our repository: https://github.com/ideas-labo/SeMPL."],"url":"http://arxiv.org/abs/2402.03183v1","category":"cs.SE"}
{"created":"2024-02-05 16:46:16","title":"C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models","abstract":"Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.","sentences":["Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments.","Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored.","In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks.","We propose C-RAG, the first framework to certify generation risks for RAG models.","Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk.","We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts.","We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial.","Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models."],"url":"http://arxiv.org/abs/2402.03181v1","category":"cs.AI"}
{"created":"2024-02-05 16:43:53","title":"Comparison of Topic Modelling Approaches in the Banking Context","abstract":"Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.","sentences":["Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems.","The approach is vital for service industries to monitor their customer discussions.","The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document.","Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture.","We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches.","Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463."],"url":"http://arxiv.org/abs/2402.03176v1","category":"cs.IR"}
{"created":"2024-02-05 16:42:10","title":"The Matrix: A Bayesian learning model for LLMs","abstract":"In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications.","sentences":["In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs).","We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle.","Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix.","We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior.","Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated.","Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications."],"url":"http://arxiv.org/abs/2402.03175v1","category":"cs.LG"}
{"created":"2024-02-05 16:41:02","title":"Multi: Multimodal Understanding Leaderboard with Text and Images","abstract":"Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.","sentences":["Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community.","Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions.","This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests.","It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning.","Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats.","We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces.","Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%.","Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI."],"url":"http://arxiv.org/abs/2402.03173v1","category":"cs.CL"}
{"created":"2024-02-05 16:40:23","title":"Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings","abstract":"Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification.","sentences":["Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches.","This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work.","We specifically employ a strong Transformer-based model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently.","The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms.","Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance.","Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification."],"url":"http://arxiv.org/abs/2402.03172v1","category":"cs.CL"}
{"created":"2024-02-05 16:32:12","title":"Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories","abstract":"Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus. The most commonly used approach represents time by adding a real-valued fluent $\\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation. We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects. We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators. %that only allow comparisons against fixed rationals. With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable. Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence that is a successful execution of a given program.","sentences":["Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus.","The most commonly used approach represents time by adding a real-valued fluent $\\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation.","We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects.","We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators.","%that only allow comparisons against fixed rationals.","With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable.","Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence that is a successful execution of a given program."],"url":"http://arxiv.org/abs/2402.03164v1","category":"cs.AI"}
{"created":"2024-02-05 16:28:29","title":"State Dependent and Independent Uncertainty Relations for Skew Informations and Standard Deviations","abstract":"To understand the direct impact of noncommutativity of incompatible observables, the commutator of incompatible observables should be present explicitly in any uncertainty relation. The Robertson-Heisenberg uncertainty relation contains such commutator and thus implies that both the standard deviations of incompatible observables can not attain their respective minimum values (i.e., zero). In this work, we derive state dependent uncertainty relations (in which commutators are explicitly present) and state independent uncertainty relations based on Wigner-Yanase (-Dyson) skew information. Also we derive uncertainty equality based on Wigner-Yanase (-Dyson) skew information and standard deviation for mixed states. We show that for pure states, Wigner-Yanase skew information based state independent uncertainty relations become standard deviation based state independent uncertainty relations which turn out to be tighter uncertainty relations than the ones given in the work of Giorda et al. [Phys. Rev. A 99, 052121 (2019)] for some cases, and we generalize the work of Giorda et al. for arbitrary operators. State independent uncertainty relation for Wigner-Yanase skew informations of a collection of quantum channels is also derived. We show that state dependent and independent uncertainty relations based on a more general version of skew information called generalized skew information appeared in Yang et al. [Phys. Rev. A 106, 052401 (2022)] which includes the Wigner-Yanase (-Dyson) skew information and Fisher information as special cases hold. In a spin-1/2 system, we derive state independent uncertainty inequalities and equalities for different form of generalized skew informations and standard deviations, and discuss in details.","sentences":["To understand the direct impact of noncommutativity of incompatible observables, the commutator of incompatible observables should be present explicitly in any uncertainty relation.","The Robertson-Heisenberg uncertainty relation contains such commutator and thus implies that both the standard deviations of incompatible observables can not attain their respective minimum values (i.e., zero).","In this work, we derive state dependent uncertainty relations (in which commutators are explicitly present) and state independent uncertainty relations based on Wigner-Yanase (-Dyson) skew information.","Also we derive uncertainty equality based on Wigner-Yanase (-Dyson) skew information and standard deviation for mixed states.","We show that for pure states, Wigner-Yanase skew information based state independent uncertainty relations become standard deviation based state independent uncertainty relations which turn out to be tighter uncertainty relations than the ones given in the work of Giorda et al.","[Phys. Rev.","A 99, 052121 (2019)] for some cases, and we generalize the work of Giorda et al. for arbitrary operators.","State independent uncertainty relation for Wigner-Yanase skew informations of a collection of quantum channels is also derived.","We show that state dependent and independent uncertainty relations based on a more general version of skew information called generalized skew information appeared in Yang et al.","[Phys. Rev.","A 106, 052401 (2022)] which includes the Wigner-Yanase (-Dyson) skew information and Fisher information as special cases hold.","In a spin-1/2 system, we derive state independent uncertainty inequalities and equalities for different form of generalized skew informations and standard deviations, and discuss in details."],"url":"http://arxiv.org/abs/2402.03159v1","category":"quant-ph"}
{"created":"2024-02-05 16:24:12","title":"DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation","abstract":"This paper introduces DogSurf - a newapproach of using quadruped robots to help visually impaired people navigate in real world. The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop. A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots. A dataset was collected on a Unitree Go1 Edu robot. The dataset and code have been posted to the public domain.","sentences":["This paper introduces DogSurf - a newapproach of using quadruped robots to help visually impaired people navigate in real world.","The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop.","A state-of-the-art GRU-based neural network architecture with mean accuracy of 99.925% was proposed for the task of multiclass surface classification for quadruped robots.","A dataset was collected on a Unitree Go1 Edu robot.","The dataset and code have been posted to the public domain."],"url":"http://arxiv.org/abs/2402.03156v1","category":"cs.RO"}
{"created":"2024-02-05 16:13:54","title":"Detecting Scams Using Large Language Models","abstract":"Large Language Models (LLMs) have gained prominence in various applications, including security. This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity. Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams. We present notable security applications of LLMs and discuss the unique challenges posed by scams. Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems. Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails. The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks. The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats.","sentences":["Large Language Models (LLMs) have gained prominence in various applications, including security.","This paper explores the utility of LLMs in scam detection, a critical aspect of cybersecurity.","Unlike traditional applications, we propose a novel use case for LLMs to identify scams, such as phishing, advance fee fraud, and romance scams.","We present notable security applications of LLMs and discuss the unique challenges posed by scams.","Specifically, we outline the key steps involved in building an effective scam detector using LLMs, emphasizing data collection, preprocessing, model selection, training, and integration into target systems.","Additionally, we conduct a preliminary evaluation using GPT-3.5 and GPT-4 on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails.","The results demonstrate the models' effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks.","The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats."],"url":"http://arxiv.org/abs/2402.03147v1","category":"cs.CR"}
{"created":"2024-02-05 16:11:03","title":"Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task","abstract":"Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficiency and policy performance.","sentences":["Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays.","State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments.","To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments.","Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task.","We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task.","On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficiency and policy performance."],"url":"http://arxiv.org/abs/2402.03141v1","category":"cs.LG"}
{"created":"2024-02-05 16:08:58","title":"Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations","abstract":"In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.","sentences":["In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem.","We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant.","We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts.","Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations.","Overall, this presents a pathway for integrating pre-trained biases into exploration.","We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings."],"url":"http://arxiv.org/abs/2402.03138v1","category":"cs.LG"}
{"created":"2024-02-05 16:03:44","title":"Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games","abstract":"The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatross is able to exploit weak agents in the competitive game of Battlesnake. Additionally, it yields an improvement of 37.6% compared to previous state of the art in the cooperative Overcooked benchmark.","sentences":["The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go.","However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge.","In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all.","Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games.","To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play.","Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength.","We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games.","In contrast to AlphaZero, Albatross is able to exploit weak agents in the competitive game of Battlesnake.","Additionally, it yields an improvement of 37.6% compared to previous state of the art in the cooperative Overcooked benchmark."],"url":"http://arxiv.org/abs/2402.03136v1","category":"cs.AI"}
{"created":"2024-02-05 16:01:15","title":"GPU-Accelerated 3D Polygon Visibility Volumes for Synergistic Perception and Navigation","abstract":"UAV missions often require specific geometric constraints to be satisfied between ground locations and the vehicle location. Such requirements are typical for contexts where line-of-sight must be maintained between the vehicle location and the ground control location and are also important in surveillance applications where the UAV wishes to be able to sense, e.g., with a camera sensor, a specific region within a complex geometric environment. This problem is further complicated when the ground location is generalized to a convex 2D polygonal region. This article describes the theory and implementation of a system which can quickly calculate the 3D volume that encloses all 3D coordinates from which a 2D convex planar region can be entirely viewed; referred to as a visibility volume. The proposed approach computes visibility volumes using a combination of depth map computation using GPU-acceleration and geometric boolean operations. Solutions to this problem require complex 3D geometric analysis techniques that must execute using arbitrary precision arithmetic on a collection of discontinuous and non-analytic surfaces. Post-processing steps incorporate navigational constraints to further restrict the enclosed coordinates to include both visibility and navigation constraints. Integration of sensing visibility constraints with navigational constraints yields a range of navigable space where a vehicle will satisfy both perceptual sensing and navigational needs of the mission. This algorithm then provides a synergistic perception and navigation sensitive solution yielding a volume of coordinates in 3D that satisfy both the mission path and sensing needs.","sentences":["UAV missions often require specific geometric constraints to be satisfied between ground locations and the vehicle location.","Such requirements are typical for contexts where line-of-sight must be maintained between the vehicle location and the ground control location and are also important in surveillance applications where the UAV wishes to be able to sense, e.g., with a camera sensor, a specific region within a complex geometric environment.","This problem is further complicated when the ground location is generalized to a convex 2D polygonal region.","This article describes the theory and implementation of a system which can quickly calculate the 3D volume that encloses all 3D coordinates from which a 2D convex planar region can be entirely viewed; referred to as a visibility volume.","The proposed approach computes visibility volumes using a combination of depth map computation using GPU-acceleration and geometric boolean operations.","Solutions to this problem require complex 3D geometric analysis techniques that must execute using arbitrary precision arithmetic on a collection of discontinuous and non-analytic surfaces.","Post-processing steps incorporate navigational constraints to further restrict the enclosed coordinates to include both visibility and navigation constraints.","Integration of sensing visibility constraints with navigational constraints yields a range of navigable space where a vehicle will satisfy both perceptual sensing and navigational needs of the mission.","This algorithm then provides a synergistic perception and navigation sensitive solution yielding a volume of coordinates in 3D that satisfy both the mission path and sensing needs."],"url":"http://arxiv.org/abs/2402.03135v1","category":"cs.RO"}
{"created":"2024-02-05 15:56:19","title":"User-Centric Evaluation of ChatGPT Capability of Generating R Program Code","abstract":"This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters. Our experiments demonstrated that ChatGPT is in general highly capable of generating high quality R program code as well as textual explanations although it may fail on hard programming tasks. The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using ChatGPT to generate code.","sentences":["This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input.","A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs.","The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts.","In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed.","In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters.","Our experiments demonstrated that ChatGPT is in general highly capable of generating high quality R program code as well as textual explanations although it may fail on hard programming tasks.","The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using ChatGPT to generate code."],"url":"http://arxiv.org/abs/2402.03130v1","category":"cs.SE"}
{"created":"2024-02-05 15:50:36","title":"Behavioral transition of a fish school in a crowded environment","abstract":"In open water, social fish gather to form schools, in which fish generally align with each other. In this work, we study how this social behavior evolves when perturbed by artificial obstacles. We measure the collective behavior of a group of zebrafish in the presence of a periodic array of pillars. When pillar density is low, the fish regroup with a typical inter-distance and a well-polarized state with parallel orientations, similar to their behavior in open water conditions. Above a critical density of pillars, their social interactions, which are mostly based on vision, are screened and the fish spread randomly through the aquarium, orienting themselves along the free axes of the pillar lattice. The abrupt transition from natural to artificial orientation happens when the pillar inter-distance is comparable to the social distance of the fish, i.e., their most probable inter-distance. We develop a stochastic model of the relative orientation between fish pairs, taking into account alignment, anti-alignment and tumbling, from a distribution biased by the environment. This model provides a good description of the experimental probability distribution of the relative orientation between the fish and captures the behavioral transition. Using the model to fit the experimental data provides qualitative information on the evolution of cognitive parameters, such as the alignment or the tumbling rates, as the pillar density increases. At high pillar density, we find that the artificial environment imposes its geometrical constraints to the fish school, drastically increasing the tumbling rate.","sentences":["In open water, social fish gather to form schools, in which fish generally align with each other.","In this work, we study how this social behavior evolves when perturbed by artificial obstacles.","We measure the collective behavior of a group of zebrafish in the presence of a periodic array of pillars.","When pillar density is low, the fish regroup with a typical inter-distance and a well-polarized state with parallel orientations, similar to their behavior in open water conditions.","Above a critical density of pillars, their social interactions, which are mostly based on vision, are screened and the fish spread randomly through the aquarium, orienting themselves along the free axes of the pillar lattice.","The abrupt transition from natural to artificial orientation happens when the pillar inter-distance is comparable to the social distance of the fish, i.e., their most probable inter-distance.","We develop a stochastic model of the relative orientation between fish pairs, taking into account alignment, anti-alignment and tumbling, from a distribution biased by the environment.","This model provides a good description of the experimental probability distribution of the relative orientation between the fish and captures the behavioral transition.","Using the model to fit the experimental data provides qualitative information on the evolution of cognitive parameters, such as the alignment or the tumbling rates, as the pillar density increases.","At high pillar density, we find that the artificial environment imposes its geometrical constraints to the fish school, drastically increasing the tumbling rate."],"url":"http://arxiv.org/abs/2402.03123v1","category":"physics.bio-ph"}
{"created":"2024-02-05 15:47:54","title":"Good Teachers Explain: Explanation-Enhanced Knowledge Distillation","abstract":"Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations.","sentences":["Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models.","While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function.","It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers.","In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student.","Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with 'approximate', pre-computed explanations."],"url":"http://arxiv.org/abs/2402.03119v1","category":"cs.CV"}
{"created":"2024-02-05 15:44:43","title":"Infrared Spectra Prediction for Diazo Groups Utilizing a Machine Learning Approach with Structural Attention Mechanism","abstract":"Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions. However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges. Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds. Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions. This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions.","sentences":["Infrared (IR) spectroscopy is a pivotal technique in chemical research for elucidating molecular structures and dynamics through vibrational and rotational transitions.","However, the intricate molecular fingerprints characterized by unique vibrational and rotational patterns present substantial analytical challenges.","Here, we present a machine learning approach employing a Structural Attention Mechanism tailored to enhance the prediction and interpretation of infrared spectra, particularly for diazo compounds.","Our model distinguishes itself by honing in on chemical information proximal to functional groups, thereby significantly bolstering the accuracy, robustness, and interpretability of spectral predictions.","This method not only demystifies the correlations between infrared spectral features and molecular structures but also offers a scalable and efficient paradigm for dissecting complex molecular interactions."],"url":"http://arxiv.org/abs/2402.03112v1","category":"cs.LG"}
{"created":"2024-02-05 15:38:01","title":"Non-Stationary Latent Auto-Regressive Bandits","abstract":"We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\\tilde{O}(k\\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.","sentences":["We consider the stochastic multi-armed bandit problem with non-stationary rewards.","We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$.","We call this new environment the latent AR bandit.","Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment.","If the AR order $k$ is known, we propose an algorithm that achieves $\\tilde{O}(k\\sqrt{T})$ regret in this setting.","Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified."],"url":"http://arxiv.org/abs/2402.03110v1","category":"cs.LG"}
{"created":"2024-02-05 15:36:03","title":"Perceived Vulnerability to Disease Scale: Factorial structure, reliability, and validity in times of Portugal's COVID-19 pandemic lockdown","abstract":"The present study examines the factor structure of a Portuguese version of the Perceived Vulnerability to Disease Scale (PVD), designed to assess individual differences in chronic concerns about transmission of infectious diseases. Method: Data from a Portuguese convenience sample (n=1203), collected during the first Covid-19 pandemic lockdown. Results: the scale revealed, through an exploratory factor analysis (EFA) and a confirmatory factor analysis (CFA), a slight superiority of a three-factor model over the existing two-factor models of the 15-item original PVD and of the 10-item PVD established with another Portuguese sample (Ferreira et al., 2022). Conclusions: This higher level of differentiation in terms of a perceived resistance to infectious diseases could be explained by the pandemic context which may have differentiated the responses regarding the perception of Resistance. On the other hand, this new factor increases the comprehensive and evaluative dimension and implications of the construct assessed by PVD.","sentences":["The present study examines the factor structure of a Portuguese version of the Perceived Vulnerability to Disease Scale (PVD), designed to assess individual differences in chronic concerns about transmission of infectious diseases.","Method: Data from a Portuguese convenience sample (n=1203), collected during the first Covid-19 pandemic lockdown.","Results: the scale revealed, through an exploratory factor analysis (EFA) and a confirmatory factor analysis (CFA), a slight superiority of a three-factor model over the existing two-factor models of the 15-item original PVD and of the 10-item PVD established with another Portuguese sample (Ferreira et al., 2022).","Conclusions: This higher level of differentiation in terms of a perceived resistance to infectious diseases could be explained by the pandemic context which may have differentiated the responses regarding the perception of Resistance.","On the other hand, this new factor increases the comprehensive and evaluative dimension and implications of the construct assessed by PVD."],"url":"http://arxiv.org/abs/2402.03108v1","category":"stat.AP"}
{"created":"2024-02-05 15:33:34","title":"Reply to: Effects of density and temperature variations on the metallicity of Mrk 71","abstract":"In Chen et al., 2023 (C23; arXiv:2304.09898), we introduced a new method to directly measure temperature fluctuations and applied it to a nearby dwarf galaxy, Mrk 71, finding a temperature fluctuation parameter $t^2 = 0.008\\pm 0.043$. This result is lower by $\\sim 2\\sigma$ than the value required to explain the abundance discrepancy (AD) in this object. In the Matters Arising article submitted by Mendez-Delgado et al. (arXiv:2310.01197), the authors claim that using the same data presented in C23 in a different way, it is possible to conclude that the measurements are consistent with a larger $t^2 \\simeq 0.1$ inferred indirectly from recombination lines (RLs). However, this requires a higher density such that the infrared [O III] 52 $\\mu$m and [O III] 88 $\\mu$m lines -- which form the basis of the direct measurement method -- are mutually inconsistent. Moreover, to reach agreement between the direct $t^2$ measurement and the larger $t^2$ value inferred from RLs requires systematically varying four parameters by $\\sim 1\\sigma$ from their best-determined values, which collectively amount to a $\\sim2\\sigma$ difference, consistent with the significance ($\\sim 2 \\sigma$) originally reported in C23. Therefore, we conclude that the results of C23 hold, and that the combined optical and infrared [O III] data disfavour $t^2 \\simeq 0.1$ at the $\\approx2\\sigma$ level in Mrk 71. Future work is nonetheless warranted to better understand the AD associated with both optical and infrared emission line analysis.","sentences":["In Chen et al., 2023 (C23; arXiv:2304.09898), we introduced a new method to directly measure temperature fluctuations and applied it to a nearby dwarf galaxy, Mrk 71, finding a temperature fluctuation parameter $t^2 = 0.008\\pm 0.043$. This result is lower by $\\sim 2\\sigma$ than the value required to explain the abundance discrepancy (AD) in this object.","In the Matters Arising article submitted by Mendez-Delgado et al. (arXiv:2310.01197), the authors claim that using the same data presented in C23 in a different way, it is possible to conclude that the measurements are consistent with a larger $t^2 \\simeq 0.1$ inferred indirectly from recombination lines (RLs).","However, this requires a higher density such that the infrared [O III] 52 $\\mu$m and [O III] 88","$\\mu$m lines -- which form the basis of the direct measurement method -- are mutually inconsistent.","Moreover, to reach agreement between the direct $t^2$ measurement and the larger $t^2$ value inferred from RLs requires systematically varying four parameters by $\\sim 1\\sigma$ from their best-determined values, which collectively amount to a $\\sim2\\sigma$ difference, consistent with the significance ($\\sim 2 \\sigma$) originally reported in C23.","Therefore, we conclude that the results of C23 hold, and that the combined optical and infrared [O III] data disfavour $t^2 \\simeq 0.1$ at the $\\approx2\\sigma$ level in Mrk 71.","Future work is nonetheless warranted to better understand the AD associated with both optical and infrared emission line analysis."],"url":"http://arxiv.org/abs/2402.03105v1","category":"astro-ph.GA"}
{"created":"2024-02-06 18:51:47","title":"Invariant Set Estimation for Piecewise Affine Dynamical Systems Using Piecewise Affine Barrier Function","abstract":"This paper introduces an algorithm for approximating the invariant set of closed-loop controlled dynamical systems identified using ReLU neural networks or piecewise affine PWA functions, particularly addressing the challenge of providing safety guarantees for ReLU networks commonly used in safety-critical applications. The invariant set of PWA dynamical system is estimated using ReLU networks or its equivalent PWA function. This method entails formulating the barrier function as a PWA function and converting the search process into a linear optimization problem using vertices. We incorporate a domain refinement strategy to increase flexibility in case the optimization does not find a valid barrier function. Moreover, the objective of optimization is to maximize the invariant set based on the current partition. Our experimental results demonstrate the effectiveness and efficiency of our approach, demonstrating its potential for ensuring the safety of PWA dynamical systems.","sentences":["This paper introduces an algorithm for approximating the invariant set of closed-loop controlled dynamical systems identified using ReLU neural networks or piecewise affine PWA functions, particularly addressing the challenge of providing safety guarantees for ReLU networks commonly used in safety-critical applications.","The invariant set of PWA dynamical system is estimated using ReLU networks or its equivalent PWA function.","This method entails formulating the barrier function as a PWA function and converting the search process into a linear optimization problem using vertices.","We incorporate a domain refinement strategy to increase flexibility in case the optimization does not find a valid barrier function.","Moreover, the objective of optimization is to maximize the invariant set based on the current partition.","Our experimental results demonstrate the effectiveness and efficiency of our approach, demonstrating its potential for ensuring the safety of PWA dynamical systems."],"url":"http://arxiv.org/abs/2402.04243v1","category":"eess.SY"}
{"created":"2024-02-06 18:49:51","title":"Algebraic identifiability of partial differential equation models","abstract":"Differential equation models are crucial to scientific processes. The values of model parameters are important for analyzing the behaviour of solutions. A parameter is called globally identifiable if its value can be uniquely determined from the input and output functions. To determine if a parameter estimation problem is well-posed for a given model, one must check if the model parameters are globally identifiable. This problem has been intensively studied for ordinary differential equation models, with theory and several efficient algorithms and software packages developed. A comprehensive theory of algebraic identifiability for PDEs has hitherto not been developed due to the complexity of initial and boundary conditions. Here, we provide theory and algorithms, based on differential algebra, for testing identifiability of polynomial PDE models. We showcase this approach on PDE models arising in the sciences.","sentences":["Differential equation models are crucial to scientific processes.","The values of model parameters are important for analyzing the behaviour of solutions.","A parameter is called globally identifiable if its value can be uniquely determined from the input and output functions.","To determine if a parameter estimation problem is well-posed for a given model, one must check if the model parameters are globally identifiable.","This problem has been intensively studied for ordinary differential equation models, with theory and several efficient algorithms and software packages developed.","A comprehensive theory of algebraic identifiability for PDEs has hitherto not been developed due to the complexity of initial and boundary conditions.","Here, we provide theory and algorithms, based on differential algebra, for testing identifiability of polynomial PDE models.","We showcase this approach on PDE models arising in the sciences."],"url":"http://arxiv.org/abs/2402.04241v1","category":"q-bio.QM"}
{"created":"2024-02-06 18:33:15","title":"Estimation of the precision of a structured light system in oil paintings on canvas","abstract":"The conservation and authentication of pictorial artworks is considered an important part of the preservation of the cultural heritage. The use of non-destructive testing allows the obtaining of accurate information about the state of pictorial artworks, without direct contact between the equipment used and the sample. In particular, the use of this kind of technology is recommended in obtaining three-dimensional surface digital models, as it provides high-resolution information that constitutes a kind of fingerprint of the samples. In the case of pictorial artworks with some kind of surface relief, one of the most useful technologies is structured light (SL). In this paper the minimum difference in height that can be distinguished with this technology is estimated, establishing experimentally both the error committed in the measurement process and the precision in the use of this technology. The study, focused on the case of oil paintings on canvas, has been developed using a low-cost system to ensure its wide use.","sentences":["The conservation and authentication of pictorial artworks is considered an important part of the preservation of the cultural heritage.","The use of non-destructive testing allows the obtaining of accurate information about the state of pictorial artworks, without direct contact between the equipment used and the sample.","In particular, the use of this kind of technology is recommended in obtaining three-dimensional surface digital models, as it provides high-resolution information that constitutes a kind of fingerprint of the samples.","In the case of pictorial artworks with some kind of surface relief, one of the most useful technologies is structured light (SL).","In this paper the minimum difference in height that can be distinguished with this technology is estimated, establishing experimentally both the error committed in the measurement process and the precision in the use of this technology.","The study, focused on the case of oil paintings on canvas, has been developed using a low-cost system to ensure its wide use."],"url":"http://arxiv.org/abs/2402.04223v1","category":"physics.app-ph"}
{"created":"2024-02-06 18:23:41","title":"First-principles phase diagram of an interacting ionic chain","abstract":"The widely-used Kohn-Sham implementation of density functional theory (DFT) maps a system of interacting electrons onto an auxiliary non-interacting one and is presumably inaccurate for strongly correlated materials. We present a concrete benchmark for DFT by examining the electronic ground state phase diagram of a strongly interacting chain with uneven, non-integer nuclear charges. The interplay between charge imbalance and Coulomb repulsion yields two competing phases, a band insulator and a Mott insulator. By including infinitesimal lattice distortions, DFT stabilizes the intermediate spontaneously dimerized insulator phase that results from this competition. We assess the phase diagram by mapping the bond length and nuclear charge ratio of the chain to the ionic Hubbard model and performing highly accurate density matrix renormalization group calculations. Our study demonstrates that explicit symmetry breaking can be used as a tool to predict phase diagrams with competing orders from DFT.","sentences":["The widely-used Kohn-Sham implementation of density functional theory (DFT) maps a system of interacting electrons onto an auxiliary non-interacting one and is presumably inaccurate for strongly correlated materials.","We present a concrete benchmark for DFT by examining the electronic ground state phase diagram of a strongly interacting chain with uneven, non-integer nuclear charges.","The interplay between charge imbalance and Coulomb repulsion yields two competing phases, a band insulator and a Mott insulator.","By including infinitesimal lattice distortions, DFT stabilizes the intermediate spontaneously dimerized insulator phase that results from this competition.","We assess the phase diagram by mapping the bond length and nuclear charge ratio of the chain to the ionic Hubbard model and performing highly accurate density matrix renormalization group calculations.","Our study demonstrates that explicit symmetry breaking can be used as a tool to predict phase diagrams with competing orders from DFT."],"url":"http://arxiv.org/abs/2402.04218v1","category":"cond-mat.str-el"}
{"created":"2024-02-06 18:17:02","title":"Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks","abstract":"Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm. Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints. Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines.","sentences":["Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks.","Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs).","As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands.","Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests.","A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content.","Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm.","Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints.","Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines."],"url":"http://arxiv.org/abs/2402.04216v1","category":"cs.NI"}
{"created":"2024-02-06 18:01:05","title":"On the growing length scale in a replica-coupled glassforming liquid","abstract":"Computer simulations are used to study a three-dimensional polydisperse model glassformer in a replica-coupling setup where an attractive field $\\propto - \\varepsilon Q$ of strength $\\varepsilon$ can adjust the similarity of the system to a fixed reference configuration with the overlap parameter $Q$. The polydispersity in the model enables the efficient use of swap Monte Carlo in combination with molecular-dynamics simulation from which we obtain fully equilibrated liquid configurations at very low temperature, i.e., far below the critical temperature of mode-coupling theory, $T_{\\rm MCT}$. When the $\\varepsilon$-field is switched on, the fast dynamics with swaps allow relaxation to the stationary state at temperatures below $T_{\\rm MCT}$. In the stationary state, the overlap $Q$ has a finite value that increases with increasing $\\varepsilon$. For a given temperature $T$, fluctuations of the overlap around the average value become maximal at a critical field strength $\\varepsilon^\\star(T)$. With decreasing $T$ along this $\\varepsilon^\\star(T)$-line, overlap fluctuations increase and a transition from a unimodal overlap distribution to a bimodal shape occurs. We give evidence that these bimodal distributions are not due to first-order phase transitions. However, they reflect finite-size effects due to a rapidly growing length scale with decreasing temperature. We discuss the significance of this length scale for the understanding of the glass transition.","sentences":["Computer simulations are used to study a three-dimensional polydisperse model glassformer in a replica-coupling setup where an attractive field $\\propto - \\varepsilon Q$ of strength $\\varepsilon$ can adjust the similarity of the system to a fixed reference configuration with the overlap parameter $Q$. The polydispersity in the model enables the efficient use of swap Monte Carlo in combination with molecular-dynamics simulation from which we obtain fully equilibrated liquid configurations at very low temperature, i.e., far below the critical temperature of mode-coupling theory, $T_{\\rm MCT}$. When the $\\varepsilon$-field is switched on, the fast dynamics with swaps allow relaxation to the stationary state at temperatures below $T_{\\rm MCT}$. In the stationary state, the overlap $Q$ has a finite value that increases with increasing $\\varepsilon$. For a given temperature $T$, fluctuations of the overlap around the average value become maximal at a critical field strength $\\varepsilon^\\star(T)$. With decreasing $T$ along this $\\varepsilon^\\star(T)$-line, overlap fluctuations increase and a transition from a unimodal overlap distribution to a bimodal shape occurs.","We give evidence that these bimodal distributions are not due to first-order phase transitions.","However, they reflect finite-size effects due to a rapidly growing length scale with decreasing temperature.","We discuss the significance of this length scale for the understanding of the glass transition."],"url":"http://arxiv.org/abs/2402.04205v1","category":"cond-mat.soft"}
{"created":"2024-02-06 17:55:42","title":"Information Systems and Software Engineering: The Case for Convergence","abstract":"The Information Systems (IS) and Software Engineering (SE) fields share a remarkable number of similarities in their historical evolution to date. These similarities are briefly outlined below. An analysis of 10 years (2001-2010) of publications in the primary journals in both fields also reveals a good deal of overlap in research topics. Given the challenges faced by both as young disciplines, there is potentially much to gain from a closer interaction between both fields than has traditionally been the case. This article seeks to encourage such interaction, and illustrates how this might usefully occur in the area of design. It concludes by proposing a number of practical initiatives that could stimulate and facilitate interaction between the IS and SE fields","sentences":["The Information Systems (IS) and Software Engineering (SE) fields share a remarkable number of similarities in their historical evolution to date.","These similarities are briefly outlined below.","An analysis of 10 years (2001-2010) of publications in the primary journals in both fields also reveals a good deal of overlap in research topics.","Given the challenges faced by both as young disciplines, there is potentially much to gain from a closer interaction between both fields than has traditionally been the case.","This article seeks to encourage such interaction, and illustrates how this might usefully occur in the area of design.","It concludes by proposing a number of practical initiatives that could stimulate and facilitate interaction between the IS and SE fields"],"url":"http://arxiv.org/abs/2402.04200v1","category":"cs.SE"}
{"created":"2024-02-06 17:50:30","title":"Instance by Instance: An Iterative Framework for Multi-instance 3D Registration","abstract":"Multi-instance registration is a challenging problem in computer vision and robotics, where multiple instances of an object need to be registered in a standard coordinate system. In this work, we propose the first iterative framework called instance-by-instance (IBI) for multi-instance 3D registration (MI-3DReg). It successively registers all instances in a given scenario, starting from the easiest and progressing to more challenging ones. Throughout the iterative process, outliers are eliminated continuously, leading to an increasing inlier rate for the remaining and more challenging instances. Under the IBI framework, we further propose a sparse-to-dense-correspondence-based multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg. Experiments on the synthetic and real datasets have demonstrated the effectiveness of IBI and suggested the new state-of-the-art performance of IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing state-of-the-art method ECC on the synthetic/real datasets.","sentences":["Multi-instance registration is a challenging problem in computer vision and robotics, where multiple instances of an object need to be registered in a standard coordinate system.","In this work, we propose the first iterative framework called instance-by-instance (IBI) for multi-instance 3D registration (MI-3DReg).","It successively registers all instances in a given scenario, starting from the easiest and progressing to more challenging ones.","Throughout the iterative process, outliers are eliminated continuously, leading to an increasing inlier rate for the remaining and more challenging instances.","Under the IBI framework, we further propose a sparse-to-dense-correspondence-based multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg.","Experiments on the synthetic and real datasets have demonstrated the effectiveness of IBI and suggested the new state-of-the-art performance of IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing state-of-the-art method ECC on the synthetic/real datasets."],"url":"http://arxiv.org/abs/2402.04195v1","category":"cs.CV"}
{"created":"2024-02-06 17:49:20","title":"Rotational holographic transport from KN-AdS black hole","abstract":"We consider rotational holographic transport in strongly coupled 2+1 dimensional systems, from the point of view of 3+1 dimensional gravity. We consider the moment of inertia $I$ as a kind of transport coefficient, identified with the moment of inertia of a charged rotating black hole in $AdS_4$ background. In the low-temperature region, we find the behaviour of the density $I/A$ with temperature $T$ and angular velocity $\\Omega$, and find a quadratic behaviour for $\\partial(I/A)/\\partial\\Omega$ with $T$, in the presence of some charge $Q$.","sentences":["We consider rotational holographic transport in strongly coupled 2+1 dimensional systems, from the point of view of 3+1 dimensional gravity.","We consider the moment of inertia $I$ as a kind of transport coefficient, identified with the moment of inertia of a charged rotating black hole in $AdS_4$ background.","In the low-temperature region, we find the behaviour of the density $I/A$ with temperature $T$ and angular velocity $\\Omega$, and find a quadratic behaviour for $\\partial(I/A)/\\partial\\Omega$ with $T$, in the presence of some charge $Q$."],"url":"http://arxiv.org/abs/2402.04194v1","category":"hep-th"}
{"created":"2024-02-06 17:36:35","title":"Scattering images from autocorrelation functions of P-wave seismic velocity images: the case of Tenerife Island (Canary Islands, Spain)","abstract":"We present a P-wave scattering image of the volcanic structures under Tenerife Island using the autocorrelation functions of P-wave vertical velocity fluctuations. We have applied cluster analysis to total quality factor attenuation (Q inv t) and scattering quality factor attenuation (Q inv PSc) images to interpret the structures in terms of intrinsic and scattering attenuation variations on a 2D plane, corresponding to a depth of 2000 m, and check the robustness of the scattering imaging. The results show that scattering patterns are similar to total attenuation patterns in the South of the island. There are two main areas where patterns differ: at Ca\\~nadas-Teide-Pico Viejo Complex high total attenuation and average-to-low scattering values are observed. We interpret the difference as induced by intrinsic attenuation. In the Santiago Ridge Zone (SRZ) region, high scattering values correspond to average total attenuation. In our interpretation, the anomaly is induced by an extended scatterer, geometrically related to the surficial traces of Garachico and El Chinyero historical eruptions and the area of highest seismic activity during the 2004-2008 seismic crises.","sentences":["We present a P-wave scattering image of the volcanic structures under Tenerife Island using the autocorrelation functions of P-wave vertical velocity fluctuations.","We have applied cluster analysis to total quality factor attenuation (Q inv t) and scattering quality factor attenuation (Q inv PSc) images to interpret the structures in terms of intrinsic and scattering attenuation variations on a 2D plane, corresponding to a depth of 2000 m, and check the robustness of the scattering imaging.","The results show that scattering patterns are similar to total attenuation patterns in the South of the island.","There are two main areas where patterns differ: at Ca\\~nadas-Teide-Pico Viejo Complex high total attenuation and average-to-low scattering values are observed.","We interpret the difference as induced by intrinsic attenuation.","In the Santiago Ridge Zone (SRZ) region, high scattering values correspond to average total attenuation.","In our interpretation, the anomaly is induced by an extended scatterer, geometrically related to the surficial traces of Garachico and El Chinyero historical eruptions and the area of highest seismic activity during the 2004-2008 seismic crises."],"url":"http://arxiv.org/abs/2402.04179v1","category":"physics.geo-ph"}
{"created":"2024-02-06 17:29:34","title":"Constrained curve fitting for semi-parametric models with radial basis function networks","abstract":"Common to many analysis pipelines in lattice gauge theory and the broader scientific discipline is the need to fit a semi-parametric model to data. We propose a fit method that utilizes a radial basis function network to approximate the non-parametric component of such models. The approximate parametric model is fit to data using the basin hopping global optimization algorithm. Parameter constraints are enforced through Gaussian priors. The viability of our method is tested by examining its use in a finite-size scaling analysis of the $q$-state Potts model and $p$-state clock model with $q=2,3$ and $p=4,\\infty$.","sentences":["Common to many analysis pipelines in lattice gauge theory and the broader scientific discipline is the need to fit a semi-parametric model to data.","We propose a fit method that utilizes a radial basis function network to approximate the non-parametric component of such models.","The approximate parametric model is fit to data using the basin hopping global optimization algorithm.","Parameter constraints are enforced through Gaussian priors.","The viability of our method is tested by examining its use in a finite-size scaling analysis of the $q$-state Potts model and $p$-state clock model with $q=2,3$ and $p=4,\\infty$."],"url":"http://arxiv.org/abs/2402.04175v1","category":"hep-lat"}
{"created":"2024-02-06 17:26:18","title":"3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN","abstract":"This study introduces the 3D Residual-in-Residual Dense Block GAN (3D RRDB-GAN) for 3D super-resolution for radiology imagery. A key aspect of 3D RRDB-GAN is the integration of a 2.5D perceptual loss function, which contributes to improved volumetric image quality and realism. The effectiveness of our model was evaluated through 4x super-resolution experiments across diverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6. These evaluations, encompassing both quantitative metrics like LPIPS and FID and qualitative assessments through sample visualizations, demonstrate the models effectiveness in detailed image analysis. The 3D RRDB-GAN offers a significant contribution to medical imaging, particularly by enriching the depth, clarity, and volumetric detail of medical images. Its application shows promise in enhancing the interpretation and analysis of complex medical imagery from a comprehensive 3D perspective.","sentences":["This study introduces the 3D Residual-in-Residual Dense Block GAN (3D RRDB-GAN) for 3D super-resolution for radiology imagery.","A key aspect of 3D RRDB-GAN is the integration of a 2.5D perceptual loss function, which contributes to improved volumetric image quality and realism.","The effectiveness of our model was evaluated through 4x super-resolution experiments across diverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6.","These evaluations, encompassing both quantitative metrics like LPIPS and FID and qualitative assessments through sample visualizations, demonstrate the models effectiveness in detailed image analysis.","The 3D RRDB-GAN offers a significant contribution to medical imaging, particularly by enriching the depth, clarity, and volumetric detail of medical images.","Its application shows promise in enhancing the interpretation and analysis of complex medical imagery from a comprehensive 3D perspective."],"url":"http://arxiv.org/abs/2402.04171v1","category":"eess.IV"}
{"created":"2024-02-06 17:24:06","title":"Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions","abstract":"Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.","sentences":["Reinforcement Learning is a highly active research field with promising advancements.","In the field of autonomous driving, however, often very simple scenarios are being examined.","Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure.","In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source.","We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions.","Our method is applicable to arbitrary RL models.","We successfully demonstrate high completion rates of complex scenarios with recent model-based agents."],"url":"http://arxiv.org/abs/2402.04168v1","category":"cs.LG"}
{"created":"2024-02-06 17:22:52","title":"Estimates for oscillatory integrals with phase having $D$ type singularities","abstract":"In this paper, we consider estimates for the two-dimensional oscillatory integrals. The phase function of the oscillatory integrals is the linear perturbation of a function having $D$ type singularities. We consider estimates for the oscillatory integrals in terms of the Randol's type maximal functions. We obtain a sharp $L^p_{loc}$ estimates for the Randol's maximal functions. Moreover, we investigate the sharp exponent $p$ depending on whether, the phase function has linearly adapted coordinates system or not.","sentences":["In this paper, we consider estimates for the two-dimensional oscillatory integrals.","The phase function of the oscillatory integrals is the linear perturbation of a function having $D$ type singularities.","We consider estimates for the oscillatory integrals in terms of the Randol's type maximal functions.","We obtain a sharp $L^p_{loc}$ estimates for the Randol's maximal functions.","Moreover, we investigate the sharp exponent $p$ depending on whether, the phase function has linearly adapted coordinates system or not."],"url":"http://arxiv.org/abs/2402.04167v1","category":"math.CA"}
{"created":"2024-02-06 17:16:46","title":"Optimal transport in the frame of abstract Lax-Oleinik operator revisited","abstract":"This is our first paper on the extension of our recent work on the Lax-Oleinik commutators and its applications to the intrinsic approach of propagation of singularities of the viscosity solutions of Hamilton-Jacobi equations. We reformulate Kantorovich-Rubinstein duality theorem in the theory of optimal transport in terms of abstract Lax-Oleinik operators, and analyze the relevant optimal transport problem in the case the cost function $c(x,y)=h(t_1,t_2,x,y)$ is the fundamental solution of Hamilton-Jacobi equation. For further applications to the problem of cut locus and propagation of singularities in optimal transport, we introduce corresponding random Lax-Oleinik operators. We also study the problem of singularities for $c$-concave functions and its dynamical implication when $c$ is the fundamental solution with $t_2-t_1\\ll1$ and $t_2-t_1<\\infty$, and $c$ is the Peierls' barrier respectively.","sentences":["This is our first paper on the extension of our recent work on the Lax-Oleinik commutators and its applications to the intrinsic approach of propagation of singularities of the viscosity solutions of Hamilton-Jacobi equations.","We reformulate Kantorovich-Rubinstein duality theorem in the theory of optimal transport in terms of abstract Lax-Oleinik operators, and analyze the relevant optimal transport problem in the case the cost function $c(x,y)=h(t_1,t_2,x,y)$ is the fundamental solution of Hamilton-Jacobi equation.","For further applications to the problem of cut locus and propagation of singularities in optimal transport, we introduce corresponding random Lax-Oleinik operators.","We also study the problem of singularities for $c$-concave functions and its dynamical implication when $c$ is the fundamental solution with $t_2-t_1\\ll1$ and $t_2-t_1<\\infty$, and $c$ is the Peierls' barrier respectively."],"url":"http://arxiv.org/abs/2402.04159v1","category":"math.AP"}
{"created":"2024-02-06 17:13:51","title":"Controller synthesis for input-state data with measurement errors","abstract":"We consider the problem of designing a state-feedback controller for a linear system, based only on noisy input-state data. We focus on input-state data corrupted by additive measurement errors, which, albeit less investigated, are as relevant as process disturbances in applications. For energy and instantaneous bounds on these measurement errors, we derive linear matrix inequalities for controller design where the one for the energy bound is actually equivalent to robust stabilization of all systems consistent with the noisy data points.","sentences":["We consider the problem of designing a state-feedback controller for a linear system, based only on noisy input-state data.","We focus on input-state data corrupted by additive measurement errors, which, albeit less investigated, are as relevant as process disturbances in applications.","For energy and instantaneous bounds on these measurement errors, we derive linear matrix inequalities for controller design where the one for the energy bound is actually equivalent to robust stabilization of all systems consistent with the noisy data points."],"url":"http://arxiv.org/abs/2402.04157v1","category":"eess.SY"}
{"created":"2024-02-06 16:50:30","title":"On the fate of slow boulders ejected after DART impact on Dimorphos","abstract":"On 2022 September 26th, 23:14 UT the NASA/DART (Double Asteroid Redirection Test) spacecraft successfully impacted Dimorphos, the secondary component of the binary (65803) Didymos system, demonstrating asteroid orbit deflection for the first time. A large amount of debris, consisting on a wide size frequency distribution of particulates (from micron-sized dust to meter-sized boulders), was released, and a long-lasting tail has been observed over more than 9 months since impact. An important fraction of the ejecta mass has been ejected as individual meter-sized boulders, as have been found in images obtained by the Light Italian CubeSat for Imaging of Asteroid (LICIACube), as well as from the Hubble Space Telescope (HST). While the boulders observed by LICIACube had projected speeds of several tens of meter per second, those seen by the HST were about one hundred time slower. In this paper we analyze the long-term orbital evolution of those slow boulders using different dynamical codes, providing constraints on the fate of such large particles, and giving insight on the possibility of observing some of those boulders that might remain in orbit at the time of the ESA/Hera mission arrival to the binary system in late 2026.","sentences":["On 2022 September 26th, 23:14 UT the NASA/DART (Double Asteroid Redirection Test) spacecraft successfully impacted Dimorphos, the secondary component of the binary (65803)","Didymos system, demonstrating asteroid orbit deflection for the first time.","A large amount of debris, consisting on a wide size frequency distribution of particulates (from micron-sized dust to meter-sized boulders), was released, and a long-lasting tail has been observed over more than 9 months since impact.","An important fraction of the ejecta mass has been ejected as individual meter-sized boulders, as have been found in images obtained by the Light Italian CubeSat for Imaging of Asteroid (LICIACube), as well as from the Hubble Space Telescope (HST).","While the boulders observed by LICIACube had projected speeds of several tens of meter per second, those seen by the HST were about one hundred time slower.","In this paper we analyze the long-term orbital evolution of those slow boulders using different dynamical codes, providing constraints on the fate of such large particles, and giving insight on the possibility of observing some of those boulders that might remain in orbit at the time of the ESA/Hera mission arrival to the binary system in late 2026."],"url":"http://arxiv.org/abs/2402.04145v1","category":"astro-ph.EP"}
{"created":"2024-02-06 16:49:23","title":"Quantitative Predictive Theories through Integrating Quantum, Statistical, and Equilibrium, and Nonequilibrium Thermodynamics","abstract":"Today's thermodynamics is largely based on the combined law for equilibrium systems and statistical mechanics derived by Gibbs in 1873 and 1901, respectively, while irreversible thermodynamics for nonequilibrium systems resides essentially on the Onsager Theorem as a separate branch of thermodynamics developed in 1930s. Between them, quantum mechanics was invented and was quantitatively solved in terms of density functional theory (DFT) in 1960s. These three scientific domains operate based on different principles and are very much separated from each other. In analogy to the parable of the blind men and the elephant articulated by Perdew, they individually represent different portions of a complex system and thus are incomplete by themselves alone, resulting in the lack of quantitative agreement between their predictions and experimental observations. Over the last two decades, the author's group has developed a multiscale entropy approach (recently termed as zentropy theory) that integrates DFT-based quantum mechanics and Gibbs statistical mechanics and is capable of accurately predicting entropy and free energy of complex systems. Furthermore, in combination with the combined law for nonequilibrium systems developed by Hillert, the author developed the theory of cross phenomena beyond the phenomenological Onsager Theorem. The zentropy theory and theory of cross phenomena jointly provide quantitative predictive theories for experimental observables as reviewed in the present work.","sentences":["Today's thermodynamics is largely based on the combined law for equilibrium systems and statistical mechanics derived by Gibbs in 1873 and 1901, respectively, while irreversible thermodynamics for nonequilibrium systems resides essentially on the Onsager Theorem as a separate branch of thermodynamics developed in 1930s.","Between them, quantum mechanics was invented and was quantitatively solved in terms of density functional theory (DFT) in 1960s.","These three scientific domains operate based on different principles and are very much separated from each other.","In analogy to the parable of the blind men and the elephant articulated by Perdew, they individually represent different portions of a complex system and thus are incomplete by themselves alone, resulting in the lack of quantitative agreement between their predictions and experimental observations.","Over the last two decades, the author's group has developed a multiscale entropy approach (recently termed as zentropy theory) that integrates DFT-based quantum mechanics and Gibbs statistical mechanics and is capable of accurately predicting entropy and free energy of complex systems.","Furthermore, in combination with the combined law for nonequilibrium systems developed by Hillert, the author developed the theory of cross phenomena beyond the phenomenological Onsager Theorem.","The zentropy theory and theory of cross phenomena jointly provide quantitative predictive theories for experimental observables as reviewed in the present work."],"url":"http://arxiv.org/abs/2402.04143v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-06 16:46:28","title":"U-shaped Vision Mamba for Single Image Dehazing","abstract":"Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \\url{https://github.com/zzr-idam}.","sentences":["Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices.","To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network.","Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies.","Extensive experimental results demonstrate the effectiveness of our method.","Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks.","The URL of the code is \\url{https://github.com/zzr-idam}."],"url":"http://arxiv.org/abs/2402.04139v1","category":"cs.CV"}
{"created":"2024-02-06 16:40:52","title":"Reducing two-level system dissipations in 3D superconducting Niobium resonators by atomic layer deposition and high temperature heat treatment","abstract":"Superconducting qubits have arisen as a leading technology platform for quantum computing which is on the verge of revolutionizing the world's calculation capacities. Nonetheless, the fabrication of computationally reliable qubit circuits requires increasing the quantum coherence lifetimes, which are predominantly limited by the dissipations of two-level system (TLS) defects present in the thin superconducting film and the adjacent dielectric regions. In this paper, we demonstrate the reduction of two-level system losses in three-dimensional superconducting radio frequency (SRF) niobium resonators by atomic layer deposition (ALD) of a 10 nm aluminum oxide Al2O3 thin films followed by a high vacuum (HV) heat treatment at 650 {\\deg}C for few hours. By probing the effect of several heat treatments on Al2O3-coated niobium samples by X-ray photoelectron spectroscopy (XPS) plus scanning and conventional high resolution transmission electron microscopy (STEM/HRTEM) coupled with electron energy loss spectroscopy (EELS) and (EDX) , we witness a dissolution of niobium native oxides and the modification of the Al2O3-Nb interface, which correlates with the enhancement of the quality factor at low fields of two 1.3 GHz niobium cavities coated with 10 nm of Al2O3.","sentences":["Superconducting qubits have arisen as a leading technology platform for quantum computing which is on the verge of revolutionizing the world's calculation capacities.","Nonetheless, the fabrication of computationally reliable qubit circuits requires increasing the quantum coherence lifetimes, which are predominantly limited by the dissipations of two-level system (TLS) defects present in the thin superconducting film and the adjacent dielectric regions.","In this paper, we demonstrate the reduction of two-level system losses in three-dimensional superconducting radio frequency (SRF) niobium resonators by atomic layer deposition (ALD) of a 10 nm aluminum oxide Al2O3 thin films followed by a high vacuum (HV) heat treatment at 650 {\\deg}C for few hours.","By probing the effect of several heat treatments on Al2O3-coated niobium samples by X-ray photoelectron spectroscopy (XPS) plus scanning and conventional high resolution transmission electron microscopy (STEM/HRTEM) coupled with electron energy loss spectroscopy (EELS) and (EDX) , we witness a dissolution of niobium native oxides and the modification of the Al2O3-Nb interface, which correlates with the enhancement of the quality factor at low fields of two 1.3 GHz niobium cavities coated with 10 nm of Al2O3."],"url":"http://arxiv.org/abs/2402.04137v1","category":"physics.app-ph"}
{"created":"2024-02-06 16:38:11","title":"A quasi-optimal lower bound for skew polynomial multiplication","abstract":"We establish a lower bound for the complexity of multiplying two skew polynomials. The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor. We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal. In fact, our lower bound is quasi-optimal in the sense of bilinear complexity. In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions.","sentences":["We establish a lower bound for the complexity of multiplying two skew polynomials.","The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor.","We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal.","In fact, our lower bound is quasi-optimal in the sense of bilinear complexity.","In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions."],"url":"http://arxiv.org/abs/2402.04134v1","category":"cs.CC"}
{"created":"2024-02-06 16:34:02","title":"Non-Abelian anyons in a periodically-driven Abelian model","abstract":"We show that non-Abelian anyons can emerge from an Abelian topologically-ordered model subject to time-periodic driving, with the specific example of Ising anyons in a driven toric-code model. The toric code possesses fermionic and bosonic quasiparticles which are mutual semions, namely they see each other as $\\pi$ fluxes. The Floquet modulation addresses the fermionic quasiparticles only and is designed to induce a topologically non-trivial band structure, realizing drive-assisted $p+ip$ pairing in the high-frequency regime. As a result, the fermions fractionalize into Floquet-Majorana zero modes hosted by the bosonic quasiparticles, which then develop non-Abelian character. These properties are analyzed through high-frequency expansions in a quasiparticle picture and, numerically, via computation of the exact quasienergy spectra and of the non-Abelian exchange phases. Our findings shed light on the nonequilibrium physics of driven topologically-ordered quantum matter and may facilitate the observation of non-Abelian behaviour in engineered quantum systems.","sentences":["We show that non-Abelian anyons can emerge from an Abelian topologically-ordered model subject to time-periodic driving, with the specific example of Ising anyons in a driven toric-code model.","The toric code possesses fermionic and bosonic quasiparticles which are mutual semions, namely they see each other as $\\pi$ fluxes.","The Floquet modulation addresses the fermionic quasiparticles only and is designed to induce a topologically non-trivial band structure, realizing drive-assisted $p+ip$ pairing in the high-frequency regime.","As a result, the fermions fractionalize into Floquet-Majorana zero modes hosted by the bosonic quasiparticles, which then develop non-Abelian character.","These properties are analyzed through high-frequency expansions in a quasiparticle picture and, numerically, via computation of the exact quasienergy spectra and of the non-Abelian exchange phases.","Our findings shed light on the nonequilibrium physics of driven topologically-ordered quantum matter and may facilitate the observation of non-Abelian behaviour in engineered quantum systems."],"url":"http://arxiv.org/abs/2402.04131v1","category":"quant-ph"}
{"created":"2024-02-06 16:21:12","title":"A Mean-Field Study of Quantum Oscillations in Two-Dimensional Kondo Insulators","abstract":"Magnetic oscillations in strongly correlated insulating systems have garnered interest due to oscillations seemingly originating from the bulk, despite an anticipated gapped spectrum. We use the large-$N$ mean-field theory to study the behavior of normal and topological Kondo insulators under a magnetic field. In both cases spinons acquire a charge and hybridize with electrons, producing magnetic oscillations that resemble two-band noninteracting systems. We show that in such band insulators magnetic oscillations are exponentially suppressed at weak magnetic fields. A self-consistent mean-field calculation for the Kondo insulators reveals that the temperature dependence of the oscillations departs from the noninteracting case due to the temperature and magnetic-field dependence of the hybridization, even though mean-field parameters remain homogeneous at low fields. Larger magnetic fields result in the Kondo breakdown, where the magnetic oscillation is solely due to the decoupled conduction electrons. These findings offer new insights into the magnetic properties of Kondo insulators, with implications for interpreting experimental results in heavy fermion materials like SmB$_6$.","sentences":["Magnetic oscillations in strongly correlated insulating systems have garnered interest due to oscillations seemingly originating from the bulk, despite an anticipated gapped spectrum.","We use the large-$N$ mean-field theory to study the behavior of normal and topological Kondo insulators under a magnetic field.","In both cases spinons acquire a charge and hybridize with electrons, producing magnetic oscillations that resemble two-band noninteracting systems.","We show that in such band insulators magnetic oscillations are exponentially suppressed at weak magnetic fields.","A self-consistent mean-field calculation for the Kondo insulators reveals that the temperature dependence of the oscillations departs from the noninteracting case due to the temperature and magnetic-field dependence of the hybridization, even though mean-field parameters remain homogeneous at low fields.","Larger magnetic fields result in the Kondo breakdown, where the magnetic oscillation is solely due to the decoupled conduction electrons.","These findings offer new insights into the magnetic properties of Kondo insulators, with implications for interpreting experimental results in heavy fermion materials like SmB$_6$."],"url":"http://arxiv.org/abs/2402.04125v1","category":"cond-mat.str-el"}
{"created":"2024-02-06 16:08:57","title":"Origami Nano-gap Electrodes for Reversible Nanoparticle Trapping","abstract":"We present a facile desktop fabrication method for origami-based nano-gap indium tin oxide (ITO) electrokinetic particle traps, providing a simplified approach compared to traditional lithographic techniques and effectively trapping of nanoparticles. Our approach involves bending ITO thin films on optically transparent polyethylene terephthalate (PET), creating an array of parallel nano-gaps. By strategically introducing weak points through cut-sharp edges, we successfully controlled the spread of nano-cracks. A single crack spanning the constriction width and splitting the conductive layers forms a nano-gap that can effectively trap small nanoparticles after applying an alternating electric potential across the nanogap. We analyze the conditions for reversible trapping and optimal performance of the nano-gap ITO electrodes with optical microscopy and electrokinetic impedance spectroscopy. Our findings highlight the potential of this facile fabrication method for the use of ITO at active electro-actuated traps in microfluidic systems.","sentences":["We present a facile desktop fabrication method for origami-based nano-gap indium tin oxide (ITO) electrokinetic particle traps, providing a simplified approach compared to traditional lithographic techniques and effectively trapping of nanoparticles.","Our approach involves bending ITO thin films on optically transparent polyethylene terephthalate (PET), creating an array of parallel nano-gaps.","By strategically introducing weak points through cut-sharp edges, we successfully controlled the spread of nano-cracks.","A single crack spanning the constriction width and splitting the conductive layers forms a nano-gap that can effectively trap small nanoparticles after applying an alternating electric potential across the nanogap.","We analyze the conditions for reversible trapping and optimal performance of the nano-gap ITO electrodes with optical microscopy and electrokinetic impedance spectroscopy.","Our findings highlight the potential of this facile fabrication method for the use of ITO at active electro-actuated traps in microfluidic systems."],"url":"http://arxiv.org/abs/2402.04116v1","category":"physics.app-ph"}
{"created":"2024-02-06 16:07:56","title":"Twin-core fiber sensor integrated in laser cavity","abstract":"In this work, we report on a twin-core fiber sensor system that provides improved spectral efficiency, allows for multiplexing and gives low level of crosstalk. Pieces of the referred strongly coupled multicore fiber are used as sensors in a laser cavity incorporating a pulsed semiconductor optical amplifier (SOA). Each sensor has its unique cavity length and can be addressed individually by electrically matching the periodic gating of the SOA to the sensors cavity roundtrip time. The interrogator acts as a laser and provides a narrow spectrum with high signal-to-noise ratio. Furthermore, it allows distinguishing the response of individual sensors even in the case of overlapping spectra. Potentially, the number of interrogated sensors can be increased significantly, which is an appealing feature for multipoint sensing.","sentences":["In this work, we report on a twin-core fiber sensor system that provides improved spectral efficiency, allows for multiplexing and gives low level of crosstalk.","Pieces of the referred strongly coupled multicore fiber are used as sensors in a laser cavity incorporating a pulsed semiconductor optical amplifier (SOA).","Each sensor has its unique cavity length and can be addressed individually by electrically matching the periodic gating of the SOA to the sensors cavity roundtrip time.","The interrogator acts as a laser and provides a narrow spectrum with high signal-to-noise ratio.","Furthermore, it allows distinguishing the response of individual sensors even in the case of overlapping spectra.","Potentially, the number of interrogated sensors can be increased significantly, which is an appealing feature for multipoint sensing."],"url":"http://arxiv.org/abs/2402.04115v1","category":"physics.optics"}
{"created":"2024-02-06 16:06:59","title":"SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning","abstract":"In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.","sentences":["In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm.","We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm.","We show that the communication complexity of FedLSA scales polynomially with the desired precision $\\epsilon$, which limits the benefits of federation.","To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity.","We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements."],"url":"http://arxiv.org/abs/2402.04114v1","category":"stat.ML"}
{"created":"2024-02-06 16:05:18","title":"The open XYZ spin 1/2 chain: Separation of Variables and scalar products for boundary fields related by a constraint","abstract":"We consider the open XYZ spin chain with boundary fields. We solve the model by the new Separation of Variables approach introduced in arXiv:1904.00852. In this framework, the transfer matrix eigenstates are obtained as a particular sub-class of the class of so-called separate states. We consider the problem of computing scalar products of such separate states. As usual, they can be represented as determinants with rows labelled by the inhomogeneity parameters of the model. We notably focus on the special case in which the boundary parameters parametrising the two boundary fields satisfy one constraint, hence enabling for the description of part of the transfer matrix spectrum and eigenstates in terms of some elliptic polynomial Q-solution of a usual TQ-equation. In this case, we show how to transform the aforementioned determinant for the scalar product into some more convenient form for the consideration of the homogeneous and thermodynamic limits: as in the open XXX or XXZ cases, our result can be expressed as some generalisation of the so-called Slavnov determinant.","sentences":["We consider the open XYZ spin chain with boundary fields.","We solve the model by the new Separation of Variables approach introduced in arXiv:1904.00852.","In this framework, the transfer matrix eigenstates are obtained as a particular sub-class of the class of so-called separate states.","We consider the problem of computing scalar products of such separate states.","As usual, they can be represented as determinants with rows labelled by the inhomogeneity parameters of the model.","We notably focus on the special case in which the boundary parameters parametrising the two boundary fields satisfy one constraint, hence enabling for the description of part of the transfer matrix spectrum and eigenstates in terms of some elliptic polynomial Q-solution of a usual TQ-equation.","In this case, we show how to transform the aforementioned determinant for the scalar product into some more convenient form for the consideration of the homogeneous and thermodynamic limits: as in the open XXX or XXZ cases, our result can be expressed as some generalisation of the so-called Slavnov determinant."],"url":"http://arxiv.org/abs/2402.04112v1","category":"math-ph"}
{"created":"2024-02-06 16:00:20","title":"Hybrid moir\u00e9 excitons and trions in twisted MoTe$_2$-MoSe$_2$ heterobilayers","abstract":"We report experimental and theoretical studies of MoTe$_2$-MoSe$_2$ heterobilayers with rigid moir\\'e superlattices controlled by the twist angle. Using an effective continuum model that combines resonant interlayer electron tunneling with stacking-dependent moir\\'e potentials, we identify the nature of moir\\'e excitons and the dependence of their energies, oscillator strengths and Land\\'e $g$-factors on the twist angle. Within the same framework, we interpret distinct signatures of bound complexes among electrons and moir\\'e excitons in nearly collinear heterostacks. Our work provides fundamental understanding of hybrid moir\\'e excitons and trions in MoTe$_2$-MoSe$_2$ heterobilayers, and establishes the material system as a prime candidate for optical studies of correlated phenomena in moir\\'e lattices.","sentences":["We report experimental and theoretical studies of MoTe$_2$-MoSe$_2$ heterobilayers with rigid moir\\'e superlattices controlled by the twist angle.","Using an effective continuum model that combines resonant interlayer electron tunneling with stacking-dependent moir\\'e potentials, we identify the nature of moir\\'e excitons and the dependence of their energies, oscillator strengths and Land\\'e $g$-factors on the twist angle.","Within the same framework, we interpret distinct signatures of bound complexes among electrons and moir\\'e excitons in nearly collinear heterostacks.","Our work provides fundamental understanding of hybrid moir\\'e excitons and trions in MoTe$_2$-MoSe$_2$ heterobilayers, and establishes the material system as a prime candidate for optical studies of correlated phenomena in moir\\'e lattices."],"url":"http://arxiv.org/abs/2402.04106v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 15:59:23","title":"Measuring Implicit Bias in Explicitly Unbiased Large Language Models","abstract":"Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates with embedding-based methods but better predicts downstream behaviors measured by LLM Decision Bias. This measure is based on asking the LLM to decide between individuals, motivated by psychological results indicating that relative not absolute evaluations are more related to implicit biases. Using prompt-based measures informed by psychology allows us to effectively expose nuanced biases and subtle discrimination in proprietary LLMs that do not show explicit bias on standard benchmarks.","sentences":["Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases.","Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make.","We address both of these challenges by introducing two measures of bias inspired by psychology:","LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks.","Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others).","Our prompt-based measure of implicit bias correlates with embedding-based methods but better predicts downstream behaviors measured by LLM Decision Bias.","This measure is based on asking the LLM to decide between individuals, motivated by psychological results indicating that relative not absolute evaluations are more related to implicit biases.","Using prompt-based measures informed by psychology allows us to effectively expose nuanced biases and subtle discrimination in proprietary LLMs that do not show explicit bias on standard benchmarks."],"url":"http://arxiv.org/abs/2402.04105v1","category":"cs.CY"}
{"created":"2024-02-06 15:54:54","title":"A programmable photonic memory","abstract":"The significant advancements in integrated photonics have enabled high-speed and energy efficient systems for various applications from data communications and high-performance computing, to medical diagnosis, sensing and ranging. However, data storage in these systems has been dominated by electronic memories which necessitates signal conversion between optical and electrical as well as analog and digital domains, and data movement between processor and memory that reduce the speed and energy efficiency. To date, a scalable optical memory with optical control has remained an open problem. Here we report an integrated photonic set-reset latch as a fundamental optical static memory unit based on universal optical logic gates. While the proposed memory is compatible with different photonic platforms, its functionality is demonstrated on a programmable silicon photonic chip as a proof of concept. Optical set, reset, and complementary outputs, scalability to a large number of memory units via the independent latch supply light, and compatibility with different photonic platforms enable more efficient and lower latency optical processing systems.","sentences":["The significant advancements in integrated photonics have enabled high-speed and energy efficient systems for various applications from data communications and high-performance computing, to medical diagnosis, sensing and ranging.","However, data storage in these systems has been dominated by electronic memories which necessitates signal conversion between optical and electrical as well as analog and digital domains, and data movement between processor and memory that reduce the speed and energy efficiency.","To date, a scalable optical memory with optical control has remained an open problem.","Here we report an integrated photonic set-reset latch as a fundamental optical static memory unit based on universal optical logic gates.","While the proposed memory is compatible with different photonic platforms, its functionality is demonstrated on a programmable silicon photonic chip as a proof of concept.","Optical set, reset, and complementary outputs, scalability to a large number of memory units via the independent latch supply light, and compatibility with different photonic platforms enable more efficient and lower latency optical processing systems."],"url":"http://arxiv.org/abs/2402.04100v1","category":"physics.optics"}
{"created":"2024-02-06 15:44:06","title":"Quantum correlations under decoherence","abstract":"Taking a system of two coupled qubits described by a X-shaped state and interacting through an anisotropic Heisenberg XY interaction, we examine the evolution of quantum entanglement and a few quantum correlations beyond entanglement, local quantum uncertainty and measurement-induced nonlocality, under the environmental decoherence both for zero and finite temperatures. The relation between concurrence and log negativity as two well-known quantifiers of entanglement is argued. It will be proven that measurement-induced nonlocality equals correlated coherence. The interaction of qubits with the environment causes quantum entanglement to suddenly die for independent qubits, but other correlations do not experience this phenomenon. The time of entanglement sudden death is calculated analytically for zero temperature, while numerically for finite temperatures. Contrary to its usual destructive role, the environment plays a constructive role in some situations, inducing quantum correlations even when the initial quantum correlations are zero. The steady state quantum correlations, being independent of the initial state, are found to remain all non-zero for low, finite temperatures. It is found that quantum correlations beyond entanglement are more robust against temperature than entanglement. The zero-temperature steady state exhibits less local quantum uncertainty than the other correlations.","sentences":["Taking a system of two coupled qubits described by a X-shaped state and interacting through an anisotropic Heisenberg XY interaction, we examine the evolution of quantum entanglement and a few quantum correlations beyond entanglement, local quantum uncertainty and measurement-induced nonlocality, under the environmental decoherence both for zero and finite temperatures.","The relation between concurrence and log negativity as two well-known quantifiers of entanglement is argued.","It will be proven that measurement-induced nonlocality equals correlated coherence.","The interaction of qubits with the environment causes quantum entanglement to suddenly die for independent qubits, but other correlations do not experience this phenomenon.","The time of entanglement sudden death is calculated analytically for zero temperature, while numerically for finite temperatures.","Contrary to its usual destructive role, the environment plays a constructive role in some situations, inducing quantum correlations even when the initial quantum correlations are zero.","The steady state quantum correlations, being independent of the initial state, are found to remain all non-zero for low, finite temperatures.","It is found that quantum correlations beyond entanglement are more robust against temperature than entanglement.","The zero-temperature steady state exhibits less local quantum uncertainty than the other correlations."],"url":"http://arxiv.org/abs/2402.04086v1","category":"quant-ph"}
{"created":"2024-02-06 15:34:30","title":"Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning","abstract":"This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.","sentences":["This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL).","At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy.","We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets.","To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement.","By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks.","Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}."],"url":"http://arxiv.org/abs/2402.04080v1","category":"cs.LG"}
{"created":"2024-02-06 15:31:17","title":"A metronome spin stabilizes time-crystalline dynamics","abstract":"We investigate a disorder-free quantum Ising chain subject to a time-periodic drive that rotates each spin by an angle $\\pi(1-\\epsilon_i)$. In case all spins experience the same deviation $\\epsilon$ and the system is initialized in a fully polarized state, the dynamics is known to be time-crystalline: the magnetization of the system exhibits period-doubled oscillations for timescales that grow exponentially with the length of the chain. In this work, we study the effect of a deviation $\\epsilon$ that differs between spins. We find that reducing $\\epsilon$ for a single spin drastically enhances the lifetime of spatio-temporal order, suggesting the name ``metronome\" spin. Employing perturbative arguments in an average Hamiltonian picture, we explain this observation for initial states with macroscopic bulk magnetization. Furthermore, in the case of random bitstring initial states, we report the enhancement of the lifetime of a topological edge mode, which can also be understood in the same picture. Finally, we discuss an altered geometry in which the metronome spin is not directly part of the chain, affecting the dynamics in different ways in the two scenarios considered. Our findings unveil the intricate dynamics that emerge in Floquet systems under the influence of a spatially varying drive, thereby uncovering new avenues for Floquet engineering.","sentences":["We investigate a disorder-free quantum Ising chain subject to a time-periodic drive that rotates each spin by an angle $\\pi(1-\\epsilon_i)$. In case all spins experience the same deviation $\\epsilon$ and the system is initialized in a fully polarized state, the dynamics is known to be time-crystalline: the magnetization of the system exhibits period-doubled oscillations for timescales that grow exponentially with the length of the chain.","In this work, we study the effect of a deviation $\\epsilon$ that differs between spins.","We find that reducing $\\epsilon$ for a single spin drastically enhances the lifetime of spatio-temporal order, suggesting the name ``metronome\" spin.","Employing perturbative arguments in an average Hamiltonian picture, we explain this observation for initial states with macroscopic bulk magnetization.","Furthermore, in the case of random bitstring initial states, we report the enhancement of the lifetime of a topological edge mode, which can also be understood in the same picture.","Finally, we discuss an altered geometry in which the metronome spin is not directly part of the chain, affecting the dynamics in different ways in the two scenarios considered.","Our findings unveil the intricate dynamics that emerge in Floquet systems under the influence of a spatially varying drive, thereby uncovering new avenues for Floquet engineering."],"url":"http://arxiv.org/abs/2402.04078v1","category":"quant-ph"}
{"created":"2024-02-06 15:22:13","title":"Mean-Square Stability and Stabilizability for LTI and Stochastic Systems Connected in Feedback","abstract":"In this paper, the feedback stabilization of a linear time-invariant (LTI) multiple-input multiple-output (MIMO) system cascaded by a linear stochastic system is studied in the mean-square sense. Here, the linear stochastic system can model a class of correlated stochastic uncertainties such as channel uncertainties induced by packet loss and random transmission delays in networked systems. By proposing a key parameter called coefficient of frequency variation to characterize the correlation of the stochastic uncertainties, we present a necessary and sufficient condition of the mean-square stability for this MIMO stochastic feedback system. After then a necessary and sufficient condition for the mean-square stabilizability is provided, which reveals a fundamental limit imposed by the system's unstable poles, nonminimum-phase (NMP) zeros, relative degrees (input delays), and the coefficient of frequency variation of the stochastic uncertainties. A numerical example is presented to illustrate the fundamental constraints in the mean-square stabilizability of MIMO networked systems with parallel communication channels.","sentences":["In this paper, the feedback stabilization of a linear time-invariant (LTI) multiple-input multiple-output (MIMO) system cascaded by a linear stochastic system is studied in the mean-square sense.","Here, the linear stochastic system can model a class of correlated stochastic uncertainties such as channel uncertainties induced by packet loss and random transmission delays in networked systems.","By proposing a key parameter called coefficient of frequency variation to characterize the correlation of the stochastic uncertainties, we present a necessary and sufficient condition of the mean-square stability for this MIMO stochastic feedback system.","After then a necessary and sufficient condition for the mean-square stabilizability is provided, which reveals a fundamental limit imposed by the system's unstable poles, nonminimum-phase (NMP) zeros, relative degrees (input delays), and the coefficient of frequency variation of the stochastic uncertainties.","A numerical example is presented to illustrate the fundamental constraints in the mean-square stabilizability of MIMO networked systems with parallel communication channels."],"url":"http://arxiv.org/abs/2402.04074v1","category":"eess.SY"}
{"created":"2024-02-06 15:17:09","title":"Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality","abstract":"Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks. Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient co-working. In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR). This framework incorporates a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches. The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations. The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control. This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map. We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset.","sentences":["Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks.","Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient co-working.","In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR).","This framework incorporates a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches.","The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations.","The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control.","This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map.","We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset."],"url":"http://arxiv.org/abs/2402.04070v1","category":"cs.RO"}
{"created":"2024-02-06 15:03:28","title":"A Digital Twin Design Methodology for Control, Simulation, and Monitoring of Fluidic Circuits","abstract":"We propose a synthesis method for the design of digital twins applicable to various systems (pneumatic, hydraulic, electrical/electronic circuits). The methodology allows representing the operation of these systems through an active digital twin, thereby enabling a more suitable and easier computer-aided design, simulation, control, and monitoring. Furthermore, our methodology enables the detection of a system's actions on its own inputs (for example, in pneumatics: backflow of gases trapped in part of a fluidic system onto its own inputs). During the simulation or monitoring phase, the approach also facilitates real-time diagnosis of the controlled system. The outputs, on the controlled physical system or its digital twin, do not depend only on the current inputs but also on the history of the inputs and the history of internal states and variables. In other words, the underlying sequential logic has a memory while an only combinational logic approach does not. These capabilities can contribute to the digital transformation of the factory of the future.","sentences":["We propose a synthesis method for the design of digital twins applicable to various systems (pneumatic, hydraulic, electrical/electronic circuits).","The methodology allows representing the operation of these systems through an active digital twin, thereby enabling a more suitable and easier computer-aided design, simulation, control, and monitoring.","Furthermore, our methodology enables the detection of a system's actions on its own inputs (for example, in pneumatics: backflow of gases trapped in part of a fluidic system onto its own inputs).","During the simulation or monitoring phase, the approach also facilitates real-time diagnosis of the controlled system.","The outputs, on the controlled physical system or its digital twin, do not depend only on the current inputs but also on the history of the inputs and the history of internal states and variables.","In other words, the underlying sequential logic has a memory while an only combinational logic approach does not.","These capabilities can contribute to the digital transformation of the factory of the future."],"url":"http://arxiv.org/abs/2402.04058v1","category":"eess.SY"}
{"created":"2024-02-06 15:02:09","title":"Collaborative Deep Reinforcement Learning for Resource Optimization in Non-Terrestrial Networks","abstract":"Non-terrestrial networks (NTNs) with low-earth orbit (LEO) satellites have been regarded as promising remedies to support global ubiquitous wireless services. Due to the rapid mobility of LEO satellite, inter-beam/satellite handovers happen frequently for a specific user equipment (UE). To tackle this issue, earth-fixed cell scenarios have been under studied, in which the LEO satellite adjusts its beam direction towards a fixed area within its dwell duration, to maintain stable transmission performance for the UE. Therefore, it is required that the LEO satellite performs real-time resource allocation, which however is unaffordable by the LEO satellite with limited computing capability. To address this issue, in this paper, we propose a two-time-scale collaborative deep reinforcement learning (DRL) scheme for beam management and resource allocation in NTNs, in which LEO satellite and UE with different control cycles update their decision-making policies through a sequential manner. Specifically, UE updates its policy subject to improving the value functions of both the agents. Furthermore, the LEO satellite only makes decisions through finite-step rollouts with a reference decision trajectory received from the UE. Simulation results show that the proposed scheme can effectively balance the throughput performance and computational complexity over traditional greedy-searching schemes.","sentences":["Non-terrestrial networks (NTNs) with low-earth orbit (LEO) satellites have been regarded as promising remedies to support global ubiquitous wireless services.","Due to the rapid mobility of LEO satellite, inter-beam/satellite handovers happen frequently for a specific user equipment (UE).","To tackle this issue, earth-fixed cell scenarios have been under studied, in which the LEO satellite adjusts its beam direction towards a fixed area within its dwell duration, to maintain stable transmission performance for the UE.","Therefore, it is required that the LEO satellite performs real-time resource allocation, which however is unaffordable by the LEO satellite with limited computing capability.","To address this issue, in this paper, we propose a two-time-scale collaborative deep reinforcement learning (DRL) scheme for beam management and resource allocation in NTNs, in which LEO satellite and UE with different control cycles update their decision-making policies through a sequential manner.","Specifically, UE updates its policy subject to improving the value functions of both the agents.","Furthermore, the LEO satellite only makes decisions through finite-step rollouts with a reference decision trajectory received from the UE.","Simulation results show that the proposed scheme can effectively balance the throughput performance and computational complexity over traditional greedy-searching schemes."],"url":"http://arxiv.org/abs/2402.04056v1","category":"eess.SP"}
{"created":"2024-02-06 14:26:22","title":"On provable privacy vulnerabilities of graph representations","abstract":"Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases. Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models. Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA. We empirically delineate instances wherein COSERA demonstrates both efficacy and deficiency in its capacity to function as an instrument for elucidating the trade-off between privacy and utility.","sentences":["Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations.","This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks.","Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases.","Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models.","Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA.","We empirically delineate instances wherein COSERA demonstrates both efficacy and deficiency in its capacity to function as an instrument for elucidating the trade-off between privacy and utility."],"url":"http://arxiv.org/abs/2402.04033v1","category":"cs.LG"}
{"created":"2024-02-06 14:24:29","title":"Positive concave deep equilibrium models","abstract":"Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis. Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of geometric convergence, which, in particular, simplifies the training process. Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models.","sentences":["Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks.","These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks.","However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established.","As a result, DEQ models are potentially unstable in practice.","To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models.","Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant.","By imposing these constraints, we can easily ensure the existence and uniqueness of the fixed point without relying on additional complex assumptions commonly found in the DEQ literature, such as those based on monotone operator theory in convex analysis.","Furthermore, the fixed point can be computed with the standard fixed point algorithm, and we provide theoretical guarantees of geometric convergence, which, in particular, simplifies the training process.","Experiments demonstrate the competitiveness of our pcDEQ models against other implicit models."],"url":"http://arxiv.org/abs/2402.04029v1","category":"cs.LG"}
{"created":"2024-02-06 14:23:11","title":"A realization theorem for the modal logic of transitive closure $\\mathsf{K}^+$","abstract":"We present a justification logic corresponding to the modal logic of transitive closure $\\mathsf{K}^+$ and establish a normal realization theorem relating these two systems. The result is obtained by means of a sequent calculus allowing non-well-founded proofs.","sentences":["We present a justification logic corresponding to the modal logic of transitive closure $\\mathsf{K}^+$ and establish a normal realization theorem relating these two systems.","The result is obtained by means of a sequent calculus allowing non-well-founded proofs."],"url":"http://arxiv.org/abs/2402.04027v1","category":"math.LO"}
{"created":"2024-02-06 13:56:57","title":"An ALMA molecular inventory of warm Herbig Ae disks: II. Abundant complex organics and volatile sulphur in the IRS 48 disk","abstract":"The Atacama Large Millimeter/submillimeter Array (ALMA) can probe the molecular content of planet-forming disks with unprecedented sensitivity. These observations allow us to build up an inventory of the volatiles available for forming planets and comets. Herbig Ae transition disks are fruitful targets due to the thermal sublimation of complex organic molecule (COM) and likely H2O-rich ices in these disks. The IRS 48 disk shows a particularly rich chemistry that can be directly linked to its asymmetric dust trap. Here, we present ALMA observations of the IRS 48 disk where we detect 16 different molecules and make the first robust detections of H213CO, 34SO, 33SO and c-H2COCH2 (ethylene oxide) in a protoplanetary disk. All of the molecular emissions, aside from CO, are colocated with the dust trap and this includes newly detected simple molecules such as HCO+, HCN and CS. Interestingly, there are spatial offsets between different molecular families, including between the COMs and sulphur-bearing species, with the latter being more azimuthally extended and located radially further from the star. The abundances of the newly detected COMs relative to CH3OH are higher than the expected protostellar ratios, which implies some degree of chemical processing of the inherited ices during the disk lifetime. These data highlight IRS 48 as a unique astrochemical laboratory to unravel the full volatile reservoir at the epoch of planet and comet formation and the role of the disk in (re)setting chemical complexity.","sentences":["The Atacama Large Millimeter/submillimeter Array (ALMA) can probe the molecular content of planet-forming disks with unprecedented sensitivity.","These observations allow us to build up an inventory of the volatiles available for forming planets and comets.","Herbig Ae transition disks are fruitful targets due to the thermal sublimation of complex organic molecule (COM) and likely H2O-rich ices in these disks.","The IRS 48 disk shows a particularly rich chemistry that can be directly linked to its asymmetric dust trap.","Here, we present ALMA observations of the IRS 48 disk where we detect 16 different molecules and make the first robust detections of H213CO, 34SO, 33SO and c-H2COCH2 (ethylene oxide) in a protoplanetary disk.","All of the molecular emissions, aside from CO, are colocated with the dust trap and this includes newly detected simple molecules such as HCO+, HCN and CS.","Interestingly, there are spatial offsets between different molecular families, including between the COMs and sulphur-bearing species, with the latter being more azimuthally extended and located radially further from the star.","The abundances of the newly detected COMs relative to CH3OH are higher than the expected protostellar ratios, which implies some degree of chemical processing of the inherited ices during the disk lifetime.","These data highlight IRS 48 as a unique astrochemical laboratory to unravel the full volatile reservoir at the epoch of planet and comet formation and the role of the disk in (re)setting chemical complexity."],"url":"http://arxiv.org/abs/2402.04002v1","category":"astro-ph.EP"}
{"created":"2024-02-06 13:56:55","title":"An ALMA molecular inventory of warm Herbig Ae disks: I. Molecular rings, asymmetries and complexity in the HD 100546 disk","abstract":"Observations of disks with the Atacama Large Millimeter/submillimeter Array (ALMA) allow us to map the chemical makeup of nearby protoplanetary disks with unprecedented spatial resolution and sensitivity. The typical outer Class II disk observed with ALMA is one with an elevated C/O ratio and a lack of oxygen-bearing complex organic molecules, but there are now some interesting exceptions: three transition disks around Herbig Ae stars all show oxygen-rich gas traced via the unique detections of the molecules SO and CH3OH. We present the first results of an ALMA line survey at 337 to 357 GHz of such disks and focus this paper on the first Herbig Ae disk to exhibit this chemical signature - HD 100546. In these data, we detect 19 different molecules including NO, SO and CH3OCHO (methyl formate). We also make the first tentative detections of H213CO and 34SO in protoplanetary disks. Multiple molecular species are detected in rings, which are, surprisingly, all peaking just beyond the underlying millimeter continuum ring at 200 au. This result demonstrates a clear connection between the large dust distribution and the chemistry in this flat outer disk. We discuss the physical and/or chemical origin of these sub-structures in relation to ongoing planet formation in the HD 100546 disk. We also investigate how similar and/or different the molecular make up of this disk is to other chemically well-characterised Herbig Ae disks. The line-rich data we present motivates the need for more ALMA line surveys to probe the observable chemistry in Herbig Ae systems which offer unique insight into the composition of disk ices, including complex organic molecules.","sentences":["Observations of disks with the Atacama Large Millimeter/submillimeter Array (ALMA) allow us to map the chemical makeup of nearby protoplanetary disks with unprecedented spatial resolution and sensitivity.","The typical outer Class II disk observed with ALMA is one with an elevated C/O ratio and a lack of oxygen-bearing complex organic molecules, but there are now some interesting exceptions: three transition disks around Herbig Ae stars all show oxygen-rich gas traced via the unique detections of the molecules SO and CH3OH.","We present the first results of an ALMA line survey at 337 to 357 GHz of such disks and focus this paper on the first Herbig Ae disk to exhibit this chemical signature - HD 100546.","In these data, we detect 19 different molecules including NO, SO and CH3OCHO (methyl formate).","We also make the first tentative detections of H213CO and 34SO in protoplanetary disks.","Multiple molecular species are detected in rings, which are, surprisingly, all peaking just beyond the underlying millimeter continuum ring at 200 au.","This result demonstrates a clear connection between the large dust distribution and the chemistry in this flat outer disk.","We discuss the physical and/or chemical origin of these sub-structures in relation to ongoing planet formation in the HD 100546 disk.","We also investigate how similar and/or different the molecular make up of this disk is to other chemically well-characterised Herbig Ae disks.","The line-rich data we present motivates the need for more ALMA line surveys to probe the observable chemistry in Herbig Ae systems which offer unique insight into the composition of disk ices, including complex organic molecules."],"url":"http://arxiv.org/abs/2402.04001v1","category":"astro-ph.EP"}
{"created":"2024-02-06 13:19:38","title":"A note on the completeness of Fourier-based metrics on measures","abstract":"Resolving an open question of J. A. Carillo and G. Toscani, M. Stawiska recently proved that certain metric spaces of probability measures equipped with Fourier-based metrics are complete. In this note, we demonstrate the fine nature of Stawiska's result and show that, when such metrics are defined on complex measures, the resulting metric spaces are not complete.","sentences":["Resolving an open question of J. A. Carillo and G. Toscani, M. Stawiska recently proved that certain metric spaces of probability measures equipped with Fourier-based metrics are complete.","In this note, we demonstrate the fine nature of Stawiska's result and show that, when such metrics are defined on complex measures, the resulting metric spaces are not complete."],"url":"http://arxiv.org/abs/2402.03983v1","category":"math.CA"}
{"created":"2024-02-06 13:16:52","title":"Large-time optimal observation domain for linear parabolic systems","abstract":"Given a well-posed linear evolution system settled on a domain $\\Omega$ of $\\mathbb{R}^d$, an observation subset $\\omega\\subset\\Omega$ and a time horizon $T$, the observability constant is defined as the largest possible nonnegative constant such that the observability inequality holds for the pair $(\\omega,T)$. In this article we investigate the large-time behavior of the observation domain that maximizes the observability constant over all possible measurable subsets of a given Lebesgue measure. We prove that it converges exponentially, as the time horizon goes to infinity, to a limit set that we characterize. The mathematical technique is new and relies on a quantitative version of the bathtub principle.","sentences":["Given a well-posed linear evolution system settled on a domain $\\Omega$ of $\\mathbb{R}^d$, an observation subset $\\omega\\subset\\Omega$ and a time horizon $T$, the observability constant is defined as the largest possible nonnegative constant such that the observability inequality holds for the pair $(\\omega,T)$.","In this article we investigate the large-time behavior of the observation domain that maximizes the observability constant over all possible measurable subsets of a given Lebesgue measure.","We prove that it converges exponentially, as the time horizon goes to infinity, to a limit set that we characterize.","The mathematical technique is new and relies on a quantitative version of the bathtub principle."],"url":"http://arxiv.org/abs/2402.03980v1","category":"math.AP"}
{"created":"2024-02-06 13:15:03","title":"Reconfigurable Power Converters with Increased Utilization for Unbalanced Power Distribution System Applications","abstract":"A low-cost reconfiguration stage connected at the output of balanced three-phase, multi-terminal ac/dc/ac converters can increase the feasible set of power injections substantially, increasing converter utilization and therefore achieving a lower system cost. However, the approach has yet to be explored for phase unbalance mitigation in power distribution networks, an important application for future energy systems. This study addresses this by considering power converter reconfiguration's potential for increasing the feasible set of power transfers of four-wire power converters. Reconfigurable topologies are compared against both conventional four-wire designs and an idealised, fully reconfigurable converter. Results show that conventional converters need up to 75.3% greater capacity to yield a capability chart of equivalent size to an idealised reconfigurable converter. The number and capacity of legs impact the capability chart's size, as do constraints on dc-side power injections. The proposed approach shows significant promise for maximizing the utilization of power electronics used to mitigate impacts of phase unbalance.","sentences":["A low-cost reconfiguration stage connected at the output of balanced three-phase, multi-terminal ac/dc/ac converters can increase the feasible set of power injections substantially, increasing converter utilization and therefore achieving a lower system cost.","However, the approach has yet to be explored for phase unbalance mitigation in power distribution networks, an important application for future energy systems.","This study addresses this by considering power converter reconfiguration's potential for increasing the feasible set of power transfers of four-wire power converters.","Reconfigurable topologies are compared against both conventional four-wire designs and an idealised, fully reconfigurable converter.","Results show that conventional converters need up to 75.3% greater capacity to yield a capability chart of equivalent size to an idealised reconfigurable converter.","The number and capacity of legs impact the capability chart's size, as do constraints on dc-side power injections.","The proposed approach shows significant promise for maximizing the utilization of power electronics used to mitigate impacts of phase unbalance."],"url":"http://arxiv.org/abs/2402.03978v1","category":"eess.SY"}
{"created":"2024-02-06 13:06:14","title":"Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time","abstract":"Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extra viewing time may be key to attain such robustness.","sentences":["Deep learning is closing the gap with humans on several object recognition benchmarks.","Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints.","We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition.","Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses.","Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks.","We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system.","Understanding the nature of the mental processes taking place during extra viewing time may be key to attain such robustness."],"url":"http://arxiv.org/abs/2402.03973v1","category":"cs.CV"}
{"created":"2024-02-06 13:01:21","title":"The CMS Fast Beam Condition Monitor for HL-LHC","abstract":"The high-luminosity upgrade of the LHC brings unprecedented requirements for real-time and precision bunch-by-bunch online luminosity measurement and beam-induced background monitoring. A key component of the CMS Beam Radiation, Instrumentation and Luminosity system is a stand-alone luminometer, the Fast Beam Condition Monitor (FBCM), which is fully independent from the CMS central trigger and data acquisition services and able to operate at all times with a triggerless readout. FBCM utilizes a dedicated front-end application-specific integrated circuit (ASIC) to amplify the signals from CO$_2$-cooled silicon-pad sensors with a timing resolution of a few nanoseconds, which enables the measurement of the beam-induced background. FBCM uses a modular design with two half-disks of twelve modules at each end of CMS, with four service modules placed close to the outer edge to reduce radiation-induced aging. The electronics system design adapts several components from the CMS Tracker for power, control and read-out functionalities. The dedicated FBCM23 ASIC contains six channels and adjustable shaping time to optimize the noise with regards to sensor leakage current. Each ASIC channel outputs a single binary high-speed asynchronous signal carrying time-of-arrival and time-over-threshold information. The chip output signal is digitized, encoded and sent via a radiation-hard gigabit transceiver and an optical link to the back-end electronics for analysis. This paper reports on the updated design of the FBCM detector and the ongoing testing program.","sentences":["The high-luminosity upgrade of the LHC brings unprecedented requirements for real-time and precision bunch-by-bunch online luminosity measurement and beam-induced background monitoring.","A key component of the CMS Beam Radiation, Instrumentation and Luminosity system is a stand-alone luminometer, the Fast Beam Condition Monitor (FBCM), which is fully independent from the CMS central trigger and data acquisition services and able to operate at all times with a triggerless readout.","FBCM utilizes a dedicated front-end application-specific integrated circuit (ASIC) to amplify the signals from CO$_2$-cooled silicon-pad sensors with a timing resolution of a few nanoseconds, which enables the measurement of the beam-induced background.","FBCM uses a modular design with two half-disks of twelve modules at each end of CMS, with four service modules placed close to the outer edge to reduce radiation-induced aging.","The electronics system design adapts several components from the CMS Tracker for power, control and read-out functionalities.","The dedicated FBCM23 ASIC contains six channels and adjustable shaping time to optimize the noise with regards to sensor leakage current.","Each ASIC channel outputs a single binary high-speed asynchronous signal carrying time-of-arrival and time-over-threshold information.","The chip output signal is digitized, encoded and sent via a radiation-hard gigabit transceiver and an optical link to the back-end electronics for analysis.","This paper reports on the updated design of the FBCM detector and the ongoing testing program."],"url":"http://arxiv.org/abs/2402.03971v1","category":"physics.ins-det"}
{"created":"2024-02-06 12:57:29","title":"Mutant fate in spatially structured populations on graphs: connecting models to experiments","abstract":"In nature, most microbial populations have complex spatial structures that can affect their evolution. Evolutionary graph theory predicts that some spatial structures modelled by placing individuals on the nodes of a graph affect the probability that a mutant will fix. Evolution experiments are beginning to explicitly address the impact of graph structures on mutant fixation. However, the assumptions of evolutionary graph theory differ from the conditions of modern evolution experiments, making the comparison between theory and experiment challenging. Here, we aim to bridge this gap. We use our new model of spatially structured populations with well-mixed demes at the nodes of a graph, which allows asymmetric migrations, can handle large populations, and explicitly models serial passage events with migrations, thus closely mimicking experimental conditions. We analyze recent experiments in this light. We suggest useful parameter regimes for future experiments, and we make quantitative predictions for these experiments. In particular, we propose experiments to directly test our recent prediction that the star graph with asymmetric migrations suppresses natural selection and can accelerate mutant fixation or extinction, compared to a well-mixed population.","sentences":["In nature, most microbial populations have complex spatial structures that can affect their evolution.","Evolutionary graph theory predicts that some spatial structures modelled by placing individuals on the nodes of a graph affect the probability that a mutant will fix.","Evolution experiments are beginning to explicitly address the impact of graph structures on mutant fixation.","However, the assumptions of evolutionary graph theory differ from the conditions of modern evolution experiments, making the comparison between theory and experiment challenging.","Here, we aim to bridge this gap.","We use our new model of spatially structured populations with well-mixed demes at the nodes of a graph, which allows asymmetric migrations, can handle large populations, and explicitly models serial passage events with migrations, thus closely mimicking experimental conditions.","We analyze recent experiments in this light.","We suggest useful parameter regimes for future experiments, and we make quantitative predictions for these experiments.","In particular, we propose experiments to directly test our recent prediction that the star graph with asymmetric migrations suppresses natural selection and can accelerate mutant fixation or extinction, compared to a well-mixed population."],"url":"http://arxiv.org/abs/2402.03967v1","category":"q-bio.PE"}
{"created":"2024-02-06 12:50:39","title":"A Novel Local and Hyper-Local Multicast Services Transmission Scheme for Beyond 5G Networks","abstract":"The efficiency of the broadcast network is impacted by the different types of services that may be transmitted over it. Global services serve users across the entire network, while local services cater to specific regions, and hyper-local services have even narrower coverage. Multimedia Broadcast over a Single-Frequency Network (MBSFN) is typically used for global service transmission while existing literature extensively discusses schemes for transmitting local or hyper-local services with or without Single Frequency Network (SFN) gain. However, these schemes fall short when network-wide requests for only local and hyper-local services are made, leading operators to scale down to either Single Cell-Point to Multipoint (SCPtM) or Multi-Frequency Network (MFN). SCPtM is highly susceptible to interference, and MFN requires substantial amounts of valuable spectrum. They both employ the Least Channel Gain (LCG) strategy for transmitting hyper-local services without SFN gain. Our proposed Local and Hyper-Local Services (LHS) transmission scheme utilizes the knowledge of user distribution and their corresponding radio link channel quality to schedule single or multi-resolution, local or hyper-local services within a three-cell cluster and aims to enhance spectral efficiency and maximize system throughput. It leverages Scalable Video Coding (SVC) in conjunction with Hierarchical Modulation (HM) for transmitting multi-resolution multimedia content to address the problem of heterogeneity amongst the multicast group users. The proposed scheme also employs macro-diversity combining with optimal HM parameters for each gNB catering to a local service area in order to minimize the service outage. System-level simulation results testify to the better performance achieved by the proposed LHS transmission scheme with respect to SCPtM.","sentences":["The efficiency of the broadcast network is impacted by the different types of services that may be transmitted over it.","Global services serve users across the entire network, while local services cater to specific regions, and hyper-local services have even narrower coverage.","Multimedia Broadcast over a Single-Frequency Network (MBSFN) is typically used for global service transmission while existing literature extensively discusses schemes for transmitting local or hyper-local services with or without Single Frequency Network (SFN) gain.","However, these schemes fall short when network-wide requests for only local and hyper-local services are made, leading operators to scale down to either Single Cell-Point to Multipoint (SCPtM) or Multi-Frequency Network (MFN).","SCPtM is highly susceptible to interference, and MFN requires substantial amounts of valuable spectrum.","They both employ the Least Channel Gain (LCG) strategy for transmitting hyper-local services without SFN gain.","Our proposed Local and Hyper-Local Services (LHS) transmission scheme utilizes the knowledge of user distribution and their corresponding radio link channel quality to schedule single or multi-resolution, local or hyper-local services within a three-cell cluster and aims to enhance spectral efficiency and maximize system throughput.","It leverages Scalable Video Coding (SVC) in conjunction with Hierarchical Modulation (HM) for transmitting multi-resolution multimedia content to address the problem of heterogeneity amongst the multicast group users.","The proposed scheme also employs macro-diversity combining with optimal HM parameters for each gNB catering to a local service area in order to minimize the service outage.","System-level simulation results testify to the better performance achieved by the proposed LHS transmission scheme with respect to SCPtM."],"url":"http://arxiv.org/abs/2402.03963v1","category":"eess.SP"}
{"created":"2024-02-06 12:28:27","title":"A linear dissipativity approach to incremental input-to-state stability for a class of positive Lur'e systems","abstract":"Incremental stability properties are considered for certain systems of forced, nonlinear differential equations with a particular positivity structure. An incremental stability estimate is derived for pairs of input/state/output trajectories of the Lur'e systems under consideration, from which a number of consequences are obtained, including the incremental exponential input-to-state stability property and certain input-output stability concepts with linear gain. Incremental stability estimates provide a basis for an investigation into the response to convergent and (almost) periodic forcing terms, and is treated presently. Our results show that an incremental version of the real Aizerman conjecture is true for positive Lur'e systems when an incremental gain condition is imposed on the nonlinear term, as we describe. Our argumentation is underpinned by linear dissipativity theory -- a property of positive linear control systems.","sentences":["Incremental stability properties are considered for certain systems of forced, nonlinear differential equations with a particular positivity structure.","An incremental stability estimate is derived for pairs of input/state/output trajectories of the Lur'e systems under consideration, from which a number of consequences are obtained, including the incremental exponential input-to-state stability property and certain input-output stability concepts with linear gain.","Incremental stability estimates provide a basis for an investigation into the response to convergent and (almost) periodic forcing terms, and is treated presently.","Our results show that an incremental version of the real Aizerman conjecture is true for positive Lur'e systems when an incremental gain condition is imposed on the nonlinear term, as we describe.","Our argumentation is underpinned by linear dissipativity theory -- a property of positive linear control systems."],"url":"http://arxiv.org/abs/2402.03955v1","category":"eess.SY"}
{"created":"2024-02-06 12:26:58","title":"Mixed Matrix Completion in Complex Survey Sampling under Heterogeneous Missingness","abstract":"Modern surveys with large sample sizes and growing mixed-type questionnaires require robust and scalable analysis methods. In this work, we consider recovering a mixed dataframe matrix, obtained by complex survey sampling, with entries following different canonical exponential distributions and subject to heterogeneous missingness. To tackle this challenging task, we propose a two-stage procedure: in the first stage, we model the entry-wise missing mechanism by logistic regression, and in the second stage, we complete the target parameter matrix by maximizing a weighted log-likelihood with a low-rank constraint. We propose a fast and scalable estimation algorithm that achieves sublinear convergence, and the upper bound for the estimation error of the proposed method is rigorously derived. Experimental results support our theoretical claims, and the proposed estimator shows its merits compared to other existing methods. The proposed method is applied to analyze the National Health and Nutrition Examination Survey data.","sentences":["Modern surveys with large sample sizes and growing mixed-type questionnaires require robust and scalable analysis methods.","In this work, we consider recovering a mixed dataframe matrix, obtained by complex survey sampling, with entries following different canonical exponential distributions and subject to heterogeneous missingness.","To tackle this challenging task, we propose a two-stage procedure: in the first stage, we model the entry-wise missing mechanism by logistic regression, and in the second stage, we complete the target parameter matrix by maximizing a weighted log-likelihood with a low-rank constraint.","We propose a fast and scalable estimation algorithm that achieves sublinear convergence, and the upper bound for the estimation error of the proposed method is rigorously derived.","Experimental results support our theoretical claims, and the proposed estimator shows its merits compared to other existing methods.","The proposed method is applied to analyze the National Health and Nutrition Examination Survey data."],"url":"http://arxiv.org/abs/2402.03954v1","category":"stat.ME"}
{"created":"2024-02-06 12:23:00","title":"Banach algebra mappings preserving the invertibility of linear pencils","abstract":"Let $A$ and $B$ be complex unital Banach algebras, and let $\\varphi, \\psi: A \\to B$ be surjective mappings. If $A$ is semisimple with an essential socle and $\\varphi$ and $\\psi$ preserves the invertibility of linear pencils in both directions, that is, for any $x, y \\in A$ and $\\lambda \\in \\mathbb{C}$, $\\lambda x+y$ is invertible in $A$ if and only if $\\lambda \\varphi(x) + \\psi(y)$ is invertible in $B$, then we show that there exists an invertible element $u$ in $B$ and a Jordan isomorphism $J: A \\to B$ such that $\\varphi(x) = \\psi(x) = uJ(x)$ for all $x \\in A$.","sentences":["Let $A$ and $B$ be complex unital Banach algebras, and let $\\varphi, \\psi: A \\to B$ be surjective mappings.","If $A$ is semisimple with an essential socle and $\\varphi$ and $\\psi$ preserves the invertibility of linear pencils in both directions, that is, for any $x, y \\in A$ and $\\lambda \\in \\mathbb{C}$, $\\lambda x+y$ is invertible in $A$ if and only if $\\lambda \\varphi(x) + \\psi(y)$ is invertible in $B$, then we show that there exists an invertible element $u$ in $B$ and a Jordan isomorphism $J: A \\to B$ such that $\\varphi(x) = \\psi(x) = uJ(x)$ for all $x \\in A$."],"url":"http://arxiv.org/abs/2402.03950v1","category":"math.FA"}
{"created":"2024-02-06 12:15:03","title":"Formation of a boron-oxide termination for the (100) diamond surface","abstract":"A boron-oxide termination of the diamond (100) surface has been formed by depositing molecular boron oxide $\\rm{B_2O_3}$ onto the hydrogen-terminated (100) diamond surface under ultrahigh vacuum conditions and annealing to $\\rm{950^{\\circ} C}$. The resulting termination was highly oriented and chemically homogeneous, although further optimisation is required to increase the surface coverage beyond the 0.4 ML achieved here. This work demonstrates the possibility of using molecular deposition under ultrahigh vacuum conditions for complex surface engineering of the diamond surface, and may be a first step in an alternative approach to fabricating boron doped delta layers in diamond.","sentences":["A boron-oxide termination of the diamond (100) surface has been formed by depositing molecular boron oxide $\\rm{B_2O_3}$ onto the hydrogen-terminated (100) diamond surface under ultrahigh vacuum conditions and annealing to $\\rm{950^{\\circ} C}$.","The resulting termination was highly oriented and chemically homogeneous, although further optimisation is required to increase the surface coverage beyond the 0.4 ML achieved here.","This work demonstrates the possibility of using molecular deposition under ultrahigh vacuum conditions for complex surface engineering of the diamond surface, and may be a first step in an alternative approach to fabricating boron doped delta layers in diamond."],"url":"http://arxiv.org/abs/2402.03940v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 12:09:34","title":"Thermal transport through a single trapped ion under strong laser illumination","abstract":"In this work, we study quantum heat transport in a single trapped ion, driven by laser excitation and coupled to thermal reservoirs operating at different temperatures. Our focus lies in understanding how different laser coupling scenarios impact the system dynamics. As the laser intensity reaches a regime where the ion's electronic and motional degrees of freedom strongly couple, traditional approaches using phenomenological models for thermal reservoirs become inadequate. Therefore, the adoption of the dressed master equation (DME) formalism becomes crucial, enabling a deeper understanding of how distinct laser intensities influence heat transport. Analyzing the heat current within the parameter space defined by detuning and coupling strength, we observe intriguing circular patterns which are influenced by the ion's vibrational frequency and laser parameters, and reveal nuanced relationships between heat transport, residual coherence, and system characteristics. Our study also reveals phenomena such as negative differential heat conductivity and asymmetry in heat current flow, offering insights into the thermal properties of this essential quantum technology setup.","sentences":["In this work, we study quantum heat transport in a single trapped ion, driven by laser excitation and coupled to thermal reservoirs operating at different temperatures.","Our focus lies in understanding how different laser coupling scenarios impact the system dynamics.","As the laser intensity reaches a regime where the ion's electronic and motional degrees of freedom strongly couple, traditional approaches using phenomenological models for thermal reservoirs become inadequate.","Therefore, the adoption of the dressed master equation (DME) formalism becomes crucial, enabling a deeper understanding of how distinct laser intensities influence heat transport.","Analyzing the heat current within the parameter space defined by detuning and coupling strength, we observe intriguing circular patterns which are influenced by the ion's vibrational frequency and laser parameters, and reveal nuanced relationships between heat transport, residual coherence, and system characteristics.","Our study also reveals phenomena such as negative differential heat conductivity and asymmetry in heat current flow, offering insights into the thermal properties of this essential quantum technology setup."],"url":"http://arxiv.org/abs/2402.03937v1","category":"quant-ph"}
{"created":"2024-02-06 12:09:04","title":"On the Accuracy of Phase Extraction from a Known-Frequency Noisy Sinusoidal Signal","abstract":"Accurate phase extraction from sinusoidal signals is a crucial task in various signal processing applications. While prior research predominantly addresses the case of asynchronous sampling with unknown signal frequency, this study focuses on the more specific situation where synchronous sampling is possible, and the signal's frequency is known. In this framework, a comprehensive analysis of phase estimation accuracy in the presence of both additive and phase noises is presented. A closed-form expression for the asymptotic Probability Density Function (PDF) of the resulting phase estimator is presented, and validated by simulations that depict Root Mean Square Error (RMSE) trends in different noise scenarios. The latter estimator is asymptotically efficient, exhibiting fast convergence towards its Cram\\'er-Rao Lower Bound (CRLB). Three distinct RMSE behaviours were identified depending on the Signal to Noise Ratio (SNR), sample count (N), and noise level: (i) saturation towards a random guess at low SNR values, (ii) linear decreasing relationship with the square roots of N and SNR at moderate noise levels, and (iii) saturation at high SNR towards a noise floor function of the phase noise level. By quantifying the impact of sample count, additive noise, and phase noise on phase estimation accuracy, this work provides valuable insights for designing systems that require precise phase extraction, such as phase-based fluorescence assays or system identification.","sentences":["Accurate phase extraction from sinusoidal signals is a crucial task in various signal processing applications.","While prior research predominantly addresses the case of asynchronous sampling with unknown signal frequency, this study focuses on the more specific situation where synchronous sampling is possible, and the signal's frequency is known.","In this framework, a comprehensive analysis of phase estimation accuracy in the presence of both additive and phase noises is presented.","A closed-form expression for the asymptotic Probability Density Function (PDF) of the resulting phase estimator is presented, and validated by simulations that depict Root Mean Square Error (RMSE) trends in different noise scenarios.","The latter estimator is asymptotically efficient, exhibiting fast convergence towards its Cram\\'er-Rao Lower Bound (CRLB).","Three distinct RMSE behaviours were identified depending on the Signal to Noise Ratio (SNR), sample count (N), and noise level: (i) saturation towards a random guess at low SNR values, (ii) linear decreasing relationship with the square roots of N and SNR at moderate noise levels, and (iii) saturation at high SNR towards a noise floor function of the phase noise level.","By quantifying the impact of sample count, additive noise, and phase noise on phase estimation accuracy, this work provides valuable insights for designing systems that require precise phase extraction, such as phase-based fluorescence assays or system identification."],"url":"http://arxiv.org/abs/2402.03935v1","category":"eess.SP"}
{"created":"2024-02-06 12:03:05","title":"Revisiting the Lee-Yang singularities in the four-dimensional Ising model: A tribute to the memory of Ralph Kenna","abstract":"We have studied numerically the Lee-Yang singularities of the four dimensional Ising model at criticality, which is believed to be in the same universality class as the $\\phi_4^4$ scalar field theory. We have focused in the numerical characterization of the logarithmic corrections to the scaling of the zeros of the partition function and its cumulative probability distribution, finding a very good agreement with the predictions of the renormalization group computation on the $\\phi_4^4$ scalar field theory. We have found that this agreement improves much more with the order of the Lee-Yang zeros. To obtain these results, we have extended a previous study [R. Kenna and C. B. Lang, Nucl. Phys. {\\bf B393} 461 (1993)] in which were computed numerically the first two zeros for $L\\le 24$ lattices, to the computation of the first four zeros for $L\\le 64$ lattices.","sentences":["We have studied numerically the Lee-Yang singularities of the four dimensional Ising model at criticality, which is believed to be in the same universality class as the $\\phi_4^4$ scalar field theory.","We have focused in the numerical characterization of the logarithmic corrections to the scaling of the zeros of the partition function and its cumulative probability distribution, finding a very good agreement with the predictions of the renormalization group computation on the $\\phi_4^4$ scalar field theory.","We have found that this agreement improves much more with the order of the Lee-Yang zeros.","To obtain these results, we have extended a previous study [R. Kenna and C. B. Lang, Nucl.","Phys. {\\bf B393} 461 (1993)] in which were computed numerically the first two zeros for $L\\le 24$ lattices, to the computation of the first four zeros for $L\\le 64$ lattices."],"url":"http://arxiv.org/abs/2402.03932v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-06 12:01:00","title":"Fully autonomous tuning of a spin qubit","abstract":"Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs. However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits. Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces. This presents a real 'needle in the haystack' problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections. In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation. We demonstrate this automation, achieved without human intervention, in a Ge/Si core/shell nanowire device. Our approach integrates deep learning, Bayesian optimization, and computer vision techniques. We expect this automation algorithm to apply to a wide range of semiconductor qubit devices, allowing for statistical studies of qubit quality metrics. As a demonstration of the potential of full automation, we characterise how the Rabi frequency and g-factor depend on barrier gate voltages for one of the qubits found by the algorithm. Twenty years after the initial demonstrations of spin qubit operation, this significant advancement is poised to finally catalyze the operation of large, previously unexplored quantum circuits.","sentences":["Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs.","However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits.","Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces.","This presents a real 'needle in the haystack' problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections.","In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation.","We demonstrate this automation, achieved without human intervention, in a Ge/Si core/shell nanowire device.","Our approach integrates deep learning, Bayesian optimization, and computer vision techniques.","We expect this automation algorithm to apply to a wide range of semiconductor qubit devices, allowing for statistical studies of qubit quality metrics.","As a demonstration of the potential of full automation, we characterise how the Rabi frequency and g-factor depend on barrier gate voltages for one of the qubits found by the algorithm.","Twenty years after the initial demonstrations of spin qubit operation, this significant advancement is poised to finally catalyze the operation of large, previously unexplored quantum circuits."],"url":"http://arxiv.org/abs/2402.03931v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 11:39:35","title":"Sensing Mutual Information with Random Signals in Gaussian Channels: Bridging Sensing and Communication Metrics","abstract":"Sensing performance is typically evaluated by classical radar metrics, such as Cramer-Rao bound and signal-to-clutter-plus-noise ratio. The recent development of the integrated sensing and communication (ISAC) framework motivated the efforts to unify the performance metric for sensing and communication, where mutual information (MI) was proposed as a sensing performance metric with deterministic signals. However, the need of communication in ISAC systems necessitates the transmission of random signals for sensing applications, whereas an explicit evaluation for the sensing mutual information (SMI) with random signals is not yet available in the literature. This paper aims to fill the research gap and investigate the unification of sensing and communication performance metrics. For that purpose, we first derive the explicit expression for the SMI with random signals utilizing random matrix theory. On top of that, we further build up the connections between SMI and traditional sensing metrics, such as ergodic minimum mean square error (EMMSE), ergodic linear minimum mean square error (ELMMSE), and ergodic Bayesian Cram\\'{e}r-Rao bound (EBCRB). Such connections open up the opportunity to unify sensing and communication performance metrics, which facilitates the analysis and design for ISAC systems. Finally, SMI is utilized to optimize the precoder for both sensing-only and ISAC applications. Simulation results validate the accuracy of the theoretical results and the effectiveness of the proposed precoding designs.","sentences":["Sensing performance is typically evaluated by classical radar metrics, such as Cramer-Rao bound and signal-to-clutter-plus-noise ratio.","The recent development of the integrated sensing and communication (ISAC) framework motivated the efforts to unify the performance metric for sensing and communication, where mutual information (MI) was proposed as a sensing performance metric with deterministic signals.","However, the need of communication in ISAC systems necessitates the transmission of random signals for sensing applications, whereas an explicit evaluation for the sensing mutual information (SMI) with random signals is not yet available in the literature.","This paper aims to fill the research gap and investigate the unification of sensing and communication performance metrics.","For that purpose, we first derive the explicit expression for the SMI with random signals utilizing random matrix theory.","On top of that, we further build up the connections between SMI and traditional sensing metrics, such as ergodic minimum mean square error (EMMSE), ergodic linear minimum mean square error (ELMMSE), and ergodic Bayesian Cram\\'{e}r-Rao bound (EBCRB).","Such connections open up the opportunity to unify sensing and communication performance metrics, which facilitates the analysis and design for ISAC systems.","Finally, SMI is utilized to optimize the precoder for both sensing-only and ISAC applications.","Simulation results validate the accuracy of the theoretical results and the effectiveness of the proposed precoding designs."],"url":"http://arxiv.org/abs/2402.03919v1","category":"cs.IT"}
{"created":"2024-02-06 11:33:57","title":"Can Large Language Models Detect Rumors on Social Media?","abstract":"In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.","sentences":["In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media.","However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information.","Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden.","We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%.","Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios."],"url":"http://arxiv.org/abs/2402.03916v1","category":"cs.IR"}
{"created":"2024-02-06 11:28:14","title":"Machine learning stochastic differential equations for the evolution of order parameters of classical many-body systems in and out of equilibrium","abstract":"We develop a machine learning algorithm to infer the emergent stochastic equation governing the evolution of an order parameter of a many-body system. We train our neural network to independently learn the directed force acting on the order parameter as well as an effective diffusive noise. We illustrate our approach using the classical Ising model endowed with Glauber dynamics, and the contact process as test cases. For both models, which represent paradigmatic equilibrium and nonequilibrium scenarios, the directed force and noise can be efficiently inferred. The directed force term of the Ising model allows us to reconstruct an effective potential for the order parameter which develops the characteristic double-well shape below the critical temperature. Despite its genuine nonequilibrium nature, such an effective potential can also be obtained for the contact process and its shape signals a phase transition into an absorbing state. Also, in contrast to the equilibrium Ising model, the presence of an absorbing state renders the noise term dependent on the value of the order parameter itself.","sentences":["We develop a machine learning algorithm to infer the emergent stochastic equation governing the evolution of an order parameter of a many-body system.","We train our neural network to independently learn the directed force acting on the order parameter as well as an effective diffusive noise.","We illustrate our approach using the classical Ising model endowed with Glauber dynamics, and the contact process as test cases.","For both models, which represent paradigmatic equilibrium and nonequilibrium scenarios, the directed force and noise can be efficiently inferred.","The directed force term of the Ising model allows us to reconstruct an effective potential for the order parameter which develops the characteristic double-well shape below the critical temperature.","Despite its genuine nonequilibrium nature, such an effective potential can also be obtained for the contact process and its shape signals a phase transition into an absorbing state.","Also, in contrast to the equilibrium Ising model, the presence of an absorbing state renders the noise term dependent on the value of the order parameter itself."],"url":"http://arxiv.org/abs/2402.03913v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-06 11:22:36","title":"Dust and Cold Gas Properties of Starburst HyLIRG-Quasars at $z \\sim 2.5$","abstract":"Some high-z active galactic nuclei (AGNs) are found to reside in extreme star-forming galaxies, such as hyper-luminous infrared galaxies (HyLIRGs), with AGN-removed $L_{\\rm{IR}}$ of $>10^{13} L_{\\rm{\\odot}}$. In this paper, we report NOEMA observations of six apparent starburst HyLIRGs associated with optical quasars at $z\\sim2-3$ in the Stripe 82 field, to study their dust and molecular CO properties. Five out of the six candidates are detected with CO(4-3) or CO(5-4) emission, and four in 2mm dust continuum. Based on the linewidth-$L'_{\\rm{CO(1-0)}}$ diagnostics, we find that four galaxies are likely unlensed or weakly lensed sources. The molecular gas mass is in the range of $\\mu M_{\\rm{H_2}} \\sim0.8-9.7\\times10^{10} M_{\\odot}$ (with $\\alpha = 0.8 M_{\\odot} (\\rm{K km s^{-1} pc^2})^{-1}$ and $\\mu$ is the unknown possible gravitational magnification factor). We fit their SEDs, after including the observed 2mm fluxes and upper limits, and estimate their apparent (uncorrected for possible lensing effect) star formation rates ($\\mu$SFRs) to be $\\sim400-2500$ $M_{\\rm{\\odot}} \\rm{yr^{-1}}$ with depletion time of $\\sim20-110$ Myr. We notice interesting offsets, of $\\sim10-40$ kpc spatially or $\\sim1000-2000$ km s$^{-1}$ spectroscopically, between the optical quasar and the mm continuum or CO emissions. The observed velocity shift is likely related to the blueshifted broad-emission-line region of quasars, though mergers or recoiling black holes are also possible causes, which can explain the spatial offset and the high intrinsic SFRs in the HyLIRG-quasar systems.","sentences":["Some high-z active galactic nuclei (AGNs) are found to reside in extreme star-forming galaxies, such as hyper-luminous infrared galaxies (HyLIRGs), with AGN-removed $L_{\\rm{IR}}$ of $>10^{13} L_{\\rm{\\odot}}$. In this paper, we report NOEMA observations of six apparent starburst HyLIRGs associated with optical quasars at $z\\sim2-3$ in the Stripe 82 field, to study their dust and molecular CO properties.","Five out of the six candidates are detected with CO(4-3) or CO(5-4) emission, and four in 2mm dust continuum.","Based on the linewidth-$L'_{\\rm{CO(1-0)}}$ diagnostics, we find that four galaxies are likely unlensed or weakly lensed sources.","The molecular gas mass is in the range of $\\mu M_{\\rm{H_2}} \\sim0.8-9.7\\times10^{10} M_{\\odot}$","(with $\\alpha = 0.8 M_{\\odot} (\\rm{K km s^{-1} pc^2})^{-1}$ and $\\mu$ is the unknown possible gravitational magnification factor).","We fit their SEDs, after including the observed 2mm fluxes and upper limits, and estimate their apparent (uncorrected for possible lensing effect) star formation rates ($\\mu$SFRs) to be $\\sim400-2500$ $M_{\\rm{\\odot}} \\rm{yr^{-1}}$ with depletion time of $\\sim20-110$ Myr.","We notice interesting offsets, of $\\sim10-40$ kpc spatially or $\\sim1000-2000$ km s$^{-1}$ spectroscopically, between the optical quasar and the mm continuum or CO emissions.","The observed velocity shift is likely related to the blueshifted broad-emission-line region of quasars, though mergers or recoiling black holes are also possible causes, which can explain the spatial offset and the high intrinsic SFRs in the HyLIRG-quasar systems."],"url":"http://arxiv.org/abs/2402.03909v1","category":"astro-ph.GA"}
{"created":"2024-02-06 11:17:16","title":"Employee Turnover Analysis Using Machine Learning Algorithms","abstract":"Employee's knowledge is an organization asset. Turnover may impose apparent and hidden costs and irreparable damages. To overcome and mitigate this risk, employee's condition should be monitored. Due to high complexity of analyzing well-being features, employee's turnover predicting can be delegated to machine learning techniques. In this paper, we discuss employee's attrition rate. Three different supervised learning algorithms comprising AdaBoost, SVM and RandomForest are used to benchmark employee attrition accuracy. Attained models can help out at establishing predictive analytics.","sentences":["Employee's knowledge is an organization asset.","Turnover may impose apparent and hidden costs and irreparable damages.","To overcome and mitigate this risk, employee's condition should be monitored.","Due to high complexity of analyzing well-being features, employee's turnover predicting can be delegated to machine learning techniques.","In this paper, we discuss employee's attrition rate.","Three different supervised learning algorithms comprising AdaBoost, SVM and RandomForest are used to benchmark employee attrition accuracy.","Attained models can help out at establishing predictive analytics."],"url":"http://arxiv.org/abs/2402.03905v1","category":"cs.LG"}
{"created":"2024-02-06 11:13:54","title":"A phase transition between positional and semantic learning in a solvable model of dot-product attention","abstract":"We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.","sentences":["We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning).","For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism.","On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices.","In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape.","We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity.","Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data."],"url":"http://arxiv.org/abs/2402.03902v1","category":"cs.LG"}
{"created":"2024-02-06 10:55:23","title":"Control-Flow Refinement for Probabilistic Programs in KoAT","abstract":"Recently, we showed how to use control-flow refinement (CFR) to improve automatic complexity analysis of integer programs. While up to now CFR was limited to classical programs, in this paper we extend CFR to probabilistic programs and show its soundness for complexity analysis. To demonstrate its benefits, we implemented our new CFR technique in our complexity analysis tool KoAT.","sentences":["Recently, we showed how to use control-flow refinement (CFR) to improve automatic complexity analysis of integer programs.","While up to now CFR was limited to classical programs, in this paper we extend CFR to probabilistic programs and show its soundness for complexity analysis.","To demonstrate its benefits, we implemented our new CFR technique in our complexity analysis tool KoAT."],"url":"http://arxiv.org/abs/2402.03891v1","category":"cs.LO"}
{"created":"2024-02-06 10:53:59","title":"Loss and decoherence in superconducting circuits on silicon: Insights from electron spin resonance","abstract":"Solid-state devices used for quantum computation and quantum sensing applications are adversely affected by loss and noise caused by spurious, charged two-level systems (TLS) and stray paramagnetic spins. These two sources of noise are interconnected, exacerbating the impact on circuit performance. We use an on-chip electron spin resonance (ESR) technique, with niobium nitride (NbN) superconducting resonators, to study surface spins on silicon and the effect of post-fabrication surface treatments. We identify two distinct spin species that are characterized by different spin-relaxation times and respond selectively to various surface treatments (annealing and hydrofluoric acid). Only one of the two spin species has a significant impact on the TLS-limited resonator quality factor at low-power (near single-photon) excitation. We observe a 3-to-5-fold reduction in the total density of spins after surface treatments, and demonstrate the efficacy of ESR spectroscopy in developing strategies to mitigate loss and decoherence in quantum systems.","sentences":["Solid-state devices used for quantum computation and quantum sensing applications are adversely affected by loss and noise caused by spurious, charged two-level systems (TLS) and stray paramagnetic spins.","These two sources of noise are interconnected, exacerbating the impact on circuit performance.","We use an on-chip electron spin resonance (ESR) technique, with niobium nitride (NbN) superconducting resonators, to study surface spins on silicon and the effect of post-fabrication surface treatments.","We identify two distinct spin species that are characterized by different spin-relaxation times and respond selectively to various surface treatments (annealing and hydrofluoric acid).","Only one of the two spin species has a significant impact on the TLS-limited resonator quality factor at low-power (near single-photon) excitation.","We observe a 3-to-5-fold reduction in the total density of spins after surface treatments, and demonstrate the efficacy of ESR spectroscopy in developing strategies to mitigate loss and decoherence in quantum systems."],"url":"http://arxiv.org/abs/2402.03889v1","category":"quant-ph"}
{"created":"2024-02-06 10:49:28","title":"Shifting social norms as a driving force for linguistic change: Struggles about language and gender in the German Bundestag","abstract":"This paper focuses on language change based on shifting social norms, in particular with regard to the debate on language and gender. It is a recurring argument in this debate that language develops \"naturally\" and that \"severe interventions\" - such as gender-inclusive language is often claimed to be - in the allegedly \"organic\" language system are inappropriate and even \"dangerous\". Such interventions are, however, not unprecedented. Socially motivated processes of language change are neither unusual nor new. We focus in our contribution on one important political-social space in Germany, the German Bundestag. Taking other struggles about language and gender in the plenaries of the Bundestag as a starting point, our article illustrates that language and gender has been a recurring issue in the German Bundestag since the 1980s. We demonstrate how this is reflected in linguistic practices of the Bundestag, by the use of a) designations for gays and lesbians; b) pair forms such as B\\\"urgerinnen und B\\\"urger (female and male citizens); and c) female forms of addresses and personal nouns ('Pr\\\"asidentin' in addition to 'Pr\\\"asident'). Lastly, we will discuss implications of these earlier language battles for the currently very heated debate about gender-inclusive language, especially regarding new forms with gender symbols like the asterisk or the colon (Lehrer*innen, Lehrer:innen; male*female teachers) which are intended to encompass all gender identities.","sentences":["This paper focuses on language change based on shifting social norms, in particular with regard to the debate on language and gender.","It is a recurring argument in this debate that language develops \"naturally\" and that \"severe interventions\" - such as gender-inclusive language is often claimed to be - in the allegedly \"organic\" language system are inappropriate and even \"dangerous\".","Such interventions are, however, not unprecedented.","Socially motivated processes of language change are neither unusual nor new.","We focus in our contribution on one important political-social space in Germany, the German Bundestag.","Taking other struggles about language and gender in the plenaries of the Bundestag as a starting point, our article illustrates that language and gender has been a recurring issue in the German Bundestag since the 1980s.","We demonstrate how this is reflected in linguistic practices of the Bundestag, by the use of a) designations for gays and lesbians; b) pair forms such as B\\\"urgerinnen und B\\\"urger (female and male citizens); and c) female forms of addresses and personal nouns ('Pr\\\"asidentin' in addition to 'Pr\\\"asident').","Lastly, we will discuss implications of these earlier language battles for the currently very heated debate about gender-inclusive language, especially regarding new forms with gender symbols like the asterisk or the colon (Lehrer*innen, Lehrer:innen; male*female teachers) which are intended to encompass all gender identities."],"url":"http://arxiv.org/abs/2402.03887v1","category":"cs.CL"}
{"created":"2024-02-06 10:48:59","title":"Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network Approach","abstract":"Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement. We study full-duplex transmissions as an effective way to improve mmWave MIMO systems. Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency. However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible. In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs). Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system. Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals. To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs. We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels). Our work provides novel insights into NN-based channel estimators.","sentences":["Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement.","We study full-duplex transmissions as an effective way to improve mmWave MIMO systems.","Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency.","However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible.","In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs).","Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system.","Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals.","To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs.","We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels).","Our work provides novel insights into NN-based channel estimators."],"url":"http://arxiv.org/abs/2402.03886v1","category":"cs.IT"}
{"created":"2024-02-06 10:39:48","title":"Quantum Trajectories. Spectral Gap, Quasi-compactness & Limit Theorems","abstract":"Quantum trajectories are Markov processes modeling the evolution of a quantum system subjected to repeated independent measurements. Inspired by the theory of random products of matrices, it has been shown that these Markov processes admit a unique invariant measure under a purification and an irreducibility assumptions. This paper is devoted to the spectral study of the underlying Markov operator. Using Quasi-compactness, it is shown that this operator admits a spectral gap and the peripheral spectrum is described in a precise manner. Next two perturbations of this operator are studied. This allows to derive limit theorems (Central Limit Theorem, Berry-Esseen bounds and Large Deviation Principle) for the empirical mean of functions of the Markov chain as well as the Lyapounov exponent of the underlying random dynamical system.","sentences":["Quantum trajectories are Markov processes modeling the evolution of a quantum system subjected to repeated independent measurements.","Inspired by the theory of random products of matrices, it has been shown that these Markov processes admit a unique invariant measure under a purification and an irreducibility assumptions.","This paper is devoted to the spectral study of the underlying Markov operator.","Using Quasi-compactness, it is shown that this operator admits a spectral gap and the peripheral spectrum is described in a precise manner.","Next two perturbations of this operator are studied.","This allows to derive limit theorems (Central Limit Theorem, Berry-Esseen bounds and Large Deviation Principle) for the empirical mean of functions of the Markov chain as well as the Lyapounov exponent of the underlying random dynamical system."],"url":"http://arxiv.org/abs/2402.03879v1","category":"math.PR"}
{"created":"2024-02-06 10:34:28","title":"Convolutional Neural Networks and Volcano Plots: Screening and Prediction of Two-Dimensional Single-Atom Catalysts","abstract":"Single-atom catalysts (SACs) have emerged as frontiers for catalyzing chemical reactions, yet the diverse combinations of active elements and support materials, the nature of coordination environments, elude traditional methodologies in searching optimal SAC systems with superior catalytic performance. Herein, by integrating multi-branch Convolutional Neural Network (CNN) analysis models to hybrid descriptor based activity volcano plot, 2D SAC system composed of diverse metallic single atoms anchored on six type of 2D supports, including graphitic carbon nitride, nitrogen-doped graphene, graphene with dual-vacancy, black phosphorous, boron nitride, and C2N, are screened for efficient CO2RR. Starting from establishing a correlation map between the adsorption energies of intermediates and diverse electronic and elementary descriptors, sole singular descriptor lost magic to predict catalytic activity. Deep learning method utilizing multi-branch CNN model therefore was employed, using 2D electronic density of states as input to predict adsorption energies. Hybrid-descriptor enveloping both C- and O-types of CO2RR intermediates was introduced to construct volcano plots and limiting potential periodic table, aiming for intuitive screening of catalyst candidates for efficient CO2 reduction to CH4. The eDOS occlusion experiments were performed to unravel individual orbital contribution to adsorption energy. To explore the electronic scale principle governing practical engineering catalytic CO2RR activity, orbitalwise eDOS shifting experiments based on CNN model were employed. The study involves examining the adsorption energy and, consequently, catalytic activities while varying supported single atoms. This work offers a tangible framework to inform both theoretical screening and experimental synthesis, thereby paving the way for systematically designing efficient SACs.","sentences":["Single-atom catalysts (SACs) have emerged as frontiers for catalyzing chemical reactions, yet the diverse combinations of active elements and support materials, the nature of coordination environments, elude traditional methodologies in searching optimal SAC systems with superior catalytic performance.","Herein, by integrating multi-branch Convolutional Neural Network (CNN) analysis models to hybrid descriptor based activity volcano plot, 2D SAC system composed of diverse metallic single atoms anchored on six type of 2D supports, including graphitic carbon nitride, nitrogen-doped graphene, graphene with dual-vacancy, black phosphorous, boron nitride, and C2N, are screened for efficient CO2RR.","Starting from establishing a correlation map between the adsorption energies of intermediates and diverse electronic and elementary descriptors, sole singular descriptor lost magic to predict catalytic activity.","Deep learning method utilizing multi-branch CNN model therefore was employed, using 2D electronic density of states as input to predict adsorption energies.","Hybrid-descriptor enveloping both C- and O-types of CO2RR intermediates was introduced to construct volcano plots and limiting potential periodic table, aiming for intuitive screening of catalyst candidates for efficient CO2 reduction to CH4.","The eDOS occlusion experiments were performed to unravel individual orbital contribution to adsorption energy.","To explore the electronic scale principle governing practical engineering catalytic CO2RR activity, orbitalwise eDOS shifting experiments based on CNN model were employed.","The study involves examining the adsorption energy and, consequently, catalytic activities while varying supported single atoms.","This work offers a tangible framework to inform both theoretical screening and experimental synthesis, thereby paving the way for systematically designing efficient SACs."],"url":"http://arxiv.org/abs/2402.03876v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 10:33:59","title":"Emergent flux in a one-dimensional Bose-Fermi mixture","abstract":"We find a novel chiral superfluid (CSF) phase in a chain of Bose-Fermi mixture, which has been validated using two unbiased numerical methods, density matrix renormalization group and Grassmann multi-scale entanglement renormalization ansatz. The system hosts the interplay between two types of fermions: bare spinless fermions and composite fermions, the latter consisting of a fermion and a boson. In the CSF phase, bosons condensate at non-zero momentum |2{\\pi}/L|, where L represents the size of the chain. In essence, the local superfluid order parameter continuously rotates along the chain, indicating that the 1D CSF phase spontaneously breaks time-reversal symmetry. This symmetry breaking gives rise to an emergent flux in the background, effectively optimizing the kinetic energy of the composite fermions within the system. We provide a physical understanding at the mean-field level. Furthermore, we demonstrate that the 1D CSF phase can emerge in a more widely applicable extended Hubbard model. The potential realization of this phase in cold-atom experiments has also been explored.","sentences":["We find a novel chiral superfluid (CSF) phase in a chain of Bose-Fermi mixture, which has been validated using two unbiased numerical methods, density matrix renormalization group and Grassmann multi-scale entanglement renormalization ansatz.","The system hosts the interplay between two types of fermions: bare spinless fermions and composite fermions, the latter consisting of a fermion and a boson.","In the CSF phase, bosons condensate at non-zero momentum |2{\\pi}/L|, where L represents the size of the chain.","In essence, the local superfluid order parameter continuously rotates along the chain, indicating that the 1D CSF phase spontaneously breaks time-reversal symmetry.","This symmetry breaking gives rise to an emergent flux in the background, effectively optimizing the kinetic energy of the composite fermions within the system.","We provide a physical understanding at the mean-field level.","Furthermore, we demonstrate that the 1D CSF phase can emerge in a more widely applicable extended Hubbard model.","The potential realization of this phase in cold-atom experiments has also been explored."],"url":"http://arxiv.org/abs/2402.03875v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-06 10:32:39","title":"Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers","abstract":"Geometric quantum machine learning (GQML) aims to embed problem symmetries for learning efficient solving protocols. However, the question remains if (G)QML can be routinely used for constructing protocols with an exponential separation from classical analogs. In this Letter we consider Simon's problem for learning properties of Boolean functions, and show that this can be related to an unsupervised circuit classification problem. Using the workflow of geometric QML, we learn from first principles Simon's algorithm, thus discovering an example of BQP$^A\\neq$BPP protocol with respect to some dataset (oracle $A$). Our key findings include the development of an equivariant feature map for embedding Boolean functions, based on twirling with respect to identified bitflip and permutational symmetries, and measurement based on invariant observables with a sampling advantage. The proposed workflow points to the importance of data embeddings and classical post-processing, while keeping the variational circuit as a trivial identity operator. Next, developing the intuition for the function learning, we visualize instances as directed computational hypergraphs, and observe that the GQML protocol can access their global topological features for distinguishing bijective and surjective functions. Finally, we discuss the prospects for learning other BQP$^A$-type protocols, and conjecture that this depends on the ability of simplifying embeddings-based oracles $A$ applied as a linear combination of unitaries.","sentences":["Geometric quantum machine learning (GQML) aims to embed problem symmetries for learning efficient solving protocols.","However, the question remains if (G)QML can be routinely used for constructing protocols with an exponential separation from classical analogs.","In this Letter we consider Simon's problem for learning properties of Boolean functions, and show that this can be related to an unsupervised circuit classification problem.","Using the workflow of geometric QML, we learn from first principles Simon's algorithm, thus discovering an example of BQP$^A\\neq$BPP protocol with respect to some dataset (oracle $A$).","Our key findings include the development of an equivariant feature map for embedding Boolean functions, based on twirling with respect to identified bitflip and permutational symmetries, and measurement based on invariant observables with a sampling advantage.","The proposed workflow points to the importance of data embeddings and classical post-processing, while keeping the variational circuit as a trivial identity operator.","Next, developing the intuition for the function learning, we visualize instances as directed computational hypergraphs, and observe that the GQML protocol can access their global topological features for distinguishing bijective and surjective functions.","Finally, we discuss the prospects for learning other BQP$^A$-type protocols, and conjecture that this depends on the ability of simplifying embeddings-based oracles $A$ applied as a linear combination of unitaries."],"url":"http://arxiv.org/abs/2402.03871v1","category":"quant-ph"}
{"created":"2024-02-06 10:26:06","title":"Design and implementation of multiprotocol framework for residential prosumer incorporation in flexibility markets","abstract":"The growth of distributed renewable energy in the electrical grid presents challenges to its stability and quality. To address this at the local level, flexibility energy strategies emerge as an innovative technique. However, managing these strategies in residential areas becomes complex due to the unique characteristics of each prosumer. A major challenge lies in managing communication among diverse devices with different protocols. To address these issues, a comprehensive framework is designed and implemented to facilitate prosumers' integration in flexibility strategies, addressing communication at various levels. The effectiveness of the proposed framework is demonstrated through its implementation in a real smart home environment with diverse devices. The framework enables seamless integration and communication between IoT devices and IEC 61,850-compliant power devices. This research presents a novel approach to address the challenges of managing flexibility strategies in residential areas, providing a practical solution for prosumers to actively participate in optimizing energy consumption and enhancing the stability and quality of the electricity system amidst the growing integration of distributed renewable energy.","sentences":["The growth of distributed renewable energy in the electrical grid presents challenges to its stability and quality.","To address this at the local level, flexibility energy strategies emerge as an innovative technique.","However, managing these strategies in residential areas becomes complex due to the unique characteristics of each prosumer.","A major challenge lies in managing communication among diverse devices with different protocols.","To address these issues, a comprehensive framework is designed and implemented to facilitate prosumers' integration in flexibility strategies, addressing communication at various levels.","The effectiveness of the proposed framework is demonstrated through its implementation in a real smart home environment with diverse devices.","The framework enables seamless integration and communication between IoT devices and IEC 61,850-compliant power devices.","This research presents a novel approach to address the challenges of managing flexibility strategies in residential areas, providing a practical solution for prosumers to actively participate in optimizing energy consumption and enhancing the stability and quality of the electricity system amidst the growing integration of distributed renewable energy."],"url":"http://arxiv.org/abs/2402.03865v1","category":"eess.SY"}
{"created":"2024-02-06 10:21:52","title":"A Bernoulli-barycentric rational matrix collocation method with preconditioning for a class of evolutionary PDEs","abstract":"We propose a Bernoulli-barycentric rational matrix collocation method for two-dimensional evolutionary partial differential equations (PDEs) with variable coefficients that combines Bernoulli polynomials with barycentric rational interpolations in time and space, respectively. The theoretical accuracy $O\\left((2\\pi)^{-N}+h_x^{d_x-1}+h_y^{d_y-1}\\right)$ of our numerical scheme is proven, where $N$ is the number of basis functions in time, $h_x$ and $h_y$ are the grid sizes in the $x$, $y$-directions, respectively, and $0\\leq d_x\\leq \\frac{b-a}{h_x},~0\\leq d_y\\leq\\frac{d-c}{h_y}$. For the efficient solution of the relevant linear system arising from the discretizations, we introduce a class of dimension expanded preconditioners that take the advantage of structural properties of the coefficient matrices, and we present a theoretical analysis of eigenvalue distributions of the preconditioned matrices. The effectiveness of our proposed method and preconditioners are studied for solving some real-world examples represented by the heat conduction equation, the advection-diffusion equation, the wave equation and telegraph equations.","sentences":["We propose a Bernoulli-barycentric rational matrix collocation method for two-dimensional evolutionary partial differential equations (PDEs) with variable coefficients that combines Bernoulli polynomials with barycentric rational interpolations in time and space, respectively.","The theoretical accuracy $O\\left((2\\pi)^{-N}+h_x^{d_x-1}+h_y^{d_y-1}\\right)$ of our numerical scheme is proven, where $N$ is the number of basis functions in time, $h_x$ and $h_y$ are the grid sizes in the $x$, $y$-directions, respectively, and $0\\leq d_x\\leq \\frac{b-a}{h_x},~0\\leq d_y\\leq\\frac{d-c}{h_y}$.","For the efficient solution of the relevant linear system arising from the discretizations, we introduce a class of dimension expanded preconditioners that take the advantage of structural properties of the coefficient matrices, and we present a theoretical analysis of eigenvalue distributions of the preconditioned matrices.","The effectiveness of our proposed method and preconditioners are studied for solving some real-world examples represented by the heat conduction equation, the advection-diffusion equation, the wave equation and telegraph equations."],"url":"http://arxiv.org/abs/2402.03861v1","category":"math.NA"}
{"created":"2024-02-06 10:18:30","title":"AED: Adaptable Error Detection for Few-shot Imitation Policy","abstract":"We study how to report few-shot imitation (FSI) policies' behavior errors in novel environments, a novel task named adaptable error detection (AED). The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios. Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations. We develop a cross-domain benchmark for the challenging AED task, consisting of 329 base and 158 novel environments. This task introduces three challenges, including (1) detecting behavior errors in novel environments, (2) behavior errors occurring without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection. To address these challenges, we propose Pattern Observer (PrObe) to parse discernible patterns in the policy feature representations of normal or error states, whose effectiveness is verified in the proposed benchmark. Through our comprehensive evaluation, PrObe consistently surpasses strong baselines and demonstrates a robust capability to identify errors arising from a wide range of FSI policies. Moreover, we conduct comprehensive ablations and experiments (error correction, demonstration quality, etc.) to validate the practicality of our proposed task and methodology.","sentences":["We study how to report few-shot imitation (FSI) policies' behavior errors in novel environments, a novel task named adaptable error detection (AED).","The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios.","Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations.","We develop a cross-domain benchmark for the challenging AED task, consisting of 329 base and 158 novel environments.","This task introduces three challenges, including (1) detecting behavior errors in novel environments, (2) behavior errors occurring without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection.","To address these challenges, we propose Pattern Observer (PrObe) to parse discernible patterns in the policy feature representations of normal or error states, whose effectiveness is verified in the proposed benchmark.","Through our comprehensive evaluation, PrObe consistently surpasses strong baselines and demonstrates a robust capability to identify errors arising from a wide range of FSI policies.","Moreover, we conduct comprehensive ablations and experiments (error correction, demonstration quality, etc.) to validate the practicality of our proposed task and methodology."],"url":"http://arxiv.org/abs/2402.03860v1","category":"cs.RO"}
{"created":"2024-02-06 10:00:46","title":"On the calculation and use of effective single-particle energies. The example of the neutron $1d_{3/2}$-$1d_{5/2}$ splitting along $\\text{N}=20$ isotones","abstract":"The rich phenomenology of quantum many-body systems such as atomic nuclei is complex to interpret. Often, the behaviour (e.g. evolution with the number of constituents) of measurable/observable quantities such as binding or excitation energies can be best understood on the basis of a simplified picture involving auxiliary quantities that are not observable, i.e. whose values vary with parameters that are internal to the theoretical construction (contrarily to measurable/observable quantities). While being useful, the simplified interpretation is thus theoretical-scheme-dependent. This applies, in particular, to the so-called single-nucleon shell structure based on auxiliary effective single-particle energies (ESPEs). In this context, the present work aims at (i) recalling the way to compute ESPEs out of solutions of many-body Schr\\\"odinger's equation, (ii) illustrating the use of ESPEs within the frame of state-of-the-art ab initio calculations to interpret the outcome of a recent nuclear experiment and (iii) demonstrating the impact of several alterations to the computation of ESPEs. While the chosen alterations constitute approximations within the ab initio scheme, they are built-in when employing other theoretical constructs at play in nuclear physics. The present considerations are thus meant to empirically illustrate variations that can be expected between ESPEs computed within different (equally valid) theoretical schemes.","sentences":["The rich phenomenology of quantum many-body systems such as atomic nuclei is complex to interpret.","Often, the behaviour (e.g. evolution with the number of constituents) of measurable/observable quantities such as binding or excitation energies can be best understood on the basis of a simplified picture involving auxiliary quantities that are not observable, i.e. whose values vary with parameters that are internal to the theoretical construction (contrarily to measurable/observable quantities).","While being useful, the simplified interpretation is thus theoretical-scheme-dependent.","This applies, in particular, to the so-called single-nucleon shell structure based on auxiliary effective single-particle energies (ESPEs).","In this context, the present work aims at (i) recalling the way to compute ESPEs out of solutions of many-body Schr\\\"odinger's equation, (ii) illustrating the use of ESPEs within the frame of state-of-the-art ab initio calculations to interpret the outcome of a recent nuclear experiment and (iii) demonstrating the impact of several alterations to the computation of ESPEs.","While the chosen alterations constitute approximations within the ab initio scheme, they are built-in when employing other theoretical constructs at play in nuclear physics.","The present considerations are thus meant to empirically illustrate variations that can be expected between ESPEs computed within different (equally valid) theoretical schemes."],"url":"http://arxiv.org/abs/2402.03854v1","category":"nucl-th"}
{"created":"2024-02-06 09:52:09","title":"Controlling Inter-Particle Distances in Crowds of Motile, Cognitive, Active Particles","abstract":"Distance control in many-particle systems is a fundamental problem in nature. This becomes particularly relevant in systems of active agents, which can sense their environment and react by adjusting their direction of motion. We employ agent-based simulations to investigate the complex interplay between agent activity, characterized by P{\\'e}clet number $Pe$, reorientation maneuverability $\\Omega$, vision angle $\\theta$ and vision range $R_0$, and agent density, which determines agent distancing and dynamics. We focus on semi-dense crowds, where the vision range is much larger than the particle size. The minimal distance to the nearest neighbors, exposure time, and persistence of orientation direction are analyzed to characterize the behavior. With increasing particle speed at fixed maneuverability, particles approach each other more closely, and exhibit shorter exposure times. The temporal persistence of motion decreases with increasing $Pe$, reflecting the impact of activity and maneuverability on direction changes. For a vision angle $\\theta=\\pi/4$, we observe the emergence of flocking aggregates with a band-like structure, reminiscent of the Viscek model. Additionally, for vision angles $\\theta\\ge \\pi/2$, several quantities are found to display a universal scaling behavior with scaling variable $Pe^{3/2}/\\Omega$. Our results are in good agreement with recent experiments of pedestrians in confined spaces.","sentences":["Distance control in many-particle systems is a fundamental problem in nature.","This becomes particularly relevant in systems of active agents, which can sense their environment and react by adjusting their direction of motion.","We employ agent-based simulations to investigate the complex interplay between agent activity, characterized by P{\\'e}clet number $Pe$, reorientation maneuverability $\\Omega$, vision angle $\\theta$ and vision range $R_0$, and agent density, which determines agent distancing and dynamics.","We focus on semi-dense crowds, where the vision range is much larger than the particle size.","The minimal distance to the nearest neighbors, exposure time, and persistence of orientation direction are analyzed to characterize the behavior.","With increasing particle speed at fixed maneuverability, particles approach each other more closely, and exhibit shorter exposure times.","The temporal persistence of motion decreases with increasing $Pe$, reflecting the impact of activity and maneuverability on direction changes.","For a vision angle $\\theta=\\pi/4$, we observe the emergence of flocking aggregates with a band-like structure, reminiscent of the Viscek model.","Additionally, for vision angles $\\theta\\ge \\pi/2$, several quantities are found to display a universal scaling behavior with scaling variable $Pe^{3/2}/\\Omega$.","Our results are in good agreement with recent experiments of pedestrians in confined spaces."],"url":"http://arxiv.org/abs/2402.03851v1","category":"physics.bio-ph"}
{"created":"2024-02-06 09:38:26","title":"Estimation of the lifetime distribution from fluctuations in Bellman-Harris processes","abstract":"The growth of a population is often modeled as branching process where each individual at the end of its life is replaced by a certain number of offspring. An example of these branching models is the Bellman-Harris process, where the lifetime of individuals is assumed to be independent and identically distributed. Here, we are interested in the estimation of the parameters of the Bellman-Harris model, motivated by the estimation of cell division time. Lifetimes are distributed according a Gamma distribution and we follow a population that starts from a small number of individuals by performing time-resolved measurements of the population size. The exponential growth of the population size at the beginning offers an easy estimation of the mean of the lifetime. Going farther and describing lifetime variability is a challenging task however, due to the complexity of the fluctuations of non-Markovian branching processes. Using fine and recent results on these fluctuations, we describe two time-asymptotic regimes and explain how to estimate the parameters. Then, we both consider simulations and biological data to validate and discuss our method. The results described here provide a method to determine single-cell parameters from time-resolved measurements of populations without the need to track each individual or to know the details of the initial condition.","sentences":["The growth of a population is often modeled as branching process where each individual at the end of its life is replaced by a certain number of offspring.","An example of these branching models is the Bellman-Harris process, where the lifetime of individuals is assumed to be independent and identically distributed.","Here, we are interested in the estimation of the parameters of the Bellman-Harris model, motivated by the estimation of cell division time.","Lifetimes are distributed according a Gamma distribution and we follow a population that starts from a small number of individuals by performing time-resolved measurements of the population size.","The exponential growth of the population size at the beginning offers an easy estimation of the mean of the lifetime.","Going farther and describing lifetime variability is a challenging task however, due to the complexity of the fluctuations of non-Markovian branching processes.","Using fine and recent results on these fluctuations, we describe two time-asymptotic regimes and explain how to estimate the parameters.","Then, we both consider simulations and biological data to validate and discuss our method.","The results described here provide a method to determine single-cell parameters from time-resolved measurements of populations without the need to track each individual or to know the details of the initial condition."],"url":"http://arxiv.org/abs/2402.03842v1","category":"math.PR"}
{"created":"2024-02-06 09:37:06","title":"Random features models: a way to study the success of naive imputation","abstract":"Constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data. Yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data. However, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (MCAR). This paper completes the picture for linear predictors by confirming the intuition that the bias is negligible and that surprisingly naive imputation also remains relevant in very low dimension.To this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies.Building on these theoretical results, we establish finite-sample bounds on stochastic gradient (SGD) predictors applied to zero-imputed data, a strategy particularly well suited for large-scale learning.If the MCAR assumption appears to be strong, we show that similar favorable behaviors occur for more complex missing data scenarios.","sentences":["Constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data.","Yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data.","However, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (MCAR).","This paper completes the picture for linear predictors by confirming the intuition that the bias is negligible and that surprisingly naive imputation also remains relevant in very low dimension.","To this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies.","Building on these theoretical results, we establish finite-sample bounds on stochastic gradient (SGD) predictors applied to zero-imputed data, a strategy particularly well suited for large-scale learning.","If the MCAR assumption appears to be strong, we show that similar favorable behaviors occur for more complex missing data scenarios."],"url":"http://arxiv.org/abs/2402.03839v1","category":"math.ST"}
{"created":"2024-02-06 09:35:40","title":"Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels","abstract":"Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver. This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes. In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on graph classification for molecular datasets, where the input graphs have a few tens of nodes. The efficiency of the SWWL kernel is then illustrated on graph regression in computational fluid dynamics and solid mechanics, where the input graphs are made up of tens of thousands of nodes.","sentences":["Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties.","Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver.","This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes.","In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel.","In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which makes it possible to process datasets that were previously impossible to handle.","The new kernel is first validated on graph classification for molecular datasets, where the input graphs have a few tens of nodes.","The efficiency of the SWWL kernel is then illustrated on graph regression in computational fluid dynamics and solid mechanics, where the input graphs are made up of tens of thousands of nodes."],"url":"http://arxiv.org/abs/2402.03838v1","category":"stat.ML"}
{"created":"2024-02-06 09:23:26","title":"Rethinking Skill Extraction in the Job Market Domain using Large Language Models","abstract":"Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.","sentences":["Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes.","The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags.","However, the reliance on manually annotated data limits the generalizability of such approaches.","Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions.","In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets.","Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences.","We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks."],"url":"http://arxiv.org/abs/2402.03832v1","category":"cs.CL"}
{"created":"2024-02-06 09:21:46","title":"Soliton Management for ultrashort pulse: dark and anti-dark solitons of Fokas-Lenells equation with a damping like perturbation and a gauge equivalent spin system","abstract":"We investigate the propagation of an ultrashort optical pulse using Fokas-Lenells equation (FLE) under varying dispersion, nonlinear effects and perturbation. Such a system can be said to be under soliton management (SM) scheme. At first, under a gauge transformation, followed by shifting of variables, we transform FLE under SM into a simplified form, which is similar to an equation given by Davydova and Lashkin for plasma waves, we refer to this form as DLFLE. Then, we propose a bilinearization for DLFLE in a non-vanishing background by introducing an auxiliary function which transforms DLFLE into three bilinear equations. We solve these equations and obtain dark and anti-dark one-soliton solution (1SS) of DLFLE. From here, by reverse transformation of the solution, we obtain the 1SS of FLE and explore the soliton behavior under different SM schemes. Thereafter, we obtain dark and anti-dark two-soliton solution (2SS) of DLFLE and determine the shift in phase of the individual solitons on interaction through asymptotic analysis. We then, obtain the 2SS of FLE and represent the soliton graph for different SM scheme. Thereafter, we present the procedure to determine N-soliton solution (NSS) of DLFLE and FLE. Later, we introduce a Lax pair for DLFLE and through a gauge transformation we convert the spectral problem of our system into that of an equivalent spin system which is termed as Landau-Lifshitz (LL) system. LL equation (LLE) holds the potential to provide information about various nonlinear structures and properties of the system.","sentences":["We investigate the propagation of an ultrashort optical pulse using Fokas-Lenells equation (FLE) under varying dispersion, nonlinear effects and perturbation.","Such a system can be said to be under soliton management (SM) scheme.","At first, under a gauge transformation, followed by shifting of variables, we transform FLE under SM into a simplified form, which is similar to an equation given by Davydova and Lashkin for plasma waves, we refer to this form as DLFLE.","Then, we propose a bilinearization for DLFLE in a non-vanishing background by introducing an auxiliary function which transforms DLFLE into three bilinear equations.","We solve these equations and obtain dark and anti-dark one-soliton solution (1SS) of DLFLE.","From here, by reverse transformation of the solution, we obtain the 1SS of FLE and explore the soliton behavior under different SM schemes.","Thereafter, we obtain dark and anti-dark two-soliton solution (2SS) of DLFLE and determine the shift in phase of the individual solitons on interaction through asymptotic analysis.","We then, obtain the 2SS of FLE and represent the soliton graph for different SM scheme.","Thereafter, we present the procedure to determine N-soliton solution (NSS) of DLFLE and FLE.","Later, we introduce a Lax pair for DLFLE and through a gauge transformation we convert the spectral problem of our system into that of an equivalent spin system which is termed as Landau-Lifshitz (LL) system.","LL equation (LLE) holds the potential to provide information about various nonlinear structures and properties of the system."],"url":"http://arxiv.org/abs/2402.03831v1","category":"nlin.SI"}
{"created":"2024-02-06 09:10:44","title":"Learning immune receptor representations with protein language models","abstract":"Protein language models (PLMs) learn contextual representations from protein sequences and are profoundly impacting various scientific disciplines spanning protein design, drug discovery, and structural predictions. One particular research area where PLMs have gained considerable attention is adaptive immune receptors, whose tremendous sequence diversity dictates the functional recognition of the adaptive immune system. The self-supervised nature underlying the training of PLMs has been recently leveraged to implement a variety of immune receptor-specific PLMs. These models have demonstrated promise in tasks such as predicting antigen-specificity and structure, computationally engineering therapeutic antibodies, and diagnostics. However, challenges including insufficient training data and considerations related to model architecture, training strategies, and data and model availability must be addressed before fully unlocking the potential of PLMs in understanding, translating, and engineering immune receptors.","sentences":["Protein language models (PLMs) learn contextual representations from protein sequences and are profoundly impacting various scientific disciplines spanning protein design, drug discovery, and structural predictions.","One particular research area where PLMs have gained considerable attention is adaptive immune receptors, whose tremendous sequence diversity dictates the functional recognition of the adaptive immune system.","The self-supervised nature underlying the training of PLMs has been recently leveraged to implement a variety of immune receptor-specific PLMs.","These models have demonstrated promise in tasks such as predicting antigen-specificity and structure, computationally engineering therapeutic antibodies, and diagnostics.","However, challenges including insufficient training data and considerations related to model architecture, training strategies, and data and model availability must be addressed before fully unlocking the potential of PLMs in understanding, translating, and engineering immune receptors."],"url":"http://arxiv.org/abs/2402.03823v1","category":"q-bio.QM"}
{"created":"2024-02-06 09:07:46","title":"PMSM transient response optimization by end-to-end optimal control","abstract":"Speed responses of motors, especially Permanent Magnet Synchronous Motors (PMSMs), are increasing in importance for recent applications, such as electric vehicles or quadrotors. These applications require quick acceleration performance. However, commercial controllers are based mainly on Proportional-Integral (PI) controllers, which are suitable for eliminating steady-state errors but unsuitable for transient response optimization. In this paper, we replaced whole conventional controllers with an end-to-end Recurrent Neural Network (RNN) that has a regularized transition matrix. Our end-to-end controller directly minimizes the transient response time on the basis of optimal control theory. Computer-simulated results show that speed response indices improved using the RNN rather than a PI controller, while both were under comparable power losses. The current vector trajectories of the RNN showed that the RNN could automatically determine arbitrary trajectories in the flux-weakening region in accordance with an arbitrarily designed loss function. In contrast, the traditional flux-weakening methods using PI controllers have pre-determined current vector trajectories.","sentences":["Speed responses of motors, especially Permanent Magnet Synchronous Motors (PMSMs), are increasing in importance for recent applications, such as electric vehicles or quadrotors.","These applications require quick acceleration performance.","However, commercial controllers are based mainly on Proportional-Integral (PI) controllers, which are suitable for eliminating steady-state errors but unsuitable for transient response optimization.","In this paper, we replaced whole conventional controllers with an end-to-end Recurrent Neural Network (RNN) that has a regularized transition matrix.","Our end-to-end controller directly minimizes the transient response time on the basis of optimal control theory.","Computer-simulated results show that speed response indices improved using the RNN rather than a PI controller, while both were under comparable power losses.","The current vector trajectories of the RNN showed that the RNN could automatically determine arbitrary trajectories in the flux-weakening region in accordance with an arbitrarily designed loss function.","In contrast, the traditional flux-weakening methods using PI controllers have pre-determined current vector trajectories."],"url":"http://arxiv.org/abs/2402.03820v1","category":"eess.SY"}
{"created":"2024-02-06 09:05:26","title":"Improvement of Frequency Source Phase Noise Reduction Design under Vibration Condition","abstract":"Reasonable vibration reduction design is an important way to achieve low phase noise index of airborne frequency source output signal. Aiming at the problem of phase noise deterioration of an airborne frequency source under random condition, this paper proposes to improve the vibration reduction mode crystal oscillator and reduce the distance between the barycenter of frequency source and crystal oscillator vibration based on the analysis of the relationship between the frequency source and the phase noise of output signal. Experimental results show that the active noise control system achieves 62dB phase noise compensation under the random vibration of 0.04-0.1g*g/Hz amplitude range and 5-2000 Hz frequency range.","sentences":["Reasonable vibration reduction design is an important way to achieve low phase noise index of airborne frequency source output signal.","Aiming at the problem of phase noise deterioration of an airborne frequency source under random condition, this paper proposes to improve the vibration reduction mode crystal oscillator and reduce the distance between the barycenter of frequency source and crystal oscillator vibration based on the analysis of the relationship between the frequency source and the phase noise of output signal.","Experimental results show that the active noise control system achieves 62dB phase noise compensation under the random vibration of 0.04-0.1g*g/Hz amplitude range and 5-2000 Hz frequency range."],"url":"http://arxiv.org/abs/2402.03817v1","category":"eess.SY"}
{"created":"2024-02-06 08:57:49","title":"NK Hybrid Genetic Algorithm for Clustering","abstract":"The NK hybrid genetic algorithm for clustering is proposed in this paper. In order to evaluate the solutions, the hybrid algorithm uses the NK clustering validation criterion 2 (NKCV2). NKCV2 uses information about the disposition of $N$ small groups of objects. Each group is composed of $K+1$ objects of the dataset. Experimental results show that density-based regions can be identified by using NKCV2 with fixed small $K$. In NKCV2, the relationship between decision variables is known, which in turn allows us to apply gray box optimization. Mutation operators, a partition crossover, and a local search strategy are proposed, all using information about the relationship between decision variables. In partition crossover, the evaluation function is decomposed into $q$ independent components; partition crossover then deterministically returns the best among $2^q$ possible offspring with computational complexity $O(N)$. The NK hybrid genetic algorithm allows the detection of clusters with arbitrary shapes and the automatic estimation of the number of clusters. In the experiments, the NK hybrid genetic algorithm produced very good results when compared to another genetic algorithm approach and to state-of-art clustering algorithms.","sentences":["The NK hybrid genetic algorithm for clustering is proposed in this paper.","In order to evaluate the solutions, the hybrid algorithm uses the NK clustering validation criterion 2 (NKCV2).","NKCV2 uses information about the disposition of $N$ small groups of objects.","Each group is composed of $K+1$ objects of the dataset.","Experimental results show that density-based regions can be identified by using NKCV2 with fixed small $K$. In NKCV2, the relationship between decision variables is known, which in turn allows us to apply gray box optimization.","Mutation operators, a partition crossover, and a local search strategy are proposed, all using information about the relationship between decision variables.","In partition crossover, the evaluation function is decomposed into $q$ independent components; partition crossover then deterministically returns the best among $2^q$ possible offspring with computational complexity $O(N)$. The NK hybrid genetic algorithm allows the detection of clusters with arbitrary shapes and the automatic estimation of the number of clusters.","In the experiments, the NK hybrid genetic algorithm produced very good results when compared to another genetic algorithm approach and to state-of-art clustering algorithms."],"url":"http://arxiv.org/abs/2402.03813v1","category":"cs.NE"}
{"created":"2024-02-06 08:44:16","title":"Robot voice a voice controlled robot using arduino","abstract":"Robotic assistants reduce the manual efforts being put in by humans in their day-to-day tasks. In this paper, we develop a voice-controlled personal assistant robot. The robot takes the human voice commands by its own built-in microphone. This robot not only takes the commands and executes them but also acknowledges them through speech output. This robot can perform different movements, turns, wakeup/shutdown operations, relocate an object from one place to another, and can also develop a conversation with humans. The voice commands are processed in real time using an offline server. The speech signal commands are directly communicated to the server using a USB cable. The personal assistant robot is developed on a microcontroller-based platform. Performance evaluation is carried out with encouraging results of the initial experiments. Possible improvements for applications in homes, hospitals, car systems, and industries are also discussed.","sentences":["Robotic assistants reduce the manual efforts being put in by humans in their day-to-day tasks.","In this paper, we develop a voice-controlled personal assistant robot.","The robot takes the human voice commands by its own built-in microphone.","This robot not only takes the commands and executes them but also acknowledges them through speech output.","This robot can perform different movements, turns, wakeup/shutdown operations, relocate an object from one place to another, and can also develop a conversation with humans.","The voice commands are processed in real time using an offline server.","The speech signal commands are directly communicated to the server using a USB cable.","The personal assistant robot is developed on a microcontroller-based platform.","Performance evaluation is carried out with encouraging results of the initial experiments.","Possible improvements for applications in homes, hospitals, car systems, and industries are also discussed."],"url":"http://arxiv.org/abs/2402.03803v1","category":"cs.RO"}
{"created":"2024-02-06 08:39:44","title":"On Practical Diversified Recommendation with Controllable Category Diversity Framework","abstract":"Recommender systems have made significant strides in various industries, primarily driven by extensive efforts to enhance recommendation accuracy. However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects. Especially in industry, it could impair user's experiences and prevent user from accessing a wider range of items. One of the solutions is to take diversity into account. However, most of existing works focus on user's explicit preferences, while rarely exploring user's non-interaction preferences. These neglected non-interaction preferences are especially important for broadening user's interests in alleviating echo chamber/filter bubble effects.Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors. Then, we propose a succinct and effective method, named as Controllable Category Diversity Framework (CCDF) to achieve both high U-diversity and N-diversity simultaneously.Specifically, CCDF consists of two stages, User-Category Matching and Constrained Item Matching. The User-Category Matching utilizes the DeepU2C model and a combined loss to capture user's preferences in categories, and then selects the top-$K$ categories with a controllable parameter $K$.These top-$K$ categories will be used as trigger information in Constrained Item Matching. Offline experimental results show that our proposed DeepU2C outperforms state-of-the-art diversity-oriented methods, especially on N-diversity task. The whole framework is validated in a real-world production environment by conducting online A/B testing.","sentences":["Recommender systems have made significant strides in various industries, primarily driven by extensive efforts to enhance recommendation accuracy.","However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects.","Especially in industry, it could impair user's experiences and prevent user from accessing a wider range of items.","One of the solutions is to take diversity into account.","However, most of existing works focus on user's explicit preferences, while rarely exploring user's non-interaction preferences.","These neglected non-interaction preferences are especially important for broadening user's interests in alleviating echo chamber/filter bubble effects.","Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors.","Then, we propose a succinct and effective method, named as Controllable Category Diversity Framework (CCDF) to achieve both high U-diversity and N-diversity simultaneously.","Specifically, CCDF consists of two stages, User-Category Matching and Constrained Item Matching.","The User-Category Matching utilizes the DeepU2C model and a combined loss to capture user's preferences in categories, and then selects the top-$K$ categories with a controllable parameter $K$.These top-$K$ categories will be used as trigger information in Constrained Item Matching.","Offline experimental results show that our proposed DeepU2C outperforms state-of-the-art diversity-oriented methods, especially on N-diversity task.","The whole framework is validated in a real-world production environment by conducting online A/B testing."],"url":"http://arxiv.org/abs/2402.03801v1","category":"cs.IR"}
{"created":"2024-02-06 08:35:19","title":"Partial-dual polynomial as a framed weight system","abstract":"Recently, Chmutov proved that the partial-dual polynomial considered as a function on chord diagrams satisfies the four-term relations. In this paper, we show that this function on framed chord diagrams also satisfies the four-term relations, i.e., is a framed weight system.","sentences":["Recently, Chmutov proved that the partial-dual polynomial considered as a function on chord diagrams satisfies the four-term relations.","In this paper, we show that this function on framed chord diagrams also satisfies the four-term relations, i.e., is a framed weight system."],"url":"http://arxiv.org/abs/2402.03799v1","category":"math.CO"}
{"created":"2024-02-06 08:33:55","title":"The gravitational Vlasov-Poisson system with infinite mass and velocities in $\\mathbb{R}^3$","abstract":"We study existence and uniqueness of the solution to the gravitational Vlasov-Poisson system evolving in $\\mathbb{R}^3$.   It is assumed that initially the particles are distributed according to a spatial density with a power-law decay in space, allowing for unbounded mass, and an exponential decay in velocities given by a Maxwell-Boltzmann law. We extend a classical result which holds for systems with finite total mass.","sentences":["We study existence and uniqueness of the solution to the gravitational Vlasov-Poisson system evolving in $\\mathbb{R}^3$.   ","It is assumed that initially the particles are distributed according to a spatial density with a power-law decay in space, allowing for unbounded mass, and an exponential decay in velocities given by a Maxwell-Boltzmann law.","We extend a classical result which holds for systems with finite total mass."],"url":"http://arxiv.org/abs/2402.03798v1","category":"math.AP"}
{"created":"2024-02-06 08:12:01","title":"Strong approximation of the time-fractional Cahn--Hilliard equation driven by a fractionally integrated additive noise","abstract":"In this paper, we consider the numerical approximation of a time-fractional stochastic Cahn--Hilliard equation driven by an additive fractionally integrated Gaussian noise. The model involves a Caputo fractional derivative in time of order $\\alpha\\in(0,1)$ and a fractional time-integral noise of order $\\gamma\\in[0,1]$. The numerical scheme approximates the model by a piecewise linear finite element method in space and a convolution quadrature in time (for both time-fractional operators), along with the $L^2$-projection for the noise. We carefully investigate the spatially semidiscrete and fully discrete schemes, and obtain strong convergence rates by using clever energy arguments. The temporal H\\\"older continuity property of the solution played a key role in the error analysis. Unlike the stochastic Allen--Cahn equation, the presence of the unbounded elliptic operator in front of the cubic nonlinearity in the underlying model adds complexity and challenges to the error analysis. To overcome these difficulties, several new techniques and error estimates are developed. The study concludes with numerical examples that validate the theoretical findings.","sentences":["In this paper, we consider the numerical approximation of a time-fractional stochastic Cahn--Hilliard equation driven by an additive fractionally integrated Gaussian noise.","The model involves a Caputo fractional derivative in time of order $\\alpha\\in(0,1)$ and a fractional time-integral noise of order $\\gamma\\in[0,1]$. The numerical scheme approximates the model by a piecewise linear finite element method in space and a convolution quadrature in time (for both time-fractional operators), along with the $L^2$-projection for the noise.","We carefully investigate the spatially semidiscrete and fully discrete schemes, and obtain strong convergence rates by using clever energy arguments.","The temporal H\\\"older continuity property of the solution played a key role in the error analysis.","Unlike the stochastic Allen--Cahn equation, the presence of the unbounded elliptic operator in front of the cubic nonlinearity in the underlying model adds complexity and challenges to the error analysis.","To overcome these difficulties, several new techniques and error estimates are developed.","The study concludes with numerical examples that validate the theoretical findings."],"url":"http://arxiv.org/abs/2402.03790v1","category":"math.NA"}
{"created":"2024-02-06 08:00:27","title":"High sampling rate single-pixel digital holography system employing a DMD and phase-encoded patterns","abstract":"A single-pixel digital holography system with phase-encoded illumination using a digital micromirror device (DMD) as a spatial light modulator (SLM) is presented. The enhanced switching rate of DMDs, far exceeding the stringent frame-rate of liquid crystal SLMs, allows recording and reconstruction of complex amplitude distributions in just a few seconds. A single amplitude binary modulation device is used for concurrently displaying the phase-encoded sampling patterns, compensating the distortion of the wavefront, and applying phase-shifting, by means of computer generated holograms. Our detection system consists of a simple photodiode that sequentially records the irradiance fluctuations corresponding to the interference between object and reference beams. The system recovers phase and amplitude information even when a diffuser is placed in front of the photodiode.","sentences":["A single-pixel digital holography system with phase-encoded illumination using a digital micromirror device (DMD) as a spatial light modulator (SLM) is presented.","The enhanced switching rate of DMDs, far exceeding the stringent frame-rate of liquid crystal SLMs, allows recording and reconstruction of complex amplitude distributions in just a few seconds.","A single amplitude binary modulation device is used for concurrently displaying the phase-encoded sampling patterns, compensating the distortion of the wavefront, and applying phase-shifting, by means of computer generated holograms.","Our detection system consists of a simple photodiode that sequentially records the irradiance fluctuations corresponding to the interference between object and reference beams.","The system recovers phase and amplitude information even when a diffuser is placed in front of the photodiode."],"url":"http://arxiv.org/abs/2402.03786v1","category":"physics.optics"}
{"created":"2024-02-06 07:50:27","title":"EERO: Early Exit with Reject Option for Efficient Classification with limited budget","abstract":"The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively. One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances. In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance. We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption. Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenarios.","sentences":["The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively.","One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances.","In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance.","We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We","consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption.","Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenarios."],"url":"http://arxiv.org/abs/2402.03779v1","category":"stat.ML"}
{"created":"2024-02-06 07:22:50","title":"AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection","abstract":"Biometric security is the cornerstone of modern identity verification and authentication systems, where the integrity and reliability of biometric samples is of paramount importance. This paper introduces AttackNet, a bespoke Convolutional Neural Network architecture, meticulously designed to combat spoofing threats in biometric systems. Rooted in deep learning methodologies, this model offers a layered defense mechanism, seamlessly transitioning from low-level feature extraction to high-level pattern discernment. Three distinctive architectural phases form the crux of the model, each underpinned by judiciously chosen activation functions, normalization techniques, and dropout layers to ensure robustness and resilience against adversarial attacks. Benchmarking our model across diverse datasets affirms its prowess, showcasing superior performance metrics in comparison to contemporary models. Furthermore, a detailed comparative analysis accentuates the model's efficacy, drawing parallels with prevailing state-of-the-art methodologies. Through iterative refinement and an informed architectural strategy, AttackNet underscores the potential of deep learning in safeguarding the future of biometric security.","sentences":["Biometric security is the cornerstone of modern identity verification and authentication systems, where the integrity and reliability of biometric samples is of paramount importance.","This paper introduces AttackNet, a bespoke Convolutional Neural Network architecture, meticulously designed to combat spoofing threats in biometric systems.","Rooted in deep learning methodologies, this model offers a layered defense mechanism, seamlessly transitioning from low-level feature extraction to high-level pattern discernment.","Three distinctive architectural phases form the crux of the model, each underpinned by judiciously chosen activation functions, normalization techniques, and dropout layers to ensure robustness and resilience against adversarial attacks.","Benchmarking our model across diverse datasets affirms its prowess, showcasing superior performance metrics in comparison to contemporary models.","Furthermore, a detailed comparative analysis accentuates the model's efficacy, drawing parallels with prevailing state-of-the-art methodologies.","Through iterative refinement and an informed architectural strategy, AttackNet underscores the potential of deep learning in safeguarding the future of biometric security."],"url":"http://arxiv.org/abs/2402.03769v1","category":"cs.CV"}
{"created":"2024-02-06 07:21:03","title":"DC current generation by dielectric loss in ferroelectrics","abstract":"We study dc current generation induced by microwave irradiation to ferroelectric materials. The dc current generation originates from microwave absorption called dielectric loss due to the delay of dielectric response. Such current generation can be formulated as the low-frequency limit of the phonon shift current which arises from an increase of electric polarization accompanying photoexcitation of phonons due to the electron-phonon coupling. To study the dc current generation by the dielectric loss, we apply the diagrammatic treatment of nonlinear optical responses to photoexcitations of phonons and derive the general formula for phonon shift current. We then study the dc current generation in the low-frequency region and find that the dc current scales as $\\propto w^2$ for the linearly polarized light and time reversal symmetric systems. We estimate the order of magnitude of the dc current generation by dielectric loss, indicating its feasibility for experimental detection in the GHz region.","sentences":["We study dc current generation induced by microwave irradiation to ferroelectric materials.","The dc current generation originates from microwave absorption called dielectric loss due to the delay of dielectric response.","Such current generation can be formulated as the low-frequency limit of the phonon shift current which arises from an increase of electric polarization accompanying photoexcitation of phonons due to the electron-phonon coupling.","To study the dc current generation by the dielectric loss, we apply the diagrammatic treatment of nonlinear optical responses to photoexcitations of phonons and derive the general formula for phonon shift current.","We then study the dc current generation in the low-frequency region and find that the dc current scales as $\\propto w^2$ for the linearly polarized light and time reversal symmetric systems.","We estimate the order of magnitude of the dc current generation by dielectric loss, indicating its feasibility for experimental detection in the GHz region."],"url":"http://arxiv.org/abs/2402.03768v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 07:19:01","title":"Magnetic Field Gated and Current Controlled Spintronic Mem-transistor Neuron -based Spiking Neural Networks","abstract":"Spintronic devices, such as the domain walls and skyrmions, have shown significant potential for applications in energy-efficient data storage and beyond CMOS computing architectures. In recent years, spiking neural networks have shown more bio-plausibility. Based on the magnetic multilayer spintronic devices, we demonstrate the magnetic field-gated Leaky integrate and fire neuron characteristics for the spiking neural network applications. The LIF characteristics are controlled by the current pulses, which drive the domain wall, and an external magnetic field is used as the bias to tune the firing properties of the neuron. Thus, the device works like a gate-controlled LIF neuron, acting like a spintronic Mem-Transistor device. We develop a LIF neuron model based on the measured characteristics to show the device integration in the system-level SNNs. We extend the study and propose a scaled version of the demonstrated device with a multilayer spintronic domain wall magnetic tunnel junction as a LIF neuron. using the combination of SOT and the variation of the demagnetization energy across the thin film, the modified leaky integrate and fire LIF neuron characteristics are realized in the proposed devices. The neuron device characteristics are modeled as the modified LIF neuron model. Finally, we integrate the measured and simulated neuron models in the 3-layer spiking neural network and convolutional spiking neural network CSNN framework to test these spiking neuron models for classification of the MNIST and FMNIST datasets. In both architectures, the network achieves classification accuracy above 96%. Considering the good system-level performance, mem-transistor properties, and promise for scalability. The presented devices show an excellent properties for neuromorphic computing applications.","sentences":["Spintronic devices, such as the domain walls and skyrmions, have shown significant potential for applications in energy-efficient data storage and beyond CMOS computing architectures.","In recent years, spiking neural networks have shown more bio-plausibility.","Based on the magnetic multilayer spintronic devices, we demonstrate the magnetic field-gated Leaky integrate and fire neuron characteristics for the spiking neural network applications.","The LIF characteristics are controlled by the current pulses, which drive the domain wall, and an external magnetic field is used as the bias to tune the firing properties of the neuron.","Thus, the device works like a gate-controlled LIF neuron, acting like a spintronic Mem-Transistor device.","We develop a LIF neuron model based on the measured characteristics to show the device integration in the system-level SNNs.","We extend the study and propose a scaled version of the demonstrated device with a multilayer spintronic domain wall magnetic tunnel junction as a LIF neuron.","using the combination of SOT and the variation of the demagnetization energy across the thin film, the modified leaky integrate and fire LIF neuron characteristics are realized in the proposed devices.","The neuron device characteristics are modeled as the modified LIF neuron model.","Finally, we integrate the measured and simulated neuron models in the 3-layer spiking neural network and convolutional spiking neural network CSNN framework to test these spiking neuron models for classification of the MNIST and FMNIST datasets.","In both architectures, the network achieves classification accuracy above 96%.","Considering the good system-level performance, mem-transistor properties, and promise for scalability.","The presented devices show an excellent properties for neuromorphic computing applications."],"url":"http://arxiv.org/abs/2402.03767v1","category":"physics.app-ph"}
{"created":"2024-02-06 07:13:18","title":"Unveiling the charge density wave mechanism in vanadium-based Bi-layered kagome metals","abstract":"The charge density wave (CDW), as a hallmark of vanadium-based kagome superconductor AV3Sb5 (A = K, Rb, Cs), has attracted intensive attention. However, the fundamental controversy regarding the underlying mechanism of CDW therein persists. Recently, the vanadium-based bi-layered kagome metal ScV6Sn6, reported to exhibit a long-range charge order below 94 K, has emerged as a promising candidate to further clarify this core issue. Here, employing micro-focusing angle-resolved photoemission spectroscopy ({\\mu}-ARPES) and first-principles calculations, we systematically studied the unique CDW order in vanadium-based bi-layered kagome metals by comparing ScV6Sn6 with its isostructural counterpart YV6Sn6, which lacks a CDW ground state. Combining ARPES data and the corresponding joint density of states (DOS), we suggest that the VHS nesting mechanism might be invalid in these materials. Besides, in ScV6Sn6, we identified multiple hybridization energy gaps resulting from CDW-induced band folding, along with an anomalous band dispersion, implying a potential electron-phonon coupling driven mechanism underlying the formation of the CDW order. Our finding not only comprehensively maps the electronic structure of V-based bi-layer kagome metals but also provide constructive experimental evidence for the unique origin of CDW in this system.","sentences":["The charge density wave (CDW), as a hallmark of vanadium-based kagome superconductor AV3Sb5 (A = K, Rb, Cs), has attracted intensive attention.","However, the fundamental controversy regarding the underlying mechanism of CDW therein persists.","Recently, the vanadium-based bi-layered kagome metal ScV6Sn6, reported to exhibit a long-range charge order below 94 K, has emerged as a promising candidate to further clarify this core issue.","Here, employing micro-focusing angle-resolved photoemission spectroscopy ({\\mu}-ARPES) and first-principles calculations, we systematically studied the unique CDW order in vanadium-based bi-layered kagome metals by comparing ScV6Sn6 with its isostructural counterpart YV6Sn6, which lacks a CDW ground state.","Combining ARPES data and the corresponding joint density of states (DOS), we suggest that the VHS nesting mechanism might be invalid in these materials.","Besides, in ScV6Sn6, we identified multiple hybridization energy gaps resulting from CDW-induced band folding, along with an anomalous band dispersion, implying a potential electron-phonon coupling driven mechanism underlying the formation of the CDW order.","Our finding not only comprehensively maps the electronic structure of V-based bi-layer kagome metals but also provide constructive experimental evidence for the unique origin of CDW in this system."],"url":"http://arxiv.org/abs/2402.03765v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 06:56:00","title":"DeMarking: A Defense for Network Flow Watermarking in Real-Time","abstract":"The network flow watermarking technique associates the two communicating parties by actively modifying certain characteristics of the stream generated by the sender so that it covertly carries some special marking information. Some curious users communicating with the hidden server as a Tor client may attempt de-anonymization attacks to uncover the real identity of the hidden server by using this technique. This compromises the privacy of the anonymized communication system. Therefore, we propose a defense scheme against flow watermarking. The scheme is based on deep neural networks and utilizes generative adversarial networks to convert the original Inter-Packet Delays (IPD) into new IPDs generated by the model. We also adopt the concept of adversarial attacks to ensure that the detector will produce an incorrect classification when detecting these new IPDs. This approach ensures that these IPDs are considered \"clean\", effectively covering the potential watermarks. This scheme is effective against time-based flow watermarking techniques.","sentences":["The network flow watermarking technique associates the two communicating parties by actively modifying certain characteristics of the stream generated by the sender so that it covertly carries some special marking information.","Some curious users communicating with the hidden server as a Tor client may attempt de-anonymization attacks to uncover the real identity of the hidden server by using this technique.","This compromises the privacy of the anonymized communication system.","Therefore, we propose a defense scheme against flow watermarking.","The scheme is based on deep neural networks and utilizes generative adversarial networks to convert the original Inter-Packet Delays (IPD) into new IPDs generated by the model.","We also adopt the concept of adversarial attacks to ensure that the detector will produce an incorrect classification when detecting these new IPDs.","This approach ensures that these IPDs are considered \"clean\", effectively covering the potential watermarks.","This scheme is effective against time-based flow watermarking techniques."],"url":"http://arxiv.org/abs/2402.03760v1","category":"cs.NI"}
{"created":"2024-02-06 06:49:04","title":"Virtual Classification: Modulating Domain-Specific Knowledge for Multidomain Crowd Counting","abstract":"Multidomain crowd counting aims to learn a general model for multiple diverse datasets. However, deep networks prefer modeling distributions of the dominant domains instead of all domains, which is known as domain bias. In this study, we propose a simple-yet-effective Modulating Domain-specific Knowledge Network (MDKNet) to handle the domain bias issue in multidomain crowd counting. MDKNet is achieved by employing the idea of `modulating', enabling deep network balancing and modeling different distributions of diverse datasets with little bias. Specifically, we propose an Instance-specific Batch Normalization (IsBN) module, which serves as a base modulator to refine the information flow to be adaptive to domain distributions. To precisely modulating the domain-specific information, the Domain-guided Virtual Classifier (DVC) is then introduced to learn a domain-separable latent space. This space is employed as an input guidance for the IsBN modulator, such that the mixture distributions of multiple datasets can be well treated. Extensive experiments performed on popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the superiority of MDKNet in tackling multidomain crowd counting and the effectiveness for multidomain learning. Code is available at \\url{https://github.com/csguomy/MDKNet}.","sentences":["Multidomain crowd counting aims to learn a general model for multiple diverse datasets.","However, deep networks prefer modeling distributions of the dominant domains instead of all domains, which is known as domain bias.","In this study, we propose a simple-yet-effective Modulating Domain-specific Knowledge Network (MDKNet) to handle the domain bias issue in multidomain crowd counting.","MDKNet is achieved by employing the idea of `modulating', enabling deep network balancing and modeling different distributions of diverse datasets with little bias.","Specifically, we propose an Instance-specific Batch Normalization (IsBN) module, which serves as a base modulator to refine the information flow to be adaptive to domain distributions.","To precisely modulating the domain-specific information, the Domain-guided Virtual Classifier (DVC) is then introduced to learn a domain-separable latent space.","This space is employed as an input guidance for the IsBN modulator, such that the mixture distributions of multiple datasets can be well treated.","Extensive experiments performed on popular benchmarks, including Shanghai-tech A/B, QNRF and NWPU, validate the superiority of MDKNet in tackling multidomain crowd counting and the effectiveness for multidomain learning.","Code is available at \\url{https://github.com/csguomy/MDKNet}."],"url":"http://arxiv.org/abs/2402.03758v1","category":"cs.CV"}
{"created":"2024-02-06 06:47:15","title":"Uniform error bounds of the ensemble transform Kalman filter for infinite-dimensional dynamics with multiplicative covariance inflation","abstract":"Data assimilation is a method of uncertainty quantification to estimate the hidden true state by updating the prediction owing to model dynamics with observation data. As a prediction model, we consider a class of nonlinear dynamical systems on Hilbert spaces including the two-dimensional Navier-Stokes equations and the Lorenz '63 and '96 equations. For nonlinear model dynamics, the ensemble Kalman filter (EnKF) is often used to approximate the mean and covariance of the probability distribution with a set of particles called an ensemble. In this paper, we consider a deterministic version of the EnKF known as the ensemble transform Kalman filter (ETKF), performing well even with limited ensemble sizes in comparison to other stochastic implementations of the EnKF. When the ETKF is applied to large-scale systems, an ad-hoc numerical technique called a covariance inflation is often employed to reduce approximation errors. Despite the practical effectiveness of the ETKF, little is theoretically known. The present study aims to establish the theoretical analysis of the ETKF. We obtain that the estimation error of the ETKF with and without the covariance inflation is bounded for any finite time. In particular, the uniform-in-time error bound is obtained when an inflation parameter is chosen appropriately, justifying the effectiveness of the covariance inflation in the ETKF.","sentences":["Data assimilation is a method of uncertainty quantification to estimate the hidden true state by updating the prediction owing to model dynamics with observation data.","As a prediction model, we consider a class of nonlinear dynamical systems on Hilbert spaces including the two-dimensional Navier-Stokes equations and the Lorenz '63 and '96 equations.","For nonlinear model dynamics, the ensemble Kalman filter (EnKF) is often used to approximate the mean and covariance of the probability distribution with a set of particles called an ensemble.","In this paper, we consider a deterministic version of the EnKF known as the ensemble transform Kalman filter (ETKF), performing well even with limited ensemble sizes in comparison to other stochastic implementations of the EnKF.","When the ETKF is applied to large-scale systems, an ad-hoc numerical technique called a covariance inflation is often employed to reduce approximation errors.","Despite the practical effectiveness of the ETKF, little is theoretically known.","The present study aims to establish the theoretical analysis of the ETKF.","We obtain that the estimation error of the ETKF with and without the covariance inflation is bounded for any finite time.","In particular, the uniform-in-time error bound is obtained when an inflation parameter is chosen appropriately, justifying the effectiveness of the covariance inflation in the ETKF."],"url":"http://arxiv.org/abs/2402.03756v1","category":"math.ST"}
{"created":"2024-02-06 06:27:40","title":"Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback","abstract":"Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules. Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data. We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.","sentences":["Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs).","The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules.","Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data.","We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities.","In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content.","Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model.","We commit to open-sourcing our code, models, and datasets to foster further research in this area."],"url":"http://arxiv.org/abs/2402.03746v1","category":"cs.CV"}
{"created":"2024-02-06 06:23:11","title":"Effects of the strong Breit interaction on the $2s2p$-$1s2s$ transitions of inner shell hole states of Helium-like ions","abstract":"We have calculated the transition energies and probabilities of one-electron one photon and one-electron two photon transitions of middle-Z and high-Z He-like ions using the fully relativistic multiconfiguration Dirac-Hartree-Fock method with active space method. The relativistic, electron correlation, Breit and QED effects are systemically taken into account in the present work. Results showcase consistent agreement with the experimental and theoretical data, uncovering intriguing inversion phenomena in One-Electron One-Photon transitions energy, particularly in double-hole states. Theoretical spectra intensities provide valuable insights into high-energy X-ray radiation processes from double \\textit{K}-hole states.","sentences":["We have calculated the transition energies and probabilities of one-electron one photon and one-electron two photon transitions of middle-Z and high-Z He-like ions using the fully relativistic multiconfiguration Dirac-Hartree-Fock method with active space method.","The relativistic, electron correlation, Breit and QED effects are systemically taken into account in the present work.","Results showcase consistent agreement with the experimental and theoretical data, uncovering intriguing inversion phenomena in One-Electron One-Photon transitions energy, particularly in double-hole states.","Theoretical spectra intensities provide valuable insights into high-energy X-ray radiation processes from double \\textit{K}-hole states."],"url":"http://arxiv.org/abs/2402.03743v1","category":"physics.atom-ph"}
{"created":"2024-02-06 06:13:13","title":"BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning","abstract":"The detection of automated accounts, also known as \"social bots\", has been an increasingly important concern for online social networks (OSNs). While several methods have been proposed for detecting social bots, significant research gaps remain. First, current models exhibit limitations in detecting sophisticated bots that aim to mimic genuine OSN users. Second, these methods often rely on simplistic profile features, which are susceptible to manipulation. In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another.   To address these challenges, we propose a novel framework for social Bot detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework leverages contrastive learning to distinguish between social bots and humans in the embedding space to improve linear separability. The high-level representations derived by BotSSCL enhance its resilience to variations in data distribution and ensure generalizability. We evaluate BotSSCL's robustness against adversarial attempts to manipulate bot accounts to evade detection. Experiments on two datasets featuring sophisticated bots demonstrate that BotSSCL outperforms other supervised, unsupervised, and self-supervised baseline methods. We achieve approx. 6% and approx. 8% higher (F1) performance than SOTA on both datasets. In addition, BotSSCL also achieves 67% F1 when trained on one dataset and tested with another, demonstrating its generalizability. Lastly, BotSSCL increases adversarial complexity and only allows 4% success to the adversary in evading detection.","sentences":["The detection of automated accounts, also known as \"social bots\", has been an increasingly important concern for online social networks (OSNs).","While several methods have been proposed for detecting social bots, significant research gaps remain.","First, current models exhibit limitations in detecting sophisticated bots that aim to mimic genuine OSN users.","Second, these methods often rely on simplistic profile features, which are susceptible to manipulation.","In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another.   ","To address these challenges, we propose a novel framework for social Bot detection with Self-Supervised Contrastive Learning (BotSSCL).","Our framework leverages contrastive learning to distinguish between social bots and humans in the embedding space to improve linear separability.","The high-level representations derived by BotSSCL enhance its resilience to variations in data distribution and ensure generalizability.","We evaluate BotSSCL's robustness against adversarial attempts to manipulate bot accounts to evade detection.","Experiments on two datasets featuring sophisticated bots demonstrate that BotSSCL outperforms other supervised, unsupervised, and self-supervised baseline methods.","We achieve approx.","6% and approx.","8% higher (F1) performance than SOTA on both datasets.","In addition, BotSSCL also achieves 67% F1 when trained on one dataset and tested with another, demonstrating its generalizability.","Lastly, BotSSCL increases adversarial complexity and only allows 4% success to the adversary in evading detection."],"url":"http://arxiv.org/abs/2402.03740v1","category":"cs.SI"}
{"created":"2024-02-06 06:10:46","title":"Differentially Private High Dimensional Bandits","abstract":"We consider a high-dimensional stochastic contextual linear bandit problem when the parameter vector is $s_{0}$-sparse and the decision maker is subject to privacy constraints under both central and local models of differential privacy. We present PrivateLASSO, a differentially private LASSO bandit algorithm. PrivateLASSO is based on two sub-routines: (i) a sparse hard-thresholding-based privacy mechanism and (ii) an episodic thresholding rule for identifying the support of the parameter $\\theta$. We prove minimax private lower bounds and establish privacy and utility guarantees for PrivateLASSO for the central model under standard assumptions.","sentences":["We consider a high-dimensional stochastic contextual linear bandit problem when the parameter vector is $s_{0}$-sparse and the decision maker is subject to privacy constraints under both central and local models of differential privacy.","We present PrivateLASSO, a differentially private LASSO bandit algorithm.","PrivateLASSO is based on two sub-routines: (i) a sparse hard-thresholding-based privacy mechanism and (ii) an episodic thresholding rule for identifying the support of the parameter $\\theta$. We prove minimax private lower bounds and establish privacy and utility guarantees for PrivateLASSO for the central model under standard assumptions."],"url":"http://arxiv.org/abs/2402.03737v1","category":"cs.LG"}
{"created":"2024-02-06 06:03:05","title":"Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study","abstract":"Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users' requirements. However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution. Recently, an increasing number of developers are turning to AI tools like ChatGPT to enhance problem-solving efficiency. While previous studies have demonstrated the potential of ChatGPT in areas such as automatic program repair, debugging, and code generation, there is a lack of study on how developers explicitly utilize ChatGPT to resolve issues in their tracking system. Hence, this study aims to examine the interaction between ChatGPT and developers to analyze their prevalent activities and provide a resolution. In addition, we assess the code reliability by confirming if the code produced by ChatGPT was integrated into the project's codebase using the clone detection tool NiCad. Our investigation reveals that developers mainly use ChatGPT for brainstorming solutions but often opt to write their code instead of using ChatGPT-generated code, possibly due to concerns over the generation of \"hallucinated code\", as highlighted in the literature.","sentences":["Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users' requirements.","However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution.","Recently, an increasing number of developers are turning to AI tools like ChatGPT to enhance problem-solving efficiency.","While previous studies have demonstrated the potential of ChatGPT in areas such as automatic program repair, debugging, and code generation, there is a lack of study on how developers explicitly utilize ChatGPT to resolve issues in their tracking system.","Hence, this study aims to examine the interaction between ChatGPT and developers to analyze their prevalent activities and provide a resolution.","In addition, we assess the code reliability by confirming if the code produced by ChatGPT was integrated into the project's codebase using the clone detection tool NiCad.","Our investigation reveals that developers mainly use ChatGPT for brainstorming solutions but often opt to write their code instead of using ChatGPT-generated code, possibly due to concerns over the generation of \"hallucinated code\", as highlighted in the literature."],"url":"http://arxiv.org/abs/2402.03735v1","category":"cs.SE"}
{"created":"2024-02-06 06:00:41","title":"Magnon mediated spin pumping by coupled ferrimagnetic garnets heterostructure","abstract":"Spin pumping has significant implications for spintronics, providing a mechanism to manipulate and transport spins for information processing. Understanding and harnessing spin currents through spin pumping is critical for the development of efficient spintronic devices. The use of a magnetic insulator with low damping, enhances the signal-to-noise ratio in crucial experiments such as spin-torque ferromagnetic resonance (FMR) and spin pumping. A magnetic insulator coupled with a heavy metal or quantum material offers a more straight forward model system, especially when investigating spin-charge interconversion processes to greater accuracy. This simplicity arises from the absence of unwanted effects caused by conduction electrons unlike in ferromagnetic metals. Here, we investigate the spin pumping in coupled ferrimagnetic (FiM) Y3Fe5O12 (YIG)/Tm3Fe5O12 (TmIG) bilayers combined with heavy-metal (Pt) using the inverse spin Hall effect (ISHE). It is observed that magnon transmission occurs at both of the FiMs FMR positions. The enhancement of spin pumping voltage (Vsp) in the FiM garnet heterostructures is attributed to the strong interfacial exchange coupling between FiMs. The modulation of Vsp is achieved by tuning the bilayer structure. Further, the spin mixing conductance for these coupled systems is found to be 10^18 m^-2. Our findings describe a novel coupled FiM system for the investigation of magnon coupling providing new prospects for magnonic devices.","sentences":["Spin pumping has significant implications for spintronics, providing a mechanism to manipulate and transport spins for information processing.","Understanding and harnessing spin currents through spin pumping is critical for the development of efficient spintronic devices.","The use of a magnetic insulator with low damping, enhances the signal-to-noise ratio in crucial experiments such as spin-torque ferromagnetic resonance (FMR) and spin pumping.","A magnetic insulator coupled with a heavy metal or quantum material offers a more straight forward model system, especially when investigating spin-charge interconversion processes to greater accuracy.","This simplicity arises from the absence of unwanted effects caused by conduction electrons unlike in ferromagnetic metals.","Here, we investigate the spin pumping in coupled ferrimagnetic (FiM) Y3Fe5O12 (YIG)/Tm3Fe5O12 (TmIG) bilayers combined with heavy-metal (Pt) using the inverse spin Hall effect (ISHE).","It is observed that magnon transmission occurs at both of the FiMs FMR positions.","The enhancement of spin pumping voltage (Vsp) in the FiM garnet heterostructures is attributed to the strong interfacial exchange coupling between FiMs.","The modulation of Vsp is achieved by tuning the bilayer structure.","Further, the spin mixing conductance for these coupled systems is found to be 10^18 m^-2.","Our findings describe a novel coupled FiM system for the investigation of magnon coupling providing new prospects for magnonic devices."],"url":"http://arxiv.org/abs/2402.03734v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 05:53:21","title":"Theory of parametric resonance for discrete time crystals in fully-connected spin-cavity systems","abstract":"We pinpoint the conditions necessary for discrete time crystal formation in fully-connected spin-cavity systems from the perspective of parametric resonance by mapping these systems onto oscillator-like models. We elucidate the role of nonlinearity and dissipation by mapping the periodically driven open Dicke model (DM) onto effective linear and nonlinear oscillator models, while we analyze the effect of global symmetry breaking using the Lipkin-Meshkov-Glick (LMG) model with tunable anisotropy. We show that the system's nonlinearity restrains the dynamics from becoming unbounded when driven resonantly. On the other hand, dissipation keeps the oscillation amplitude of the period-doubling instability fixed, which is a key feature of DTCs. The presence of global symmetry breaking in the absence of driving is found to be crucial in the parametric resonant activation of period-doubling response. We provide analytic predictions for the resonant frequencies and amplitudes leading to DTC formation for both systems using their respective oscillator models.","sentences":["We pinpoint the conditions necessary for discrete time crystal formation in fully-connected spin-cavity systems from the perspective of parametric resonance by mapping these systems onto oscillator-like models.","We elucidate the role of nonlinearity and dissipation by mapping the periodically driven open Dicke model (DM) onto effective linear and nonlinear oscillator models, while we analyze the effect of global symmetry breaking using the Lipkin-Meshkov-Glick (LMG) model with tunable anisotropy.","We show that the system's nonlinearity restrains the dynamics from becoming unbounded when driven resonantly.","On the other hand, dissipation keeps the oscillation amplitude of the period-doubling instability fixed, which is a key feature of DTCs.","The presence of global symmetry breaking in the absence of driving is found to be crucial in the parametric resonant activation of period-doubling response.","We provide analytic predictions for the resonant frequencies and amplitudes leading to DTC formation for both systems using their respective oscillator models."],"url":"http://arxiv.org/abs/2402.03729v1","category":"quant-ph"}
{"created":"2024-02-06 05:46:51","title":"Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes","abstract":"We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an unsupervised manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality. We empirically demonstrate that ISAHP is capable of discovering complex instance-level causal structures that cannot be handled by classical models. We also show that ISAHP achieves state-of-the-art performance in proxy tasks involving type-level causal discovery and instance-level event type prediction.","sentences":["We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences.","In particular, we are interested in discovering instance-level causal structures in an unsupervised manner.","Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making.","Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality.","We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level.","ISAHP is the first neural point process model that meets the requirements of Granger causality.","It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality.","We empirically demonstrate that ISAHP is capable of discovering complex instance-level causal structures that cannot be handled by classical models.","We also show that ISAHP achieves state-of-the-art performance in proxy tasks involving type-level causal discovery and instance-level event type prediction."],"url":"http://arxiv.org/abs/2402.03726v1","category":"cs.LG"}
{"created":"2024-02-06 05:44:36","title":"Charge correlator expansion for free fermion negativity","abstract":"Logarithmic negativity is a widely used entanglement measure in quantum information theories, which can also be efficiently computed in quantum many-body systems by replica trick or by relating to correlation matrices. In this paper, we demonstrate that in free-fermion systems with conserved charge, R\\'enyi and logarithmic negativity can be expanded by connected charge correlators, analogous to the case for entanglement entropy in the context of full counting statistics (FCS). We confirm the rapid convergence of this expansion in random all-connected Hamiltonian through numerical verification, especially for systems with only local hopping. We find that the replica trick that get logarithmic negativity from the limit of R\\'enyi negativity is valid in this method only for translational invariant systems. Using this expansion, we analyze the scaling behavior of negativity in extensive free-fermion systems. In particular, in 1+1 dimensional free-fermion systems, we observe that the scaling behavior of negativity from our expansion is consistent with known results from the method with Toeplitz matrix. These findings provide insights into the entanglement properties of free-fermion systems, and demonstrate the efficacy of the expansion approach in studying entanglement measures.","sentences":["Logarithmic negativity is a widely used entanglement measure in quantum information theories, which can also be efficiently computed in quantum many-body systems by replica trick or by relating to correlation matrices.","In this paper, we demonstrate that in free-fermion systems with conserved charge, R\\'enyi and logarithmic negativity can be expanded by connected charge correlators, analogous to the case for entanglement entropy in the context of full counting statistics (FCS).","We confirm the rapid convergence of this expansion in random all-connected Hamiltonian through numerical verification, especially for systems with only local hopping.","We find that the replica trick that get logarithmic negativity from the limit of R\\'enyi negativity is valid in this method only for translational invariant systems.","Using this expansion, we analyze the scaling behavior of negativity in extensive free-fermion systems.","In particular, in 1+1 dimensional free-fermion systems, we observe that the scaling behavior of negativity from our expansion is consistent with known results from the method with Toeplitz matrix.","These findings provide insights into the entanglement properties of free-fermion systems, and demonstrate the efficacy of the expansion approach in studying entanglement measures."],"url":"http://arxiv.org/abs/2402.03725v1","category":"quant-ph"}
{"created":"2024-02-06 05:32:49","title":"Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory","abstract":"Deep-learning and large scale language-image training have produced image object detectors that generalise well to diverse environments and semantic classes. However, single-image object detectors trained on internet data are not optimally tailored for the embodied conditions inherent in robotics. Instead, robots must detect objects from complex multi-modal data streams involving depth, localisation and temporal correlation, a task termed embodied object detection. Paradigms such as Video Object Detection (VOD) and Semantic Mapping have been proposed to leverage such embodied data streams, but existing work fails to enhance performance using language-image training. In response, we investigate how an image object detector pre-trained using language-image data can be extended to perform embodied object detection. We propose a novel implicit object memory that uses projective geometry to aggregate the features of detected objects across long temporal horizons. The spatial and temporal information accumulated in memory is then used to enhance the image features of the base detector. When tested on embodied data streams sampled from diverse indoor scenes, our approach improves the base object detector by 3.09 mAP, outperforming alternative external memories designed for VOD and Semantic Mapping. Our method also shows a significant improvement of 16.90 mAP relative to baselines that perform embodied object detection without first training on language-image data, and is robust to sensor noise and domain shift experienced in real-world deployment.","sentences":["Deep-learning and large scale language-image training have produced image object detectors that generalise well to diverse environments and semantic classes.","However, single-image object detectors trained on internet data are not optimally tailored for the embodied conditions inherent in robotics.","Instead, robots must detect objects from complex multi-modal data streams involving depth, localisation and temporal correlation, a task termed embodied object detection.","Paradigms such as Video Object Detection (VOD) and Semantic Mapping have been proposed to leverage such embodied data streams, but existing work fails to enhance performance using language-image training.","In response, we investigate how an image object detector pre-trained using language-image data can be extended to perform embodied object detection.","We propose a novel implicit object memory that uses projective geometry to aggregate the features of detected objects across long temporal horizons.","The spatial and temporal information accumulated in memory is then used to enhance the image features of the base detector.","When tested on embodied data streams sampled from diverse indoor scenes, our approach improves the base object detector by 3.09 mAP, outperforming alternative external memories designed for VOD and Semantic Mapping.","Our method also shows a significant improvement of 16.90 mAP relative to baselines that perform embodied object detection without first training on language-image data, and is robust to sensor noise and domain shift experienced in real-world deployment."],"url":"http://arxiv.org/abs/2402.03721v1","category":"cs.RO"}
{"created":"2024-02-06 05:15:29","title":"Retrospective Cost-based Extremum Seeking Control with Vanishing Perturbation for Online Output Minimization","abstract":"Extremum seeking control (ESC) constitutes a powerful technique for online optimization with theoretical guarantees for convergence to the neighborhood of the optimizer under well-understood conditions. However, ESC requires a nonconstant perturbation signal to provide persistent excitation to the target system to yield convergent results, which usually results in steady state oscillations. While certain techniques have been proposed to eliminate perturbations once the neighborhood of the minimizer is reached, system modifications and environmental perturbations can suddenly change the minimizer and nonconstant perturbations would once more be required to convergence to the new minimizer. Hence, this paper develops a retrospective cost-based ESC(RC/ESC) technique for online output minimization with a vanishing perturbation, that is, a perturbation that becomes zero as time increases independently from the state of the controller or the controlled system. The performance of the proposed algorithm is illustrated via numerical examples.","sentences":["Extremum seeking control (ESC) constitutes a powerful technique for online optimization with theoretical guarantees for convergence to the neighborhood of the optimizer under well-understood conditions.","However, ESC requires a nonconstant perturbation signal to provide persistent excitation to the target system to yield convergent results, which usually results in steady state oscillations.","While certain techniques have been proposed to eliminate perturbations once the neighborhood of the minimizer is reached, system modifications and environmental perturbations can suddenly change the minimizer and nonconstant perturbations would once more be required to convergence to the new minimizer.","Hence, this paper develops a retrospective cost-based ESC(RC/ESC) technique for online output minimization with a vanishing perturbation, that is, a perturbation that becomes zero as time increases independently from the state of the controller or the controlled system.","The performance of the proposed algorithm is illustrated via numerical examples."],"url":"http://arxiv.org/abs/2402.03717v1","category":"math.OC"}
{"created":"2024-02-06 18:46:27","title":"Error budget of parametric resonance entangling gate with a tunable coupler","abstract":"We analyze the experimental error budget of parametric resonance gates in a tunable coupler architecture. We identify and characterize various sources of errors, including incoherent, leakage, amplitude, and phase errors. By varying the two-qubit gate time, we explore the dynamics of these errors and their impact on the gate fidelity. To accurately capture the impact of incoherent errors on gate fidelity, we measure the coherence times of qubits under gate operating conditions. Our findings reveal that the incoherent errors, mainly arising from qubit relaxation and dephasing due to white noise, limit the fidelity of the two-qubit gates. Moreover, we demonstrate that leakage to noncomputational states is the second largest contributor to the two-qubit gates infidelity, as characterized using leakage-randomized benchmarking. The error budgeting methodology we developed here can be effectively applied to other types of gate implementations.","sentences":["We analyze the experimental error budget of parametric resonance gates in a tunable coupler architecture.","We identify and characterize various sources of errors, including incoherent, leakage, amplitude, and phase errors.","By varying the two-qubit gate time, we explore the dynamics of these errors and their impact on the gate fidelity.","To accurately capture the impact of incoherent errors on gate fidelity, we measure the coherence times of qubits under gate operating conditions.","Our findings reveal that the incoherent errors, mainly arising from qubit relaxation and dephasing due to white noise, limit the fidelity of the two-qubit gates.","Moreover, we demonstrate that leakage to noncomputational states is the second largest contributor to the two-qubit gates infidelity, as characterized using leakage-randomized benchmarking.","The error budgeting methodology we developed here can be effectively applied to other types of gate implementations."],"url":"http://arxiv.org/abs/2402.04238v1","category":"quant-ph"}
{"created":"2024-02-06 18:23:47","title":"A classification of nonzero skew immaculate functions","abstract":"This article presents conditions under which the skewed version of immaculate noncommutative symmetric functions are nonzero. The work is motivated by the quest to determine when the matrix definition of a skew immaculate function aligns with the Hopf algberaic definition. We describe a necessary condition for a skew immaculate function to include a non-zero term, as well as a sufficient condition for there to be at least one non-zero term that survives any cancellation. We bring in several classical theorems such as the Pigeonhole Principle from combinatorics and Hall's Matching Theorem from graph theory to prove our theorems.","sentences":["This article presents conditions under which the skewed version of immaculate noncommutative symmetric functions are nonzero.","The work is motivated by the quest to determine when the matrix definition of a skew immaculate function aligns with the Hopf algberaic definition.","We describe a necessary condition for a skew immaculate function to include a non-zero term, as well as a sufficient condition for there to be at least one non-zero term that survives any cancellation.","We bring in several classical theorems such as the Pigeonhole Principle from combinatorics and Hall's Matching Theorem from graph theory to prove our theorems."],"url":"http://arxiv.org/abs/2402.04219v1","category":"math.CO"}
{"created":"2024-02-06 17:58:09","title":"Transfer maps and the Morley product in NIP theories","abstract":"In an important (yet unpublished) research note, Ben Yaacov describes how to turn a global Keisler measures into a type over a monster model of the randomization. This transfer methods allow one to turn questions involving measures into those involving types (in continuous logic). Assuming that T is NIP, we do the following: We verify some of the main claims from that note. We also present some new proofs via the (almost) standardized Keisler measures calculus. We then show that the Morley product commutes with the transfer map for invariant measures. On the other hand, we observe that the Morley product does not commute with the restriction map from random types to global measures. We characterize when the Morley product commutes with the restriction map for pairs of global finitely satisfiable types in the randomization. We end by making some brief observations about the Ellis semigroup in this context.","sentences":["In an important (yet unpublished) research note, Ben Yaacov describes how to turn a global Keisler measures into a type over a monster model of the randomization.","This transfer methods allow one to turn questions involving measures into those involving types (in continuous logic).","Assuming that T is NIP, we do the following: We verify some of the main claims from that note.","We also present some new proofs via the (almost) standardized Keisler measures calculus.","We then show that the Morley product commutes with the transfer map for invariant measures.","On the other hand, we observe that the Morley product does not commute with the restriction map from random types to global measures.","We characterize when the Morley product commutes with the restriction map for pairs of global finitely satisfiable types in the randomization.","We end by making some brief observations about the Ellis semigroup in this context."],"url":"http://arxiv.org/abs/2402.04202v1","category":"math.LO"}
{"created":"2024-02-06 17:50:39","title":"Observation of the double quantum spin Hall phase in moir\u00e9 WSe2","abstract":"Quantum spin Hall (QSH) insulators are a topologically protected phase of matter in two dimensions that can support non-dissipative spin transport. A hallmark of the phase is a pair of helical edge states surrounding an insulating bulk. A higher (even) number of helical edge state pairs is usually not possible in real materials because spin mixing would gap out the edge states. Multiple pairs of helical edge states have been proposed in materials with spin conservation symmetry and high spin Chern bands, but remained experimentally elusive. Here, we demonstrate a QSH phase with one and two pairs of helical edge states in twisted bilayer WSe2 at moir\\'e hole filling factor {\\nu}= 2 and 4, respectively. We observe nearly quantized conductance or resistance plateaus of h/({\\nu}e^2 ) at {\\nu} = 2 and 4 while the bulk is insulating. The conductance is nearly independent of out-of-plane magnetic field and decreases under an in-plane magnetic field. We also observe nonlocal transport, which is sensitive only to the in-plane magnetic field. The results agree with quantum transport of helical edge states protected by Ising spin conservation symmetry and open a promising platform for low-power spintronics.","sentences":["Quantum spin Hall (QSH) insulators are a topologically protected phase of matter in two dimensions that can support non-dissipative spin transport.","A hallmark of the phase is a pair of helical edge states surrounding an insulating bulk.","A higher (even) number of helical edge state pairs is usually not possible in real materials because spin mixing would gap out the edge states.","Multiple pairs of helical edge states have been proposed in materials with spin conservation symmetry and high spin Chern bands, but remained experimentally elusive.","Here, we demonstrate a QSH phase with one and two pairs of helical edge states in twisted bilayer WSe2 at moir\\'e hole filling factor {\\nu}= 2 and 4, respectively.","We observe nearly quantized conductance or resistance plateaus of h/({\\nu}e^2 ) at {\\nu} = 2 and 4 while the bulk is insulating.","The conductance is nearly independent of out-of-plane magnetic field and decreases under an in-plane magnetic field.","We also observe nonlocal transport, which is sensitive only to the in-plane magnetic field.","The results agree with quantum transport of helical edge states protected by Ising spin conservation symmetry and open a promising platform for low-power spintronics."],"url":"http://arxiv.org/abs/2402.04196v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 17:48:31","title":"Collider Signatures of $W_R$ boson in the Alternative Left-Right Model","abstract":"Alternative Left-Right Models offer an attractive option to left-right models. Emerging from $E_6$ grand unification, these models are consistent with light scalars which do not induce flavour-changing neutral currents due to the presence of exotic quarks. Here we investigate the signature at the LHC collider of the charged $W_R$ boson, which can be lighter than in left-right models. We include constraints from collider data and show that $W_R$ can be produced in pairs, or in conjunction with a light charged Higgs boson. The final decay products involve leptons or jets. We explore all production and decay possibilities and indicate which ones are most promising to be observed at the colliders. Our analysis shows that signals of $W_R$ bosons can be observed at the LHC at 27 TeV, some for lower luminosity, and under most favourable conditions, even at 13 TeV.","sentences":["Alternative Left-Right Models offer an attractive option to left-right models.","Emerging from $E_6$ grand unification, these models are consistent with light scalars which do not induce flavour-changing neutral currents due to the presence of exotic quarks.","Here we investigate the signature at the LHC collider of the charged $W_R$ boson, which can be lighter than in left-right models.","We include constraints from collider data and show that $W_R$ can be produced in pairs, or in conjunction with a light charged Higgs boson.","The final decay products involve leptons or jets.","We explore all production and decay possibilities and indicate which ones are most promising to be observed at the colliders.","Our analysis shows that signals of $W_R$ bosons can be observed at the LHC at 27 TeV, some for lower luminosity, and under most favourable conditions, even at 13 TeV."],"url":"http://arxiv.org/abs/2402.04192v1","category":"hep-ph"}
{"created":"2024-02-06 17:21:39","title":"Monthly GDP nowcasting with Machine Learning and Unstructured Data","abstract":"In the dynamic landscape of continuous change, Machine Learning (ML) \"nowcasting\" models offer a distinct advantage for informed decision-making in both public and private sectors. This study introduces ML-based GDP growth projection models for monthly rates in Peru, integrating structured macroeconomic indicators with high-frequency unstructured sentiment variables. Analyzing data from January 2007 to May 2023, encompassing 91 leading economic indicators, the study evaluates six ML algorithms to identify optimal predictors. Findings highlight the superior predictive capability of ML models using unstructured data, particularly Gradient Boosting Machine, LASSO, and Elastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to traditional AR and Dynamic Factor Models (DFM). This enhanced performance is attributed to better handling of data of ML models in high-uncertainty periods, such as economic crises.","sentences":["In the dynamic landscape of continuous change, Machine Learning (ML) \"nowcasting\" models offer a distinct advantage for informed decision-making in both public and private sectors.","This study introduces ML-based GDP growth projection models for monthly rates in Peru, integrating structured macroeconomic indicators with high-frequency unstructured sentiment variables.","Analyzing data from January 2007 to May 2023, encompassing 91 leading economic indicators, the study evaluates six ML algorithms to identify optimal predictors.","Findings highlight the superior predictive capability of ML models using unstructured data, particularly Gradient Boosting Machine, LASSO, and Elastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to traditional AR and Dynamic Factor Models (DFM).","This enhanced performance is attributed to better handling of data of ML models in high-uncertainty periods, such as economic crises."],"url":"http://arxiv.org/abs/2402.04165v1","category":"econ.EM"}
{"created":"2024-02-06 16:40:37","title":"Correlation between objective and subjective assessment of noise barriers","abstract":"There are several international standards that define the way to evaluate the attenuation capacity of noise reducing devices, by single-number quantities representing airborne sound insulation and insertion loss. These two single-value ratings define the quality and performance of acoustic barriers, the former being related to intrinsic and the latter to both intrinsic and extrinsic acoustic characteristics of the devices. However, not many studies can be found on whether these objective parameters correlate to the perception of annoyance reduction. The aim of the present work is to analyze the adequacy of these objective ratings to indicate the performance of noise barriers, by comparing their values with the perception of annoyance reduction. For this purpose, ninety individuals of two different nationalities (Spanish and Portuguese) were asked to rate the perceived annoyance reduction in a listening experimental test, in which they were exposed, under controlled conditions, to several environmental noises and acoustic screened stimuli simulated by audio filters. The obtained results show a high correlation between objective ratings and subjective annoyance perception, with a better correlation being observed for insertion loss single-number parameter than for the airborne sound insulation single-number rating. Furthermore, significant differences were found depending on the gender and nationality of the respondents. The results, from this ongoing research work, may be of great interest for future acoustic barriers design.","sentences":["There are several international standards that define the way to evaluate the attenuation capacity of noise reducing devices, by single-number quantities representing airborne sound insulation and insertion loss.","These two single-value ratings define the quality and performance of acoustic barriers, the former being related to intrinsic and the latter to both intrinsic and extrinsic acoustic characteristics of the devices.","However, not many studies can be found on whether these objective parameters correlate to the perception of annoyance reduction.","The aim of the present work is to analyze the adequacy of these objective ratings to indicate the performance of noise barriers, by comparing their values with the perception of annoyance reduction.","For this purpose, ninety individuals of two different nationalities (Spanish and Portuguese) were asked to rate the perceived annoyance reduction in a listening experimental test, in which they were exposed, under controlled conditions, to several environmental noises and acoustic screened stimuli simulated by audio filters.","The obtained results show a high correlation between objective ratings and subjective annoyance perception, with a better correlation being observed for insertion loss single-number parameter than for the airborne sound insulation single-number rating.","Furthermore, significant differences were found depending on the gender and nationality of the respondents.","The results, from this ongoing research work, may be of great interest for future acoustic barriers design."],"url":"http://arxiv.org/abs/2402.04136v1","category":"physics.app-ph"}
{"created":"2024-02-06 16:31:11","title":"OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning","abstract":"Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks. Our source code is available at https://github.com/jpmorganchase/ovor.","sentences":["Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones.","Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together.","In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated.","Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool.","This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper.","We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost.","Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks.","Our source code is available at https://github.com/jpmorganchase/ovor."],"url":"http://arxiv.org/abs/2402.04129v1","category":"cs.LG"}
{"created":"2024-02-06 16:12:12","title":"An explicit Euler method for the continuity equation with Sobolev velocity fields","abstract":"We prove a stability estimate, in a suitable expected value, of the $1$-Wasserstein distance between the solution of the continuity equation under a Sobolev velocity field and a measure obtained by pushing forward Dirac deltas whose centers belong to a partition of the domain by a (sort of) explicit forward Euler method. The main tool is a $L^\\infty_t (L^p_x)$ estimate on the difference between the regular Lagrangian flow of the velocity field and an explicitly constructed approximation of such flow. Although our result only gives estimates in expected value, it has the advantage of being easily parallelizable and of not relying on any particular structure on the mesh. At the end, we also provide estimates with a logarithmic Wasserstein distance, already used in other works on this particular problem.","sentences":["We prove a stability estimate, in a suitable expected value, of the $1$-Wasserstein distance between the solution of the continuity equation under a Sobolev velocity field and a measure obtained by pushing forward Dirac deltas whose centers belong to a partition of the domain by a (sort of) explicit forward Euler method.","The main tool is a $L^\\infty_t (L^p_x)$ estimate on the difference between the regular Lagrangian flow of the velocity field and an explicitly constructed approximation of such flow.","Although our result only gives estimates in expected value, it has the advantage of being easily parallelizable and of not relying on any particular structure on the mesh.","At the end, we also provide estimates with a logarithmic Wasserstein distance, already used in other works on this particular problem."],"url":"http://arxiv.org/abs/2402.04118v1","category":"math.AP"}
{"created":"2024-02-06 16:12:11","title":"Optimization of Neumann Eigenvalues under convexity and geometric constraints","abstract":"In this paper we study optimization problems for Neumann eigenvalues $\\mu_k$ among convex domains with a constraint on the diameter or the perimeter. We work mainly in the plane, though some results are stated in higher dimension. We study the existence of an optimal domain in all considered cases. We also consider the case of the unit disk, giving values of the index $k$ for which it can be or cannot be extremal. We give some numerical examples for small values of $k$ that lead us to state some conjectures.","sentences":["In this paper we study optimization problems for Neumann eigenvalues $\\mu_k$ among convex domains with a constraint on the diameter or the perimeter.","We work mainly in the plane, though some results are stated in higher dimension.","We study the existence of an optimal domain in all considered cases.","We also consider the case of the unit disk, giving values of the index $k$ for which it can be or cannot be extremal.","We give some numerical examples for small values of $k$ that lead us to state some conjectures."],"url":"http://arxiv.org/abs/2402.04117v1","category":"math.AP"}
{"created":"2024-02-06 16:06:18","title":"TESS and ESPRESSO discover a super-Earth and a mini-Neptune orbiting the K-dwarf TOI-238","abstract":"The number of super-Earth and mini-Neptune planet discoveries has increased significantly in the last two decades thanks to transit and radial velocity surveys. When it is possible to apply both techniques, we can characterise the internal composition of exoplanets, which in turn provides unique insights on their architecture, formation and evolution.   We performed a combined photometric and radial velocity analysis of TOI-238 (TYC 6398-132-1), which has one short-orbit super-Earth planet candidate announced by NASA's TESS team. We aim to confirm its planetary nature using radial velocities taken with the ESPRESSO and HARPS spectrographs, to measure its mass and to detect the presence of other possible planetary companions. We carried out a joint analysis by including Gaussian processes and Keplerian orbits to account for the stellar activity and planetary signals simultaneously.   We detected the signal induced by TOI-238 b in the radial velocity time-series, and the presence of a second transiting planet, TOI-238 c, whose signal appears in RV and TESS data. TOI-238 b is a planet with a radius of 1.402$^{+0.084}_{-0.086}$ R$_{\\oplus}$ and a mass of 3.40$^{+0.46}_{-0.45}$ M$_{\\oplus}$. It orbits at a separation of 0.02118 $\\pm$ 0.00038 AU of its host star, with an orbital period of 1.2730988 $\\pm$ 0.0000029 days, and has an equilibrium temperature of 1311 $\\pm$ 28 K. TOI-238 c has a radius of 2.18$\\pm$ 0.18 R$_{\\oplus}$ and a mass of 6.7 $\\pm$ 1.1 M$_{\\oplus}$. It orbits at a separation of 0.0749 $\\pm$ 0.0013 AU of its host star, with an orbital period of 8.465652 $\\pm$ 0.000031 days, and has an equilibrium temperature of 696 $\\pm$ 15 K. The mass and radius of planet b are fully consistent with an Earth-like composition, making it likely a rocky super-Earth. Planet c could be a water-rich planet or a rocky planet with a small H-He atmosphere.","sentences":["The number of super-Earth and mini-Neptune planet discoveries has increased significantly in the last two decades thanks to transit and radial velocity surveys.","When it is possible to apply both techniques, we can characterise the internal composition of exoplanets, which in turn provides unique insights on their architecture, formation and evolution.   ","We performed a combined photometric and radial velocity analysis of TOI-238 (TYC 6398-132-1), which has one short-orbit super-Earth planet candidate announced by NASA's TESS team.","We aim to confirm its planetary nature using radial velocities taken with the ESPRESSO and HARPS spectrographs, to measure its mass and to detect the presence of other possible planetary companions.","We carried out a joint analysis by including Gaussian processes and Keplerian orbits to account for the stellar activity and planetary signals simultaneously.   ","We detected the signal induced by TOI-238 b in the radial velocity time-series, and the presence of a second transiting planet, TOI-238 c, whose signal appears in RV and TESS data.","TOI-238 b is a planet with a radius of 1.402$^{+0.084}_{-0.086}$ R$_{\\oplus}$ and a mass of 3.40$^{+0.46}_{-0.45}$ M$_{\\oplus}$. It orbits at a separation of 0.02118 $\\pm$ 0.00038 AU of its host star, with an orbital period of 1.2730988 $\\pm$ 0.0000029 days, and has an equilibrium temperature of 1311 $\\pm$ 28 K. TOI-238 c has a radius of 2.18$\\pm$ 0.18 R$_{\\oplus}$ and a mass of 6.7 $\\pm$ 1.1 M$_{\\oplus}$. It orbits at a separation of 0.0749 $\\pm$ 0.0013 AU of its host star, with an orbital period of 8.465652 $\\pm$ 0.000031 days, and has an equilibrium temperature of 696 $\\pm$ 15 K. The mass and radius of planet b are fully consistent with an Earth-like composition, making it likely a rocky super-Earth.","Planet c could be a water-rich planet or a rocky planet with a small H-He atmosphere."],"url":"http://arxiv.org/abs/2402.04113v1","category":"astro-ph.EP"}
{"created":"2024-02-06 16:01:28","title":"Casimir energy of hyperbolic elements","abstract":"The contribution, E, of hyperbolic elements to the scalar Casimir energy on a compact quotient of the upper half hyperbolic plane is computed for a propagation operator conformal in three dimensions. Due to the proliferation of prime closed geodesics, the series form for the Casimir energy has an IR divergence. The expression for E is given as a sum of polylogarithms which allows the divergence to be isolated and rendered finite by an {\\it ad hoc} Ramanujan renormalisation. The remaining part of E is computed using the specific lower length spectrum of the (2,3,7) triangle and a universal asymptotic form for larger lengths. The tentative value of E found is such as to make the total conformal Casimir energy on the triangle probably negative.","sentences":["The contribution, E, of hyperbolic elements to the scalar Casimir energy on a compact quotient of the upper half hyperbolic plane is computed for a propagation operator conformal in three dimensions.","Due to the proliferation of prime closed geodesics, the series form for the Casimir energy has an IR divergence.","The expression for E is given as a sum of polylogarithms which allows the divergence to be isolated and rendered finite by an {\\it ad hoc} Ramanujan renormalisation.","The remaining part of E is computed using the specific lower length spectrum of the (2,3,7) triangle and a universal asymptotic form for larger lengths.","The tentative value of E found is such as to make the total conformal Casimir energy on the triangle probably negative."],"url":"http://arxiv.org/abs/2402.04107v1","category":"hep-th"}
{"created":"2024-02-06 15:51:56","title":"New Physical Mechanism for Lightning","abstract":"The article is devoted to electromagnetic phenomena in the atmosphere. The set of experimental data on the thunderstorm activity is analyzed. It helps to identify a possible physical mechanism of lightning flashes. This mechanism can involve the formation of metallic bonds in thunderclouds. The analysis of the problem is performed at a microphysical level within the framework of quantum mechanics. The mechanism of appearance of metallic conductivity includes the resonant tunneling of electrons along resonance-percolation trajectories. Such bonds allow the charges from the vast cloud charged subsystems concentrate quickly in lightning channel. The formation of metal bonds in the thunderstorm cloudiness is described as the second-order phase transition. A successive mechanism for the process of formation and development of the lightning channel is suggested. This mechanism is associated with the change in the orientation of crystals in growing electric field. Possible consequences of the quantum-mechanical mechanism under discussion are compared with the results of observations.","sentences":["The article is devoted to electromagnetic phenomena in the atmosphere.","The set of experimental data on the thunderstorm activity is analyzed.","It helps to identify a possible physical mechanism of lightning flashes.","This mechanism can involve the formation of metallic bonds in thunderclouds.","The analysis of the problem is performed at a microphysical level within the framework of quantum mechanics.","The mechanism of appearance of metallic conductivity includes the resonant tunneling of electrons along resonance-percolation trajectories.","Such bonds allow the charges from the vast cloud charged subsystems concentrate quickly in lightning channel.","The formation of metal bonds in the thunderstorm cloudiness is described as the second-order phase transition.","A successive mechanism for the process of formation and development of the lightning channel is suggested.","This mechanism is associated with the change in the orientation of crystals in growing electric field.","Possible consequences of the quantum-mechanical mechanism under discussion are compared with the results of observations."],"url":"http://arxiv.org/abs/2402.04096v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 15:46:48","title":"Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures","abstract":"This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget. A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features. The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers. Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements. Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture.","sentences":["This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget.","A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.","LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features.","The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers.","Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements.","Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture."],"url":"http://arxiv.org/abs/2402.04090v1","category":"cs.PF"}
{"created":"2024-02-06 15:46:47","title":"Large Volume Scenario from Schoen Manifold with de Sitter under Swampland Conjecture","abstract":"To naturally allow for string compactification with duality manifested, here we investigate in the self-mirror large volume scenarios from Schoen Calabi-Yau manifold. We explicitly study the geometry of Schoen Calabi-Yau threefold and complete its triple intersection from both ambient and non-ambient spaces. Based on these, we study the large volume scenario of self-mirror Calabi-Yau compactification with Schoen type. Moreover, by studying the leading non-perturbative terms of the effective scalar potential, we find special uplift terms in order of F-term $\\mathcal{O}(\\frac{1}{\\mathcal{V}^2})$ arising from self-mirror large volume scenario. In particular, the quotient Schoen and Schoen Calabi-Yau large volume scenarios both give rise to de Sitter vacua. In addition, we discussed on the criteria to the effective scalar potential derived from self-mirror large volume scenario according to the swampland conjecture with the constraints fulfilled.","sentences":["To naturally allow for string compactification with duality manifested, here we investigate in the self-mirror large volume scenarios from Schoen Calabi-Yau manifold.","We explicitly study the geometry of Schoen Calabi-Yau threefold and complete its triple intersection from both ambient and non-ambient spaces.","Based on these, we study the large volume scenario of self-mirror Calabi-Yau compactification with Schoen type.","Moreover, by studying the leading non-perturbative terms of the effective scalar potential, we find special uplift terms in order of F-term $\\mathcal{O}(\\frac{1}{\\mathcal{V}^2})$ arising from self-mirror large volume scenario.","In particular, the quotient Schoen and Schoen Calabi-Yau large volume scenarios both give rise to de Sitter vacua.","In addition, we discussed on the criteria to the effective scalar potential derived from self-mirror large volume scenario according to the swampland conjecture with the constraints fulfilled."],"url":"http://arxiv.org/abs/2402.04089v1","category":"hep-th"}
{"created":"2024-02-06 15:38:03","title":"Cooperation and profit allocation in distribution chains","abstract":"We study the coordination of actions and the allocation of profit in supply chains under decentralized control in which a single supplier supplies several retailers with goods for replenishment of stocks. The goal of the supplier and the retailers is to maximize their individual profits. Since the outcome under decentralized control is inefficient, cooperation among firms by means of coordination of actions may improve the individual profits. Cooperation is studied by means of cooperative game theory. Among others we show that the corresponding games are balanced and we propose a stable solution concept for these games.","sentences":["We study the coordination of actions and the allocation of profit in supply chains under decentralized control in which a single supplier supplies several retailers with goods for replenishment of stocks.","The goal of the supplier and the retailers is to maximize their individual profits.","Since the outcome under decentralized control is inefficient, cooperation among firms by means of coordination of actions may improve the individual profits.","Cooperation is studied by means of cooperative game theory.","Among others we show that the corresponding games are balanced and we propose a stable solution concept for these games."],"url":"http://arxiv.org/abs/2402.04083v1","category":"cs.GT"}
{"created":"2024-02-06 15:05:25","title":"TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments","abstract":"Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immediate tasks like obstacle avoidance while maintaining focus on the overall goal. We demonstrate TopoNav's effectiveness in simulated environments that replicate real-world conditions. Our results reveal significant improvements in exploration efficiency, navigational accuracy, and adaptability to unforeseen obstacles, showcasing its potential to revolutionize autonomous exploration in a wide range of applications, including search and rescue, environmental monitoring, and planetary exploration.","sentences":["Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback.","This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail.","In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration.","TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization.","Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways.","It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings.","To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immediate tasks like obstacle avoidance while maintaining focus on the overall goal.","We demonstrate TopoNav's effectiveness in simulated environments that replicate real-world conditions.","Our results reveal significant improvements in exploration efficiency, navigational accuracy, and adaptability to unforeseen obstacles, showcasing its potential to revolutionize autonomous exploration in a wide range of applications, including search and rescue, environmental monitoring, and planetary exploration."],"url":"http://arxiv.org/abs/2402.04061v1","category":"cs.RO"}
{"created":"2024-02-06 14:53:28","title":"Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching","abstract":"Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly align the directions of singular vectors associated with large singular values across models. This alignment brings the singular vectors with large singular values, which determine the model functionality, closer between pre-merged and post-merged models, so that the post-merged model retains functionality similar to the pre-merged models, making it easy to satisfy LMC. Finally, we analyze the difference between WM and straight-through estimator (STE), a dataset-dependent permutation search method, and show that WM outperforms STE, especially when merging three or more models.","sentences":["Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant.","This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging.","We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself.","We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer.","This finding shows that permutations found by WM mainly align the directions of singular vectors associated with large singular values across models.","This alignment brings the singular vectors with large singular values, which determine the model functionality, closer between pre-merged and post-merged models, so that the post-merged model retains functionality similar to the pre-merged models, making it easy to satisfy LMC.","Finally, we analyze the difference between WM and straight-through estimator (STE), a dataset-dependent permutation search method, and show that WM outperforms STE, especially when merging three or more models."],"url":"http://arxiv.org/abs/2402.04051v1","category":"cs.LG"}
{"created":"2024-02-06 14:49:58","title":"Does $E=mc^2$ Require Relativity?","abstract":"It is universally believed that with his 1905 paper ``Does the inertia of a body depend on its energy content?\" Einstein first demonstrated the equivalence of mass and energy by making use of his new special theory of relativity. In the final step of that paper, however, Einstein equates the kinetic energy of a body to its Newtonian value, indicating that his result is at best a low-velocity approximation. Today, several characters debate whether a mid-nineteenth century physicist, employing only Galilean and pre-Maxwellian physics could plausibly arrive at the celebrated result. In other words, is Einsteinian relativity necessary to derive ${\\mathcal E}=mc^2$?","sentences":["It is universally believed that with his 1905 paper ``Does the inertia of a body depend on its energy content?\"","Einstein first demonstrated the equivalence of mass and energy by making use of his new special theory of relativity.","In the final step of that paper, however, Einstein equates the kinetic energy of a body to its Newtonian value, indicating that his result is at best a low-velocity approximation.","Today, several characters debate whether a mid-nineteenth century physicist, employing only Galilean and pre-Maxwellian physics could plausibly arrive at the celebrated result.","In other words, is Einsteinian relativity necessary to derive ${\\mathcal E}=mc^2$?"],"url":"http://arxiv.org/abs/2402.04047v1","category":"physics.hist-ph"}
{"created":"2024-02-06 14:43:00","title":"Interlaboratory comparison of particle filtration efficiency testing equipment","abstract":"This work presents the results of two interlaboratory comparisons of particle filtration efficiency measurements performed by a network of laboratories across Canada and Australia. Testing across multiple layers of a common verification material demonstrates a constant size-resolved quality factor when layering uncharged materials. Size-resolved filtration curves also match expectations, with increasingly size-dependent curves and a predictable increase in the PFE. Candidate reference materials with controlled material properties were also tested across multiple laboratories. Each set of materials sharing a common charge level show specific trends with the material basis weight. Respirators showed more consistency between the laboratories than the other filters. However, across a majority of the tests, dark uncertainties, which are otherwise unexplained variability between laboratories, are significant. This leaves room to improve the test method by developing improved verification procedures and additional reference materials.","sentences":["This work presents the results of two interlaboratory comparisons of particle filtration efficiency measurements performed by a network of laboratories across Canada and Australia.","Testing across multiple layers of a common verification material demonstrates a constant size-resolved quality factor when layering uncharged materials.","Size-resolved filtration curves also match expectations, with increasingly size-dependent curves and a predictable increase in the PFE.","Candidate reference materials with controlled material properties were also tested across multiple laboratories.","Each set of materials sharing a common charge level show specific trends with the material basis weight.","Respirators showed more consistency between the laboratories than the other filters.","However, across a majority of the tests, dark uncertainties, which are otherwise unexplained variability between laboratories, are significant.","This leaves room to improve the test method by developing improved verification procedures and additional reference materials."],"url":"http://arxiv.org/abs/2402.04044v1","category":"physics.med-ph"}
{"created":"2024-02-06 14:33:11","title":"Electrical conductivity of MgSiO3 at high temperatures and pressures: implications for the Earth's mantle","abstract":"The electrical conductivity of magnesium silicate MgSiO3 has been studied, using the framework of the first-principles density functional theory and the Boltzmann transport theory, under the thermodynamic conditions of the Earth's lower mantle. We find that the conductivity of pristine MgSiO3 depends strongly on the structural phase of the material, as well as on temperature and pressure. The conductivity of the perovskite phase increases with increasing pressure (depth of the lower mantle) up to 90 GPa, then decreases at higher pressures due to a change in the material's band gap transition from direct to indirect. Finally, the structural phase transition that MgSiO3 undergoes near the bottom of the lower mantle, from perovskite to post-perovskite, causes an increase in the conductivity of MgSiO3, which should contribute to the increase in the electrical conductivity of the Earth's mantle under the thermodynamic conditions of the Earth's D\" layer.","sentences":["The electrical conductivity of magnesium silicate MgSiO3 has been studied, using the framework of the first-principles density functional theory and the Boltzmann transport theory, under the thermodynamic conditions of the Earth's lower mantle.","We find that the conductivity of pristine MgSiO3 depends strongly on the structural phase of the material, as well as on temperature and pressure.","The conductivity of the perovskite phase increases with increasing pressure (depth of the lower mantle) up to 90 GPa, then decreases at higher pressures due to a change in the material's band gap transition from direct to indirect.","Finally, the structural phase transition that MgSiO3 undergoes near the bottom of the lower mantle, from perovskite to post-perovskite, causes an increase in the conductivity of MgSiO3, which should contribute to the increase in the electrical conductivity of the Earth's mantle under the thermodynamic conditions of the Earth's D\" layer."],"url":"http://arxiv.org/abs/2402.04036v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 14:22:56","title":"Demonstration of 4H silicon carbide on aluminum nitride integrated photonics platform","abstract":"The existing silicon-carbide-on-insulator photonic platform utilizes a thin layer of silicon dioxide under silicon carbide to provide optical confinement and mode isolation. Here, we replace the underneath silicon dioxide layer with a 1-$\\mu$m-thick aluminum nitride and demonstrate a 4H-silicon-carbide-on-aluminum-nitride integrated photonics platform for the first time. Efficient grating couplers, low-loss waveguides, and compact microring resonators with intrinsic quality factors up to 210,000 are fabricated. In addition, by undercutting the aluminum nitride layer, the intrinsic quality factor of the silicon carbide microring is improved by nearly one order of magnitude (1.8 million). Finally, an optical pump-probe method is developed to measure the thermal conductivity of the aluminum nitride layer, which is estimated to be over 30 times of that of silicon dioxide.","sentences":["The existing silicon-carbide-on-insulator photonic platform utilizes a thin layer of silicon dioxide under silicon carbide to provide optical confinement and mode isolation.","Here, we replace the underneath silicon dioxide layer with a 1-$\\mu$m-thick aluminum nitride and demonstrate a 4H-silicon-carbide-on-aluminum-nitride integrated photonics platform for the first time.","Efficient grating couplers, low-loss waveguides, and compact microring resonators with intrinsic quality factors up to 210,000 are fabricated.","In addition, by undercutting the aluminum nitride layer, the intrinsic quality factor of the silicon carbide microring is improved by nearly one order of magnitude (1.8 million).","Finally, an optical pump-probe method is developed to measure the thermal conductivity of the aluminum nitride layer, which is estimated to be over 30 times of that of silicon dioxide."],"url":"http://arxiv.org/abs/2402.04026v1","category":"physics.optics"}
{"created":"2024-02-06 14:00:43","title":"Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning","abstract":"As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the benefits of our approach in a variety of datasets, achieving state-of-the-art performance.","sentences":["As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel.","Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL).","MTL aims at learning a single model that solves several tasks efficiently.","Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction.","However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions.","Here, we introduce a novel gradient aggregation approach using Bayesian inference.","We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks.","This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them.","We empirically demonstrate the benefits of our approach in a variety of datasets, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.04005v1","category":"cs.LG"}
{"created":"2024-02-06 13:26:19","title":"REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR","abstract":"Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity. We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech. We comprehensively analyze why the boundaries learned by REBORN improve the unsupervised ASR performance.","sentences":["Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data.","A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data.","In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR.","REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription.","Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity.","We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech.","We comprehensively analyze why the boundaries learned by REBORN improve the unsupervised ASR performance."],"url":"http://arxiv.org/abs/2402.03988v1","category":"eess.AS"}
{"created":"2024-02-06 13:16:50","title":"Cross Entropy versus Label Smoothing: A Neural Collapse Perspective","abstract":"Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.","sentences":["Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks.","This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training.","We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse.","Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2.","These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss.","We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster.","Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs."],"url":"http://arxiv.org/abs/2402.03979v1","category":"cs.LG"}
{"created":"2024-02-06 12:55:57","title":"Cyclic and BCH Codes whose Minimum Distance Equals their Maximum BCH bound","abstract":"In this paper we study the family of cyclic codes such that its minimum distance reaches the maximum of its BCH bounds. We also show a way to construct cyclic codes with that property by means of computations of some divisors of a polynomial of the form X^n-1. We apply our results to the study of those BCH codes C, with designed distance delta, that have minimum distance d(C)= delta. Finally, we present some examples of new binary BCH codes satisfying that condition. To do this, we make use of two related tools: the discrete Fourier transform and the notion of apparent distance of a code, originally defined for multivariate abelian codes.","sentences":["In this paper we study the family of cyclic codes such that its minimum distance reaches the maximum of its BCH bounds.","We also show a way to construct cyclic codes with that property by means of computations of some divisors of a polynomial of the form X^n-1.","We apply our results to the study of those BCH codes C, with designed distance delta, that have minimum distance d(C)= delta.","Finally, we present some examples of new binary BCH codes satisfying that condition.","To do this, we make use of two related tools: the discrete Fourier transform and the notion of apparent distance of a code, originally defined for multivariate abelian codes."],"url":"http://arxiv.org/abs/2402.03965v1","category":"cs.IT"}
{"created":"2024-02-06 12:26:21","title":"Perpetual Future Contracts in Centralized and Decentralized Exchanges: Mechanism and Traders' Behavior","abstract":"This study presents a groundbreaking Systematization of Knowledge (SoK) initiative, focusing on an in-depth exploration of the dynamics and behavior of traders on perpetual future contracts across both centralized exchanges (CEXs), and decentralized exchanges (DEXs). We have refined the existing model for investigating traders' behavior in reaction to price volatility to create a new analytical framework specifically for these contract platforms, while also highlighting the role of blockchain technology in their application. Our research includes a comparative analysis of historical data from CEXs and a more extensive examination of complete transactional data on DEXs. On DEX of Virtual Automated Market Making (VAMM) Model, open interest on short and long positions exert effect on price volatility in opposite direction, attributable to VAMM's price formation mechanism. In the DEXs with Oracle Pricing Model, we observed a distinct asymmetry in trader behavior between buyers and sellers. Such asymmetry might stem from uninformed traders reacting more strongly to positive news than to negative, leading to a tendency to accumulate long positions. This study sheds light on the potential risks and advantages of using perpetual future contracts within the DeFi space while provides mathematical basis and empirical insights based on which future theoretical works can be configurated, offering crucial insights into the rapidly evolving world of blockchain-based financial instruments.","sentences":["This study presents a groundbreaking Systematization of Knowledge (SoK) initiative, focusing on an in-depth exploration of the dynamics and behavior of traders on perpetual future contracts across both centralized exchanges (CEXs), and decentralized exchanges (DEXs).","We have refined the existing model for investigating traders' behavior in reaction to price volatility to create a new analytical framework specifically for these contract platforms, while also highlighting the role of blockchain technology in their application.","Our research includes a comparative analysis of historical data from CEXs and a more extensive examination of complete transactional data on DEXs.","On DEX of Virtual Automated Market Making (VAMM) Model, open interest on short and long positions exert effect on price volatility in opposite direction, attributable to VAMM's price formation mechanism.","In the DEXs with Oracle Pricing Model, we observed a distinct asymmetry in trader behavior between buyers and sellers.","Such asymmetry might stem from uninformed traders reacting more strongly to positive news than to negative, leading to a tendency to accumulate long positions.","This study sheds light on the potential risks and advantages of using perpetual future contracts within the DeFi space while provides mathematical basis and empirical insights based on which future theoretical works can be configurated, offering crucial insights into the rapidly evolving world of blockchain-based financial instruments."],"url":"http://arxiv.org/abs/2402.03953v1","category":"q-fin.TR"}
{"created":"2024-02-06 12:23:59","title":"Spin-density-wave transition in double-layer nickelate La3Ni2O7","abstract":"Recently, a signature of high-temperature superconductivity above the liquid nitrogen temperature (77 K) was reported for La3Ni2O7 under pressure. This finding immediately stimulates intense interest in the possible high-Tc superconducting mechanism in double-layer nickelates. Interestingly, the pressure-dependent phase diagram inferred from transport measurements indicates that superconductivity under high pressure emerges from the suppression of a density-wave-like transition at ambient pressure, which is similar to high-temperature superconductors. Therefore, clarifying the exact nature of the density-wave-like transition is important for determining the mechanism of superconductivity in double-layer nickelates. Here, nuclear magnetic resonance (NMR) spectroscopy of 139La nuclei was performed to study the density-wave-like transition in a single crystal of La3Ni2O7. The temperature-dependent 139La NMR spectrum and nuclear spin-lattice relaxation rate (1/T1) provide unambiguous evidence for a spin-density-wave (SDW) transition with a transition temperature TSDW of ~ 150 K. Furthermore, the anisotropic splitting of the NMR spectrum suggests a possible double spin stripe with magnetic moments along the c axis. In addition, the present NMR measurements also revealed spatial inhomogeneity of magnetism due to inner apical oxygen vacancies. All these results will be helpful for building a connection between superconductivity and magnetic interactions in double-layer nickelates.","sentences":["Recently, a signature of high-temperature superconductivity above the liquid nitrogen temperature (77 K) was reported for La3Ni2O7 under pressure.","This finding immediately stimulates intense interest in the possible high-Tc superconducting mechanism in double-layer nickelates.","Interestingly, the pressure-dependent phase diagram inferred from transport measurements indicates that superconductivity under high pressure emerges from the suppression of a density-wave-like transition at ambient pressure, which is similar to high-temperature superconductors.","Therefore, clarifying the exact nature of the density-wave-like transition is important for determining the mechanism of superconductivity in double-layer nickelates.","Here, nuclear magnetic resonance (NMR) spectroscopy of 139La nuclei was performed to study the density-wave-like transition in a single crystal of La3Ni2O7.","The temperature-dependent 139La NMR spectrum and nuclear spin-lattice relaxation rate (1/T1) provide unambiguous evidence for a spin-density-wave (SDW) transition with a transition temperature TSDW of ~ 150 K. Furthermore, the anisotropic splitting of the NMR spectrum suggests a possible double spin stripe with magnetic moments along the c axis.","In addition, the present NMR measurements also revealed spatial inhomogeneity of magnetism due to inner apical oxygen vacancies.","All these results will be helpful for building a connection between superconductivity and magnetic interactions in double-layer nickelates."],"url":"http://arxiv.org/abs/2402.03952v1","category":"cond-mat.supr-con"}
{"created":"2024-02-06 12:19:08","title":"Wasserstein distributionally robust optimization and its tractable regularization formulations","abstract":"We study a variety of Wasserstein distributionally robust optimization (WDRO) problems where the distributions in the ambiguity set are chosen by constraining their Wasserstein discrepancies to the empirical distribution. Using the notion of weak Lipschitz property, we derive lower and upper bounds of the corresponding worst-case loss quantity and propose sufficient conditions under which this quantity coincides with its regularization scheme counterpart. Our constructive methodology and elementary analysis also directly characterize the closed-form of the approximate worst-case distribution. Extensive applications show that our theoretical results are applicable to various problems, including regression, classification and risk measure problems.","sentences":["We study a variety of Wasserstein distributionally robust optimization (WDRO) problems where the distributions in the ambiguity set are chosen by constraining their Wasserstein discrepancies to the empirical distribution.","Using the notion of weak Lipschitz property, we derive lower and upper bounds of the corresponding worst-case loss quantity and propose sufficient conditions under which this quantity coincides with its regularization scheme counterpart.","Our constructive methodology and elementary analysis also directly characterize the closed-form of the approximate worst-case distribution.","Extensive applications show that our theoretical results are applicable to various problems, including regression, classification and risk measure problems."],"url":"http://arxiv.org/abs/2402.03942v1","category":"math.OC"}
{"created":"2024-02-06 11:54:07","title":"Weibel- and non-resonant Whistler wave growth in an expanding plasma in a 1D simulation geometry","abstract":"Ablating a target with an ultraintense laser pulse can create a cloud of collisionless plasma. A density ramp forms, in which the plasma density decreases and the ion's mean speed increases with distance from the plasma source. Its width increases with time. Electrons lose energy in the ion's expansion direction, which gives them a temperature anisotropy. We study with one-dimensional particle-in-cell simulations the expansion of a dense plasma into a dilute one, yielding a density ramp similar to that in laser plasma experiments and a thermal-anisotropy-driven instability. Non-propagating Weibel-type modes grow in the simulation with no initial magnetic field. Their magnetic field diffuses across the the shock and expands upstream. Circularly polarized propagating Whistler waves grow in a second simulation, in which a magnetic field is aligned with the ion expansion direction. Both wave modes are driven by non-resonant instabilities, they have a similar exponential growth rates, and they can leave the density ramp and expand into the dilute plasma. Their large magnetic amplitude should make them detectable in experimental settings.","sentences":["Ablating a target with an ultraintense laser pulse can create a cloud of collisionless plasma.","A density ramp forms, in which the plasma density decreases and the ion's mean speed increases with distance from the plasma source.","Its width increases with time.","Electrons lose energy in the ion's expansion direction, which gives them a temperature anisotropy.","We study with one-dimensional particle-in-cell simulations the expansion of a dense plasma into a dilute one, yielding a density ramp similar to that in laser plasma experiments and a thermal-anisotropy-driven instability.","Non-propagating Weibel-type modes grow in the simulation with no initial magnetic field.","Their magnetic field diffuses across the the shock and expands upstream.","Circularly polarized propagating Whistler waves grow in a second simulation, in which a magnetic field is aligned with the ion expansion direction.","Both wave modes are driven by non-resonant instabilities, they have a similar exponential growth rates, and they can leave the density ramp and expand into the dilute plasma.","Their large magnetic amplitude should make them detectable in experimental settings."],"url":"http://arxiv.org/abs/2402.03925v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 11:28:08","title":"Terahertz plasmonic resonances in coplanar graphene nanoribbon structures","abstract":"We analyze plasmonic oscillations in the coplanar graphene nanoribbon (GNR) structures induced by the applied terahertz (THz) signals and calculate the GNR impedance. The plasmonic oscillations in the CNR structures are associated with the electron and hole inductances and the lateral inter-CNR capacitance. A relatively low inter-GNR capacitance enables the resonant excitation of the THz plasmonic oscillations in the CNR structures with long GNRs. The GNR structures under consideration can be used in different THz devices as the resonant structures incorporated in THz detectors, THz sources using resonant-tunneling diodes, photomixers, and surface acoustic wave sensors.","sentences":["We analyze plasmonic oscillations in the coplanar graphene nanoribbon (GNR) structures induced by the applied terahertz (THz) signals and calculate the GNR impedance.","The plasmonic oscillations in the CNR structures are associated with the electron and hole inductances and the lateral inter-CNR capacitance.","A relatively low inter-GNR capacitance enables the resonant excitation of the THz plasmonic oscillations in the CNR structures with long GNRs.","The GNR structures under consideration can be used in different THz devices as the resonant structures incorporated in THz detectors, THz sources using resonant-tunneling diodes, photomixers, and surface acoustic wave sensors."],"url":"http://arxiv.org/abs/2402.03912v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 11:22:39","title":"Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A Network Perspective","abstract":"Studying acquisitions offers invaluable insights into startup trends, aiding informed investment decisions for businesses. However, the scarcity of studies in this domain prompts our focus on shedding light in this area. Employing Crunchbase data, our study delves into the global network of company acquisitions using diverse network analysis techniques. Our findings unveil an acquisition network characterized by a primarily sparse structure comprising localized dense connections. We reveal a prevalent tendency among organizations to acquire companies within their own country and industry. Furthermore, our temporal analysis indicates a growth in network communities over time, accompanied by a trend toward a sparser network. Through centrality metrics computation in the cross-city acquisition network, we identify New York, London, and San Francisco as pivotal and central hubs in the global economic landscape. Finally, we show that the United States, United Kingdom, and Germany are predominant countries in international acquisitions.","sentences":["Studying acquisitions offers invaluable insights into startup trends, aiding informed investment decisions for businesses.","However, the scarcity of studies in this domain prompts our focus on shedding light in this area.","Employing Crunchbase data, our study delves into the global network of company acquisitions using diverse network analysis techniques.","Our findings unveil an acquisition network characterized by a primarily sparse structure comprising localized dense connections.","We reveal a prevalent tendency among organizations to acquire companies within their own country and industry.","Furthermore, our temporal analysis indicates a growth in network communities over time, accompanied by a trend toward a sparser network.","Through centrality metrics computation in the cross-city acquisition network, we identify New York, London, and San Francisco as pivotal and central hubs in the global economic landscape.","Finally, we show that the United States, United Kingdom, and Germany are predominant countries in international acquisitions."],"url":"http://arxiv.org/abs/2402.03910v1","category":"cs.SI"}
{"created":"2024-02-06 10:22:29","title":"B rare decays: theory overview","abstract":"In this proceeding we will review the current theoretical status of rare $B$ decays. These decays are indeed excellent indirect probes for New Physics searches, and in the current situation where no new states have been directly observed at collider, they provide a fundamental and alternative approach in the quest for Physics beyond the Standard Model. We will focus on the following classes of decays: $B_q\\to\\tau\\nu$, $B_q\\to\\mu\\mu$, $B\\to K^{(*)}\\nu\\bar\\nu$, $B\\to K^{(*)}\\ell\\ell$, $B_s\\to\\phi\\ell\\ell$ and $b\\to s\\gamma$. The most updated Standard Model predictions will be provided, highlighting which are the main sources of uncertainty, and what is the possibility for New Physics effects when confronting the theory numbers to current experimental results.","sentences":["In this proceeding we will review the current theoretical status of rare $B$ decays.","These decays are indeed excellent indirect probes for New Physics searches, and in the current situation where no new states have been directly observed at collider, they provide a fundamental and alternative approach in the quest for Physics beyond the Standard Model.","We will focus on the following classes of decays: $B_q\\to\\tau\\nu$, $B_q\\to\\mu\\mu$, $B\\to K^{(*)}\\nu\\bar\\nu$, $B\\to K^{(*)}\\ell\\ell$, $B_s\\to\\phi\\ell\\ell$ and $b\\to","s\\gamma$. The most updated Standard Model predictions will be provided, highlighting which are the main sources of uncertainty, and what is the possibility for New Physics effects when confronting the theory numbers to current experimental results."],"url":"http://arxiv.org/abs/2402.03863v1","category":"hep-ph"}
{"created":"2024-02-06 10:11:09","title":"Steady periodic hydroelastic waves in polar regions","abstract":"We construct two-dimensional steady periodic hydroelastic waves with vorticity that propagate on water of finite depth under a deformable floating elastic plate which is modeled by using the special Cosserat theory of hyperelastic shells satisfying Kirchhoff's hypothesis. This is achieved by providing necessary and sufficient condition for local bifurcation from the trivial branch of laminar flow solutions.","sentences":["We construct two-dimensional steady periodic hydroelastic waves with vorticity that propagate on water of finite depth under a deformable floating elastic plate which is modeled by using the special Cosserat theory of hyperelastic shells satisfying Kirchhoff's hypothesis.","This is achieved by providing necessary and sufficient condition for local bifurcation from the trivial branch of laminar flow solutions."],"url":"http://arxiv.org/abs/2402.03857v1","category":"math.AP"}
{"created":"2024-02-06 09:14:32","title":"Optical absorption and photoluminescence of single layer boron nitride from a first principles cumulant approach","abstract":"The photoluminescence spectrum of a single-layer boron nitride remains elusive, marked by enigmatic satellites that hint at a significant but unidentified exciton-phonon coupling.Here, by employing a first principles approach based on the many-body cumulant expansion of the charge response, we calculate the optical absorption and photoluminescence of a single layer boron nitride. We identify the specific exciton-phonon scattering channels and unravel their impact on the optical absorption and photoluminescence spectra thereby providing an interpretation of the experimental features. Finally, we show that, even in a strongly polar material such as h-BN monolayer, the electron-hole interaction responsible for the excitonic effect results in the cancellation of the Fr\\\"olich interaction at small phonon momenta. This effect is captured only if the invariance of the exciton-phonon matrix elements under unitary transformations in the Bloch functions manifold is preserved in the calculation.","sentences":["The photoluminescence spectrum of a single-layer boron nitride remains elusive, marked by enigmatic satellites that hint at a significant but unidentified exciton-phonon coupling.","Here, by employing a first principles approach based on the many-body cumulant expansion of the charge response, we calculate the optical absorption and photoluminescence of a single layer boron nitride.","We identify the specific exciton-phonon scattering channels and unravel their impact on the optical absorption and photoluminescence spectra thereby providing an interpretation of the experimental features.","Finally, we show that, even in a strongly polar material such as h-BN monolayer, the electron-hole interaction responsible for the excitonic effect results in the cancellation of the Fr\\\"olich interaction at small phonon momenta.","This effect is captured only if the invariance of the exciton-phonon matrix elements under unitary transformations in the Bloch functions manifold is preserved in the calculation."],"url":"http://arxiv.org/abs/2402.03826v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 08:27:06","title":"Simulating the epoch of Helium Reionization in photon-conserving semi-numerical code SCRIPT","abstract":"The reionization of the second electron of helium (HeII) leaves important imprints on the thermal and ionization state of the intergalactic medium (IGM). Observational evidence suggests that HeII reionization ended at $z \\simeq 3$ due to ionizing photons emitted predominantly by quasars. We present efficient semi-numerical simulations of helium reionization in a $230 \\ \\mathrm{h^{-1}~Mpc}$ box, that takes into account the spatial patchiness of reionization coupled with photoheating of the IGM. Dark matter haloes are assigned quasars using empirical measurements of the quasar luminosity function, assuming a universal quasar lifetime consistent with duty cycle values inferred from measurements of the quasar clustering. The ionizing photon field from quasars is then included in the semi-numerical Code for ReionIzation with PhoTon conservation (SCRIPT), which was originally developed for modeling hydrogen reionization. In this work, we make appropriate modifications to SCRIPT for modeling inhomogenous HeII reionization and the corresponding thermal history of the IGM is modelled via a subgrid prescription. Our model has three main free parameters i.e. the global clumping factor $\\mathcal{C}_{HeIII}$, the temperature increase due to photoheating $T^{re}_{He}$ and the quasar spectral energy distribution (SED) index, $\\alpha_{UV}$. Our \\textit{fiducial} model with $\\mathcal{C}_{HeIII}=15.6$ and $T^{re}_{He} \\sim 6000 \\ K$ gives reasonable values for the empirical measurements of the temperature density equation of state at these redshifts, assuming that quasars brighter than $\\mr{M_{1450}}<-21$ and having $\\alpha_{UV}=1.7$ contribute to HeII reionization. The efficiency of our code shows promising prospects for performing parameter estimation in future, for models of HeII reionization using observations of the Ly$\\alpha$ forest.","sentences":["The reionization of the second electron of helium (HeII) leaves important imprints on the thermal and ionization state of the intergalactic medium (IGM).","Observational evidence suggests that HeII reionization ended at $z \\simeq 3$ due to ionizing photons emitted predominantly by quasars.","We present efficient semi-numerical simulations of helium reionization in a $230 \\ \\mathrm{h^{-1}~Mpc}$ box, that takes into account the spatial patchiness of reionization coupled with photoheating of the IGM.","Dark matter haloes are assigned quasars using empirical measurements of the quasar luminosity function, assuming a universal quasar lifetime consistent with duty cycle values inferred from measurements of the quasar clustering.","The ionizing photon field from quasars is then included in the semi-numerical Code for ReionIzation with PhoTon conservation (SCRIPT), which was originally developed for modeling hydrogen reionization.","In this work, we make appropriate modifications to SCRIPT for modeling inhomogenous HeII reionization and the corresponding thermal history of the IGM is modelled via a subgrid prescription.","Our model has three main free parameters i.e. the global clumping factor $\\mathcal{C}_{HeIII}$, the temperature increase due to photoheating $T^{re}_{He}$ and the quasar spectral energy distribution (SED) index, $\\alpha_{UV}$. Our \\textit{fiducial} model with $\\mathcal{C}_{HeIII}=15.6$ and $T^{re}_{He} \\sim 6000 \\ K$ gives reasonable values for the empirical measurements of the temperature density equation of state at these redshifts, assuming that quasars brighter than $\\mr{M_{1450}}<-21$ and having $\\alpha_{UV}=1.7$ contribute to HeII reionization.","The efficiency of our code shows promising prospects for performing parameter estimation in future, for models of HeII reionization using observations of the Ly$\\alpha$ forest."],"url":"http://arxiv.org/abs/2402.03794v1","category":"astro-ph.CO"}
{"created":"2024-02-06 07:26:44","title":"Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution","abstract":"In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standard RL in Markov Decision Processes (MDPs). To effectively explore the reward distributions within the bagged rewards, we propose a Transformer-based reward model, the Reward Bag Transformer (RBT), which uses the self-attention mechanism for interpreting the contextual nuances and temporal dependencies within each bag. Extensive experimental analyses demonstrate the superiority of our method, particularly in its ability to mimic the original MDP's reward distribution, highlighting its proficiency in contextual understanding and adaptability to environmental dynamics.","sentences":["In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy.","However, in many real-world applications, the instant reward signals are not obtainable by the agent.","Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory.","In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions.","To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained.","We provide the theoretical study to establish the connection between RLBR and standard RL in Markov Decision Processes (MDPs).","To effectively explore the reward distributions within the bagged rewards, we propose a Transformer-based reward model, the Reward Bag Transformer (RBT), which uses the self-attention mechanism for interpreting the contextual nuances and temporal dependencies within each bag.","Extensive experimental analyses demonstrate the superiority of our method, particularly in its ability to mimic the original MDP's reward distribution, highlighting its proficiency in contextual understanding and adaptability to environmental dynamics."],"url":"http://arxiv.org/abs/2402.03771v1","category":"cs.LG"}
{"created":"2024-02-06 07:04:35","title":"Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery","abstract":"Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection enables visualization of differences between tissues that are not distinguishable to humans. This augmentation can maximize brain tumor resection, improving patient outcomes. However, much of the processing in HSI uses simplified linear methods that are unable to capture the non-linear, wavelength-dependent phenomena that must be modeled for accurate recovery of fluorophore abundances. We therefore propose two deep learning models for correction and unmixing, which can account for the nonlinear effects and produce more accurate estimates of abundances. Both models use an autoencoder-like architecture to process the captured spectra. One is trained with protoporphyrin IX (PpIX) concentration labels. The other undergoes semi-supervised training, first learning hyperspectral unmixing self-supervised and then learning to correct fluorescence emission spectra for heterogeneous optical and geometric properties using a reference white-light reflectance spectrum in a few-shot manner. The models were evaluated against phantom and pig brain data with known PpIX concentration; the supervised model achieved Pearson correlation coefficients (R values) between the known and computed PpIX concentrations of 0.997 and 0.990, respectively, whereas the classical approach achieved only 0.93 and 0.82. The semi-supervised approach's R values were 0.98 and 0.91, respectively. On human data, the semi-supervised model gives qualitatively more realistic results than the classical method, better removing bright spots of specular reflectance and reducing the variance in PpIX abundance over biopsies that should be relatively homogeneous. These results show promise for using deep learning to improve HSI in fluorescence-guided neurosurgery.","sentences":["Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection enables visualization of differences between tissues that are not distinguishable to humans.","This augmentation can maximize brain tumor resection, improving patient outcomes.","However, much of the processing in HSI uses simplified linear methods that are unable to capture the non-linear, wavelength-dependent phenomena that must be modeled for accurate recovery of fluorophore abundances.","We therefore propose two deep learning models for correction and unmixing, which can account for the nonlinear effects and produce more accurate estimates of abundances.","Both models use an autoencoder-like architecture to process the captured spectra.","One is trained with protoporphyrin IX (PpIX) concentration labels.","The other undergoes semi-supervised training, first learning hyperspectral unmixing self-supervised and then learning to correct fluorescence emission spectra for heterogeneous optical and geometric properties using a reference white-light reflectance spectrum in a few-shot manner.","The models were evaluated against phantom and pig brain data with known PpIX concentration; the supervised model achieved Pearson correlation coefficients (R values) between the known and computed PpIX concentrations of 0.997 and 0.990, respectively, whereas the classical approach achieved only 0.93 and 0.82.","The semi-supervised approach's R values were 0.98 and 0.91, respectively.","On human data, the semi-supervised model gives qualitatively more realistic results than the classical method, better removing bright spots of specular reflectance and reducing the variance in PpIX abundance over biopsies that should be relatively homogeneous.","These results show promise for using deep learning to improve HSI in fluorescence-guided neurosurgery."],"url":"http://arxiv.org/abs/2402.03761v1","category":"eess.IV"}
{"created":"2024-02-06 06:51:15","title":"Amoeboid movement utilizes the shape coupled bifurcation of an active droplet to boost ballistic motion","abstract":"One of the essential functions of living organisms is spontaneous migration through the deformation of their body, such as crawling, swimming, and walking. Depending on the size of the object, the efficient migratory mode should be altered because the contribution from the inertial and frictional forces acting on the object switches. Although the self-propelling motion characterizing active matter has been extensively studied, it is still elusive how a living cell utilizes the mode switching of the self-propulsion. Here, we studied the migration dynamics of amoeboid movement of free-living amoeba, Amoeba proteus, for starved and vegetative phases, as typified by dynamic and stationary states, respectively. Fourier-mode analysis on the cell shape and migration velocity extracted two characteristic migration modes, which makes a coexistence of amoeboid-swimmer like random motion and the active-droplet like ballistic motion. While the amoeboid-swimmer mode governs random motion, the active-droplet mode performs non-negligible contribution on the migration strength. By employing the symmetry argument of the active-droplet, we discover the supercritical pitchfork bifurcation of the migration velocity due to the symmetry breaking of the cell shape represents the switching manner from the motionless state to the random and the ballistic motions. Our results suggest that sub-mm sized A. proteus utilizes both shape oscillatory migration of deformed-swimmer driven by surface wave and convection based mass transfer, called blebbing, as like as cm-sized active droplet to optimize the movement efficiency.","sentences":["One of the essential functions of living organisms is spontaneous migration through the deformation of their body, such as crawling, swimming, and walking.","Depending on the size of the object, the efficient migratory mode should be altered because the contribution from the inertial and frictional forces acting on the object switches.","Although the self-propelling motion characterizing active matter has been extensively studied, it is still elusive how a living cell utilizes the mode switching of the self-propulsion.","Here, we studied the migration dynamics of amoeboid movement of free-living amoeba, Amoeba proteus, for starved and vegetative phases, as typified by dynamic and stationary states, respectively.","Fourier-mode analysis on the cell shape and migration velocity extracted two characteristic migration modes, which makes a coexistence of amoeboid-swimmer like random motion and the active-droplet like ballistic motion.","While the amoeboid-swimmer mode governs random motion, the active-droplet mode performs non-negligible contribution on the migration strength.","By employing the symmetry argument of the active-droplet, we discover the supercritical pitchfork bifurcation of the migration velocity due to the symmetry breaking of the cell shape represents the switching manner from the motionless state to the random and the ballistic motions.","Our results suggest that sub-mm sized A. proteus utilizes both shape oscillatory migration of deformed-swimmer driven by surface wave and convection based mass transfer, called blebbing, as like as cm-sized active droplet to optimize the movement efficiency."],"url":"http://arxiv.org/abs/2402.03759v1","category":"cond-mat.soft"}
{"created":"2024-02-06 06:26:47","title":"A unified approach to fluctuations of spectral statistics of generalized patterned random matrices","abstract":"An $N \\times N$ generalized patterned random matrix is a symmetric matrix defined as $A=(x_{L(i,j)}\\mathbf{1}_{\\Delta}(i,j))$, where $\\{x_k; k \\in \\mathbb{Z}^d\\}$ is known as the input sequence, $L:\\{1,2,\\ldots, N\\}^2 \\rightarrow \\mathbb{Z}^d$ is known as the link function and $\\Delta \\subseteq \\{1,2,\\ldots , N\\}^2$. In 2008, Bose and Sen showed that under some restrictions on the link function $L$ and for $\\Delta = \\{1,2,\\ldots , N\\}^2$, the corresponding patterned matrices always have a sub-Gaussian limiting spectral distribution.   Let $N: \\mathbb{N} \\rightarrow \\mathbb{N}$ be a strictly increasing function and $A_{n}=\\left(x_{L(i, j)} \\mathbf{1}_{\\Delta}(i,j) \\right)_{i,j=1}^{N(n)}$ be a sequence of generalized patterned matrices. We consider the LES of $A_n/\\sqrt{N(n)}$ for the test function $\\phi(x)=x^p$, given by $$ \\frac{1}{\\sqrt{N(n)}} \\sum_{i=1}^{N(n)} \\lambda_i^p=\\eta_p, \\mbox{ say}, $$ where $p$ is a positive integer and $\\lambda_i$'s are the eigenvalues of $A_n/\\sqrt{N(n)}$. Under some restrictions on $L$ and some moment assumptions on input entries, we show that when $p$ is even, $(\\eta_p - \\mathbb{E} [\\eta_p])$ converges in distribution either to a normal distribution or to the degenerate distribution at zero. We show that under further assumptions on $L$, the limit is always a normal distribution.   For odd degree monomial test functions, we derive the limiting moments of $\\eta_p$ and show that the LES may not converge to a Gaussian distribution. We also study the LES for independent Brownian motion entries. We show that under some assumption on link function, and that if the limiting moment sequence of the LES of the generalized patterned random matrix with independent standard Gaussian entries is M-determinate, then the LES for Brownian motion entries converge to a deterministic real-valued process.","sentences":["An $N \\times N$ generalized patterned random matrix is a symmetric matrix defined as $A=(x_{L(i,j)}\\mathbf{1}_{\\Delta}(i,j))$, where $\\{x_k; k \\in \\mathbb{Z}^d\\}$ is known as the input sequence, $L:\\{1,2,\\ldots, N\\}^2 \\rightarrow \\mathbb{Z}^d$ is known as the link function and $\\Delta \\subseteq \\{1,2,\\ldots , N\\}^2$.","In 2008, Bose and Sen showed that under some restrictions on the link function $L$ and for $\\Delta = \\{1,2,\\ldots , N\\}^2$, the corresponding patterned matrices always have a sub-Gaussian limiting spectral distribution.   ","Let $N: \\mathbb{N} \\rightarrow \\mathbb{N}$ be a strictly increasing function and $A_{n}=\\left(x_{L(i, j)} \\mathbf{1}_{\\Delta}(i,j) \\right)_{i,j=1}^{N(n)}$ be a sequence of generalized patterned matrices.","We consider the LES of $A_n/\\sqrt{N(n)}$ for the test function $\\phi(x)=x^p$, given by $$ \\frac{1}{\\sqrt{N(n)}} \\sum_{i=1}^{N(n)} \\lambda_i^p=\\eta_p, \\mbox{ say}, $$ where $p$ is a positive integer and $\\lambda_i$'s are the eigenvalues of $A_n/\\sqrt{N(n)}$. Under some restrictions on $L$ and some moment assumptions on input entries, we show that when $p$ is even, $(\\eta_p - \\mathbb{E} [\\eta_p])$ converges in distribution either to a normal distribution or to the degenerate distribution at zero.","We show that under further assumptions on $L$, the limit is always a normal distribution.   ","For odd degree monomial test functions, we derive the limiting moments of $\\eta_p$ and show that the LES may not converge to a Gaussian distribution.","We also study the LES for independent Brownian motion entries.","We show that under some assumption on link function, and that if the limiting moment sequence of the LES of the generalized patterned random matrix with independent standard Gaussian entries is M-determinate, then the LES for Brownian motion entries converge to a deterministic real-valued process."],"url":"http://arxiv.org/abs/2402.03745v1","category":"math.PR"}
{"created":"2024-02-06 06:23:12","title":"INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection","abstract":"Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates for halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular, a simple yet effective \\textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.","sentences":["Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs.","Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure.","Thus, we propose to explore the dense semantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates for halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}).","In particular, a simple yet effective \\textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space.","Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations.","Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal."],"url":"http://arxiv.org/abs/2402.03744v1","category":"cs.CL"}
{"created":"2024-02-06 05:59:40","title":"Cooling-Heating Properties of the FRW Universe in Gravity with a Generalized Conformal Scalar Field","abstract":"In this paper, within the framework of modified gravity involving a conformal scalar field, we investigate the Joule-Thomson expansion of the FRW universe to identify cooling and heating regions. Notably, we observe that the Joule-Thomson coefficient, denoted as $\\mu$, diverges at $R_A=\\sqrt{-2\\alpha}$ when $\\alpha<0$, aligning with the thermodynamic singularity of the FRW universe. Additionally, we determine the inversion temperature and inversion pressure for the FRW universe, and illustrate the characteristics of inversion curves and isenthalpic curves in the $T$-$P$ plane. We compare these findings with results obtained under Einstein gravity, discussing the influence of the modification term on the cooling and heating properties of the FRW universe. This work contributes significantly to a deeper understanding of the formation of cooling and heating regions within the FRW universe, thereby advancing our comprehension of the physical mechanisms that govern the expansion of our universe.","sentences":["In this paper, within the framework of modified gravity involving a conformal scalar field, we investigate the Joule-Thomson expansion of the FRW universe to identify cooling and heating regions.","Notably, we observe that the Joule-Thomson coefficient, denoted as $\\mu$, diverges at $R_A=\\sqrt{-2\\alpha}$ when $\\alpha<0$, aligning with the thermodynamic singularity of the FRW universe.","Additionally, we determine the inversion temperature and inversion pressure for the FRW universe, and illustrate the characteristics of inversion curves and isenthalpic curves in the $T$-$P$ plane.","We compare these findings with results obtained under Einstein gravity, discussing the influence of the modification term on the cooling and heating properties of the FRW universe.","This work contributes significantly to a deeper understanding of the formation of cooling and heating regions within the FRW universe, thereby advancing our comprehension of the physical mechanisms that govern the expansion of our universe."],"url":"http://arxiv.org/abs/2402.03733v1","category":"gr-qc"}
{"created":"2024-02-06 05:56:54","title":"On a positive-preserving, energy-stable numerical scheme to mass-action kinetics with detailed balance","abstract":"In this paper, we provide a detailed theoretical analysis of the numerical scheme introduced in J. Comput. Phys. 436 (2021) 110253 for the reaction kinetics of a class of chemical reaction networks that satisfies detailed balance condition. In contrast to conventional numerical approximations, which are typically constructed based on ordinary differential equations (ODEs) for the concentrations of all involved species, the scheme is developed using the equations of reaction trajectories, which can be viewed as a generalized gradient flow of physically relevant free energy. The unique solvability, positivity-preserving, and energy-stable properties are proved for the general case involving multiple reactions, under a mild condition on the stoichiometric matrix.","sentences":["In this paper, we provide a detailed theoretical analysis of the numerical scheme introduced in J. Comput.","Phys. 436 (2021) 110253 for the reaction kinetics of a class of chemical reaction networks that satisfies detailed balance condition.","In contrast to conventional numerical approximations, which are typically constructed based on ordinary differential equations (ODEs) for the concentrations of all involved species, the scheme is developed using the equations of reaction trajectories, which can be viewed as a generalized gradient flow of physically relevant free energy.","The unique solvability, positivity-preserving, and energy-stable properties are proved for the general case involving multiple reactions, under a mild condition on the stoichiometric matrix."],"url":"http://arxiv.org/abs/2402.03731v1","category":"math.NA"}
{"created":"2024-02-06 05:42:27","title":"Statistical Test for Anomaly Detections by Variational Auto-Encoders","abstract":"In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is theoretically guaranteed in finite samples. To demonstrate the validity and effectiveness of the proposed VAE-AD Test, numerical experiments on artificial data and applications to brain image analysis are conducted.","sentences":["In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE).","Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research.","However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies.","In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing.","Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values.","This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level.","Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is theoretically guaranteed in finite samples.","To demonstrate the validity and effectiveness of the proposed VAE-AD Test, numerical experiments on artificial data and applications to brain image analysis are conducted."],"url":"http://arxiv.org/abs/2402.03724v1","category":"stat.ML"}
{"created":"2024-02-06 05:11:46","title":"Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification","abstract":"Current state-of-the-art Video-based Person Re-Identification (Re-ID) primarily relies on appearance features extracted by deep learning models. These methods are not applicable for long-term analysis in real-world scenarios where persons have changed clothes, making appearance information unreliable. In this work, we deal with the practical problem of Video-based Cloth-Changing Person Re-ID (VCCRe-ID) by proposing \"Attention-based Shape and Gait Representations Learning\" (ASGL) for VCCRe-ID. Our ASGL framework improves Re-ID performance under clothing variations by learning clothing-invariant gait cues using a Spatial-Temporal Graph Attention Network (ST-GAT). Given the 3D-skeleton-based spatial-temporal graph, our proposed ST-GAT comprises multi-head attention modules, which are able to enhance the robustness of gait embeddings under viewpoint changes and occlusions. The ST-GAT amplifies the important motion ranges and reduces the influence of noisy poses. Then, the multi-head learning module effectively reserves beneficial local temporal dynamics of movement. We also boost discriminative power of person representations by learning body shape cues using a GAT. Experiments on two large-scale VCCRe-ID datasets demonstrate that our proposed framework outperforms state-of-the-art methods by 12.2% in rank-1 accuracy and 7.0% in mAP.","sentences":["Current state-of-the-art Video-based Person Re-Identification (Re-ID) primarily relies on appearance features extracted by deep learning models.","These methods are not applicable for long-term analysis in real-world scenarios where persons have changed clothes, making appearance information unreliable.","In this work, we deal with the practical problem of Video-based Cloth-Changing Person Re-ID (VCCRe-ID) by proposing \"Attention-based Shape and Gait Representations Learning\" (ASGL) for VCCRe-ID.","Our ASGL framework improves Re-ID performance under clothing variations by learning clothing-invariant gait cues using a Spatial-Temporal Graph Attention Network (ST-GAT).","Given the 3D-skeleton-based spatial-temporal graph, our proposed ST-GAT comprises multi-head attention modules, which are able to enhance the robustness of gait embeddings under viewpoint changes and occlusions.","The ST-GAT amplifies the important motion ranges and reduces the influence of noisy poses.","Then, the multi-head learning module effectively reserves beneficial local temporal dynamics of movement.","We also boost discriminative power of person representations by learning body shape cues using a GAT.","Experiments on two large-scale VCCRe-ID datasets demonstrate that our proposed framework outperforms state-of-the-art methods by 12.2% in rank-1 accuracy and 7.0% in mAP."],"url":"http://arxiv.org/abs/2402.03716v1","category":"cs.CV"}
{"created":"2024-02-06 05:04:24","title":"Adaptive Backstepping Control of a Bicopter in Pure Feedback Form with Dynamic Extension","abstract":"This paper presents a model-based, adaptive, nonlinear controller for the bicopter stabilization and trajectory-tracking problem. The nonlinear controller is designed using the backstepping technique. Due to the non-invertibility of the input map, the bicopter system is first dynamically extended. However, the resulting dynamically extended system is in the pure feedback form with the uncertainty appearing in the input map. The adaptive backstepping technique is then extended and applied to design the controller. The proposed controller is validated in simulation for a smooth and nonsmooth trajectory-tracking problem.","sentences":["This paper presents a model-based, adaptive, nonlinear controller for the bicopter stabilization and trajectory-tracking problem.","The nonlinear controller is designed using the backstepping technique.","Due to the non-invertibility of the input map, the bicopter system is first dynamically extended.","However, the resulting dynamically extended system is in the pure feedback form with the uncertainty appearing in the input map.","The adaptive backstepping technique is then extended and applied to design the controller.","The proposed controller is validated in simulation for a smooth and nonsmooth trajectory-tracking problem."],"url":"http://arxiv.org/abs/2402.03709v1","category":"math.OC"}
{"created":"2024-02-06 04:47:58","title":"WhisperFuzz: White-Box Fuzzing for Detecting and Locating Timing Vulnerabilities in Processors","abstract":"Timing vulnerabilities in processors have emerged as a potent threat. As processors are the foundation of any computing system, identifying these flaws is imperative. Recently fuzzing techniques, traditionally used for detecting software vulnerabilities, have shown promising results for uncovering vulnerabilities in large-scale hardware designs, such as processors. Researchers have adapted black-box or grey-box fuzzing to detect timing vulnerabilities in processors. However, they cannot identify the locations or root causes of these timing vulnerabilities, nor do they provide coverage feedback to enable the designer's confidence in the processor's security.   To address the deficiencies of the existing fuzzers, we present WhisperFuzz--the first white-box fuzzer with static analysis--aiming to detect and locate timing vulnerabilities in processors and evaluate the coverage of microarchitectural timing behaviors. WhisperFuzz uses the fundamental nature of processors' timing behaviors, microarchitectural state transitions, to localize timing vulnerabilities. WhisperFuzz automatically extracts microarchitectural state transitions from a processor design at the register-transfer level (RTL) and instruments the design to monitor the state transitions as coverage. Moreover, WhisperFuzz measures the time a design-under-test (DUT) takes to process tests, identifying any minor, abnormal variations that may hint at a timing vulnerability. WhisperFuzz detects 12 new timing vulnerabilities across advanced open-sourced RISC-V processors: BOOM, Rocket Core, and CVA6. Eight of these violate the zero latency requirements of the Zkt extension and are considered serious security vulnerabilities. Moreover, WhisperFuzz also pinpoints the locations of the new and the existing vulnerabilities.","sentences":["Timing vulnerabilities in processors have emerged as a potent threat.","As processors are the foundation of any computing system, identifying these flaws is imperative.","Recently fuzzing techniques, traditionally used for detecting software vulnerabilities, have shown promising results for uncovering vulnerabilities in large-scale hardware designs, such as processors.","Researchers have adapted black-box or grey-box fuzzing to detect timing vulnerabilities in processors.","However, they cannot identify the locations or root causes of these timing vulnerabilities, nor do they provide coverage feedback to enable the designer's confidence in the processor's security.   ","To address the deficiencies of the existing fuzzers, we present WhisperFuzz--the first white-box fuzzer with static analysis--aiming to detect and locate timing vulnerabilities in processors and evaluate the coverage of microarchitectural timing behaviors.","WhisperFuzz uses the fundamental nature of processors' timing behaviors, microarchitectural state transitions, to localize timing vulnerabilities.","WhisperFuzz automatically extracts microarchitectural state transitions from a processor design at the register-transfer level (RTL) and instruments the design to monitor the state transitions as coverage.","Moreover, WhisperFuzz measures the time a design-under-test (DUT) takes to process tests, identifying any minor, abnormal variations that may hint at a timing vulnerability.","WhisperFuzz detects 12 new timing vulnerabilities across advanced open-sourced RISC-V processors: BOOM, Rocket Core, and CVA6.","Eight of these violate the zero latency requirements of the Zkt extension and are considered serious security vulnerabilities.","Moreover, WhisperFuzz also pinpoints the locations of the new and the existing vulnerabilities."],"url":"http://arxiv.org/abs/2402.03704v1","category":"cs.CR"}
{"created":"2024-02-06 04:25:07","title":"3Doodle: Compact Abstraction of Objects with 3D Strokes","abstract":"While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations. Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object. Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches. We express 2D sketches as a union of view-independent and view-dependent components. 3D cubic B ezier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints. Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner. The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects. We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches.","sentences":["While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations.","Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes.","We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object.","Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches.","We express 2D sketches as a union of view-independent and view-dependent components.","3D cubic B ezier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints.","Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner.","The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects.","We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches."],"url":"http://arxiv.org/abs/2402.03690v1","category":"cs.CV"}
{"created":"2024-02-06 03:58:41","title":"Physics-based Modeling of Pulse and Relaxation of High-rate Li/CF$_{x}$-SVO batteries in Implantable Medical Devices","abstract":"We present a physics-based model that accurately predicts the performance of Medtronic's implantable medical device battery lithium/carbon monofluoride (CF$_x$) - silver vanadium oxide (SVO) under both low-rate background monitoring and high-rate pulsing currents. The distinct properties of multiple active materials are reflected by parameterizing their thermodynamics, kinetics, and mass transport properties separately. Diffusion limitations of Li$^+$ in SVO are used to explain cell voltage transient behavior during pulse and post-pulse relaxation. We also introduce change in cathode electronic conductivity, Li metal anode surface morphology, and film resistance buildup to capture evolution of cell internal resistance throughout multi-year electrical tests. We share our insights on how the Li$^+$ redistribution process between active materials can restore pulse capability of the hybrid electrode, allow CF$_x$ to indirectly contribute to capacity release during pulsing, and affect the operation protocols and design principles of batteries with other hybrid electrodes. We also discuss additional complexities in porous electrode model parameterization and electrochemical characterization techniques due to parallel reactions and solid diffusion pathways across active materials. We hope our models implemented in the Hybrid Multiphase Porous Electrode Theory (Hybrid-MPET) framework can complement future experimental research and accelerate development of multi-active material electrodes with targeted performance.","sentences":["We present a physics-based model that accurately predicts the performance of Medtronic's implantable medical device battery lithium/carbon monofluoride (CF$_x$) - silver vanadium oxide (SVO) under both low-rate background monitoring and high-rate pulsing currents.","The distinct properties of multiple active materials are reflected by parameterizing their thermodynamics, kinetics, and mass transport properties separately.","Diffusion limitations of Li$^+$ in SVO are used to explain cell voltage transient behavior during pulse and post-pulse relaxation.","We also introduce change in cathode electronic conductivity, Li metal anode surface morphology, and film resistance buildup to capture evolution of cell internal resistance throughout multi-year electrical tests.","We share our insights on how the Li$^+$ redistribution process between active materials can restore pulse capability of the hybrid electrode, allow CF$_x$ to indirectly contribute to capacity release during pulsing, and affect the operation protocols and design principles of batteries with other hybrid electrodes.","We also discuss additional complexities in porous electrode model parameterization and electrochemical characterization techniques due to parallel reactions and solid diffusion pathways across active materials.","We hope our models implemented in the Hybrid Multiphase Porous Electrode Theory (Hybrid-MPET) framework can complement future experimental research and accelerate development of multi-active material electrodes with targeted performance."],"url":"http://arxiv.org/abs/2402.03677v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 03:47:49","title":"ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor","abstract":"As Graph Neural Networks (GNNs) become popular, libraries like PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these libraries have emerged as the de facto standard for implementing GNNs because they provide graph-oriented APIs and are purposefully designed to manage the inherent sparsity and irregularity in graph structures. However, these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance. This is because GNN training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently. To address this challenge, we propose ARGO, a novel runtime system for GNN training that offers scalable performance. ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization. We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding. The auto-tuner works automatically, making it completely transparent from the user. Furthermore, the auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets, etc. We evaluate ARGO on two representative GNN models and four widely-used datasets on two platforms. With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space. ARGO speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively. Finally, ARGO can seamlessly integrate into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and speed up GNN training.","sentences":["As Graph Neural Networks (GNNs) become popular, libraries like PyTorch-Geometric (PyG) and Deep Graph Library (DGL) are proposed; these libraries have emerged as the de facto standard for implementing GNNs because they provide graph-oriented APIs and are purposefully designed to manage the inherent sparsity and irregularity in graph structures.","However, these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance.","This is because GNN training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently.","To address this challenge, we propose ARGO, a novel runtime system for GNN training that offers scalable performance.","ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization.","We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding.","The auto-tuner works automatically, making it completely transparent from the user.","Furthermore, the auto-tuner allows ARGO to adapt to various platforms, GNN models, datasets, etc.","We evaluate ARGO on two representative GNN models and four widely-used datasets on two platforms.","With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space.","ARGO speeds up state-of-the-art GNN libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively.","Finally, ARGO can seamlessly integrate into widely-used GNN libraries (e.g., DGL, PyG) with few lines of code and speed up GNN training."],"url":"http://arxiv.org/abs/2402.03671v1","category":"cs.DC"}
{"created":"2024-02-06 03:46:31","title":"Large order behavior near the AD point: the case of $\\mathcal{N} =2$, $su(2)$, $N_f =2$","abstract":"A non-perturbative effect in $\\kappa$ (renormalized string coupling) obtained from the large order behavior in the vicinity of the prototypical Argyres-Douglas critical point of $su(2)$, $N_f =2$, $\\mathcal{N} =2$ susy gauge theory can be studied in the GWW unitary matrix model with the log term: the one as the work done against the barrier of the effective potential by a single eigenvalue lifted from the sea and the other as a non-perturbative function contained in the solutions of the nonlinear differential equation PII that goes beyond the asymptotic series.   The leading behaviors are of the form $\\exp (-\\frac{4}{3}\\frac{1}{\\kappa} \\, (1, \\left(\\frac{s}{K}\\right)^{\\frac{3}{2}} ))$ respectively.   We make comments on their agreement.","sentences":["A non-perturbative effect in $\\kappa$ (renormalized string coupling) obtained from the large order behavior in the vicinity of the prototypical Argyres-Douglas critical point of $su(2)$, $N_f =2$, $\\mathcal{N} =2$ susy gauge theory can be studied in the GWW unitary matrix model with the log term: the one as the work done against the barrier of the effective potential by a single eigenvalue lifted from the sea and the other as a non-perturbative function contained in the solutions of the nonlinear differential equation PII that goes beyond the asymptotic series.   ","The leading behaviors are of the form $\\exp (-\\frac{4}{3}\\frac{1}{\\kappa} \\, (1, \\left(\\frac{s}{K}\\right)^{\\frac{3}{2}} ))$ respectively.   ","We make comments on their agreement."],"url":"http://arxiv.org/abs/2402.03670v1","category":"hep-th"}
{"created":"2024-02-06 03:46:26","title":"Convergence Analysis of Distributed Generalized Nash Equilibria Seeking Algorithm with Asynchrony and Delays","abstract":"This paper considers a class of noncooperative games in which the feasible decision sets of all players are coupled together by a coupled inequality constraint. Adopting the variational inequality formulation of the game, we first introduce a new local edge-based equilibrium condition and develop a distributed primal-dual proximal algorithm with full information. Considering challenges when communication delays occur, we devise an asynchronous distributed algorithm to seek a generalized Nash equilibrium. This asynchronous scheme arbitrarily activates one player to start new computations independently at different iteration instants, which means that the picked player can use the involved out-dated information from itself and its neighbors to perform new updates. A distinctive attribute is that the proposed algorithms enable the derivation of new distributed forward-backward-like extensions. In theoretical aspect, we provide explicit conditions on algorithm parameters, for instance, the step-sizes to establish a sublinear convergence rate for the proposed synchronous algorithm. Moreover, the asynchronous algorithm guarantees almost sure convergence in expectation under the same step-size conditions and some standard assumptions. An interesting observation is that our analysis approach improves the convergence rate of prior synchronous distributed forward-backward-based algorithms. Finally, the viability and performance of the proposed algorithms are demonstrated by numerical studies on the networked Cournot competition.","sentences":["This paper considers a class of noncooperative games in which the feasible decision sets of all players are coupled together by a coupled inequality constraint.","Adopting the variational inequality formulation of the game, we first introduce a new local edge-based equilibrium condition and develop a distributed primal-dual proximal algorithm with full information.","Considering challenges when communication delays occur, we devise an asynchronous distributed algorithm to seek a generalized Nash equilibrium.","This asynchronous scheme arbitrarily activates one player to start new computations independently at different iteration instants, which means that the picked player can use the involved out-dated information from itself and its neighbors to perform new updates.","A distinctive attribute is that the proposed algorithms enable the derivation of new distributed forward-backward-like extensions.","In theoretical aspect, we provide explicit conditions on algorithm parameters, for instance, the step-sizes to establish a sublinear convergence rate for the proposed synchronous algorithm.","Moreover, the asynchronous algorithm guarantees almost sure convergence in expectation under the same step-size conditions and some standard assumptions.","An interesting observation is that our analysis approach improves the convergence rate of prior synchronous distributed forward-backward-based algorithms.","Finally, the viability and performance of the proposed algorithms are demonstrated by numerical studies on the networked Cournot competition."],"url":"http://arxiv.org/abs/2402.03669v1","category":"cs.GT"}
{"created":"2024-02-06 03:39:44","title":"QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning","abstract":"Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion.","sentences":["Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption.","While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits.","In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules.","To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution.","Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency.","We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization.","Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion."],"url":"http://arxiv.org/abs/2402.03666v1","category":"cs.CV"}
{"created":"2024-02-06 03:18:58","title":"Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models","abstract":"Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to generate explainable stock predictions in a fully autonomous manner. The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts. The training samples for the PPO trainer are also the responses generated during the reflective process, which eliminates the need for human annotators. Using our SEP framework, we fine-tune a LLM that can outperform both traditional deep-learning and LLM methods in prediction accuracy and Matthews correlation coefficient for the stock classification task. To justify the generalization capability of our framework, we further test it on the portfolio construction task, and demonstrate its effectiveness through various portfolio metrics.","sentences":["Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts.","Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process.","However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices.","The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others.","On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale.","To tackle these issues, we propose our Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to generate explainable stock predictions in a fully autonomous manner.","The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts.","The training samples for the PPO trainer are also the responses generated during the reflective process, which eliminates the need for human annotators.","Using our SEP framework, we fine-tune a LLM that can outperform both traditional deep-learning and LLM methods in prediction accuracy and Matthews correlation coefficient for the stock classification task.","To justify the generalization capability of our framework, we further test it on the portfolio construction task, and demonstrate its effectiveness through various portfolio metrics."],"url":"http://arxiv.org/abs/2402.03659v1","category":"cs.LG"}
{"created":"2024-02-06 02:44:48","title":"Photophoretic Movement of a Micron-Sized Light-Absorbing Capsule: Numerical Simulation","abstract":"Multilayer microparticles with a liquid core and a polycomposite light-absorbing shell (microcapsules) are important components of modern bio- and medical technologies. Opening of the microcapsule shell and payload release can be realized by optical radiation. The photophoretic force is due to the radiation-stimulated thermal gradient and arises from the temperature inhomogeneity of the microparticle. Photophoretic forces, as well as radiation pressure forces, are inherently mechanical forces and can cause microcapsules to move during the opening cycle. We numerically simulate the microcapsule photophoretic motion when illuminated by an intense laser pulse. Numerical calculations of the temperature field in a spherical microcapsule are carried out using the finite element method, taking into account the auxiliary nanoparticles, which are randomly distributed around the capsule and serve to enhance the heating of the capsule under short pulse exposure. The spatial distribution of the absorbed optical power as well as the temporal dynamics of microcapsule heating depending of its size are investigated in detail. We show, for the first time to our knowledge, that under the action of photophoretic gradient, the microcapsule can move along the laser incidence direction both forward and backward at the distance of several tens of nanometers depending on the particle size and conditions of optical absorption.","sentences":["Multilayer microparticles with a liquid core and a polycomposite light-absorbing shell (microcapsules) are important components of modern bio- and medical technologies.","Opening of the microcapsule shell and payload release can be realized by optical radiation.","The photophoretic force is due to the radiation-stimulated thermal gradient and arises from the temperature inhomogeneity of the microparticle.","Photophoretic forces, as well as radiation pressure forces, are inherently mechanical forces and can cause microcapsules to move during the opening cycle.","We numerically simulate the microcapsule photophoretic motion when illuminated by an intense laser pulse.","Numerical calculations of the temperature field in a spherical microcapsule are carried out using the finite element method, taking into account the auxiliary nanoparticles, which are randomly distributed around the capsule and serve to enhance the heating of the capsule under short pulse exposure.","The spatial distribution of the absorbed optical power as well as the temporal dynamics of microcapsule heating depending of its size are investigated in detail.","We show, for the first time to our knowledge, that under the action of photophoretic gradient, the microcapsule can move along the laser incidence direction both forward and backward at the distance of several tens of nanometers depending on the particle size and conditions of optical absorption."],"url":"http://arxiv.org/abs/2402.03645v1","category":"physics.optics"}
{"created":"2024-02-06 02:30:53","title":"A Short Intense Dynamo at the Onset of Crystallization in White Dwarfs","abstract":"The origin of large magnetic fields ($\\gtrsim 10^6~\\mathrm{G}$) in isolated white dwarfs is not clear. One possible explanation is that crystallization of the star's core drives compositional convection, which when combined with the star's rotation, can drive a dynamo. However, whether convection is efficient enough to explain the large intensity of the observed magnetic fields is still under debate. Recent work has shown that convection in cooling white dwarfs spans two regimes: efficient convection at the onset of crystallization, and thermohaline convection during most of the star's cooling history. Here, we calculate the properties of crystallization-driven convection for cooling models of several white dwarfs of different masses. We combine mixing-length theory with scalings from magneto-rotational convection to estimate the typical magnitude of the convective velocity and induced magnetic field for both scenarios. In the thermohaline regime, we find velocities $\\sim 10^{-6}$--$10^{-5}~\\mathrm{cm~s^{-1}}$, with fields restricted to $\\lesssim~100~\\mathrm{G}$. However, when convection is efficient, the flow velocity can reach magnitudes of $\\sim 10^2$--$10^3~\\mathrm{cm~s^{-1}}$, with fields of $\\sim 10^6$--$10^8~\\mathrm{G}$, independent of the star's rotation rate. Thus, dynamos driven at the onset of crystallization could explain the large intensity magnetic fields measured for single white dwarfs.","sentences":["The origin of large magnetic fields ($\\gtrsim 10^6~\\mathrm{G}$) in isolated white dwarfs is not clear.","One possible explanation is that crystallization of the star's core drives compositional convection, which when combined with the star's rotation, can drive a dynamo.","However, whether convection is efficient enough to explain the large intensity of the observed magnetic fields is still under debate.","Recent work has shown that convection in cooling white dwarfs spans two regimes: efficient convection at the onset of crystallization, and thermohaline convection during most of the star's cooling history.","Here, we calculate the properties of crystallization-driven convection for cooling models of several white dwarfs of different masses.","We combine mixing-length theory with scalings from magneto-rotational convection to estimate the typical magnitude of the convective velocity and induced magnetic field for both scenarios.","In the thermohaline regime, we find velocities $\\sim 10^{-6}$--$10^{-5}~\\mathrm{cm~s^{-1}}$, with fields restricted to $\\lesssim~100~\\mathrm{G}$. However, when convection is efficient, the flow velocity can reach magnitudes of $\\sim 10^2$--$10^3~\\mathrm{cm~s^{-1}}$, with fields of $\\sim 10^6$--$10^8~\\mathrm{G}$, independent of the star's rotation rate.","Thus, dynamos driven at the onset of crystallization could explain the large intensity magnetic fields measured for single white dwarfs."],"url":"http://arxiv.org/abs/2402.03639v1","category":"astro-ph.SR"}
{"created":"2024-02-06 02:00:18","title":"CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model","abstract":"The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot capability and flexible geometric prompting in general image segmentation. However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just few-shot target samples. CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters. The core design is a prompt bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder. The bridging maps the prompt token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits. We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable prompt tokens in the input space and the other inserting lightweight adapter networks. Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup. Project page: \\url{https://xiaoaoran.github.io/projects/CAT-SAM}","sentences":["The recent Segment Anything Model (SAM) has demonstrated remarkable zero-shot capability and flexible geometric prompting in general image segmentation.","However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images.","This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just few-shot target samples.","CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters.","The core design is a prompt bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder.","The bridging maps the prompt token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits.","We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable prompt tokens in the input space and the other inserting lightweight adapter networks.","Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup.","Project page: \\url{https://xiaoaoran.github.io/projects/CAT-SAM}"],"url":"http://arxiv.org/abs/2402.03631v1","category":"cs.CV"}
{"created":"2024-02-06 01:56:29","title":"Disparate Impact on Group Accuracy of Linearization for Private Inference","abstract":"Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models can serve as an effective mitigation strategy.","sentences":["Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge.","To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks.","This technique results in significantly reduced runtimes with often negligible impacts on accuracy.","In this paper, we demonstrate that such computational benefits may lead to increased fairness costs.","Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups.","To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures.","Finally, we show how a simple procedure altering the fine-tuning step for linearized models can serve as an effective mitigation strategy."],"url":"http://arxiv.org/abs/2402.03629v1","category":"cs.LG"}
{"created":"2024-02-06 01:29:35","title":"Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time","abstract":"In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\\sqrt{\\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.","sentences":["In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations.","We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\\sqrt{\\log n})$, where $n$ is the number of training samples.","A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor.","Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss.","Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well."],"url":"http://arxiv.org/abs/2402.03625v1","category":"cs.LG"}
{"created":"2024-02-06 01:01:23","title":"Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series Data","abstract":"We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data. Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks. Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs. Instead, we propose a new Bayesian VAR model with a hierarchical graph prior over binary Granger causal graphs, separately from the VAR coefficients. We develop an efficient algorithm to infer the posterior over binary Granger causal graphs. Our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data.","sentences":["We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data.","Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks.","Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs.","Instead, we propose a new Bayesian VAR model with a hierarchical graph prior over binary Granger causal graphs, separately from the VAR coefficients.","We develop an efficient algorithm to infer the posterior over binary Granger causal graphs.","Our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data."],"url":"http://arxiv.org/abs/2402.03614v1","category":"cs.LG"}
{"created":"2024-02-06 00:52:24","title":"Sensitivity and Bandwidth of a Point-Source-Interferometry-based Inertial Measurement Unit Employing Large Momentum Transfer and Launched Atoms","abstract":"We analyze theoretically the sensitivity and bandwidth of accelerometry and rotation sensing with a point source interferometer employing large momentum transfer (LMT) with molasses-launched atoms. The launching process makes it possible to realize the LMT process without the need to physically change directions of the Raman pulses, thus significantly simplifying the apparatus and reducing the amount of time needed to make the measurements. These advantages become more important when this process is used for realizing an inertial measurement unit (IMU) that can measure rotation around and acceleration along each of the three axes. We describe an explicit scheme for a such an IMU and determine the expected sensitivity and bandwidth thereof for experimentally accessible parameters.","sentences":["We analyze theoretically the sensitivity and bandwidth of accelerometry and rotation sensing with a point source interferometer employing large momentum transfer (LMT) with molasses-launched atoms.","The launching process makes it possible to realize the LMT process without the need to physically change directions of the Raman pulses, thus significantly simplifying the apparatus and reducing the amount of time needed to make the measurements.","These advantages become more important when this process is used for realizing an inertial measurement unit (IMU) that can measure rotation around and acceleration along each of the three axes.","We describe an explicit scheme for a such an IMU and determine the expected sensitivity and bandwidth thereof for experimentally accessible parameters."],"url":"http://arxiv.org/abs/2402.03608v1","category":"quant-ph"}
{"created":"2024-02-06 00:15:11","title":"Exploring the effectiveness of documentary film for science communication","abstract":"The complexity of science and its frequent lack of accessibility often creates disinterest among the general public. Furthermore, there exists a gap between the public perception of science and the reality of scientific research, which severely limits the scope of public engagement with science. A new docuseries, Curiosity-The Making of a Scientist, created at the STAGE Lab at the University of Chicago, addresses this issue by increasing the awareness and excitement around science and scientists. Each film of Curiosity focuses on a single scientist and tells the story of how they became a scientist by interweaving elements of their personal life together with the successes and failures they encounter in their scientific work. Our intended audience includes scientists and non-scientists with little or no previous exposure to quantum physics or science. The pilot of the series, SUPERPOSITION, is a 25-minute film about a graduate student in quantum physics at the University of Chicago. To evaluate the success of this film, it was screened to several audiences covering most age groups, and audience members were requested to fill out a detailed anonymous survey after the viewing. Via these surveys, viewers reported an increased interest in science and a connection to the personal story of the graduate student that was purposefully woven throughout the film.","sentences":["The complexity of science and its frequent lack of accessibility often creates disinterest among the general public.","Furthermore, there exists a gap between the public perception of science and the reality of scientific research, which severely limits the scope of public engagement with science.","A new docuseries, Curiosity-The Making of a Scientist, created at the STAGE Lab at the University of Chicago, addresses this issue by increasing the awareness and excitement around science and scientists.","Each film of Curiosity focuses on a single scientist and tells the story of how they became a scientist by interweaving elements of their personal life together with the successes and failures they encounter in their scientific work.","Our intended audience includes scientists and non-scientists with little or no previous exposure to quantum physics or science.","The pilot of the series, SUPERPOSITION, is a 25-minute film about a graduate student in quantum physics at the University of Chicago.","To evaluate the success of this film, it was screened to several audiences covering most age groups, and audience members were requested to fill out a detailed anonymous survey after the viewing.","Via these surveys, viewers reported an increased interest in science and a connection to the personal story of the graduate student that was purposefully woven throughout the film."],"url":"http://arxiv.org/abs/2402.03598v1","category":"physics.pop-ph"}
{"created":"2024-02-06 00:12:06","title":"PandaX-xT: a Multi-ten-tonne Liquid Xenon Observatory at the China Jinping Underground Laboratory","abstract":"We propose a major upgrade to the existing PandaX-4T experiment in the China Jinping Underground Laboratory. The new experiment, PandaX-xT, will be a multi-ten-tonne liquid xenon, ultra-low background, and general-purpose observatory. The full-scaled PandaX-xT contains a 43-tonne liquid xenon active target. Such an experiment will significantly advance our fundamental understanding of particle physics and astrophysics. The sensitivity of dark matter direct detection will be improved by nearly two orders of magnitude compared to the current best limits, approaching the so-called \"neutrino floor\" for a dark matter mass above 10 GeV/$c^2$, providing a decisive test to the Weakly Interacting Massive Particle paradigm. By searching for the neutrinoless double beta decay of $^{136}$Xe isotope in the detector, the effective Majorana neutrino mass can be measured to a [10 -- 41] meV/$c^2$ sensitivity, providing a key test to the Dirac/Majorana nature of neutrino s. Astrophysical neutrinos and other ultra-rare interactions can also be measured and searched for with an unprecedented background level, opening up new windows of discovery. Depending on the findings, PandaX-xT will seek the next stage upgrade utilizing isotopic separation on natural xenon.","sentences":["We propose a major upgrade to the existing PandaX-4T experiment in the China Jinping Underground Laboratory.","The new experiment, PandaX-xT, will be a multi-ten-tonne liquid xenon, ultra-low background, and general-purpose observatory.","The full-scaled PandaX-xT contains a 43-tonne liquid xenon active target.","Such an experiment will significantly advance our fundamental understanding of particle physics and astrophysics.","The sensitivity of dark matter direct detection will be improved by nearly two orders of magnitude compared to the current best limits, approaching the so-called \"neutrino floor\" for a dark matter mass above 10 GeV/$c^2$, providing a decisive test to the Weakly Interacting Massive Particle paradigm.","By searching for the neutrinoless double beta decay of $^{136}$Xe isotope in the detector, the effective Majorana neutrino mass can be measured to a [10 -- 41] meV/$c^2$ sensitivity, providing a key test to the Dirac/Majorana nature of neutrino s. Astrophysical neutrinos and other ultra-rare interactions can also be measured and searched for with an unprecedented background level, opening up new windows of discovery.","Depending on the findings, PandaX-xT will seek the next stage upgrade utilizing isotopic separation on natural xenon."],"url":"http://arxiv.org/abs/2402.03596v1","category":"hep-ex"}
{"created":"2024-02-06 00:10:28","title":"Upgrading the GRAVITY fringe tracker for GRAVITY+: Tracking the white light fringe in the non-observable Optical Path Length state-space","abstract":"Aims. As part of the ongoing GRAVITY+ upgrade of the Very Large Telescope Interferometer infrastructure, we aim to improve the performance of the GRAVITY Fringe-Tracker, and to enable its use by other instruments. Methods. We modify the group delay controller to consistently maintain tracking in the white light fringe, characterised by a minimum group delay. Additionally, we introduce a novel approach in which fringe-tracking is performed in the non-observable Optical Path Length state-space, using a covariance-weighted Kalman filter and an auto-regressive model of the disturbance. We outline this new state-space representation, and the formalism we use to propagate the state-vector and generate the control signal. While our approach is presented specifically in the context of GRAVITY/GRAVITY+, it can easily be adapted to other instruments or interferometric facilities. Results. We successfully demonstrate phase delay tracking within a single fringe, with any spurious phase jumps detected and corrected in less than 100 ms. We also report a significant performance improvement, as evidenced by a reduction of about 30 to 40% in phase residuals, and a much better behaviour under sub-optimal atmospheric conditions. Compared to what was observed in 2019, the median residuals have decreased from 150 nm to 100 nm on the Auxiliary Telescopes and from 250 nm to 150 nm on the Unit Telescopes. Conclusions. The improved phase-delay tracking combined with whit light fringe tracking means that from now-on, the GRAVITY Fringe-Tracker can be used by other instruments operating in different wavebands. The only limitation remains the need for an optical path dispersion adjustment.","sentences":["Aims.","As part of the ongoing GRAVITY+ upgrade of the Very Large Telescope Interferometer infrastructure, we aim to improve the performance of the GRAVITY Fringe-Tracker, and to enable its use by other instruments.","Methods.","We modify the group delay controller to consistently maintain tracking in the white light fringe, characterised by a minimum group delay.","Additionally, we introduce a novel approach in which fringe-tracking is performed in the non-observable Optical Path Length state-space, using a covariance-weighted Kalman filter and an auto-regressive model of the disturbance.","We outline this new state-space representation, and the formalism we use to propagate the state-vector and generate the control signal.","While our approach is presented specifically in the context of GRAVITY/GRAVITY+, it can easily be adapted to other instruments or interferometric facilities.","Results.","We successfully demonstrate phase delay tracking within a single fringe, with any spurious phase jumps detected and corrected in less than 100 ms.","We also report a significant performance improvement, as evidenced by a reduction of about 30 to 40% in phase residuals, and a much better behaviour under sub-optimal atmospheric conditions.","Compared to what was observed in 2019, the median residuals have decreased from 150 nm to 100 nm on the Auxiliary Telescopes and from 250 nm to 150 nm on the Unit Telescopes.","Conclusions.","The improved phase-delay tracking combined with whit light fringe tracking means that from now-on, the GRAVITY Fringe-Tracker can be used by other instruments operating in different wavebands.","The only limitation remains the need for an optical path dispersion adjustment."],"url":"http://arxiv.org/abs/2402.03594v1","category":"astro-ph.IM"}
{"created":"2024-02-06 00:03:44","title":"GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation","abstract":"Cancer subtyping is one of the most challenging tasks in digital pathology, where Multiple Instance Learning (MIL) by processing gigapixel whole slide images (WSIs) has been in the spotlight of recent research. However, MIL approaches do not take advantage of inter- and intra-magnification information contained in WSIs. In this work, we present GRASP, a novel graph-structured multi-magnification framework for processing WSIs in digital pathology. Our approach is designed to dynamically emulate the pathologist's behavior in handling WSIs and benefits from the hierarchical structure of WSIs. GRASP, which introduces a convergence-based node aggregation instead of traditional pooling mechanisms, outperforms state-of-the-art methods over two distinct cancer datasets by a margin of up to 10% balanced accuracy, while being 7 times smaller than the closest-performing state-of-the-art model in terms of the number of parameters. Our results show that GRASP is dynamic in finding and consulting with different magnifications for subtyping cancers and is reliable and stable across different hyperparameters. The model's behavior has been evaluated by two expert pathologists confirming the interpretability of the model's dynamic. We also provide a theoretical foundation, along with empirical evidence, for our work, explaining how GRASP interacts with different magnifications and nodes in the graph to make predictions. We believe that the strong characteristics yet simple structure of GRASP will encourage the development of interpretable, structure-based designs for WSI representation in digital pathology. Furthermore, we publish two large graph datasets of rare Ovarian and Bladder cancers to contribute to the field.","sentences":["Cancer subtyping is one of the most challenging tasks in digital pathology, where Multiple Instance Learning (MIL) by processing gigapixel whole slide images (WSIs) has been in the spotlight of recent research.","However, MIL approaches do not take advantage of inter- and intra-magnification information contained in WSIs.","In this work, we present GRASP, a novel graph-structured multi-magnification framework for processing WSIs in digital pathology.","Our approach is designed to dynamically emulate the pathologist's behavior in handling WSIs and benefits from the hierarchical structure of WSIs.","GRASP, which introduces a convergence-based node aggregation instead of traditional pooling mechanisms, outperforms state-of-the-art methods over two distinct cancer datasets by a margin of up to 10% balanced accuracy, while being 7 times smaller than the closest-performing state-of-the-art model in terms of the number of parameters.","Our results show that GRASP is dynamic in finding and consulting with different magnifications for subtyping cancers and is reliable and stable across different hyperparameters.","The model's behavior has been evaluated by two expert pathologists confirming the interpretability of the model's dynamic.","We also provide a theoretical foundation, along with empirical evidence, for our work, explaining how GRASP interacts with different magnifications and nodes in the graph to make predictions.","We believe that the strong characteristics yet simple structure of GRASP will encourage the development of interpretable, structure-based designs for WSI representation in digital pathology.","Furthermore, we publish two large graph datasets of rare Ovarian and Bladder cancers to contribute to the field."],"url":"http://arxiv.org/abs/2402.03592v1","category":"cs.CV"}
{"created":"2024-02-05 23:30:37","title":"Decoder-Only Image Registration","abstract":"In unsupervised medical image registration, the predominant approaches involve the utilization of a encoder-decoder network architecture, allowing for precise prediction of dense, full-resolution displacement fields from given paired images. Despite its widespread use in the literature, we argue for the necessity of making both the encoder and decoder learnable in such an architecture. For this, we propose a novel network architecture, termed LessNet in this paper, which contains only a learnable decoder, while entirely omitting the utilization of a learnable encoder. LessNet substitutes the learnable encoder with simple, handcrafted features, eliminating the need to learn (optimize) network parameters in the encoder altogether. Consequently, this leads to a compact, efficient, and decoder-only architecture for 3D medical image registration. Evaluated on two publicly available brain MRI datasets, we demonstrate that our decoder-only LessNet can effectively and efficiently learn both dense displacement and diffeomorphic deformation fields in 3D. Furthermore, our decoder-only LessNet can achieve comparable registration performance to state-of-the-art methods such as VoxelMorph and TransMorph, while requiring significantly fewer computational resources. Our code and pre-trained models are available at https://github.com/xi-jia/LessNet.","sentences":["In unsupervised medical image registration, the predominant approaches involve the utilization of a encoder-decoder network architecture, allowing for precise prediction of dense, full-resolution displacement fields from given paired images.","Despite its widespread use in the literature, we argue for the necessity of making both the encoder and decoder learnable in such an architecture.","For this, we propose a novel network architecture, termed LessNet in this paper, which contains only a learnable decoder, while entirely omitting the utilization of a learnable encoder.","LessNet substitutes the learnable encoder with simple, handcrafted features, eliminating the need to learn (optimize) network parameters in the encoder altogether.","Consequently, this leads to a compact, efficient, and decoder-only architecture for 3D medical image registration.","Evaluated on two publicly available brain MRI datasets, we demonstrate that our decoder-only LessNet can effectively and efficiently learn both dense displacement and diffeomorphic deformation fields in 3D.","Furthermore, our decoder-only LessNet can achieve comparable registration performance to state-of-the-art methods such as VoxelMorph and TransMorph, while requiring significantly fewer computational resources.","Our code and pre-trained models are available at https://github.com/xi-jia/LessNet."],"url":"http://arxiv.org/abs/2402.03585v1","category":"cs.CV"}
{"created":"2024-02-05 22:21:54","title":"A novel pattern recognition system for detecting Android malware by analyzing suspicious boot sequences","abstract":"This paper introduces a malware detection system for smartphones based on studying the dynamic behavior of suspicious applications. The main goal is to prevent the installation of the malicious software on the victim systems. The approach focuses on identifying malware addressed against the Android platform. For that purpose, only the system calls performed during the boot process of the recently installed applications are studied. Thereby the amount of information to be considered is reduced, since only activities related with their initialization are taken into account. The proposal defines a pattern recognition system with three processing layers: monitoring, analysis and decision-making. First, in order to extract the sequences of system calls, the potentially compromised applications are executed on a safe and isolated environment. Then the analysis step generates the metrics required for decision-making. This level combines sequence alignment algorithms with bagging, which allow scoring the similarity between the extracted sequences considering their regions of greatest resemblance. At the decision-making stage, the Wilcoxon signed-rank test is implemented, which determines if the new software is labeled as legitimate or malicious. The proposal has been tested in different experiments that include an in-depth study of a particular use case, and the evaluation of its effectiveness when analyzing samples of well-known public datasets. Promising experimental results have been shown, hence demonstrating that the approach is a good complement to the strategies of the bibliography.","sentences":["This paper introduces a malware detection system for smartphones based on studying the dynamic behavior of suspicious applications.","The main goal is to prevent the installation of the malicious software on the victim systems.","The approach focuses on identifying malware addressed against the Android platform.","For that purpose, only the system calls performed during the boot process of the recently installed applications are studied.","Thereby the amount of information to be considered is reduced, since only activities related with their initialization are taken into account.","The proposal defines a pattern recognition system with three processing layers: monitoring, analysis and decision-making.","First, in order to extract the sequences of system calls, the potentially compromised applications are executed on a safe and isolated environment.","Then the analysis step generates the metrics required for decision-making.","This level combines sequence alignment algorithms with bagging, which allow scoring the similarity between the extracted sequences considering their regions of greatest resemblance.","At the decision-making stage, the Wilcoxon signed-rank test is implemented, which determines if the new software is labeled as legitimate or malicious.","The proposal has been tested in different experiments that include an in-depth study of a particular use case, and the evaluation of its effectiveness when analyzing samples of well-known public datasets.","Promising experimental results have been shown, hence demonstrating that the approach is a good complement to the strategies of the bibliography."],"url":"http://arxiv.org/abs/2402.03562v1","category":"cs.CR"}
{"created":"2024-02-05 22:15:55","title":"Robust Analysis of Multi-Task Learning on a Complex Vision System","abstract":"Multi-task learning (MTL) has been widely studied in the past decade. In particular, dozens of optimization algorithms have been proposed for different settings. While each of them claimed improvement when applied to certain models on certain datasets, there is still lack of deep understanding on the performance in complex real-worlds scenarios. We identify the gaps between research and application and make the following 4 contributions. (1) We comprehensively evaluate a large set of existing MTL optimization algorithms on the MetaGraspNet dataset designed for robotic grasping task, which is complex and has high real-world application values, and conclude the best-performing methods. (2) We empirically compare the method performance when applied on feature-level gradients versus parameter-level gradients over a large set of MTL optimization algorithms, and conclude that this feature-level gradients surrogate is reasonable when there are method-specific theoretical guarantee but not generalizable to all methods. (3) We provide insights on the problem of task interference and show that the existing perspectives of gradient angles and relative gradient norms do not precisely reflect the challenges of MTL, as the rankings of the methods based on these two indicators do not align well with those based on the test-set performance. (4) We provide a novel view of the task interference problem from the perspective of the latent space induced by the feature extractor and provide training monitoring results based on feature disentanglement.","sentences":["Multi-task learning (MTL) has been widely studied in the past decade.","In particular, dozens of optimization algorithms have been proposed for different settings.","While each of them claimed improvement when applied to certain models on certain datasets, there is still lack of deep understanding on the performance in complex real-worlds scenarios.","We identify the gaps between research and application and make the following 4 contributions.","(1) We comprehensively evaluate a large set of existing MTL optimization algorithms on the MetaGraspNet dataset designed for robotic grasping task, which is complex and has high real-world application values, and conclude the best-performing methods.","(2) We empirically compare the method performance when applied on feature-level gradients versus parameter-level gradients over a large set of MTL optimization algorithms, and conclude that this feature-level gradients surrogate is reasonable when there are method-specific theoretical guarantee but not generalizable to all methods.","(3) We provide insights on the problem of task interference and show that the existing perspectives of gradient angles and relative gradient norms do not precisely reflect the challenges of MTL, as the rankings of the methods based on these two indicators do not align well with those based on the test-set performance.","(4) We provide a novel view of the task interference problem from the perspective of the latent space induced by the feature extractor and provide training monitoring results based on feature disentanglement."],"url":"http://arxiv.org/abs/2402.03557v1","category":"cs.CV"}
{"created":"2024-02-05 22:14:21","title":"A security framework for Ethereum smart contracts","abstract":"The use of blockchain and smart contracts have not stopped growing in recent years. Like all software that begins to expand its use, it is also beginning to be targeted by hackers who will try to exploit vulnerabilities in both the underlying technology and the smart contract code itself. While many tools already exist for analyzing vulnerabilities in smart contracts, the heterogeneity and variety of approaches and differences in providing the analysis data makes the learning curve for the smart contract developer steep. In this article the authors present ESAF (Ethereum Security Analysis Framework), a framework for analysis of smart contracts that aims to unify and facilitate the task of analyzing smart contract vulnerabilities which can be used as a persistent security monitoring tool for a set of target contracts as well as a classic vulnerability analysis tool among other uses.","sentences":["The use of blockchain and smart contracts have not stopped growing in recent years.","Like all software that begins to expand its use, it is also beginning to be targeted by hackers who will try to exploit vulnerabilities in both the underlying technology and the smart contract code itself.","While many tools already exist for analyzing vulnerabilities in smart contracts, the heterogeneity and variety of approaches and differences in providing the analysis data makes the learning curve for the smart contract developer steep.","In this article the authors present ESAF (Ethereum Security Analysis Framework), a framework for analysis of smart contracts that aims to unify and facilitate the task of analyzing smart contract vulnerabilities which can be used as a persistent security monitoring tool for a set of target contracts as well as a classic vulnerability analysis tool among other uses."],"url":"http://arxiv.org/abs/2402.03555v1","category":"cs.CR"}
{"created":"2024-02-05 22:10:59","title":"The Green Mirage: Impact of Location- and Market-based Carbon Intensity Estimation on Carbon Optimization Efficacy","abstract":"In recent years, there has been an increased emphasis on reducing the carbon emissions from electricity consumption. Many organizations have set ambitious targets to reduce the carbon footprint of their operations as a part of their sustainability goals. The carbon footprint of any consumer of electricity is computed as the product of the total energy consumption and the carbon intensity of electricity. Third-party carbon information services provide information on carbon intensity across regions that consumers can leverage to modulate their energy consumption patterns to reduce their overall carbon footprint. In addition, to accelerate their decarbonization process, large electricity consumers increasingly acquire power purchase agreements (PPAs) from renewable power plants to obtain renewable energy credits that offset their \"brown\" energy consumption. There are primarily two methods for attributing carbon-free energy, or renewable energy credits, to electricity consumers: location-based and market-based. These two methods yield significantly different carbon intensity values for various consumers. As there is a lack of consensus which method to use for carbon-free attribution, a concurrent application of both approaches is observed in practice. In this paper, we show that such concurrent applications can cause discrepancies in the carbon savings reported by carbon optimization techniques. Our analysis across three state-of-the-art carbon optimization techniques shows possible overestimation of up to 55.1% in the carbon reductions reported by the consumers and even increased emissions for consumers in some cases. We also find that carbon optimization techniques make different decisions under the market-based method and location-based method, and the market-based method can yield up to 28.2% less carbon savings than those claimed by the location-based method for consumers without PPAs.","sentences":["In recent years, there has been an increased emphasis on reducing the carbon emissions from electricity consumption.","Many organizations have set ambitious targets to reduce the carbon footprint of their operations as a part of their sustainability goals.","The carbon footprint of any consumer of electricity is computed as the product of the total energy consumption and the carbon intensity of electricity.","Third-party carbon information services provide information on carbon intensity across regions that consumers can leverage to modulate their energy consumption patterns to reduce their overall carbon footprint.","In addition, to accelerate their decarbonization process, large electricity consumers increasingly acquire power purchase agreements (PPAs) from renewable power plants to obtain renewable energy credits that offset their \"brown\" energy consumption.","There are primarily two methods for attributing carbon-free energy, or renewable energy credits, to electricity consumers: location-based and market-based.","These two methods yield significantly different carbon intensity values for various consumers.","As there is a lack of consensus which method to use for carbon-free attribution, a concurrent application of both approaches is observed in practice.","In this paper, we show that such concurrent applications can cause discrepancies in the carbon savings reported by carbon optimization techniques.","Our analysis across three state-of-the-art carbon optimization techniques shows possible overestimation of up to 55.1% in the carbon reductions reported by the consumers and even increased emissions for consumers in some cases.","We also find that carbon optimization techniques make different decisions under the market-based method and location-based method, and the market-based method can yield up to 28.2% less carbon savings than those claimed by the location-based method for consumers without PPAs."],"url":"http://arxiv.org/abs/2402.03550v1","category":"cs.DC"}
{"created":"2024-02-05 22:06:27","title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype Identification Using a Novel AUROC Loss Function for Convolutional Neural Networks","abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial for treatment planning. However, the gold standard to determine the PLGNT subtype is biopsy, which can be impractical or dangerous for patients. This research improves the performance of Convolutional Neural Networks (CNNs) in classifying PLGNT subtypes through MRI scans by introducing a loss function that specifically improves the model's Area Under the Receiver Operating Characteristic (ROC) Curve (AUROC), offering a non-invasive diagnostic alternative. In this study, a retrospective dataset of 339 children with PLGNT (143 BRAF fusion, 71 with BRAF V600E mutation, and 125 non-BRAF) was curated. We employed a CNN model with Monte Carlo random data splitting. The baseline model was trained using binary cross entropy (BCE), and achieved an AUROC of 86.11% for differentiating BRAF fusion and BRAF V600E mutations, which was improved to 87.71% using our proposed AUROC loss function (p-value 0.045). With multiclass classification, the AUROC improved from 74.42% to 76. 59% (p-value 0.0016).","sentences":["Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial for treatment planning.","However, the gold standard to determine the PLGNT subtype is biopsy, which can be impractical or dangerous for patients.","This research improves the performance of Convolutional Neural Networks (CNNs) in classifying PLGNT subtypes through MRI scans by introducing a loss function that specifically improves the model's Area Under the Receiver Operating Characteristic (ROC) Curve (AUROC), offering a non-invasive diagnostic alternative.","In this study, a retrospective dataset of 339 children with PLGNT (143 BRAF fusion, 71 with BRAF V600E mutation, and 125 non-BRAF) was curated.","We employed a CNN model with Monte Carlo random data splitting.","The baseline model was trained using binary cross entropy (BCE), and achieved an AUROC of 86.11% for differentiating BRAF fusion and BRAF V600E mutations, which was improved to 87.71% using our proposed AUROC loss function (p-value 0.045).","With multiclass classification, the AUROC improved from 74.42% to 76.","59% (p-value 0.0016)."],"url":"http://arxiv.org/abs/2402.03547v1","category":"eess.IV"}
{"created":"2024-02-05 22:03:25","title":"Online Feature Updates Improve Online (Generalized) Label Shift Adaptation","abstract":"This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.","sentences":["This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging.","While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time.","Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model.","Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement.","Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts."],"url":"http://arxiv.org/abs/2402.03545v1","category":"cs.LG"}
{"created":"2024-02-05 21:55:24","title":"HAMLET: Graph Transformer Neural Operator for Partial Differential Equations","abstract":"We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.","sentences":["We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks.","The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process.","This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.","Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness.","HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains.","Moreover, it boosts model resilience and performance, especially in scenarios with limited data.","We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs."],"url":"http://arxiv.org/abs/2402.03541v1","category":"cs.LG"}
{"created":"2024-02-05 21:39:32","title":"Enhancing Reduced Density Matrix Functional Theory Calculations by Coupling Orbital and Occupation Optimizations","abstract":"Reduced density matrix functional theory (RDMFT) calculations are usually implemented in a decoupled manner, where the orbital and occupation optimizations are repeated alternately. Although the unitary optimization method and the recently developed explicit-by-implicit (EBI) method in Su group perform well for the optimizations of orbitals and occupations, respectively, the decoupled optimization methods often suffer from slow convergence and require dozens of alternations between the orbital and occupation optimizations. To address this issue, this work proposes a coupled optimization method that combines unitary and EBI optimizations to update orbitals and occupations simultaneously at each step. To achieve favorable convergence in coupled optimization using a simple first-order algorithm, an effective and efficient preconditioner and line search are further introduced. The superiority of the new method is demonstrated through numerous tests on different molecules, random initial guesses, different basis sets and different functionals. It outperforms all decoupled optimization methods in terms of convergence speed, convergence results and convergence stability. Even a large system like $\\mathrm{C_{60}}$ can converge to $10^{-8}$ au in 154 iterations, which shows that the coupled optimization method can make RDMFT more practical and facilitate its wider application and further development.","sentences":["Reduced density matrix functional theory (RDMFT) calculations are usually implemented in a decoupled manner, where the orbital and occupation optimizations are repeated alternately.","Although the unitary optimization method and the recently developed explicit-by-implicit (EBI) method in Su group perform well for the optimizations of orbitals and occupations, respectively, the decoupled optimization methods often suffer from slow convergence and require dozens of alternations between the orbital and occupation optimizations.","To address this issue, this work proposes a coupled optimization method that combines unitary and EBI optimizations to update orbitals and occupations simultaneously at each step.","To achieve favorable convergence in coupled optimization using a simple first-order algorithm, an effective and efficient preconditioner and line search are further introduced.","The superiority of the new method is demonstrated through numerous tests on different molecules, random initial guesses, different basis sets and different functionals.","It outperforms all decoupled optimization methods in terms of convergence speed, convergence results and convergence stability.","Even a large system like $\\mathrm{C_{60}}$ can converge to $10^{-8}$ au in 154 iterations, which shows that the coupled optimization method can make RDMFT more practical and facilitate its wider application and further development."],"url":"http://arxiv.org/abs/2402.03532v1","category":"physics.chem-ph"}
{"created":"2024-02-05 21:33:22","title":"Consistent Validation for Predictive Methods in Spatial Settings","abstract":"Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.","sentences":["Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors.","Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions.","Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions.","This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d.","from two distributions.","In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense.","We show that classical and covariate-shift methods can fail this check.","We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand.","We prove that our proposal passes our check.","And we demonstrate its advantages empirically on simulated and real data."],"url":"http://arxiv.org/abs/2402.03527v1","category":"stat.ML"}
{"created":"2024-02-05 21:01:51","title":"Single-Item Continuous-Review Inventory Models with Random Supplies","abstract":"This paper analyzes single-item continuous-review inventory models with random supplies in which the inventory dynamic between orders is described by a diffusion process, and a long-term average cost criterion is used to evaluate decisions. The class of models have general drift and diffusion coefficients and boundary points that are consistent with the notion that demand should tend to reduce the inventory level. Random yield is described by a (probability) transition function which depends on the inventory-on-hand and the {\\em nominal} amount ordered; it is assumed to be a distribution with support in the interval determined by the order-from and the nominal order-to locations of the stock level. Using weak convergence arguments involving average expected occupation and ordering measures, conditions are given for the optimality of an $(s,S)$ ordering policy in the general class of policies with finite expected cost. The characterization of the cost of an $(s,S)$-policy as a function of two variables naturally leads to a nonlinear optimization problem over the stock levels $s$ and $S$ and existence of an optimizing pair $(s^*,S^*)$ is established under weak conditions. Thus, optimal policies of inventory models with random supplies can be (easily) numerically computed. The range of applicability of the optimality result is illustrated on several inventory models with random yields.","sentences":["This paper analyzes single-item continuous-review inventory models with random supplies in which the inventory dynamic between orders is described by a diffusion process, and a long-term average cost criterion is used to evaluate decisions.","The class of models have general drift and diffusion coefficients and boundary points that are consistent with the notion that demand should tend to reduce the inventory level.","Random yield is described by a (probability) transition function which depends on the inventory-on-hand and the {\\em nominal} amount ordered; it is assumed to be a distribution with support in the interval determined by the order-from and the nominal order-to locations of the stock level.","Using weak convergence arguments involving average expected occupation and ordering measures, conditions are given for the optimality of an $(s,S)$ ordering policy in the general class of policies with finite expected cost.","The characterization of the cost of an $(s,S)$-policy as a function of two variables naturally leads to a nonlinear optimization problem over the stock levels $s$ and $S$ and existence of an optimizing pair $(s^*,S^*)$ is established under weak conditions.","Thus, optimal policies of inventory models with random supplies can be (easily) numerically computed.","The range of applicability of the optimality result is illustrated on several inventory models with random yields."],"url":"http://arxiv.org/abs/2402.03515v1","category":"math.OC"}
{"created":"2024-02-05 20:49:43","title":"A new track finding algorithm based on a multi-dimensional extension of the Hough Transform","abstract":"We introduce a new pattern recognition algorithm for track finding in High Energy Physics Experiments based on an extension of the Hough Transform to multiple dimensions. A remarkable property of this algorithm is that the execution time is simply proportional to the total number of the hits to be processed, making it particularly attractive for high occupancy situations. The algorithm needs to be trained using a sufficiently large set of simulated tracks. The same track finding algorithm can be used for very different detector geometries and only the set of simulated tracks used for training needs to be changed. The particular structure of the algorithm also lends itself naturally to parallel hardware implementations which, combined with its intrinsic flexibility, should provide a most powerful tool for triggering at future colliders.","sentences":["We introduce a new pattern recognition algorithm for track finding in High Energy Physics Experiments based on an extension of the Hough Transform to multiple dimensions.","A remarkable property of this algorithm is that the execution time is simply proportional to the total number of the hits to be processed, making it particularly attractive for high occupancy situations.","The algorithm needs to be trained using a sufficiently large set of simulated tracks.","The same track finding algorithm can be used for very different detector geometries and only the set of simulated tracks used for training needs to be changed.","The particular structure of the algorithm also lends itself naturally to parallel hardware implementations which, combined with its intrinsic flexibility, should provide a most powerful tool for triggering at future colliders."],"url":"http://arxiv.org/abs/2402.03508v1","category":"hep-ex"}
{"created":"2024-02-05 20:42:35","title":"The JWST Resolved Stellar Populations Early Release Science Program V. DOLPHOT Stellar Photometry for NIRCam and NIRISS","abstract":"We present NIRCam and NIRISS modules for DOLPHOT, a widely-used crowded field stellar photometry package. We describe details of the modules including pixel masking, astrometric alignment, star finding, photometry, catalog creation, and artificial star tests (ASTs). We tested these modules using NIRCam and NIRISS images of M92 (a Milky Way globular cluster), Draco II (an ultra-faint dwarf galaxy), and WLM (a star-forming dwarf galaxy). DOLPHOT's photometry is highly precise and the color-magnitude diagrams are deeper and have better definition than anticipated during original program design in 2017. The primary systematic uncertainties in DOLPHOT's photometry arise from mismatches in the model and observed point spread functions (PSFs) and aperture corrections, each contributing $\\lesssim0.01$ mag to the photometric error budget. Version 1.2 of WebbPSF models, which include charge diffusion and interpixel capacitance effects, significantly reduced PSF-related uncertainties. We also observed minor ($\\lesssim0.05$ mag) chip-to-chip variations in NIRCam's zero points, which will be addressed by the JWST flux calibration program. Globular cluster observations are crucial for photometric calibration. Temporal variations in the photometry are generally $\\lesssim0.01$ mag, although rare large misalignment events can introduce errors up to 0.08 mag. We provide recommended DOLPHOT parameters, guidelines for photometric reduction, and advice for improved observing strategies. Our ERS DOLPHOT data products are available on MAST, complemented by comprehensive online documentation and tutorials for using DOLPHOT with JWST imaging data.","sentences":["We present NIRCam and NIRISS modules for DOLPHOT, a widely-used crowded field stellar photometry package.","We describe details of the modules including pixel masking, astrometric alignment, star finding, photometry, catalog creation, and artificial star tests (ASTs).","We tested these modules using NIRCam and NIRISS images of M92 (a Milky Way globular cluster), Draco II (an ultra-faint dwarf galaxy), and WLM (a star-forming dwarf galaxy).","DOLPHOT's photometry is highly precise and the color-magnitude diagrams are deeper and have better definition than anticipated during original program design in 2017.","The primary systematic uncertainties in DOLPHOT's photometry arise from mismatches in the model and observed point spread functions (PSFs) and aperture corrections, each contributing $\\lesssim0.01$ mag to the photometric error budget.","Version 1.2 of WebbPSF models, which include charge diffusion and interpixel capacitance effects, significantly reduced PSF-related uncertainties.","We also observed minor ($\\lesssim0.05$ mag) chip-to-chip variations in NIRCam's zero points, which will be addressed by the JWST flux calibration program.","Globular cluster observations are crucial for photometric calibration.","Temporal variations in the photometry are generally $\\lesssim0.01$ mag, although rare large misalignment events can introduce errors up to 0.08 mag.","We provide recommended DOLPHOT parameters, guidelines for photometric reduction, and advice for improved observing strategies.","Our ERS DOLPHOT data products are available on MAST, complemented by comprehensive online documentation and tutorials for using DOLPHOT with JWST imaging data."],"url":"http://arxiv.org/abs/2402.03504v1","category":"astro-ph.GA"}
{"created":"2024-02-05 20:38:34","title":"Bimode Fosters Equivalent Circuit of Arbitrary Planar Periodic Structures and Its Application to Design Polarization Controller Devices","abstract":"A Fosters equivalent circuit for 2-D Planar Periodic Structures (PPSs) that exhibit an arbitrary geometry is presented for first time in this paper. The proposed 4-port network shows an invariant circuit topology to the PPS geometry and is completely comprised of invariant-frequency lumped elements. The circuit is the simplest in terms of number of elements within the bi-mode bandwidth for a certain geometry, and its topology makes it possible both a standardized process to obtain the equivalent circuit of an arbitrary geometry and the use of the circuit theory to design a multitude of devices. To perform a validation, the equivalent circuit of two different PPSs (a rotated dipole and a defected slotted ring) have been obtained and analyzed in single and multi-layer configurations. The circuit is also used as a tool to develop Elliptical to Linear (or Circular) Polarization converters. One of the designs presented at 20 GHz (in transmission) converts an incident elliptical polarization of Axial Ratio 5 and tilted 20 deg into a purely linear vertical polarization of XP<-30 dB, with near-zero reflections and insertion loss better than 0.1 dB. The designed device is also manufactured and tested, and the measurements are in good agreement with simulations.","sentences":["A Fosters equivalent circuit for 2-D Planar Periodic Structures (PPSs) that exhibit an arbitrary geometry is presented for first time in this paper.","The proposed 4-port network shows an invariant circuit topology to the PPS geometry and is completely comprised of invariant-frequency lumped elements.","The circuit is the simplest in terms of number of elements within the bi-mode bandwidth for a certain geometry, and its topology makes it possible both a standardized process to obtain the equivalent circuit of an arbitrary geometry and the use of the circuit theory to design a multitude of devices.","To perform a validation, the equivalent circuit of two different PPSs (a rotated dipole and a defected slotted ring) have been obtained and analyzed in single and multi-layer configurations.","The circuit is also used as a tool to develop Elliptical to Linear (or Circular) Polarization converters.","One of the designs presented at 20 GHz (in transmission) converts an incident elliptical polarization of Axial Ratio 5 and tilted 20 deg into a purely linear vertical polarization of XP<-30 dB, with near-zero reflections and insertion loss better than 0.1 dB.","The designed device is also manufactured and tested, and the measurements are in good agreement with simulations."],"url":"http://arxiv.org/abs/2402.03503v1","category":"physics.app-ph"}
{"created":"2024-02-05 20:15:19","title":"Partially Stochastic Infinitely Deep Bayesian Neural Networks","abstract":"In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators. Lastly, empirical evaluations across multiple tasks show that our proposed architectures achieve better downstream task performance and uncertainty quantification than their counterparts while being significantly more efficient.","sentences":["In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks.","Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time.","To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time.","We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition.","We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators.","Lastly, empirical evaluations across multiple tasks show that our proposed architectures achieve better downstream task performance and uncertainty quantification than their counterparts while being significantly more efficient."],"url":"http://arxiv.org/abs/2402.03495v1","category":"cs.LG"}
{"created":"2024-02-05 20:05:27","title":"Freezing in the Infinite-Bin Model","abstract":"The infinite-bin model is a one-dimensional particle system on $\\mathbb{Z}$ introduced by Foss and Konstantopoulos in relation with last passage percolation on complete directed acyclic graphs. In this model, at each integer time, a particle is selected at random according to its rank, and produces a child at the location immediately to its right. In this article, we consider the limiting distribution of particles after an infinite number of branching events have occurred. Under mild assumptions, we prove that the event (called freezing) that a location contains only a finite number of balls satisfies a $0-1$ law and we provide various criteria to determine whether freezing occurs.","sentences":["The infinite-bin model is a one-dimensional particle system on $\\mathbb{Z}$ introduced by Foss and Konstantopoulos in relation with last passage percolation on complete directed acyclic graphs.","In this model, at each integer time, a particle is selected at random according to its rank, and produces a child at the location immediately to its right.","In this article, we consider the limiting distribution of particles after an infinite number of branching events have occurred.","Under mild assumptions, we prove that the event (called freezing) that a location contains only a finite number of balls satisfies a $0-1$ law and we provide various criteria to determine whether freezing occurs."],"url":"http://arxiv.org/abs/2402.03489v1","category":"math.PR"}
{"created":"2024-02-05 19:53:34","title":"FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning","abstract":"Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a ``reference'' rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FINEST can ensure that recommender models output stable recommendations under a wide range of different perturbations without compromising next-item prediction accuracy.","sentences":["Modern recommender systems may output considerably different recommendations due to small perturbations in the training data.","Changes in the data from a single user will alter the recommendations as well as the recommendations of other users.","In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience.","We propose a method to stabilize a given recommender system against such perturbations.","This is a challenging task due to (1) the lack of a ``reference'' rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data.","Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items.","Our experiments on real-world datasets demonstrate that FINEST can ensure that recommender models output stable recommendations under a wide range of different perturbations without compromising next-item prediction accuracy."],"url":"http://arxiv.org/abs/2402.03481v1","category":"cs.IR"}
{"created":"2024-02-05 19:39:52","title":"Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model","abstract":"Estimating and disentangling epistemic uncertainty (uncertainty that can be reduced with more training data) and aleatoric uncertainty (uncertainty that is inherent to the task at hand) is critically important when applying machine learning (ML) to high-stakes applications such as medical imaging and weather forecasting. Conditional diffusion models' breakthrough ability to accurately and efficiently sample from the posterior distribution of a dataset now makes uncertainty estimation conceptually straightforward: One need only train and sample from a large ensemble of diffusion models. Unfortunately, training such an ensemble becomes computationally intractable as the complexity of the model architecture grows.   In this work we introduce a new approach to ensembling, hyper-diffusion, which allows one to accurately estimate epistemic and aleatoric uncertainty with a single model. Unlike existing Monte Carlo dropout based single-model ensembling methods, hyper-diffusion offers the same prediction accuracy as multi-model ensembles. We validate our approach on two distinct tasks: x-ray computed tomography (CT) reconstruction and weather temperature forecasting.","sentences":["Estimating and disentangling epistemic uncertainty (uncertainty that can be reduced with more training data) and aleatoric uncertainty (uncertainty that is inherent to the task at hand) is critically important when applying machine learning (ML) to high-stakes applications such as medical imaging and weather forecasting.","Conditional diffusion models' breakthrough ability to accurately and efficiently sample from the posterior distribution of a dataset now makes uncertainty estimation conceptually straightforward: One need only train and sample from a large ensemble of diffusion models.","Unfortunately, training such an ensemble becomes computationally intractable as the complexity of the model architecture grows.   ","In this work we introduce a new approach to ensembling, hyper-diffusion, which allows one to accurately estimate epistemic and aleatoric uncertainty with a single model.","Unlike existing Monte Carlo dropout based single-model ensembling methods, hyper-diffusion offers the same prediction accuracy as multi-model ensembles.","We validate our approach on two distinct tasks: x-ray computed tomography (CT) reconstruction and weather temperature forecasting."],"url":"http://arxiv.org/abs/2402.03478v1","category":"cs.LG"}
{"created":"2024-02-05 19:35:57","title":"CT Material Decomposition using Spectral Diffusion Posterior Sampling","abstract":"In this work, we introduce a new deep learning approach based on diffusion posterior sampling (DPS) to perform material decomposition from spectral CT measurements. This approach combines sophisticated prior knowledge from unsupervised training with a rigorous physical model of the measurements. A faster and more stable variant is proposed that uses a jumpstarted process to reduce the number of time steps required in the reverse process and a gradient approximation to reduce the computational cost. Performance is investigated for two spectral CT systems: dual-kVp and dual-layer detector CT. On both systems, DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10% of iterations as used in the model-based material decomposition(MBMD). Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and achieves the highest accuracy, the lowest uncertainty, and the lowest computational costs compared to classic DPS and MBMD. The results demonstrate the potential of JSDPS for providing relatively fast and accurate material decomposition based on spectral CT data.","sentences":["In this work, we introduce a new deep learning approach based on diffusion posterior sampling (DPS) to perform material decomposition from spectral CT measurements.","This approach combines sophisticated prior knowledge from unsupervised training with a rigorous physical model of the measurements.","A faster and more stable variant is proposed that uses a jumpstarted process to reduce the number of time steps required in the reverse process and a gradient approximation to reduce the computational cost.","Performance is investigated for two spectral CT systems: dual-kVp and dual-layer detector CT.","On both systems, DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10% of iterations as used in the model-based material decomposition(MBMD).","Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and achieves the highest accuracy, the lowest uncertainty, and the lowest computational costs compared to classic DPS and MBMD.","The results demonstrate the potential of JSDPS for providing relatively fast and accurate material decomposition based on spectral CT data."],"url":"http://arxiv.org/abs/2402.03476v1","category":"eess.IV"}
{"created":"2024-02-06 17:48:07","title":"Strain Functionals: A Complete and Symmetry-adapted Set of Descriptors to Characterize Atomistic Configurations","abstract":"Extracting relevant information from atomistic simulations relies on a complete and accurate characterization of atomistic configurations. We present a framework for characterizing atomistic configurations in terms of a complete and symmetry-adapted basis, referred to as strain functionals. In this approach a Gaussian kernel is used to map discrete atomic quantities, such as number density, velocities, and forces, to continuous fields. The local atomic configurations are then characterized using nth order central moments of the local number density. The initial Cartesian moments are recast unitarily into a Solid Harmonic Polynomial basis using SO(3) decompositions. Rotationally invariant metrics, referred to as Strain Functional Descriptors (SFDs), are constructed from the terms in the SO(3) decomposition using Clebsch-Gordan coupling. A key distinction compared to related methods is that a minimal but complete set of descriptors is identified. These descriptors characterize the local geometries numerically in terms of shape, size, and orientation descriptors that recognize n-fold symmetry axes and net shapes such as trigonal, cubic, hexagonal, etc. They can easily distinguish between most different crystal symmetries using n = 4, identify defects (such as dislocations and stacking faults), measure local deformation, and can be used in conjunction with machine learning techniques for in situ analysis of finite temperature atomistic simulation data and quantification of defect dynamics.","sentences":["Extracting relevant information from atomistic simulations relies on a complete and accurate characterization of atomistic configurations.","We present a framework for characterizing atomistic configurations in terms of a complete and symmetry-adapted basis, referred to as strain functionals.","In this approach a Gaussian kernel is used to map discrete atomic quantities, such as number density, velocities, and forces, to continuous fields.","The local atomic configurations are then characterized using nth order central moments of the local number density.","The initial Cartesian moments are recast unitarily into a Solid Harmonic Polynomial basis using SO(3) decompositions.","Rotationally invariant metrics, referred to as Strain Functional Descriptors (SFDs), are constructed from the terms in the SO(3) decomposition using Clebsch-Gordan coupling.","A key distinction compared to related methods is that a minimal but complete set of descriptors is identified.","These descriptors characterize the local geometries numerically in terms of shape, size, and orientation descriptors that recognize n-fold symmetry axes and net shapes such as trigonal, cubic, hexagonal, etc.","They can easily distinguish between most different crystal symmetries using n = 4, identify defects (such as dislocations and stacking faults), measure local deformation, and can be used in conjunction with machine learning techniques for in situ analysis of finite temperature atomistic simulation data and quantification of defect dynamics."],"url":"http://arxiv.org/abs/2402.04191v1","category":"physics.app-ph"}
{"created":"2024-02-06 15:13:17","title":"Retrieve to Explain: Evidence-driven Predictions with Language Models","abstract":"Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.","sentences":["Machine learning models, particularly language models, are notoriously difficult to introspect.","Black-box models can mask both issues in model training and harmful biases.","For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively.","To address these issues, we introduce Retrieve to Explain (R2E).","R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction.","R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language.","We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes."],"url":"http://arxiv.org/abs/2402.04068v1","category":"cs.LG"}
{"created":"2024-02-06 15:01:06","title":"Teleparallel Geroch geometry","abstract":"We construct the teleparallel dynamics for extended geometry where the structure algebra is (an extension of) an untwisted affine Kac-Moody algebra. This provides a geometrisation of the Geroch symmetry appearing on dimensional reduction of a gravitational theory to two dimensions. The formalism is adapted to the underlying tensor hierarchy algebra, and will serve as a stepping stone towards the geometrisation of other infinite-dimensional, e.g. hyperbolic, symmetries.","sentences":["We construct the teleparallel dynamics for extended geometry where the structure algebra is (an extension of) an untwisted affine Kac-Moody algebra.","This provides a geometrisation of the Geroch symmetry appearing on dimensional reduction of a gravitational theory to two dimensions.","The formalism is adapted to the underlying tensor hierarchy algebra, and will serve as a stepping stone towards the geometrisation of other infinite-dimensional, e.g. hyperbolic, symmetries."],"url":"http://arxiv.org/abs/2402.04055v1","category":"hep-th"}
{"created":"2024-02-06 14:22:47","title":"NNLO+PS predictions for Higgs production through bottom-quark annihilation with MINNLO$_{\\text{PS}}$","abstract":"We consider Higgs production through bottom-quark annihilation at hadron colliders and we calculate next-to-next-to-leading-order (NNLO) corrections in QCD perturbation theory matched to parton showers (NNLO+PS). To this end, we have adapted the MINNLO$_{\\text{PS}}$ method to account for the extra scale dependence induced by an overall Yukawa coupling that is $\\overline{\\rm MS}$ renormalized. We compare our results against state-of-the-art fixed-order predictions at NNLO as well as resummed predictions at next-to-next-to-leading-logarithmic (NNLL) accuracy.","sentences":["We consider Higgs production through bottom-quark annihilation at hadron colliders and we calculate next-to-next-to-leading-order (NNLO) corrections in QCD perturbation theory matched to parton showers (NNLO+PS).","To this end, we have adapted the MINNLO$_{\\text{PS}}$ method to account for the extra scale dependence induced by an overall Yukawa coupling that is $\\overline{\\rm MS}$ renormalized.","We compare our results against state-of-the-art fixed-order predictions at NNLO as well as resummed predictions at next-to-next-to-leading-logarithmic (NNLL) accuracy."],"url":"http://arxiv.org/abs/2402.04025v1","category":"hep-ph"}
{"created":"2024-02-06 12:58:38","title":"In-context learning agents are asymmetric belief updaters","abstract":"We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.","sentences":["We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology.","We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones.","Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied.","We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns.","Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition."],"url":"http://arxiv.org/abs/2402.03969v1","category":"cs.LG"}
{"created":"2024-02-06 08:27:49","title":"Energy-based Domain-Adaptive Segmentation with Depth Guidance","abstract":"Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation. Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance. To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimates. Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion. The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance. Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach.","sentences":["Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation.","Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance.","To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimates.","Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules.","The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion.","The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance.","Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach."],"url":"http://arxiv.org/abs/2402.03795v1","category":"cs.CV"}
{"created":"2024-02-06 08:14:56","title":"Adaptive Blockwise Task-interleaved Pipeline Parallelism","abstract":"Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks. In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed. In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units. ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit. Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule. We conduct experiments with popular GPT-style models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism. Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method.","sentences":["Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks.","In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed.","In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units.","ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit.","Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule.","We conduct experiments with popular GPT-style models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism.","Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method."],"url":"http://arxiv.org/abs/2402.03791v1","category":"cs.DC"}
{"created":"2024-02-06 06:46:46","title":"Intensive Vision-guided Network for Radiology Report Generation","abstract":"Automatic radiology report generation is booming due to its huge application potential for the healthcare industry. However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects. First, when extracting image features, most of them neglect multi-view reasoning in vision and model single-view structure of medical images, such as space-view or channel-view. However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis. Second, when generating reports, they overlook context reasoning with multi-modal information and focus on pure textual optimization utilizing retrieval-based methods. We aim to address these two issues by proposing a model that better simulates clinicians' perspectives and generates more accurate reports. Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception. GIA aims to learn three types of vision perception: depth view, space view, and pixel view. On the other hand, to address the above problem in report generation, we explore how to involve multi-modal signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction. Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction. Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD. Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches.","sentences":["Automatic radiology report generation is booming due to its huge application potential for the healthcare industry.","However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects.","First, when extracting image features, most of them neglect multi-view reasoning in vision and model single-view structure of medical images, such as space-view or channel-view.","However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis.","Second, when generating reports, they overlook context reasoning with multi-modal information and focus on pure textual optimization utilizing retrieval-based methods.","We aim to address these two issues by proposing a model that better simulates clinicians' perspectives and generates more accurate reports.","Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception.","GIA aims to learn three types of vision perception: depth view, space view, and pixel view.","On the other hand, to address the above problem in report generation, we explore how to involve multi-modal signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction.","Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction.","Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD.","Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.03754v1","category":"cs.CV"}
{"created":"2024-02-06 06:30:34","title":"Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models","abstract":"Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.","sentences":["Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment.","In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits.","We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision.","Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings.","The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets.","This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models.","The code is available at https://github.com/ggjy/vision_weak_to_strong."],"url":"http://arxiv.org/abs/2402.03749v1","category":"cs.CV"}
{"created":"2024-02-06 03:08:19","title":"PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process","abstract":"This paper addresses the particularities of adaptive optimal control of the uranium extraction-scrubbing operation in the PUREX process. The process dynamics are nonlinear, high dimensional, and have limited online measurements. In addition, analysis and developments are based on a qualified simulation program called PAREX, which was validated with laboratory and industrial data. The control objective is to stabilize the process at a desired solvent saturation level, guaranteeing constraints and handling disturbances. The developed control strategy relies on optimization-based methods for computing control inputs and estimates, i.e., Nonlinear Model Predictive Control (NMPC) and Nonlinear Moving Horizon Estimation (NMHE). The designs of these two associated algorithms are tailored for this process's particular dynamics and are implemented through an enhanced Particle Swarm Optimization (PSO) to guarantee constraint satisfaction. Software-in-the-loop simulations using PAREX show that the designed control scheme effectively satisfies control objectives and guarantees constraints during operation.","sentences":["This paper addresses the particularities of adaptive optimal control of the uranium extraction-scrubbing operation in the PUREX process.","The process dynamics are nonlinear, high dimensional, and have limited online measurements.","In addition, analysis and developments are based on a qualified simulation program called PAREX, which was validated with laboratory and industrial data.","The control objective is to stabilize the process at a desired solvent saturation level, guaranteeing constraints and handling disturbances.","The developed control strategy relies on optimization-based methods for computing control inputs and estimates, i.e., Nonlinear Model Predictive Control (NMPC) and Nonlinear Moving Horizon Estimation (NMHE).","The designs of these two associated algorithms are tailored for this process's particular dynamics and are implemented through an enhanced Particle Swarm Optimization (PSO) to guarantee constraint satisfaction.","Software-in-the-loop simulations using PAREX show that the designed control scheme effectively satisfies control objectives and guarantees constraints during operation."],"url":"http://arxiv.org/abs/2402.03656v1","category":"eess.SY"}
{"created":"2024-02-06 01:02:14","title":"Vortex Stretching of Non-premixed, Diluted Hydrogen/Oxygen Flamelets","abstract":"A three-dimensional flamelet model considering vortex stretching with unitary Lewis number is used to simulate diluted hydrogen-oxygen diffusion flames. Non-reacting nitrogen is used as the diluent gas in the fuel stream. Unitary Lewis number provides a common thermal and mass diffusivity from which to create scalar dissipation rate. Both stable and unstable branches of flammability curves (S-curves) are calculated with three vorticity levels and plotted against multiple input and output parameters. The description of the three-dimensional flamelet structure, allowing vorticity and variable density to produce a centrifugal effect, is seen to be necessary for an accurate determination of the water burning rate. Both maximum temperature and integrated water burning rate nearly collapse to a single curve when plotted versus maximum scalar dissipation rate but do not collapse when plotted versus the local maximum strain rate or the ambient strain rate; however, local strain rate and scalar dissipation rate depend strongly on vorticity and ambient strain rate. It is argued that the controlling inputs and natural parameters for a flamelet embedded in a turbulent eddy are the ambient vorticity and strain rate which are thus the natural parameterizing variables as opposed to local strain rate or scalar dissipation rate within the flame zone. Furthermore, established practices of the scaling of velocity derivatives exist in the literature while scaling laws for scalar gradients are sparse and unsubstantiated, further supporting the use of ambient parameterizing quantities.","sentences":["A three-dimensional flamelet model considering vortex stretching with unitary Lewis number is used to simulate diluted hydrogen-oxygen diffusion flames.","Non-reacting nitrogen is used as the diluent gas in the fuel stream.","Unitary Lewis number provides a common thermal and mass diffusivity from which to create scalar dissipation rate.","Both stable and unstable branches of flammability curves (S-curves) are calculated with three vorticity levels and plotted against multiple input and output parameters.","The description of the three-dimensional flamelet structure, allowing vorticity and variable density to produce a centrifugal effect, is seen to be necessary for an accurate determination of the water burning rate.","Both maximum temperature and integrated water burning rate nearly collapse to a single curve when plotted versus maximum scalar dissipation rate but do not collapse when plotted versus the local maximum strain rate or the ambient strain rate; however, local strain rate and scalar dissipation rate depend strongly on vorticity and ambient strain rate.","It is argued that the controlling inputs and natural parameters for a flamelet embedded in a turbulent eddy are the ambient vorticity and strain rate which are thus the natural parameterizing variables as opposed to local strain rate or scalar dissipation rate within the flame zone.","Furthermore, established practices of the scaling of velocity derivatives exist in the literature while scaling laws for scalar gradients are sparse and unsubstantiated, further supporting the use of ambient parameterizing quantities."],"url":"http://arxiv.org/abs/2402.03615v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 23:13:06","title":"MINLP-based hybrid strategy for operating mode selection of TES-backed-up refrigeration systems","abstract":"This brief deals with the satisfaction of the daily cooling demand by a hybrid system that consists of a vapour-compression refrigeration cycle and a thermal energy storage (TES) unit, based on phase change materials. The addition of the TES tank to the original refrigeration plant allows to schedule the cooling production regardless of the instantaneous demand, given that the TES tank can store cold energy and release it whenever deemed appropriate. The scheduling problem is posed as an optimization problem based on mixed-integer non-linear programming (MINLP), since it includes both discrete and continuous variables. The latter corresponds to the references on the main cooling powers involved in the problem (cooling production at the evaporator and TES charging/discharging), whereas the discrete variables define the operating mode scheduling. Therefore, in addition to the hybrid features of the physical plant, a hybrid optimal control strategy is also proposed. A receding horizon approach is applied, similar to model predictive control (MPC) strategies, while economic criteria are imposed in the objective function, as well as feasibility issues. The TES state estimation is also addressed, since its instantaneous \\emph{charge ratio} is not measurable. The proposed strategy is applied in simulation to a challenging cooling demand profile and the main advantages of the MINLP-based strategy over a non-linear MPC-based scheduling strategy previously developed are highlighted, regarding operating cost, ease of tuning, and ability to adapt to cooling demand variations.","sentences":["This brief deals with the satisfaction of the daily cooling demand by a hybrid system that consists of a vapour-compression refrigeration cycle and a thermal energy storage (TES) unit, based on phase change materials.","The addition of the TES tank to the original refrigeration plant allows to schedule the cooling production regardless of the instantaneous demand, given that the TES tank can store cold energy and release it whenever deemed appropriate.","The scheduling problem is posed as an optimization problem based on mixed-integer non-linear programming (MINLP), since it includes both discrete and continuous variables.","The latter corresponds to the references on the main cooling powers involved in the problem (cooling production at the evaporator and TES charging/discharging), whereas the discrete variables define the operating mode scheduling.","Therefore, in addition to the hybrid features of the physical plant, a hybrid optimal control strategy is also proposed.","A receding horizon approach is applied, similar to model predictive control (MPC) strategies, while economic criteria are imposed in the objective function, as well as feasibility issues.","The TES state estimation is also addressed, since its instantaneous \\emph{charge ratio} is not measurable.","The proposed strategy is applied in simulation to a challenging cooling demand profile and the main advantages of the MINLP-based strategy over a non-linear MPC-based scheduling strategy previously developed are highlighted, regarding operating cost, ease of tuning, and ability to adapt to cooling demand variations."],"url":"http://arxiv.org/abs/2402.03580v1","category":"math.OC"}
{"created":"2024-02-05 22:29:51","title":"Breakpoint based online anomaly detection","abstract":"The goal of anomaly detection is to identify observations that are generated by a distribution that differs from the reference distribution that qualifies normal behavior. When examining a time series, the reference distribution may evolve over time. The anomaly detector must therefore be able to adapt to such changes. In the online context, it is particularly difficult to adapt to abrupt and unpredictable changes. Our solution to this problem is based on the detection of breakpoints in order to adapt in real time to the new reference behavior of the series and to increase the accuracy of the anomaly detection. This solution also provides a control of the False Discovery Rate by extending methods developed for stationary series.","sentences":["The goal of anomaly detection is to identify observations that are generated by a distribution that differs from the reference distribution that qualifies normal behavior.","When examining a time series, the reference distribution may evolve over time.","The anomaly detector must therefore be able to adapt to such changes.","In the online context, it is particularly difficult to adapt to abrupt and unpredictable changes.","Our solution to this problem is based on the detection of breakpoints in order to adapt in real time to the new reference behavior of the series and to increase the accuracy of the anomaly detection.","This solution also provides a control of the False Discovery Rate by extending methods developed for stationary series."],"url":"http://arxiv.org/abs/2402.03565v1","category":"stat.ME"}
{"created":"2024-02-05 21:37:03","title":"Basic principles drive self-organization of brain-like connectivity structure","abstract":"The brain can be considered as a system that dynamically optimizes the structure of anatomical connections based on the efficiency requirements of functional connectivity. To illustrate the power of this principle in organizing the complexity of brain architecture, we portray the functional connectivity as diffusion on the current network structure. The diffusion drives adaptive rewiring, resulting in changes to the network to enhance its efficiency. This dynamic evolution of the network structure generates, and thus explains, modular small-worlds with rich club effects, f eatures commonly observed in neural anatomy. Taking wiring length and propagating waves into account leads to the morphogenesis of more specific neural structures that are stalwarts of the detailed brain functional anatomy, such as parallelism, divergence, convergence, super-rings, and super-chains. By showing how such structures emerge, largely independently of their specific biological realization, we offer a new conjecture on how natural and artificial brain-like structures can be physically implemented.","sentences":["The brain can be considered as a system that dynamically optimizes the structure of anatomical connections based on the efficiency requirements of functional connectivity.","To illustrate the power of this principle in organizing the complexity of brain architecture, we portray the functional connectivity as diffusion on the current network structure.","The diffusion drives adaptive rewiring, resulting in changes to the network to enhance its efficiency.","This dynamic evolution of the network structure generates, and thus explains, modular small-worlds with rich club effects, f eatures commonly observed in neural anatomy.","Taking wiring length and propagating waves into account leads to the morphogenesis of more specific neural structures that are stalwarts of the detailed brain functional anatomy, such as parallelism, divergence, convergence, super-rings, and super-chains.","By showing how such structures emerge, largely independently of their specific biological realization, we offer a new conjecture on how natural and artificial brain-like structures can be physically implemented."],"url":"http://arxiv.org/abs/2402.03529v1","category":"q-bio.NC"}
{"created":"2024-02-05 21:01:01","title":"Video Super-Resolution for Optimized Bitrate and Green Online Streaming","abstract":"Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.","sentences":["Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation.","Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications.","Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream.","This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality.","In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications.","ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency.","Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment.","Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds.","Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming."],"url":"http://arxiv.org/abs/2402.03513v1","category":"cs.MM"}
{"created":"2024-02-05 20:55:10","title":"Autopilot System for Depth and Pitch Control in Underwater Vehicles: Navigating Near-Surface Waves and Disturbances","abstract":"This paper introduces a framework for depth and pitch control of underwater vehicles in near-surface wave conditions. By effectively managing tail, sail plane angles and hover tank operations utilizing a Linear Quadratic Regulator controller and L1 Adaptive Autopilot augmentation, the system ensures balanced control input distribution and significantly attenuates wave disturbances. This development in underwater vehicle control systems offers potential for improved functionality across a range of marine applications. The proposed framework is demonstrated to be robust in a variety of wave conditions, enabling more precise navigation and improved safety in operational scenarios. The effectiveness of this control strategy is validated through extensive simulations using the Joubert BB2 model.","sentences":["This paper introduces a framework for depth and pitch control of underwater vehicles in near-surface wave conditions.","By effectively managing tail, sail plane angles and hover tank operations utilizing a Linear Quadratic Regulator controller and L1 Adaptive Autopilot augmentation, the system ensures balanced control input distribution and significantly attenuates wave disturbances.","This development in underwater vehicle control systems offers potential for improved functionality across a range of marine applications.","The proposed framework is demonstrated to be robust in a variety of wave conditions, enabling more precise navigation and improved safety in operational scenarios.","The effectiveness of this control strategy is validated through extensive simulations using the Joubert BB2 model."],"url":"http://arxiv.org/abs/2402.03510v1","category":"eess.SY"}
{"created":"2024-02-05 20:25:39","title":"An Analytic Solution for Kernel Adaptive Filtering","abstract":"Conventional kernel adaptive filtering (KAF) uses a prescribed, positive definite, nonlinear function to define the Reproducing Kernel Hilbert Space (RKHS), where the optimal solution for mean square error estimation is approximated using search techniques. Instead, this paper proposes to embed the full statistics of the input data in the kernel definition, obtaining the first analytical solution for nonlinear regression and nonlinear adaptive filtering applications. We call this solution the Functional Wiener Filter (FWF). Conceptually, the methodology is an extension of Parzen's work on the autocorrelation RKHS to nonlinear functional spaces. We provide an extended functional Wiener equation, and present a solution to this equation in an explicit, finite dimensional, data-dependent RKHS. We further explain the necessary requirements to compute the analytical solution in RKHS, which is beyond traditional methodologies based on the kernel trick. The FWF analytic solution to the nonlinear minimum mean square error problem has better accuracy than other kernel-based algorithms in synthetic, stationary data. In real world time series, it has comparable accuracy to KAF but displays constant complexity with respect to number of training samples. For evaluation, it is as computationally efficient as the Wiener solution (with a larger number of dimensions than the linear case). We also show how the difference equation learned by the FWF from data can be extracted leading to system identification applications, which extend the possible applications of the FWF beyond optimal nonlinear filtering.","sentences":["Conventional kernel adaptive filtering (KAF) uses a prescribed, positive definite, nonlinear function to define the Reproducing Kernel Hilbert Space (RKHS), where the optimal solution for mean square error estimation is approximated using search techniques.","Instead, this paper proposes to embed the full statistics of the input data in the kernel definition, obtaining the first analytical solution for nonlinear regression and nonlinear adaptive filtering applications.","We call this solution the Functional Wiener Filter (FWF).","Conceptually, the methodology is an extension of Parzen's work on the autocorrelation RKHS to nonlinear functional spaces.","We provide an extended functional Wiener equation, and present a solution to this equation in an explicit, finite dimensional, data-dependent RKHS.","We further explain the necessary requirements to compute the analytical solution in RKHS, which is beyond traditional methodologies based on the kernel trick.","The FWF analytic solution to the nonlinear minimum mean square error problem has better accuracy than other kernel-based algorithms in synthetic, stationary data.","In real world time series, it has comparable accuracy to KAF but displays constant complexity with respect to number of training samples.","For evaluation, it is as computationally efficient as the Wiener solution (with a larger number of dimensions than the linear case).","We also show how the difference equation learned by the FWF from data can be extracted leading to system identification applications, which extend the possible applications of the FWF beyond optimal nonlinear filtering."],"url":"http://arxiv.org/abs/2402.03497v1","category":"eess.SP"}
{"created":"2024-02-05 20:15:19","title":"Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective","abstract":"Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square roots and therefore work well in low precision, which we demonstrate empirically. This raises important questions regarding the currently overlooked role of adaptivity for the success of adaptive methods.","sentences":["Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers.","Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root.","While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference.","In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation.","Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers.","The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner.","In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square roots and therefore work well in low precision, which we demonstrate empirically.","This raises important questions regarding the currently overlooked role of adaptivity for the success of adaptive methods."],"url":"http://arxiv.org/abs/2402.03496v1","category":"cs.LG"}
{"created":"2024-02-05 19:00:05","title":"Quantum Decoherence Effects: a complete treatment","abstract":"Physical systems in real life are inextricably linked to their surroundings and never completely separated from them. Truly closed systems do not exist. The phenomenon of decoherence, which is brought about by the interaction with the environment, removes the relative phase of quantum states in superposition and makes them incoherent. In neutrino physics, decoherence, although extensively studied has only been analyzed thus far, exclusively in terms of its dissipative characteristics. While it is true that dissipation, or the exponential suppression, eventually is the main observable effect, the exchange of energy between the medium and the system, is an important factor that has been overlooked up until now. In this work, we introduce this term and analyze its consequences.","sentences":["Physical systems in real life are inextricably linked to their surroundings and never completely separated from them.","Truly closed systems do not exist.","The phenomenon of decoherence, which is brought about by the interaction with the environment, removes the relative phase of quantum states in superposition and makes them incoherent.","In neutrino physics, decoherence, although extensively studied has only been analyzed thus far, exclusively in terms of its dissipative characteristics.","While it is true that dissipation, or the exponential suppression, eventually is the main observable effect, the exchange of energy between the medium and the system, is an important factor that has been overlooked up until now.","In this work, we introduce this term and analyze its consequences."],"url":"http://arxiv.org/abs/2402.03438v1","category":"hep-ph"}
{"created":"2024-02-05 18:59:52","title":"Test-Time Adaptation for Depth Completion","abstract":"It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.","sentences":["It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them.","Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data.","We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass.","We first present a study on how the domain shift in each data modality affects model performance.","Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth.","During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain.","We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%."],"url":"http://arxiv.org/abs/2402.03312v1","category":"cs.CV"}
{"created":"2024-02-05 18:43:05","title":"A Lennard-Jones Layer for Distribution Normalization","abstract":"We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization). LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time. This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process. We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution. Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process. The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively. Finally, we apply LJLs to improve the point distribution of a score-based 3D point cloud denoising network. In general, we demonstrate that LJLs are effective for distribution normalization which can be applied at negligible cost without retraining the given neural network.","sentences":["We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and 3D point clouds through systematically rearranging points without destroying their overall structure (distribution normalization).","LJL simulates a dissipative process of repulsive and weakly attractive interactions between individual points by considering the nearest neighbor of each point at a given moment in time.","This pushes the particles into a potential valley, reaching a well-defined stable configuration that approximates an equidistant sampling after the stabilization process.","We apply LJLs to redistribute randomly generated point clouds into a randomized uniform distribution.","Moreover, LJLs are embedded in the generation process of point cloud networks by adding them at later stages of the inference process.","The improvements in 3D point cloud generation utilizing LJLs are evaluated qualitatively and quantitatively.","Finally, we apply LJLs to improve the point distribution of a score-based 3D point cloud denoising network.","In general, we demonstrate that LJLs are effective for distribution normalization which can be applied at negligible cost without retraining the given neural network."],"url":"http://arxiv.org/abs/2402.03287v1","category":"cs.LG"}
{"created":"2024-02-05 18:35:40","title":"Wild orbits and generalised singularity modules: stratifications and quantisation","abstract":"We study isomorphism classes of untwisted irregular singular meromorphic connections on principal bundles over (wild) Riemann surfaces, for any complex reductive structure group $G$ and polar divisor. In particular we compute the stabilisers of suitable marked points on their principal part orbits, showing the stabilisers are connected and controlled by the corresponding filtration of (Levi factors of) nested parabolic subgroups of $G$; this uniquely determines the orbits as complex homogeneous manifolds for groups of jets of principal $G$-bundle automorphisms. Moreover, when the residue is semisimple we stratify the space of orbits by the stabilisers, relating this to local wild mapping class groups and generalising the Levi stratification of a Cartan subalgebra $\\mathfrak{t} \\subseteq \\mathfrak{g} = \\operatorname{Lie}(G)$: the dense stratum corresponds to the generic setting of irregular isomonodromic deformations \\`a la Jimbo--Miwa--Ueno.   Then we adapt a result of Alekseev--Lachowska to deformation-quantise nongeneric orbits: the $\\ast$-product involves affine-Lie-algebra modules, extending the generalised Verma modules (in the case of regular singularities) and the `singularity' modules of F.--R. (in the case of generic irregular singularities). As in the generic case, the modules contain Whittaker vectors for the Gaiotto--Teschner Virasoro pairs from irregular Liouville conformal field theory; but they now provide all the quotients which are obtained when the corresponding parameters leave the aforementioned dense strata. We also construct Shapovalov forms for the corresponding representations of truncated (holomorphic) current Lie algebras, leading to a conjectural irreducibility criterion. Finally, we use these representations to construct new flat vector bundles of vacua/covacua \\`a la Wess--Zumino--Novikov--Witten, equipped with connections \\`a la Knizhnik--Zamolodchikov.","sentences":["We study isomorphism classes of untwisted irregular singular meromorphic connections on principal bundles over (wild) Riemann surfaces, for any complex reductive structure group $G$ and polar divisor.","In particular we compute the stabilisers of suitable marked points on their principal part orbits, showing the stabilisers are connected and controlled by the corresponding filtration of (Levi factors of) nested parabolic subgroups of $G$; this uniquely determines the orbits as complex homogeneous manifolds for groups of jets of principal $G$-bundle automorphisms.","Moreover, when the residue is semisimple we stratify the space of orbits by the stabilisers, relating this to local wild mapping class groups and generalising the Levi stratification of a Cartan subalgebra $\\mathfrak{t} \\subseteq \\mathfrak{g} = \\operatorname{Lie}(G)$: the dense stratum corresponds to the generic setting of irregular isomonodromic deformations \\`a la Jimbo--Miwa--Ueno.   ","Then we adapt a result of Alekseev--Lachowska to deformation-quantise nongeneric orbits: the $\\ast$-product involves affine-Lie-algebra modules, extending the generalised Verma modules (in the case of regular singularities) and the `singularity' modules of F.--R. (in the case of generic irregular singularities).","As in the generic case, the modules contain Whittaker vectors for the Gaiotto--Teschner Virasoro pairs from irregular Liouville conformal field theory; but they now provide all the quotients which are obtained when the corresponding parameters leave the aforementioned dense strata.","We also construct Shapovalov forms for the corresponding representations of truncated (holomorphic) current Lie algebras, leading to a conjectural irreducibility criterion.","Finally, we use these representations to construct new flat vector bundles of vacua/covacua \\`a la Wess--Zumino--Novikov--Witten, equipped with connections \\`a la Knizhnik--Zamolodchikov."],"url":"http://arxiv.org/abs/2402.03278v1","category":"math.QA"}
{"created":"2024-02-05 18:09:48","title":"Fair Active Ranking from Pairwise Preferences","abstract":"We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\\epsilon, \\delta)$-PACF-Ranking according to a fair objective function that we propose. We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle. Our proposed objective function asks to minimize the $\\ell_q$ norm of the error of the groups, where the error of a group is the $\\ell_p$ norm of the error of all the items within that group, for $p, q \\geq 1$. This generalizes the objective function of $\\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework. Adjusting parameters $p$ and $q$ allows tailoring to specific fairness preferences. We present both group-blind and group-aware algorithms and analyze their sample complexity. We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms. For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds. We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings.","sentences":["We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons.","Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\\epsilon, \\delta)$-PACF-Ranking according to a fair objective function that we propose.","We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle.","Our proposed objective function asks to minimize the $\\ell_q$ norm of the error of the groups, where the error of a group is the $\\ell_p$ norm of the error of all the items within that group, for $p, q \\geq 1$.","This generalizes the objective function of $\\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).   ","By adopting our objective function, we gain the flexibility to explore fundamental fairness concepts like equal or proportionate errors within a unified framework.","Adjusting parameters $p$ and $q$ allows tailoring to specific fairness preferences.","We present both group-blind and group-aware algorithms and analyze their sample complexity.","We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms.","For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds.","We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings."],"url":"http://arxiv.org/abs/2402.03252v1","category":"cs.LG"}
{"created":"2024-02-05 17:57:26","title":"JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching","abstract":"Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.","sentences":["Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations.","However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences.","In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching.","Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks.","We introduce several offline metrics that show that our dataset resembles real-world data.","Additionally, we present a multi-step pipeline for skill extraction and matching tasks using large language models (LLMs), benchmarking against known supervised methodologies.","We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability."],"url":"http://arxiv.org/abs/2402.03242v1","category":"cs.CL"}
{"created":"2024-02-05 17:56:41","title":"FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition","abstract":"In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining. Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: https://visual-ai.github.io/froster.","sentences":["In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition.","The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs.","However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP's pretraining.","Further, fine-tuning CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions.   ","To address these issues, FROSTER employs a residual feature distillation approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task.","Specifically, the residual feature distillation treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos.","Meanwhile, it uses a residual sub-network for feature distillation to reach a balance between the two distinct objectives of learning generalizable and video-specific features.   ","We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings.","FROSTER consistently achieves state-of-the-art performance on all datasets across the board.","Project page: https://visual-ai.github.io/froster."],"url":"http://arxiv.org/abs/2402.03241v1","category":"cs.CV"}
{"created":"2024-02-05 17:53:16","title":"Declipping and the recovery of vectors from saturated measurements","abstract":"A frame $(x_j)_{j\\in J}$ for a Hilbert space $H$ allows for a linear and stable reconstruction of any vector $x\\in H$ from the linear measurements $(\\langle x,x_j\\rangle)_{j\\in J}$. However, there are many situations where some information in the frame coefficients is lost. In applications where one is using sensors with a fixed dynamic range, any measurement above that range is registered as the maximum, and any measurement below that range is registered as the minimum. Depending on the context, recovering a vector from such measurements is called either declipping or saturation recovery. We initiate a frame theoretic approach to saturation recovery in a similar way to what [BCE06] did for phase retrieval. We characterize when saturation recovery is possible, show optimal frames for use with saturation recovery correspond to minimal multi-fold packings in projective space, and prove that the classical frame algorithm may be adapted to this non-linear problem to provide a reconstruction algorithm.","sentences":["A frame $(x_j)_{j\\in J}$ for a Hilbert space $H$ allows for a linear and stable reconstruction of any vector $x\\in H$ from the linear measurements $(\\langle x,x_j\\rangle)_{j\\in J}$.","However, there are many situations where some information in the frame coefficients is lost.","In applications where one is using sensors with a fixed dynamic range, any measurement above that range is registered as the maximum, and any measurement below that range is registered as the minimum.","Depending on the context, recovering a vector from such measurements is called either declipping or saturation recovery.","We initiate a frame theoretic approach to saturation recovery in a similar way to what [BCE06] did for phase retrieval.","We characterize when saturation recovery is possible, show optimal frames for use with saturation recovery correspond to minimal multi-fold packings in projective space, and prove that the classical frame algorithm may be adapted to this non-linear problem to provide a reconstruction algorithm."],"url":"http://arxiv.org/abs/2402.03237v1","category":"math.FA"}
{"created":"2024-02-05 17:22:34","title":"Universal Gradient Methods for Stochastic Convex Optimization","abstract":"We develop universal gradient methods for Stochastic Convex Optimization (SCO). Our algorithms automatically adapt not only to the oracle's noise but also to the H\\\"older smoothness of the objective function without a priori knowledge of the particular setting. The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD). Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences. The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire H\\\"older class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient. We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates.","sentences":["We develop universal gradient methods for Stochastic Convex Optimization (SCO).","Our algorithms automatically adapt not only to the oracle's noise but also to the H\\\"older smoothness of the objective function without a priori knowledge of the particular setting.","The key ingredient is a novel strategy for adjusting step-size coefficients in the Stochastic Gradient Method (SGD).","Unlike AdaGrad, which accumulates gradient norms, our Universal Gradient Method accumulates appropriate combinations of gradient- and iterate differences.","The resulting algorithm has state-of-the-art worst-case convergence rate guarantees for the entire H\\\"older class including, in particular, both nonsmooth functions and those with Lipschitz continuous gradient.","We also present the Universal Fast Gradient Method for SCO enjoying optimal efficiency estimates."],"url":"http://arxiv.org/abs/2402.03210v1","category":"math.OC"}
{"created":"2024-02-05 17:17:57","title":"Light and Optimal Schr\u00f6dinger Bridge Matching","abstract":"Schr\\\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \\textbf{optimal Schr\\\"odinger bridge matching}. It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \\textbf{(a)} with a single bridge matching step and \\textbf{(b)} with arbitrary transport plan as the input. Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\\\"odinger potential. We experimentally showcase the performance of our solver in a range of practical tasks. The code for the LightSB-M solver can be found at \\url{https://github.com/SKholkin/LightSB-Matching}.","sentences":["Schr\\\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT).","Recent solvers for SB exploit the pervasive bridge matching procedures.","Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them.","In particular, given the EOT plan, these procedures can be adapted to solve SB.","This fact is heavily exploited by recent works giving rives to matching-based SB solvers.","The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training.","We address these limitations and propose a novel procedure to learn SB which we call the \\textbf{optimal Schr\\\"odinger bridge matching}.","It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \\textbf{(a)} with a single bridge matching step and \\textbf{(b)} with arbitrary transport plan as the input.","Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB.","Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\\\"odinger potential.","We experimentally showcase the performance of our solver in a range of practical tasks.","The code for the LightSB-M solver can be found at \\url{https://github.com/SKholkin/LightSB-Matching}."],"url":"http://arxiv.org/abs/2402.03207v1","category":"cs.LG"}
{"created":"2024-02-05 17:13:31","title":"Right-censored models by the expectile method","abstract":"Based on the expectile loss function and the adaptive LASSO penalty, the paper proposes and studies the estimation methods for the accelerated failure time (AFT) model. In this approach, we need to estimate the survival function of the censoring variable by the Kaplan-Meier estimator. The AFT model parameters are first estimated by the expectile method and afterwards, when the number of explanatory variables can be large, by the adaptive LASSO expectile method which directly carries out the automatic selection of variables. We also obtain the convergence rate and asymptotic normality for the two estimators, while showing the sparsity property for the censored adaptive LASSO expectile estimator. A numerical study using Monte Carlo simulations confirms the theoretical results and demonstrates the competitive performance of the two proposed estimators. The usefulness of these estimators is illustrated by applying them to three survival data sets.","sentences":["Based on the expectile loss function and the adaptive LASSO penalty, the paper proposes and studies the estimation methods for the accelerated failure time (AFT) model.","In this approach, we need to estimate the survival function of the censoring variable by the Kaplan-Meier estimator.","The AFT model parameters are first estimated by the expectile method and afterwards, when the number of explanatory variables can be large, by the adaptive LASSO expectile method which directly carries out the automatic selection of variables.","We also obtain the convergence rate and asymptotic normality for the two estimators, while showing the sparsity property for the censored adaptive LASSO expectile estimator.","A numerical study using Monte Carlo simulations confirms the theoretical results and demonstrates the competitive performance of the two proposed estimators.","The usefulness of these estimators is illustrated by applying them to three survival data sets."],"url":"http://arxiv.org/abs/2402.03203v1","category":"math.ST"}
{"created":"2024-02-05 17:12:21","title":"Guidance with Spherical Gaussian Constraint for Conditional Diffusion","abstract":"Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint. Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods. Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements. Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency.","sentences":["Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training.","While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes.","This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed.","We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance.","To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions.","DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps.","Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint.","Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods.","Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements.","Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency."],"url":"http://arxiv.org/abs/2402.03201v1","category":"cs.LG"}
{"created":"2024-02-05 16:58:22","title":"Multiple testing using uniform filtering of ordered p-values","abstract":"We investigate the multiplicity model with m values of some test statistic independently drawn from a mixture of no effect (null) and positive effect (alternative), where we seek to identify, the alternative test results with a controlled error rate. We are interested in the case where the alternatives are rare. A number of multiple testing procedures filter the set of ordered p-values in order to eliminate the nulls. Such an approach can only work if the p-values originating from the alternatives form one or several identifiable clusters. The Benjamini and Hochberg (BH) method, for example, assumes that this cluster occurs in a small interval $(0,\\Delta)$ and filters out all or most of the ordered p-values $p_{(r)}$ above a linear threshold $s \\times r$. In repeated applications this filter controls the false discovery rate via the slope s. We propose a new adaptive filter that deletes the p-values from regions of uniform distribution. In cases where a single cluster remains, the p-values in an interval are declared alternatives, with the mid-point and the length of the interval chosen by controlling the data-dependent FDR at a desired level.","sentences":["We investigate the multiplicity model with m values of some test statistic independently drawn from a mixture of no effect (null) and positive effect (alternative), where we seek to identify, the alternative test results with a controlled error rate.","We are interested in the case where the alternatives are rare.","A number of multiple testing procedures filter the set of ordered p-values in order to eliminate the nulls.","Such an approach can only work if the p-values originating from the alternatives form one or several identifiable clusters.","The Benjamini and Hochberg (BH) method, for example, assumes that this cluster occurs in a small interval $(0,\\Delta)$ and filters out all or most of the ordered p-values $p_{(r)}$ above a linear threshold $s \\times r$.","In repeated applications this filter controls the false discovery rate via the slope s.","We propose a new adaptive filter that deletes the p-values from regions of uniform distribution.","In cases where a single cluster remains, the p-values in an interval are declared alternatives, with the mid-point and the length of the interval chosen by controlling the data-dependent FDR at a desired level."],"url":"http://arxiv.org/abs/2402.03192v1","category":"stat.ME"}
{"created":"2024-02-05 16:46:35","title":"Empowering Time Series Analysis with Large Language Models: A Survey","abstract":"Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.","sentences":["Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks.","However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.","Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications.","In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis.","Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs.","Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group.","We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains.","Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs."],"url":"http://arxiv.org/abs/2402.03182v1","category":"cs.LG"}
{"created":"2024-02-05 16:36:17","title":"Data-driven reconstruction of limit cycle position provides side information for improved model identification with SINDy","abstract":"Many important systems in nature are characterized by oscillations. To understand and interpret such behavior, researchers use the language of mathematical models, often in the form of differential equations. Nowadays, these equations can be derived using data-driven machine learning approaches, such as the white-box method 'Sparse Identification of Nonlinear Dynamics' (SINDy). In this paper, we show that to ensure the identification of sparse and meaningful models, it is crucial to identify the correct position of the system limit cycle in phase space. Therefore, we propose how the limit cycle position and the system's nullclines can be identified by applying SINDy to the data set with varying offsets, using three model evaluation criteria (complexity, coefficient of determination, generalization error). We successfully test the method on an oscillatory FitzHugh-Nagumo model and a more complex model consisting of two coupled cubic differential equations. Finally, we demonstrate that using this additional side information on the limit cycle in phase space can improve the success of model identification efforts in oscillatory systems.","sentences":["Many important systems in nature are characterized by oscillations.","To understand and interpret such behavior, researchers use the language of mathematical models, often in the form of differential equations.","Nowadays, these equations can be derived using data-driven machine learning approaches, such as the white-box method 'Sparse Identification of Nonlinear Dynamics' (SINDy).","In this paper, we show that to ensure the identification of sparse and meaningful models, it is crucial to identify the correct position of the system limit cycle in phase space.","Therefore, we propose how the limit cycle position and the system's nullclines can be identified by applying SINDy to the data set with varying offsets, using three model evaluation criteria (complexity, coefficient of determination, generalization error).","We successfully test the method on an oscillatory FitzHugh-Nagumo model and a more complex model consisting of two coupled cubic differential equations.","Finally, we demonstrate that using this additional side information on the limit cycle in phase space can improve the success of model identification efforts in oscillatory systems."],"url":"http://arxiv.org/abs/2402.03168v1","category":"nlin.AO"}
{"created":"2024-02-05 16:30:49","title":"Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization","abstract":"In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.","sentences":["In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos.","Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics.","In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions.","These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text.","At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content.","Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation.","Our code and models will be available at https://video-lavit.github.io."],"url":"http://arxiv.org/abs/2402.03161v2","category":"cs.CV"}
{"created":"2024-02-05 16:27:59","title":"Optimal and Near-Optimal Adaptive Vector Quantization","abstract":"Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.","sentences":["Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets.","The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case.","However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   ","We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity.","We also present an even faster near-optimal algorithm for large inputs.","Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications."],"url":"http://arxiv.org/abs/2402.03158v1","category":"cs.LG"}
{"created":"2024-02-05 15:32:10","title":"High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy","abstract":"Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions. We evaluate our proposed method on various benchmark synthetic and real-world problems. The results demonstrate that our method outperforms existing state-of-the-art techniques.","sentences":["Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions.","However, it is well known that applying BO to high-dimensional optimization problems is challenging.","To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions.","In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy.","Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function.","Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum.","Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions.","We evaluate our proposed method on various benchmark synthetic and real-world problems.","The results demonstrate that our method outperforms existing state-of-the-art techniques."],"url":"http://arxiv.org/abs/2402.03104v1","category":"stat.ML"}
{"created":"2024-02-05 15:29:28","title":"A flow approach to the generalized KPZ equation","abstract":"We show that the flow approach of Duch [Duc21] can be adapted to prove local well-posedness for the generalised KPZ equation. The key step is to extend the flow approach so that it can accommodate semilinear equations involving smooth functions of the solution instead of only polynomials - this is accomplished by introducing coordinates for the flow built out of the elementary differentials associated to the equation.","sentences":["We show that the flow approach of Duch [Duc21] can be adapted to prove local well-posedness for the generalised KPZ equation.","The key step is to extend the flow approach so that it can accommodate semilinear equations involving smooth functions of the solution instead of only polynomials - this is accomplished by introducing coordinates for the flow built out of the elementary differentials associated to the equation."],"url":"http://arxiv.org/abs/2402.03101v1","category":"math.PR"}
{"created":"2024-02-05 15:28:43","title":"Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases","abstract":"Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system's key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$.","sentences":["Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction.","Automatic prompt engineering is essential to achieve optimized performance from LLMs.","Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt.","However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases.","In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent.","During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset.","We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation.","Our method outperforms state-of-the-art methods with a limited number of annotated samples.","Furthermore, we validate the advantages of each one of the system's key components.","Our system is built in a modular way, facilitating easy adaptation to other tasks.","The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$."],"url":"http://arxiv.org/abs/2402.03099v1","category":"cs.CL"}
{"created":"2024-02-05 14:59:35","title":"A Note on Rounding Matchings in General Graphs","abstract":"In this note, we revisit the rounding algorithm of Wajc. Wajc gave a fully-adaptive randomized algorithm that rounds a dynamic fractional matching in an unweighted bipartite graph to an integral matching of nearly the same value in $O(\\text{poly}(\\log n,\\frac{1}{\\varepsilon}))$ update time. We give show that the guarantees of this algorithm hold for general graphs as well. Additionally, we show useful properties of this subroutine which have applications in rounding weighted fractional matchings.","sentences":["In this note, we revisit the rounding algorithm of Wajc.","Wajc gave a fully-adaptive randomized algorithm that rounds a dynamic fractional matching in an unweighted bipartite graph to an integral matching of nearly the same value in $O(\\text{poly}(\\log n,\\frac{1}{\\varepsilon}))$ update time.","We give show that the guarantees of this algorithm hold for general graphs as well.","Additionally, we show useful properties of this subroutine which have applications in rounding weighted fractional matchings."],"url":"http://arxiv.org/abs/2402.03068v1","category":"cs.DS"}
{"created":"2024-02-05 14:42:45","title":"Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty","abstract":"We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement learning. We report empirical evaluations demonstrating PAC's enhanced stability and improved performance over the state of the art in diverse continuous control problems.","sentences":["We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off.","PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training.","The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis.","This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process.","PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art.","The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement learning.","We report empirical evaluations demonstrating PAC's enhanced stability and improved performance over the state of the art in diverse continuous control problems."],"url":"http://arxiv.org/abs/2402.03055v1","category":"cs.LG"}
{"created":"2024-02-05 14:15:22","title":"Superconducting Qubits Above 20 GHz Operating over 200 mK","abstract":"Current state-of-the-art superconducting microwave qubits are cooled to extremely low temperatures to avoid sources of decoherence. Higher qubit operating temperatures would significantly increase the cooling power available, which is desirable for scaling up the number of qubits in quantum computing architectures and integrating qubits in experiments requiring increased heat dissipation. To operate superconducting qubits at higher temperatures, it is necessary to address both quasiparticle decoherence (which becomes significant for aluminum junctions above 160 mK) and dephasing from thermal microwave photons (which are problematic above 50 mK). Using low-loss niobium trilayer junctions, which have reduced sensitivity to quasiparticles due to niobium's higher superconducting transition temperature, we fabricate transmons with higher frequencies than previously studied, up to 24 GHz. We measure decoherence and dephasing times of about 1 us, corresponding to average qubit quality factors of approximately $10^5$, and find that decoherence is unaffected by quasiparticles up to 1 K. Without relaxation from quasiparticles, we are able to explore dephasing from purely thermal sources, finding that our qubits can operate up to approximately 250 mK while maintaining similar performance. The thermal resilience of these qubits creates new options for scaling up quantum processors, enables hybrid quantum experiments with high heat dissipation budgets, and introduces a material platform for even higher-frequency qubits.","sentences":["Current state-of-the-art superconducting microwave qubits are cooled to extremely low temperatures to avoid sources of decoherence.","Higher qubit operating temperatures would significantly increase the cooling power available, which is desirable for scaling up the number of qubits in quantum computing architectures and integrating qubits in experiments requiring increased heat dissipation.","To operate superconducting qubits at higher temperatures, it is necessary to address both quasiparticle decoherence (which becomes significant for aluminum junctions above 160 mK) and dephasing from thermal microwave photons (which are problematic above 50 mK).","Using low-loss niobium trilayer junctions, which have reduced sensitivity to quasiparticles due to niobium's higher superconducting transition temperature, we fabricate transmons with higher frequencies than previously studied, up to 24 GHz.","We measure decoherence and dephasing times of about 1 us, corresponding to average qubit quality factors of approximately $10^5$, and find that decoherence is unaffected by quasiparticles up to 1 K. Without relaxation from quasiparticles, we are able to explore dephasing from purely thermal sources, finding that our qubits can operate up to approximately 250 mK while maintaining similar performance.","The thermal resilience of these qubits creates new options for scaling up quantum processors, enables hybrid quantum experiments with high heat dissipation budgets, and introduces a material platform for even higher-frequency qubits."],"url":"http://arxiv.org/abs/2402.03031v1","category":"quant-ph"}
{"created":"2024-02-05 13:55:54","title":"Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches","abstract":"Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.","sentences":["Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications.","Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years.","This survey provides a comprehensive overview of the field's latest advancements.","Initially, FSL is formally defined, and its relationship with different learning fields is presented.","A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described.","Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed."],"url":"http://arxiv.org/abs/2402.03017v1","category":"cs.LG"}
{"created":"2024-02-05 13:40:07","title":"A structure-preserving reconstruction scheme for compressible single- and multi-phase flows based on artificial neural networks","abstract":"In the present study, we introduce an advanced reconstruction indicator, deepMTBVD, which is an evolution of the MUSCL-THINC-BVD algorithm\\cite{RN2, RN12}. This novel indicator is developed by employing a deep learning neural network to train on numerical results derived from 1D Euler equations, thereby enhancing the capability to discern the most suitable reconstruction scheme. In particular, we have designed a multilayer perceptron for offline training, culminating in a simple two-hidden-layer neural network. This network identifies the reconstruction schemes employed within the target cell. Our innovative deepMTBVD indicator stands out due to its problem-independent nature, relying solely on the relative values of neighboring cells. This approach eliminates the need to pre-construct MUSCL and THINC schemes at each time step, thereby reducing the computational effort. Without further modifications, the deepMTBVD seamlessly lends itself to direct application in the compressible multiphase model and straightforward to multi-dimensional cases on Cartesian grids. Numerical results demonstrate that deepMTBVD performs as well as the MUSCL-THINC-BVD scheme in accurately capturing shock waves, contact discontinuities, material interfaces, and vertical solutions with enhanced sharpness and reduced oscillations in smooth regions. Compared with the MUSCL scheme, the deepMTBVD exhibits lower numerical dissipation and superior resolution of complex flow structures, thereby enhancing the overall quality of the solution.","sentences":["In the present study, we introduce an advanced reconstruction indicator, deepMTBVD, which is an evolution of the MUSCL-THINC-BVD algorithm\\cite{RN2, RN12}.","This novel indicator is developed by employing a deep learning neural network to train on numerical results derived from 1D Euler equations, thereby enhancing the capability to discern the most suitable reconstruction scheme.","In particular, we have designed a multilayer perceptron for offline training, culminating in a simple two-hidden-layer neural network.","This network identifies the reconstruction schemes employed within the target cell.","Our innovative deepMTBVD indicator stands out due to its problem-independent nature, relying solely on the relative values of neighboring cells.","This approach eliminates the need to pre-construct MUSCL and THINC schemes at each time step, thereby reducing the computational effort.","Without further modifications, the deepMTBVD seamlessly lends itself to direct application in the compressible multiphase model and straightforward to multi-dimensional cases on Cartesian grids.","Numerical results demonstrate that deepMTBVD performs as well as the MUSCL-THINC-BVD scheme in accurately capturing shock waves, contact discontinuities, material interfaces, and vertical solutions with enhanced sharpness and reduced oscillations in smooth regions.","Compared with the MUSCL scheme, the deepMTBVD exhibits lower numerical dissipation and superior resolution of complex flow structures, thereby enhancing the overall quality of the solution."],"url":"http://arxiv.org/abs/2402.03002v1","category":"physics.flu-dyn"}
{"created":"2024-02-05 13:16:38","title":"A Safety-Adapted Loss for Pedestrian Detection in Automated Driving","abstract":"In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU). As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector. However, those approaches do not consider the safety factor in the deep neural network (DNN) training process. Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality. Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians. In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training. We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality. Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone.","sentences":["In safety-critical domains like automated driving (AD), errors by the object detector may endanger pedestrians and other vulnerable road users (VRU).","As common evaluation metrics are not an adequate safety indicator, recent works employ approaches to identify safety-critical VRU and back-annotate the risk to the object detector.","However, those approaches do not consider the safety factor in the deep neural network (DNN) training process.","Thus, state-of-the-art DNN penalizes all misdetections equally irrespective of their criticality.","Subsequently, to mitigate the occurrence of critical failure cases, i.e., false negatives, a safety-aware training strategy might be required to enhance the detection performance for critical pedestrians.","In this paper, we propose a novel safety-aware loss variation that leverages the estimated per-pedestrian criticality scores during training.","We exploit the reachability set-based time-to-collision (TTC-RSB) metric from the motion domain along with distance information to account for the worst-case threat quantifying the criticality.","Our evaluation results using RetinaNet and FCOS on the nuScenes dataset demonstrate that training the models with our safety-aware loss function mitigates the misdetection of critical pedestrians without sacrificing performance for the general case, i.e., pedestrians outside the safety-critical zone."],"url":"http://arxiv.org/abs/2402.02986v1","category":"cs.LG"}
{"created":"2024-02-05 12:50:30","title":"Retrieval-Augmented Score Distillation for Text-to-3D Generation","abstract":"Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.","sentences":["Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry.","Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem.","However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data.","To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream.","We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process.","To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation.","We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes.","We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency.","Project page is available at https://ku-cvlab.github.io/RetDream/."],"url":"http://arxiv.org/abs/2402.02972v1","category":"cs.CV"}
{"created":"2024-02-05 12:47:09","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives","abstract":"Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS","sentences":["Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems.","In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding.","Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene.","In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes.","Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms.","These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability.","Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models.","To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS"],"url":"http://arxiv.org/abs/2402.02968v1","category":"cs.CV"}
{"created":"2024-02-05 12:46:12","title":"Atomistic-to-continuum convergence for quasi-static crack growth in brittle materials","abstract":"We study the atomistic-to-continuum limit for a model of a quasi-static crack evolution driven by time-dependent boundary conditions. We consider a two-dimensional atomic mass spring system whose interactions are modeled by classical interaction potentials, supplemented by a suitable irreversibility condition accounting for the breaking of atmoic bonding. In a simultaneous limit of vanishing interatomic distance and discretized time step, we identify a continuum model of quasi-static crack growth in brittle fracture featuring an irreversibility condition, a global stability, and an energy balance. The proof of global stability relies on a careful adaptation of the jump-transfer argument by Francfort and Larsen to the atomistic setting.","sentences":["We study the atomistic-to-continuum limit for a model of a quasi-static crack evolution driven by time-dependent boundary conditions.","We consider a two-dimensional atomic mass spring system whose interactions are modeled by classical interaction potentials, supplemented by a suitable irreversibility condition accounting for the breaking of atmoic bonding.","In a simultaneous limit of vanishing interatomic distance and discretized time step, we identify a continuum model of quasi-static crack growth in brittle fracture featuring an irreversibility condition, a global stability, and an energy balance.","The proof of global stability relies on a careful adaptation of the jump-transfer argument by Francfort and Larsen to the atomistic setting."],"url":"http://arxiv.org/abs/2402.02966v1","category":"math.AP"}
{"created":"2024-02-05 12:34:03","title":"AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image","abstract":"The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains. We also adopt adversarial learning into the framework to further reduce the gap between source and target domains. Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly.","sentences":["The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing.","However, it plays a crucial role in the management of forests.","The huge variety of trees in varied topography severely hinders tree counting models to perform well.","The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees.","Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains.","It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps.","For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different domains while generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target domains.","We also adopt adversarial learning into the framework to further reduce the gap between source and target domains.","Our AdaTreeFormer is evaluated on six designed domain adaptation tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly."],"url":"http://arxiv.org/abs/2402.02956v1","category":"cs.CV"}
{"created":"2024-02-05 12:26:01","title":"Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers","abstract":"Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\\textsf{DynaBRO}$ -- a new method capable of withstanding $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentage of Byzantine workers.","sentences":["Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework.","However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process.","This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks.","Addressing this limitation, we propose $\\textsf{DynaBRO}$ -- a new method capable of withstanding $\\mathcal{O}(\\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting.","Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies.","Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentage of Byzantine workers."],"url":"http://arxiv.org/abs/2402.02951v1","category":"cs.LG"}
{"created":"2024-02-05 12:25:02","title":"Semantic Entropy Can Simultaneously Benefit Transmission Efficiency and Channel Security of Wireless Semantic Communications","abstract":"Recently proliferated deep learning-based semantic communications (DLSC) focus on how transmitted symbols efficiently convey a desired meaning to the destination. However, the sensitivity of neural models and the openness of wireless channels cause the DLSC system to be extremely fragile to various malicious attacks. This inspires us to ask a question: ``Can we further exploit the advantages of transmission efficiency in wireless semantic communications while also alleviating its security disadvantages?''. Keeping this in mind, we propose SemEntropy, a novel method that answers the above question by exploring the semantics of data for both adaptive transmission and physical layer encryption. Specifically, we first introduce semantic entropy, which indicates the expectation of various semantic scores regarding the transmission goal of the DLSC. Equipped with such semantic entropy, we can dynamically assign informative semantics to Orthogonal Frequency Division Multiplexing (OFDM) subcarriers with better channel conditions in a fine-grained manner. We also use the entropy to guide semantic key generation to safeguard communications over open wireless channels. By doing so, both transmission efficiency and channel security can be simultaneously improved. Extensive experiments over various benchmarks show the effectiveness of the proposed SemEntropy. We discuss the reason why our proposed method benefits secure transmission of DLSC, and also give some interesting findings, e.g., SemEntropy can keep the semantic accuracy remain 95\\% with 60\\% less transmission.","sentences":["Recently proliferated deep learning-based semantic communications (DLSC) focus on how transmitted symbols efficiently convey a desired meaning to the destination.","However, the sensitivity of neural models and the openness of wireless channels cause the DLSC system to be extremely fragile to various malicious attacks.","This inspires us to ask a question: ``Can we further exploit the advantages of transmission efficiency in wireless semantic communications while also alleviating its security disadvantages?''.","Keeping this in mind, we propose SemEntropy, a novel method that answers the above question by exploring the semantics of data for both adaptive transmission and physical layer encryption.","Specifically, we first introduce semantic entropy, which indicates the expectation of various semantic scores regarding the transmission goal of the DLSC.","Equipped with such semantic entropy, we can dynamically assign informative semantics to Orthogonal Frequency Division Multiplexing (OFDM) subcarriers with better channel conditions in a fine-grained manner.","We also use the entropy to guide semantic key generation to safeguard communications over open wireless channels.","By doing so, both transmission efficiency and channel security can be simultaneously improved.","Extensive experiments over various benchmarks show the effectiveness of the proposed SemEntropy.","We discuss the reason why our proposed method benefits secure transmission of DLSC, and also give some interesting findings, e.g., SemEntropy can keep the semantic accuracy remain 95\\% with 60\\% less transmission."],"url":"http://arxiv.org/abs/2402.02950v1","category":"cs.CR"}
{"created":"2024-02-05 11:55:50","title":"InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks","abstract":"Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks. We demonstrate variations of the InterpretCC architecture for text and tabular data across several real-world benchmarks: six online education courses, news classification, breast cancer diagnosis, and review sentiment.","sentences":["Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees).","These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions.","In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction.","We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks.","We demonstrate variations of the InterpretCC architecture for text and tabular data across several real-world benchmarks: six online education courses, news classification, breast cancer diagnosis, and review sentiment."],"url":"http://arxiv.org/abs/2402.02933v1","category":"cs.LG"}
{"created":"2024-02-05 11:37:47","title":"Entangled multiplets, asymmetry, and quantum Mpemba effect in dissipative systems","abstract":"Recently, the entanglement asymmetry emerged as an informative tool to understand dynamical symmetry restoration in out-of-equilibrium quantum many-body systems after a quantum quench. For integrable systems the asymmetry can be understood in the space-time scaling limit via the quasiparticle picture, as it was pointed out in Ref. [1]. However, a quasiparticle picture for quantum quenches from generic initial states was still lacking. Here we conjecture a full-fledged quasiparticle picture for the charged moments of the reduced density matrix, which are the main ingredients to construct the asymmetry. Our formula works for quenches producing entangled multiplets of an arbitrary number of excitations. We benchmark our results in the $XX$ spin chain. First, by using an elementary approach based on the multidimensional stationary phase approximation we provide an $\\textit{ab initio}$ rigorous derivation of the dynamics of the charged moments for the quench treated in [2]. Then, we show that the same results can be straightforwardly obtained within our quasiparticle picture. As a byproduct of our analysis, we obtain a general criterion ensuring a vanishing entanglement asymmetry at long times. Next, by using the Lindblad master equation, we study the effect of gain and loss dissipation on the entanglement asymmetry. Specifically, we investigate the fate of the so-called quantum Mpemba effect (QME) in the presence of dissipation. We show that dissipation can induce QME even if unitary dynamics that does not show it, and we provide a quasiparticle-based interpretation of the condition for the QME.","sentences":["Recently, the entanglement asymmetry emerged as an informative tool to understand dynamical symmetry restoration in out-of-equilibrium quantum many-body systems after a quantum quench.","For integrable systems the asymmetry can be understood in the space-time scaling limit via the quasiparticle picture, as it was pointed out in Ref.","[1].","However, a quasiparticle picture for quantum quenches from generic initial states was still lacking.","Here we conjecture a full-fledged quasiparticle picture for the charged moments of the reduced density matrix, which are the main ingredients to construct the asymmetry.","Our formula works for quenches producing entangled multiplets of an arbitrary number of excitations.","We benchmark our results in the $XX$ spin chain.","First, by using an elementary approach based on the multidimensional stationary phase approximation we provide an $\\textit{ab initio}$ rigorous derivation of the dynamics of the charged moments for the quench treated in [2].","Then, we show that the same results can be straightforwardly obtained within our quasiparticle picture.","As a byproduct of our analysis, we obtain a general criterion ensuring a vanishing entanglement asymmetry at long times.","Next, by using the Lindblad master equation, we study the effect of gain and loss dissipation on the entanglement asymmetry.","Specifically, we investigate the fate of the so-called quantum Mpemba effect (QME) in the presence of dissipation.","We show that dissipation can induce QME even if unitary dynamics that does not show it, and we provide a quasiparticle-based interpretation of the condition for the QME."],"url":"http://arxiv.org/abs/2402.02918v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 11:22:14","title":"ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis","abstract":"Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available.","sentences":["Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures.","Each approach offers specific strengths but also comes with specific limitations in their applicability.","This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility.","ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account.","Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods.","Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR.","Code is available."],"url":"http://arxiv.org/abs/2402.02906v1","category":"cs.CV"}
{"created":"2024-02-05 10:55:47","title":"Time-, Memory- and Parameter-Efficient Visual Adaptation","abstract":"As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly. We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.","sentences":["As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks.","Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained.","They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly.","We propose an adaptation method which does not backpropagate gradients through the backbone.","We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone.","As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage.","Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too.","We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism.","Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time."],"url":"http://arxiv.org/abs/2402.02887v1","category":"cs.CV"}
{"created":"2024-02-05 10:54:17","title":"Time-Distributed Backdoor Attacks on Federated Spiking Learning","abstract":"This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.","sentences":["This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data.","Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks.","We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage.","Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance.","Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance.","Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness.","In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM.","Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs.","This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks."],"url":"http://arxiv.org/abs/2402.02886v1","category":"cs.CR"}
{"created":"2024-02-05 10:18:47","title":"Importance sampling for online variational learning","abstract":"This article addresses online variational estimation in state-space models. We focus on learning the smoothing distribution, i.e. the joint distribution of the latent states given the observations, using a variational approach together with Monte Carlo importance sampling. We propose an efficient algorithm for computing the gradient of the evidence lower bound (ELBO) in the context of streaming data, where observations arrive sequentially. Our contributions include a computationally efficient online ELBO estimator, demonstrated performance in offline and true online settings, and adaptability for computing general expectations under joint smoothing distributions.","sentences":["This article addresses online variational estimation in state-space models.","We focus on learning the smoothing distribution, i.e. the joint distribution of the latent states given the observations, using a variational approach together with Monte Carlo importance sampling.","We propose an efficient algorithm for computing the gradient of the evidence lower bound (ELBO) in the context of streaming data, where observations arrive sequentially.","Our contributions include a computationally efficient online ELBO estimator, demonstrated performance in offline and true online settings, and adaptability for computing general expectations under joint smoothing distributions."],"url":"http://arxiv.org/abs/2402.02859v1","category":"stat.AP"}
{"created":"2024-02-05 10:17:36","title":"Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation","abstract":"Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.","sentences":["Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks.","Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods.","This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions.","Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator.","In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case.","Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning."],"url":"http://arxiv.org/abs/2402.02857v1","category":"stat.ML"}
{"created":"2024-02-05 09:48:46","title":"HAPI-FHIR Server Implementation to Enhancing Interoperability among Primary Care Health Information Systems in Sri Lanka: Review of the Technical Use Case","abstract":"This review underscores the vital role of interoperability in digital health, advocating for a standardized framework. It focuses on implementing a Fast Healthcare Interoperability Resources (FHIR) server, addressing technical, semantic, and process challenges. FHIR's adaptability ensures uniformity within Primary Care Health Information Systems, fostering interoperability. Patient data management complexities highlight the pivotal role of semantic interoperability in seamless patient care. FHIR standards enhance these efforts, offering multiple pathways for data search. The ADR-guided FHIR server implementation systematically addresses challenges related to patient identity, biometrics, and data security. The detailed development phases emphasize architecture, API integration, and security. The concluding stages incorporate forward-looking approaches, including HHIMS Synthetic Dataset testing. Envisioning FHIR integration as transformative, it anticipates a responsive healthcare environment aligned with the evolving digital health landscape, ensuring comprehensive, dynamic, and interconnected systems for efficient data exchange and access.","sentences":["This review underscores the vital role of interoperability in digital health, advocating for a standardized framework.","It focuses on implementing a Fast Healthcare Interoperability Resources (FHIR) server, addressing technical, semantic, and process challenges.","FHIR's adaptability ensures uniformity within Primary Care Health Information Systems, fostering interoperability.","Patient data management complexities highlight the pivotal role of semantic interoperability in seamless patient care.","FHIR standards enhance these efforts, offering multiple pathways for data search.","The ADR-guided FHIR server implementation systematically addresses challenges related to patient identity, biometrics, and data security.","The detailed development phases emphasize architecture, API integration, and security.","The concluding stages incorporate forward-looking approaches, including HHIMS Synthetic Dataset testing.","Envisioning FHIR integration as transformative, it anticipates a responsive healthcare environment aligned with the evolving digital health landscape, ensuring comprehensive, dynamic, and interconnected systems for efficient data exchange and access."],"url":"http://arxiv.org/abs/2402.02838v1","category":"cs.CY"}
{"created":"2024-02-05 08:38:03","title":"A Hybrid Finite-Difference-Particle Method for Chemotaxis Models","abstract":"Chemotaxis systems play a crucial role in modeling the dynamics of bacterial and cellular behaviors, including propagation, aggregation, and pattern formation, all under the influence of chemical signals. One notable characteristic of these systems is their ability to simulate concentration phenomena, where cell density undergoes rapid growth near specific concentration points or along certain curves. Such growth can result in singular, spiky structures and lead to finite-time blowups.   Our investigation focuses on the dynamics of the Patlak-Keller-Segel chemotaxis system and its two-species extensions. In the latter case, different species may exhibit distinct chemotactic sensitivities, giving rise to very different rates of cell density growth. Such a situation may be extremely challenging for numerical methods as they may fail to accurately capture the blowup of the slower-growing species mainly due to excessive numerical dissipation.   In this paper, we propose a hybrid finite-difference-particle (FDP) method, in which a particle method is used to solve the chemotaxis equation(s), while finite difference schemes are employed to solve the chemoattractant equation. Thanks to the low-dissipation nature of the particle method, the proposed hybrid scheme is particularly adept at capturing the blowup behaviors in both one- and two-species cases. The proposed hybrid FDP methods are tested on a series of challenging examples, and the obtained numerical results demonstrate that our hybrid method can provide sharp resolution of the singular structures even with a relatively small number of particles. Moreover, in the two-species case, our method adeptly captures the blowing-up solution for the component with lower chemotactic sensitivity, a feature not observed in other works.","sentences":["Chemotaxis systems play a crucial role in modeling the dynamics of bacterial and cellular behaviors, including propagation, aggregation, and pattern formation, all under the influence of chemical signals.","One notable characteristic of these systems is their ability to simulate concentration phenomena, where cell density undergoes rapid growth near specific concentration points or along certain curves.","Such growth can result in singular, spiky structures and lead to finite-time blowups.   ","Our investigation focuses on the dynamics of the Patlak-Keller-Segel chemotaxis system and its two-species extensions.","In the latter case, different species may exhibit distinct chemotactic sensitivities, giving rise to very different rates of cell density growth.","Such a situation may be extremely challenging for numerical methods as they may fail to accurately capture the blowup of the slower-growing species mainly due to excessive numerical dissipation.   ","In this paper, we propose a hybrid finite-difference-particle (FDP) method, in which a particle method is used to solve the chemotaxis equation(s), while finite difference schemes are employed to solve the chemoattractant equation.","Thanks to the low-dissipation nature of the particle method, the proposed hybrid scheme is particularly adept at capturing the blowup behaviors in both one-","and two-species cases.","The proposed hybrid FDP methods are tested on a series of challenging examples, and the obtained numerical results demonstrate that our hybrid method can provide sharp resolution of the singular structures even with a relatively small number of particles.","Moreover, in the two-species case, our method adeptly captures the blowing-up solution for the component with lower chemotactic sensitivity, a feature not observed in other works."],"url":"http://arxiv.org/abs/2402.02808v1","category":"math.NA"}
{"created":"2024-02-05 08:25:24","title":"Time-velocity decay of solutions to the non-cutoff Boltzmann equation in the whole space","abstract":"In this paper, we consider the perturbed solutions with polynomial tail in large velocities for the non-cutoff Boltzmann equation near global Maxwellians in the whole space. The global in time existence is proved in the weighted Sobolev spaces and the almost optimal time decay is obtained in Fourier transform based low-regularity spaces. The result shows a time-velocity decay structure of solutions that can be decomposed into two parts. One part allows the slow polynomial tail in large velocities, carries the initial data and enjoys the exponential or arbitrarily large polynomial time decay. The other part, with zero initial data, is dominated by the non-negative definite symmetric dissipation and has the exponential velocity decay but only the slow polynomial time decay.","sentences":["In this paper, we consider the perturbed solutions with polynomial tail in large velocities for the non-cutoff Boltzmann equation near global Maxwellians in the whole space.","The global in time existence is proved in the weighted Sobolev spaces and the almost optimal time decay is obtained in Fourier transform based low-regularity spaces.","The result shows a time-velocity decay structure of solutions that can be decomposed into two parts.","One part allows the slow polynomial tail in large velocities, carries the initial data and enjoys the exponential or arbitrarily large polynomial time decay.","The other part, with zero initial data, is dominated by the non-negative definite symmetric dissipation and has the exponential velocity decay but only the slow polynomial time decay."],"url":"http://arxiv.org/abs/2402.02804v1","category":"math.AP"}
{"created":"2024-02-05 08:25:22","title":"Large Language Model Distilling Medication Recommendation Model","abstract":"The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function. Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.","sentences":["The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs.","Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities.","Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon.","To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs).","Our research aims to transform existing medication recommendation methodologies using LLMs.","In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER).","We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively.","However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs.","We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function.","Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector.","To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model.","Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient.","To ease the reproducibility of our experiments, we release the implementation code online."],"url":"http://arxiv.org/abs/2402.02803v1","category":"cs.IR"}
{"created":"2024-02-05 08:10:16","title":"Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects","abstract":"Surface defect inspection plays an important role in the process of industrial manufacture and production. Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales. The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion. The attention map makes the model pay more attention to defect features. The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input. The obtained DRF features capture rich context information with a large range of receptive fields. Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods. Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.","sentences":["Surface defect inspection plays an important role in the process of industrial manufacture and production.","Though Convolutional Neural Network (CNN) based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on.","To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network.","JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features.","The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects.","In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales.","The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion.","The attention map makes the model pay more attention to defect features.","The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input.","The obtained DRF features capture rich context information with a large range of receptive fields.","Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods.","Meanwhile, our method reaches a real-time defect detection speed of 66 FPS."],"url":"http://arxiv.org/abs/2402.02797v1","category":"cs.CV"}
{"created":"2024-02-05 07:52:04","title":"Artificial-intelligence-based surrogate solution of dissipative quantum dynamics: physics-informed reconstruction of the universal propagator","abstract":"The accurate (or even approximate) solution of the equations that govern the dynamics of dissipative quantum systems remains a challenging task for quantum science. While several algorithms have been designed to solve those equations with different degrees of flexibility, they rely mainly on highly expensive iterative schemes. Most recently, deep neural networks have been used for quantum dynamics but current architectures are highly dependent on the physics of the particular system and usually limited to population dynamics. Here we introduce an artificial-intelligence-based surrogate model that solves dissipative quantum dynamics by parameterizing quantum propagators as Fourier neural operators, which we train using both dataset and physics-informed loss functions. Compared with conventional algorithms, our quantum neural propagator avoids time-consuming iterations and provides a universal super-operator that can be used to evolve any initial quantum state for arbitrarily long times. To illustrate the wide applicability of the approach, we employ our quantum neural propagator to compute population dynamics and time-correlation functions of the Fenna-Matthews-Olson complex.","sentences":["The accurate (or even approximate) solution of the equations that govern the dynamics of dissipative quantum systems remains a challenging task for quantum science.","While several algorithms have been designed to solve those equations with different degrees of flexibility, they rely mainly on highly expensive iterative schemes.","Most recently, deep neural networks have been used for quantum dynamics but current architectures are highly dependent on the physics of the particular system and usually limited to population dynamics.","Here we introduce an artificial-intelligence-based surrogate model that solves dissipative quantum dynamics by parameterizing quantum propagators as Fourier neural operators, which we train using both dataset and physics-informed loss functions.","Compared with conventional algorithms, our quantum neural propagator avoids time-consuming iterations and provides a universal super-operator that can be used to evolve any initial quantum state for arbitrarily long times.","To illustrate the wide applicability of the approach, we employ our quantum neural propagator to compute population dynamics and time-correlation functions of the Fenna-Matthews-Olson complex."],"url":"http://arxiv.org/abs/2402.02788v1","category":"quant-ph"}
{"created":"2024-02-05 07:20:38","title":"Chiral switching of many-body steady states in a dissipative Rydberg gas","abstract":"Dissipative Rydberg gases are an outstanding platform for the investigation of many-body quantum open systems. Despite the wealth of existing studies, the non-equilibrium dynamics of dissipative Rydberg gases are rarely examined or harnessed from the perspective of non-Hermitian physics, which is but intrinsic to open systems. Here we report the experimental observation of a chiral switching between many-body steady states in a dissipative thermal Rydberg vapor, where the interplay of many-body effects and non-Hermiticity plays a key role. Specifically, as the parameters are adiabatically varied around a closed contour, depending on the chirality of the parameter modulation, the Rydberg vapor can change between two collective steady states with distinct Rydberg excitations and optical transmissions. Adopting a mean-field description, we reveal that both the existence of the bistable steady states and chiral dynamics derive from an exceptional structure in the parameter space, where multiple steady states of the many-body Liouvillian superoperator coalesce. We demonstrate that both the exceptional structure and the resulting state-switching dynamics are tunable through microwave dressing and temperature variations, confirming their reliance on the many-body dissipative nature of the Rydberg vapor.","sentences":["Dissipative Rydberg gases are an outstanding platform for the investigation of many-body quantum open systems.","Despite the wealth of existing studies, the non-equilibrium dynamics of dissipative Rydberg gases are rarely examined or harnessed from the perspective of non-Hermitian physics, which is but intrinsic to open systems.","Here we report the experimental observation of a chiral switching between many-body steady states in a dissipative thermal Rydberg vapor, where the interplay of many-body effects and non-Hermiticity plays a key role.","Specifically, as the parameters are adiabatically varied around a closed contour, depending on the chirality of the parameter modulation, the Rydberg vapor can change between two collective steady states with distinct Rydberg excitations and optical transmissions.","Adopting a mean-field description, we reveal that both the existence of the bistable steady states and chiral dynamics derive from an exceptional structure in the parameter space, where multiple steady states of the many-body Liouvillian superoperator coalesce.","We demonstrate that both the exceptional structure and the resulting state-switching dynamics are tunable through microwave dressing and temperature variations, confirming their reliance on the many-body dissipative nature of the Rydberg vapor."],"url":"http://arxiv.org/abs/2402.02779v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-05 06:58:58","title":"Multiple Neuronal Specializations Elicited By Socially Driven Recognition Of Food Odors","abstract":"This study investigates the dynamics of non-spatial specializations in hippocampal place cells during exposure to novel environments. Hippocampal place cells, known for their role in spatial mapping, exhibit multi-modal responses to sensory cues. The research focuses on understanding how these cells adapt their specialization in response to novel stimuli, specifically examining non-spatial determinants such as odors and social interactions. Using a social-driven food odor recognition model in mice, the study records CA1 hippocampal neuron activity through miniscope imaging. The experimental design involves demonstrations of novel odors to mice, followed by observation sessions with food options. The analysis employs deep neural network tools for behavior tracking and the custom-developed INTENS software package for identifying neural specializations. Results indicate multiple specializations, particularly those related to odor, with differences observed between training and testing sessions. The findings suggest a temporal aspect to the formation of these specializations in novel conditions, necessitating further investigation for precise tracking.","sentences":["This study investigates the dynamics of non-spatial specializations in hippocampal place cells during exposure to novel environments.","Hippocampal place cells, known for their role in spatial mapping, exhibit multi-modal responses to sensory cues.","The research focuses on understanding how these cells adapt their specialization in response to novel stimuli, specifically examining non-spatial determinants such as odors and social interactions.","Using a social-driven food odor recognition model in mice, the study records CA1 hippocampal neuron activity through miniscope imaging.","The experimental design involves demonstrations of novel odors to mice, followed by observation sessions with food options.","The analysis employs deep neural network tools for behavior tracking and the custom-developed INTENS software package for identifying neural specializations.","Results indicate multiple specializations, particularly those related to odor, with differences observed between training and testing sessions.","The findings suggest a temporal aspect to the formation of these specializations in novel conditions, necessitating further investigation for precise tracking."],"url":"http://arxiv.org/abs/2402.02766v1","category":"q-bio.NC"}
{"created":"2024-02-05 06:30:47","title":"Exploring the Effects of Shared Autonomy on Cognitive Load and Trust in Human-Robot Interaction","abstract":"Teleoperation is increasingly recognized as a viable solution for deploying robots in hazardous environments. Controlling a robot to perform a complex or demanding task may overload operators resulting in poor performance. To design a robot controller to assist the human in executing such challenging tasks, a comprehensive understanding of the interplay between the robot's autonomous behavior and the operator's internal state is essential. In this paper, we investigate the relationships between robot autonomy and both the human user's cognitive load and trust levels, and the potential existence of three-way interactions in the robot-assisted execution of the task. Our user study (N=24) results indicate that while autonomy level influences the teleoperator's perceived cognitive load and trust, there is no clear interaction between these factors. Instead, these elements appear to operate independently, thus highlighting the need to consider both cognitive load and trust as distinct but interrelated factors in varying the robot autonomy level in shared-control settings. This insight is crucial for the development of more effective and adaptable assistive robotic systems.","sentences":["Teleoperation is increasingly recognized as a viable solution for deploying robots in hazardous environments.","Controlling a robot to perform a complex or demanding task may overload operators resulting in poor performance.","To design a robot controller to assist the human in executing such challenging tasks, a comprehensive understanding of the interplay between the robot's autonomous behavior and the operator's internal state is essential.","In this paper, we investigate the relationships between robot autonomy and both the human user's cognitive load and trust levels, and the potential existence of three-way interactions in the robot-assisted execution of the task.","Our user study (N=24) results indicate that while autonomy level influences the teleoperator's perceived cognitive load and trust, there is no clear interaction between these factors.","Instead, these elements appear to operate independently, thus highlighting the need to consider both cognitive load and trust as distinct but interrelated factors in varying the robot autonomy level in shared-control settings.","This insight is crucial for the development of more effective and adaptable assistive robotic systems."],"url":"http://arxiv.org/abs/2402.02758v1","category":"cs.RO"}
{"created":"2024-02-05 06:24:58","title":"Heterogeneous Solvent Dissipation Coupled with Particle Rearrangement in Shear Thinning Non-Brownian Suspensions","abstract":"Dense non-Brownian suspensions exhibit significant shear thinning, although a comprehensive understanding of the full scope of this phenomenon remains elusive. This study numerically reveals intimate heterogenous coupled dynamics between many-body particle motions and solvent hydrodynamics in shear-thinning non-Brownian suspensions. We demonstrate the spatially correlated viscous dissipation and particle motions; they share the same characteristic length, which decreases with increasing shear rate. We further show that, at lower shear rates, significant particle density changes are induced against the incompressibility of the solvent, suggesting the cooperative creation and annihilation of gaps and flow channels. We discuss that hydrodynamic interactions may substantially restrict particle rearrangements even in highly dense suspensions, influencing the quantitative aspects of macroscopic rheology.","sentences":["Dense non-Brownian suspensions exhibit significant shear thinning, although a comprehensive understanding of the full scope of this phenomenon remains elusive.","This study numerically reveals intimate heterogenous coupled dynamics between many-body particle motions and solvent hydrodynamics in shear-thinning non-Brownian suspensions.","We demonstrate the spatially correlated viscous dissipation and particle motions; they share the same characteristic length, which decreases with increasing shear rate.","We further show that, at lower shear rates, significant particle density changes are induced against the incompressibility of the solvent, suggesting the cooperative creation and annihilation of gaps and flow channels.","We discuss that hydrodynamic interactions may substantially restrict particle rearrangements even in highly dense suspensions, influencing the quantitative aspects of macroscopic rheology."],"url":"http://arxiv.org/abs/2402.02756v2","category":"cond-mat.soft"}
{"created":"2024-02-05 06:16:40","title":"Fast single pixel modal wavefront sensing using neural networks","abstract":"Dynamic wavefront aberrations negatively impact a wide range of optical applications including astronomy, optical free-space telecommunications and bio-imaging. Wavefront errors can be compensated by an adaptive optics system comprised of a deformable mirror and wavefront sensor connected by a control loop. For satellite optical communications (SatCom), wavefront sensing is particularly challenging due to the rapid wavefront fluctuations induced by strong turbulence and movement of the transmitting satellite across the sky. Existing wavefront sensing techniques require fast cameras (>kHz) that are not widely available at wavelengths suitable for SatCom (e.g., 1550nm and mid-to-long wave infrared). Here, we propose a new wavefront sensing technique that uses a single photodiode and a fast mirror to make phase-diverse intensity measurements of the incoming wavefront. We train neural networks to accurately estimate the input phase given this phase-diverse sub-millisecond intensity trace. Our simulations show that our technique is robust in cases of strong turbulence where previous modal wavefront sensors fail due to modal crosstalk, achieving 99% of the optimal Strehl ratio from a 50-mode correction at a sensing rate of 2kHz. We explore typical cases of turbulence magnitude, sensing speed and noise that might be encountered by such a system.","sentences":["Dynamic wavefront aberrations negatively impact a wide range of optical applications including astronomy, optical free-space telecommunications and bio-imaging.","Wavefront errors can be compensated by an adaptive optics system comprised of a deformable mirror and wavefront sensor connected by a control loop.","For satellite optical communications (SatCom), wavefront sensing is particularly challenging due to the rapid wavefront fluctuations induced by strong turbulence and movement of the transmitting satellite across the sky.","Existing wavefront sensing techniques require fast cameras (>kHz) that are not widely available at wavelengths suitable for SatCom (e.g., 1550nm and mid-to-long wave infrared).","Here, we propose a new wavefront sensing technique that uses a single photodiode and a fast mirror to make phase-diverse intensity measurements of the incoming wavefront.","We train neural networks to accurately estimate the input phase given this phase-diverse sub-millisecond intensity trace.","Our simulations show that our technique is robust in cases of strong turbulence where previous modal wavefront sensors fail due to modal crosstalk, achieving 99% of the optimal Strehl ratio from a 50-mode correction at a sensing rate of 2kHz.","We explore typical cases of turbulence magnitude, sensing speed and noise that might be encountered by such a system."],"url":"http://arxiv.org/abs/2402.02752v1","category":"physics.optics"}
{"created":"2024-02-05 05:59:34","title":"Optimal dynamic climate adaptation pathways: a case study of New York City","abstract":"Assessing climate risk and its potential impacts on our cities and economies is of fundamental importance. Extreme weather events, such as hurricanes, floods, and storm surges can lead to catastrophic damages. We propose a flexible approach based on real options analysis and extreme value theory, which enables the selection of optimal adaptation pathways for a portfolio of climate adaptation projects. We model the severity of extreme sea level events using the block maxima approach from extreme value theory, and then develop a real options framework, factoring in climate change, sea level rise uncertainty, and the growth in asset exposure. We then apply the proposed framework to a real-world problem, considering sea level data as well as different adaptation investment options for New York City. Our research can assist governments and policy makers in taking informed decisions about optimal adaptation pathways and more specifically about reducing flood and storm surge risk in a dynamic settings.","sentences":["Assessing climate risk and its potential impacts on our cities and economies is of fundamental importance.","Extreme weather events, such as hurricanes, floods, and storm surges can lead to catastrophic damages.","We propose a flexible approach based on real options analysis and extreme value theory, which enables the selection of optimal adaptation pathways for a portfolio of climate adaptation projects.","We model the severity of extreme sea level events using the block maxima approach from extreme value theory, and then develop a real options framework, factoring in climate change, sea level rise uncertainty, and the growth in asset exposure.","We then apply the proposed framework to a real-world problem, considering sea level data as well as different adaptation investment options for New York City.","Our research can assist governments and policy makers in taking informed decisions about optimal adaptation pathways and more specifically about reducing flood and storm surge risk in a dynamic settings."],"url":"http://arxiv.org/abs/2402.02745v1","category":"q-fin.RM"}
{"created":"2024-02-05 05:38:50","title":"Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective","abstract":"In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.","sentences":["In recent years, LiDAR-camera fusion models have markedly advanced 3D object detection tasks in autonomous driving.","However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored.","In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset.","Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios.","Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach."],"url":"http://arxiv.org/abs/2402.02738v1","category":"cs.CV"}
{"created":"2024-02-05 05:31:03","title":"Timed-Elastic-Band Based Variable Splitting for Autonomous Trajectory Planning","abstract":"Existing trajectory planning methods are struggling to handle the issue of autonomous track swinging during navigation, resulting in significant errors when reaching the destination. In this article, we address autonomous trajectory planning problems, which aims at developing innovative solutions to enhance the adaptability and robustness of unmanned systems in navigating complex and dynamic environments. We first introduce the variable splitting (VS) method as a constrained optimization method to reimagine the renowned Timed-Elastic-Band (TEB) algorithm, resulting in a novel collision avoidance approach named Timed-Elastic-Band based variable splitting (TEB-VS). The proposed TEB-VS demonstrates superior navigation stability, while maintaining nearly identical resource consumption to TEB. We then analyze the convergence of the proposed TEB-VS method. To evaluate the effectiveness and efficiency of TEB-VS, extensive experiments have been conducted using TurtleBot2 in both simulated environments and real-world datasets.","sentences":["Existing trajectory planning methods are struggling to handle the issue of autonomous track swinging during navigation, resulting in significant errors when reaching the destination.","In this article, we address autonomous trajectory planning problems, which aims at developing innovative solutions to enhance the adaptability and robustness of unmanned systems in navigating complex and dynamic environments.","We first introduce the variable splitting (VS) method as a constrained optimization method to reimagine the renowned Timed-Elastic-Band (TEB) algorithm, resulting in a novel collision avoidance approach named Timed-Elastic-Band based variable splitting (TEB-VS).","The proposed TEB-VS demonstrates superior navigation stability, while maintaining nearly identical resource consumption to TEB.","We then analyze the convergence of the proposed TEB-VS method.","To evaluate the effectiveness and efficiency of TEB-VS, extensive experiments have been conducted using TurtleBot2 in both simulated environments and real-world datasets."],"url":"http://arxiv.org/abs/2402.02735v1","category":"eess.SY"}
{"created":"2024-02-05 04:29:39","title":"Discounted Adaptive Online Prediction","abstract":"Online learning is not always about memorizing everything. Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.","sentences":["Online learning is not always about memorizing everything.","Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in.","To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning.","Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate.","In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning.","We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions."],"url":"http://arxiv.org/abs/2402.02720v1","category":"cs.LG"}
{"created":"2024-02-05 04:16:55","title":"Unconditionally energy stable IEQ-FEMs for the Cahn-Hilliard equation and Allen-Cahn equation","abstract":"In this paper, we present several unconditionally energy-stable invariant energy quadratization (IEQ) finite element methods (FEMs) with linear, first- and second-order accuracy for solving both the Cahn-Hilliard equation and the Allen-Cahn equation. For time discretization, we compare three distinct IEQ-FEM schemes that position the intermediate function introduced by the IEQ approach in different function spaces: finite element space, continuous function space, or a combination of these spaces. Rigorous proofs establishing the existence and uniqueness of the numerical solution, along with analyses of energy dissipation for both equations and mass conservation for the Cahn-Hilliard equation, are provided. The proposed schemes' accuracy, efficiency, and solution properties are demonstrated through numerical experiments.","sentences":["In this paper, we present several unconditionally energy-stable invariant energy quadratization (IEQ) finite element methods (FEMs) with linear, first- and second-order accuracy for solving both the Cahn-Hilliard equation and the Allen-Cahn equation.","For time discretization, we compare three distinct IEQ-FEM schemes that position the intermediate function introduced by the IEQ approach in different function spaces: finite element space, continuous function space, or a combination of these spaces.","Rigorous proofs establishing the existence and uniqueness of the numerical solution, along with analyses of energy dissipation for both equations and mass conservation for the Cahn-Hilliard equation, are provided.","The proposed schemes' accuracy, efficiency, and solution properties are demonstrated through numerical experiments."],"url":"http://arxiv.org/abs/2402.02712v1","category":"math.NA"}
{"created":"2024-02-05 04:07:20","title":"Entangling two exciton modes using exciton optomechanics","abstract":"Exciton optomechanics, bridging cavity exciton polaritons and optomechanics, opens new opportunities for the study of light-matter strong interactions and nonlinearities, due to the rich nonlinear couplings among excitons, phonons, and photons. Here, we propose to entangle two exciton modes in an exciton-optomechanical system, which consists of a semiconductor optomechanical microcavity integrated with two quantum wells. The quantum wells support two exciton modes, which simultaneously couple to an optical cavity mode via a linear coupling, and the cavity mode also couples to a mechanical vibration mode via a dispersive optomechanical interaction, accounting for both the radiation pressure and the photoelastic effect. We show that by strongly driving the microcavity with a red-detuned laser field and when the two exciton modes are respectively resonant with the Stokes and anti-Stokes sidebands scattered by the mechanical motion, stationary entanglement between the two exciton modes can be established under currently available parameters. The entanglement is robust against various dissipations of the system and can be achieved at room temperature for a mechanical quality factor higher than $\\sim10^4$.","sentences":["Exciton optomechanics, bridging cavity exciton polaritons and optomechanics, opens new opportunities for the study of light-matter strong interactions and nonlinearities, due to the rich nonlinear couplings among excitons, phonons, and photons.","Here, we propose to entangle two exciton modes in an exciton-optomechanical system, which consists of a semiconductor optomechanical microcavity integrated with two quantum wells.","The quantum wells support two exciton modes, which simultaneously couple to an optical cavity mode via a linear coupling, and the cavity mode also couples to a mechanical vibration mode via a dispersive optomechanical interaction, accounting for both the radiation pressure and the photoelastic effect.","We show that by strongly driving the microcavity with a red-detuned laser field and when the two exciton modes are respectively resonant with the Stokes and anti-Stokes sidebands scattered by the mechanical motion, stationary entanglement between the two exciton modes can be established under currently available parameters.","The entanglement is robust against various dissipations of the system and can be achieved at room temperature for a mechanical quality factor higher than $\\sim10^4$."],"url":"http://arxiv.org/abs/2402.02710v1","category":"quant-ph"}
{"created":"2024-02-05 03:41:03","title":"Dielectrics for Two-Dimensional Transition Metal Dichalcogenide Applications","abstract":"Despite over a decade of intense research efforts, the full potential of two-dimensional transition metal dichalcogenides continues to be limited by major challenges. The lack of compatible and scalable dielectric materials and integration techniques restrict device performances and their commercial applications Conventional dielectric integration techniques for bulk semiconductors are difficult to adapt for atomically thin two-dimensional materials. This review provides a brief introduction into various common and emerging dielectric synthesis and integration techniques and discusses their applicability for 2D transition metal dichalcogenides. Dielectric integration for various applications is reviewed in subsequent sections including nanoelectronics, optoelectronics, flexible electronics, valleytronics, biosensing, quantum information processing, and quantum sensing. For each application, we introduce basic device working principles, discuss the specific dielectric requirements, review current progress, present key challenges, and offer insights into future prospects and opportunities.","sentences":["Despite over a decade of intense research efforts, the full potential of two-dimensional transition metal dichalcogenides continues to be limited by major challenges.","The lack of compatible and scalable dielectric materials and integration techniques restrict device performances and their commercial applications Conventional dielectric integration techniques for bulk semiconductors are difficult to adapt for atomically thin two-dimensional materials.","This review provides a brief introduction into various common and emerging dielectric synthesis and integration techniques and discusses their applicability for 2D transition metal dichalcogenides.","Dielectric integration for various applications is reviewed in subsequent sections including nanoelectronics, optoelectronics, flexible electronics, valleytronics, biosensing, quantum information processing, and quantum sensing.","For each application, we introduce basic device working principles, discuss the specific dielectric requirements, review current progress, present key challenges, and offer insights into future prospects and opportunities."],"url":"http://arxiv.org/abs/2402.02707v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 03:31:44","title":"Causal inference under transportability assumptions for conditional relative effect measures","abstract":"When extending inferences from a randomized trial to a new target population, an assumption of transportability of difference effect measures (e.g., conditional average treatment effects) -- or even stronger assumptions of transportability in expectation or distribution of potential outcomes -- is invoked to identify the marginal causal mean difference in the target population. However, many clinical investigators believe that relative effect measures conditional on covariates, such as conditional risk ratios and mean ratios, are more likely to be ``transportable'' across populations compared with difference effect measures. Here, we examine the identification and estimation of the marginal counterfactual mean difference and ratio under a transportability assumption for conditional relative effect measures. We obtain identification results for two scenarios that often arise in practice when individuals in the target population (1) only have access to the control treatment, or (2) have access to the control and other treatments but not necessarily the experimental treatment evaluated in the trial. We then propose multiply robust and nonparametric efficient estimators that allow for the use of data-adaptive methods (e.g., machine learning techniques) to model the nuisance parameters. We examine the performance of the methods in simulation studies and illustrate their use with data from two trials of paliperidone for patients with schizophrenia. We conclude that the proposed methods are attractive when background knowledge suggests that the transportability assumption for conditional relative effect measures is more plausible than alternative assumptions.","sentences":["When extending inferences from a randomized trial to a new target population, an assumption of transportability of difference effect measures (e.g., conditional average treatment effects) -- or even stronger assumptions of transportability in expectation or distribution of potential outcomes -- is invoked to identify the marginal causal mean difference in the target population.","However, many clinical investigators believe that relative effect measures conditional on covariates, such as conditional risk ratios and mean ratios, are more likely to be ``transportable'' across populations compared with difference effect measures.","Here, we examine the identification and estimation of the marginal counterfactual mean difference and ratio under a transportability assumption for conditional relative effect measures.","We obtain identification results for two scenarios that often arise in practice when individuals in the target population (1) only have access to the control treatment, or (2) have access to the control and other treatments but not necessarily the experimental treatment evaluated in the trial.","We then propose multiply robust and nonparametric efficient estimators that allow for the use of data-adaptive methods (e.g., machine learning techniques) to model the nuisance parameters.","We examine the performance of the methods in simulation studies and illustrate their use with data from two trials of paliperidone for patients with schizophrenia.","We conclude that the proposed methods are attractive when background knowledge suggests that the transportability assumption for conditional relative effect measures is more plausible than alternative assumptions."],"url":"http://arxiv.org/abs/2402.02702v1","category":"stat.ME"}
{"created":"2024-02-05 03:00:08","title":"ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module ensuring easy portability and remote tracking of operational logistics to the last mile","abstract":"The COVID-19 pandemic has profoundly reshaped our lives, prompting a search for solutions to its far-reaching effects. Vaccines emerged as a beacon of hope, yet reaching remote areas faces last-mile hurdles and cost issues due to loss of vaccine potency due to poor temperature regulation of the storage units and unanticipated vaccine wastage en route, a common occurrence in conventional vaccine transportation methods. We introduce ALIVE, a low-cost Interactive Vaccine Storage Environment module. ALIVE provides an off-grid, self-sufficient solution for vaccine storage and transport, enabled by active cooling technology. ALIVE's innovation lies in its integration with the Internet of Things (IoT), allowing real-time monitoring and control. This IoT-enabled Application Programming Interface (API) features a data acquisition and environment parameter control system, managing oversight and decision-making. ALIVE's compact, lightweight design makes it adaptable to various logistical scenarios, while its versatility enables it to maintain both time-invariant and time-dependent thermophysical and spatial parameters. Operationalized through a PID algorithm, ALIVE ensures precise temperature control within the vaccine chamber. Its dynamic features, such as remote actuation and data sharing, demonstrate its adaptability and potential applications. Despite the frugal nature of development, the system promises significant benefits, including reduced vaccine loss and remote monitoring advantages. Collaborations with healthcare partners seek to further enhance ALIVE's readiness and expand its impact. ALIVE revolutionizes vaccine logistics, offering scalable, cost-effective solutions for bridging accessibility gaps in challenging distribution scenarios. Its adaptability positions it for widespread application, from last-mile vaccine delivery to environment-controlled supply chains and beyond.","sentences":["The COVID-19 pandemic has profoundly reshaped our lives, prompting a search for solutions to its far-reaching effects.","Vaccines emerged as a beacon of hope, yet reaching remote areas faces last-mile hurdles and cost issues due to loss of vaccine potency due to poor temperature regulation of the storage units and unanticipated vaccine wastage en route, a common occurrence in conventional vaccine transportation methods.","We introduce ALIVE, a low-cost Interactive Vaccine Storage Environment module.","ALIVE provides an off-grid, self-sufficient solution for vaccine storage and transport, enabled by active cooling technology.","ALIVE's innovation lies in its integration with the Internet of Things (IoT), allowing real-time monitoring and control.","This IoT-enabled Application Programming Interface (API) features a data acquisition and environment parameter control system, managing oversight and decision-making.","ALIVE's compact, lightweight design makes it adaptable to various logistical scenarios, while its versatility enables it to maintain both time-invariant and time-dependent thermophysical and spatial parameters.","Operationalized through a PID algorithm, ALIVE ensures precise temperature control within the vaccine chamber.","Its dynamic features, such as remote actuation and data sharing, demonstrate its adaptability and potential applications.","Despite the frugal nature of development, the system promises significant benefits, including reduced vaccine loss and remote monitoring advantages.","Collaborations with healthcare partners seek to further enhance ALIVE's readiness and expand its impact.","ALIVE revolutionizes vaccine logistics, offering scalable, cost-effective solutions for bridging accessibility gaps in challenging distribution scenarios.","Its adaptability positions it for widespread application, from last-mile vaccine delivery to environment-controlled supply chains and beyond."],"url":"http://arxiv.org/abs/2402.02691v1","category":"eess.SY"}
{"created":"2024-02-05 02:43:18","title":"Efficient estimation of subgroup treatment effects using multi-source data","abstract":"Investigators often use multi-source data (e.g., multi-center trials, meta-analyses of randomized trials, pooled analyses of observational cohorts) to learn about the effects of interventions in subgroups of some well-defined target population. Such a target population can correspond to one of the data sources of the multi-source data or an external population in which the treatment and outcome information may not be available. We develop and evaluate methods for using multi-source data to estimate subgroup potential outcome means and treatment effects in a target population. We consider identifiability conditions and propose doubly robust estimators that, under mild conditions, are non-parametrically efficient and allow for nuisance functions to be estimated using flexible data-adaptive methods (e.g., machine learning techniques). We also show how to construct confidence intervals and simultaneous confidence bands for the estimated subgroup treatment effects. We examine the properties of the proposed estimators in simulation studies and compare performance against alternative estimators. We also conclude that our methods work well when the sample size of the target population is much larger than the sample size of the multi-source data. We illustrate the proposed methods in a meta-analysis of randomized trials for schizophrenia.","sentences":["Investigators often use multi-source data (e.g., multi-center trials, meta-analyses of randomized trials, pooled analyses of observational cohorts) to learn about the effects of interventions in subgroups of some well-defined target population.","Such a target population can correspond to one of the data sources of the multi-source data or an external population in which the treatment and outcome information may not be available.","We develop and evaluate methods for using multi-source data to estimate subgroup potential outcome means and treatment effects in a target population.","We consider identifiability conditions and propose doubly robust estimators that, under mild conditions, are non-parametrically efficient and allow for nuisance functions to be estimated using flexible data-adaptive methods (e.g., machine learning techniques).","We also show how to construct confidence intervals and simultaneous confidence bands for the estimated subgroup treatment effects.","We examine the properties of the proposed estimators in simulation studies and compare performance against alternative estimators.","We also conclude that our methods work well when the sample size of the target population is much larger than the sample size of the multi-source data.","We illustrate the proposed methods in a meta-analysis of randomized trials for schizophrenia."],"url":"http://arxiv.org/abs/2402.02684v1","category":"stat.ME"}
{"created":"2024-02-05 02:02:57","title":"On the integrability of spinning-body dynamics around black holes","abstract":"In general relativity, the trajectory of a celestial body in a given spacetime is influenced by its proper rotation, or \\textit{spin}. We present a covariant and physically self-consistent Hamiltonian framework to study this motion, at linear order in the body's spin and in an arbitrary fixed spacetime. The choice of center-of-mass and degeneracies coming from Lorentz invariance are treated rigorously with adapted tools from Hamiltonian mechanics. Applying the formalism to a background space-time described by the Kerr metric, we prove that the motion of a spinning body around a generic rotating black hole is an \\textit{integrable} Hamiltonian system. In particular, linear-in-spin effects do not break the integrability of Kerr geodesics, and induce no \\textit{chaos} within the associated phase space. Our findings suggest a natural way to improve current gravitational waveform modelling for asymmetric binary systems, and provide a mean to extend classical features of Kerr geodesics to linear-in-spin trajectories.","sentences":["In general relativity, the trajectory of a celestial body in a given spacetime is influenced by its proper rotation, or \\textit{spin}.","We present a covariant and physically self-consistent Hamiltonian framework to study this motion, at linear order in the body's spin and in an arbitrary fixed spacetime.","The choice of center-of-mass and degeneracies coming from Lorentz invariance are treated rigorously with adapted tools from Hamiltonian mechanics.","Applying the formalism to a background space-time described by the Kerr metric, we prove that the motion of a spinning body around a generic rotating black hole is an \\textit{integrable} Hamiltonian system.","In particular, linear-in-spin effects do not break the integrability of Kerr geodesics, and induce no \\textit{chaos} within the associated phase space.","Our findings suggest a natural way to improve current gravitational waveform modelling for asymmetric binary systems, and provide a mean to extend classical features of Kerr geodesics to linear-in-spin trajectories."],"url":"http://arxiv.org/abs/2402.02670v1","category":"gr-qc"}
{"created":"2024-02-05 01:50:04","title":"A Priori Error Estimation of Physics-Informed Neural Networks Solving Allen--Cahn and Cahn--Hilliard Equations","abstract":"This paper aims to analyze errors in the implementation of the Physics-Informed Neural Network (PINN) for solving the Allen--Cahn (AC) and Cahn--Hilliard (CH) partial differential equations (PDEs). The accuracy of PINN is still challenged when dealing with strongly non-linear and higher-order time-varying PDEs. To address this issue, we introduce a stable and bounded self-adaptive weighting scheme known as Residuals-RAE, which ensures fair training and effectively captures the solution. By incorporating this new training loss function, we conduct numerical experiments on 1D and 2D AC and CH systems to validate our theoretical findings. Our theoretical analysis demonstrates that feedforward neural networks with two hidden layers and tanh activation function effectively bound the PINN approximation errors for the solution field, temporal derivative, and nonlinear term of the AC and CH equations by the training loss and number of collocation points.","sentences":["This paper aims to analyze errors in the implementation of the Physics-Informed Neural Network (PINN) for solving the Allen--Cahn (AC) and Cahn--Hilliard (CH) partial differential equations (PDEs).","The accuracy of PINN is still challenged when dealing with strongly non-linear and higher-order time-varying PDEs.","To address this issue, we introduce a stable and bounded self-adaptive weighting scheme known as Residuals-RAE, which ensures fair training and effectively captures the solution.","By incorporating this new training loss function, we conduct numerical experiments on 1D and 2D AC and CH systems to validate our theoretical findings.","Our theoretical analysis demonstrates that feedforward neural networks with two hidden layers and tanh activation function effectively bound the PINN approximation errors for the solution field, temporal derivative, and nonlinear term of the AC and CH equations by the training loss and number of collocation points."],"url":"http://arxiv.org/abs/2402.02667v1","category":"math.NA"}
{"created":"2024-02-05 00:44:57","title":"Densely Decoded Networks with Adaptive Deep Supervision for Medical Image Segmentation","abstract":"Medical image segmentation using deep neural networks has been highly successful. However, the effectiveness of these networks is often limited by inadequate dense prediction and inability to extract robust features. To achieve refined dense prediction, we propose densely decoded networks (ddn), by selectively introducing 'crutch' network connections. Such 'crutch' connections in each upsampling stage of the network decoder (1) enhance target localization by incorporating high resolution features from the encoder, and (2) improve segmentation by facilitating multi-stage contextual information flow. Further, we present a training strategy based on adaptive deep supervision (ads), which exploits and adapts specific attributes of input dataset, for robust feature extraction. In particular, ads strategically locates and deploys auxiliary supervision, by matching the average input object size with the layer-wise effective receptive fields (lerf) of a network, resulting in a class of ddns. Such inclusion of 'companion objective' from a specific hidden layer, helps the model pay close attention to some distinct input-dependent features, which the network might otherwise 'ignore' during training. Our new networks and training strategy are validated on 4 diverse datasets of different modalities, demonstrating their effectiveness.","sentences":["Medical image segmentation using deep neural networks has been highly successful.","However, the effectiveness of these networks is often limited by inadequate dense prediction and inability to extract robust features.","To achieve refined dense prediction, we propose densely decoded networks (ddn), by selectively introducing 'crutch' network connections.","Such 'crutch' connections in each upsampling stage of the network decoder (1) enhance target localization by incorporating high resolution features from the encoder, and (2) improve segmentation by facilitating multi-stage contextual information flow.","Further, we present a training strategy based on adaptive deep supervision (ads), which exploits and adapts specific attributes of input dataset, for robust feature extraction.","In particular, ads strategically locates and deploys auxiliary supervision, by matching the average input object size with the layer-wise effective receptive fields (lerf) of a network, resulting in a class of ddns.","Such inclusion of 'companion objective' from a specific hidden layer, helps the model pay close attention to some distinct input-dependent features, which the network might otherwise 'ignore' during training.","Our new networks and training strategy are validated on 4 diverse datasets of different modalities, demonstrating their effectiveness."],"url":"http://arxiv.org/abs/2402.02649v1","category":"cs.CV"}
{"created":"2024-02-05 00:36:20","title":"A multi-objective optimization framework for reducing the impact of ship noise on marine mammals","abstract":"The underwater radiated noise (URN) emanating from ships presents a significant threat to marine mammals, given their heavy reliance on hearing for essential life activities. The intensity of URN from ships is directly correlated to the speed, making speed reduction a crucial operational mitigation strategy. This paper presents a new multi-objective optimization framework to optimize the ship speed for effective URN mitigation without compromising fuel consumption. The proposed framework addresses a fixed-path voyage scheduling problem, incorporating two objective functions namely (i) noise intensity levels and (ii) fuel consumption. The optimization is performed using the state-of-the-art non-dominated sorting genetic algorithm under voyage constraints. A 2D ocean acoustic environment, comprising randomly scattered marine mammals of diverse audiogram groups and realistic conditions, including sound speed profiles and bathymetry, is simulated. To estimate the objective functions, we consider empirical relations for fuel consumption and near-field noise modeling together with a ray-tracing approach for far-field noise propagation. The optimization problem is solved using the genetic algorithm to determine the Pareto solutions and subsequently the trade-off solution. The effectiveness of the optimization framework is demonstrated via both simplified tests and practical case studies involving a large container ship. A comparative analysis illustrates the adaptability of the optimization framework across different oceanic environments, affirming its potential as a robust tool for reducing the URN from shipping.","sentences":["The underwater radiated noise (URN) emanating from ships presents a significant threat to marine mammals, given their heavy reliance on hearing for essential life activities.","The intensity of URN from ships is directly correlated to the speed, making speed reduction a crucial operational mitigation strategy.","This paper presents a new multi-objective optimization framework to optimize the ship speed for effective URN mitigation without compromising fuel consumption.","The proposed framework addresses a fixed-path voyage scheduling problem, incorporating two objective functions namely (i) noise intensity levels and (ii) fuel consumption.","The optimization is performed using the state-of-the-art non-dominated sorting genetic algorithm under voyage constraints.","A 2D ocean acoustic environment, comprising randomly scattered marine mammals of diverse audiogram groups and realistic conditions, including sound speed profiles and bathymetry, is simulated.","To estimate the objective functions, we consider empirical relations for fuel consumption and near-field noise modeling together with a ray-tracing approach for far-field noise propagation.","The optimization problem is solved using the genetic algorithm to determine the Pareto solutions and subsequently the trade-off solution.","The effectiveness of the optimization framework is demonstrated via both simplified tests and practical case studies involving a large container ship.","A comparative analysis illustrates the adaptability of the optimization framework across different oceanic environments, affirming its potential as a robust tool for reducing the URN from shipping."],"url":"http://arxiv.org/abs/2402.02647v1","category":"math.OC"}
{"created":"2024-02-04 23:42:02","title":"LLM-Enhanced Data Management","abstract":"Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.","sentences":["Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years.","However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context).","Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis, database tuning).","However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks.","To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy.","LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering.","LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities.","LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions.","We showcase three real-world scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics.","We also summarize the open research challenges of LLMDB."],"url":"http://arxiv.org/abs/2402.02643v1","category":"cs.DB"}
{"created":"2024-02-04 22:47:34","title":"Learning to Understand: Identifying Interactions via the Mobius Transform","abstract":"One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn. The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables. The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance. This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs. When there are $K = O(2^{n \\delta})$ with $\\delta \\leq \\frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \\rightarrow \\infty$, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group testing and the Mobius transform. In the case where all interactions are between at most $t = \\Theta(n^{\\alpha})$ inputs, for $\\alpha < 0.409$, we are able to leverage results from group testing to provide the first algorithm that computes the Mobius transform in $O(Kt\\log n)$ sample complexity and $O(K\\mathrm{poly}(n))$ time with vanishing error as $K \\rightarrow \\infty$. Finally, we present a robust version of this algorithm that achieves the same sample and time complexity under some assumptions, but with a factor depending on noise variance. Our work is deeply interdisciplinary, drawing from tools spanning across signal processing, algebra, information theory, learning theory and group testing to address this important problem at the forefront of machine learning.","sentences":["One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn.","The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables.","The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance.","This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs.","When there are $K = O(2^{n \\delta})$ with $\\delta \\leq \\frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \\rightarrow \\infty$, the first non-adaptive algorithm to do so.","We also uncover a surprising connection between group testing and the Mobius transform.","In the case where all interactions are between at most $t = \\Theta(n^{\\alpha})$ inputs, for $\\alpha < 0.409$, we are able to leverage results from group testing to provide the first algorithm that computes the Mobius transform in $O(Kt\\log n)$ sample complexity and $O(K\\mathrm{poly}(n))$ time with vanishing error as $K \\rightarrow \\infty$. Finally, we present a robust version of this algorithm that achieves the same sample and time complexity under some assumptions, but with a factor depending on noise variance.","Our work is deeply interdisciplinary, drawing from tools spanning across signal processing, algebra, information theory, learning theory and group testing to address this important problem at the forefront of machine learning."],"url":"http://arxiv.org/abs/2402.02631v1","category":"cs.LG"}
{"created":"2024-02-04 22:09:28","title":"A Safe Reinforcement Learning driven Weights-varying Model Predictive Control for Autonomous Vehicle Motion Control","abstract":"Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights. However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime. Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states. To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight sets. We conceive a RL agent not to learn in a continuous space but to proactively anticipate upcoming control tasks and to choose the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, context-dependent. Hence, even an untrained RL agent guarantees a safe and optimal performance. Experimental results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front.","sentences":["Determining the optimal cost function parameters of Model Predictive Control (MPC) to optimize multiple control objectives is a challenging and time-consuming task.","Multiobjective Bayesian Optimization (BO) techniques solve this problem by determining a Pareto optimal parameter set for an MPC with static weights.","However, a single parameter set may not deliver the most optimal closed-loop control performance when the context of the MPC operating conditions changes during its operation, urging the need to adapt the cost function weights at runtime.","Deep Reinforcement Learning (RL) algorithms can automatically learn context-dependent optimal parameter sets and dynamically adapt for a Weightsvarying MPC (WMPC).","However, learning cost function weights from scratch in a continuous action space may lead to unsafe operating states.","To solve this, we propose a novel approach limiting the RL actions within a safe learning space representing a catalog of pre-optimized BO Pareto-optimal weight sets.","We conceive a RL agent not to learn in a continuous space but to proactively anticipate upcoming control tasks and to choose the most optimal discrete actions, each corresponding to a single set of Pareto optimal weights, context-dependent.","Hence, even an untrained RL agent guarantees a safe and optimal performance.","Experimental results demonstrate that an untrained RL-WMPC shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps exhibit a performance beyond the Pareto-front."],"url":"http://arxiv.org/abs/2402.02624v1","category":"cs.RO"}
{"created":"2024-02-04 20:29:43","title":"A Review of Full-Sized Autonomous Racing Vehicle Sensor Architecture","abstract":"In the landscape of technological innovation, autonomous racing is a dynamic and challenging domain that not only pushes the limits of technology, but also plays a crucial role in advancing and fostering a greater acceptance of autonomous systems. This paper thoroughly explores challenges and advances in autonomous racing vehicle design and performance, focusing on Roborace and the Indy Autonomous Challenge (IAC). This review provides a detailed analysis of sensor setups, architectural nuances, and test metrics on these cutting-edge platforms. In Roborace, the evolution from Devbot 1.0 to Robocar and Devbot 2.0 is detailed, revealing insights into sensor configurations and performance outcomes. The examination extends to the IAC, which is dedicated to high-speed self-driving vehicles, emphasizing developmental trajectories and sensor adaptations. By reviewing these platforms, the analysis provides valuable insight into autonomous driving racing, contributing to a broader understanding of sensor architectures and the challenges faced. This review supports future advances in full-scale autonomous racing technology.","sentences":["In the landscape of technological innovation, autonomous racing is a dynamic and challenging domain that not only pushes the limits of technology, but also plays a crucial role in advancing and fostering a greater acceptance of autonomous systems.","This paper thoroughly explores challenges and advances in autonomous racing vehicle design and performance, focusing on Roborace and the Indy Autonomous Challenge (IAC).","This review provides a detailed analysis of sensor setups, architectural nuances, and test metrics on these cutting-edge platforms.","In Roborace, the evolution from Devbot 1.0 to Robocar and Devbot 2.0 is detailed, revealing insights into sensor configurations and performance outcomes.","The examination extends to the IAC, which is dedicated to high-speed self-driving vehicles, emphasizing developmental trajectories and sensor adaptations.","By reviewing these platforms, the analysis provides valuable insight into autonomous driving racing, contributing to a broader understanding of sensor architectures and the challenges faced.","This review supports future advances in full-scale autonomous racing technology."],"url":"http://arxiv.org/abs/2402.02603v1","category":"cs.RO"}
{"created":"2024-02-04 19:01:27","title":"Grover-QAOA for 3-SAT: Quadratic Speedup, Fair-Sampling, and Parameter Clustering","abstract":"The SAT problem is a prototypical NP-complete problem of fundamental importance in computational complexity theory with many applications in science and engineering; as such, it has long served as an essential benchmark for classical and quantum algorithms. This study shows numerical evidence for a quadratic speedup of the Grover Quantum Approximate Optimization Algorithm (G-QAOA) over random sampling for finding all solutions to 3-SAT problems (All-SAT). G-QAOA is less resource-intensive and more adaptable for 3-SAT and Max-SAT than Grover's algorithm, and it surpasses conventional QAOA in its ability to sample all solutions. We show these benefits by classical simulations of many-round G-QAOA on thousands of random 3-SAT instances. We also observe G-QAOA advantages on the IonQ Aria quantum computer for small instances, finding that current hardware suffices to determine and sample all solutions. Interestingly, a single-angle-pair constraint that uses the same pair of angles at each G-QAOA round greatly reduces the classical computational overhead of optimizing the G-QAOA angles while preserving its quadratic speedup. We also find parameter clustering of the angles. The single-angle-pair protocol and parameter clustering significantly reduce obstacles to classical optimization of the G-QAOA angles.","sentences":["The SAT problem is a prototypical NP-complete problem of fundamental importance in computational complexity theory with many applications in science and engineering; as such, it has long served as an essential benchmark for classical and quantum algorithms.","This study shows numerical evidence for a quadratic speedup of the Grover Quantum Approximate Optimization Algorithm (G-QAOA) over random sampling for finding all solutions to 3-SAT problems (All-SAT).","G-QAOA is less resource-intensive and more adaptable for 3-SAT and Max-SAT than Grover's algorithm, and it surpasses conventional QAOA in its ability to sample all solutions.","We show these benefits by classical simulations of many-round G-QAOA on thousands of random 3-SAT instances.","We also observe G-QAOA advantages on the IonQ Aria quantum computer for small instances, finding that current hardware suffices to determine and sample all solutions.","Interestingly, a single-angle-pair constraint that uses the same pair of angles at each G-QAOA round greatly reduces the classical computational overhead of optimizing the G-QAOA angles while preserving its quadratic speedup.","We also find parameter clustering of the angles.","The single-angle-pair protocol and parameter clustering significantly reduce obstacles to classical optimization of the G-QAOA angles."],"url":"http://arxiv.org/abs/2402.02585v1","category":"quant-ph"}
{"created":"2024-02-04 18:53:11","title":"SYK Correlators from 2D Liouville-de Sitter Gravity","abstract":"We introduce and study a candidate gravity dual to the double scaled SYK model in the form of an exactly soluble 2D de Sitter gravity model consisting of two spacelike Liouville CFTs with complex central charge adding up to $c_+ + c_- = 26$. In [1] it was shown that the two-point function of physical operators in a doubled SYK model matches in the semi-classical limit with the Green's function of a massive scalar field in 3D de Sitter space. As further evidence of the duality, we adapt a result from Zamolodchikov to compute the boundary two-point function of the 2D Liouville-de Sitter gravity model on a disk and find that it reproduces the exact DSSYK two-point function to all orders in $\\lambda=p^2/N$. We describe how the 2D Liouville-de Sitter gravity model arises from quantizing 3D de Sitter gravity.","sentences":["We introduce and study a candidate gravity dual to the double scaled SYK model in the form of an exactly soluble 2D de Sitter gravity model consisting of two spacelike Liouville CFTs with complex central charge adding up to $c_+ + c_- = 26$.","In [1] it was shown that the two-point function of physical operators in a doubled SYK model matches in the semi-classical limit with the Green's function of a massive scalar field in 3D de Sitter space.","As further evidence of the duality, we adapt a result from Zamolodchikov to compute the boundary two-point function of the 2D Liouville-de Sitter gravity model on a disk and find that it reproduces the exact DSSYK two-point function to all orders in $\\lambda=p^2/N$. We describe how the 2D Liouville-de Sitter gravity model arises from quantizing 3D de Sitter gravity."],"url":"http://arxiv.org/abs/2402.02584v1","category":"hep-th"}
{"created":"2024-02-04 17:05:27","title":"STAGE: Scalable and Traversability-Aware Graph based Exploration Planner for Dynamically Varying Environments","abstract":"In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas. The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively. The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning. Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs. Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer. Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable. This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions. As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph. Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor.","sentences":["In this article, we propose a novel navigation framework that leverages a two layered graph representation of the environment for efficient large-scale exploration, while it integrates a novel uncertainty awareness scheme to handle dynamic scene changes in previously explored areas.","The framework is structured around a novel goal oriented graph representation, that consists of, i) the local sub-graph and ii) the global graph layer respectively.","The local sub-graphs encode local volumetric gain locations as frontiers, based on the direct pointcloud visibility, allowing fast graph building and path planning.","Additionally, the global graph is build in an efficient way, using node-edge information exchange only on overlapping regions of sequential sub-graphs.","Different from the state-of-the-art graph based exploration methods, the proposed approach efficiently re-uses sub-graphs built in previous iterations to construct the global navigation layer.","Another merit of the proposed scheme is the ability to handle scene changes (e.g. blocked pathways), adaptively updating the obstructed part of the global graph from traversable to not-traversable.","This operation involved oriented sample space of a path segment in the global graph layer, while removing the respective edges from connected nodes of the global graph in cases of obstructions.","As such, the exploration behavior is directing the robot to follow another route in the global re-positioning phase through path-way updates in the global graph.","Finally, we showcase the performance of the method both in simulation runs as well as deployed in real-world scene involving a legged robot carrying camera and lidar sensor."],"url":"http://arxiv.org/abs/2402.02566v1","category":"cs.RO"}
{"created":"2024-02-04 15:54:03","title":"Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators","abstract":"In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimize the time required to reach, time taken to stabilize, the maximum deviation from the desired value, and persistent tracking error in the steady state. This approach guarantees that position and velocity errors exponentially converge to zero in an unfamiliar environment, despite unknown robotic manipulator modeling. Theoretical assertions are validated through the presentation of simulation outcomes.","sentences":["In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety.","To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low-","and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment.","This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks.","First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning.","This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimize the time required to reach, time taken to stabilize, the maximum deviation from the desired value, and persistent tracking error in the steady state.","This approach guarantees that position and velocity errors exponentially converge to zero in an unfamiliar environment, despite unknown robotic manipulator modeling.","Theoretical assertions are validated through the presentation of simulation outcomes."],"url":"http://arxiv.org/abs/2402.02551v2","category":"cs.RO"}
{"created":"2024-02-04 15:50:42","title":"Integration of cognitive tasks into artificial general intelligence test for large models","abstract":"During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities. The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifaceted social scenarios; and embodied intelligence, denoting the ability to interact with its physical environment. To assess the multidimensional intelligence of large models, the AGI test consists of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community. We propose that the complexity of AGI testing tasks should increase commensurate with the advancements in large models. We underscore the necessity for the interpretation of test results to avoid false negatives and false positives. We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.","sentences":["During the evolution of large models, performance evaluation is necessarily performed on the intermediate models to assess their capabilities, and on the well-trained model to ensure safety before practical application.","However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models.","In this perspective, we advocate for a comprehensive framework of artificial general intelligence (AGI) test, aimed at fulfilling the testing needs of large language models and multi-modal large models with enhanced capabilities.","The AGI test framework bridges cognitive science and natural language processing to encompass the full spectrum of intelligence facets, including crystallized intelligence, a reflection of amassed knowledge and experience; fluid intelligence, characterized by problem-solving and adaptive reasoning; social intelligence, signifying comprehension and adaptation within multifaceted social scenarios; and embodied intelligence, denoting the ability to interact with its physical environment.","To assess the multidimensional intelligence of large models, the AGI test consists of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community.","We propose that the complexity of AGI testing tasks should increase commensurate with the advancements in large models.","We underscore the necessity for the interpretation of test results to avoid false negatives and false positives.","We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society."],"url":"http://arxiv.org/abs/2402.02547v1","category":"cs.AI"}
{"created":"2024-02-04 15:27:46","title":"Identifying and Extracting Pedestrian Behavior in Critical Traffic Situations","abstract":"A better understanding of interactive pedestrian behavior in critical traffic situations is essential for the development of enhanced pedestrian safety systems. Real-world traffic observations play a decisive role in this, since they represent behavior in an unbiased way. In this work, we present an approach of how a subset of very considerable pedestrian-vehicle interactions can be derived from a camera-based observation system. For this purpose, we have examined road user trajectories automatically for establishing temporal and spatial relationships, using 110h hours of video recordings. In order to identify critical interactions, our approach combines the metric post-encroachment time with a newly introduced motion adaption metric. From more than 11,000 reconstructed pedestrian trajectories, 259 potential scenarios remained, using a post-encroachment time threshold of 2s. However, in 95% of cases, no adaptation of the pedestrian behavior was observed due to avoiding criticality. Applying the proposed motion adaption metric, only 21 critical scenarios remained. Manual investigations revealed that critical pedestrian vehicle interactions were present in 7 of those. They were further analyzed and made publicly available for developing pedestrian behavior models3. The results indicate that critical interactions in which the pedestrian perceives and reacts to the vehicle at a relatively late stage can be extracted using the proposed method.","sentences":["A better understanding of interactive pedestrian behavior in critical traffic situations is essential for the development of enhanced pedestrian safety systems.","Real-world traffic observations play a decisive role in this, since they represent behavior in an unbiased way.","In this work, we present an approach of how a subset of very considerable pedestrian-vehicle interactions can be derived from a camera-based observation system.","For this purpose, we have examined road user trajectories automatically for establishing temporal and spatial relationships, using 110h hours of video recordings.","In order to identify critical interactions, our approach combines the metric post-encroachment time with a newly introduced motion adaption metric.","From more than 11,000 reconstructed pedestrian trajectories, 259 potential scenarios remained, using a post-encroachment time threshold of 2s.","However, in 95% of cases, no adaptation of the pedestrian behavior was observed due to avoiding criticality.","Applying the proposed motion adaption metric, only 21 critical scenarios remained.","Manual investigations revealed that critical pedestrian vehicle interactions were present in 7 of those.","They were further analyzed and made publicly available for developing pedestrian behavior models3.","The results indicate that critical interactions in which the pedestrian perceives and reacts to the vehicle at a relatively late stage can be extracted using the proposed method."],"url":"http://arxiv.org/abs/2402.02533v1","category":"cs.RO"}
{"created":"2024-02-04 15:10:34","title":"Absolute convergence and error thresholds in non-active adaptive sampling","abstract":"Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size. In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described. We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection. The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme. Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study.","sentences":["Non-active adaptive sampling is a way of building machine learning models from a training data base which are supposed to dynamically and automatically derive guaranteed sample size.","In this context and regardless of the strategy used in both scheduling and generating of weak predictors, a proposal for calculating absolute convergence and error thresholds is described.","We not only make it possible to establish when the quality of the model no longer increases, but also supplies a proximity condition to estimate in absolute terms how close it is to achieving such a goal, thus supporting decision making for fine-tuning learning parameters in model selection.","The technique proves its correctness and completeness with respect to our working hypotheses, in addition to strengthening the robustness of the sampling scheme.","Tests meet our expectations and illustrate the proposal in the domain of natural language processing, taking the generation of part-of-speech taggers as case study."],"url":"http://arxiv.org/abs/2402.02522v1","category":"cs.CL"}
{"created":"2024-02-04 15:08:02","title":"A minimal model of cognition based on oscillatory and reinforcement processes","abstract":"Building mathematical models of brains is difficult because of the sheer complexity of the problem. One potential approach is to start by identifying models of basal cognition, which give an abstract representation of a range organisms without central nervous systems, including fungi, slime moulds and bacteria. We propose one such model, demonstrating how a combination of oscillatory and current-based reinforcement processes can be used to couple resources in an efficient manner. We first show that our model connects resources in an efficient manner when the environment is constant. We then show that in an oscillatory environment our model builds efficient solutions, provided the environmental oscillations are sufficiently out of phase. We show that amplitude differences can promote efficient solutions and that the system is robust to frequency differences. We identify connections between our model and basal cognition in biological systems and slime moulds, in particular, showing how oscillatory and problem-solving properties of these systems are captured by our model.","sentences":["Building mathematical models of brains is difficult because of the sheer complexity of the problem.","One potential approach is to start by identifying models of basal cognition, which give an abstract representation of a range organisms without central nervous systems, including fungi, slime moulds and bacteria.","We propose one such model, demonstrating how a combination of oscillatory and current-based reinforcement processes can be used to couple resources in an efficient manner.","We first show that our model connects resources in an efficient manner when the environment is constant.","We then show that in an oscillatory environment our model builds efficient solutions, provided the environmental oscillations are sufficiently out of phase.","We show that amplitude differences can promote efficient solutions and that the system is robust to frequency differences.","We identify connections between our model and basal cognition in biological systems and slime moulds, in particular, showing how oscillatory and problem-solving properties of these systems are captured by our model."],"url":"http://arxiv.org/abs/2402.02520v1","category":"q-bio.NC"}
{"created":"2024-02-04 15:02:17","title":"Adaptive scheduling for adaptive sampling in POS taggers construction","abstract":"We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers. The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration. In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time. The algorithm proves to be formally correct regarding our working hypotheses. Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition. We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematurely.   The proposal has been evaluated on the basis of its reliability to identify the convergence of models, corroborating our expectations. While a concrete halting condition is used for testing, users can choose any condition whatsoever to suit their own specific needs.","sentences":["We introduce an adaptive scheduling for adaptive sampling as a novel way of machine learning in the construction of part-of-speech taggers.","The goal is to speed up the training on large data sets, without significant loss of performance with regard to an optimal configuration.","In contrast to previous methods using a random, fixed or regularly rising spacing between the instances, ours analyzes the shape of the learning curve geometrically in conjunction with a functional model to increase or decrease it at any time.","The algorithm proves to be formally correct regarding our working hypotheses.","Namely, given a case, the following one is the nearest ensuring a net gain of learning ability from the former, it being possible to modulate the level of requirement for this condition.","We also improve the robustness of sampling by paying greater attention to those regions of the training data base subject to a temporary inflation in performance, thus preventing the learning from stopping prematurely.   ","The proposal has been evaluated on the basis of its reliability to identify the convergence of models, corroborating our expectations.","While a concrete halting condition is used for testing, users can choose any condition whatsoever to suit their own specific needs."],"url":"http://arxiv.org/abs/2402.02516v1","category":"cs.CL"}
{"created":"2024-02-04 14:51:49","title":"PoCo: Policy Composition from and for Heterogeneous Robot Learning","abstract":"Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real robot data and evaluate in tool-use tasks. The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments. See https://liruiw.github.io/policycomp for more details .","sentences":["Training general robotic policies from heterogeneous data for different tasks is a significant challenge.","Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos.","Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult.","In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models.","Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time.","We train our method on simulation, human, and real robot data and evaluate in tool-use tasks.","The composed policy achieves robust and dexterous performance under varying scenes and tasks and outperforms baselines from a single data source in both simulation and real-world experiments.","See https://liruiw.github.io/policycomp for more details ."],"url":"http://arxiv.org/abs/2402.02511v1","category":"cs.RO"}
{"created":"2024-02-04 13:46:03","title":"Indicator Random Processes and its Application for Modeling Open Stochastic Systems","abstract":"The authors present a method of indicator random processes, applicable to constructing models of jump processes associated with diffusion process. Indicator random processes are processes that take only two values: 1 and 0, in accordance with some probabilistic laws. It is shown that the indicator random process is invariant when reduced to an arbitrary positive degree. Equations with random coefficients used in modeling dynamic systems, when applying the method of indicator random processes, can take into account the possibility of adaptation to external changes, including random ones, in order to preserve indicators important for the existence of the system, which can be continuous or discrete. In the case of indicator random processes, defined as functions of the Poisson process, equations for dynamic processes in a media with abruptly changing properties are constructed and studied. To study the capabilities of the proposed method, dynamic models of the diffusion process in media with delay centers and diffusion processes during transitions by switching from one subspace to another were studied. For these models, equations for characteristic functions are constructed. Using the method of indicator random processes, a characteristic function for the Kac model was constructed. It is shown that in the case of dependence of the indicator random process on the Poisson process, the equation for the characteristic function corresponds to the telegraph equation. This result coincides with the result of Kac.","sentences":["The authors present a method of indicator random processes, applicable to constructing models of jump processes associated with diffusion process.","Indicator random processes are processes that take only two values: 1 and 0, in accordance with some probabilistic laws.","It is shown that the indicator random process is invariant when reduced to an arbitrary positive degree.","Equations with random coefficients used in modeling dynamic systems, when applying the method of indicator random processes, can take into account the possibility of adaptation to external changes, including random ones, in order to preserve indicators important for the existence of the system, which can be continuous or discrete.","In the case of indicator random processes, defined as functions of the Poisson process, equations for dynamic processes in a media with abruptly changing properties are constructed and studied.","To study the capabilities of the proposed method, dynamic models of the diffusion process in media with delay centers and diffusion processes during transitions by switching from one subspace to another were studied.","For these models, equations for characteristic functions are constructed.","Using the method of indicator random processes, a characteristic function for the Kac model was constructed.","It is shown that in the case of dependence of the indicator random process on the Poisson process, the equation for the characteristic function corresponds to the telegraph equation.","This result coincides with the result of Kac."],"url":"http://arxiv.org/abs/2402.02493v1","category":"math.DS"}
{"created":"2024-02-04 13:07:19","title":"Adaptive Downlink Localization in Near-Field and Far-Field","abstract":"This paper considers the problem of downlink localization of user equipment devices (UEs) that are either in the near-field (NF) or in the far-field (FF) of the array of the serving base station (BS). We propose a dual signaling scheme, which can be implemented at the BS, for localizing such UEs. The first scheme assumes FF, while the other assumes NF conditions. Both schemes comprise a beam-sweeping technique, employed by the BS, and a localization algorithm, employed by the UEs. The FF-based scheme enables beam-steering with a low signaling overhead, which is utilized for the proposed localization algorithm, while the NF-based scheme operates with a higher complexity. Specifically, our proposed localization scheme takes advantage of the relaxed structure of the FF, which yields low computational complexity, but is not suitable for operating in the NF. Since the compatibility and the performance of the FF- based scheme depends on the BS-to-UE distance, we study the limitations of FF-based procedure, explore the trade-off in terms of performance and resource requirements for the two schemes, and propose a triggering condition for operating the component schemes of the dual scheme. Also, we study the performance of an iterative localization algorithm that takes into account the accuracy-complexity trade-off and adapts to the actual position of the UE. We find that the conventional Fraunhofer distance is not sufficient for adapting localization algorithms in the mixed NF and FF environment.","sentences":["This paper considers the problem of downlink localization of user equipment devices (UEs) that are either in the near-field (NF) or in the far-field (FF) of the array of the serving base station (BS).","We propose a dual signaling scheme, which can be implemented at the BS, for localizing such UEs.","The first scheme assumes FF, while the other assumes NF conditions.","Both schemes comprise a beam-sweeping technique, employed by the BS, and a localization algorithm, employed by the UEs.","The FF-based scheme enables beam-steering with a low signaling overhead, which is utilized for the proposed localization algorithm, while the NF-based scheme operates with a higher complexity.","Specifically, our proposed localization scheme takes advantage of the relaxed structure of the FF, which yields low computational complexity, but is not suitable for operating in the NF.","Since the compatibility and the performance of the FF- based scheme depends on the BS-to-UE distance, we study the limitations of FF-based procedure, explore the trade-off in terms of performance and resource requirements for the two schemes, and propose a triggering condition for operating the component schemes of the dual scheme.","Also, we study the performance of an iterative localization algorithm that takes into account the accuracy-complexity trade-off and adapts to the actual position of the UE.","We find that the conventional Fraunhofer distance is not sufficient for adapting localization algorithms in the mixed NF and FF environment."],"url":"http://arxiv.org/abs/2402.02473v1","category":"cs.IT"}
{"created":"2024-02-04 13:06:40","title":"Current Induced Hidden States in Josephson Junctions","abstract":"Josephson junctions enable dissipation-less electrical current through metals and insulators below a critical current. Despite being central to quantum technology based on superconducting quantum bits and fundamental research into self-conjugate quasiparticles, the spatial distribution of super current flow at the junction and its predicted evolution with current bias and external magnetic field remain experimentally elusive. Revealing the hidden current flow, featureless in electrical resistance, helps understanding unconventional phenomena such as the nonreciprocal critical current, i.e., Josephson diode effect. Here we introduce a platform for nanoscale, quantitative, and nonperturbative visualization of the super current flow, overcoming the long-standing challenge. Utilizing a scanning magnetometer based on nitrogen vacancy centers in diamond, we uncover competing ground states electrically switchable within the zero-resistance regime. The competition results from the superconducting phase re-configuration induced by the Josephson current and kinetic inductance of thin-film superconductors. We further identify a new mechanism for the Josephson diode effect involving the Josephson current induced phase. The nanoscale super current flow emerges as a new experimental observable for elucidating unconventional superconductivity, and optimizing quantum computation and energy-efficient devices.","sentences":["Josephson junctions enable dissipation-less electrical current through metals and insulators below a critical current.","Despite being central to quantum technology based on superconducting quantum bits and fundamental research into self-conjugate quasiparticles, the spatial distribution of super current flow at the junction and its predicted evolution with current bias and external magnetic field remain experimentally elusive.","Revealing the hidden current flow, featureless in electrical resistance, helps understanding unconventional phenomena such as the nonreciprocal critical current, i.e., Josephson diode effect.","Here we introduce a platform for nanoscale, quantitative, and nonperturbative visualization of the super current flow, overcoming the long-standing challenge.","Utilizing a scanning magnetometer based on nitrogen vacancy centers in diamond, we uncover competing ground states electrically switchable within the zero-resistance regime.","The competition results from the superconducting phase re-configuration induced by the Josephson current and kinetic inductance of thin-film superconductors.","We further identify a new mechanism for the Josephson diode effect involving the Josephson current induced phase.","The nanoscale super current flow emerges as a new experimental observable for elucidating unconventional superconductivity, and optimizing quantum computation and energy-efficient devices."],"url":"http://arxiv.org/abs/2402.02472v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-04 13:02:27","title":"Fast Peer Adaptation with Context-aware Exploration","abstract":"Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds that involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed (Predator-Prey-W) games with peer agents. We demonstrate that our method induces more active exploration behavior, achieving faster adaptation and better outcomes than existing methods.","sentences":["Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games.","To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation.","However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon.","In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes.","This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident.","We evaluate our method on diverse testbeds that involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed (Predator-Prey-W) games with peer agents.","We demonstrate that our method induces more active exploration behavior, achieving faster adaptation and better outcomes than existing methods."],"url":"http://arxiv.org/abs/2402.02468v1","category":"cs.AI"}
{"created":"2024-02-04 10:00:00","title":"Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition","abstract":"Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities. Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations. Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods.","sentences":["Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction.","Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities.","To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers.","Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them.","Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities.","Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations.","Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods."],"url":"http://arxiv.org/abs/2402.02431v1","category":"cs.CV"}
{"created":"2024-02-04 09:58:42","title":"Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning","abstract":"As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\\boldsymbol{M}$ and its latent representation $\\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures, attaining the new state-of-the-art. We believe that our framework could open up avenues for new optimality bounds and COMRL algorithms.","sentences":["As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely.","Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations.","In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework.","Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\\boldsymbol{M}$ and its latent representation $\\boldsymbol{Z}$ by implementing various approximate bounds.","Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures, attaining the new state-of-the-art.","We believe that our framework could open up avenues for new optimality bounds and COMRL algorithms."],"url":"http://arxiv.org/abs/2402.02429v1","category":"cs.LG"}
{"created":"2024-02-04 09:45:35","title":"EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics","abstract":"Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into Eulerian fields to guide future prediction. Tracking key particles not only provides a clear and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency. Experimentally, EuLagNet excels in three challenging fluid prediction tasks, covering both 2D and 3D, simulated and real-world fluids.","sentences":["Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics.","However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction.","This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics.","Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time.","Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into Eulerian fields to guide future prediction.","Tracking key particles not only provides a clear and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency.","Experimentally, EuLagNet excels in three challenging fluid prediction tasks, covering both 2D and 3D, simulated and real-world fluids."],"url":"http://arxiv.org/abs/2402.02425v1","category":"cs.LG"}
{"created":"2024-02-04 09:40:52","title":"Influence of $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ Gravity on Cylindrical Collapse","abstract":"This article examines the dynamics of gravitational collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ gravity, where $\\mathcal{Q}=\\mathcal{R}_{\\mathrm{ab}}\\mathcal{T}^{\\mathrm{ab}}$. We consider self-gravitating anisotropic cylindrical geometry whose interior is filled with dissipative matter configuration and match it with exterior cylindrically symmetric spacetime at the hypersurface through junction conditions. We employ the Misner-Sharp and M\\\"{u}ler-Israel Stewart formalisms to derive the dynamical as well as transport equations corresponding to the model $\\mathcal{R}+\\Phi\\sqrt{\\mathcal{T}}+\\Psi\\mathcal{Q}$, where $\\Phi$ and $\\Psi$ are arbitrary coupling constants. We then establish some relations between these equations through which the impact of effective matter variables, heat dissipation and the bulk viscosity on the collapse rate is studied. Further, we express the Weyl scalar in terms of the effective matter sector. We also obtain the conformal flatness by applying some restrictions on the considered model and taking dust configuration into the account. Finally, we investigate various cases to check whether the modified corrections increase or decrease the collapse rate.","sentences":["This article examines the dynamics of gravitational collapse in $f(\\mathcal{R},\\mathcal{T},\\mathcal{Q})$ gravity, where $\\mathcal{Q}=\\mathcal{R}_{\\mathrm{ab}}\\mathcal{T}^{\\mathrm{ab}}$.","We consider self-gravitating anisotropic cylindrical geometry whose interior is filled with dissipative matter configuration and match it with exterior cylindrically symmetric spacetime at the hypersurface through junction conditions.","We employ the Misner-Sharp and M\\\"{u}ler-Israel Stewart formalisms to derive the dynamical as well as transport equations corresponding to the model $\\mathcal{R}+\\Phi\\sqrt{\\mathcal{T}}+\\Psi\\mathcal{Q}$, where $\\Phi$ and $\\Psi$ are arbitrary coupling constants.","We then establish some relations between these equations through which the impact of effective matter variables, heat dissipation and the bulk viscosity on the collapse rate is studied.","Further, we express the Weyl scalar in terms of the effective matter sector.","We also obtain the conformal flatness by applying some restrictions on the considered model and taking dust configuration into the account.","Finally, we investigate various cases to check whether the modified corrections increase or decrease the collapse rate."],"url":"http://arxiv.org/abs/2402.02424v1","category":"gr-qc"}
{"created":"2024-02-04 08:41:20","title":"Angle Robustness Unmanned Aerial Vehicle Navigation in GNSS-Denied Scenarios","abstract":"Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task. Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation. However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings. In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks. Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation. To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368. Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing. Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively.","sentences":["Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task.","Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation.","However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings.","In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks.","Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation.","To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368.","Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing.","Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively."],"url":"http://arxiv.org/abs/2402.02405v1","category":"cs.RO"}
{"created":"2024-02-06 18:42:51","title":"LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking","abstract":"In a zero-trust fabless paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain. Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction. While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle. In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful. Then, we develop, train, and test a corruptibility-aware graph neural network-based oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits. Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack. Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes.","sentences":["In a zero-trust fabless paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain.","Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction.","While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle.","In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful.","Then, we develop, train, and test a corruptibility-aware graph neural network-based oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits.","Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack.","Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes."],"url":"http://arxiv.org/abs/2402.04235v1","category":"cs.CR"}
{"created":"2024-02-06 17:25:22","title":"Heavy Quarkonia, Heavy-Light Tetraquarks and the Chiral Quark-Soliton Model","abstract":"We apply the Chiral Quark-Soliton Model used previously to describe baryons with one heavy quark to the case of heavy tetraquarks. We argue, that the model is insenstive to the nature of the heavy object bound by the soliton, i.e. to its mass and spin. Therefore, a heavy quark can be replaced by an anti-diquark without modifying the soliton background. Diquark dynamics is taken into account by means of the nonrelarivistic Schr\\\"odinger equation with the Cornell potential. We fix the Cornell potential parameters from the charmonia and bottomia spectra. We first compute $B_c$ meson masses to check our fitting procedure, and then compute diquark masses by appropriately rescaling color factors in the Cornell potential. We ten compute tetraquark masses and confirm previous findings that only $bb$ tetraquarks are bound.","sentences":["We apply the Chiral Quark-Soliton Model used previously to describe baryons with one heavy quark to the case of heavy tetraquarks.","We argue, that the model is insenstive to the nature of the heavy object bound by the soliton, i.e. to its mass and spin.","Therefore, a heavy quark can be replaced by an anti-diquark without modifying the soliton background.","Diquark dynamics is taken into account by means of the nonrelarivistic Schr\\\"odinger equation with the Cornell potential.","We fix the Cornell potential parameters from the charmonia and bottomia spectra.","We first compute $B_c$ meson masses to check our fitting procedure, and then compute diquark masses by appropriately rescaling color factors in the Cornell potential.","We ten compute tetraquark masses and confirm previous findings that only $bb$ tetraquarks are bound."],"url":"http://arxiv.org/abs/2402.04169v1","category":"hep-ph"}
{"created":"2024-02-06 16:26:18","title":"Multipass Quantum Process Tomography: Precision and Accuracy Enhancement","abstract":"We introduce a method to enhance the precision and accuracy of Quantum Process Tomography (QPT) by mitigating the errors caused by state preparation and measurement (SPAM), readout and shot noise. Instead of performing QPT solely on a single gate, we propose performing QPT on a sequence of multiple applications of the same gate. The method involves the measurement of the Pauli transfer matrix (PTM) by standard QPT of the multipass process, and then deduce the single-process PTM by two alternative approaches: an iterative approach which in theory delivers the exact result for small errors, and a linearized approach based on solving the Sylvester equation. We examine the efficiency of these two approaches through simulations on IBM Quantum using ibmq_qasm_simulator. Compared to the Randomized Benchmarking type of methods, the proposed method delivers the entire PTM rather than a single number (fidelity). Compared to standard QPT, our method delivers PTM with much higher accuracy and precision because it greatly reduces the SPAM, readout and shot noise errors. We use the proposed method to experimentally determine the PTM and the fidelity of the CNOT gate on the quantum processor ibmq_manila (Falcon r5.11L).","sentences":["We introduce a method to enhance the precision and accuracy of Quantum Process Tomography (QPT) by mitigating the errors caused by state preparation and measurement (SPAM), readout and shot noise.","Instead of performing QPT solely on a single gate, we propose performing QPT on a sequence of multiple applications of the same gate.","The method involves the measurement of the Pauli transfer matrix (PTM) by standard QPT of the multipass process, and then deduce the single-process PTM by two alternative approaches: an iterative approach which in theory delivers the exact result for small errors, and a linearized approach based on solving the Sylvester equation.","We examine the efficiency of these two approaches through simulations on IBM Quantum using ibmq_qasm_simulator.","Compared to the Randomized Benchmarking type of methods, the proposed method delivers the entire PTM rather than a single number (fidelity).","Compared to standard QPT, our method delivers PTM with much higher accuracy and precision because it greatly reduces the SPAM, readout and shot noise errors.","We use the proposed method to experimentally determine the PTM and the fidelity of the CNOT gate on the quantum processor ibmq_manila (Falcon r5.11L)."],"url":"http://arxiv.org/abs/2402.04128v1","category":"quant-ph"}
{"created":"2024-02-06 14:50:00","title":"A nodal ghost method based on variational formulation and regular square grid for elliptic problems on arbitrary domains in two space dimensions","abstract":"This paper focuses on the numerical solution of elliptic partial differential equations (PDEs) with Dirichlet and mixed boundary conditions, specifically addressing the challenges arising from irregular domains. Both finite element method (FEM) and finite difference method (FDM), face difficulties in dealing with arbitrary domains. The paper introduces a novel nodal symmetric ghost finite element method approach, which combines the advantages of FEM and FDM. The method employs bilinear finite elements on a structured mesh, and provides a detailed implementation description. A rigorous a priori convergence rate analysis is also presented. The convergence rates are validated with many numerical experiments, in both one and two space dimensions.","sentences":["This paper focuses on the numerical solution of elliptic partial differential equations (PDEs) with Dirichlet and mixed boundary conditions, specifically addressing the challenges arising from irregular domains.","Both finite element method (FEM) and finite difference method (FDM), face difficulties in dealing with arbitrary domains.","The paper introduces a novel nodal symmetric ghost finite element method approach, which combines the advantages of FEM and FDM.","The method employs bilinear finite elements on a structured mesh, and provides a detailed implementation description.","A rigorous a priori convergence rate analysis is also presented.","The convergence rates are validated with many numerical experiments, in both one and two space dimensions."],"url":"http://arxiv.org/abs/2402.04048v1","category":"math.NA"}
{"created":"2024-02-06 14:42:03","title":"A residual-based non-orthogonality correction for force-balanced unstructured Volume-of-Fluid methods","abstract":"Non-orthogonality errors in unstructured Finite Volume methods for simulating incompressible two-phase flows may break the force-balanced discretization. We show that applying the same explicit non-orthogonality correction for all gradient terms in the context of segregated solution algorithms is not sufficient to achieve force balance. To ensure force balance, we introduce a straightforward and deterministic residual-based control of the non-orthogonality correction, which removes the number of non-orthogonality corrections as a free parameter from the simulation. Our method is directly applicable to different unstructured finite-volume two-phase flow simulation methods as long as they discretize the one-field formulation of incompressible two-phase Navier-Stokes equations. We demonstrate force balance for the surface tension force and the gravity force near linear solver tolerance for an algebraic and a geometric Volume-of-Fluid method using the stationary droplet and stationary water column verification cases on polyhedral unstructured meshes with varying levels of non-orthogonality.","sentences":["Non-orthogonality errors in unstructured Finite Volume methods for simulating incompressible two-phase flows may break the force-balanced discretization.","We show that applying the same explicit non-orthogonality correction for all gradient terms in the context of segregated solution algorithms is not sufficient to achieve force balance.","To ensure force balance, we introduce a straightforward and deterministic residual-based control of the non-orthogonality correction, which removes the number of non-orthogonality corrections as a free parameter from the simulation.","Our method is directly applicable to different unstructured finite-volume two-phase flow simulation methods as long as they discretize the one-field formulation of incompressible two-phase Navier-Stokes equations.","We demonstrate force balance for the surface tension force and the gravity force near linear solver tolerance for an algebraic and a geometric Volume-of-Fluid method using the stationary droplet and stationary water column verification cases on polyhedral unstructured meshes with varying levels of non-orthogonality."],"url":"http://arxiv.org/abs/2402.04043v1","category":"physics.comp-ph"}
{"created":"2024-02-06 14:25:09","title":"Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory","abstract":"Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\" We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function. For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.","sentences":["Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\\text{electrons}}^3)$. Sch\\\"utt et al.","(2019) successfully approximate DFT 1000x faster with Neural Networks (NN).","Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels.","For example, it took years to create the PCQ dataset (Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week.","DFT labels molecules by minimizing energy $E(\\cdot )$ as a \"loss function.\"","We bypass dataset creation by directly training NNs with $E(\\cdot )$ as a loss function.","For comparison, Sch\\\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h."],"url":"http://arxiv.org/abs/2402.04030v1","category":"cs.LG"}
{"created":"2024-02-06 13:47:12","title":"Gradient Sketches for Training Data Attribution and Studying the Loss Landscape","abstract":"Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models.","sentences":["Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry.","Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products.","While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks.","Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms.","We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-trained language models."],"url":"http://arxiv.org/abs/2402.03994v1","category":"cs.LG"}
{"created":"2024-02-06 13:43:22","title":"Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation","abstract":"We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes. We also find a sufficient condition that implies that large batch sizes similarly reduce effective DP noise variance for one iteration of DP-SGD.","sentences":["We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes.","As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial.","However, theoretical explanations of this benefit are currently heuristic at best.","We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances.","We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size.","The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance.","We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes.","We also find a sufficient condition that implies that large batch sizes similarly reduce effective DP noise variance for one iteration of DP-SGD."],"url":"http://arxiv.org/abs/2402.03990v1","category":"stat.ML"}
{"created":"2024-02-06 12:56:55","title":"On dimensionality of feature vectors in MPNNs","abstract":"We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this paper we close this gap by showing that, for \\emph{any} non-polynomial analytic (like the sigmoid) activation function, to guarantee that MPNNs are equivalent to the WL test, feature vectors of dimension $d=1$ is all we need, independently of the size of the graphs.   Our main technical insight is that for simulating multi-sets in the WL-test, it is enough to use linear independence of feature vectors over rationals instead of reals. Countability of the set of rationals together with nice properties of analytic functions allow us to carry out the simulation invariant over the iterations of the WL test without increasing the dimension of the feature vectors.","sentences":["We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   ","Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph.","Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   ","In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs.","However, architectures used in practice have feature vectors of constant dimension.","Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice.","In this paper we close this gap by showing that, for \\emph{any} non-polynomial analytic (like the sigmoid) activation function, to guarantee that MPNNs are equivalent to the WL test, feature vectors of dimension $d=1$ is all we need, independently of the size of the graphs.   ","Our main technical insight is that for simulating multi-sets in the WL-test, it is enough to use linear independence of feature vectors over rationals instead of reals.","Countability of the set of rationals together with nice properties of analytic functions allow us to carry out the simulation invariant over the iterations of the WL test without increasing the dimension of the feature vectors."],"url":"http://arxiv.org/abs/2402.03966v1","category":"cs.LG"}
{"created":"2024-02-06 10:32:05","title":"Eigenmode Decomposition Method for Full-Wave Modeling of Microring Resonators","abstract":"We develop a theoretical predictive model for an all-pass ring resonator that enables the most complete description of linear coupling regimes. The model is based on eigenmode decomposition of Maxwell's equations with full account of the confined and leaky modes, as opposed to the existing phenomenological methods restricted to the confined modes only. This model enables quantitative description of all-pass ring resonators and provides insights into the physics underlying microring-waveguide coupling. We experimentally validate the model using transmission measurements in the linear regime of aluminium nitride resonators. The developed model is then used to explore the field enhancement in microrings crucial for nonlinear photonic applications.","sentences":["We develop a theoretical predictive model for an all-pass ring resonator that enables the most complete description of linear coupling regimes.","The model is based on eigenmode decomposition of Maxwell's equations with full account of the confined and leaky modes, as opposed to the existing phenomenological methods restricted to the confined modes only.","This model enables quantitative description of all-pass ring resonators and provides insights into the physics underlying microring-waveguide coupling.","We experimentally validate the model using transmission measurements in the linear regime of aluminium nitride resonators.","The developed model is then used to explore the field enhancement in microrings crucial for nonlinear photonic applications."],"url":"http://arxiv.org/abs/2402.03869v1","category":"physics.optics"}
{"created":"2024-02-06 10:30:00","title":"Solving Order 3 Difference Equations","abstract":"We classify order $3$ linear difference operators over $\\mathbb{C}(x)$ that are solvable in terms of lower order difference operators. To prove this result, we introduce the notions of restriction, induction, and absolute irreducibility of difference modules, and classify modules that are irreducible but not absolutely irreducible. We also show how restriction and induction give coordinate-free formulations for sectioning and interlacing of sequences.","sentences":["We classify order $3$ linear difference operators over $\\mathbb{C}(x)$ that are solvable in terms of lower order difference operators.","To prove this result, we introduce the notions of restriction, induction, and absolute irreducibility of difference modules, and classify modules that are irreducible but not absolutely irreducible.","We also show how restriction and induction give coordinate-free formulations for sectioning and interlacing of sequences."],"url":"http://arxiv.org/abs/2402.03868v1","category":"math.RA"}
{"created":"2024-02-06 10:24:36","title":"The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks","abstract":"The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.","sentences":["The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit.","We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs.","We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator.","Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs.","Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence.","Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases."],"url":"http://arxiv.org/abs/2402.03864v1","category":"cs.LG"}
{"created":"2024-02-06 10:10:47","title":"High-order stochastic integration schemes for the Rosenbluth-Trubnikov collision operator in particle simulations","abstract":"In this study, we consider a numerical implementation of the nonlinear Rosenbluth-Trubnikov collision operator for particle simulations in plasma physics in the framework of the finite element method (FEM). The relevant particle evolution equations are formulated as stochastic differential equations, both in the Stratonovich and It\\^o forms, and are then solved with advanced high-order stochastic numerical schemes. Due to its formulation as a stochastic differential equation, both the drift and diffusion components of the collision operator are treated on an equal footing. Our investigation focuses on assessing the accuracy of these schemes. Previous studies on this subject have used the Euler-Maruyama scheme, which, although popular, is of low order, and requires small time steps to achieve satisfactory accuracy. In this work, we compare the performance of the Euler-Maruyama method to other high-order stochastic methods known in the stochastic differential equations literature. Our study reveals advantageous features of these high-order schemes, such as better accuracy and improved conservation properties of the numerical solution. The main test case used in the numerical experiments is the thermalization of isotropic and anisotropic particle distributions.","sentences":["In this study, we consider a numerical implementation of the nonlinear Rosenbluth-Trubnikov collision operator for particle simulations in plasma physics in the framework of the finite element method (FEM).","The relevant particle evolution equations are formulated as stochastic differential equations, both in the Stratonovich and It\\^o forms, and are then solved with advanced high-order stochastic numerical schemes.","Due to its formulation as a stochastic differential equation, both the drift and diffusion components of the collision operator are treated on an equal footing.","Our investigation focuses on assessing the accuracy of these schemes.","Previous studies on this subject have used the Euler-Maruyama scheme, which, although popular, is of low order, and requires small time steps to achieve satisfactory accuracy.","In this work, we compare the performance of the Euler-Maruyama method to other high-order stochastic methods known in the stochastic differential equations literature.","Our study reveals advantageous features of these high-order schemes, such as better accuracy and improved conservation properties of the numerical solution.","The main test case used in the numerical experiments is the thermalization of isotropic and anisotropic particle distributions."],"url":"http://arxiv.org/abs/2402.03856v1","category":"physics.plasm-ph"}
{"created":"2024-02-06 09:30:17","title":"Endmember Extraction Algorithms Fusing for Hyperspectral Unmixing","abstract":"In recent years, transformer-based deep learning networks have gained popularity in Hyper-spectral (HS) unmixing applications due to their superior performance. The attention mechanism within transformers facilitates input-dependent weighting and enhances contextual awareness during training. Drawing inspiration from this, we propose a novel attention-based Hyperspectral Unmixing algorithm called Fusion (Fusion). This algorithm can effectively fuse endmember signatures obtained from different endmember extraction algorithms, surpassing the limitations of classical HS Unmixing approaches that rely on a single Endmember Extraction Algorithm (EEA). The Fusion network incorporates an Approximation Network (AN), introducing contextual awareness into abundance prediction by considering neighborhood pixels. Unlike Convolutional Neural Networks (CNNs), which are constrained by specific kernel shapes, the Fusion network offers flexibility in choosing any arbitrary configuration of the neighborhood. We conducted a comparative analysis between the Fusion algorithm and state-of-the-art algorithms using two popular datasets. Remarkably, Fusion outperformed other algorithms, achieving the lowest Root Mean Square Error (RMSE) for abundance predictions and competitive Spectral Angle Distance (SAD) for signatures associated with each endmember for both datasets.","sentences":["In recent years, transformer-based deep learning networks have gained popularity in Hyper-spectral (HS) unmixing applications due to their superior performance.","The attention mechanism within transformers facilitates input-dependent weighting and enhances contextual awareness during training.","Drawing inspiration from this, we propose a novel attention-based Hyperspectral Unmixing algorithm called Fusion (Fusion).","This algorithm can effectively fuse endmember signatures obtained from different endmember extraction algorithms, surpassing the limitations of classical HS Unmixing approaches that rely on a single Endmember Extraction Algorithm (EEA).","The Fusion network incorporates an Approximation Network (AN), introducing contextual awareness into abundance prediction by considering neighborhood pixels.","Unlike Convolutional Neural Networks (CNNs), which are constrained by specific kernel shapes, the Fusion network offers flexibility in choosing any arbitrary configuration of the neighborhood.","We conducted a comparative analysis between the Fusion algorithm and state-of-the-art algorithms using two popular datasets.","Remarkably, Fusion outperformed other algorithms, achieving the lowest Root Mean Square Error (RMSE) for abundance predictions and competitive Spectral Angle Distance (SAD) for signatures associated with each endmember for both datasets."],"url":"http://arxiv.org/abs/2402.03835v1","category":"eess.IV"}
{"created":"2024-02-06 09:10:06","title":"Finite volumes for the Gross-Pitaevskii equation","abstract":"We study the approximation by a Voronoi finite-volume scheme of the Gross-Pitaevskii equation with time-dependent potential in two and three dimensions. We perform an explicit splitting scheme for the time integration alongside a two-point flux approximation scheme in space. We rigorously analyze the error bounds relying on discrete uniform Sobolev inequalities. We also prove the convergence of the pseudo-vorticity of the wave function. We finally perform some numerical simulations to illustrate our theoretical results.","sentences":["We study the approximation by a Voronoi finite-volume scheme of the Gross-Pitaevskii equation with time-dependent potential in two and three dimensions.","We perform an explicit splitting scheme for the time integration alongside a two-point flux approximation scheme in space.","We rigorously analyze the error bounds relying on discrete uniform Sobolev inequalities.","We also prove the convergence of the pseudo-vorticity of the wave function.","We finally perform some numerical simulations to illustrate our theoretical results."],"url":"http://arxiv.org/abs/2402.03821v1","category":"math.NA"}
{"created":"2024-02-06 08:57:49","title":"Masked Graph Autoencoder with Non-discrete Bandwidths","abstract":"Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks. These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as \"bandwidths\". We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful graph topological learning ability both theoretically and empirically. Our proposed framework outperforms representative baselines in both self-supervised link prediction (improving the discrete edge reconstructors by at most 20%) and node classification on numerous datasets, solely with a structure-learning pretext. Our implementation is available at https://github.com/Newiz430/Bandana.","sentences":["Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored.","In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks.","These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability.","Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution.","These masks restrict the amount of output messages for each edge, referred to as \"bandwidths\".","We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective.","We demonstrate its powerful graph topological learning ability both theoretically and empirically.","Our proposed framework outperforms representative baselines in both self-supervised link prediction (improving the discrete edge reconstructors by at most 20%) and node classification on numerous datasets, solely with a structure-learning pretext.","Our implementation is available at https://github.com/Newiz430/Bandana."],"url":"http://arxiv.org/abs/2402.03814v1","category":"cs.LG"}
{"created":"2024-02-06 07:49:50","title":"All-Optical, Reconfigurable and Power Independent Neural Activation Function by Means of Phase Modulation","abstract":"In this work, we present numerical results concerning an integrated photonic non-linear activation function that relies on a power independent, non-linear phase to amplitude conversion in a passive optical resonator. The underlying mechanism is universal to all optical filters, whereas here, simulations were based on micro-ring resonators (MRRs). Investigation revealed that the photonic neural node can be tuned to support a wide variety of continuous activation functions that are relevant to the neural network architectures, such as the sigmoid and the softplus functions. The proposed photonic node is numerically evaluated in the context of time delayed reservoir computing (TDRC) scheme, targeting the one-step ahead prediction of the Santa Fe series. The proposed phase to amplitude TDRC is benchmarked versus the conventional amplitude based TDRC, showcasing a performance boost by one order of magnitude.","sentences":["In this work, we present numerical results concerning an integrated photonic non-linear activation function that relies on a power independent, non-linear phase to amplitude conversion in a passive optical resonator.","The underlying mechanism is universal to all optical filters, whereas here, simulations were based on micro-ring resonators (MRRs).","Investigation revealed that the photonic neural node can be tuned to support a wide variety of continuous activation functions that are relevant to the neural network architectures, such as the sigmoid and the softplus functions.","The proposed photonic node is numerically evaluated in the context of time delayed reservoir computing (TDRC) scheme, targeting the one-step ahead prediction of the Santa Fe series.","The proposed phase to amplitude TDRC is benchmarked versus the conventional amplitude based TDRC, showcasing a performance boost by one order of magnitude."],"url":"http://arxiv.org/abs/2402.03778v1","category":"physics.optics"}
{"created":"2024-02-06 07:07:33","title":"MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction","abstract":"Neural implicit representations have recently been demonstrated in many fields including Simultaneous Localization And Mapping (SLAM). Current neural SLAM can achieve ideal results in reconstructing bounded scenes, but this relies on the input of RGB-D images. Neural-based SLAM based only on RGB images is unable to reconstruct the scale of the scene accurately, and it also suffers from scale drift due to errors accumulated during tracking. To overcome these limitations, we present MoD-SLAM, a monocular dense mapping method that allows global pose optimization and 3D reconstruction in real-time in unbounded scenes. Optimizing scene reconstruction by monocular depth estimation and using loop closure detection to update camera pose enable detailed and precise reconstruction on large scenes. Compared to previous work, our approach is more robust, scalable and versatile. Our experiments demonstrate that MoD-SLAM has more excellent mapping performance than prior neural SLAM methods, especially in large borderless scenes.","sentences":["Neural implicit representations have recently been demonstrated in many fields including Simultaneous Localization And Mapping (SLAM).","Current neural SLAM can achieve ideal results in reconstructing bounded scenes, but this relies on the input of RGB-D images.","Neural-based SLAM based only on RGB images is unable to reconstruct the scale of the scene accurately, and it also suffers from scale drift due to errors accumulated during tracking.","To overcome these limitations, we present MoD-SLAM, a monocular dense mapping method that allows global pose optimization and 3D reconstruction in real-time in unbounded scenes.","Optimizing scene reconstruction by monocular depth estimation and using loop closure detection to update camera pose enable detailed and precise reconstruction on large scenes.","Compared to previous work, our approach is more robust, scalable and versatile.","Our experiments demonstrate that MoD-SLAM has more excellent mapping performance than prior neural SLAM methods, especially in large borderless scenes."],"url":"http://arxiv.org/abs/2402.03762v1","category":"cs.CV"}
{"created":"2024-02-06 06:41:24","title":"Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images","abstract":"Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.","sentences":["Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions?","This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling.","Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models.","Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100.","This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale."],"url":"http://arxiv.org/abs/2402.03752v1","category":"cs.CV"}
{"created":"2024-02-06 06:28:17","title":"An invariance constrained deep learning network for PDE discovery","abstract":"The discovery of partial differential equations (PDEs) from datasets has attracted increased attention. However, the discovery of governing equations from sparse data with high noise is still very challenging due to the difficulty of derivatives computation and the disturbance of noise. Moreover, the selection principles for the candidate library to meet physical laws need to be further studied. The invariance is one of the fundamental laws for governing equations. In this study, we propose an invariance constrained deep learning network (ICNet) for the discovery of PDEs. Considering that temporal and spatial translation invariance (Galilean invariance) is a fundamental property of physical laws, we filter the candidates that cannot meet the requirement of the Galilean transformations. Subsequently, we embedded the fixed and possible terms into the loss function of neural network, significantly countering the effect of sparse data with high noise. Then, by filtering out redundant terms without fixing learnable parameters during the training process, the governing equations discovered by the ICNet method can effectively approximate the real governing equations. We select the 2D Burgers equation, the equation of 2D channel flow over an obstacle, and the equation of 3D intracranial aneurysm as examples to verify the superiority of the ICNet for fluid mechanics. Furthermore, we extend similar invariance methods to the discovery of wave equation (Lorentz Invariance) and verify it through Single and Coupled Klein-Gordon equation. The results show that the ICNet method with physical constraints exhibits excellent performance in governing equations discovery from sparse and noisy data.","sentences":["The discovery of partial differential equations (PDEs) from datasets has attracted increased attention.","However, the discovery of governing equations from sparse data with high noise is still very challenging due to the difficulty of derivatives computation and the disturbance of noise.","Moreover, the selection principles for the candidate library to meet physical laws need to be further studied.","The invariance is one of the fundamental laws for governing equations.","In this study, we propose an invariance constrained deep learning network (ICNet) for the discovery of PDEs.","Considering that temporal and spatial translation invariance (Galilean invariance) is a fundamental property of physical laws, we filter the candidates that cannot meet the requirement of the Galilean transformations.","Subsequently, we embedded the fixed and possible terms into the loss function of neural network, significantly countering the effect of sparse data with high noise.","Then, by filtering out redundant terms without fixing learnable parameters during the training process, the governing equations discovered by the ICNet method can effectively approximate the real governing equations.","We select the 2D Burgers equation, the equation of 2D channel flow over an obstacle, and the equation of 3D intracranial aneurysm as examples to verify the superiority of the ICNet for fluid mechanics.","Furthermore, we extend similar invariance methods to the discovery of wave equation (Lorentz Invariance) and verify it through Single and Coupled Klein-Gordon equation.","The results show that the ICNet method with physical constraints exhibits excellent performance in governing equations discovery from sparse and noisy data."],"url":"http://arxiv.org/abs/2402.03747v1","category":"cs.LG"}
{"created":"2024-02-06 05:07:08","title":"Evidence that the AT transition disappears below six dimensions","abstract":"One of the key predictions of Parisi's broken replica symmetry theory of spin glasses is the existence of a phase transition in an applied field to a state with broken replica symmetry. This transition takes place at the de Almeida-Thouless (AT) line in the $h-T$ plane. We have studied this line in the power-law diluted Heisenberg spin glass in which the probability that two spins separated by a distance $r$ interact with each other falls as $1/r^{2\\sigma}$. In the presence of a random vector-field of variance $h_r^2$ the phase transition is in the universality class of the Ising spin glass in a field. Tuning $\\sigma$ is equivalent to changing the dimension $d$ of the short-range system, with the relation being $d =2/(2\\sigma -1)$ for $\\sigma < 2/3$. We have found by numerical simulations that $h_{\\text{AT}}^2 \\sim (2/3 -\\sigma)$ implying that the AT line does not exist below $6$ dimensions and that the Parisi scheme is not appropriate for spin glasses in three dimensions.","sentences":["One of the key predictions of Parisi's broken replica symmetry theory of spin glasses is the existence of a phase transition in an applied field to a state with broken replica symmetry.","This transition takes place at the de Almeida-Thouless (AT) line in the $h-T$ plane.","We have studied this line in the power-law diluted Heisenberg spin glass in which the probability that two spins separated by a distance $r$ interact with each other falls as $1/r^{2\\sigma}$. In the presence of a random vector-field of variance $h_r^2$ the phase transition is in the universality class of the Ising spin glass in a field.","Tuning $\\sigma$ is equivalent to changing the dimension $d$ of the short-range system, with the relation being $d =2/(2\\sigma -1)$ for $\\sigma < 2/3$. We have found by numerical simulations that $h_{\\text{AT}}^2 \\sim (2/3 -\\sigma)$ implying that the AT line does not exist below $6$ dimensions and that the Parisi scheme is not appropriate for spin glasses in three dimensions."],"url":"http://arxiv.org/abs/2402.03711v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-06 04:06:56","title":"Gluing $\\mathbb Z_2$-Harmonic Spinors and Seiberg-Witten Monopoles on 3-Manifolds","abstract":"Given a $\\mathbb Z_2$-harmonic spinor satisfying some genericity assumptions, this article constructs a 1-parameter family of two-spinor Seiberg-Witten monopoles converging to it after renormalization. The proof is a gluing construction beginning with model solutions on a neighborhood of the $\\mathbb Z_2$-harmonic spinor's singular set. The gluing is complicated by the presence of an infinite-dimensional obstruction bundle for the singular limiting linearized operator. This difficulty is overcome by introducing a generalization of Donaldson's alternating method in which a deformation of the $\\mathbb Z_2$-harmonic spinor's singular set is chosen at each stage of the alternating iteration to cancel the obstruction components.","sentences":["Given a $\\mathbb Z_2$-harmonic spinor satisfying some genericity assumptions, this article constructs a 1-parameter family of two-spinor Seiberg-Witten monopoles converging to it after renormalization.","The proof is a gluing construction beginning with model solutions on a neighborhood of the $\\mathbb Z_2$-harmonic spinor's singular set.","The gluing is complicated by the presence of an infinite-dimensional obstruction bundle for the singular limiting linearized operator.","This difficulty is overcome by introducing a generalization of Donaldson's alternating method in which a deformation of the $\\mathbb Z_2$-harmonic spinor's singular set is chosen at each stage of the alternating iteration to cancel the obstruction components."],"url":"http://arxiv.org/abs/2402.03682v1","category":"math.DG"}
{"created":"2024-02-06 04:00:59","title":"Stochastic-periodic Homogenization of Non-stationary Incompressible Navier-Stokes Type Equations","abstract":"In this paper, we study the stochastic-periodic homogenization of Non-stationary Navier-Stokes Type Equations on anisotropic heterogeneous media. More precisely, we are interested in the stochastic-periodic homogenization of its variational formulation. This problematic relies on the notion of dynamical system. It is shown by the stochastic two-scale convergence method that the resulting homogenized limit equation is of the same form of this variational formulation with suitable coefficients.","sentences":["In this paper, we study the stochastic-periodic homogenization of Non-stationary Navier-Stokes Type Equations on anisotropic heterogeneous media.","More precisely, we are interested in the stochastic-periodic homogenization of its variational formulation.","This problematic relies on the notion of dynamical system.","It is shown by the stochastic two-scale convergence method that the resulting homogenized limit equation is of the same form of this variational formulation with suitable coefficients."],"url":"http://arxiv.org/abs/2402.03679v1","category":"math.AP"}
{"created":"2024-02-06 03:50:02","title":"The spin alignment of rho mesons in a pion gas","abstract":"We study the spin alignment of neutral rho mesons in a pion gas using spin kinetic or Boltzmann equations. The $\\rho\\pi\\pi$ coupling is given by the chiral effective theory. The collision terms at the leading and next-to-leading order in spin Boltzmann equations are derived. The evolution of the spin density matrix of the neutral rho meson is simulated with different initial conditions. The numerical results show that the interaction of pions and neutral rho mesons creates very small spin alignment in the central rapidity region if there is no rho meson in the system at the initial time. Such a small spin alignment in the central rapidity region will decay rapidly toward zero in later time. If there are rho mesons with a sizable spin alignment at the initial time the spin alignment will also decrease rapidly. We also considered the effect on $\\rho_{00}$ from the elliptic flow of pions in the blast wave model. With vanishing spin alignment at the initial time, the deviation of $\\rho_{00}$ from 1/3 is positive but very small.","sentences":["We study the spin alignment of neutral rho mesons in a pion gas using spin kinetic or Boltzmann equations.","The $\\rho\\pi\\pi$ coupling is given by the chiral effective theory.","The collision terms at the leading and next-to-leading order in spin Boltzmann equations are derived.","The evolution of the spin density matrix of the neutral rho meson is simulated with different initial conditions.","The numerical results show that the interaction of pions and neutral rho mesons creates very small spin alignment in the central rapidity region if there is no rho meson in the system at the initial time.","Such a small spin alignment in the central rapidity region will decay rapidly toward zero in later time.","If there are rho mesons with a sizable spin alignment at the initial time the spin alignment will also decrease rapidly.","We also considered the effect on $\\rho_{00}$ from the elliptic flow of pions in the blast wave model.","With vanishing spin alignment at the initial time, the deviation of $\\rho_{00}$ from 1/3 is positive but very small."],"url":"http://arxiv.org/abs/2402.03672v1","category":"nucl-th"}
{"created":"2024-02-06 03:09:39","title":"Discretization form of the continuity condition at the polar axis, with application to the gyrokinetic simulation in a magnetic fusion torus","abstract":"A new computational method to solve the hyperbolic (Vlasov) equation coupled to the elliptic (Poisson-like) equation at the polar axis is proposed. It is shown that the value of a scalar function at the polar axis can be predicted by its neighbouring values based on the continuity condition. This continuity condition systematically solves the pole problems including the singular factor 1/r in the hyperbolic equation and the inner boundary in the elliptic equation. The proposed method is applied to the global gyrokinetic simulation of the tokamak plasma with the magnetic axis included.","sentences":["A new computational method to solve the hyperbolic (Vlasov) equation coupled to the elliptic (Poisson-like) equation at the polar axis is proposed.","It is shown that the value of a scalar function at the polar axis can be predicted by its neighbouring values based on the continuity condition.","This continuity condition systematically solves the pole problems including the singular factor 1/r in the hyperbolic equation and the inner boundary in the elliptic equation.","The proposed method is applied to the global gyrokinetic simulation of the tokamak plasma with the magnetic axis included."],"url":"http://arxiv.org/abs/2402.03657v1","category":"physics.comp-ph"}
{"created":"2024-02-06 03:06:06","title":"Operator SVD with Neural Networks via Nested Low-Rank Approximation","abstract":"Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$L$ singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cases in computational physics and machine learning.","sentences":["Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems.","For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques.","This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$L$ singular values and singular functions in the correct order.","The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms.","We demonstrate the effectiveness of the proposed optimization framework for use cases in computational physics and machine learning."],"url":"http://arxiv.org/abs/2402.03655v1","category":"cs.LG"}
{"created":"2024-02-06 02:35:01","title":"Stable BDF time discretization of BGN-based parametric finite element methods for geometric flows","abstract":"We propose a novel class of temporal high-order parametric finite element methods for solving a wide range of geometric flows of curves and surfaces. By incorporating the backward differentiation formulae (BDF) for time discretization into the BGN formulation, originally proposed by Barrett, Garcke, and N\\\"urnberg (J. Comput. Phys., 222 (2007), pp.~441--467), we successfully develop high-order BGN/BDF$k$ schemes. The proposed BGN/BDF$k$ schemes not only retain almost all the advantages of the classical first-order BGN scheme such as computational efficiency and good mesh quality, but also exhibit the desired $k$th-order temporal accuracy in terms of shape metrics, ranging from second-order to fourth-order accuracy. Furthermore, we validate the performance of our proposed BGN/BDF$k$ schemes through extensive numerical examples, demonstrating their high-order temporal accuracy for various types of geometric flows while maintaining good mesh quality throughout the evolution.","sentences":["We propose a novel class of temporal high-order parametric finite element methods for solving a wide range of geometric flows of curves and surfaces.","By incorporating the backward differentiation formulae (BDF) for time discretization into the BGN formulation, originally proposed by Barrett, Garcke, and N\\\"urnberg (J. Comput.","Phys., 222 (2007), pp.~441--467), we successfully develop high-order BGN/BDF$k$ schemes.","The proposed BGN/BDF$k$ schemes not only retain almost all the advantages of the classical first-order BGN scheme such as computational efficiency and good mesh quality, but also exhibit the desired $k$th-order temporal accuracy in terms of shape metrics, ranging from second-order to fourth-order accuracy.","Furthermore, we validate the performance of our proposed BGN/BDF$k$ schemes through extensive numerical examples, demonstrating their high-order temporal accuracy for various types of geometric flows while maintaining good mesh quality throughout the evolution."],"url":"http://arxiv.org/abs/2402.03641v1","category":"math.NA"}
{"created":"2024-02-06 00:52:24","title":"A regularity theory for evolution equations with time-measurable pseudo-differential operators in weighted mixed-norm Sobolev-Lipschitz spaces","abstract":"This paper investigates the existence, uniqueness, and regularity of solutions to evolution equations with time-measurable pseudo-differential operators in weighted mixed-norm Sobolev-Lipschitz spaces. We also explore trace embedding and continuity of solutions.","sentences":["This paper investigates the existence, uniqueness, and regularity of solutions to evolution equations with time-measurable pseudo-differential operators in weighted mixed-norm Sobolev-Lipschitz spaces.","We also explore trace embedding and continuity of solutions."],"url":"http://arxiv.org/abs/2402.03609v1","category":"math.AP"}
{"created":"2024-02-05 23:06:48","title":"Deconstructing the Goldilocks Zone of Neural Network Initialization","abstract":"The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort & Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the \"Goldilocks zone\". Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep networks, we optimize both fully-connected and convolutional architectures outside the Goldilocks zone and analyze the emergent behaviors. We find that strong model performance is not necessarily aligned with the Goldilocks zone, which questions the practical significance of this concept.","sentences":["The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models.","Fort & Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the \"Goldilocks zone\".","Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained.","In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks.","In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs.","Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient.","To understand the importance of positive curvature for trainability of deep networks, we optimize both fully-connected and convolutional architectures outside the Goldilocks zone and analyze the emergent behaviors.","We find that strong model performance is not necessarily aligned with the Goldilocks zone, which questions the practical significance of this concept."],"url":"http://arxiv.org/abs/2402.03579v1","category":"cs.LG"}
{"created":"2024-02-05 22:57:33","title":"Generalization Properties of Adversarial Training for $\\ell_0$-Bounded Adversarial Attacks","abstract":"We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification. In this paper, we focus on the $\\ell_0$-bounded adversarial attacks, and aim to theoretically characterize the performance of adversarial training for an important class of truncated classifiers. Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\\ell_0$-adversarial setting. The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\\ell_0$-bounded adversarial perturbation that is distribution-independent. Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\\ell_0$ ball due to adversarial training is non-convex and highly non-smooth. To tackle these challenges, we develop new coding techniques for bounding the combinatorial dimension of the truncated hypothesis class.","sentences":["We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification.","In this paper, we focus on the $\\ell_0$-bounded adversarial attacks, and aim to theoretically characterize the performance of adversarial training for an important class of truncated classifiers.","Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\\ell_0$-adversarial setting.","The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\\ell_0$-bounded adversarial perturbation that is distribution-independent.","Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\\ell_0$ ball due to adversarial training is non-convex and highly non-smooth.","To tackle these challenges, we develop new coding techniques for bounding the combinatorial dimension of the truncated hypothesis class."],"url":"http://arxiv.org/abs/2402.03576v1","category":"cs.LG"}
{"created":"2024-02-05 22:18:18","title":"Dynamic flux surrogate-based partitioned methods for interface problems","abstract":"Partitioned methods for coupled problems rely on data transfers between subdomains to synchronize the subdomain equations and enable their independent solution. By treating each subproblem as a separate entity, these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations. However, accuracy and stability of partitioned methods depends critically on the type of information exchanged between the subproblems. The exchange mechanisms can vary from minimally intrusive remap across interfaces to more accurate but also more intrusive and expensive estimates of the necessary information based on monolithic formulations of the coupled system. These transfer mechanisms are separated by accuracy, performance and intrusiveness gaps that tend to limit the scope of the resulting partitioned methods to specific simulation scenarios. Data-driven system identification techniques provide an opportunity to close these gaps by enabling the construction of accurate, computationally efficient and minimally intrusive data transfer surrogates. This approach shifts the principal computational burden to an offline phase, leaving the application of the surrogate as the sole additional cost during the online simulation phase. In this paper we formulate and demonstrate such a \\emph{dynamic flux surrogate-based} partitioned method for a model advection-diffusion transmission problem by using Dynamic Mode Decomposition (DMD) to learn the dynamics of the interface flux from data. The accuracy of the resulting DMD flux surrogate is comparable to that of a dual Schur complement reconstruction, yet its application cost is significantly lower. Numerical results confirm the attractive properties of the new partitioned approach.","sentences":["Partitioned methods for coupled problems rely on data transfers between subdomains to synchronize the subdomain equations and enable their independent solution.","By treating each subproblem as a separate entity, these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations.","However, accuracy and stability of partitioned methods depends critically on the type of information exchanged between the subproblems.","The exchange mechanisms can vary from minimally intrusive remap across interfaces to more accurate but also more intrusive and expensive estimates of the necessary information based on monolithic formulations of the coupled system.","These transfer mechanisms are separated by accuracy, performance and intrusiveness gaps that tend to limit the scope of the resulting partitioned methods to specific simulation scenarios.","Data-driven system identification techniques provide an opportunity to close these gaps by enabling the construction of accurate, computationally efficient and minimally intrusive data transfer surrogates.","This approach shifts the principal computational burden to an offline phase, leaving the application of the surrogate as the sole additional cost during the online simulation phase.","In this paper we formulate and demonstrate such a \\emph{dynamic flux surrogate-based} partitioned method for a model advection-diffusion transmission problem by using Dynamic Mode Decomposition (DMD) to learn the dynamics of the interface flux from data.","The accuracy of the resulting DMD flux surrogate is comparable to that of a dual Schur complement reconstruction, yet its application cost is significantly lower.","Numerical results confirm the attractive properties of the new partitioned approach."],"url":"http://arxiv.org/abs/2402.03560v1","category":"cs.CE"}
{"created":"2024-02-05 22:16:05","title":"Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?","abstract":"The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths. On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks. In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions. We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island. We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon. Our methodology shows promise for future advancement in earthquake prediction and sensor network analysis.","sentences":["The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths.","On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks.","In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions.","We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island.","We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon.","Our methodology shows promise for future advancement in earthquake prediction and sensor network analysis."],"url":"http://arxiv.org/abs/2402.03558v1","category":"cs.LG"}
{"created":"2024-02-05 22:12:42","title":"One-shot Neural Face Reenactment via Finding Directions in GAN's Latent Space","abstract":"In this paper, we present our framework for neural face/head reenactment whose goal is to transfer the 3D head orientation and expression of a target face to a source face. Previous methods focus on learning embedding networks for identity and head pose/expression disentanglement which proves to be a rather hard task, degrading the quality of the generated images. We take a different approach, bypassing the training of such networks, by using (fine-tuned) pre-trained GANs which have been shown capable of producing high-quality facial images. Because GANs are characterized by weak controllability, the core of our approach is a method to discover which directions in latent GAN space are responsible for controlling head pose and expression variations. We present a simple pipeline to learn such directions with the aid of a 3D shape model which, by construction, inherently captures disentangled directions for head pose, identity, and expression. Moreover, we show that by embedding real images in the GAN latent space, our method can be successfully used for the reenactment of real-world faces. Our method features several favorable properties including using a single source image (one-shot) and enabling cross-person reenactment. Extensive qualitative and quantitative results show that our approach typically produces reenacted faces of notably higher quality than those produced by state-of-the-art methods for the standard benchmarks of VoxCeleb1 & 2.","sentences":["In this paper, we present our framework for neural face/head reenactment whose goal is to transfer the 3D head orientation and expression of a target face to a source face.","Previous methods focus on learning embedding networks for identity and head pose/expression disentanglement which proves to be a rather hard task, degrading the quality of the generated images.","We take a different approach, bypassing the training of such networks, by using (fine-tuned) pre-trained GANs which have been shown capable of producing high-quality facial images.","Because GANs are characterized by weak controllability, the core of our approach is a method to discover which directions in latent GAN space are responsible for controlling head pose and expression variations.","We present a simple pipeline to learn such directions with the aid of a 3D shape model which, by construction, inherently captures disentangled directions for head pose, identity, and expression.","Moreover, we show that by embedding real images in the GAN latent space, our method can be successfully used for the reenactment of real-world faces.","Our method features several favorable properties including using a single source image (one-shot) and enabling cross-person reenactment.","Extensive qualitative and quantitative results show that our approach typically produces reenacted faces of notably higher quality than those produced by state-of-the-art methods for the standard benchmarks of VoxCeleb1 & 2."],"url":"http://arxiv.org/abs/2402.03553v1","category":"cs.CV"}
{"created":"2024-02-05 22:07:58","title":"Single-GPU GNN Systems: Traps and Pitfalls","abstract":"The current graph neural network (GNN) systems have established a clear trend of not showing training accuracy results, and directly or indirectly relying on smaller datasets for evaluations majorly. Our in-depth analysis shows that it leads to a chain of pitfalls in the system design and evaluation process, questioning the practicality of many of the proposed system optimizations, and affecting conclusions and lessons learned. We analyze many single-GPU systems and show the fundamental impact of these pitfalls. We further develop hypotheses, recommendations, and evaluation methodologies, and provide future directions. Finally, a new reference system is developed to establish a new line of optimizations rooted in solving the system-design pitfalls efficiently and practically. The proposed design can productively be integrated into prior works, thereby truly advancing the state-of-the-art.","sentences":["The current graph neural network (GNN) systems have established a clear trend of not showing training accuracy results, and directly or indirectly relying on smaller datasets for evaluations majorly.","Our in-depth analysis shows that it leads to a chain of pitfalls in the system design and evaluation process, questioning the practicality of many of the proposed system optimizations, and affecting conclusions and lessons learned.","We analyze many single-GPU systems and show the fundamental impact of these pitfalls.","We further develop hypotheses, recommendations, and evaluation methodologies, and provide future directions.","Finally, a new reference system is developed to establish a new line of optimizations rooted in solving the system-design pitfalls efficiently and practically.","The proposed design can productively be integrated into prior works, thereby truly advancing the state-of-the-art."],"url":"http://arxiv.org/abs/2402.03548v1","category":"cs.LG"}
{"created":"2024-02-05 22:00:31","title":"Polynomial Lawvere Logic","abstract":"In this paper, we study Polynomial Lawvere logic (PL), a logic on the quantale of the extended positive reals, developed for reasoning about metric spaces. PL is appropriate for encoding quantitative reasoning principles, such as quantitative equational logic. PL formulas include the polynomial functions on the extended positive reals, and its judgements include inequalities between polynomials.   We present an inference system for PL and prove a series of completeness and incompleteness results relying and the Krivine-Stengle Positivstellensatz (a variant of Hilbert's Nullstellensatz) including completeness for finitely axiomatisable PL theories.   We also study complexity results both for both PL and its affine fragment (AL). We demonstrate that the satisfiability of a finite set of judgements is NP-complete in AL and in PSPACE for PL; and that deciding the semantical consequence from a finite set of judgements is co-NP complete in AL and in PSPACE in PL.","sentences":["In this paper, we study Polynomial Lawvere logic (PL), a logic on the quantale of the extended positive reals, developed for reasoning about metric spaces.","PL is appropriate for encoding quantitative reasoning principles, such as quantitative equational logic.","PL formulas include the polynomial functions on the extended positive reals, and its judgements include inequalities between polynomials.   ","We present an inference system for PL and prove a series of completeness and incompleteness results relying and the Krivine-Stengle Positivstellensatz (a variant of Hilbert's Nullstellensatz) including completeness for finitely axiomatisable PL theories.   ","We also study complexity results both for both PL and its affine fragment (AL).","We demonstrate that the satisfiability of a finite set of judgements is NP-complete in AL and in PSPACE for PL; and that deciding the semantical consequence from a finite set of judgements is co-NP complete in AL and in PSPACE in PL."],"url":"http://arxiv.org/abs/2402.03543v1","category":"cs.LO"}
{"created":"2024-02-05 21:54:28","title":"Regulation Games for Trustworthy Machine Learning","abstract":"Existing work on trustworthy machine learning (ML) often concentrates on individual aspects of trust, such as fairness or privacy. Additionally, many techniques overlook the distinction between those who train ML models and those responsible for assessing their trustworthiness. To address these issues, we propose a framework that views trustworthy ML as a multi-objective multi-agent optimization problem. This naturally lends itself to a game-theoretic formulation we call regulation games. We illustrate a particular game instance, the SpecGame in which we model the relationship between an ML model builder and fairness and privacy regulators. Regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation. Seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria. Simulating SpecGame through ParetoPlay can provide policy guidance for ML Regulation. For instance, we show that for a gender classification application, regulators can enforce a differential privacy budget that is on average 4.0 lower if they take the initiative to specify their desired guarantee first.","sentences":["Existing work on trustworthy machine learning (ML) often concentrates on individual aspects of trust, such as fairness or privacy.","Additionally, many techniques overlook the distinction between those who train ML models and those responsible for assessing their trustworthiness.","To address these issues, we propose a framework that views trustworthy ML as a multi-objective multi-agent optimization problem.","This naturally lends itself to a game-theoretic formulation we call regulation games.","We illustrate a particular game instance, the SpecGame in which we model the relationship between an ML model builder and fairness and privacy regulators.","Regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation.","Seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay.","This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria.","Simulating SpecGame through ParetoPlay can provide policy guidance for ML Regulation.","For instance, we show that for a gender classification application, regulators can enforce a differential privacy budget that is on average 4.0 lower if they take the initiative to specify their desired guarantee first."],"url":"http://arxiv.org/abs/2402.03540v1","category":"cs.LG"}
{"created":"2024-02-05 21:46:36","title":"Left-invariant Pseudo-Riemannian metrics on Lie groups: The null cone","abstract":"We study left-invariant pseudo-Riemannian metrics on Lie groups using the bracket flow of the corresponding Lie algebra. We focus on metrics where the Lie algebra is in the null cone of the $G=O(p,q)$-action; i.e., Lie algebras $\\mu$ where zero is in the closure of the orbits: $0\\in\\overline{G\\cdot \\mu}$. We provide examples of such Lie groups in various signatures and give some general results. For signatures $(1,q)$ and $(2,q)$ we classify all cases belonging to the null cone. More generally, we show that all nilpotent and completely solvable Lie algebras are in the null cone of some $O(p,q)$ action. In addition, several examples of non-trivial Levi-decomposable Lie algebras in the null cone are given.","sentences":["We study left-invariant pseudo-Riemannian metrics on Lie groups using the bracket flow of the corresponding Lie algebra.","We focus on metrics where the Lie algebra is in the null cone of the $G=O(p,q)$-action; i.e., Lie algebras $\\mu$ where zero is in the closure of the orbits: $0\\in\\overline{G\\cdot \\mu}$.","We provide examples of such Lie groups in various signatures and give some general results.","For signatures $(1,q)$ and $(2,q)$ we classify all cases belonging to the null cone.","More generally, we show that all nilpotent and completely solvable Lie algebras are in the null cone of some $O(p,q)$ action.","In addition, several examples of non-trivial Levi-decomposable Lie algebras in the null cone are given."],"url":"http://arxiv.org/abs/2402.03536v1","category":"math.DG"}
{"created":"2024-02-05 21:43:40","title":"ANN-based position and speed sensorless estimation for BLDC motors","abstract":"BLDC motor applications require precise position and speed measurements, traditionally obtained with sensors. This article presents a method for estimating those measurements without position sensors using terminal phase voltages with attenuated spurious, acquired with a FPGA that also operates a PWM-controlled inverter. Voltages are labelled with electrical and virtual rotor states using an encoder that provides training and testing data for two three-layer ANNs with perceptron-based cascade topology. The first ANN estimates the position from features of voltages with incremental timestamps, and the second ANN estimates the speed from features of position differentials considering timestamps in an acquisition window. Sensor-based training and sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained absolute errors of 0.8 electrical degrees and 22 rpm. Results conclude that the overall position estimation significantly improved conventional and advanced methods, and the speed estimation slightly improved conventional methods, but was worse than in advanced ones.","sentences":["BLDC motor applications require precise position and speed measurements, traditionally obtained with sensors.","This article presents a method for estimating those measurements without position sensors using terminal phase voltages with attenuated spurious, acquired with a FPGA that also operates a PWM-controlled inverter.","Voltages are labelled with electrical and virtual rotor states using an encoder that provides training and testing data for two three-layer ANNs with perceptron-based cascade topology.","The first ANN estimates the position from features of voltages with incremental timestamps, and the second ANN estimates the speed from features of position differentials considering timestamps in an acquisition window.","Sensor-based training and sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained absolute errors of 0.8 electrical degrees and 22 rpm.","Results conclude that the overall position estimation significantly improved conventional and advanced methods, and the speed estimation slightly improved conventional methods, but was worse than in advanced ones."],"url":"http://arxiv.org/abs/2402.03534v1","category":"eess.SY"}
{"created":"2024-02-05 21:38:23","title":"Fairness and Privacy Guarantees in Federated Contextual Bandits","abstract":"This paper considers the contextual multi-armed bandit (CMAB) problem with fairness and privacy guarantees in a federated environment. We consider merit-based exposure as the desired fair outcome, which provides exposure to each action in proportion to the reward associated. We model the algorithm's effectiveness using fairness regret, which captures the difference between fair optimal policy and the policy output by the algorithm. Applying fair CMAB algorithm to each agent individually leads to fairness regret linear in the number of agents. We propose that collaborative -- federated learning can be more effective and provide the algorithm Fed-FairX-LinUCB that also ensures differential privacy. The primary challenge in extending the existing privacy framework is designing the communication protocol for communicating required information across agents. A naive protocol can either lead to weaker privacy guarantees or higher regret. We design a novel communication protocol that allows for (i) Sub-linear theoretical bounds on fairness regret for Fed-FairX-LinUCB and comparable bounds for the private counterpart, Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our proposed algorithm with extensive simulations-based experiments. We show that both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness regret.","sentences":["This paper considers the contextual multi-armed bandit (CMAB) problem with fairness and privacy guarantees in a federated environment.","We consider merit-based exposure as the desired fair outcome, which provides exposure to each action in proportion to the reward associated.","We model the algorithm's effectiveness using fairness regret, which captures the difference between fair optimal policy and the policy output by the algorithm.","Applying fair CMAB algorithm to each agent individually leads to fairness regret linear in the number of agents.","We propose that collaborative -- federated learning can be more effective and provide the algorithm Fed-FairX-LinUCB that also ensures differential privacy.","The primary challenge in extending the existing privacy framework is designing the communication protocol for communicating required information across agents.","A naive protocol can either lead to weaker privacy guarantees or higher regret.","We design a novel communication protocol that allows for (i) Sub-linear theoretical bounds on fairness regret for Fed-FairX-LinUCB and comparable bounds for the private counterpart, Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of privacy budget in Priv-FairX-LinUCB.","We demonstrate the efficacy of our proposed algorithm with extensive simulations-based experiments.","We show that both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness regret."],"url":"http://arxiv.org/abs/2402.03531v1","category":"cs.LG"}
{"created":"2024-02-05 21:28:47","title":"nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model","abstract":"In the field of biomedical image analysis, the quest for architectures capable of effectively capturing long-range dependencies is paramount, especially when dealing with 3D image segmentation, classification, and landmark detection. Traditional Convolutional Neural Networks (CNNs) struggle with locality respective field, and Transformers have a heavy computational load when applied to high-dimensional medical images. In this paper, we introduce nnMamba, a novel architecture that integrates the strengths of CNNs and the advanced long-range modeling capabilities of State Space Sequence Models (SSMs). nnMamba adds the SSMs to the convolutional residual-block to extract local features and model complex dependencies. For diffirent tasks, we build different blocks to learn the features. Extensive experiments demonstrate nnMamba's superiority over state-of-the-art methods in a suite of challenging tasks, including 3D image segmentation, classification, and landmark detection. nnMamba emerges as a robust solution, offering both the local representation ability of CNNs and the efficient global context processing of SSMs, setting a new standard for long-range dependency modeling in medical image analysis. Code is available at https://github.com/lhaof/nnMamba","sentences":["In the field of biomedical image analysis, the quest for architectures capable of effectively capturing long-range dependencies is paramount, especially when dealing with 3D image segmentation, classification, and landmark detection.","Traditional Convolutional Neural Networks (CNNs) struggle with locality respective field, and Transformers have a heavy computational load when applied to high-dimensional medical images.","In this paper, we introduce nnMamba, a novel architecture that integrates the strengths of CNNs and the advanced long-range modeling capabilities of State Space Sequence Models (SSMs).","nnMamba adds the SSMs to the convolutional residual-block to extract local features and model complex dependencies.","For diffirent tasks, we build different blocks to learn the features.","Extensive experiments demonstrate nnMamba's superiority over state-of-the-art methods in a suite of challenging tasks, including 3D image segmentation, classification, and landmark detection.","nnMamba emerges as a robust solution, offering both the local representation ability of CNNs and the efficient global context processing of SSMs, setting a new standard for long-range dependency modeling in medical image analysis.","Code is available at https://github.com/lhaof/nnMamba"],"url":"http://arxiv.org/abs/2402.03526v1","category":"cs.CV"}
{"created":"2024-02-05 21:03:54","title":"Deep Learning Cosmic Ray Transport from Density Maps of Simulated, Turbulent Gas","abstract":"The coarse-grained propagation of Galactic cosmic rays (CRs) is traditionally constrained by phenomenological models of Milky Way CR propagation fit to a variety of direct and indirect observables; however, constraining the fine-grained transport of CRs along individual magnetic field lines -- for instance, diffusive vs streaming transport models -- is an unsolved challenge. Leveraging a recent training set of magnetohydrodynamic turbulent box simulations, with CRs spanning a range of transport parameters, we use convolutional neural networks (CNNs) trained solely on gas density maps to classify CR transport regimes. We find that even relatively simple CNNs can quite effectively classify density slices to corresponding CR transport parameters, distinguishing between streaming and diffusive transport, as well as magnitude of diffusivity, with class accuracies between $92\\%$ and $99\\%$. As we show, the transport-dependent imprints that CRs leave on the gas are not all tied to the resulting density power spectra: classification accuracies are still high even when image spectra are flattened ($85\\%$ to $98\\%$ accuracy), highlighting CR transport-dependent changes to turbulent phase information. We interpret our results with saliency maps and image modifications, and we discuss physical insights and future applications.","sentences":["The coarse-grained propagation of Galactic cosmic rays (CRs) is traditionally constrained by phenomenological models of Milky Way CR propagation fit to a variety of direct and indirect observables; however, constraining the fine-grained transport of CRs along individual magnetic field lines -- for instance, diffusive vs streaming transport models -- is an unsolved challenge.","Leveraging a recent training set of magnetohydrodynamic turbulent box simulations, with CRs spanning a range of transport parameters, we use convolutional neural networks (CNNs) trained solely on gas density maps to classify CR transport regimes.","We find that even relatively simple CNNs can quite effectively classify density slices to corresponding CR transport parameters, distinguishing between streaming and diffusive transport, as well as magnitude of diffusivity, with class accuracies between $92\\%$ and $99\\%$. As we show, the transport-dependent imprints that CRs leave on the gas are not all tied to the resulting density power spectra: classification accuracies are still high even when image spectra are flattened ($85\\%$ to $98\\%$ accuracy), highlighting CR transport-dependent changes to turbulent phase information.","We interpret our results with saliency maps and image modifications, and we discuss physical insights and future applications."],"url":"http://arxiv.org/abs/2402.03518v1","category":"astro-ph.GA"}
{"created":"2024-02-05 21:03:34","title":"Spatially Consistent Air-to-Ground Channel Modeling via Generative Neural Networks","abstract":"This article proposes a generative neural network architecture for spatially consistent air-to-ground channel modeling. The approach considers the trajectories of uncrewed aerial vehicles along typical urban paths, capturing spatial dependencies within received signal strength (RSS) sequences from multiple cellular base stations (gNBs). Through the incorporation of conditioning data, the model accurately discriminates between gNBs and drives the correlation matrix distance between real and generated sequences to minimal values. This enables evaluating performance and mobility management metrics with spatially (and by extension temporally) consistent RSS values, rather than independent snapshots. For some tasks underpinned by these metrics, say handovers, consistency is essential.","sentences":["This article proposes a generative neural network architecture for spatially consistent air-to-ground channel modeling.","The approach considers the trajectories of uncrewed aerial vehicles along typical urban paths, capturing spatial dependencies within received signal strength (RSS) sequences from multiple cellular base stations (gNBs).","Through the incorporation of conditioning data, the model accurately discriminates between gNBs and drives the correlation matrix distance between real and generated sequences to minimal values.","This enables evaluating performance and mobility management metrics with spatially (and by extension temporally) consistent RSS values, rather than independent snapshots.","For some tasks underpinned by these metrics, say handovers, consistency is essential."],"url":"http://arxiv.org/abs/2402.03517v1","category":"cs.IT"}
{"created":"2024-02-05 21:03:19","title":"Bringing together two paradigms of non-equilibrium: Driven dynamics of aging systems","abstract":"There are two fundamental paradigms for non-equilibrium dynamics: on the one hand, aging towards an equilibrium state that cannot be reached on reasonable timescales; on the other, external driving that can lead to non-equilibrium steady states. We explore how these two mechanisms interact, by studying the behaviour of trap models, which are paradigmatic descriptions of slow glassy dynamics, when driven by trajectory bias towards high or low activity. To diagnose whether the driven systems continue to age, we establish a framework for mapping the biased dynamics to a Markovian time evolution with time-dependent transition rates. We find that the original aging dynamics reacts in two qualitatively distinct ways to the driving: it can be destroyed by driving of any nonzero strength (fragile aging), whereby the dynamics either reaches an active steady state or effectively freezes; or it can persist within a finite range of driving strengths around the undriven case (robust aging). This classification into fragile and robust aging could form the basis for distinguishing different universality classes of aging dynamics.","sentences":["There are two fundamental paradigms for non-equilibrium dynamics: on the one hand, aging towards an equilibrium state that cannot be reached on reasonable timescales; on the other, external driving that can lead to non-equilibrium steady states.","We explore how these two mechanisms interact, by studying the behaviour of trap models, which are paradigmatic descriptions of slow glassy dynamics, when driven by trajectory bias towards high or low activity.","To diagnose whether the driven systems continue to age, we establish a framework for mapping the biased dynamics to a Markovian time evolution with time-dependent transition rates.","We find that the original aging dynamics reacts in two qualitatively distinct ways to the driving: it can be destroyed by driving of any nonzero strength (fragile aging), whereby the dynamics either reaches an active steady state or effectively freezes; or it can persist within a finite range of driving strengths around the undriven case (robust aging).","This classification into fragile and robust aging could form the basis for distinguishing different universality classes of aging dynamics."],"url":"http://arxiv.org/abs/2402.03516v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 20:10:46","title":"Classification of Emerging Neural Activity from Planning to Grasp Execution using a Novel EEG-Based BCI Platform","abstract":"There have been different reports of developing Brain-Computer Interface (BCI) platforms to investigate the noninvasive electroencephalography (EEG) signals associated with plan-to-grasp tasks in humans. However, these reports were unable to clearly show evidence of emerging neural activity from the planning (observation) phase - dominated by the vision cortices - to grasp execution - dominated by the motor cortices. In this study, we developed a novel vision-based grasping BCI platform that distinguishes different grip types (power and precision) through the phases of plan-to-grasp tasks using EEG signals. Using our platform and extracting features from Filter Bank Common Spatial Patterns (FBCSP), we show that frequency-band specific EEG contains discriminative spatial patterns present in both the observation and movement phases. Support Vector Machine (SVM) classification (power vs precision) yielded high accuracy percentages of 74% and 68% for the observation and movement phases in the alpha band, respectively.","sentences":["There have been different reports of developing Brain-Computer Interface (BCI) platforms to investigate the noninvasive electroencephalography (EEG) signals associated with plan-to-grasp tasks in humans.","However, these reports were unable to clearly show evidence of emerging neural activity from the planning (observation) phase - dominated by the vision cortices - to grasp execution - dominated by the motor cortices.","In this study, we developed a novel vision-based grasping BCI platform that distinguishes different grip types (power and precision) through the phases of plan-to-grasp tasks using EEG signals.","Using our platform and extracting features from Filter Bank Common Spatial Patterns (FBCSP), we show that frequency-band specific EEG contains discriminative spatial patterns present in both the observation and movement phases.","Support Vector Machine (SVM) classification (power vs precision) yielded high accuracy percentages of 74% and 68% for the observation and movement phases in the alpha band, respectively."],"url":"http://arxiv.org/abs/2402.03493v1","category":"eess.SP"}
{"created":"2024-02-05 19:59:00","title":"Shooting Methods for Fractional Dirichlet-Type Boundary Value Problems of Order $\u03b1\\in (1,2)$ With Caputo Derivatives","abstract":"For the numerical solution of Dirichlet-type boundary value problems associated to nonlinear fractional differential equations of order $\\alpha \\in (1,2)$ that use Caputo derivatives, we suggest to employ shooting methods. In particular, we demonstrate that the so-called proportional secting technique for selecting the required initial values leads to numerical schemes that converge to high accuracy in a very small number of shooting iterations, and we provide an explanation of the analytical background for this favourable numerical behaviour.","sentences":["For the numerical solution of Dirichlet-type boundary value problems associated to nonlinear fractional differential equations of order $\\alpha \\in (1,2)$ that use Caputo derivatives, we suggest to employ shooting methods.","In particular, we demonstrate that the so-called proportional secting technique for selecting the required initial values leads to numerical schemes that converge to high accuracy in a very small number of shooting iterations, and we provide an explanation of the analytical background for this favourable numerical behaviour."],"url":"http://arxiv.org/abs/2402.03487v1","category":"math.NA"}
{"created":"2024-02-05 19:54:07","title":"On time-fractional partial differential equations of time-dependent piecewise constant order","abstract":"This contribution considers the time-fractional subdiffusion with a time-dependent variable-order fractional operator of order $\\beta(t)$. It is assumed that $\\beta(t)$ is a piecewise constant function with a finite number of jumps. A proof technique based on the Fourier method and results from constant-order fractional subdiffusion equations has been designed. This novel approach results in the well-posedness of the problem.","sentences":["This contribution considers the time-fractional subdiffusion with a time-dependent variable-order fractional operator of order $\\beta(t)$. It is assumed that $\\beta(t)$ is a piecewise constant function with a finite number of jumps.","A proof technique based on the Fourier method and results from constant-order fractional subdiffusion equations has been designed.","This novel approach results in the well-posedness of the problem."],"url":"http://arxiv.org/abs/2402.03482v1","category":"math.AP"}
{"created":"2024-02-05 19:39:07","title":"Arabic Synonym BERT-based Adversarial Examples for Text Classification","abstract":"Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification. Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English. In this paper, we introduce the first word-level study of adversarial attacks in Arabic. Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic. To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples. We also study the transferability of these newly produced Arabic adversarial examples to various models and investigate the effectiveness of defense mechanisms against these adversarial examples on the BERT models. We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that fine-tuned BERT models were more susceptible to transferred attacks. We, lastly, find that fine-tuned BERT models successfully regain at least 2% in accuracy after applying adversarial training as an initial defense mechanism.","sentences":["Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification.","Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English.","In this paper, we introduce the first word-level study of adversarial attacks in Arabic.","Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic.","To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples.","We also study the transferability of these newly produced Arabic adversarial examples to various models and investigate the effectiveness of defense mechanisms against these adversarial examples on the BERT models.","We find that fine-tuned BERT models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained.","We also find that fine-tuned BERT models were more susceptible to transferred attacks.","We, lastly, find that fine-tuned BERT models successfully regain at least 2% in accuracy after applying adversarial training as an initial defense mechanism."],"url":"http://arxiv.org/abs/2402.03477v1","category":"cs.CL"}
{"created":"2024-02-05 19:21:52","title":"Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact","abstract":"In robotics, it's crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries. We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We've made our code and dataset public to advance research in robotic simulation and grasping.","sentences":["In robotics, it's crucial to understand object deformation during tactile interactions.","A precise understanding of deformation can elevate robotic simulations and have broad implications across different industries.","We introduce a method using Physics-Encoded Graph Neural Networks (GNNs) for such predictions.","Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces.","Our approach represents both the soft body and the rigid body within graph structures, where nodes hold the physical states of the meshes.","We also incorporate cross-attention mechanisms to capture the interplay between the objects.","By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations.","We've made our code and dataset public to advance research in robotic simulation and grasping."],"url":"http://arxiv.org/abs/2402.03466v1","category":"cs.CV"}
{"created":"2024-02-05 19:11:57","title":"Breaking the Curse of Dimensionality with Distributed Neural Computation","abstract":"We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \\textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\\to \\mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\\mathcal{O}(\\varepsilon^{-1})$ parameters to be loaded in memory and $\\mathcal{O}(\\varepsilon^{-1}\\log(\\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\\mathcal{O}(\\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model that breaks the curse of dimensionality is MLPs with super-expressive activation functions. However, we demonstrate that these models have an infinite VC dimension, even with bounded depth and width restrictions, unlike the neural pathways model. This implies that only the latter generalizes. Our analysis is validated experimentally in both regression and classification tasks, demonstrating that our model exhibits superior performance compared to larger centralized benchmarks.","sentences":["We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines.","Our modular distributed deep learning paradigm, termed \\textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM.","Formally, we prove that for every error level $\\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\\to \\mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\\mathcal{O}(\\varepsilon^{-1})$ parameters to be loaded in memory and $\\mathcal{O}(\\varepsilon^{-1}\\log(\\varepsilon^{-1}))$ to be loaded during the forward pass.","This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\\mathcal{O}(\\varepsilon^{-n/2})$ parameters to achieve the same accuracy.","The only other available deep learning model that breaks the curse of dimensionality is MLPs with super-expressive activation functions.","However, we demonstrate that these models have an infinite VC dimension, even with bounded depth and width restrictions, unlike the neural pathways model.","This implies that only the latter generalizes.","Our analysis is validated experimentally in both regression and classification tasks, demonstrating that our model exhibits superior performance compared to larger centralized benchmarks."],"url":"http://arxiv.org/abs/2402.03460v1","category":"stat.ML"}
{"created":"2024-02-05 19:09:53","title":"Exact solutions via equivalence transformations of variable-coefficient fifth-order KdV equations","abstract":"In this paper, a family of variable-coefficient fifth-order KdV equations has been considered. By using an infinitesimal method based on the determination of the equivalence group, differential invariants and invariant equations are obtained. Invariants provide an alternative way to find equations from the family which may be equivalent to a specific subclass of the same family and the invertible transformation which maps both equivalent equations. Here, differential invariants are applied to obtain exact solutions.","sentences":["In this paper, a family of variable-coefficient fifth-order KdV equations has been considered.","By using an infinitesimal method based on the determination of the equivalence group, differential invariants and invariant equations are obtained.","Invariants provide an alternative way to find equations from the family which may be equivalent to a specific subclass of the same family and the invertible transformation which maps both equivalent equations.","Here, differential invariants are applied to obtain exact solutions."],"url":"http://arxiv.org/abs/2402.03458v1","category":"math.AP"}
{"created":"2024-02-05 19:09:42","title":"Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines","abstract":"Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving. Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system. Glass-box models offer a solution by providing full interpretability through methods like \\ac{GAM}. In this study, we evaluate an efficient additive model called \\ac{EBM} for traffic prediction on three popular mixed traffic datasets: \\ac{SDD}, \\ac{InD}, and Argoverse. Our results show that the \\ac{EBM} models perform competitively in predicting pedestrian destinations within \\ac{SDD} and \\ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset. Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation. The full training code will be made public upon publication.","sentences":["Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving.","Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system.","Glass-box models offer a solution by providing full interpretability through methods like \\ac{GAM}.","In this study, we evaluate an efficient additive model called \\ac{EBM} for traffic prediction on three popular mixed traffic datasets: \\ac{SDD}, \\ac{InD}, and Argoverse.","Our results show that the \\ac{EBM} models perform competitively in predicting pedestrian destinations within \\ac{SDD} and \\ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset.","Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation.","The full training code will be made public upon publication."],"url":"http://arxiv.org/abs/2402.03457v1","category":"cs.LG"}
{"created":"2024-02-05 19:00:45","title":"Denoising Diffusion via Image-Based Rendering","abstract":"Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we propose a denoising-diffusion framework to learn a prior over this novel 3D scene representation, using only 2D images without the need for any additional supervision signal such as masks or depths. This supports 3D reconstruction and generation in a unified architecture. Third, we develop a principled approach to avoid trivial 3D solutions when integrating the image-based rendering with the diffusion model, by dropping out representations of some images. We evaluate the model on several challenging datasets of real and synthetic images, and demonstrate superior results on generation, novel view synthesis and 3D reconstruction.","sentences":["Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space.","While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability.","Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers.","In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes.","To achieve this, we make three contributions.","First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image.","Second, we propose a denoising-diffusion framework to learn a prior over this novel 3D scene representation, using only 2D images without the need for any additional supervision signal such as masks or depths.","This supports 3D reconstruction and generation in a unified architecture.","Third, we develop a principled approach to avoid trivial 3D solutions when integrating the image-based rendering with the diffusion model, by dropping out representations of some images.","We evaluate the model on several challenging datasets of real and synthetic images, and demonstrate superior results on generation, novel view synthesis and 3D reconstruction."],"url":"http://arxiv.org/abs/2402.03445v1","category":"cs.CV"}
{"created":"2024-02-05 19:00:02","title":"Quantum mechanical bootstrap on the interval: obtaining the exact spectrum","abstract":"We show that for a particular model, the quantum mechanical bootstrap is capable of finding exact results. We consider a solvable system with Hamiltonian $H=SZ(1-Z)S$, where $Z$ and $S$ satisfy canonical commutation relations. While this model may appear unusual, using an appropriate coordinate transformation, the Schr\\\"odinger equation can be cast into a standard form with a P\\\"oschl-Teller-type potential. Since the system is defined on an interval, it is well-known that $S$ is not self-adjoint. Nevertheless, the bootstrap method can still be implemented, producing an infinite set of positivity constraints. Using a certain operator ordering, the energy eigenvalues are only constrained into bands. With an alternative ordering, however, we find that a finite number of constraints is sufficient to fix the low-lying energy levels exactly.","sentences":["We show that for a particular model, the quantum mechanical bootstrap is capable of finding exact results.","We consider a solvable system with Hamiltonian $H=SZ(1-Z)S$, where $Z$ and $S$ satisfy canonical commutation relations.","While this model may appear unusual, using an appropriate coordinate transformation, the Schr\\\"odinger equation can be cast into a standard form with a P\\\"oschl-Teller-type potential.","Since the system is defined on an interval, it is well-known that $S$ is not self-adjoint.","Nevertheless, the bootstrap method can still be implemented, producing an infinite set of positivity constraints.","Using a certain operator ordering, the energy eigenvalues are only constrained into bands.","With an alternative ordering, however, we find that a finite number of constraints is sufficient to fix the low-lying energy levels exactly."],"url":"http://arxiv.org/abs/2402.03434v1","category":"hep-th"}
{"created":"2024-02-05 19:00:01","title":"Microscopic inclusion statistics in a discrete 1-body spectrum","abstract":"We present the microscopic formulation of inclusion statistics, a counterpoint to exclusion statistics in which particles tend to coalesce more than ordinary bosons. We derive the microscopic occupation multiplicities of 1-body quantum states and show that they factorize into a product of clusters of neighbouring occupied states with enhanced statistical weights. Applying this statistics to a one-dimensional gas of particles in a harmonic well leads to a Calogero-like n-body inclusion spectrum with interesting physical properties.","sentences":["We present the microscopic formulation of inclusion statistics, a counterpoint to exclusion statistics in which particles tend to coalesce more than ordinary bosons.","We derive the microscopic occupation multiplicities of 1-body quantum states and show that they factorize into a product of clusters of neighbouring occupied states with enhanced statistical weights.","Applying this statistics to a one-dimensional gas of particles in a harmonic well leads to a Calogero-like n-body inclusion spectrum with interesting physical properties."],"url":"http://arxiv.org/abs/2402.03429v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-05 19:00:00","title":"The Heterogeneous Surface of Asteroid (16) Psyche","abstract":"Main-belt asteroid (16) Psyche is the largest M-type asteroid, a class of object classically thought to be the metal cores of differentiated planetesimals and the parent bodies of the iron meteorites. de Kleer, Cambioni, and Shepard (2021, https://doi.org/10.3847/psj/ac01ec) presented new data from the Atacama Large Millimeter Array (ALMA), from which they derived a global best-fit thermal inertia and dielectric constant for Psyche, proxies for regolith particle size, porosity, and/or metal content, and observed thermal anomalies that could not be explained by surface albedo variations only. Motivated by this, here we fit a model to the same ALMA data set that allows dielectric constant and thermal inertia to vary across the surface. We find that Psyche has a heterogeneous surface in both dielectric constant and thermal inertia but, intriguingly, we do not observe a direct correlation between these two properties over the surface. We explain the heterogeneity in dielectric constant as being due to variations in the relative abundance of metal and silicates. Furthermore, we observe that the lowlands of a large depression in Psyche's shape have distinctly lower thermal inertia than the surrounding highlands. We propose that the latter could be explained by a thin mantle of fine regolith, fractured bedrock, and/or implanted silicate-rich materials covering an otherwise metal-rich surface. All these scenarios are indicative of a collisionally evolved world.","sentences":["Main-belt asteroid (16) Psyche is the largest M-type asteroid, a class of object classically thought to be the metal cores of differentiated planetesimals and the parent bodies of the iron meteorites.","de Kleer, Cambioni, and Shepard (2021, https://doi.org/10.3847/psj/ac01ec) presented new data from the Atacama Large Millimeter Array (ALMA), from which they derived a global best-fit thermal inertia and dielectric constant for Psyche, proxies for regolith particle size, porosity, and/or metal content, and observed thermal anomalies that could not be explained by surface albedo variations only.","Motivated by this, here we fit a model to the same ALMA data set that allows dielectric constant and thermal inertia to vary across the surface.","We find that Psyche has a heterogeneous surface in both dielectric constant and thermal inertia but, intriguingly, we do not observe a direct correlation between these two properties over the surface.","We explain the heterogeneity in dielectric constant as being due to variations in the relative abundance of metal and silicates.","Furthermore, we observe that the lowlands of a large depression in Psyche's shape have distinctly lower thermal inertia than the surrounding highlands.","We propose that the latter could be explained by a thin mantle of fine regolith, fractured bedrock, and/or implanted silicate-rich materials covering an otherwise metal-rich surface.","All these scenarios are indicative of a collisionally evolved world."],"url":"http://arxiv.org/abs/2402.03422v1","category":"astro-ph.EP"}
{"created":"2024-02-05 18:59:31","title":"AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion","abstract":"Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering--based surface reconstruction methods. A website visualizing the results of our paper is located at this address: https://aoneus.github.io/","sentences":["Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring.","Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements.","In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging.","Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements.","By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines.","Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering--based surface reconstruction methods.","A website visualizing the results of our paper is located at this address: https://aoneus.github.io/"],"url":"http://arxiv.org/abs/2402.03309v1","category":"cs.CV"}
{"created":"2024-02-05 18:59:04","title":"4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes","abstract":"We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively.","sentences":["We consider the problem of novel view synthesis (NVS) for dynamic scenes.","Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial.","Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings.","In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes.","We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images.","As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions.","We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU.","Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2402.03307v1","category":"cs.CV"}
{"created":"2024-02-05 18:58:22","title":"Sharp $L^2$ estimates for the drift heat equation on shrinking Ricci solitons","abstract":"We prove an $L^2$ estimate for the drift heat equation on a complete gradient shrinking Ricci soliton. This estimate has a time-dependent weight which is Gaussian in its spatial asymptotics. When transferred and scaled to an estimate for the heat equation along the Ricci flow of the soliton, this estimate is uniform up to the singular time.","sentences":["We prove an $L^2$ estimate for the drift heat equation on a complete gradient shrinking Ricci soliton.","This estimate has a time-dependent weight which is Gaussian in its spatial asymptotics.","When transferred and scaled to an estimate for the heat equation along the Ricci flow of the soliton, this estimate is uniform up to the singular time."],"url":"http://arxiv.org/abs/2402.03304v1","category":"math.DG"}
{"created":"2024-02-05 18:58:11","title":"Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining","abstract":"Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%. The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba","sentences":["Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies.","However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism.","Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling.","Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden.","However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis.","This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining.","Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models.","Swin-UMamba demonstrates superior performance with a large margin compared to CNNs, ViTs, and latest Mamba-based models.","Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%.","The code and models of Swin-UMamba are publicly available at: https://github.com/JiarunLiu/Swin-UMamba"],"url":"http://arxiv.org/abs/2402.03302v1","category":"eess.IV"}
{"created":"2024-02-05 18:56:40","title":"On symmetries and conservation laws of a Gardner equation involving arbitrary functions","abstract":"In this work we study a generalized variable-coefficient Gardner equation from the point of view of Lie symmetries in partial differential equations. We find conservation laws by using the multipliers method of Anco and Bluman which does not require the use of a variational principle. We also construct conservation laws using Ibragimov theorem which is based on the concept of adjoint equation for nonlinear differential equations.","sentences":["In this work we study a generalized variable-coefficient Gardner equation from the point of view of Lie symmetries in partial differential equations.","We find conservation laws by using the multipliers method of Anco and Bluman which does not require the use of a variational principle.","We also construct conservation laws using Ibragimov theorem which is based on the concept of adjoint equation for nonlinear differential equations."],"url":"http://arxiv.org/abs/2402.03418v1","category":"math.AP"}
{"created":"2024-02-05 18:51:10","title":"Observation of the fractional quantum spin Hall effect in moir\u00e9 MoTe2","abstract":"Quantum spin Hall (QSH) insulators are two-dimensional electronic materials that have a bulk band gap like an ordinary insulator but have topologically protected pairs of edge modes of opposite chiralities. To date, experimental studies have found only integer QSH insulators with counter-propagating up-spins and down-spins at each edge leading to a quantized conductance G0=e^2/h. Here we report transport evidence of a fractional QSH insulator in 2.1-degree-twisted bilayer MoTe2, which supports spin-Sz conservation and flat spin-contrasting Chern bands. At filling factor v = 3 of the moir\\'e valence bands, each edge contributes a conductance 3/2 G0 with zero anomalous Hall conductivity. The state is likely a time-reversal pair of the even-denominator 3/2-fractional Chern insulators. Further, at v = 2, 4 and 6, we observe a single, double and triple QSH insulator with each edge contributing a conductance G0, 2G0 and 3G0, respectively. Our results open up the possibility of realizing time reversal symmetric non-abelian anyons and other unexpected topological phases in highly tunable moir\\'e materials.","sentences":["Quantum spin Hall (QSH) insulators are two-dimensional electronic materials that have a bulk band gap like an ordinary insulator but have topologically protected pairs of edge modes of opposite chiralities.","To date, experimental studies have found only integer QSH insulators with counter-propagating up-spins and down-spins at each edge leading to a quantized conductance G0=e^2/h. Here we report transport evidence of a fractional QSH insulator in 2.1-degree-twisted bilayer MoTe2, which supports spin-Sz conservation and flat spin-contrasting Chern bands.","At filling factor v = 3 of the moir\\'e valence bands, each edge contributes a conductance 3/2 G0 with zero anomalous Hall conductivity.","The state is likely a time-reversal pair of the even-denominator 3/2-fractional Chern insulators.","Further, at v = 2, 4 and 6, we observe a single, double and triple QSH insulator with each edge contributing a conductance G0, 2G0 and 3G0, respectively.","Our results open up the possibility of realizing time reversal symmetric non-abelian anyons and other unexpected topological phases in highly tunable moir\\'e materials."],"url":"http://arxiv.org/abs/2402.03294v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-05 18:22:41","title":"Symmetries and conservation laws of a fifth-order KdV equation with time-dependent coefficients and linear damping","abstract":"A fifth-order KdV equation with time dependent coefficients and linear damping has been studied. Symmetry groups have several different applications in the context of nonlinear differential equations. For instance, they can be used to determine conservation laws. We obtain the symmetries of the model applying Lie's classical method. The choice of some arbitrary functions of the equation by the equivalence transformation enhances the study of Lie symmetries of the equation. We have determined the subclasses of the equation which are nonlinearly self-adjoint. This allow us to obtain conservation laws by using a theorem proved by Ibragimov which is based on the concept of adjoint equation for nonlinear differential equations.","sentences":["A fifth-order KdV equation with time dependent coefficients and linear damping has been studied.","Symmetry groups have several different applications in the context of nonlinear differential equations.","For instance, they can be used to determine conservation laws.","We obtain the symmetries of the model applying Lie's classical method.","The choice of some arbitrary functions of the equation by the equivalence transformation enhances the study of Lie symmetries of the equation.","We have determined the subclasses of the equation which are nonlinearly self-adjoint.","This allow us to obtain conservation laws by using a theorem proved by Ibragimov which is based on the concept of adjoint equation for nonlinear differential equations."],"url":"http://arxiv.org/abs/2402.03265v1","category":"math.AP"}
{"created":"2024-02-05 18:08:22","title":"Nonsense associations in Markov random fields with pairwise dependence","abstract":"Yule (1926) identified the issue of \"nonsense correlations\" in time series data, where dependence within each of two random vectors causes overdispersion -- i.e. variance inflation -- for measures of dependence between the two. During the near century since then, much has been written about nonsense correlations -- but nearly all of it confined to the time series literature. In this paper we provide the first, to our knowledge, rigorous study of this phenomenon for more general forms of (positive) dependence, specifically for Markov random fields on lattices and graphs. We consider both binary and continuous random vectors and three different measures of association: correlation, covariance, and the ordinary least squares coefficient that results from projecting one random vector onto the other. In some settings we find variance inflation consistent with Yule's nonsense correlation. However, surprisingly, we also find variance deflation in some settings, and in others the variance is unchanged under dependence. Perhaps most notably, we find general conditions under which OLS inference that ignores dependence is valid despite positive dependence in the regression errors, contradicting the presentation of OLS in countless textbooks and courses.","sentences":["Yule (1926) identified the issue of \"nonsense correlations\" in time series data, where dependence within each of two random vectors causes overdispersion -- i.e. variance inflation -- for measures of dependence between the two.","During the near century since then, much has been written about nonsense correlations -- but nearly all of it confined to the time series literature.","In this paper we provide the first, to our knowledge, rigorous study of this phenomenon for more general forms of (positive) dependence, specifically for Markov random fields on lattices and graphs.","We consider both binary and continuous random vectors and three different measures of association: correlation, covariance, and the ordinary least squares coefficient that results from projecting one random vector onto the other.","In some settings we find variance inflation consistent with Yule's nonsense correlation.","However, surprisingly, we also find variance deflation in some settings, and in others the variance is unchanged under dependence.","Perhaps most notably, we find general conditions under which OLS inference that ignores dependence is valid despite positive dependence in the regression errors, contradicting the presentation of OLS in countless textbooks and courses."],"url":"http://arxiv.org/abs/2402.03249v1","category":"math.ST"}
{"created":"2024-02-05 18:05:53","title":"On a question of Gary G. Gundersen concerning meromorphic functions sharing three distinct values IM and a fourth value CM","abstract":"In 1992, Gundersen (Complex Var. Elliptic Equ.20 (1992), no. 1-4, 99-106.) proposed the following famous open question: if two non-constant meromorphic functions share three values IM and share a fourth value CM, then do the functions necessarily share all four values CM? The open question is a long-standing question in the studies of the Nevanlinna$'$s value distribution theory of meromorphic functions, and has not been completely resolved by now. In this paper, we prove that if two distinct non-constant meromorphic functions $f$ and $g$ of finite order share $0,$ $1,$ $c$ IM and $\\infty$ CM, where $c$ is a finite complex value such that $c\\not\\in\\{0,1\\},$ then $f$ and $g$ share $0,$ $1,$ $c,$ $\\infty$ CM. Applying the main result obtained in this paper, we completely resolve a question proposed by Gary G. Gundersen on Page 458 of his paper (J. London Math. Soc. 20(1979), no. 2, 457-466.)concerning the nonexistence of two distinct non-constant meromorphic functions sharing three distinct values DM and a fourth value CM. The obtained result also improves the corresponding result on Pages 109-117 in (E. Mues, Bemerkungen zum vier-punkte-satz, Complex Methods on Partial Diferential Equations, 109-117, Math. Res. 53, Akademie-Verlag, Berlin, 1989.) concerning the nonexistence of two distinct non-constant entire functions that share three distinct finite values DM. Examples are provided to show that the main results obtained in this paper, in a sense, are best possible.","sentences":["In 1992, Gundersen (Complex Var.","Elliptic Equ.20 (1992), no. 1-4, 99-106.) proposed the following famous open question: if two non-constant meromorphic functions share three values IM and share a fourth value CM, then do the functions necessarily share all four values CM?","The open question is a long-standing question in the studies of the Nevanlinna$'$s value distribution theory of meromorphic functions, and has not been completely resolved by now.","In this paper, we prove that if two distinct non-constant meromorphic functions $f$ and $g$ of finite order share $0,$ $1,$ $c$ IM and $\\infty$ CM, where $c$ is a finite complex value such that $c\\not\\in\\{0,1\\},$ then $f$ and $g$ share $0,$ $1,$ $c,$ $\\infty$ CM.","Applying the main result obtained in this paper, we completely resolve a question proposed by Gary G. Gundersen on Page 458 of his paper (J. London Math.","Soc.","20(1979)",", no. 2, 457-466.)concerning the nonexistence of two distinct non-constant meromorphic functions sharing three distinct values DM and a fourth value CM.","The obtained result also improves the corresponding result on Pages 109-117 in (E. Mues, Bemerkungen zum vier-punkte-satz, Complex Methods on Partial Diferential Equations, 109-117, Math. Res. 53, Akademie-Verlag, Berlin, 1989.)","concerning the nonexistence of two distinct non-constant entire functions that share three distinct finite values DM.","Examples are provided to show that the main results obtained in this paper, in a sense, are best possible."],"url":"http://arxiv.org/abs/2402.03248v2","category":"math.CV"}
{"created":"2024-02-05 18:00:15","title":"On the Popov-Belevitch-Hautus tests for functional observability and output controllability","abstract":"Functional observability and output controllability are properties that establish the conditions respectively for the partial estimation and partial control of the system state. In the special case of full-state observability and controllability, the Popov-Belevitch-Hautus (PBH) tests provide conditions for the properties to hold based on the system eigenspace. Generalizations of the Popov-Belevitch-Hautus (PBH) test have been recently proposed for functional observability and output controllability but were proved to be valid only for diagonalizable systems thus far. Here, we rigorously establish a more general class of systems based on their Jordan decomposition under which a generalized PBH test for functional observability is valid. Likewise, we determine the class of systems under which the generalized PBH test is sufficient and necessary for output controllability. These results have immediate implications for observer and controller design, pole assignment, and optimal placement of sensors and drivers.","sentences":["Functional observability and output controllability are properties that establish the conditions respectively for the partial estimation and partial control of the system state.","In the special case of full-state observability and controllability, the Popov-Belevitch-Hautus (PBH) tests provide conditions for the properties to hold based on the system eigenspace.","Generalizations of the Popov-Belevitch-Hautus (PBH) test have been recently proposed for functional observability and output controllability but were proved to be valid only for diagonalizable systems thus far.","Here, we rigorously establish a more general class of systems based on their Jordan decomposition under which a generalized PBH test for functional observability is valid.","Likewise, we determine the class of systems under which the generalized PBH test is sufficient and necessary for output controllability.","These results have immediate implications for observer and controller design, pole assignment, and optimal placement of sensors and drivers."],"url":"http://arxiv.org/abs/2402.03245v1","category":"math.OC"}
{"created":"2024-02-05 17:58:17","title":"PINN-BO: A Black-box Optimization Algorithm using Physics-Informed Neural Networks","abstract":"Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios. Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods. Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions. In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound. We perform several experiments on a variety of optimization tasks and show that our algorithm is more sample-efficient compared to existing methods.","sentences":["Black-box optimization is a powerful approach for discovering global optima in noisy and expensive black-box functions, a problem widely encountered in real-world scenarios.","Recently, there has been a growing interest in leveraging domain knowledge to enhance the efficacy of machine learning methods.","Partial Differential Equations (PDEs) often provide an effective means for elucidating the fundamental principles governing the black-box functions.","In this paper, we propose PINN-BO, a black-box optimization algorithm employing Physics-Informed Neural Networks that integrates the knowledge from Partial Differential Equations (PDEs) to improve the sample efficiency of the optimization.","We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory and prove that the use of the PDE alongside the black-box function evaluations, PINN-BO leads to a tighter regret bound.","We perform several experiments on a variety of optimization tasks and show that our algorithm is more sample-efficient compared to existing methods."],"url":"http://arxiv.org/abs/2402.03243v1","category":"cs.LG"}
{"created":"2024-02-05 17:30:42","title":"The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents","abstract":"We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamical process of the low-dimensional projections of the weights, and numerical experiments illustrating the theory.","sentences":["We investigate the training dynamics of two-layer neural networks when learning multi-index target functions.","We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent.","In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function.","We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021).","We characterize the (broad) class of functions efficiently learned in finite time.","The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT).","We further provide a closed-form description of the dynamical process of the low-dimensional projections of the weights, and numerical experiments illustrating the theory."],"url":"http://arxiv.org/abs/2402.03220v1","category":"stat.ML"}
{"created":"2024-02-05 17:28:35","title":"Determination of Schr\u00f6dinger nonlinearities from the scattering map","abstract":"We prove that the small-data scattering map uniquely determines the nonlinearity for a wide class of gauge-invariant, intercritical nonlinear Schr\\\"odinger equations. We use the Born approximation to reduce the analysis to a deconvolution problem involving the distribution function for linear Schr\\\"odinger solutions. We then solve this deconvolution problem using the Beurling--Lax Theorem.","sentences":["We prove that the small-data scattering map uniquely determines the nonlinearity for a wide class of gauge-invariant, intercritical nonlinear Schr\\\"odinger equations.","We use the Born approximation to reduce the analysis to a deconvolution problem involving the distribution function for linear Schr\\\"odinger solutions.","We then solve this deconvolution problem using the Beurling--Lax Theorem."],"url":"http://arxiv.org/abs/2402.03218v1","category":"math.AP"}
{"created":"2024-02-05 17:03:03","title":"Blow-up Whitney forms, shadow forms, and Poisson processes","abstract":"The Whitney forms on a simplex $T$ admit high-order generalizations that have received a great deal of attention in numerical analysis. Less well-known are the shadow forms of Brasselet, Goresky, and MacPherson. These forms generalize the Whitney forms, but have rational coefficients, allowing singularities near the faces of $T$. Motivated by numerical problems that exhibit these kinds of singularities, we introduce degrees of freedom for the shadow $k$-forms that are well-suited for finite element implementations. In particular, we show that the degrees of freedom for the shadow forms are given by integration over the $k$-dimensional faces of the blow-up $\\tilde T$ of the simplex $T$. Consequently, we obtain an isomorphism between the cohomology of the complex of shadow forms and the cellular cohomology of $\\tilde T$, which vanishes except in degree zero. Additionally, we discover a surprising probabilistic interpretation of shadow forms in terms of Poisson processes. This perspective simplifies several proofs and gives a way of computing bases for the shadow forms using a straightforward combinatorial calculation.","sentences":["The Whitney forms on a simplex $T$ admit high-order generalizations that have received a great deal of attention in numerical analysis.","Less well-known are the shadow forms of Brasselet, Goresky, and MacPherson.","These forms generalize the Whitney forms, but have rational coefficients, allowing singularities near the faces of $T$. Motivated by numerical problems that exhibit these kinds of singularities, we introduce degrees of freedom for the shadow $k$-forms that are well-suited for finite element implementations.","In particular, we show that the degrees of freedom for the shadow forms are given by integration over the $k$-dimensional faces of the blow-up $\\tilde T$ of the simplex $T$. Consequently, we obtain an isomorphism between the cohomology of the complex of shadow forms and the cellular cohomology of $\\tilde T$, which vanishes except in degree zero.","Additionally, we discover a surprising probabilistic interpretation of shadow forms in terms of Poisson processes.","This perspective simplifies several proofs and gives a way of computing bases for the shadow forms using a straightforward combinatorial calculation."],"url":"http://arxiv.org/abs/2402.03198v1","category":"math.NA"}
{"created":"2024-02-05 17:02:30","title":"An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping","abstract":"Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography (dFDG-PET) for human brain imaging has considerable clinical potential, yet its utilization remains limited. A key challenge in the quantitative analysis of dFDG-PET is characterizing a patient-specific blood input function, traditionally reliant on invasive arterial blood sampling. This research introduces a novel approach employing non-invasive deep learning model-based computations from the internal carotid arteries (ICA) with partial volume (PV) corrections, thereby eliminating the need for invasive arterial sampling. We present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the derivation of a model-corrected blood input function (MCIF) with PV corrections. The developed 3D U-Net and RNN was trained and validated using a 5-fold cross-validation approach on 50 human brain FDG PET datasets. The ICA-net achieved an average Dice score of 82.18% and an Intersection over Union of 68.54% across all tested scans. Furthermore, the MCIF-net exhibited a minimal root mean squared error of 0.0052. The application of this pipeline to ground truth data for dFDG-PET brain scans resulted in the precise localization of seizure onset regions, which contributed to a successful clinical outcome, with the patient achieving a seizure-free state after treatment. These results underscore the efficacy of the ICA-net and MCIF-net deep learning pipeline in learning the ICA structure's distribution and automating MCIF computation with PV corrections. This advancement marks a significant leap in non-invasive neuroimaging.","sentences":["Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography (dFDG-PET) for human brain imaging has considerable clinical potential, yet its utilization remains limited.","A key challenge in the quantitative analysis of dFDG-PET is characterizing a patient-specific blood input function, traditionally reliant on invasive arterial blood sampling.","This research introduces a novel approach employing non-invasive deep learning model-based computations from the internal carotid arteries (ICA) with partial volume (PV) corrections, thereby eliminating the need for invasive arterial sampling.","We present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the derivation of a model-corrected blood input function (MCIF) with PV corrections.","The developed 3D U-Net and RNN was trained and validated using a 5-fold cross-validation approach on 50 human brain FDG PET datasets.","The ICA-net achieved an average Dice score of 82.18% and an Intersection over Union of 68.54% across all tested scans.","Furthermore, the MCIF-net exhibited a minimal root mean squared error of 0.0052.","The application of this pipeline to ground truth data for dFDG-PET brain scans resulted in the precise localization of seizure onset regions, which contributed to a successful clinical outcome, with the patient achieving a seizure-free state after treatment.","These results underscore the efficacy of the ICA-net and MCIF-net deep learning pipeline in learning the ICA structure's distribution and automating MCIF computation with PV corrections.","This advancement marks a significant leap in non-invasive neuroimaging."],"url":"http://arxiv.org/abs/2402.03414v1","category":"eess.IV"}
{"created":"2024-02-05 16:53:54","title":"Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms","abstract":"Advances in face swapping have enabled the automatic generation of highly realistic faces. Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes. Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process. We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect. We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes. We confirm that viewed face swaps do elicit uncanny responses from viewers. Our proposed improvements significant reduce viewing angle errors between face swaps and their source material. Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks. Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications. Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach.","sentences":["Advances in face swapping have enabled the automatic generation of highly realistic faces.","Yet face swaps are perceived differently than when looking at real faces, with key differences in viewer behavior surrounding the eyes.","Face swapping algorithms generally place no emphasis on the eyes, relying on pixel or feature matching losses that consider the entire face to guide the training process.","We further investigate viewer perception of face swaps, focusing our analysis on the presence of an uncanny valley effect.","We additionally propose a novel loss equation for the training of face swapping models, leveraging a pretrained gaze estimation network to directly improve representation of the eyes.","We confirm that viewed face swaps do elicit uncanny responses from viewers.","Our proposed improvements significant reduce viewing angle errors between face swaps and their source material.","Our method additionally reduces the prevalence of the eyes as a deciding factor when viewers perform deepfake detection tasks.","Our findings have implications on face swapping for special effects, as digital avatars, as privacy mechanisms, and more; negative responses from users could limit effectiveness in said applications.","Our gaze improvements are a first step towards alleviating negative viewer perceptions via a targeted approach."],"url":"http://arxiv.org/abs/2402.03188v1","category":"cs.CV"}
{"created":"2024-02-05 16:51:59","title":"How Good is a Single Basin?","abstract":"The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles. In this work, we probe this belief by constructing various \"connected\" ensembles which are restricted to lie in the same basin. Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance. However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin. Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins.","sentences":["The multi-modal nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles.","In this work, we probe this belief by constructing various \"connected\" ensembles which are restricted to lie in the same basin.","Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance.","However, when incorporating the knowledge from other basins implicitly through distillation, we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin.","Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins."],"url":"http://arxiv.org/abs/2402.03187v1","category":"cs.LG"}
{"created":"2024-02-05 16:45:38","title":"Cool-chic video: Learned video coding with 800 parameters","abstract":"We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall. To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity. It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies. The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV. The system is made open-source: orange-opensource.github.io/Cool-Chic.","sentences":["We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall.","To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity.","It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies.","The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV.","The system is made open-source: orange-opensource.github.io/Cool-Chic."],"url":"http://arxiv.org/abs/2402.03179v2","category":"eess.IV"}
{"created":"2024-02-05 16:39:15","title":"Homograph Attacks on Maghreb Sentiment Analyzers","abstract":"We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in \"Arabizi\". The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning.","sentences":["We examine the impact of homograph attacks on the Sentiment Analysis (SA) task of different Arabic dialects from the Maghreb North-African countries.","Homograph attacks result in a 65.3% decrease in transformer classification from an F1-score of 0.95 to 0.33 when data is written in \"Arabizi\".","The goal of this study is to highlight LLMs weaknesses' and to prioritize ethical and responsible Machine Learning."],"url":"http://arxiv.org/abs/2402.03171v1","category":"cs.CL"}
{"created":"2024-02-05 16:35:29","title":"RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification","abstract":"The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions. A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique. Nonetheless, manually performing these tasks is labor-intensive and prone to human error. Various automated methods have been proposed to address this problem. However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps. This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors. The framework consists of a fully convolutional neural network with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps. Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches. In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency. The model code, weights, and predictions are publicly available at https://github.com/j-morano/rrwnet.","sentences":["The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions.","A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique.","Nonetheless, manually performing these tasks is labor-intensive and prone to human error.","Various automated methods have been proposed to address this problem.","However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps.","This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors.","The framework consists of a fully convolutional neural network with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps.","Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches.","In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency.","The model code, weights, and predictions are publicly available at https://github.com/j-morano/rrwnet."],"url":"http://arxiv.org/abs/2402.03166v1","category":"eess.IV"}
{"created":"2024-02-05 16:29:16","title":"Equations of genus $4$ curves from their theta constants","abstract":"In this article we give explicit formulas for the equations of a generic genus $4$ curve in terms of its theta constants. The method uses 20 tritangent planes as well as the Prym construction and the beautiful classical geometry around it.","sentences":["In this article we give explicit formulas for the equations of a generic genus $4$ curve in terms of its theta constants.","The method uses 20 tritangent planes as well as the Prym construction and the beautiful classical geometry around it."],"url":"http://arxiv.org/abs/2402.03160v1","category":"math.AG"}
{"created":"2024-02-05 16:24:27","title":"Trustworthiness of Optimality Condition Violation in Inverse Optimal Control Methods Based on the Minimum Principle","abstract":"In this work, we analyze the applicability of Inverse Optimal Control (IOC) methods based on the minimum principle (MP). The IOC method determines unknown cost functions in a single- or multi-agent setting from observed system trajectories by minimizing the so-called residual error, i.e. the extent to which the optimality conditions of the MP are violated with a current guess of cost functions. The main assumption of the IOC method to recover cost functions such that the resulting trajectories match the observed ones is that the given trajectories are the result of an OC problem with a known parameterized cost function structure. However, in practice, when the IOC method is used to identify the behavior of unknown agents, e.g. humans, this assumption cannot be guaranteed. Hence, we introduce the notion of the trustworthiness of the residual error and provide necessary conditions for it to define when the IOC method based on the MP is still applicable to such problems. From the necessary conditions, we conclude that the residual-based IOC method cannot be used to validate OC models for unknown agents. Finally, we illustrate this problem by validating a differential game model for the collision avoidance behavior between two mobile robots with human operators.","sentences":["In this work, we analyze the applicability of Inverse Optimal Control (IOC) methods based on the minimum principle (MP).","The IOC method determines unknown cost functions in a single- or multi-agent setting from observed system trajectories by minimizing the so-called residual error, i.e. the extent to which the optimality conditions of the MP are violated with a current guess of cost functions.","The main assumption of the IOC method to recover cost functions such that the resulting trajectories match the observed ones is that the given trajectories are the result of an OC problem with a known parameterized cost function structure.","However, in practice, when the IOC method is used to identify the behavior of unknown agents, e.g. humans, this assumption cannot be guaranteed.","Hence, we introduce the notion of the trustworthiness of the residual error and provide necessary conditions for it to define when the IOC method based on the MP is still applicable to such problems.","From the necessary conditions, we conclude that the residual-based IOC method cannot be used to validate OC models for unknown agents.","Finally, we illustrate this problem by validating a differential game model for the collision avoidance behavior between two mobile robots with human operators."],"url":"http://arxiv.org/abs/2402.03157v1","category":"math.OC"}
{"created":"2024-02-05 16:19:53","title":"Learning solutions of parametric Navier-Stokes with physics-informed neural networks","abstract":"We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE). Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE. We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters. We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest. Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters. Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function. We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum. Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels.","sentences":["We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE).","Our proposed approach results in a feasible optimization problem setup that bypasses PINNs' limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE.","We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters.","We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest.","Provision of training data from generated numerical simulations allows for interpolation of the solution functions for a range of parameters.","Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function.","We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum.","Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels."],"url":"http://arxiv.org/abs/2402.03153v1","category":"cs.CE"}
{"created":"2024-02-05 16:16:17","title":"A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators","abstract":"Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organizations with three different orders of these manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2) Aggregation-Splitting-Modulation-Weighting (ASMW), and (3) Splitting-Modulation-Weighting-Aggregation (SMWA). We show that these organizations affect the crosstalk noise and optical signal losses in different magnitudes, which renders these organizations with different levels of processing parallelism at the circuit level, and different magnitudes of throughput and energy-area efficiency at the system level. Our evaluation results for four CNN models show that SMWA organization achieves up to 4.4$\\times$, 5$\\times$, and 5.2$\\times$ better throughput, energy efficiency, and area-energy efficiency, respectively, compared to ASMW and MASW organizations on average.","sentences":["Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency.","To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals.","The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance.","In this paper, we conduct a detailed analysis of accelerator organizations with three different orders of these manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2) Aggregation-Splitting-Modulation-Weighting (ASMW), and (3) Splitting-Modulation-Weighting-Aggregation (SMWA).","We show that these organizations affect the crosstalk noise and optical signal losses in different magnitudes, which renders these organizations with different levels of processing parallelism at the circuit level, and different magnitudes of throughput and energy-area efficiency at the system level.","Our evaluation results for four CNN models show that SMWA organization achieves up to 4.4$\\times$, 5$\\times$, and 5.2$\\times$ better throughput, energy efficiency, and area-energy efficiency, respectively, compared to ASMW and MASW organizations on average."],"url":"http://arxiv.org/abs/2402.03149v1","category":"cs.AR"}
{"created":"2024-02-05 16:11:43","title":"Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models","abstract":"Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN.","sentences":["Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields.","Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications.","In this paper, we propose KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE).","KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state.","This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings.","Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%.","In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness.","Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN."],"url":"http://arxiv.org/abs/2402.03142v1","category":"cs.LG"}
{"created":"2024-02-05 16:10:52","title":"Locally Lipschitz stability of solutions to a parametric parabolic optimal control problem with mixed pointwise constraints","abstract":"A class of parametric optimal control problems governed by semilinear parabolic equations with mixed pointwise constraints is investigated. The perturbations appear in the objective functional, the state equation and in mixed pointwise constraints. By analyzing regularity and establishing stability condition of Lagrange multipliers we prove that, if the strictly second-order sufficient condition for the unperturbed problem is valid, then the solutions of the problems as well as the associated Lagrange multipliers are locally Lipschitz continuous functions of parameters.","sentences":["A class of parametric optimal control problems governed by semilinear parabolic equations with mixed pointwise constraints is investigated.","The perturbations appear in the objective functional, the state equation and in mixed pointwise constraints.","By analyzing regularity and establishing stability condition of Lagrange multipliers we prove that, if the strictly second-order sufficient condition for the unperturbed problem is valid, then the solutions of the problems as well as the associated Lagrange multipliers are locally Lipschitz continuous functions of parameters."],"url":"http://arxiv.org/abs/2402.03140v1","category":"math.OC"}
{"created":"2024-02-05 16:09:35","title":"Enhancing Neural Subset Selection: Integrating Background Information into Set Representations","abstract":"Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \\textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.","sentences":["Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications.","The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets.","However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions.","In this work, we address this oversight by adopting a probabilistic perspective.","Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \\textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning.","This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated.","Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective.","Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts."],"url":"http://arxiv.org/abs/2402.03139v1","category":"cs.LG"}
{"created":"2024-02-05 15:59:04","title":"Revivals, or the Talbot effect, for the Airy equation","abstract":"We study Dirichlet-type problems for the simplest third-order linear dispersive PDE, commonly called the Airy equation. Such problems have not been extensively studied, perhaps due to the complexity of the spectral structure of the spatial operator. Our specific interest is to determine whether the peculiar phenomenon of revivals, also known as Talbot effect, is supported by these boundary conditions, which for third order problems are not reducible to periodic ones. We prove that this is the case only for a very special choice of the boundary conditions, for which a new type of weak cusp revival phenomenon has been recently discovered. We also give some new results on the functional class of the solution for other cases.","sentences":["We study Dirichlet-type problems for the simplest third-order linear dispersive PDE, commonly called the Airy equation.","Such problems have not been extensively studied, perhaps due to the complexity of the spectral structure of the spatial operator.","Our specific interest is to determine whether the peculiar phenomenon of revivals, also known as Talbot effect, is supported by these boundary conditions, which for third order problems are not reducible to periodic ones.","We prove that this is the case only for a very special choice of the boundary conditions, for which a new type of weak cusp revival phenomenon has been recently discovered.","We also give some new results on the functional class of the solution for other cases."],"url":"http://arxiv.org/abs/2402.03133v2","category":"math.AP"}
{"created":"2024-02-05 15:55:34","title":"Clairaut anti-invariant Riemannian maps to Sasakian manifolds","abstract":"In this paper, we investigate the geometry of Clairaut anti-invariant Riemannnian maps whose base space are Sasakian manifolds. We obtain the necessary and sufficient conditions for a curve on a base manifold to be geodesic. We obtain conditions for an anti-invariant Riemannian map to be Clairaut. Further, we discuss the biharmonicity of such maps and construct some illustrative examples.","sentences":["In this paper, we investigate the geometry of Clairaut anti-invariant Riemannnian maps whose base space are Sasakian manifolds.","We obtain the necessary and sufficient conditions for a curve on a base manifold to be geodesic.","We obtain conditions for an anti-invariant Riemannian map to be Clairaut.","Further, we discuss the biharmonicity of such maps and construct some illustrative examples."],"url":"http://arxiv.org/abs/2402.03129v1","category":"math.DG"}
{"created":"2024-02-05 15:45:55","title":"Discovering interpretable models of scientific image data with deep learning","abstract":"How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images? Can we use such models to derive scientific insight from the data? In this paper, we propose some methods for achieving this. In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data. We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data. We find that such methods can produce highly parsimonious models that achieve $\\sim98\\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity. We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon.","sentences":["How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images?","Can we use such models to derive scientific insight from the data?","In this paper, we propose some methods for achieving this.","In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data.","We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data.","We find that such methods can produce highly parsimonious models that achieve $\\sim98\\%$ of the accuracy of black-box benchmark models, with a tiny fraction of the complexity.","We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon."],"url":"http://arxiv.org/abs/2402.03115v1","category":"cs.LG"}
{"created":"2024-02-05 15:44:16","title":"Computing roadmaps in unbounded smooth real algebraic sets II: algorithm and complexity","abstract":"A roadmap for an algebraic set $V$ defined by polynomials with coefficients in some real field, say $\\mathbb{R}$, is an algebraic curve contained in $V$ whose intersection with all connected components of $V\\cap\\mathbb{R}^{n}$ is connected. These objects, introduced by Canny, can be used to answer connectivity queries over $V\\cap \\mathbb{R}^{n}$ provided that they are required to contain the finite set of query points $\\mathcal{P}\\subset V$; in this case,we say that the roadmap is associated to $(V, \\mathcal{P})$.   In this paper, we make effective a connectivity result we previously proved, to design a Monte Carlo algorithm which, on input (i) a finite sequence of polynomials defining $V$ (and satisfying some regularity assumptions) and (ii) an algebraic representation of finitely many query points $\\mathcal{P}$ in $V$, computes a roadmap for $(V, \\mathcal{P})$. This algorithm generalizes the nearly optimal one introduced by the last two authors by dropping a boundedness assumption on the real trace of $V$.   The output size and running times of our algorithm are both polynomial in $(nD)^{n\\log d}$, where $D$ is the maximal degree of the input equations and $d$ is the dimension of $V$. As far as we know, the best previously known algorithm dealing with such sets has an output size and running time polynomial in $(nD)^{n\\log^2 n}$.","sentences":["A roadmap for an algebraic set $V$ defined by polynomials with coefficients in some real field, say $\\mathbb{R}$, is an algebraic curve contained in $V$ whose intersection with all connected components of $V\\cap\\mathbb{R}^{n}$ is connected.","These objects, introduced by Canny, can be used to answer connectivity queries over $V\\cap \\mathbb{R}^{n}$ provided that they are required to contain the finite set of query points $\\mathcal{P}\\subset V$; in this case,we say that the roadmap is associated to $(V, \\mathcal{P})$.   ","In this paper, we make effective a connectivity result we previously proved, to design a Monte Carlo algorithm which, on input (i) a finite sequence of polynomials defining $V$ (and satisfying some regularity assumptions) and (ii) an algebraic representation of finitely many query points $\\mathcal{P}$ in $V$, computes a roadmap for $(V, \\mathcal{P})$.","This algorithm generalizes the nearly optimal one introduced by the last two authors by dropping a boundedness assumption on the real trace of $V$.   The output size and running times of our algorithm are both polynomial in $(nD)^{n\\log d}$, where $D$ is the maximal degree of the input equations and $d$ is the dimension of $V$. As far as we know, the best previously known algorithm dealing with such sets has an output size and running time polynomial in $(nD)^{n\\log^2 n}$."],"url":"http://arxiv.org/abs/2402.03111v1","category":"cs.SC"}
{"created":"2024-02-05 15:31:41","title":"Scoped Effects as Parameterized Algebraic Theories","abstract":"Notions of computation can be modelled by monads. Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions. However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources. Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories. We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories. The translation enables a new approach to equational reasoning for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations. We demonstrate the power of our fresh perspective by way of equational characterizations of several known models of scoped effects.","sentences":["Notions of computation can be modelled by monads.","Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions.","However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources.","Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories.","We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories.","The translation enables a new approach to equational reasoning for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations.","We demonstrate the power of our fresh perspective by way of equational characterizations of several known models of scoped effects."],"url":"http://arxiv.org/abs/2402.03103v1","category":"cs.PL"}
{"created":"2024-02-06 18:56:35","title":"Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks","abstract":"State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.","sentences":["State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention.","Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers.","In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks.","Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning.","However, SSMs fall short in tasks involving non-standard retrieval functionality.","To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.","Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models."],"url":"http://arxiv.org/abs/2402.04248v1","category":"cs.LG"}
{"created":"2024-02-06 17:49:02","title":"Gradient Coding in Decentralized Learning for Evading Stragglers","abstract":"In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.","sentences":["In this paper, we consider a decentralized learning problem in the presence of stragglers.","Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios.","To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO).","In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner.","We analyze the convergence performance of GOCO for strongly convex loss functions.","And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods."],"url":"http://arxiv.org/abs/2402.04193v1","category":"cs.LG"}
{"created":"2024-02-06 17:31:20","title":"Scaling Laws for Downstream Task Performance of Large Language Models","abstract":"Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data. In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law. However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data.","sentences":["Scaling laws provide important insights that can guide the design of large language models (LLMs).","Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss.","However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance.","In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks.","Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score.","Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior.","With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data.","In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law.","However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves.","By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data."],"url":"http://arxiv.org/abs/2402.04177v1","category":"cs.CL"}
{"created":"2024-02-06 16:12:36","title":"Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science","abstract":"Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science.","sentences":["Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field.","In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM).","However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models.","To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition.","Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks.","Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering.","Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science."],"url":"http://arxiv.org/abs/2402.04119v1","category":"cs.LG"}
{"created":"2024-02-06 15:39:09","title":"Provably learning a multi-head attention layer","abstract":"The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\\{\\pm 1\\}^{k\\times d}$.   - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian. Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution. In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and \"slices\" thereof.","sentences":["The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models.","Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X})","\\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.","In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\\{\\pm 1\\}^{k\\times d}$.   - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   ","We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian.","Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution.","In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and \"slices\" thereof."],"url":"http://arxiv.org/abs/2402.04084v1","category":"cs.LG"}
{"created":"2024-02-06 14:06:23","title":"Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses","abstract":"Model Inversion (MI) attacks aim to disclose private information about the training data by abusing access to the pre-trained models. These attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training data, which has raised significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this field and presents a holistic survey. Firstly, our work briefly reviews the traditional MI on machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on \\textbf{D}eep \\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and learning tasks.","sentences":["Model Inversion (MI) attacks aim to disclose private information about the training data by abusing access to the pre-trained models.","These attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training data, which has raised significant privacy concerns.","Despite the rapid advances in the field, we lack a comprehensive overview of existing MI attacks and defenses.","To fill this gap, this paper thoroughly investigates this field and presents a holistic survey.","Firstly, our work briefly reviews the traditional MI on machine learning scenarios.","We then elaborately analyze and compare numerous recent attacks and defenses on \\textbf{D}eep \\textbf{N}eural \\textbf{N}etworks (DNNs) across multiple modalities and learning tasks."],"url":"http://arxiv.org/abs/2402.04013v1","category":"cs.CV"}
{"created":"2024-02-06 12:34:15","title":"Sparse Graph Representations for Procedural Instructional Documents","abstract":"Computation of document similarity is a critical task in various NLP domains that has applications in deduplication, matching, and recommendation. Traditional approaches for document similarity computation include learning representations of documents and employing a similarity or a distance function over the embeddings. However, pairwise similarities and differences are not efficiently captured by individual representations. Graph representations such as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a joint undirected weighted graph. JCIGs facilitate an interpretable representation of document pairs as a graph. However, JCIGs are undirected, and don't consider the sequential flow of sentences in documents. We propose two approaches to model document similarity by representing document pairs as a directed and sparse JCIG that incorporates sequential information. We propose two algorithms inspired by Supergenome Sorting and Hamiltonian Path that replace the undirected edges with directed edges. Our approach also sparsifies the graph to $O(n)$ edges from JCIG's worst case of $O(n^2)$. We show that our sparse directed graph model architecture consisting of a Siamese encoder and GCN achieves comparable results to the baseline on datasets not containing sequential information and beats the baseline by ten points on an instructional documents dataset containing sequential information.","sentences":["Computation of document similarity is a critical task in various NLP domains that has applications in deduplication, matching, and recommendation.","Traditional approaches for document similarity computation include learning representations of documents and employing a similarity or a distance function over the embeddings.","However, pairwise similarities and differences are not efficiently captured by individual representations.","Graph representations such as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a joint undirected weighted graph.","JCIGs facilitate an interpretable representation of document pairs as a graph.","However, JCIGs are undirected, and don't consider the sequential flow of sentences in documents.","We propose two approaches to model document similarity by representing document pairs as a directed and sparse JCIG that incorporates sequential information.","We propose two algorithms inspired by Supergenome Sorting and Hamiltonian Path that replace the undirected edges with directed edges.","Our approach also sparsifies the graph to $O(n)$ edges from JCIG's worst case of $O(n^2)$. We show that our sparse directed graph model architecture consisting of a Siamese encoder and GCN achieves comparable results to the baseline on datasets not containing sequential information and beats the baseline by ten points on an instructional documents dataset containing sequential information."],"url":"http://arxiv.org/abs/2402.03957v1","category":"cs.CL"}
{"created":"2024-02-06 12:20:35","title":"Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding","abstract":"This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning. The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size. This compressed encoding is combined with an estimate of the robot's odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance. A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments.","sentences":["This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and reinforcement learning.","The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using supervised learning such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size.","This compressed encoding is combined with an estimate of the robot's odometry and the desired target location to train a deep reinforcement learning navigation policy that offers low-latency computation and robust sim2real performance.","A set of simulation and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments."],"url":"http://arxiv.org/abs/2402.03947v1","category":"cs.RO"}
{"created":"2024-02-06 11:35:02","title":"Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning","abstract":"Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks. Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art.","sentences":["Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data.","In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone.","This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting.","To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias.","Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM).","The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks.","Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art."],"url":"http://arxiv.org/abs/2402.03917v1","category":"cs.CV"}
{"created":"2024-02-06 10:28:07","title":"Binaural sound source localization using a hybrid time and frequency domain model","abstract":"This paper introduces a new approach to sound source localization using head-related transfer function (HRTF) characteristics, which enable precise full-sphere localization from raw data. While previous research focused primarily on using extensive microphone arrays in the frontal plane, this arrangement often encountered limitations in accuracy and robustness when dealing with smaller microphone arrays. Our model proposes using both time and frequency domain for sound source localization while utilizing Deep Learning (DL) approach. The performance of our proposed model, surpasses the current state-of-the-art results. Specifically, it boasts an average angular error of $0.24 degrees and an average Euclidean distance of 0.01 meters, while the known state-of-the-art gives average angular error of 19.07 degrees and average Euclidean distance of 1.08 meters. This level of accuracy is of paramount importance for a wide range of applications, including robotics, virtual reality, and aiding individuals with cochlear implants (CI).","sentences":["This paper introduces a new approach to sound source localization using head-related transfer function (HRTF) characteristics, which enable precise full-sphere localization from raw data.","While previous research focused primarily on using extensive microphone arrays in the frontal plane, this arrangement often encountered limitations in accuracy and robustness when dealing with smaller microphone arrays.","Our model proposes using both time and frequency domain for sound source localization while utilizing Deep Learning (DL) approach.","The performance of our proposed model, surpasses the current state-of-the-art results.","Specifically, it boasts an average angular error of $0.24 degrees and an average Euclidean distance of 0.01 meters, while the known state-of-the-art gives average angular error of 19.07 degrees and average Euclidean distance of 1.08 meters.","This level of accuracy is of paramount importance for a wide range of applications, including robotics, virtual reality, and aiding individuals with cochlear implants (CI)."],"url":"http://arxiv.org/abs/2402.03867v1","category":"cs.SD"}
{"created":"2024-02-06 09:50:06","title":"Non-Hemolytic Peptide Classification Using A Quantum Support Vector Machine","abstract":"Quantum machine learning (QML) is one of the most promising applications of quantum computation. However, it is still unclear whether quantum advantages exist when the data is of a classical nature and the search for practical, real-world applications of QML remains active. In this work, we apply the well-studied quantum support vector machine (QSVM), a powerful QML model, to a binary classification task which classifies peptides as either hemolytic or non-hemolytic. Using three peptide datasets, we apply and contrast the performance of the QSVM, numerous classical SVMs, and the best published results on the same peptide classification task, out of which the QSVM performs best. The contributions of this work include (i) the first application of the QSVM to this specific peptide classification task, (ii) an explicit demonstration of QSVMs outperforming the best published results attained with classical machine learning models on this classification task and (iii) empirical results showing that the QSVM is capable of outperforming many (and possibly all) classical SVMs on this classification task. This foundational work paves the way to verifiable quantum advantages in the field of computational biology and facilitates safer therapeutic development.","sentences":["Quantum machine learning (QML) is one of the most promising applications of quantum computation.","However, it is still unclear whether quantum advantages exist when the data is of a classical nature and the search for practical, real-world applications of QML remains active.","In this work, we apply the well-studied quantum support vector machine (QSVM), a powerful QML model, to a binary classification task which classifies peptides as either hemolytic or non-hemolytic.","Using three peptide datasets, we apply and contrast the performance of the QSVM, numerous classical SVMs, and the best published results on the same peptide classification task, out of which the QSVM performs best.","The contributions of this work include (i) the first application of the QSVM to this specific peptide classification task, (ii) an explicit demonstration of QSVMs outperforming the best published results attained with classical machine learning models on this classification task and (iii) empirical results showing that the QSVM is capable of outperforming many (and possibly all) classical SVMs on this classification task.","This foundational work paves the way to verifiable quantum advantages in the field of computational biology and facilitates safer therapeutic development."],"url":"http://arxiv.org/abs/2402.03847v1","category":"quant-ph"}
{"created":"2024-02-06 09:07:41","title":"Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies","abstract":"Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.","sentences":["Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets.","Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples.","We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy.","Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures.","We show that rebalancing strategies are only required when the data set is highly imbalanced.","For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies."],"url":"http://arxiv.org/abs/2402.03819v1","category":"stat.ML"}
{"created":"2024-02-06 09:00:05","title":"Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression","abstract":"Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing works because the first phase can guarantee consensus compression across clients. The PS easily aligns model update indices to swiftly complete aggregation in the second phase. Finally, we conduct extensive experiments by using public datasets to demonstrate that FediAC remarkably surpasses the state-of-the-art baselines in terms of model accuracy and communication traffic.","sentences":["Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy.","To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet.","To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients.","The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS.","To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating.","In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates.","In the latter phase, clients upload global significant model updates to the PS for aggregation.","FediAC consumes much less memory space and communication traffic than existing works because the first phase can guarantee consensus compression across clients.","The PS easily aligns model update indices to swiftly complete aggregation in the second phase.","Finally, we conduct extensive experiments by using public datasets to demonstrate that FediAC remarkably surpasses the state-of-the-art baselines in terms of model accuracy and communication traffic."],"url":"http://arxiv.org/abs/2402.03815v1","category":"cs.LG"}
{"created":"2024-02-06 07:57:13","title":"Weakly Supervised Anomaly Detection via Knowledge-Data Alignment","abstract":"Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types.","sentences":["Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis.","Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels.","Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance.","Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies.","In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data.","Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data.","To facilitate this alignment, we employ the Optimal Transport (OT) technique.","We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies.","Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types."],"url":"http://arxiv.org/abs/2402.03785v1","category":"cs.LG"}
{"created":"2024-02-06 07:53:23","title":"Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning","abstract":"Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method MedPrompt to automatically generate medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt requires no manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design. Experimental results show that the model using our automatically generated prompts outperforms its full-shot learning hand-crafted prompts counterparts with only a minimal number of labeled samples for few-shot learning, and reaches superior or comparable accuracy on zero-shot image classification. The proposed prompt generator is lightweight and therefore can be embedded into any network architecture.","sentences":["Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional.","This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models via relevant medical text prompts.","However, existing pre-trained vision-language models require domain experts to carefully design the medical prompts, which greatly increases the burden on clinicians.","To address this problem, we propose a weakly supervised prompt learning method MedPrompt to automatically generate medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model.","The unsupervised pre-trained vision-language model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations.","The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt requires no manual annotations for guidance.","To the best of our knowledge, this is the first model to automatically generate medical prompts.","With these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design.","Experimental results show that the model using our automatically generated prompts outperforms its full-shot learning hand-crafted prompts counterparts with only a minimal number of labeled samples for few-shot learning, and reaches superior or comparable accuracy on zero-shot image classification.","The proposed prompt generator is lightweight and therefore can be embedded into any network architecture."],"url":"http://arxiv.org/abs/2402.03783v1","category":"cs.CV"}
{"created":"2024-02-06 07:35:36","title":"Encoding Version History Context for Better Code Representation","abstract":"With the exponential growth of AI tools that generate source code, understanding software has become crucial. When developers comprehend a program, they may refer to additional contexts to look for information, e.g. program documentation or historical code versions. Therefore, we argue that encoding this additional contextual information could also benefit code representation for deep learning. Recent papers incorporate contextual data (e.g. call hierarchy) into vector representation to address program comprehension problems. This motivates further studies to explore additional contexts, such as version history, to enhance models' understanding of programs. That is, insights from version history enable recognition of patterns in code evolution over time, recurring issues, and the effectiveness of past solutions. Our paper presents preliminary evidence of the potential benefit of encoding contextual information from the version history to predict code clones and perform code classification. We experiment with two representative deep learning models, ASTNN and CodeBERT, to investigate whether combining additional contexts with different aggregations may benefit downstream activities. The experimental result affirms the positive impact of combining version history into source code representation in all scenarios; however, to ensure the technique performs consistently, we need to conduct a holistic investigation on a larger code base using different combinations of contexts, aggregation, and models. Therefore, we propose a research agenda aimed at exploring various aspects of encoding additional context to improve code representation and its optimal utilisation in specific situations.","sentences":["With the exponential growth of AI tools that generate source code, understanding software has become crucial.","When developers comprehend a program, they may refer to additional contexts to look for information, e.g. program documentation or historical code versions.","Therefore, we argue that encoding this additional contextual information could also benefit code representation for deep learning.","Recent papers incorporate contextual data (e.g. call hierarchy) into vector representation to address program comprehension problems.","This motivates further studies to explore additional contexts, such as version history, to enhance models' understanding of programs.","That is, insights from version history enable recognition of patterns in code evolution over time, recurring issues, and the effectiveness of past solutions.","Our paper presents preliminary evidence of the potential benefit of encoding contextual information from the version history to predict code clones and perform code classification.","We experiment with two representative deep learning models, ASTNN and CodeBERT, to investigate whether combining additional contexts with different aggregations may benefit downstream activities.","The experimental result affirms the positive impact of combining version history into source code representation in all scenarios; however, to ensure the technique performs consistently, we need to conduct a holistic investigation on a larger code base using different combinations of contexts, aggregation, and models.","Therefore, we propose a research agenda aimed at exploring various aspects of encoding additional context to improve code representation and its optimal utilisation in specific situations."],"url":"http://arxiv.org/abs/2402.03773v1","category":"cs.SE"}
{"created":"2024-02-06 06:48:46","title":"The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs","abstract":"Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images. The resource is available in https://github.com/MasaiahHan/CorrelationQA.","sentences":["Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks.","However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs.","In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination.","To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images.","This benchmark contains 7,308 text-image pairs across 13 categories.","Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees.","We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images.","The resource is available in https://github.com/MasaiahHan/CorrelationQA."],"url":"http://arxiv.org/abs/2402.03757v1","category":"cs.CV"}
{"created":"2024-02-06 06:05:11","title":"An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem","abstract":"The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph. MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction. In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones. To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions. Then, we propose a new BnB algorithm that uses the initial lower bound and PUB in preprocessing for graph reduction, and uses PUB in the BnB search process for branch pruning. Extensive experiments with diverse s values demonstrate the significant progress of our algorithm over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can also be generalized to other relaxation clique problems.","sentences":["The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph.","A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph.","MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity.","Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction.","In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones.","To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions.","Then, we propose a new BnB algorithm that uses the initial lower bound and PUB in preprocessing for graph reduction, and uses PUB in the BnB search process for branch pruning.","Extensive experiments with diverse s values demonstrate the significant progress of our algorithm over state-of-the-art BnB MBP algorithms.","Moreover, our initial lower bound can also be generalized to other relaxation clique problems."],"url":"http://arxiv.org/abs/2402.03736v1","category":"cs.DS"}
{"created":"2024-02-06 05:40:53","title":"Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos","abstract":"Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html","sentences":["Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications.","The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency.","However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings.","In this paper, we introduce Rig3DGS to address this challenge.","We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space.","Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering.","Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model.","This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures.","We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments.","The project page can be found at http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html"],"url":"http://arxiv.org/abs/2402.03723v1","category":"cs.CV"}
{"created":"2024-02-06 04:42:36","title":"Improving and Unifying Discrete&Continuous-time Discrete Denoising Diffusion","abstract":"Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.","sentences":["Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs.","Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion.","However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability.","In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion.","In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion.","Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects.","Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets.","We open-source our unified code at https://github.com/LingxiaoShawn/USD3."],"url":"http://arxiv.org/abs/2402.03701v1","category":"cs.LG"}
{"created":"2024-02-06 04:37:09","title":"Estimating the Local Learning Coefficient at Scale","abstract":"The \\textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT). Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. Using a method developed in {\\tt arXiv:2308.12108 [stat.ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters. We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity.","sentences":["The \\textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT).","Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets.","Using a method developed in {\\tt arXiv:2308.12108","[stat.","ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters.","We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity."],"url":"http://arxiv.org/abs/2402.03698v1","category":"cs.LG"}
{"created":"2024-02-06 04:33:51","title":"SHMC-Net: A Mask-guided Feature Fusion Network for Sperm Head Morphology Classification","abstract":"Male infertility accounts for about one-third of global infertility cases. Manual assessment of sperm abnormalities through head morphology analysis encounters issues of observer variability and diagnostic discrepancies among experts. Its alternative, Computer-Assisted Semen Analysis (CASA), suffers from low-quality sperm images, small datasets, and noisy class labels. We propose a new approach for sperm head morphology classification, called SHMC-Net, which uses segmentation masks of sperm heads to guide the morphology classification of sperm images. SHMC-Net generates reliable segmentation masks using image priors, refines object boundaries with an efficient graph-based method, and trains an image network with sperm head crops and a mask network with the corresponding masks. In the intermediate stages of the networks, image and mask features are fused with a fusion scheme to better learn morphological features. To handle noisy class labels and regularize training on small datasets, SHMC-Net applies Soft Mixup to combine mixup augmentation and a loss function. We achieve state-of-the-art results on SCIAN and HuSHeM datasets, outperforming methods that use additional pre-training or costly ensembling techniques.","sentences":["Male infertility accounts for about one-third of global infertility cases.","Manual assessment of sperm abnormalities through head morphology analysis encounters issues of observer variability and diagnostic discrepancies among experts.","Its alternative, Computer-Assisted Semen Analysis (CASA), suffers from low-quality sperm images, small datasets, and noisy class labels.","We propose a new approach for sperm head morphology classification, called SHMC-Net, which uses segmentation masks of sperm heads to guide the morphology classification of sperm images.","SHMC-Net generates reliable segmentation masks using image priors, refines object boundaries with an efficient graph-based method, and trains an image network with sperm head crops and a mask network with the corresponding masks.","In the intermediate stages of the networks, image and mask features are fused with a fusion scheme to better learn morphological features.","To handle noisy class labels and regularize training on small datasets, SHMC-Net applies Soft Mixup to combine mixup augmentation and a loss function.","We achieve state-of-the-art results on SCIAN and HuSHeM datasets, outperforming methods that use additional pre-training or costly ensembling techniques."],"url":"http://arxiv.org/abs/2402.03697v1","category":"cs.CV"}
{"created":"2024-02-06 04:30:49","title":"ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation","abstract":"Studying the morphological development of cartilaginous and osseous structures is critical to the early detection of life-threatening skeletal dysmorphology. Embryonic cartilage undergoes rapid structural changes within hours, introducing biological variations and morphological shifts that limit the generalization of deep learning-based segmentation models that infer across multiple embryonic age groups. Obtaining individual models for each age group is expensive and less effective, while direct transfer (predicting an age unseen during training) suffers a potential performance drop due to morphological shifts. We propose a novel Transformer-based segmentation model with improved biological priors that better distills morphologically diverse information through conditional mechanisms. This enables a single model to accurately predict cartilage across multiple age groups. Experiments on the mice cartilage dataset show the superiority of our new model compared to other competitive segmentation models. Additional studies on a separate mice cartilage dataset with a distinct mutation show that our model generalizes well and effectively captures age-based cartilage morphology patterns.","sentences":["Studying the morphological development of cartilaginous and osseous structures is critical to the early detection of life-threatening skeletal dysmorphology.","Embryonic cartilage undergoes rapid structural changes within hours, introducing biological variations and morphological shifts that limit the generalization of deep learning-based segmentation models that infer across multiple embryonic age groups.","Obtaining individual models for each age group is expensive and less effective, while direct transfer (predicting an age unseen during training) suffers a potential performance drop due to morphological shifts.","We propose a novel Transformer-based segmentation model with improved biological priors that better distills morphologically diverse information through conditional mechanisms.","This enables a single model to accurately predict cartilage across multiple age groups.","Experiments on the mice cartilage dataset show the superiority of our new model compared to other competitive segmentation models.","Additional studies on a separate mice cartilage dataset with a distinct mutation show that our model generalizes well and effectively captures age-based cartilage morphology patterns."],"url":"http://arxiv.org/abs/2402.03695v1","category":"eess.IV"}
{"created":"2024-02-06 04:26:50","title":"On the non-universality of heavy quark hadronization in elementary high-energy collisions","abstract":"It has been traditionally hypothesized that the heavy quark (charm, $c$ and bottom, $b$) fragmentation is universal across different collision systems, based on the notion that hadronization as a soft process should occur at the characteristic non-perturbative QCD scale, $\\Lambda_{QCD}$. However, this universality hypothesis has recently been challenged by the observation that the $c$- and $b$-baryon production relative to their meson counterparts in minimum bias proton-proton ($pp$) collisions at the LHC energies is significantly enhanced as compared to the electron-positron ($e^+e^-$) collisions. The conception of non-universality is unambiguously reinforced by the latest measurement of the charged-particle multiplicity dependence of the $b$-baryon-to-meson yield ratio, $\\Lambda_b/B$, by the LHCb experiment in $\\sqrt{s}=13$\\,TeV $pp$ collisions at the LHC, evolving continuously from the saturation value in minimum bias $pp$ collisions toward the small value in $e^+e^-$ collisions as the system size gradually reduces. We address the multiplicity dependence of $b$-baryon production in the canonical statistical hadronization model with input $b$-hadron spectrum augmented with many hitherto unobserved states from quark model predictions. We demonstrate that the decreasing trend of the $\\Lambda_b/B$ toward low multiplicities can be quantitatively understood from the canonical suppression on the yield of $\\Lambda_b$, as caused by the requirement of strict conservation of baryon number in sufficiently small systems. We have therefore proposed a plausible scenario for understanding the origin of the non-universality of heavy quark fragmentation in elementary collisions.","sentences":["It has been traditionally hypothesized that the heavy quark (charm, $c$ and bottom, $b$) fragmentation is universal across different collision systems, based on the notion that hadronization as a soft process should occur at the characteristic non-perturbative QCD scale, $\\Lambda_{QCD}$. However, this universality hypothesis has recently been challenged by the observation that the $c$- and $b$-baryon production relative to their meson counterparts in minimum bias proton-proton ($pp$) collisions at the LHC energies is significantly enhanced as compared to the electron-positron ($e^+e^-$) collisions.","The conception of non-universality is unambiguously reinforced by the latest measurement of the charged-particle multiplicity dependence of the $b$-baryon-to-meson yield ratio, $\\Lambda_b/B$, by the LHCb experiment in $\\sqrt{s}=13$\\,TeV $pp$ collisions at the LHC, evolving continuously from the saturation value in minimum bias $pp$ collisions toward the small value in $e^+e^-$ collisions as the system size gradually reduces.","We address the multiplicity dependence of $b$-baryon production in the canonical statistical hadronization model with input $b$-hadron spectrum augmented with many hitherto unobserved states from quark model predictions.","We demonstrate that the decreasing trend of the $\\Lambda_b/B$ toward low multiplicities can be quantitatively understood from the canonical suppression on the yield of $\\Lambda_b$, as caused by the requirement of strict conservation of baryon number in sufficiently small systems.","We have therefore proposed a plausible scenario for understanding the origin of the non-universality of heavy quark fragmentation in elementary collisions."],"url":"http://arxiv.org/abs/2402.03692v1","category":"hep-ph"}
{"created":"2024-02-06 04:17:44","title":"Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation","abstract":"Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.","sentences":["Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering.","Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant.","Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance.","We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods.","PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity.","Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges.","With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network.","To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN.","Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks.","Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules."],"url":"http://arxiv.org/abs/2402.03687v1","category":"cs.LG"}
{"created":"2024-02-06 04:01:24","title":"\"Life\" of dust originating from the irregular satellites of Jupiter","abstract":"The irregular satellites of Jupiter produce dust particles through the impact of interplanetary micrometeoroids. In this paper, the dynamics of these particles is studied by both high-accuracy numerical simulation and analytical theory, in order to learn their transport, final fate, and spatial distribution. The perturbation forces that are considered in our dynamical model include the solar radiation pressure, solar gravity, Poynting-Robertson drag, Jovian oblateness, and the Galilean satellites' gravity. The trajectories of different size particles are simulated until they hit Jupiter, the Galilean satellites, or escape from the Jovian system. The average dynamical lifetimes of dust with different grain sizes are calculated, and the final fate of dust particles is reported and analysed. The steady-state spatial number density of particles is estimated by integrating the trajectories of dust particles over their initial size distribution, and compared to the previous work. The impact sites of dust on Callisto's surface are recorded and provide an important clue for the study of the hemisphere asymmetry of Callisto. Besides, the mass accretion rate, cross-sectional area influx, and mass influx density of dust on Callisto are calculated. A ring outside the orbit of Callisto dominated by dust between 2 and 25 ${\\mu}$m from Jupiter's irregular satellites is suggested, with the average normal geometric optical depth of the order of $10^{-8}$ and the configuration of the ring ansae similar to Jupiter's gossamer rings.","sentences":["The irregular satellites of Jupiter produce dust particles through the impact of interplanetary micrometeoroids.","In this paper, the dynamics of these particles is studied by both high-accuracy numerical simulation and analytical theory, in order to learn their transport, final fate, and spatial distribution.","The perturbation forces that are considered in our dynamical model include the solar radiation pressure, solar gravity, Poynting-Robertson drag, Jovian oblateness, and the Galilean satellites' gravity.","The trajectories of different size particles are simulated until they hit Jupiter, the Galilean satellites, or escape from the Jovian system.","The average dynamical lifetimes of dust with different grain sizes are calculated, and the final fate of dust particles is reported and analysed.","The steady-state spatial number density of particles is estimated by integrating the trajectories of dust particles over their initial size distribution, and compared to the previous work.","The impact sites of dust on Callisto's surface are recorded and provide an important clue for the study of the hemisphere asymmetry of Callisto.","Besides, the mass accretion rate, cross-sectional area influx, and mass influx density of dust on Callisto are calculated.","A ring outside the orbit of Callisto dominated by dust between 2 and 25 ${\\mu}$m from Jupiter's irregular satellites is suggested, with the average normal geometric optical depth of the order of $10^{-8}$ and the configuration of the ring ansae similar to Jupiter's gossamer rings."],"url":"http://arxiv.org/abs/2402.03680v1","category":"astro-ph.EP"}
{"created":"2024-02-06 03:36:05","title":"Efficient Solvers for Partial Gromov-Wasserstein","abstract":"The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.","sentences":["The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces.","In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem.","This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem.","We further establish that the PGW problem constitutes a metric for metric measure spaces.","Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines."],"url":"http://arxiv.org/abs/2402.03664v1","category":"cs.LG"}
{"created":"2024-02-06 02:56:07","title":"Temporal Graph Analysis with TGX","abstract":"Real-world networks, with their evolving relations, are best captured as temporal graphs. However, existing software libraries are largely designed for static graphs where the dynamic nature of temporal graphs is ignored. Bridging this gap, we introduce TGX, a Python package specially designed for analysis of temporal networks that encompasses an automated pipeline for data loading, data processing, and analysis of evolving graphs. TGX provides access to eleven built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as well as any novel datasets in the .csv format. Beyond data loading, TGX facilitates data processing functionalities such as discretization of temporal graphs and node subsampling to accelerate working with larger datasets. For comprehensive investigation, TGX offers network analysis by providing a diverse set of measures, including average node degree and the evolving number of nodes and edges per timestamp. Additionally, the package consolidates meaningful visualization plots indicating the evolution of temporal patterns, such as Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots. The TGX package is a robust tool for examining the features of temporal graphs and can be used in various areas like studying social networks, citation networks, and tracking user interactions. We plan to continuously support and update TGX based on community feedback. TGX is publicly available on: https://github.com/ComplexData-MILA/TGX.","sentences":["Real-world networks, with their evolving relations, are best captured as temporal graphs.","However, existing software libraries are largely designed for static graphs where the dynamic nature of temporal graphs is ignored.","Bridging this gap, we introduce TGX, a Python package specially designed for analysis of temporal networks that encompasses an automated pipeline for data loading, data processing, and analysis of evolving graphs.","TGX provides access to eleven built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as well as any novel datasets in the .csv format.","Beyond data loading, TGX facilitates data processing functionalities such as discretization of temporal graphs and node subsampling to accelerate working with larger datasets.","For comprehensive investigation, TGX offers network analysis by providing a diverse set of measures, including average node degree and the evolving number of nodes and edges per timestamp.","Additionally, the package consolidates meaningful visualization plots indicating the evolution of temporal patterns, such as Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots.","The TGX package is a robust tool for examining the features of temporal graphs and can be used in various areas like studying social networks, citation networks, and tracking user interactions.","We plan to continuously support and update TGX based on community feedback.","TGX is publicly available on: https://github.com/ComplexData-MILA/TGX."],"url":"http://arxiv.org/abs/2402.03651v1","category":"cs.SI"}
{"created":"2024-02-06 02:50:42","title":"Multilinear Kernel Regression and Imputation via Manifold Learning","abstract":"This paper introduces a novel nonparametric framework for data imputation, coined multilinear kernel regression and imputation via the manifold assumption (MultiL-KRIM). Motivated by manifold learning, MultiL-KRIM models data features as a point cloud located in or close to a user-unknown smooth manifold embedded in a reproducing kernel Hilbert space. Unlike typical manifold-learning routes, which seek low-dimensional patterns via regularizers based on graph-Laplacian matrices, MultiL-KRIM builds instead on the intuitive concept of tangent spaces to manifolds and incorporates collaboration among point-cloud neighbors (regressors) directly into the data-modeling term of the loss function. Multiple kernel functions are allowed to offer robustness and rich approximation properties, while multiple matrix factors offer low-rank modeling, integrate dimensionality reduction, and streamline computations with no need of training data. Two important application domains showcase the functionality of MultiL-KRIM: time-varying-graph-signal (TVGS) recovery, and reconstruction of highly accelerated dynamic-magnetic-resonance-imaging (dMRI) data. Extensive numerical tests on real and synthetic data demonstrate MultiL-KRIM's remarkable speedups over its predecessors, and outperformance over prevalent \"shallow\" data-imputation techniques, with a more intuitive and explainable pipeline than deep-image-prior methods.","sentences":["This paper introduces a novel nonparametric framework for data imputation, coined multilinear kernel regression and imputation via the manifold assumption (MultiL-KRIM).","Motivated by manifold learning, MultiL-KRIM models data features as a point cloud located in or close to a user-unknown smooth manifold embedded in a reproducing kernel Hilbert space.","Unlike typical manifold-learning routes, which seek low-dimensional patterns via regularizers based on graph-Laplacian matrices, MultiL-KRIM builds instead on the intuitive concept of tangent spaces to manifolds and incorporates collaboration among point-cloud neighbors (regressors) directly into the data-modeling term of the loss function.","Multiple kernel functions are allowed to offer robustness and rich approximation properties, while multiple matrix factors offer low-rank modeling, integrate dimensionality reduction, and streamline computations with no need of training data.","Two important application domains showcase the functionality of MultiL-KRIM: time-varying-graph-signal (TVGS) recovery, and reconstruction of highly accelerated dynamic-magnetic-resonance-imaging (dMRI) data.","Extensive numerical tests on real and synthetic data demonstrate MultiL-KRIM's remarkable speedups over its predecessors, and outperformance over prevalent \"shallow\" data-imputation techniques, with a more intuitive and explainable pipeline than deep-image-prior methods."],"url":"http://arxiv.org/abs/2402.03648v1","category":"eess.SP"}
{"created":"2024-02-06 02:45:13","title":"Lens: A Foundation Model for Network Traffic","abstract":"Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from large-scale network traffic. To further enhance pre-training performance, we design a novel loss that integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results on multiple benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and traffic generation. Notably, it also requires considerably less labeled data for fine-tuning compared to current methods.","sentences":["Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers.","Analyzing and understanding network traffic is vital for improving network security and management.","However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics.","To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data.","However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks.","To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data.","Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from large-scale network traffic.","To further enhance pre-training performance, we design a novel loss that integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP).","Evaluation results on multiple benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and traffic generation.","Notably, it also requires considerably less labeled data for fine-tuning compared to current methods."],"url":"http://arxiv.org/abs/2402.03646v1","category":"cs.LG"}
{"created":"2024-02-06 02:39:59","title":"Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation","abstract":"The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classification as an effective tool for identifying multicultural misinformation.","sentences":["The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation.","In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish.","The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine.","The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms.","By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues.","To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages.","This underlines the viability of stance classification as an effective tool for identifying multicultural misinformation."],"url":"http://arxiv.org/abs/2402.03642v1","category":"cs.CL"}
{"created":"2024-02-06 02:17:08","title":"Lossy Cryptography from Code-Based Assumptions","abstract":"Over the past few decades, we have seen a proliferation of advanced cryptographic primitives with lossy or homomorphic properties built from various assumptions such as Quadratic Residuosity, Decisional Diffie-Hellman, and Learning with Errors. These primitives imply hard problems in the complexity class $SZK$ (statistical zero-knowledge); as a consequence, they can only be based on assumptions that are broken in $BPP^{SZK}$. This poses a barrier for building advanced primitives from code-based assumptions, as the only known such assumption is Learning Parity with Noise (LPN) with an extremely low noise rate $\\frac{\\log^2 n}{n}$, which is broken in quasi-polynomial time.   In this work, we propose a new code-based assumption: Dense-Sparse LPN, that falls in the complexity class $BPP^{SZK}$ and is conjectured to be secure against subexponential time adversaries. Our assumption is a variant of LPN that is inspired by McEliece's cryptosystem and random $k\\mbox{-}$XOR in average-case complexity.   We leverage our assumption to build lossy trapdoor functions (Peikert-Waters STOC 08). This gives the first post-quantum alternative to the lattice-based construction in the original paper. Lossy trapdoor functions, being a fundamental cryptographic tool, are known to enable a broad spectrum of both lossy and non-lossy cryptographic primitives; our construction thus implies these primitives in a generic manner. In particular, we achieve collision-resistant hash functions with plausible subexponential security, improving over a prior construction from LPN with noise rate $\\frac{\\log^2 n}{n}$ that is only quasi-polynomially secure.","sentences":["Over the past few decades, we have seen a proliferation of advanced cryptographic primitives with lossy or homomorphic properties built from various assumptions such as Quadratic Residuosity, Decisional Diffie-Hellman, and Learning with Errors.","These primitives imply hard problems in the complexity class $SZK$ (statistical zero-knowledge); as a consequence, they can only be based on assumptions that are broken in $BPP^{SZK}$. This poses a barrier for building advanced primitives from code-based assumptions, as the only known such assumption is Learning Parity with Noise (LPN) with an extremely low noise rate $\\frac{\\log^2 n}{n}$, which is broken in quasi-polynomial time.   ","In this work, we propose a new code-based assumption: Dense-Sparse LPN, that falls in the complexity class $BPP^{SZK}$ and is conjectured to be secure against subexponential time adversaries.","Our assumption is a variant of LPN that is inspired by McEliece's cryptosystem and random $k\\mbox{-}$XOR in average-case complexity.   ","We leverage our assumption to build lossy trapdoor functions (Peikert-Waters STOC 08).","This gives the first post-quantum alternative to the lattice-based construction in the original paper.","Lossy trapdoor functions, being a fundamental cryptographic tool, are known to enable a broad spectrum of both lossy and non-lossy cryptographic primitives; our construction thus implies these primitives in a generic manner.","In particular, we achieve collision-resistant hash functions with plausible subexponential security, improving over a prior construction from LPN with noise rate $\\frac{\\log^2 n}{n}$","that is only quasi-polynomially secure."],"url":"http://arxiv.org/abs/2402.03633v1","category":"cs.CR"}
{"created":"2024-02-06 01:06:17","title":"Environment-Centric Learning Approach for Gait Synthesis in Terrestrial Soft Robots","abstract":"Locomotion gaits are fundamental for control of soft terrestrial robots. However, synthesis of these gaits is challenging due to modeling of robot-environment interaction and lack of a mathematical framework. This work presents an environment-centric, data-driven and fault-tolerant probabilistic Model-Free Control (pMFC) framework that allows for soft multi-limb robots to learn from their environment and synthesize diverse sets of locomotion gaits for realizing open-loop control. Here, discretization of factors dominating robot-environment interactions enables an environment-specific graphical representation where the edges encode experimental locomotion data corresponding to the robot motion primitives. In this graph, locomotion gaits are defined as simple cycles that are transformation invariant, i.e., the locomotion is independent of the starting vertex of these periodic cycles. Gait synthesis, the problem of finding optimal locomotion gaits for a given substrate, is formulated as Binary Integer Linear Programming (BILP) problems with a linearized cost function, linear constraints, and iterative simple cycle detection. Experimentally, gaits are synthesized for varying robot-environment interactions. Variables include robot morphology - three-limb and four-limb robots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and carpet; and actuator functionality - simulated loss of robot limb actuation. On an average, gait synthesis improves the translation and rotation speeds by 82% and 97% respectively. The results highlight that data-driven methods are vital to soft robot locomotion control due to the significant influence of unexpected asymmetries in the system and the dependence of optimal gait sequences on the experimental robot-environment interaction.","sentences":["Locomotion gaits are fundamental for control of soft terrestrial robots.","However, synthesis of these gaits is challenging due to modeling of robot-environment interaction and lack of a mathematical framework.","This work presents an environment-centric, data-driven and fault-tolerant probabilistic Model-Free Control (pMFC) framework that allows for soft multi-limb robots to learn from their environment and synthesize diverse sets of locomotion gaits for realizing open-loop control.","Here, discretization of factors dominating robot-environment interactions enables an environment-specific graphical representation where the edges encode experimental locomotion data corresponding to the robot motion primitives.","In this graph, locomotion gaits are defined as simple cycles that are transformation invariant, i.e., the locomotion is independent of the starting vertex of these periodic cycles.","Gait synthesis, the problem of finding optimal locomotion gaits for a given substrate, is formulated as Binary Integer Linear Programming (BILP) problems with a linearized cost function, linear constraints, and iterative simple cycle detection.","Experimentally, gaits are synthesized for varying robot-environment interactions.","Variables include robot morphology - three-limb and four-limb robots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and carpet; and actuator functionality - simulated loss of robot limb actuation.","On an average, gait synthesis improves the translation and rotation speeds by 82% and 97% respectively.","The results highlight that data-driven methods are vital to soft robot locomotion control due to the significant influence of unexpected asymmetries in the system and the dependence of optimal gait sequences on the experimental robot-environment interaction."],"url":"http://arxiv.org/abs/2402.03617v1","category":"cs.RO"}
{"created":"2024-02-06 18:59:24","title":"Computing Approximate Nash Equilibria for Integer Programming Games","abstract":"We propose a framework to compute approximate Nash equilibria in integer programming games with nonlinear payoffs, i.e., simultaneous and non-cooperative games where each player solves a parametrized mixed-integer nonlinear program. We prove that using absolute approximations of the players' objective functions and then computing its Nash equilibria is equivalent to computing approximate Nash equilibria where the approximation factor is doubled. In practice, we propose an algorithm to approximate the players' objective functions via piecewise linear approximations. Our numerical experiments on a cybersecurity investment game show the computational effectiveness of our approach.","sentences":["We propose a framework to compute approximate Nash equilibria in integer programming games with nonlinear payoffs, i.e., simultaneous and non-cooperative games where each player solves a parametrized mixed-integer nonlinear program.","We prove that using absolute approximations of the players' objective functions and then computing its Nash equilibria is equivalent to computing approximate Nash equilibria where the approximation factor is doubled.","In practice, we propose an algorithm to approximate the players' objective functions via piecewise linear approximations.","Our numerical experiments on a cybersecurity investment game show the computational effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.04250v1","category":"math.OC"}
{"created":"2024-02-06 18:02:19","title":"Production-inventory games and pmas games: characterizations of the Owen point","abstract":"Production-inventory games were introduced in Guardiola et al. (2007) as a new class of totally balanced combinatorial optimization games. From among all core-allocations, the Owen point was proposed as a specifically appealing solution. In this paper we study some relationships of the class of production-inventory games and other classes of new and known games. In addition, we propose three axiomatic characterizations of the Owen point. We use eight axioms for these characterizations, among those, inessentiality and additivity of players' demands are used for the first time in this paper.","sentences":["Production-inventory games were introduced in Guardiola et al.","(2007) as a new class of totally balanced combinatorial optimization games.","From among all core-allocations, the Owen point was proposed as a specifically appealing solution.","In this paper we study some relationships of the class of production-inventory games and other classes of new and known games.","In addition, we propose three axiomatic characterizations of the Owen point.","We use eight axioms for these characterizations, among those, inessentiality and additivity of players' demands are used for the first time in this paper."],"url":"http://arxiv.org/abs/2402.04208v1","category":"cs.GT"}
{"created":"2024-02-06 17:13:41","title":"Optimal weighted Wente's inequality","abstract":"We prove $L^\\infty$ and $W^{1,2}$ weighted Wente's inequalities. We prove in particular the critical case: for the $|x|^2$ weighted Wente's estimate the optimal weight is $|x|^2\\log|x|$.","sentences":["We prove $L^\\infty$ and $W^{1,2}$ weighted Wente's inequalities.","We prove in particular the critical case: for the $|x|^2$ weighted Wente's estimate the optimal weight is $|x|^2\\log|x|$."],"url":"http://arxiv.org/abs/2402.04156v1","category":"math.AP"}
{"created":"2024-02-06 17:03:01","title":"$L^\\infty$-optimal transport of anisotropic log-concave measures and exponential convergence in Fisher's infinitesimal model","abstract":"We prove upper bounds on the $L^\\infty$-Wasserstein distance from optimal transport between strongly log-concave probability densities and log-Lipschitz perturbations. In the simplest setting, such a bound amounts to a transport-information inequality involving the $L^\\infty$-Wasserstein metric and the relative $L^\\infty$-Fisher information. We show that this inequality can be sharpened significantly in situations where the involved densities are anisotropic. Our proof is based on probabilistic techniques using Langevin dynamics. As an application of these results, we obtain sharp exponential rates of convergence in Fisher's infinitesimal model from quantitative genetics, generalising recent results by Calvez, Poyato, and Santambrogio in dimension 1 to arbitrary dimensions.","sentences":["We prove upper bounds on the $L^\\infty$-Wasserstein distance from optimal transport between strongly log-concave probability densities and log-Lipschitz perturbations.","In the simplest setting, such a bound amounts to a transport-information inequality involving the $L^\\infty$-Wasserstein metric and the relative $L^\\infty$-Fisher information.","We show that this inequality can be sharpened significantly in situations where the involved densities are anisotropic.","Our proof is based on probabilistic techniques using Langevin dynamics.","As an application of these results, we obtain sharp exponential rates of convergence in Fisher's infinitesimal model from quantitative genetics, generalising recent results by Calvez, Poyato, and Santambrogio in dimension 1 to arbitrary dimensions."],"url":"http://arxiv.org/abs/2402.04151v1","category":"math.PR"}
{"created":"2024-02-06 15:52:23","title":"Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction","abstract":"The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects.In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.This study sheds light on important underlying properties for DIP-based recovery.Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.","sentences":["The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI).","However, conventional DIP suffers from severe overfitting and spectral bias effects.","In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.","This study sheds light on important underlying properties for DIP-based recovery.","Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs.","However, obtaining suitable reference images requires supervision, and raises practical difficulties.","In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data.","Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.","We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting."],"url":"http://arxiv.org/abs/2402.04097v1","category":"cs.CV"}
{"created":"2024-02-06 14:32:13","title":"Low-Distortion Clustering with Ordinal and Limited Cardinal Information","abstract":"Motivated by recent work in computational social choice, we extend the metric distortion framework to clustering problems. Given a set of $n$ agents located in an underlying metric space, our goal is to partition them into $k$ clusters, optimizing some social cost objective. The metric space is defined by a distance function $d$ between the agent locations. Information about $d$ is available only implicitly via $n$ rankings, through which each agent ranks all other agents in terms of their distance from her. Still, we would like to evaluate clustering algorithms in terms of social cost objectives that are defined using $d$. This is done using the notion of distortion, which measures how far from optimality a clustering can be, taking into account all underlying metrics that are consistent with the ordinal information available. Unfortunately, the most important clustering objectives do not admit algorithms with finite distortion. To sidestep this disappointing fact, we follow two alternative approaches: We first explore whether resource augmentation can be beneficial. We consider algorithms that use more than $k$ clusters but compare their social cost to that of the optimal $k$-clusterings. We show that using exponentially (in terms of $k$) many clusters, we can get low (constant or logarithmic) distortion for the $k$-center and $k$-median objectives. Interestingly, such an exponential blowup is shown to be necessary. More importantly, we explore whether limited cardinal information can be used to obtain better results. Somewhat surprisingly, for $k$-median and $k$-center, we show that a number of queries that is polynomial in $k$ and only logarithmic in $n$ (i.e., only sublinear in the number of agents for the most relevant scenarios in practice) is enough to get constant distortion.","sentences":["Motivated by recent work in computational social choice, we extend the metric distortion framework to clustering problems.","Given a set of $n$ agents located in an underlying metric space, our goal is to partition them into $k$ clusters, optimizing some social cost objective.","The metric space is defined by a distance function $d$ between the agent locations.","Information about $d$ is available only implicitly via $n$ rankings, through which each agent ranks all other agents in terms of their distance from her.","Still, we would like to evaluate clustering algorithms in terms of social cost objectives that are defined using $d$. This is done using the notion of distortion, which measures how far from optimality a clustering can be, taking into account all underlying metrics that are consistent with the ordinal information available.","Unfortunately, the most important clustering objectives do not admit algorithms with finite distortion.","To sidestep this disappointing fact, we follow two alternative approaches: We first explore whether resource augmentation can be beneficial.","We consider algorithms that use more than $k$ clusters but compare their social cost to that of the optimal $k$-clusterings.","We show that using exponentially (in terms of $k$) many clusters, we can get low (constant or logarithmic) distortion for the $k$-center and $k$-median objectives.","Interestingly, such an exponential blowup is shown to be necessary.","More importantly, we explore whether limited cardinal information can be used to obtain better results.","Somewhat surprisingly, for $k$-median and $k$-center, we show that a number of queries that is polynomial in $k$ and only logarithmic in $n$ (i.e., only sublinear in the number of agents for the most relevant scenarios in practice) is enough to get constant distortion."],"url":"http://arxiv.org/abs/2402.04035v1","category":"cs.GT"}
{"created":"2024-02-06 13:51:52","title":"Optimal partitions of the flat torus into parts of smaller diameter","abstract":"We consider the problem of partitioning a two-dimensional flat torus $T^2$ into $m$ sets in order to minimize the maximal diameter of a part. For $m \\leqslant 25$ we give numerical estimates for the maximal diameter $d_m(T^2)$ at which the partition exists. Several approaches are proposed to obtain such estimates. In particular, we use the search for mesh partitions via the SAT solver, the global optimization approach for polygonal partitions, and the optimization of periodic hexagonal tilings. For $m=3$, the exact estimate is proved using elementary topological reasoning.","sentences":["We consider the problem of partitioning a two-dimensional flat torus $T^2$ into $m$ sets in order to minimize the maximal diameter of a part.","For $m \\leqslant 25$ we give numerical estimates for the maximal diameter $d_m(T^2)$ at which the partition exists.","Several approaches are proposed to obtain such estimates.","In particular, we use the search for mesh partitions via the SAT solver, the global optimization approach for polygonal partitions, and the optimization of periodic hexagonal tilings.","For $m=3$, the exact estimate is proved using elementary topological reasoning."],"url":"http://arxiv.org/abs/2402.03997v1","category":"math.MG"}
{"created":"2024-02-06 09:58:41","title":"Design and optimization of CdSe-CuSbSe2-based double-junction two-terminal tandem solar cells with VOC> 2.0 V and PCE over 42%","abstract":"In this article, we demonstrate CdSe-CuSbSe2-based double junction two-terminal tandem solar cells simulated with SCAPS-1D. The highest performance of the tandem cell has been confirmed by optimizing the electrical and optical properties of window, top absorber, CdSe (bandgap 1.7 eV), bottom absorber, CuSbSe2 (bandgap 1.08 eV) and back surface layers. In addition, the effect of different parameters such as thickness, doping, defect density of different layers has been investigated in details. With the optimized condition, the modeled CdSe-CuSbSe2 double-junction two-terminal tandem solar cell displays the noticeable efficiency of 42.64% with open circuit voltage of 2.09 V, short circuit current density of 24.09 mA/cm2 and fill factor of 84.36%, respectively. These results are highly propitious for the construction of all-chalcogenide based high performance tandem photovoltaic cells in the future.","sentences":["In this article, we demonstrate CdSe-CuSbSe2-based double junction two-terminal tandem solar cells simulated with SCAPS-1D. The highest performance of the tandem cell has been confirmed by optimizing the electrical and optical properties of window, top absorber, CdSe (bandgap 1.7 eV), bottom absorber, CuSbSe2 (bandgap 1.08 eV) and back surface layers.","In addition, the effect of different parameters such as thickness, doping, defect density of different layers has been investigated in details.","With the optimized condition, the modeled CdSe-CuSbSe2 double-junction two-terminal tandem solar cell displays the noticeable efficiency of 42.64% with open circuit voltage of 2.09 V, short circuit current density of 24.09 mA/cm2 and fill factor of 84.36%, respectively.","These results are highly propitious for the construction of all-chalcogenide based high performance tandem photovoltaic cells in the future."],"url":"http://arxiv.org/abs/2402.03853v1","category":"physics.app-ph"}
{"created":"2024-02-06 08:57:12","title":"FDO Manager: Minimum Viable FAIR Digital Object Implementation","abstract":"The concept of FAIR Digital Objects (FDOs) aims to revolutionise the field of digital preservation and accessibility in the next few years. Central to this revolution is the alignment of FDOs with the FAIR (Findable, Accessible, Interoperable, Reusable) Principles, particularly emphasizing machine-actionability and interoperability across diverse data ecosystems. This abstract introduces the \"FDO Manager\", a Minimum Viable Implementation, designed to optimize the management of FDOs following these principles and the FDO specifications. The FDO Manager is tailored to manage research artefacts such as datasets, codes, and publications, to foster increased transparency and reproducibility in research. The abstract presents the implementation details of the FDO Manager, its underlying architecture, and the metadata schemas it employs, thereby offering a clear and comprehensive understanding of its functionalities and impact on the research domain.","sentences":["The concept of FAIR Digital Objects (FDOs) aims to revolutionise the field of digital preservation and accessibility in the next few years.","Central to this revolution is the alignment of FDOs with the FAIR (Findable, Accessible, Interoperable, Reusable) Principles, particularly emphasizing machine-actionability and interoperability across diverse data ecosystems.","This abstract introduces the \"FDO Manager\", a Minimum Viable Implementation, designed to optimize the management of FDOs following these principles and the FDO specifications.","The FDO Manager is tailored to manage research artefacts such as datasets, codes, and publications, to foster increased transparency and reproducibility in research.","The abstract presents the implementation details of the FDO Manager, its underlying architecture, and the metadata schemas it employs, thereby offering a clear and comprehensive understanding of its functionalities and impact on the research domain."],"url":"http://arxiv.org/abs/2402.03812v1","category":"cs.DC"}
{"created":"2024-02-06 04:56:43","title":"FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution","abstract":"Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings. However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images. This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns. For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit. To expose this potential vulnerability, we aim to build an adversarial attack forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input's attribute characteristics. We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the adversarial noise added to the input stroke painting. Empirical studies reveal that traditional adversarial noise struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images' attributes. To execute effective attacks, we introduce FoolSDEdit: We design a joint adversarial exposure and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together. We optimize the execution strategy of various perturbations, framing it as a network architecture search problem. We create the SuperPert, a graph representing diverse execution strategies for different perturbations. After training, we obtain the optimized execution strategy for effective TAGA against SDEdit. Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines.","sentences":["Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings.","However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images.","This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns.","For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit.","To expose this potential vulnerability, we aim to build an adversarial attack forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input's attribute characteristics.","We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the adversarial noise added to the input stroke painting.","Empirical studies reveal that traditional adversarial noise struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images' attributes.","To execute effective attacks, we introduce FoolSDEdit: We design a joint adversarial exposure and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together.","We optimize the execution strategy of various perturbations, framing it as a network architecture search problem.","We create the SuperPert, a graph representing diverse execution strategies for different perturbations.","After training, we obtain the optimized execution strategy for effective TAGA against SDEdit.","Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines."],"url":"http://arxiv.org/abs/2402.03705v1","category":"cs.CV"}
{"created":"2024-02-06 03:57:05","title":"Maximum-Norm Error Estimates of Fourth-Order Compact and ADI Compact Finite Difference Methods for Nonlinear Coupled Bacterial Systems","abstract":"In this paper, by introducing two temporal-derivative-dependent auxiliary variables, a linearized and decoupled fourth-order compact finite difference method is developed and analyzed for the nonlinear coupled bacterial systems. The temporal-spatial error splitting technique and discrete energy method are employed to prove the unconditional stability and convergence of the method in discrete maximum norm. Furthermore, to improve the computational efficiency, an alternating direction implicit (ADI) compact difference algorithm is proposed, and the unconditional stability and optimal-order maximum-norm error estimate for the ADI scheme are also strictly established. Finally, several numerical experiments are conducted to validate the theoretical convergence and to simulate the phenomena of bacterial extinction as well as the formation of endemic diseases.","sentences":["In this paper, by introducing two temporal-derivative-dependent auxiliary variables, a linearized and decoupled fourth-order compact finite difference method is developed and analyzed for the nonlinear coupled bacterial systems.","The temporal-spatial error splitting technique and discrete energy method are employed to prove the unconditional stability and convergence of the method in discrete maximum norm.","Furthermore, to improve the computational efficiency, an alternating direction implicit (ADI) compact difference algorithm is proposed, and the unconditional stability and optimal-order maximum-norm error estimate for the ADI scheme are also strictly established.","Finally, several numerical experiments are conducted to validate the theoretical convergence and to simulate the phenomena of bacterial extinction as well as the formation of endemic diseases."],"url":"http://arxiv.org/abs/2402.03674v1","category":"math.NA"}
{"created":"2024-02-06 00:53:38","title":"A back-to-back diode model applied to MoS2 van der Waals Schottky diodes","abstract":"The use of metal van der Waals contacts and the implicit reduction in Fermi-level pinning in contacted semiconductors has led to remarkable device optimizations. For example, using graphene as an electrical contact allows for tunable Schottky barriers in transistors and barristors. In this study, we present a double Schottky barrier model and apply it to barrier tunable all van der Waals transistors. In a molybdenum disulfide (MoS$_2$) transistor with graphene and few-layer graphene contacts, we find that the model can be applied to extract Schottky barrier heights that agree with the Schottky-Mott rule from simple two-terminal current-voltage measurements at room temperature. Furthermore, we show tunability of the Schottky barrier \\textit{in-situ} using a regional contact gate. Our results show that a basic back-to-back diode model, applied to two terminal measurements, can capture the diode properties of all-van-der-Waals transistors relatively well.","sentences":["The use of metal van der Waals contacts and the implicit reduction in Fermi-level pinning in contacted semiconductors has led to remarkable device optimizations.","For example, using graphene as an electrical contact allows for tunable Schottky barriers in transistors and barristors.","In this study, we present a double Schottky barrier model and apply it to barrier tunable all van der Waals transistors.","In a molybdenum disulfide (MoS$_2$) transistor with graphene and few-layer graphene contacts, we find that the model can be applied to extract Schottky barrier heights that agree with the Schottky-Mott rule from simple two-terminal current-voltage measurements at room temperature.","Furthermore, we show tunability of the Schottky barrier \\textit{in-situ} using a regional contact gate.","Our results show that a basic back-to-back diode model, applied to two terminal measurements, can capture the diode properties of all-van-der-Waals transistors relatively well."],"url":"http://arxiv.org/abs/2402.03611v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 00:20:49","title":"Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction","abstract":"Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets.","sentences":["Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users.","While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models.","To address this research gap, our study performs a top-down analysis on representative CTR models.","Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias.","We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them.","Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias.","Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models.","Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods.","The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets."],"url":"http://arxiv.org/abs/2402.03600v1","category":"cs.IR"}
{"created":"2024-02-05 21:34:21","title":"Efficient Generation of Grids and Traversal Graphs in Compositional Spaces towards Exploration and Path Planning Exemplified in Materials","abstract":"Many disciplines of science and engineering deal with problems related to compositions, ranging from chemical compositions in materials science to portfolio compositions in economics. They exist in non-Euclidean simplex spaces, causing many standard tools to be incorrect or inefficient, which is significant in combinatorically or structurally challenging spaces exemplified by Compositionally Complex Materials (CCMs) and Functionally Graded Materials (FGMs). Here, we explore them conceptually in terms of problem spaces and quantitatively in terms of computational feasibility.   This work implements several essential methods specific to the compositional (simplex) spaces through a high-performance open-source library nimplex. Most significantly, we derive and implement an algorithm for constructing a novel n-dimensional simplex graph data structure, which contains all discretized compositions and all possible neighbor-to-neighbor transitions as pointer arrays. Critically, no distance or neighborhood calculations are performed, instead leveraging pure combinatorics and the ordering in procedurally generated simplex grids, keeping the algorithm $\\mathcal{O}(N)$, so that graphs with billions of transitions take seconds to construct on a laptop. Furthermore, we demonstrate how such graph representations can be combined to express path-planning problem spaces and to incorporate prior knowledge while keeping the problem space homogeneous. This allows for efficient deployment of existing high-performance gradient descent, graph traversal search, and other path optimization algorithms.","sentences":["Many disciplines of science and engineering deal with problems related to compositions, ranging from chemical compositions in materials science to portfolio compositions in economics.","They exist in non-Euclidean simplex spaces, causing many standard tools to be incorrect or inefficient, which is significant in combinatorically or structurally challenging spaces exemplified by Compositionally Complex Materials (CCMs) and Functionally Graded Materials (FGMs).","Here, we explore them conceptually in terms of problem spaces and quantitatively in terms of computational feasibility.   ","This work implements several essential methods specific to the compositional (simplex) spaces through a high-performance open-source library nimplex.","Most significantly, we derive and implement an algorithm for constructing a novel n-dimensional simplex graph data structure, which contains all discretized compositions and all possible neighbor-to-neighbor transitions as pointer arrays.","Critically, no distance or neighborhood calculations are performed, instead leveraging pure combinatorics and the ordering in procedurally generated simplex grids, keeping the algorithm $\\mathcal{O}(N)$, so that graphs with billions of transitions take seconds to construct on a laptop.","Furthermore, we demonstrate how such graph representations can be combined to express path-planning problem spaces and to incorporate prior knowledge while keeping the problem space homogeneous.","This allows for efficient deployment of existing high-performance gradient descent, graph traversal search, and other path optimization algorithms."],"url":"http://arxiv.org/abs/2402.03528v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 21:09:49","title":"ParSplice: strong exa-scaling of molecular dynamics","abstract":"ParSplice D. Perez, E. D. Cubuk, A. Waterland, E. Kaxiras, and A. F. Voter, Long-Time Dynamics through Parallel Trajectory Splicing, Journal of Chemical Theory and Computation, 2016 is a molecular dynamics method for parallel-in-time trajectory generation, allowing this workhorse of in silico science to strong scale on massively parallel computers. Trajectories generated by ParSplice always have robust theoretical guarantees on their validity, with parameter choices only affecting the parallel efficiency. This commentary summarizes the ParSplice approach with minimal mathematical development, emphasizing how the theoretical underpinning is essential for deployment at the exascale.","sentences":["ParSplice D. Perez, E. D. Cubuk, A. Waterland, E. Kaxiras, and A. F. Voter, Long-Time Dynamics through Parallel Trajectory Splicing, Journal of Chemical Theory and Computation, 2016 is a molecular dynamics method for parallel-in-time trajectory generation, allowing this workhorse of in silico science to strong scale on massively parallel computers.","Trajectories generated by ParSplice always have robust theoretical guarantees on their validity, with parameter choices only affecting the parallel efficiency.","This commentary summarizes the ParSplice approach with minimal mathematical development, emphasizing how the theoretical underpinning is essential for deployment at the exascale."],"url":"http://arxiv.org/abs/2402.03521v1","category":"physics.comp-ph"}
{"created":"2024-02-05 20:36:33","title":"How Does Unlabeled Data Provably Help Out-of-Distribution Detection?","abstract":"Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier. Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. Code is publicly available at https://github.com/deeplearning-wisc/sal.","sentences":["Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data.","Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data.","This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier.","Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection.","This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness.","The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data.","Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm.","Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier.","Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights.","Code is publicly available at https://github.com/deeplearning-wisc/sal."],"url":"http://arxiv.org/abs/2402.03502v1","category":"cs.LG"}
{"created":"2024-02-05 19:29:55","title":"An efficient prescription to search for linear gravitational wave memory in pulsar timing array data and its application to the NANOGrav 12.5-year dataset","abstract":"Burst with memory events are potential transient gravitational wave sources for the maturing pulsar timing array (PTA) efforts. We provide a computationally efficient prescription to model pulsar timing residuals induced by supermassive black hole pairs in general relativistic hyperbolic trajectories employing a Keplerian-type parametric solution. Injection studies have been pursued on the resulting bursts with linear GW memory (LGWM) events with simulated datasets to test the performance of our pipeline, followed by its application to the publicly available NANOGrav 12.5-year (NG12.5) dataset. Given the absence of any evidence of LGWM events within the real NG12.5 dataset, we impose 95\\% upper limits on the PTA signal amplitude as a function of the sky location of the source and certain characteristic frequency ($n$) of the signal. The upper limits are computed using a signal model that takes into account the presence of intrinsic timing noise specific to each pulsar, as well as a common, spatially uncorrelated red noise, alongside the LGWM signal. Our investigations reveal that the $95\\%$ upper limits on LGWM amplitude, marginalized over all other parameters, is 3.48 $\\pm 0.51 \\ \\mu$s for $n>3.16$ nHz. This effort should be relevant for constraining both burst and memory events in the upcoming International Pulsar Timing Array data releases.","sentences":["Burst with memory events are potential transient gravitational wave sources for the maturing pulsar timing array (PTA) efforts.","We provide a computationally efficient prescription to model pulsar timing residuals induced by supermassive black hole pairs in general relativistic hyperbolic trajectories employing a Keplerian-type parametric solution.","Injection studies have been pursued on the resulting bursts with linear GW memory (LGWM) events with simulated datasets to test the performance of our pipeline, followed by its application to the publicly available NANOGrav 12.5-year (NG12.5) dataset.","Given the absence of any evidence of LGWM events within the real NG12.5 dataset, we impose 95\\% upper limits on the PTA signal amplitude as a function of the sky location of the source and certain characteristic frequency ($n$) of the signal.","The upper limits are computed using a signal model that takes into account the presence of intrinsic timing noise specific to each pulsar, as well as a common, spatially uncorrelated red noise, alongside the LGWM signal.","Our investigations reveal that the $95\\%$ upper limits on LGWM amplitude, marginalized over all other parameters, is 3.48 $\\pm 0.51 \\ \\mu$s for $n>3.16$ nHz.","This effort should be relevant for constraining both burst and memory events in the upcoming International Pulsar Timing Array data releases."],"url":"http://arxiv.org/abs/2402.03472v1","category":"astro-ph.HE"}
{"created":"2024-02-05 19:02:19","title":"Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees","abstract":"Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstrate that $\\texttt{DSpodFL}$ achieves significantly improved training speeds and robustness to variations in system parameters compared to the state-of-the-art.","sentences":["Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients.","In this work, we propose Decentralized Sporadic Federated Learning ($\\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings.","$\\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework.","We analytically characterize the convergence behavior of $\\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works.","Through experiments, we demonstrate that $\\texttt{DSpodFL}$ achieves significantly improved training speeds and robustness to variations in system parameters compared to the state-of-the-art."],"url":"http://arxiv.org/abs/2402.03448v1","category":"cs.LG"}
{"created":"2024-02-05 18:44:45","title":"The origin of kinematically-persistent planes of satellite galaxies as driven by the early evolution of the local Cosmic Web in $\u039b$CDM","abstract":"Kinematically-persistent planes of satellites (KPPs) are fixed sets of satellites co-orbiting around their host galaxy, whose orbital poles are conserved and clustered across long cosmic time intervals. They play the role of 'skeletons', ensuring the long-term durability of positional planes. We explore the physical processes behind their formation in terms of the dynamics of the local Cosmic Web (CW), characterized via the so-called Lagrangian Volumes (LVs) built up around two zoom-in, cosmological hydro-simulations of MW-mass disk galaxy + satellites systems, where three KPPs have been identified. By analyzing the LVs deformations in terms of the reduced Tensor of Inertia (TOI), we find an outstanding alignment between the LV principal directions and KPP satellites' orbital poles. The most compressive local mass flows (along the $\\hat{e}_3$ eigenvector) are strong at early times, feeding the so-called $\\hat{e}_3$-structure, while the smallest TOI axis rapidly decreases. The $\\hat{e}_3$-structure collapse marks the end of this regime and is the timescale for the establishment of satellite orbital pole clustering when the Universe is $\\lesssim$ 4 Gyr old. KPP proto-satellites aligned with $\\hat{e}_3$ are those whose orbital poles are either aligned from early times, or have been successfully bent at $\\hat{e}_3$-structure collapse. KPP satellites associated to $\\hat{e}_1$ tend to have early trajectories already parallel to $\\hat{e}_3$. We show that KPPs can arise as a result of the $\\Lambda$CDM-predicted large-scale dynamics acting on particular sets of proto-satellites, the same dynamics that shape the local CW environment.","sentences":["Kinematically-persistent planes of satellites (KPPs) are fixed sets of satellites co-orbiting around their host galaxy, whose orbital poles are conserved and clustered across long cosmic time intervals.","They play the role of 'skeletons', ensuring the long-term durability of positional planes.","We explore the physical processes behind their formation in terms of the dynamics of the local Cosmic Web (CW), characterized via the so-called Lagrangian Volumes (LVs) built up around two zoom-in, cosmological hydro-simulations of MW-mass disk galaxy + satellites systems, where three KPPs have been identified.","By analyzing the LVs deformations in terms of the reduced Tensor of Inertia (TOI), we find an outstanding alignment between the LV principal directions and KPP satellites' orbital poles.","The most compressive local mass flows (along the $\\hat{e}_3$ eigenvector) are strong at early times, feeding the so-called $\\hat{e}_3$-structure, while the smallest TOI axis rapidly decreases.","The $\\hat{e}_3$-structure collapse marks the end of this regime and is the timescale for the establishment of satellite orbital pole clustering when the Universe is $\\lesssim$ 4 Gyr old.","KPP proto-satellites aligned with $\\hat{e}_3$ are those whose orbital poles are either aligned from early times, or have been successfully bent at $\\hat{e}_3$-structure collapse.","KPP satellites associated to $\\hat{e}_1$ tend to have early trajectories already parallel to $\\hat{e}_3$. We show that KPPs can arise as a result of the $\\Lambda$CDM-predicted large-scale dynamics acting on particular sets of proto-satellites, the same dynamics that shape the local CW environment."],"url":"http://arxiv.org/abs/2402.03288v1","category":"astro-ph.GA"}
{"created":"2024-02-05 18:38:15","title":"A note on the Winterbottom shape","abstract":"In this short note we review results on equilibrium shapes of minimizers to the sessile drop problem. More precisely, we study the Winterbottom problem and prove that the Winterbottom shape is indeed optimal. The arguments presented here are based on relaxation and the (anisotropic) isoperimetric inequality.","sentences":["In this short note we review results on equilibrium shapes of minimizers to the sessile drop problem.","More precisely, we study the Winterbottom problem and prove that the Winterbottom shape is indeed optimal.","The arguments presented here are based on relaxation and the (anisotropic) isoperimetric inequality."],"url":"http://arxiv.org/abs/2402.03281v1","category":"math.AP"}
{"created":"2024-02-05 18:22:21","title":"MobilityGPT: Enhanced Human Mobility Modeling with a GPT model","abstract":"Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.","sentences":["Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories.","However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits.","To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT).","To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT.","We propose a gravity-based sampling method to train a transformer for semantic sequence similarity.","Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits.","Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories.","Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions."],"url":"http://arxiv.org/abs/2402.03264v1","category":"cs.LG"}
{"created":"2024-02-05 18:21:11","title":"A hyperbolastic type-I diffusion process: Parameter estimation bymeans of the firefly algorithm","abstract":"A stochastic diffusion process, whose mean function is a hyperbolastic curve of type I, is presented. Themain characteristics of the process are studied and the problem of maximum likelihood estimation forthe parameters of the process is considered. To this end, the firefly metaheuristic optimization algo-rithm is applied after bounding the parametric space by a stagewise procedure. Some examples basedon simulated sample paths and real data illustrate this development.","sentences":["A stochastic diffusion process, whose mean function is a hyperbolastic curve of type I, is presented.","Themain characteristics of the process are studied and the problem of maximum likelihood estimation forthe parameters of the process is considered.","To this end, the firefly metaheuristic optimization algo-rithm is applied after bounding the parametric space by a stagewise procedure.","Some examples basedon simulated sample paths and real data illustrate this development."],"url":"http://arxiv.org/abs/2402.03416v1","category":"stat.ME"}
{"created":"2024-02-05 18:17:15","title":"Meeting Bridges: Designing Information Artifacts that Bridge from Synchronous Meetings to Asynchronous Collaboration","abstract":"A recent surge in remote meetings has led to complaints of ``Zoom fatigue'' and ``collaboration overload,'' negatively impacting worker productivity and well-being. One way to alleviate the burden of meetings is to de-emphasize their synchronous participation by shifting work to and enabling sensemaking during post-meeting asynchronous activities. Towards this goal, we propose the design concept of meeting bridges, or information artifacts that can encapsulate meeting information towards bridging to and facilitating post-meeting activities. Through 13 interviews and a survey of 198 information workers, we learn how people use online meeting information after meetings are over, finding five main uses: as an archive, as task reminders, to onboard or support inclusion, for group sensemaking, and as a launching point for follow-on collaboration. However, we also find that current common meeting artifacts, such as notes and recordings, present challenges in serving as meeting bridges. After conducting co-design sessions with 16 participants, we distill key principles for the design of meeting bridges to optimally support asynchronous collaboration goals. Overall, our findings point to the opportunity of designing information artifacts that not only support users to access but also continue to transform and engage in meeting information post-meeting.","sentences":["A recent surge in remote meetings has led to complaints of ``Zoom fatigue'' and ``collaboration overload,'' negatively impacting worker productivity and well-being.","One way to alleviate the burden of meetings is to de-emphasize their synchronous participation by shifting work to and enabling sensemaking during post-meeting asynchronous activities.","Towards this goal, we propose the design concept of meeting bridges, or information artifacts that can encapsulate meeting information towards bridging to and facilitating post-meeting activities.","Through 13 interviews and a survey of 198 information workers, we learn how people use online meeting information after meetings are over, finding five main uses: as an archive, as task reminders, to onboard or support inclusion, for group sensemaking, and as a launching point for follow-on collaboration.","However, we also find that current common meeting artifacts, such as notes and recordings, present challenges in serving as meeting bridges.","After conducting co-design sessions with 16 participants, we distill key principles for the design of meeting bridges to optimally support asynchronous collaboration goals.","Overall, our findings point to the opportunity of designing information artifacts that not only support users to access but also continue to transform and engage in meeting information post-meeting."],"url":"http://arxiv.org/abs/2402.03259v1","category":"cs.HC"}
{"created":"2024-02-05 18:15:50","title":"Freeze-Tag in $L_1$ has Wake-up Time Five","abstract":"The Freeze-Tag Problem, introduced in Arkin et al. (SODA'02) consists of waking up a swarm of $n$ robots, starting from a single active robot. In the basic geometric version, every robot is given coordinates in the plane. As soon as a robot is awakened, it can move towards inactive robots to wake them up. The goal is to minimize the wake-up time of the last robot, the makespan.   Despite significant progress on the computational complexity of this problem and on approximation algorithms, the characterization of exact bounds on the makespan remains one of the main open questions. In this paper, we settle this question for the $\\ell_1$-norm, showing that a makespan of at most $5r$ can always be achieved, where $r$ is the maximum distance between the initial active robot and any sleeping robot. Moreover, a schedule achieving a makespan of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time and the makespan are optimal. This implies a new upper bound of $5\\sqrt{2}r \\approx 7.07r$ on the makespan in the $\\ell_2$-norm, improving the best known bound so far $(5+2\\sqrt{2}+\\sqrt{5})r \\approx 10.06r$.","sentences":["The Freeze-Tag Problem, introduced in Arkin et al.","(SODA'02) consists of waking up a swarm of $n$ robots, starting from a single active robot.","In the basic geometric version, every robot is given coordinates in the plane.","As soon as a robot is awakened, it can move towards inactive robots to wake them up.","The goal is to minimize the wake-up time of the last robot, the makespan.   ","Despite significant progress on the computational complexity of this problem and on approximation algorithms, the characterization of exact bounds on the makespan remains one of the main open questions.","In this paper, we settle this question for the $\\ell_1$-norm, showing that a makespan of at most $5r$ can always be achieved, where $r$ is the maximum distance between the initial active robot and any sleeping robot.","Moreover, a schedule achieving a makespan of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time and the makespan are optimal.","This implies a new upper bound of $5\\sqrt{2}r \\approx 7.07r$ on the makespan in the $\\ell_2$-norm, improving the best known bound so far $(5+2\\sqrt{2}+\\sqrt{5})r \\approx 10.06r$."],"url":"http://arxiv.org/abs/2402.03258v1","category":"cs.DS"}
{"created":"2024-02-05 18:14:28","title":"Learning Best-in-Class Policies for the Predict-then-Optimize Framework","abstract":"We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware learning.","sentences":["We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework.","These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods.","Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows.","This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings.","This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric.","Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware learning."],"url":"http://arxiv.org/abs/2402.03256v1","category":"cs.LG"}
{"created":"2024-02-05 18:08:42","title":"The Fefferman-Phong uncertainty principle for representations of Lie groups and applications","abstract":"We prove a general uncertainty principle for square-integrable irreducible unitary representations of connected Lie groups. The concentration of the matrix coefficients is measured in terms of weighted $L^p$ norms, with weights in the local Muckenhoupt class $A_{\\infty,{\\rm loc}}$ associated with a subRiemannian left-invariant metric of $G$ and a relatively invariant measure. The result is reminiscent of the Fefferman-Phong uncertainty principle, and is new even for the Schr\\\"odinger representation of the reduced Heisenberg group, which corresponds to the short-time Fourier transform. As an application, we give an optimal estimate of the order of magnitude of the bottom of the spectrum and of the essential spectrum of semiclassical anti-Wick operators in $\\mathbb{R}^d$ with a nonnegative symbol $a$ in the class $A_{\\infty}$ (in particular, for polynomial symbols). Precisely, we show that the infimum $\\inf _{(x_0,\\omega_0)\\in{\\mathbb{R}^{2d}}} -\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$ represents (up to multiplicative constants) both a lower bound and an upper bound for the bottom of the spectrum, uniformly with respect to $\\hbar>0$. Similarly the quantity $$\\liminf_{(x_0,\\omega_0)\\to\\infty} -\\!\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$$ represents both a lower bound and an upper bound for the bottom of the essential spectrum, uniformly with respect to $\\hbar>0$.","sentences":["We prove a general uncertainty principle for square-integrable irreducible unitary representations of connected Lie groups.","The concentration of the matrix coefficients is measured in terms of weighted $L^p$ norms, with weights in the local Muckenhoupt class $A_{\\infty,{\\rm loc}}$ associated with a subRiemannian left-invariant metric of $G$ and a relatively invariant measure.","The result is reminiscent of the Fefferman-Phong uncertainty principle, and is new even for the Schr\\\"odinger representation of the reduced Heisenberg group, which corresponds to the short-time Fourier transform.","As an application, we give an optimal estimate of the order of magnitude of the bottom of the spectrum and of the essential spectrum of semiclassical anti-Wick operators in $\\mathbb{R}^d$ with a nonnegative symbol $a$ in the class $A_{\\infty}$ (in particular, for polynomial symbols).","Precisely, we show that the infimum $\\inf _{(x_0,\\omega_0)\\in{\\mathbb{R}^{2d}}} -\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$ represents (up to multiplicative constants) both a lower bound and an upper bound for the bottom of the spectrum, uniformly with respect to $\\hbar>0$. Similarly the quantity $$\\liminf_{(x_0,\\omega_0)\\to\\infty} -\\!\\!\\!\\!\\!\\!\\int_{B((x_0,\\omega_0),\\sqrt{\\hbar})} a(x,\\omega)\\, dx\\,d\\omega$$ represents both a lower bound and an upper bound for the bottom of the essential spectrum, uniformly with respect to $\\hbar>0$."],"url":"http://arxiv.org/abs/2402.03250v1","category":"math.CA"}
{"created":"2024-02-05 17:59:00","title":"Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills","abstract":"Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.","sentences":["Large language models (LLMs) have recently been used for sequential decision making in interactive environments.","However, leveraging environment reward signals for continual LLM actor improvement is not straightforward.","We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills.","SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill.","These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards.","Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards.","We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement.","SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%."],"url":"http://arxiv.org/abs/2402.03244v1","category":"cs.LG"}
{"created":"2024-02-05 17:44:21","title":"Improved prediction of future user activity in online A/B testing","abstract":"In online randomized experiments or A/B tests, accurate predictions of participant inclusion rates are of paramount importance. These predictions not only guide experimenters in optimizing the experiment's duration but also enhance the precision of treatment effect estimates. In this paper we present a novel, straightforward, and scalable Bayesian nonparametric approach for predicting the rate at which individuals will be exposed to interventions within the realm of online A/B testing. Our approach stands out by offering dual prediction capabilities: it forecasts both the quantity of new customers expected in future time windows and, unlike available alternative methods, the number of times they will be observed. We derive closed-form expressions for the posterior distributions of the quantities needed to form predictions about future user activity, thereby bypassing the need for numerical algorithms such as Markov chain Monte Carlo. After a comprehensive exposition of our model, we test its performance on experiments on real and simulated data, where we show its superior performance with respect to existing alternatives in the literature.","sentences":["In online randomized experiments or A/B tests, accurate predictions of participant inclusion rates are of paramount importance.","These predictions not only guide experimenters in optimizing the experiment's duration but also enhance the precision of treatment effect estimates.","In this paper we present a novel, straightforward, and scalable Bayesian nonparametric approach for predicting the rate at which individuals will be exposed to interventions within the realm of online A/B testing.","Our approach stands out by offering dual prediction capabilities: it forecasts both the quantity of new customers expected in future time windows and, unlike available alternative methods, the number of times they will be observed.","We derive closed-form expressions for the posterior distributions of the quantities needed to form predictions about future user activity, thereby bypassing the need for numerical algorithms such as Markov chain Monte Carlo.","After a comprehensive exposition of our model, we test its performance on experiments on real and simulated data, where we show its superior performance with respect to existing alternatives in the literature."],"url":"http://arxiv.org/abs/2402.03231v1","category":"stat.ME"}
{"created":"2024-02-05 17:30:24","title":"Experiment-driven atomistic materials modeling: A case study combining XPS and ML potentials to infer the structure of oxygen-rich amorphous carbon","abstract":"An important yet challenging aspect of atomistic materials modeling is reconciling experimental and computational results. Conventional approaches involve generating numerous configurations through molecular dynamics or Monte Carlo structure optimization and selecting the one with the closest match to experiment. However, this inefficient process is not guaranteed to succeed. We introduce a general method to combine atomistic machine learning (ML) with experimental observables that produces atomistic structures compatible with experiment by design. We use this approach in combination with grand-canonical Monte Carlo within a modified Hamiltonian formalism, to generate configurations that agree with experimental data and are chemically sound (low in energy). We apply our approach to understand the atomistic structure of oxygenated amorphous carbon (a-CO$_{x}$), an intriguing carbon-based material, to answer the question of how much oxygen can be added to carbon before it fully decomposes into CO and CO$_2$. Utilizing an ML-based X-ray photoelectron spectroscopy (XPS) model trained from $GW$ and density functional theory (DFT) data, in conjunction with an ML interatomic potential, we identify a-CO$_{x}$ structures compliant with experimental XPS predictions that are also energetically favorable with respect to DFT. Employing a network analysis, we accurately deconvolve the XPS spectrum into motif contributions, both revealing the inaccuracies inherent to experimental XPS interpretation and granting us atomistic insight into the structure of a-CO$_{x}$. This method generalizes to multiple experimental observables and allows for the elucidation of the atomistic structure of materials directly from experimental data, thereby enabling experiment-driven materials modeling with a degree of realism previously out of reach.","sentences":["An important yet challenging aspect of atomistic materials modeling is reconciling experimental and computational results.","Conventional approaches involve generating numerous configurations through molecular dynamics or Monte Carlo structure optimization and selecting the one with the closest match to experiment.","However, this inefficient process is not guaranteed to succeed.","We introduce a general method to combine atomistic machine learning (ML) with experimental observables that produces atomistic structures compatible with experiment by design.","We use this approach in combination with grand-canonical Monte Carlo within a modified Hamiltonian formalism, to generate configurations that agree with experimental data and are chemically sound (low in energy).","We apply our approach to understand the atomistic structure of oxygenated amorphous carbon (a-CO$_{x}$), an intriguing carbon-based material, to answer the question of how much oxygen can be added to carbon before it fully decomposes into CO and CO$_2$. Utilizing an ML-based X-ray photoelectron spectroscopy (XPS) model trained from $GW$ and density functional theory (DFT) data, in conjunction with an ML interatomic potential, we identify a-CO$_{x}$ structures compliant with experimental XPS predictions that are also energetically favorable with respect to DFT.","Employing a network analysis, we accurately deconvolve the XPS spectrum into motif contributions, both revealing the inaccuracies inherent to experimental XPS interpretation and granting us atomistic insight into the structure of a-CO$_{x}$.","This method generalizes to multiple experimental observables and allows for the elucidation of the atomistic structure of materials directly from experimental data, thereby enabling experiment-driven materials modeling with a degree of realism previously out of reach."],"url":"http://arxiv.org/abs/2402.03219v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 17:16:43","title":"Inverse regression for spatially distributed functional data","abstract":"Spatially distributed functional data are prevalent in many statistical applications such as meteorology, energy forecasting, census data, disease mapping, and neurological studies. Given their complex and high-dimensional nature, functional data often require dimension reduction methods to extract meaningful information. Inverse regression is one such approach that has become very popular in the past two decades. We study the inverse regression in the framework of functional data observed at irregularly positioned spatial sites. The functional predictor is the sum of a spatially dependent functional effect and a spatially independent functional nugget effect, while the relation between the scalar response and the functional predictor is modeled using the inverse regression framework. For estimation, we consider local linear smoothing with a general weighting scheme, which includes as special cases the schemes under which equal weights are assigned to each observation or to each subject. This framework enables us to present the asymptotic results for different types of sampling plans over time such as non-dense, dense, and ultra-dense. We discuss the domain-expanding infill (DEI) framework for spatial asymptotics, which is a mix of the traditional expanding domain and infill frameworks. The DEI framework overcomes the limitations of traditional spatial asymptotics in the existing literature. Under this unified framework, we develop asymptotic theory and identify conditions that are necessary for the estimated eigen-directions to achieve optimal rates of convergence. Our asymptotic results include pointwise and $L_2$ convergence rates. Simulation studies using synthetic data and an application to a real-world dataset confirm the effectiveness of our methods.","sentences":["Spatially distributed functional data are prevalent in many statistical applications such as meteorology, energy forecasting, census data, disease mapping, and neurological studies.","Given their complex and high-dimensional nature, functional data often require dimension reduction methods to extract meaningful information.","Inverse regression is one such approach that has become very popular in the past two decades.","We study the inverse regression in the framework of functional data observed at irregularly positioned spatial sites.","The functional predictor is the sum of a spatially dependent functional effect and a spatially independent functional nugget effect, while the relation between the scalar response and the functional predictor is modeled using the inverse regression framework.","For estimation, we consider local linear smoothing with a general weighting scheme, which includes as special cases the schemes under which equal weights are assigned to each observation or to each subject.","This framework enables us to present the asymptotic results for different types of sampling plans over time such as non-dense, dense, and ultra-dense.","We discuss the domain-expanding infill (DEI) framework for spatial asymptotics, which is a mix of the traditional expanding domain and infill frameworks.","The DEI framework overcomes the limitations of traditional spatial asymptotics in the existing literature.","Under this unified framework, we develop asymptotic theory and identify conditions that are necessary for the estimated eigen-directions to achieve optimal rates of convergence.","Our asymptotic results include pointwise and $L_2$ convergence rates.","Simulation studies using synthetic data and an application to a real-world dataset confirm the effectiveness of our methods."],"url":"http://arxiv.org/abs/2402.03206v1","category":"math.ST"}
{"created":"2024-02-05 16:49:30","title":"Stateless Quantum Structures and Extremal Graph Theory","abstract":"We study hypergraphs which represent finite quantum event structures. We contribute to results of graph theory, regarding bounds on the number of edges, given the number of vertices. We develop a missing one for 3-graphs of girth 4. As an application of the graph-theoretical approach to quantum structures, we show that the smallest orthoalgebra with an empty state space has 10 atoms. Optimized constructions of an orthomodular poset and an orthomodular lattice with no group-valued measures are given. We present also a handcrafted construction of an orthoalgebra with no group-valued measure; it is larger, but its properties can be verified without a computer","sentences":["We study hypergraphs which represent finite quantum event structures.","We contribute to results of graph theory, regarding bounds on the number of edges, given the number of vertices.","We develop a missing one for 3-graphs of girth 4.","As an application of the graph-theoretical approach to quantum structures, we show that the smallest orthoalgebra with an empty state space has 10 atoms.","Optimized constructions of an orthomodular poset and an orthomodular lattice with no group-valued measures are given.","We present also a handcrafted construction of an orthoalgebra with no group-valued measure; it is larger, but its properties can be verified without a computer"],"url":"http://arxiv.org/abs/2402.03185v1","category":"math.QA"}
{"created":"2024-02-05 16:45:56","title":"Nitrogen-polar growth of AlN on vicinal (0001) sapphire by MOVPE","abstract":"We report about metalorganic vapour phase epitaxy of smooth nitrogen-polar AlN templates on vicinal (0001) sapphire substrates. The influence of V/III ratio, growth temperature, growth rate, as well as sapphire-nitridation time and temperature were studied. With 4{\\deg} offcut sapphire, step-flow growth was possible only with V/III ratios below 2. However, optimal V/III ratio required precise adjustment, possibly dependent on reactor history and geometry. A rather narrow temperature window of less than 40{\\deg}C existed for smooth surface morphology. Reducing the temperature affected adatom mobility, eventually disrupting step-flow growth; increasing the temperature favoured the formation of high-aspect-ratio defects on the epilayer. A low thermal-budget nitridation step with a short nitridation time of 15 s proved to be effective in controlling polarity without inducing excessive surface damage on the sapphire substrate. Growth rate also influenced surface morphology, with an increase in RMS roughness and step-bouncing for faster growths; however, at growth rates of 1.4 ${\\mu}$m/h or higher step-flow growth could no longer form. Finally, we developed a V/III ratio fine-tuning procedure, whereby the reactor-specific value that induces optimal growth is inferred by growth-rate variations. With this method, N polar AlN templates with sub-nanometre RMS roughness were demonstrated for both 4{\\deg} and 2{\\deg} offcut sapphire substrates.","sentences":["We report about metalorganic vapour phase epitaxy of smooth nitrogen-polar AlN templates on vicinal (0001) sapphire substrates.","The influence of V/III ratio, growth temperature, growth rate, as well as sapphire-nitridation time and temperature were studied.","With 4{\\deg} offcut sapphire, step-flow growth was possible only with V/III ratios below 2.","However, optimal V/III ratio required precise adjustment, possibly dependent on reactor history and geometry.","A rather narrow temperature window of less than 40{\\deg}C existed for smooth surface morphology.","Reducing the temperature affected adatom mobility, eventually disrupting step-flow growth; increasing the temperature favoured the formation of high-aspect-ratio defects on the epilayer.","A low thermal-budget nitridation step with a short nitridation time of 15 s proved to be effective in controlling polarity without inducing excessive surface damage on the sapphire substrate.","Growth rate also influenced surface morphology, with an increase in RMS roughness and step-bouncing for faster growths; however, at growth rates of 1.4 ${\\mu}$m/h or higher step-flow growth could no longer form.","Finally, we developed a V/III ratio fine-tuning procedure, whereby the reactor-specific value that induces optimal growth is inferred by growth-rate variations.","With this method, N polar AlN templates with sub-nanometre RMS roughness were demonstrated for both 4{\\deg} and 2{\\deg} offcut sapphire substrates."],"url":"http://arxiv.org/abs/2402.03180v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-05 16:39:12","title":"Is Mamba Capable of In-Context Learning?","abstract":"This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.","sentences":["This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers.","We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems.","Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL.","Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations.","Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences."],"url":"http://arxiv.org/abs/2402.03170v1","category":"cs.LG"}
{"created":"2024-02-05 16:35:30","title":"Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity","abstract":"Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transient iteration complexity under more relaxed assumptions compared to existing methods. Numerical experiments validate our theoretical findings.","sentences":["Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures.","To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness.","However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures.","In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms.","D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transient iteration complexity under more relaxed assumptions compared to existing methods.","Numerical experiments validate our theoretical findings."],"url":"http://arxiv.org/abs/2402.03167v1","category":"math.OC"}
{"created":"2024-02-05 16:30:57","title":"Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion","abstract":"Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page: https://direct-a-video.github.io/.","sentences":["Recent text-to-video diffusion models have achieved impressive progress.","In practice, users often desire the ability to control object motion and camera movement independently for customized video creation.","However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models.","In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video.","We propose a simple yet effective strategy for the decoupled control of object motion and camera movement.","Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization.","For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters.","We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation.","Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios.","Extensive experiments demonstrate the superiority and effectiveness of our method.","Project page: https://direct-a-video.github.io/."],"url":"http://arxiv.org/abs/2402.03162v1","category":"cs.CV"}
{"created":"2024-02-05 16:13:00","title":"A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning","abstract":"In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications.","sentences":["In model-based reinforcement learning, most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data.","A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows.","In this paper we tackle this issue by using a multi-step objective to train one-step models.","Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons.","We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments.","To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system.","Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons.","Finally, in the pure batch reinforcement learning setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications."],"url":"http://arxiv.org/abs/2402.03146v1","category":"cs.LG"}
{"created":"2024-02-05 16:12:36","title":"SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems","abstract":"The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion. To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming. We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods.","sentences":["The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD).","In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion.","To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming.","We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.03145v1","category":"eess.SY"}
{"created":"2024-02-05 16:11:04","title":"See More Details: Efficient Image Super-Resolution by Experts Mining","abstract":"Reconstructing high-resolution (HR) images from low-resolution (LR) inputs poses a significant challenge in image super-resolution (SR). While recent approaches have demonstrated the efficacy of intricate operations customized for various objectives, the straightforward stacking of these disparate operations can result in a substantial computational burden, hampering their practical utility. In response, we introduce SeemoRe, an efficient SR model employing expert mining. Our approach strategically incorporates experts at different levels, adopting a collaborative methodology. At the macro scale, our experts address rank-wise and spatial-wise informative features, providing a holistic understanding. Subsequently, the model delves into the subtleties of rank choice by leveraging a mixture of low-rank experts. By tapping into experts specialized in distinct key factors crucial for accurate SR, our model excels in uncovering intricate intra-feature details. This collaborative approach is reminiscent of the concept of \"see more\", allowing our model to achieve an optimal performance with minimal computational costs in efficient settings. The source will be publicly made available at https://github.com/eduardzamfir/seemoredetails","sentences":["Reconstructing high-resolution (HR) images from low-resolution (LR) inputs poses a significant challenge in image super-resolution (SR).","While recent approaches have demonstrated the efficacy of intricate operations customized for various objectives, the straightforward stacking of these disparate operations can result in a substantial computational burden, hampering their practical utility.","In response, we introduce SeemoRe, an efficient SR model employing expert mining.","Our approach strategically incorporates experts at different levels, adopting a collaborative methodology.","At the macro scale, our experts address rank-wise and spatial-wise informative features, providing a holistic understanding.","Subsequently, the model delves into the subtleties of rank choice by leveraging a mixture of low-rank experts.","By tapping into experts specialized in distinct key factors crucial for accurate SR, our model excels in uncovering intricate intra-feature details.","This collaborative approach is reminiscent of the concept of \"see more\", allowing our model to achieve an optimal performance with minimal computational costs in efficient settings.","The source will be publicly made available at https://github.com/eduardzamfir/seemoredetails"],"url":"http://arxiv.org/abs/2402.03412v1","category":"eess.IV"}
{"created":"2024-02-05 15:51:49","title":"How Free is Parameter-Free Stochastic Optimization?","abstract":"We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound.","sentences":["We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters.","Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc.","In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms.","We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions.","Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free stochastic convex optimization infeasible, and provide a method which is (partially) parameter-free up to the limit indicated by our lower bound."],"url":"http://arxiv.org/abs/2402.03126v1","category":"cs.LG"}
{"created":"2024-02-05 15:47:54","title":"Integrating Random Regret Minimization-Based Discrete Choice Models with Mixed Integer Linear Programming for Revenue Optimization","abstract":"This paper explores the critical domain of Revenue Management (RM) within Operations Research (OR), focusing on intricate pricing dynamics. Utilizing Mixed Integer Linear Programming (MILP) models, the study enhances revenue optimization by considering product prices as decision variables and emphasizing the interplay between demand and supply. Recent advancements in Discrete Choice Models (DCMs), particularly those rooted in Random Regret Minimization (RRM) theory, are investigated as potent alternatives to established Random Utility Maximization (RUM) based DCMs. Despite the widespread use of DCMs in RM, a significant gap exists between cutting-edge RRM-based models and their practical integration into RM strategies. The study addresses this gap by incorporating an advanced RRM-based DCM into MILP models, addressing pricing challenges in both capacitated and uncapacitated supply scenarios. The developed models demonstrate feasibility and offer diverse interpretations of consumer choice behavior, drawing inspiration from established RUM-based frameworks. This research contributes to bridging the existing gap in the application of advanced DCMs within practical RM implementations.","sentences":["This paper explores the critical domain of Revenue Management (RM) within Operations Research (OR), focusing on intricate pricing dynamics.","Utilizing Mixed Integer Linear Programming (MILP) models, the study enhances revenue optimization by considering product prices as decision variables and emphasizing the interplay between demand and supply.","Recent advancements in Discrete Choice Models (DCMs), particularly those rooted in Random Regret Minimization (RRM) theory, are investigated as potent alternatives to established Random Utility Maximization (RUM) based DCMs.","Despite the widespread use of DCMs in RM, a significant gap exists between cutting-edge RRM-based models and their practical integration into RM strategies.","The study addresses this gap by incorporating an advanced RRM-based DCM into MILP models, addressing pricing challenges in both capacitated and uncapacitated supply scenarios.","The developed models demonstrate feasibility and offer diverse interpretations of consumer choice behavior, drawing inspiration from established RUM-based frameworks.","This research contributes to bridging the existing gap in the application of advanced DCMs within practical RM implementations."],"url":"http://arxiv.org/abs/2402.03118v1","category":"math.OC"}
{"created":"2024-02-05 15:44:49","title":"Optimal sampling for stochastic and natural gradient descent","abstract":"We consider the problem of optimising the expected value of a loss functional over a nonlinear model class of functions, assuming that we have only access to realisations of the gradient of the loss. This is a classical task in statistics, machine learning and physics-informed machine learning. A straightforward solution is to replace the exact objective with a Monte Carlo estimate before employing standard first-order methods like gradient descent, which yields the classical stochastic gradient descent method. But replacing the true objective with an estimate ensues a ``generalisation error''. Rigorous bounds for this error typically require strong compactness and Lipschitz continuity assumptions while providing a very slow decay with sample size. We propose a different optimisation strategy relying on a natural gradient descent in which the true gradient is approximated in local linearisations of the model class via (quasi-)projections based on optimal sampling methods. Under classical assumptions on the loss and the nonlinear model class, we prove that this scheme converges almost surely monotonically to a stationary point of the true objective and we provide convergence rates.","sentences":["We consider the problem of optimising the expected value of a loss functional over a nonlinear model class of functions, assuming that we have only access to realisations of the gradient of the loss.","This is a classical task in statistics, machine learning and physics-informed machine learning.","A straightforward solution is to replace the exact objective with a Monte Carlo estimate before employing standard first-order methods like gradient descent, which yields the classical stochastic gradient descent method.","But replacing the true objective with an estimate ensues a ``generalisation error''.","Rigorous bounds for this error typically require strong compactness and Lipschitz continuity assumptions while providing a very slow decay with sample size.","We propose a different optimisation strategy relying on a natural gradient descent in which the true gradient is approximated in local linearisations of the model class via (quasi-)projections based on optimal sampling methods.","Under classical assumptions on the loss and the nonlinear model class, we prove that this scheme converges almost surely monotonically to a stationary point of the true objective and we provide convergence rates."],"url":"http://arxiv.org/abs/2402.03113v1","category":"math.OC"}
{"created":"2024-02-05 15:23:04","title":"Optimal rate of convergence in periodic homogenization of viscous Hamilton-Jacobi equations","abstract":"We study the optimal rate of convergence in periodic homogenization of the viscous Hamilton-Jacobi equation $u^\\varepsilon_t + H(\\frac{x}{\\varepsilon},Du^\\varepsilon) = \\varepsilon \\Delta u^\\varepsilon$ in $\\mathbb R^n\\times (0,\\infty)$ subject to a given initial datum. We prove that $\\|u^\\varepsilon-u\\|_{L^\\infty(\\mathbb R^n \\times [0,T])} \\leq C(1+T) \\sqrt{\\varepsilon}$ for any given $T>0$. Moreover, we show that the $O(\\sqrt{\\varepsilon})$ rate is optimal in general, both theoretically and through numerical experiments. Finally, we propose a numerical scheme for the approximation of the effective Hamiltonian based on a finite element approximation of approximate corrector problems.","sentences":["We study the optimal rate of convergence in periodic homogenization of the viscous Hamilton-Jacobi equation $u^\\varepsilon_t + H(\\frac{x}{\\varepsilon},Du^\\varepsilon) = \\varepsilon \\Delta u^\\varepsilon$ in $\\mathbb R^n\\times (0,\\infty)$ subject to a given initial datum.","We prove that $\\|u^\\varepsilon-u\\|_{L^\\infty(\\mathbb R^n \\times","[0,T])} \\leq C(1+T)","\\sqrt{\\varepsilon}$ for any given $T>0$.","Moreover, we show that the $O(\\sqrt{\\varepsilon})$ rate is optimal in general, both theoretically and through numerical experiments.","Finally, we propose a numerical scheme for the approximation of the effective Hamiltonian based on a finite element approximation of approximate corrector problems."],"url":"http://arxiv.org/abs/2402.03091v1","category":"math.AP"}
{"created":"2024-02-05 15:21:58","title":"Increasing TeraHertz spintronic emission with planar antennas","abstract":"Spintronic THz emitters, consisting of Ta/Co/Pt trilayers patterned into rectangles of lateral size in the 10 ${\\mu}$m range, have been integrated in planar electromagnetic antennas of various types (dipole, bow-tie, spiral). Antenna dimensions and shapes have been optimized with the help of electromagnetic simulations so as to maximize antenna efficiency in both narrow-band and broad-band geometries at/around 1 THz. The THz emission has been studied using a pump probe free space electro-optic sampling set up, both for a single emitter geometry and for arrays of emitters. Results show an increase of the detected THz signal for all antenna geometries, with enhancement ratios in the range of three to fifteen depending on antenna type and frequency range, together with changes of the emission bandwidth consistent with simulated characteristics.","sentences":["Spintronic THz emitters, consisting of Ta/Co/Pt trilayers patterned into rectangles of lateral size in the 10 ${\\mu}$m range, have been integrated in planar electromagnetic antennas of various types (dipole, bow-tie, spiral).","Antenna dimensions and shapes have been optimized with the help of electromagnetic simulations so as to maximize antenna efficiency in both narrow-band and broad-band geometries at/around 1 THz.","The THz emission has been studied using a pump probe free space electro-optic sampling set up, both for a single emitter geometry and for arrays of emitters.","Results show an increase of the detected THz signal for all antenna geometries, with enhancement ratios in the range of three to fifteen depending on antenna type and frequency range, together with changes of the emission bandwidth consistent with simulated characteristics."],"url":"http://arxiv.org/abs/2402.03089v1","category":"physics.app-ph"}
{"created":"2024-02-05 15:14:08","title":"Dual Lagrangian Learning for Conic Optimization","abstract":"This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the representation power of ML models. DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for parametric linear and nonlinear conic optimization problems. The paper introduces differentiable conic projection layers, a systematic dual completion procedure, and a self-supervised learning framework. The effectiveness of DLL is demonstrated on linear and nonlinear parametric optimization problems for which DLL provides valid dual bounds within 0.5% of optimality.","sentences":["This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the representation power of ML models.","DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for parametric linear and nonlinear conic optimization problems.","The paper introduces differentiable conic projection layers, a systematic dual completion procedure, and a self-supervised learning framework.","The effectiveness of DLL is demonstrated on linear and nonlinear parametric optimization problems for which DLL provides valid dual bounds within 0.5% of optimality."],"url":"http://arxiv.org/abs/2402.03086v1","category":"math.OC"}
{"created":"2024-02-05 15:09:41","title":"Markov Persuasion Processes: Learning to Persuade from Scratch","abstract":"In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover, we provide a lower bound for our setting matching the guarantees of our algorithm.","sentences":["In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions.","Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially.","Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment.","The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards.","We fix such issues by addressing MPPs where the sender has no knowledge about the environment.","We design a learning algorithm for the sender, working with partial feedback.","We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning.","Moreover, we provide a lower bound for our setting matching the guarantees of our algorithm."],"url":"http://arxiv.org/abs/2402.03077v1","category":"cs.GT"}
{"created":"2024-02-05 14:53:20","title":"Energy transport in diffusive waveguides","abstract":"The guiding and transport of energy, for example of electromagnetic waves underpins many technologies that have shaped modern society, ranging from long distance optical fibre telecommunications to on-chip optical processors. Traditionally, a mechanism is required that exponentially localises the waves or particles in the confinement region, e.g. total internal reflection at a boundary. We introduce a waveguiding mechanism that relies on a different origin for the exponential confinement and that arises due to the physics of diffusion. We demonstrate this concept using light and show that photon density waves can propagate as a guided mode along a core-structure embedded in a scattering, opaque material, enhancing light transmission by orders of magnitude and along non-trivial, e.g. curved trajectories. This waveguiding mechanism can also occur naturally, for example in the cerebral spinal fluid surrounding the brain, along tendons in the human body and is to be expected in other systems that follow the same physics e.g. neutron diffusion.","sentences":["The guiding and transport of energy, for example of electromagnetic waves underpins many technologies that have shaped modern society, ranging from long distance optical fibre telecommunications to on-chip optical processors.","Traditionally, a mechanism is required that exponentially localises the waves or particles in the confinement region, e.g. total internal reflection at a boundary.","We introduce a waveguiding mechanism that relies on a different origin for the exponential confinement and that arises due to the physics of diffusion.","We demonstrate this concept using light and show that photon density waves can propagate as a guided mode along a core-structure embedded in a scattering, opaque material, enhancing light transmission by orders of magnitude and along non-trivial, e.g. curved trajectories.","This waveguiding mechanism can also occur naturally, for example in the cerebral spinal fluid surrounding the brain, along tendons in the human body and is to be expected in other systems that follow the same physics e.g. neutron diffusion."],"url":"http://arxiv.org/abs/2402.03064v1","category":"physics.optics"}
