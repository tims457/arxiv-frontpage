{"created":"2024-05-08 17:59:53","title":"Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving","abstract":"Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.","sentences":["Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods.","Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets.","We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning.","Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models.","The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution.","Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets.","Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines.","This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems."],"url":"http://arxiv.org/abs/2405.05258v1","category":"cs.CV"}
{"created":"2024-05-08 17:59:11","title":"THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models","abstract":"Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \"Type I hallucinations\". Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term \"Type II hallucinations\". Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.","sentences":["Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem.","Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \"Type I hallucinations\".","Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term \"Type II hallucinations\".","Additionally, such benchmarks often require external API calls to models which are subject to change.","In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated.","To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs.","We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics.","By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type","I hallucinations are incomplete.","Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline."],"url":"http://arxiv.org/abs/2405.05256v1","category":"cs.CV"}
{"created":"2024-05-08 17:59:03","title":"Diffusion-HMC: Parameter Inference with Diffusion Model driven Hamiltonian Monte Carlo","abstract":"Diffusion generative models have excelled at diverse image generation and reconstruction tasks across fields. A less explored avenue is their application to discriminative tasks involving regression or classification problems. The cornerstone of modern cosmology is the ability to generate predictions for observed astrophysical fields from theory and constrain physical models from observations using these predictions. This work uses a single diffusion generative model to address these interlinked objectives -- as a surrogate model or emulator for cold dark matter density fields conditional on input cosmological parameters, and as a parameter inference model that solves the inverse problem of constraining the cosmological parameters of an input field. The model is able to emulate fields with summary statistics consistent with those of the simulated target distribution. We then leverage the approximate likelihood of the diffusion generative model to derive tight constraints on cosmology by using the Hamiltonian Monte Carlo method to sample the posterior on cosmological parameters for a given test image. Finally, we demonstrate that this parameter inference approach is more robust to the addition of noise than baseline parameter inference networks.","sentences":["Diffusion generative models have excelled at diverse image generation and reconstruction tasks across fields.","A less explored avenue is their application to discriminative tasks involving regression or classification problems.","The cornerstone of modern cosmology is the ability to generate predictions for observed astrophysical fields from theory and constrain physical models from observations using these predictions.","This work uses a single diffusion generative model to address these interlinked objectives -- as a surrogate model or emulator for cold dark matter density fields conditional on input cosmological parameters, and as a parameter inference model that solves the inverse problem of constraining the cosmological parameters of an input field.","The model is able to emulate fields with summary statistics consistent with those of the simulated target distribution.","We then leverage the approximate likelihood of the diffusion generative model to derive tight constraints on cosmology by using the Hamiltonian Monte Carlo method to sample the posterior on cosmological parameters for a given test image.","Finally, we demonstrate that this parameter inference approach is more robust to the addition of noise than baseline parameter inference networks."],"url":"http://arxiv.org/abs/2405.05255v1","category":"astro-ph.CO"}
{"created":"2024-05-08 17:57:39","title":"Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge","abstract":"Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.","sentences":["Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts.","However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models.","This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied.","This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning.","Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course.","First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert.","We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator.","Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback.","We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings."],"url":"http://arxiv.org/abs/2405.05253v1","category":"cs.CL"}
{"created":"2024-05-08 17:56:47","title":"Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models","abstract":"Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io.","sentences":["Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images.","However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models.","Existing works mainly adopt a retraining process to enhance DM efficiency.","This is computationally expensive and not very scalable.","To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining.","Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation.","In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality.","Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model.","Project webpage: https://atedm.github.io."],"url":"http://arxiv.org/abs/2405.05252v1","category":"cs.CV"}
{"created":"2024-05-08 17:51:53","title":"LLMs with Personalities in Multi-issue Negotiation Games","abstract":"Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be \"jail broken\" to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.","sentences":["Powered by large language models (LLMs), AI agents have become capable of many human tasks.","Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk.","Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation.","Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies.","Low conscientiousness is associated with high toxicity.","These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be \"jail broken\" to exploit agreeable opponents.","We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science."],"url":"http://arxiv.org/abs/2405.05248v1","category":"cs.CL"}
{"created":"2024-05-08 17:40:12","title":"SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan","abstract":"The rapid advancement of AI-generated singing voices, which now closely mimic natural human singing and align seamlessly with musical scores, has led to heightened concerns for artists and the music industry. Unlike spoken voice, singing voice presents unique challenges due to its musical nature and the presence of strong background music, making singing voice deepfake detection (SVDD) a specialized field requiring focused attention. To promote SVDD research, we recently proposed the \"SVDD Challenge,\" the very first research challenge focusing on SVDD for lab-controlled and in-the-wild bonafide and deepfake singing voice recordings. The challenge will be held in conjunction with the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).","sentences":["The rapid advancement of AI-generated singing voices, which now closely mimic natural human singing and align seamlessly with musical scores, has led to heightened concerns for artists and the music industry.","Unlike spoken voice, singing voice presents unique challenges due to its musical nature and the presence of strong background music, making singing voice deepfake detection (SVDD) a specialized field requiring focused attention.","To promote SVDD research, we recently proposed the \"SVDD Challenge,\" the very first research challenge focusing on SVDD for lab-controlled and in-the-wild bonafide and deepfake singing voice recordings.","The challenge will be held in conjunction with the 2024 IEEE Spoken Language Technology Workshop (SLT 2024)."],"url":"http://arxiv.org/abs/2405.05244v1","category":"eess.AS"}
{"created":"2024-05-08 17:40:03","title":"Deep learning-based variational autoencoder for classification of quantum and classical states of light","abstract":"Advancements in optical quantum technologies have been enabled by the generation, manipulation, and characterization of light, with identification based on its photon statistics. However, characterizing light and its sources through single photon measurements often requires efficient detectors and longer measurement times to obtain high-quality photon statistics. Here we introduce a deep learning-based variational autoencoder (VAE) method for classifying single photon added coherent state (SPACS), single photon added thermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of light. Our semisupervised learning-based VAE efficiently maps the photon statistics features of light to a lower dimension, enabling quasi-instantaneous classification with low average photon counts. The proposed VAE method is robust and maintains classification accuracy in the presence of losses inherent in an experiment, such as finite collection efficiency, non-unity quantum efficiency, finite number of detectors, etc. Additionally, leveraging the transfer learning capabilities of VAE enables successful classification of data of any quality using a single trained model. We envision that such a deep learning methodology will enable better classification of quantum light and light sources even in the presence of poor detection quality.","sentences":["Advancements in optical quantum technologies have been enabled by the generation, manipulation, and characterization of light, with identification based on its photon statistics.","However, characterizing light and its sources through single photon measurements often requires efficient detectors and longer measurement times to obtain high-quality photon statistics.","Here we introduce a deep learning-based variational autoencoder (VAE) method for classifying single photon added coherent state (SPACS), single photon added thermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of light.","Our semisupervised learning-based VAE efficiently maps the photon statistics features of light to a lower dimension, enabling quasi-instantaneous classification with low average photon counts.","The proposed VAE method is robust and maintains classification accuracy in the presence of losses inherent in an experiment, such as finite collection efficiency, non-unity quantum efficiency, finite number of detectors, etc.","Additionally, leveraging the transfer learning capabilities of VAE enables successful classification of data of any quality using a single trained model.","We envision that such a deep learning methodology will enable better classification of quantum light and light sources even in the presence of poor detection quality."],"url":"http://arxiv.org/abs/2405.05243v1","category":"quant-ph"}
{"created":"2024-05-08 17:38:47","title":"Quantum Steenrod operations and Fukaya categories","abstract":"This paper is concerned with quantum cohomology and Fukaya categories of a closed monotone symplectic manifold $X$, where we use coefficients in a field $\\mathbf{k}$ of characteristic $p>0$. The first main result of this paper is that the quantum Steenrod operations $Q\\Sigma$ admit an interpretation in terms of the Fukaya category of $X$, via suitable versions of the open-closed maps. Using this, we show that $Q\\Sigma$, whose definition is intrinsic to characteristic $p$, is compatible with certain structures inherited from the quantum connection in characteristic $0$. We then turn to applications of these results. The first application is an arithmetic proof of the unramified exponential type conjecture for $X$ that satisfies Abouzaid's generation criterion over $\\overline{\\mathbb{Q}}$, which uses a reduction mod $p$ argument. Next, we demonstrate how the categorical perspective provides new tools for computing $Q\\Sigma$ beyond the reach of known technology. We also explore potential connections of our work to arithmetic homological mirror symmetry.","sentences":["This paper is concerned with quantum cohomology and Fukaya categories of a closed monotone symplectic manifold $X$, where we use coefficients in a field $\\mathbf{k}$ of characteristic $p>0$. The first main result of this paper is that the quantum Steenrod operations $Q\\Sigma$ admit an interpretation in terms of the Fukaya category of $X$, via suitable versions of the open-closed maps.","Using this, we show that $Q\\Sigma$, whose definition is intrinsic to characteristic $p$, is compatible with certain structures inherited from the quantum connection in characteristic $0$. We then turn to applications of these results.","The first application is an arithmetic proof of the unramified exponential type conjecture for $X$ that satisfies Abouzaid's generation criterion over $\\overline{\\mathbb{Q}}$, which uses a reduction mod $p$ argument.","Next, we demonstrate how the categorical perspective provides new tools for computing $Q\\Sigma$ beyond the reach of known technology.","We also explore potential connections of our work to arithmetic homological mirror symmetry."],"url":"http://arxiv.org/abs/2405.05242v1","category":"math.SG"}
{"created":"2024-05-08 17:36:29","title":"An LSTM-Based Chord Generation System Using Chroma Histogram Representations","abstract":"This paper proposes a system for chord generation to monophonic symbolic melodies using an LSTM-based model trained on chroma histogram representations of chords. Chroma representations promise more harmonically rich generation than chord label-based approaches, whilst maintaining a small number of dimensions in the dataset. This system is shown to be suitable for limited real-time use. While it does not meet the state-of-the-art for coherent long-term generation, it does show diatonic generation with cadential chord relationships. The need for further study into chroma histograms as an extracted feature in chord generation tasks is highlighted.","sentences":["This paper proposes a system for chord generation to monophonic symbolic melodies using an LSTM-based model trained on chroma histogram representations of chords.","Chroma representations promise more harmonically rich generation than chord label-based approaches, whilst maintaining a small number of dimensions in the dataset.","This system is shown to be suitable for limited real-time use.","While it does not meet the state-of-the-art for coherent long-term generation, it does show diatonic generation with cadential chord relationships.","The need for further study into chroma histograms as an extracted feature in chord generation tasks is highlighted."],"url":"http://arxiv.org/abs/2405.05240v1","category":"cs.SD"}
{"created":"2024-05-08 17:36:05","title":"Fast Exact/Conservative Monte Carlo Confidence Intervals","abstract":"Monte Carlo tests about parameters can be \"inverted\" to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\\alpha$. When the tests are exact or conservative -- as some families of such tests are -- so are the confidence sets. Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases. When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way. For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible. An open-source Python implementation of the approach for the one-sample and two-sample problems is available.","sentences":["Monte Carlo tests about parameters can be \"inverted\" to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\\alpha$.","When the tests are exact or conservative -- as some families of such tests are -- so are the confidence sets.","Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data.","The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases.","When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way.","For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible.","An open-source Python implementation of the approach for the one-sample and two-sample problems is available."],"url":"http://arxiv.org/abs/2405.05238v1","category":"stat.CO"}
{"created":"2024-05-08 17:33:42","title":"EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning","abstract":"The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination. Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks. EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation. Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field. Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning. The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice. Our codes and models are available at: https://github.com/hustvl/EVA-X.","sentences":["The diagnosis and treatment of chest diseases play a crucial role in maintaining human health.","X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness.","Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination.","Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks.","EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation.","Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field.","Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning.","The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice.","Our codes and models are available at: https://github.com/hustvl/EVA-X."],"url":"http://arxiv.org/abs/2405.05237v1","category":"cs.CV"}
{"created":"2024-05-08 17:30:50","title":"Stability and Performance Analysis of Discrete-Time ReLU Recurrent Neural Networks","abstract":"This paper presents sufficient conditions for the stability and $\\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions. These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU. Our stability and performance condition uses these QCs along with a \"lifted\" representation for the ReLU RNN. We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU. We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon.","sentences":["This paper presents sufficient conditions for the stability and $\\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions.","These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs.","We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU.","Our stability and performance condition uses these QCs along with a \"lifted\" representation for the ReLU RNN.","We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU.","We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon."],"url":"http://arxiv.org/abs/2405.05236v1","category":"eess.SY"}
{"created":"2024-05-08 17:28:07","title":"RACH Traffic Prediction in Massive Machine Type Communications","abstract":"Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks. However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments. Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks. This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks. The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns. Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network. Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios. We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics. Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system.","sentences":["Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks.","However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments.","Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks.","This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks.","The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns.","Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network.","Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios.","We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics.","Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system."],"url":"http://arxiv.org/abs/2405.05235v1","category":"eess.SY"}
{"created":"2024-05-08 17:27:16","title":"Classical Grand Angular Momentum in N-Body Problems","abstract":"The concept of grand angular momentum is widely used in the study of N-body problems quantum mechanically. Here, we applied it to a classical analysis of N-body problems. Utilizing the tree representation for Jacobi and hyperspherical coordinates, we found a decomposition of its magnitude into magnitudes of one-body angular momenta in 3 dimensions. We generalized some results from the two-body problem. Furthermore, we derive the general expression for the scattering angle for the N-body problem.","sentences":["The concept of grand angular momentum is widely used in the study of N-body problems quantum mechanically.","Here, we applied it to a classical analysis of N-body problems.","Utilizing the tree representation for Jacobi and hyperspherical coordinates, we found a decomposition of its magnitude into magnitudes of one-body angular momenta in 3 dimensions.","We generalized some results from the two-body problem.","Furthermore, we derive the general expression for the scattering angle for the N-body problem."],"url":"http://arxiv.org/abs/2405.05233v1","category":"math-ph"}
{"created":"2024-05-08 17:25:11","title":"On $\\operatorname{Alt}(n)$-modules with an additive dimension when $n\\le6$","abstract":"Working in the general context of \"modules with an additive dimension,\" we complete the determination of the minimal dimension of a faithful Alt(n)-module and classify those modules in three of the exceptional cases: 2-dimensional Alt(5)-modules in characteristic 2, 3-dimensional Alt(5)-modules in characteristic 5, and 3-dimensional Alt(6)-modules in characteristic 3. We also highlight the remaining work needed to complete the classification of the faithful Alt(n)-modules of minimal dimension for all n; these open problems seem well suited as projects for advanced undergraduate or master's students.","sentences":["Working in the general context of \"modules with an additive dimension,\" we complete the determination of the minimal dimension of a faithful Alt(n)-module and classify those modules in three of the exceptional cases: 2-dimensional Alt(5)-modules in characteristic 2, 3-dimensional Alt(5)-modules in characteristic 5, and 3-dimensional Alt(6)-modules in characteristic 3.","We also highlight the remaining work needed to complete the classification of the faithful Alt(n)-modules of minimal dimension for all n; these open problems seem well suited as projects for advanced undergraduate or master's students."],"url":"http://arxiv.org/abs/2405.05230v1","category":"math.GR"}
{"created":"2024-05-08 17:22:49","title":"Generalized vector potential and Trace Theorem for Lipschitz domains","abstract":"The vector potential is a fundamental concept widely applied across various fields. This paper presents an existence theorem of a vector potential for divergence-free functions in $W^{m,p}(\\mathbb{R}^N,\\mathbb{T})$ with general $m,p,N$. Based on this theorem, one can establish the space decomposition theorem for functions in $W^{m,p}_0(\\operatorname{curl};\\Omega,\\mathbb{R}^N)$ and the trace theorem for functions in $W^{m,p}(\\Omega)$ within the Lipschitz domain $\\Omega \\subset \\mathbb{R}^N$. The methods of proof employed in this paper are straightforward, natural, and consistent.","sentences":["The vector potential is a fundamental concept widely applied across various fields.","This paper presents an existence theorem of a vector potential for divergence-free functions in $W^{m,p}(\\mathbb{R}^N,\\mathbb{T})$ with general $m,p,N$. Based on this theorem, one can establish the space decomposition theorem for functions in $W^{m,p}_0(\\operatorname{curl};\\Omega,\\mathbb{R}^N)$ and the trace theorem for functions in $W^{m,p}(\\Omega)$ within the Lipschitz domain $\\Omega \\subset \\mathbb{R}^N$.","The methods of proof employed in this paper are straightforward, natural, and consistent."],"url":"http://arxiv.org/abs/2405.05228v1","category":"math-ph"}
{"created":"2024-05-08 17:22:26","title":"Are Economically Advanced Countries More Efficient in Basic and Applied Research?","abstract":"Research and development (R&D) of countries play a major role in a long-term development of the economy. We measure the R&D efficiency of all 28 member countries of the European Union in the years 2008--2014. Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted. We use the number of citations as output of basic research, the number of patents as output of applied research and R&D expenditures with manpower as inputs. To meet DEA assumptions and to capture R&D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs. We find that the efficiency of general R&D is higher for countries with higher GDP per capita. This relation also holds for specialized efficiencies of basic and applied research. However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured. Our findings are important in the evaluation of research and policy making.","sentences":["Research and development (R&D) of countries play a major role in a long-term development of the economy.","We measure the R&D efficiency of all 28 member countries of the European Union in the years 2008--2014.","Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted.","We use the number of citations as output of basic research, the number of patents as output of applied research and R&D expenditures with manpower as inputs.","To meet DEA assumptions and to capture R&D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs.","We find that the efficiency of general R&D is higher for countries with higher GDP per capita.","This relation also holds for specialized efficiencies of basic and applied research.","However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured.","Our findings are important in the evaluation of research and policy making."],"url":"http://arxiv.org/abs/2405.05227v1","category":"stat.AP"}
{"created":"2024-05-08 17:18:37","title":"\"Community Guidelines Make this the Best Party on the Internet\": An In-Depth Study of Online Platforms' Content Moderation Policies","abstract":"Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.","sentences":["Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech.","Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content.","Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics.","This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content.","We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components.","We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings.","We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users."],"url":"http://arxiv.org/abs/2405.05225v1","category":"cs.HC"}
{"created":"2024-05-08 17:15:18","title":"Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation","abstract":"Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation.","sentences":["Diffusion models are a powerful generative framework, but come with expensive inference.","Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime.","In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps.","Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction.","Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations.","Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation."],"url":"http://arxiv.org/abs/2405.05224v1","category":"cs.CV"}
{"created":"2024-05-08 17:15:01","title":"Brooks-type colourings of digraphs in linear time","abstract":"Brooks' Theorem is a fundamental result on graph colouring, stating that the chromatic number of a graph is almost always upper bounded by its maximal degree. Lov\\'asz showed that such a colouring may then be computed in linear time when it exists. Many analogues are known for variants of (di)graph colouring, notably for list-colouring and partitions into subgraphs with prescribed degeneracy. One of the most general results of this kind is due to Borodin, Kostochka, and Toft, when asking for classes of colours to satisfy \"variable degeneracy\" constraints. An extension of this result to digraphs has recently been proposed by Bang-Jensen, Schweser, and Stiebitz, by considering colourings as partitions into \"variable weakly degenerate\" subdigraphs. Unlike earlier variants, there exists no linear-time algorithm to produce colourings for these generalisations.   We introduce the notion of (variable) bidegeneracy for digraphs, capturing multiple (di)graph degeneracy variants. We define the corresponding concept of $F$-dicolouring, where $F = (f_1,...,f_s)$ is a vector of functions, and an $F$-dicolouring requires vertices coloured $i$ to induce a \"strictly-$f_i$-bidegenerate\" subdigraph. We prove an analogue of Brooks' theorem for $F$-dicolouring, generalising the result of Bang-Jensen et al., and earlier analogues in turn.   Our new approach provides a linear-time algorithm that, given a digraph $D$, either produces an $F$-dicolouring of $D$, or correctly certifies that none exist. This yields the first linear-time algorithms to compute (di)colourings corresponding to the aforementioned generalisations of Brooks' theorem. In turn, it gives an unified framework to compute such colourings for various intermediate generalisations of Brooks' theorem such as list-(di)colouring and partitioning into (variable) degenerate sub(di)graphs.","sentences":["Brooks' Theorem is a fundamental result on graph colouring, stating that the chromatic number of a graph is almost always upper bounded by its maximal degree.","Lov\\'asz showed that such a colouring may then be computed in linear time when it exists.","Many analogues are known for variants of (di)graph colouring, notably for list-colouring and partitions into subgraphs with prescribed degeneracy.","One of the most general results of this kind is due to Borodin, Kostochka, and Toft, when asking for classes of colours to satisfy \"variable degeneracy\" constraints.","An extension of this result to digraphs has recently been proposed by Bang-Jensen, Schweser, and Stiebitz, by considering colourings as partitions into \"variable weakly degenerate\" subdigraphs.","Unlike earlier variants, there exists no linear-time algorithm to produce colourings for these generalisations.   ","We introduce the notion of (variable) bidegeneracy for digraphs, capturing multiple (di)graph degeneracy variants.","We define the corresponding concept of $F$-dicolouring, where $F = (f_1,...,f_s)$ is a vector of functions, and an $F$-dicolouring requires vertices coloured $i$ to induce a \"strictly-$f_i$-bidegenerate\" subdigraph.","We prove an analogue of Brooks' theorem for $F$-dicolouring, generalising the result of Bang-Jensen et al., and earlier analogues in turn.   ","Our new approach provides a linear-time algorithm that, given a digraph $D$, either produces an $F$-dicolouring of $D$, or correctly certifies that none exist.","This yields the first linear-time algorithms to compute (di)colourings corresponding to the aforementioned generalisations of Brooks' theorem.","In turn, it gives an unified framework to compute such colourings for various intermediate generalisations of Brooks' theorem such as list-(di)colouring and partitioning into (variable) degenerate sub(di)graphs."],"url":"http://arxiv.org/abs/2405.05222v1","category":"math.CO"}
{"created":"2024-05-08 17:13:34","title":"Causal Duration Analysis with Diff-in-Diff","abstract":"In economic program evaluation, it is common to obtain panel data in which outcomes are indicators that an individual has reached an absorbing state. For example, they may indicate whether an individual has exited a period of unemployment, passed an exam, left a marriage, or had their parole revoked. The parallel trends assumption that underpins difference-in-differences generally fails in such settings. We suggest identifying conditions that are analogous to those of difference-in-differences but apply to hazard rates rather than mean outcomes. These alternative assumptions motivate estimators that retain the simplicity and transparency of standard diff-in-diff, and we suggest analogous specification tests. Our approach can be adapted to general linear restrictions between the hazard rates of different groups, motivating duration analogues of the triple differences and synthetic control methods. We apply our procedures to examine the impact of a policy that increased the generosity of unemployment benefits, using a cross-cohort comparison.","sentences":["In economic program evaluation, it is common to obtain panel data in which outcomes are indicators that an individual has reached an absorbing state.","For example, they may indicate whether an individual has exited a period of unemployment, passed an exam, left a marriage, or had their parole revoked.","The parallel trends assumption that underpins difference-in-differences generally fails in such settings.","We suggest identifying conditions that are analogous to those of difference-in-differences but apply to hazard rates rather than mean outcomes.","These alternative assumptions motivate estimators that retain the simplicity and transparency of standard diff-in-diff, and we suggest analogous specification tests.","Our approach can be adapted to general linear restrictions between the hazard rates of different groups, motivating duration analogues of the triple differences and synthetic control methods.","We apply our procedures to examine the impact of a policy that increased the generosity of unemployment benefits, using a cross-cohort comparison."],"url":"http://arxiv.org/abs/2405.05220v1","category":"econ.EM"}
{"created":"2024-05-08 17:11:38","title":"Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers","abstract":"Large Language Models (LLMs) have profoundly changed the world. Their self-attention mechanism is the key to the success of transformers in LLMs. However, the quadratic computational cost $O(n^2)$ to the length $n$ input sequence is the notorious obstacle for further improvement and scalability in the longer context. In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices. We propose a $\\mathsf{conv}$ basis system, \"similar\" to the rank basis, and show that any lower triangular (attention) matrix can always be decomposed as a sum of $k$ structured convolution matrices in this basis system. We then design an algorithm to quickly decompose the attention matrix into $k$ convolution matrices. Thanks to Fast Fourier Transforms (FFT), the attention {\\it inference} can be computed in $O(knd \\log n)$ time, where $d$ is the hidden dimension. In practice, we have $ d \\ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma. Thus, when $kd = n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. Our approach can avoid explicitly computing the $n \\times n$ attention matrix, which may largely alleviate the quadratic computational complexity. Furthermore, our algorithm works on any input matrices. This work provides a new paradigm for accelerating attention computation in transformers to enable their application to longer contexts.","sentences":["Large Language Models (LLMs) have profoundly changed the world.","Their self-attention mechanism is the key to the success of transformers in LLMs.","However, the quadratic computational cost $O(n^2)$ to the length $n$ input sequence is the notorious obstacle for further improvement and scalability in the longer context.","In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices.","We propose a $\\mathsf{conv}$ basis system, \"similar\" to the rank basis, and show that any lower triangular (attention) matrix can always be decomposed as a sum of $k$ structured convolution matrices in this basis system.","We then design an algorithm to quickly decompose the attention matrix into $k$ convolution matrices.","Thanks to Fast Fourier Transforms (FFT), the attention {\\it inference} can be computed in $O(knd \\log n)$ time, where $d$ is the hidden dimension.","In practice, we have $ d \\ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma.","Thus, when $kd = n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well.","Our approach can avoid explicitly computing the $n \\times n$ attention matrix, which may largely alleviate the quadratic computational complexity.","Furthermore, our algorithm works on any input matrices.","This work provides a new paradigm for accelerating attention computation in transformers to enable their application to longer contexts."],"url":"http://arxiv.org/abs/2405.05219v1","category":"cs.LG"}
{"created":"2024-05-08 17:00:06","title":"The Ghent Hybrid Model in NuWro: a new neutrino single-pion production model in the GeV regime","abstract":"Neutrino-induced single-pion production constitutes an essential interaction channel in modern neutrino oscillation experiments, with its products building up a significant fraction of the observable hadronic final states. Frameworks of oscillation analyses strongly rely on Monte Carlo neutrino event generators, which provide theoretical predictions of neutrino interactions on nuclear targets. Thus, it is crucial to integrate state-of-the-art single-pion production models with Monte Carlo simulations to prepare for the upcoming systematics-dominated landscape of neutrino measurements. In this work, we present the implementation of the Ghent Hybrid model for neutrino-induced single-pion production in the NuWro Monte Carlo event generator. The interaction dynamics includes coherently-added contributions from nucleon resonances and a non-resonant background, merged into the pythia branching predictions in the deep-inelastic regime, as instrumented by NuWro. This neutrino-nucleon interaction model is fully incorporated into the nuclear framework of the generator, allowing it to account for the influence of both initial- and final-state nuclear medium effects. We compare the predictions of this integrated implementation with recent pion production data from accelerator-based neutrino experiments. The results of the novel model show improved agreement of the generator predictions with the data and point to the significance of the refined treatment of the description of pion-production processes beyond the $\\Delta$ region.","sentences":["Neutrino-induced single-pion production constitutes an essential interaction channel in modern neutrino oscillation experiments, with its products building up a significant fraction of the observable hadronic final states.","Frameworks of oscillation analyses strongly rely on Monte Carlo neutrino event generators, which provide theoretical predictions of neutrino interactions on nuclear targets.","Thus, it is crucial to integrate state-of-the-art single-pion production models with Monte Carlo simulations to prepare for the upcoming systematics-dominated landscape of neutrino measurements.","In this work, we present the implementation of the Ghent Hybrid model for neutrino-induced single-pion production in the NuWro Monte Carlo event generator.","The interaction dynamics includes coherently-added contributions from nucleon resonances and a non-resonant background, merged into the pythia branching predictions in the deep-inelastic regime, as instrumented by NuWro.","This neutrino-nucleon interaction model is fully incorporated into the nuclear framework of the generator, allowing it to account for the influence of both initial- and final-state nuclear medium effects.","We compare the predictions of this integrated implementation with recent pion production data from accelerator-based neutrino experiments.","The results of the novel model show improved agreement of the generator predictions with the data and point to the significance of the refined treatment of the description of pion-production processes beyond the $\\Delta$ region."],"url":"http://arxiv.org/abs/2405.05212v1","category":"hep-ph"}
{"created":"2024-05-08 16:58:25","title":"Broadcast Channel Synthesis from Shared Randomness","abstract":"We study the problem of synthesising a two-user broadcast channel using a common message, where each output terminal shares an independent source of randomness with the input terminal. This generalises two problems studied in the literature (Cuff, IEEE Trans. Inform. Theory, 2013; Kurri et.al., IEEE Trans. Inform. Theory, 2021). We give an inner bound on the tradeoff region between the rates of communication and shared randomness, and a lower bound on the minimum communication rate. Although the bounds presented here are not tight in general, they are tight for some special cases, including the aforementioned problems.","sentences":["We study the problem of synthesising a two-user broadcast channel using a common message, where each output terminal shares an independent source of randomness with the input terminal.","This generalises two problems studied in the literature (Cuff, IEEE Trans.","Inform.","Theory, 2013; Kurri et.al., IEEE Trans.","Inform.","Theory, 2021).","We give an inner bound on the tradeoff region between the rates of communication and shared randomness, and a lower bound on the minimum communication rate.","Although the bounds presented here are not tight in general, they are tight for some special cases, including the aforementioned problems."],"url":"http://arxiv.org/abs/2405.05211v1","category":"cs.IT"}
{"created":"2024-05-08 16:48:15","title":"Why the Universal Threshold for Primordial Black Hole Formation is Universal","abstract":"We show why the threshold for primordial black hole formation is universal (independent from the shape of the perturbation) when expressed in terms of the volume averaged compaction function. The proof is rooted in the self-similarity of the gravitational collapse phenomenon at criticality.","sentences":["We show why the threshold for primordial black hole formation is universal (independent from the shape of the perturbation) when expressed in terms of the volume averaged compaction function.","The proof is rooted in the self-similarity of the gravitational collapse phenomenon at criticality."],"url":"http://arxiv.org/abs/2405.05208v1","category":"astro-ph.CO"}
{"created":"2024-05-08 16:39:59","title":"Guided Combinatorial Algorithms for Submodular Maximization","abstract":"For constrained, not necessarily monotone submodular maximization, guiding the measured continuous greedy algorithm with a local search algorithm currently obtains the state-of-the-art approximation factor of 0.401 \\citep{buchbinder2023constrained}. These algorithms rely upon the multilinear extension and the Lovasz extension of a submodular set function. However, the state-of-the-art approximation factor of combinatorial algorithms has remained $1/e \\approx 0.367$ \\citep{buchbinder2014submodular}. In this work, we develop combinatorial analogues of the guided measured continuous greedy algorithm and obtain approximation ratio of $0.385$ in $\\oh{ kn }$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. Further, we derandomize these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$.","sentences":["For constrained, not necessarily monotone submodular maximization, guiding the measured continuous greedy algorithm with a local search algorithm currently obtains the state-of-the-art approximation factor of 0.401 \\citep{buchbinder2023constrained}.","These algorithms rely upon the multilinear extension and the Lovasz extension of a submodular set function.","However, the state-of-the-art approximation factor of combinatorial algorithms has remained $1/e \\approx 0.367$ \\citep{buchbinder2014submodular}.","In this work, we develop combinatorial analogues of the guided measured continuous greedy algorithm and obtain approximation ratio of $0.385$ in $\\oh{ kn }$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint.","Further, we derandomize these algorithms, maintaining the same ratio and asymptotic time complexity.","Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$."],"url":"http://arxiv.org/abs/2405.05202v1","category":"cs.DS"}
{"created":"2024-05-08 16:39:09","title":"Multimode amplitude squeezing through cascaded nonlinear optical processes","abstract":"Multimode squeezed light is enticing for several applications, from squeezed frequency combs for spectroscopy to signal multiplexing in optical computing. To generate squeezing in multiple frequency modes, optical parametric oscillators have been vital in realizing multimode squeezed vacuum states through second-order nonlinear processes. However, most work has focused on generating multimode squeezed vacua and squeezing in mode superpositions (supermodes). Bright squeezing in multiple discrete frequency modes, if realized, could unlock novel applications in quantum-enhanced spectroscopy and optical quantum computing. Here, we show how $Q$ factor engineering of a multimode nonlinear cavity with cascaded three wave mixing processes creates strong, spectrally tunable single mode output amplitude noise squeezing over 10 dB below the shot noise limit. In addition, we demonstrate squeezing for multiple discrete frequency modes above threshold. This bright squeezing arises from enhancement of the (noiseless) nonlinear rate relative to decay rates in the system due to the cascaded generation of photons in a single idler \"bath\" mode. A natural consequence of the strong nonlinear coupling in our system is the creation of an effective cavity in the synthetic frequency dimension that sustains Bloch oscillations in the modal energy distribution. Bloch mode engineering could provide an opportunity to better control nonlinear energy flow in the synthetic frequency dimension, with exciting applications in quantum random walks and topological photonics. Lastly, we show evidence of long-range correlations in amplitude noise between discrete frequency modes, pointing towards the potential of long-range entanglement in a synthetic frequency dimension.","sentences":["Multimode squeezed light is enticing for several applications, from squeezed frequency combs for spectroscopy to signal multiplexing in optical computing.","To generate squeezing in multiple frequency modes, optical parametric oscillators have been vital in realizing multimode squeezed vacuum states through second-order nonlinear processes.","However, most work has focused on generating multimode squeezed vacua and squeezing in mode superpositions (supermodes).","Bright squeezing in multiple discrete frequency modes, if realized, could unlock novel applications in quantum-enhanced spectroscopy and optical quantum computing.","Here, we show how $Q$ factor engineering of a multimode nonlinear cavity with cascaded three wave mixing processes creates strong, spectrally tunable single mode output amplitude noise squeezing over 10 dB below the shot noise limit.","In addition, we demonstrate squeezing for multiple discrete frequency modes above threshold.","This bright squeezing arises from enhancement of the (noiseless) nonlinear rate relative to decay rates in the system due to the cascaded generation of photons in a single idler \"bath\" mode.","A natural consequence of the strong nonlinear coupling in our system is the creation of an effective cavity in the synthetic frequency dimension that sustains Bloch oscillations in the modal energy distribution.","Bloch mode engineering could provide an opportunity to better control nonlinear energy flow in the synthetic frequency dimension, with exciting applications in quantum random walks and topological photonics.","Lastly, we show evidence of long-range correlations in amplitude noise between discrete frequency modes, pointing towards the potential of long-range entanglement in a synthetic frequency dimension."],"url":"http://arxiv.org/abs/2405.05201v1","category":"physics.optics"}
{"created":"2024-05-08 16:32:05","title":"Existence and dynamics of normalized solutions to Schr\u00f6dinger equations with generic double-behaviour nonlinearities","abstract":"We study the existence of solutions $(\\underline u,\\lambda_{\\underline u})\\in H^1(\\mathbb{R}^N; \\mathbb{R}) \\times \\mathbb{R}$ to \\[ -\\Delta u + \\lambda u = f(u) \\quad \\text{in } \\mathbb{R}^N \\] with $N \\ge 3$ and prescribed $L^2$ norm, and the dynamics of the solutions to \\[ \\begin{cases} \\mathrm{i} \\partial_t \\Psi + \\Delta \\Psi = f(\\Psi)\\\\ \\Psi(\\cdot,0) = \\psi_0 \\in H^1(\\mathbb{R}^N; \\mathbb{C}) \\end{cases} \\] with $\\psi_0$ close to $\\underline u$. Here, the nonlinear term $f$ has mass-subcritical growth at the origin, mass-supercritical growth at infinity, and is more general than the sum of two powers. Under different assumptions, we prove the existence of a locally least-energy solution, the orbital stability of all such solutions, the existence of a second solution with higher energy, and the strong instability of such a solution.","sentences":["We study the existence of solutions $(\\underline u,\\lambda_{\\underline u})\\in H^1(\\mathbb{R}^N; \\mathbb{R})","\\times \\mathbb{R}$ to \\[ -\\Delta u + \\lambda u = f(u)","\\quad \\text{in } \\mathbb{R}^N \\] with $N \\ge 3$ and prescribed $L^2$ norm, and the dynamics of the solutions to \\[ \\begin{cases} \\mathrm{i} \\partial_t \\Psi + \\Delta \\Psi = f(\\Psi)\\\\ \\Psi(\\cdot,0) = \\psi_0 \\in H^1(\\mathbb{R}^N; \\mathbb{C}) \\end{cases} \\] with $\\psi_0$ close to $\\underline u$. Here, the nonlinear term $f$ has mass-subcritical growth at the origin, mass-supercritical growth at infinity, and is more general than the sum of two powers.","Under different assumptions, we prove the existence of a locally least-energy solution, the orbital stability of all such solutions, the existence of a second solution with higher energy, and the strong instability of such a solution."],"url":"http://arxiv.org/abs/2405.05194v1","category":"math.AP"}
{"created":"2024-05-08 16:31:41","title":"Systematic Use of Random Self-Reducibility against Physical Attacks","abstract":"This work presents a novel, black-box software-based countermeasure against physical attacks including power side-channel and fault-injection attacks. The approach uses the concept of random self-reducibility and self-correctness to add randomness and redundancy in the execution for protection. Our approach is at the operation level, is not algorithm-specific, and thus, can be applied for protecting a wide range of algorithms. The countermeasure is empirically evaluated against attacks over operations like modular exponentiation, modular multiplication, polynomial multiplication, and number theoretic transforms. An end-to-end implementation of this countermeasure is demonstrated for RSA-CRT signature algorithm and Kyber Key Generation public key cryptosystems. The countermeasure reduced the power side-channel leakage by two orders of magnitude, to an acceptably secure level in TVLA analysis. For fault injection, the countermeasure reduces the number of faults to 95.4% in average.","sentences":["This work presents a novel, black-box software-based countermeasure against physical attacks including power side-channel and fault-injection attacks.","The approach uses the concept of random self-reducibility and self-correctness to add randomness and redundancy in the execution for protection.","Our approach is at the operation level, is not algorithm-specific, and thus, can be applied for protecting a wide range of algorithms.","The countermeasure is empirically evaluated against attacks over operations like modular exponentiation, modular multiplication, polynomial multiplication, and number theoretic transforms.","An end-to-end implementation of this countermeasure is demonstrated for RSA-CRT signature algorithm and Kyber Key Generation public key cryptosystems.","The countermeasure reduced the power side-channel leakage by two orders of magnitude, to an acceptably secure level in TVLA analysis.","For fault injection, the countermeasure reduces the number of faults to 95.4% in average."],"url":"http://arxiv.org/abs/2405.05193v1","category":"cs.CR"}
{"created":"2024-05-08 16:26:49","title":"Is Transductive Learning Equivalent to PAC Learning?","abstract":"Most work in the area of learning theory has focused on designing effective Probably Approximately Correct (PAC) learners. Recently, other models of learning such as transductive error have seen more scrutiny. We move toward showing that these problems are equivalent by reducing agnostic learning with a PAC guarantee to agnostic learning with a transductive guarantee by adding a small number of samples to the dataset. We first rederive the result of Aden-Ali et al. arXiv:2304.09167 reducing PAC learning to transductive learning in the realizable setting using simpler techniques and at more generality as background for our main positive result. Our agnostic transductive to PAC conversion technique extends the aforementioned argument to the agnostic case, showing that an agnostic transductive learner can be efficiently converted to an agnostic PAC learner. Finally, we characterize the performance of the agnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 for binary classification, and show that plugging it into our reduction leads to an agnostic PAC learner that is essentially optimal. Our results imply that transductive and PAC learning are essentially equivalent for supervised learning with pseudometric losses in the realizable setting, and for binary classification in the agnostic setting. We conjecture this is true more generally for the agnostic setting.","sentences":["Most work in the area of learning theory has focused on designing effective Probably Approximately Correct (PAC) learners.","Recently, other models of learning such as transductive error have seen more scrutiny.","We move toward showing that these problems are equivalent by reducing agnostic learning with a PAC guarantee to agnostic learning with a transductive guarantee by adding a small number of samples to the dataset.","We first rederive the result of Aden-Ali et al.","arXiv:2304.09167 reducing PAC learning to transductive learning in the realizable setting using simpler techniques and at more generality as background for our main positive result.","Our agnostic transductive to PAC conversion technique extends the aforementioned argument to the agnostic case, showing that an agnostic transductive learner can be efficiently converted to an agnostic PAC learner.","Finally, we characterize the performance of the agnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 for binary classification, and show that plugging it into our reduction leads to an agnostic PAC learner that is essentially optimal.","Our results imply that transductive and PAC learning are essentially equivalent for supervised learning with pseudometric losses in the realizable setting, and for binary classification in the agnostic setting.","We conjecture this is true more generally for the agnostic setting."],"url":"http://arxiv.org/abs/2405.05190v1","category":"stat.ML"}
{"created":"2024-05-08 16:25:42","title":"MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning","abstract":"We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.","sentences":["We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs).","Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability.","Additionally, relying solely on a single sample may result in the omission of true nodes and edges.","To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer.","To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM.","This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision.","Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts."],"url":"http://arxiv.org/abs/2405.05189v1","category":"cs.CL"}
{"created":"2024-05-08 16:20:20","title":"The role of AGN winds in galaxy formation: connecting AGN outflows at low redshifts to the formation/evolution of their host galaxies","abstract":"Using SDSS spectra, we applied an automatic method to search for outflows (OFs) in three large samples of narrow-line AGN at low redshifts (z < 0.4), separated in three spectral activity classes: radio-loud RG, 15,793, radio-quiet, Sy2, 18,585, and LINER, 25,656. In general, the probability of detecting an OF decreases along the sequence Sy1->Sy2->LINER/RG and, independently of the AGN class, the wind velocity, traced by W80, increases with the AGN luminosity. Moreover W80 is systematically higher in RG or any of the other AGN class when detected in radio. These results support the idea that there are two main modes of production of OF, the radiative mode dominant in radio-quiet AGN and the jet mode dominant in radio-loud galaxies, although both modes could also happen simultaneously at different levels. From the spectra and SDSS photometry, the characteristics of the AGN host galaxies and their super-massive black holes (SMBHs) were also retrieved using the stellar population synthesis code STARLIGHT. This revealed that, independently of their spectral class, 1) galaxy hosts with OFs have systematically later morphological types and higher star formation rates than their counterpart without OF, 2) they occupy different positions in the specific diagnostic diagram, sSMBH vs. sSFR, which suggests they follow different evolutionary paths congruent with the morphology of their galaxy hosts, and 3) they show no evidence of AGN quenching or triggering of star formation. These results are consistent with a scenario explaining the different AGN classes as consequences of different formation processes of galaxies: early-type galaxies (LINER and RG) formed bigger bulges and more massive SMBHs, exhausting their reservoir of gas more rapidly than late-type galaxies (Sy2 and Sy1), quenching their star formation and starving their SMBHs.","sentences":["Using SDSS spectra, we applied an automatic method to search for outflows (OFs) in three large samples of narrow-line AGN at low redshifts (z < 0.4), separated in three spectral activity classes: radio-loud RG, 15,793, radio-quiet, Sy2, 18,585, and LINER, 25,656.","In general, the probability of detecting an OF decreases along the sequence Sy1->Sy2->LINER/RG and, independently of the AGN class, the wind velocity, traced by W80, increases with the AGN luminosity.","Moreover W80 is systematically higher in RG or any of the other AGN class when detected in radio.","These results support the idea that there are two main modes of production of OF, the radiative mode dominant in radio-quiet AGN and the jet mode dominant in radio-loud galaxies, although both modes could also happen simultaneously at different levels.","From the spectra and SDSS photometry, the characteristics of the AGN host galaxies and their super-massive black holes (SMBHs) were also retrieved using the stellar population synthesis code STARLIGHT.","This revealed that, independently of their spectral class, 1) galaxy hosts with OFs have systematically later morphological types and higher star formation rates than their counterpart without OF, 2) they occupy different positions in the specific diagnostic diagram, sSMBH vs. sSFR, which suggests they follow different evolutionary paths congruent with the morphology of their galaxy hosts, and 3) they show no evidence of AGN quenching or triggering of star formation.","These results are consistent with a scenario explaining the different AGN classes as consequences of different formation processes of galaxies: early-type galaxies (LINER and RG) formed bigger bulges and more massive SMBHs, exhausting their reservoir of gas more rapidly than late-type galaxies (Sy2 and Sy1), quenching their star formation and starving their SMBHs."],"url":"http://arxiv.org/abs/2405.05184v1","category":"astro-ph.GA"}
{"created":"2024-05-08 16:20:06","title":"Exact solution of Dynamical Mean-Field Theory for a linear system with annealed disorder","abstract":"We investigate a disordered multi-dimensional linear system in which the interaction parameters vary stochastically in time with defined temporal correlations. We refer to this type of disorder as \"annealed\", in contrast to quenched disorder in which couplings are fixed in time. We extend Dynamical Mean-Field Theory to accommodate annealed disorder and employ it to find the exact solution of the linear model in the limit of a large number of degrees of freedom. Our analysis yields analytical results for the non-stationary auto-correlation, the stationary variance, the power spectral density, and the phase diagram of the model. Interestingly, some unexpected features emerge upon changing the correlation time of the interactions. The stationary variance of the system and the critical variance of the disorder are generally found to be a non-monotonic function of the correlation time of the interactions. We also find that in some cases a re-entrant phase transition takes place when this correlation time is varied.","sentences":["We investigate a disordered multi-dimensional linear system in which the interaction parameters vary stochastically in time with defined temporal correlations.","We refer to this type of disorder as \"annealed\", in contrast to quenched disorder in which couplings are fixed in time.","We extend Dynamical Mean-Field Theory to accommodate annealed disorder and employ it to find the exact solution of the linear model in the limit of a large number of degrees of freedom.","Our analysis yields analytical results for the non-stationary auto-correlation, the stationary variance, the power spectral density, and the phase diagram of the model.","Interestingly, some unexpected features emerge upon changing the correlation time of the interactions.","The stationary variance of the system and the critical variance of the disorder are generally found to be a non-monotonic function of the correlation time of the interactions.","We also find that in some cases a re-entrant phase transition takes place when this correlation time is varied."],"url":"http://arxiv.org/abs/2405.05183v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-08 16:14:02","title":"Fusion rule in conformal field theories and topological orders: A unified view of correspondence and (fractional) supersymmetry and their relation to topological holography","abstract":"Generalized symmetry, including non-invertible and categorical symmetry, plays a central role in contemporary studies on topological orders (TOs) and the corresponding conformal field theories (CFTs). The generators of such symmetries have a close connection to non-abelian anyonic objects in a bulk CFT or chiral CFT (CCFT), but it has been known that the construction of a CCFT contains theoretical difficulties in general. In this work, we revisit the structure of the fusion rule in $Z_{N}$ symmetric chiral and bulk conformal field theories and the corresponding TOs. We propose a nontrivial expression of subalgebra structure in the fusion rule of a bulk CFT. We name this subalgebra \"bulk semionization\" which corresponds to the fusion rule of the CCFTs and categorical symmetry of the TOs. This is a bulk-edge correspondence based on the symmetry analysis and can be interpreted as a version of topological holography in the recent literature. The topological holography has been expected to be applicable to the systems in general space-time dimensions. Moreover, we give a concise way of unifying duality (or fractional supersymmetry), generalized or categorical symmetry, and Lagrangian subalgebra. Our method is potentially useful to formulate and study general TOs, fundamentally only from the data of bulk CFTs.","sentences":["Generalized symmetry, including non-invertible and categorical symmetry, plays a central role in contemporary studies on topological orders (TOs) and the corresponding conformal field theories (CFTs).","The generators of such symmetries have a close connection to non-abelian anyonic objects in a bulk CFT or chiral CFT (CCFT), but it has been known that the construction of a CCFT contains theoretical difficulties in general.","In this work, we revisit the structure of the fusion rule in $Z_{N}$ symmetric chiral and bulk conformal field theories and the corresponding TOs.","We propose a nontrivial expression of subalgebra structure in the fusion rule of a bulk CFT.","We name this subalgebra \"bulk semionization\" which corresponds to the fusion rule of the CCFTs and categorical symmetry of the TOs.","This is a bulk-edge correspondence based on the symmetry analysis and can be interpreted as a version of topological holography in the recent literature.","The topological holography has been expected to be applicable to the systems in general space-time dimensions.","Moreover, we give a concise way of unifying duality (or fractional supersymmetry), generalized or categorical symmetry, and Lagrangian subalgebra.","Our method is potentially useful to formulate and study general TOs, fundamentally only from the data of bulk CFTs."],"url":"http://arxiv.org/abs/2405.05178v1","category":"hep-th"}
{"created":"2024-05-08 16:13:40","title":"Encoder-Decoder Framework for Interactive Free Verses with Generation with Controllable High-Quality Rhyming","abstract":"Composing poetry or lyrics involves several creative factors, but a challenging aspect of generation is the adherence to a more or less strict metric and rhyming pattern. To address this challenge specifically, previous work on the task has mainly focused on reverse language modeling, which brings the critical selection of each rhyming word to the forefront of each verse. On the other hand, reversing the word order requires that models be trained from scratch with this task-specific goal and cannot take advantage of transfer learning from a Pretrained Language Model (PLM). We propose a novel fine-tuning approach that prepends the rhyming word at the start of each lyric, which allows the critical rhyming decision to be made before the model commits to the content of the lyric (as during reverse language modeling), but maintains compatibility with the word order of regular PLMs as the lyric itself is still generated in left-to-right order. We conducted extensive experiments to compare this fine-tuning against the current state-of-the-art strategies for rhyming, finding that our approach generates more readable text and better rhyming capabilities. Furthermore, we furnish a high-quality dataset in English and 12 other languages, analyse the approach's feasibility in a multilingual context, provide extensive experimental results shedding light on good and bad practices for lyrics generation, and propose metrics to compare methods in the future.","sentences":["Composing poetry or lyrics involves several creative factors, but a challenging aspect of generation is the adherence to a more or less strict metric and rhyming pattern.","To address this challenge specifically, previous work on the task has mainly focused on reverse language modeling, which brings the critical selection of each rhyming word to the forefront of each verse.","On the other hand, reversing the word order requires that models be trained from scratch with this task-specific goal and cannot take advantage of transfer learning from a Pretrained Language Model (PLM).","We propose a novel fine-tuning approach that prepends the rhyming word at the start of each lyric, which allows the critical rhyming decision to be made before the model commits to the content of the lyric (as during reverse language modeling), but maintains compatibility with the word order of regular PLMs as the lyric itself is still generated in left-to-right order.","We conducted extensive experiments to compare this fine-tuning against the current state-of-the-art strategies for rhyming, finding that our approach generates more readable text and better rhyming capabilities.","Furthermore, we furnish a high-quality dataset in English and 12 other languages, analyse the approach's feasibility in a multilingual context, provide extensive experimental results shedding light on good and bad practices for lyrics generation, and propose metrics to compare methods in the future."],"url":"http://arxiv.org/abs/2405.05176v1","category":"cs.CL"}
{"created":"2024-05-08 16:10:46","title":"A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective","abstract":"3D occupancy perception technology aims to observe and understand dense 3D environments for autonomous vehicles. Owing to its comprehensive perception capability, this technology is emerging as a trend in autonomous driving perception systems, and is attracting significant attention from both industry and academia. Similar to traditional bird's-eye view (BEV) perception, 3D occupancy perception has the nature of multi-source input and the necessity for information fusion. However, the difference is that it captures vertical structures that are ignored by 2D BEV. In this survey, we review the most recent works on 3D occupancy perception, and provide in-depth analyses of methodologies with various input modalities. Specifically, we summarize general network pipelines, highlight information fusion techniques, and discuss effective network training. We evaluate and analyze the occupancy perception performance of the state-of-the-art on the most popular datasets. Furthermore, challenges and future research directions are discussed. We hope this report will inspire the community and encourage more research work on 3D occupancy perception. A comprehensive list of studies in this survey is available in an active repository that continuously collects the latest work: https://github.com/HuaiyuanXu/3D-Occupancy-Perception.","sentences":["3D occupancy perception technology aims to observe and understand dense 3D environments for autonomous vehicles.","Owing to its comprehensive perception capability, this technology is emerging as a trend in autonomous driving perception systems, and is attracting significant attention from both industry and academia.","Similar to traditional bird's-eye view (BEV) perception, 3D occupancy perception has the nature of multi-source input and the necessity for information fusion.","However, the difference is that it captures vertical structures that are ignored by 2D BEV.","In this survey, we review the most recent works on 3D occupancy perception, and provide in-depth analyses of methodologies with various input modalities.","Specifically, we summarize general network pipelines, highlight information fusion techniques, and discuss effective network training.","We evaluate and analyze the occupancy perception performance of the state-of-the-art on the most popular datasets.","Furthermore, challenges and future research directions are discussed.","We hope this report will inspire the community and encourage more research work on 3D occupancy perception.","A comprehensive list of studies in this survey is available in an active repository that continuously collects the latest work: https://github.com/HuaiyuanXu/3D-Occupancy-Perception."],"url":"http://arxiv.org/abs/2405.05173v1","category":"cs.CV"}
{"created":"2024-05-08 16:09:40","title":"Sobolev mappings on metric spaces and Minkowski dimension","abstract":"We introduce the class of compactly H\\\"older mappings between metric spaces and determine the extent to which they distort the Minkowski dimension of a given set. These mappings are defined purely with metric notions and can be seen as a generalization of Sobolev mappings, without the requirement for a measure on the source space. In fact, we show that if $f:X\\rightarrow Y$ is a continuous mapping lying in some super-critical Newtonian-Sobolev space $N^{1,p}(X,\\mu)$, under standard assumptions on the metric measure space $(X,d,\\mu)$, it is then a compactly H\\\"older mapping. The dimension distortion result we obtain is new even for Sobolev mappings between weighted Euclidean spaces and generalizes previous results of Kaufman and Bishop-Hakobyan-Williams.","sentences":["We introduce the class of compactly H\\\"older mappings between metric spaces and determine the extent to which they distort the Minkowski dimension of a given set.","These mappings are defined purely with metric notions and can be seen as a generalization of Sobolev mappings, without the requirement for a measure on the source space.","In fact, we show that if $f:X\\rightarrow Y$ is a continuous mapping lying in some super-critical Newtonian-Sobolev space $N^{1,p}(X,\\mu)$, under standard assumptions on the metric measure space $(X,d,\\mu)$, it is then a compactly H\\\"older mapping.","The dimension distortion result we obtain is new even for Sobolev mappings between weighted Euclidean spaces and generalizes previous results of Kaufman and Bishop-Hakobyan-Williams."],"url":"http://arxiv.org/abs/2405.05172v1","category":"math.DS"}
{"created":"2024-05-08 16:06:22","title":"Asymmetric Symmetry Breaking: Unequal Probabilities of Vacuum Selection","abstract":"We study the probabilities of a field, subject to random perturbations, to roll down from the top of a potential, where the top is only $C^1$ continuous. We find that the probability to roll down to the left or right depends the square root of the second derivative of the potential at the top. We solve this problem theoretically by using the Fokker-Planck equations in stochastic process and verify our findings numerically. This study may potentially be a new mechanism to explain the origins of asymmetries in the Universe.","sentences":["We study the probabilities of a field, subject to random perturbations, to roll down from the top of a potential, where the top is only $C^1$ continuous.","We find that the probability to roll down to the left or right depends the square root of the second derivative of the potential at the top.","We solve this problem theoretically by using the Fokker-Planck equations in stochastic process and verify our findings numerically.","This study may potentially be a new mechanism to explain the origins of asymmetries in the Universe."],"url":"http://arxiv.org/abs/2405.05168v1","category":"hep-th"}
{"created":"2024-05-08 16:04:50","title":"Data-Error Scaling in Machine Learning on Natural Discrete Combinatorial Mutation-prone Sets: Case Studies on Peptides and Small Molecules","abstract":"We investigate trends in the data-error scaling behavior of machine learning (ML) models trained on discrete combinatorial spaces that are prone-to-mutation, such as proteins or organic small molecules. We trained and evaluated kernel ridge regression machines using variable amounts of computationally generated training data. Our synthetic datasets comprise i) two na\\\"ive functions based on many-body theory; ii) binding energy estimates between a protein and a mutagenised peptide; and iii) solvation energies of two 6-heavy atom structural graphs. In contrast to typical data-error scaling, our results showed discontinuous monotonic phase transitions during learning, observed as rapid drops in the test error at particular thresholds of training data. We observed two learning regimes, which we call saturated and asymptotic decay, and found that they are conditioned by the level of complexity (i.e. number of mutations) enclosed in the training set. We show that during training on this class of problems, the predictions were clustered by the ML models employed in the calibration plots. Furthermore, we present an alternative strategy to normalize learning curves (LCs) and the concept of mutant based shuffling. This work has implications for machine learning on mutagenisable discrete spaces such as chemical properties or protein phenotype prediction, and improves basic understanding of concepts in statistical learning theory.","sentences":["We investigate trends in the data-error scaling behavior of machine learning (ML) models trained on discrete combinatorial spaces that are prone-to-mutation, such as proteins or organic small molecules.","We trained and evaluated kernel ridge regression machines using variable amounts of computationally generated training data.","Our synthetic datasets comprise i) two na\\\"ive functions based on many-body theory; ii) binding energy estimates between a protein and a mutagenised peptide; and iii) solvation energies of two 6-heavy atom structural graphs.","In contrast to typical data-error scaling, our results showed discontinuous monotonic phase transitions during learning, observed as rapid drops in the test error at particular thresholds of training data.","We observed two learning regimes, which we call saturated and asymptotic decay, and found that they are conditioned by the level of complexity (i.e. number of mutations) enclosed in the training set.","We show that during training on this class of problems, the predictions were clustered by the ML models employed in the calibration plots.","Furthermore, we present an alternative strategy to normalize learning curves (LCs) and the concept of mutant based shuffling.","This work has implications for machine learning on mutagenisable discrete spaces such as chemical properties or protein phenotype prediction, and improves basic understanding of concepts in statistical learning theory."],"url":"http://arxiv.org/abs/2405.05167v1","category":"physics.chem-ph"}
{"created":"2024-05-08 15:52:50","title":"Selective Classification Under Distribution Shifts","abstract":"In selective classification (SC), a classifier abstains from making predictions that are likely to be wrong to avoid excessive errors. To deploy imperfect classifiers -- imperfect either due to intrinsic statistical noise of data or for robustness issue of the classifier or beyond -- in high-stakes scenarios, SC appears to be an attractive and necessary path to follow. Despite decades of research in SC, most previous SC methods still focus on the ideal statistical setting only, i.e., the data distribution at deployment is the same as that of training, although practical data can come from the wild. To bridge this gap, in this paper, we propose an SC framework that takes into account distribution shifts, termed generalized selective classification, that covers label-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature. We focus on non-training-based confidence-score functions for generalized SC on deep learning (DL) classifiers and propose two novel margin-based score functions. Through extensive analysis and experiments, we show that our proposed score functions are more effective and reliable than the existing ones for generalized SC on a variety of classification tasks and DL classifiers.","sentences":["In selective classification (SC), a classifier abstains from making predictions that are likely to be wrong to avoid excessive errors.","To deploy imperfect classifiers -- imperfect either due to intrinsic statistical noise of data or for robustness issue of the classifier or beyond -- in high-stakes scenarios, SC appears to be an attractive and necessary path to follow.","Despite decades of research in SC, most previous SC methods still focus on the ideal statistical setting only, i.e., the data distribution at deployment is the same as that of training, although practical data can come from the wild.","To bridge this gap, in this paper, we propose an SC framework that takes into account distribution shifts, termed generalized selective classification, that covers label-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature.","We focus on non-training-based confidence-score functions for generalized SC on deep learning (DL) classifiers and propose two novel margin-based score functions.","Through extensive analysis and experiments, we show that our proposed score functions are more effective and reliable than the existing ones for generalized SC on a variety of classification tasks and DL classifiers."],"url":"http://arxiv.org/abs/2405.05160v1","category":"cs.LG"}
{"created":"2024-05-08 15:49:24","title":"Filtering and smoothing estimation algorithms from uncertain nonlinear observations with time-correlated additive noise and random deception attacks","abstract":"This paper discusses the problem of estimating a stochastic signal from nonlinear uncertain observations with time-correlated additive noise described by a first-order Markov process. Random deception attacks are assumed to be launched by an adversary, and both this phenomenon and the uncertainty in the observations are modelled by two sets of Bernoulli random variables. Under the assumption that the evolution model generating the signal to be estimated is unknown and only the mean and covariance functions of the processes involved in the observation equation are available, recursive algorithms based on linear approximations of the real observations are proposed for the least-squares filtering and fixed-point smoothing problems. Finally, the feasibility and effectiveness of the developed estimation algorithms are verified by a numerical simulation example, where the impact of uncertain observation and deception attack probabilities on estimation accuracy is evaluated.","sentences":["This paper discusses the problem of estimating a stochastic signal from nonlinear uncertain observations with time-correlated additive noise described by a first-order Markov process.","Random deception attacks are assumed to be launched by an adversary, and both this phenomenon and the uncertainty in the observations are modelled by two sets of Bernoulli random variables.","Under the assumption that the evolution model generating the signal to be estimated is unknown and only the mean and covariance functions of the processes involved in the observation equation are available, recursive algorithms based on linear approximations of the real observations are proposed for the least-squares filtering and fixed-point smoothing problems.","Finally, the feasibility and effectiveness of the developed estimation algorithms are verified by a numerical simulation example, where the impact of uncertain observation and deception attack probabilities on estimation accuracy is evaluated."],"url":"http://arxiv.org/abs/2405.05157v1","category":"eess.SP"}
{"created":"2024-05-08 15:46:31","title":"The Potential and Implications of Generative AI on HCI Education","abstract":"Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines. As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI. In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module. We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions. Our insights are based on replies to a survey sent out to the students after completing the module. Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students' understanding of key concepts and ideas and highlighting knowledge gaps. We also discuss potential pitfalls that should be considered and the need to assess students' literacies and assumptions of GAIs as pedagogical tools. Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice. We end with a discussion of our findings in relation to the TPACK framework in HCI.","sentences":["Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines.","As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI.","In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module.","We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions.","Our insights are based on replies to a survey sent out to the students after completing the module.","Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students' understanding of key concepts and ideas and highlighting knowledge gaps.","We also discuss potential pitfalls that should be considered and the need to assess students' literacies and assumptions of GAIs as pedagogical tools.","Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice.","We end with a discussion of our findings in relation to the TPACK framework in HCI."],"url":"http://arxiv.org/abs/2405.05154v1","category":"cs.HC"}
{"created":"2024-05-08 15:40:00","title":"Early and elongated epochs of planetesimal dynamo generation","abstract":"Accreting in the first few Ma after Solar System formation, planetesimals record conditions in the protoplanetary disc and are the remnants of planetary formation processes. The meteorite paleomagnetic record carries key insights into the thermal history of planetesimals and their extent of differentiation. The current paradigm splits the paleomagnetic record into three magnetic field generation epochs: an early nebula field (<5Ma after CAI formation), followed by thermal dynamos (5-34 Ma after CAI formation), then a gap in dynamo generation, before the onset of core solidification and compositional dynamos. The split between these epochs has been defined using thermal evolution and dynamo generation models of planetesimals. Here we demonstrate these epochs are not as distinct as previously thought based on our refined thermal evolution model that includes more realistic parametrisations for mantle convection, non-eutectic core solidification and radiogenic $^{60}Fe$ in the core. Inclusion of $^{60}$ in the core brings forward the onset of dynamo generation to 1-2 Ma after CAI formation, which overlaps with the existence of the nebula field. The second epoch of dynamo generation begins prior to the onset of core solidification, suggesting this epoch is not purely compositionally driven. Planetesimal radius is the dominant control on dynamo generation, and the choice of reference viscosity can widen the gap between epochs of dynamo generation from 0-200 Ma. Overall, timings of different planetesimal magnetic field generation mechanisms are more variable. This alters the information we can glean from the meteorite paleomagnetic record about the early Solar System. Evidence for the nebula field requires more careful interpretation and young paleomagnetic remanences, for example in the pallasites, may not be evidence for planetesimal core solidification.","sentences":["Accreting in the first few Ma after Solar System formation, planetesimals record conditions in the protoplanetary disc and are the remnants of planetary formation processes.","The meteorite paleomagnetic record carries key insights into the thermal history of planetesimals and their extent of differentiation.","The current paradigm splits the paleomagnetic record into three magnetic field generation epochs: an early nebula field (<5Ma after CAI formation), followed by thermal dynamos (5-34 Ma after CAI formation), then a gap in dynamo generation, before the onset of core solidification and compositional dynamos.","The split between these epochs has been defined using thermal evolution and dynamo generation models of planetesimals.","Here we demonstrate these epochs are not as distinct as previously thought based on our refined thermal evolution model that includes more realistic parametrisations for mantle convection, non-eutectic core solidification and radiogenic $^{60}Fe$ in the core.","Inclusion of $^{60}$ in the core brings forward the onset of dynamo generation to 1-2 Ma after CAI formation, which overlaps with the existence of the nebula field.","The second epoch of dynamo generation begins prior to the onset of core solidification, suggesting this epoch is not purely compositionally driven.","Planetesimal radius is the dominant control on dynamo generation, and the choice of reference viscosity can widen the gap between epochs of dynamo generation from 0-200 Ma.","Overall, timings of different planetesimal magnetic field generation mechanisms are more variable.","This alters the information we can glean from the meteorite paleomagnetic record about the early Solar System.","Evidence for the nebula field requires more careful interpretation and young paleomagnetic remanences, for example in the pallasites, may not be evidence for planetesimal core solidification."],"url":"http://arxiv.org/abs/2405.05147v1","category":"astro-ph.EP"}
{"created":"2024-05-08 15:39:38","title":"Hybrid Convolutional Neural Networks with Reliability Guarantee","abstract":"Making AI safe and dependable requires the generation of dependable models and dependable execution of those models. We propose redundant execution as a well-known technique that can be used to ensure reliable execution of the AI model. This generic technique will extend the application scope of AI-accelerators that do not feature well-documented safety or dependability properties. Typical redundancy techniques incur at least double or triple the computational expense of the original. We adopt a co-design approach, integrating reliable model execution with non-reliable execution, focusing that additional computational expense only where it is strictly necessary. We describe the design, implementation and some preliminary results of a hybrid CNN.","sentences":["Making AI safe and dependable requires the generation of dependable models and dependable execution of those models.","We propose redundant execution as a well-known technique that can be used to ensure reliable execution of the AI model.","This generic technique will extend the application scope of AI-accelerators that do not feature well-documented safety or dependability properties.","Typical redundancy techniques incur at least double or triple the computational expense of the original.","We adopt a co-design approach, integrating reliable model execution with non-reliable execution, focusing that additional computational expense only where it is strictly necessary.","We describe the design, implementation and some preliminary results of a hybrid CNN."],"url":"http://arxiv.org/abs/2405.05146v1","category":"cs.AI"}
{"created":"2024-05-08 15:35:34","title":"Multivariate group sequential tests for global summary statistics","abstract":"We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses. We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic. An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints. We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation. In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places. Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model. Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements.","sentences":["We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses.","We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic.","An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints.","We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation.","In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places.","Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model.","Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements."],"url":"http://arxiv.org/abs/2405.05139v1","category":"stat.ME"}
{"created":"2024-05-08 15:25:10","title":"Web Intelligence Journal in perspective: an analysis of its two decades trajectory","abstract":"The evolution of a thematic area undergoes various changes of perspective and adopts new theoretical approaches that arise from the interactions of the community and a wide range of social needs. The advent of digital technologies, such as social networks, underlines this factor by spreading knowledge and forging links between different communities. Web intelligence is now on the verge of raising questions that broaden the understanding of how artificial intelligence impacts the Web of People, Data, and Things, among other factors. To the best of our knowledge, there is no study that has conducted a longitudinal analysis of the evolution of this community. Thus, we investigate in this paper how Web intelligence has evolved in the last twenty years by carrying out a literature review and bibliometric analysis. Concerning the impact of this research study, increasing attention is devoted to determining which are the most influential papers in the community by referring to citation networks and discovering the most popular and pressing topics through a co-citation analysis and the keywords co-occurrence. The results obtained can guide the direction of new research projects in the area and update the scope and places of interest found in current trends and the relevant journals.","sentences":["The evolution of a thematic area undergoes various changes of perspective and adopts new theoretical approaches that arise from the interactions of the community and a wide range of social needs.","The advent of digital technologies, such as social networks, underlines this factor by spreading knowledge and forging links between different communities.","Web intelligence is now on the verge of raising questions that broaden the understanding of how artificial intelligence impacts the Web of People, Data, and Things, among other factors.","To the best of our knowledge, there is no study that has conducted a longitudinal analysis of the evolution of this community.","Thus, we investigate in this paper how Web intelligence has evolved in the last twenty years by carrying out a literature review and bibliometric analysis.","Concerning the impact of this research study, increasing attention is devoted to determining which are the most influential papers in the community by referring to citation networks and discovering the most popular and pressing topics through a co-citation analysis and the keywords co-occurrence.","The results obtained can guide the direction of new research projects in the area and update the scope and places of interest found in current trends and the relevant journals."],"url":"http://arxiv.org/abs/2405.05129v1","category":"cs.SI"}
{"created":"2024-05-08 15:25:05","title":"Degree of the Grassmannian as an affine variety","abstract":"The degree of the Grassmannian with respect to the Pl\\\"ucker embedding is well-known. However, the Pl\\\"ucker embedding, while ubiquitous in pure mathematics, is almost never used in applied mathematics. In applied mathematics, the Grassmannian is usually embedded as projection matrices $\\operatorname{Gr}(k,\\mathbb{R}^n) \\cong \\{P \\in \\mathbb{R}^{n \\times n} : P^{\\scriptscriptstyle\\mathsf{T}} = P = P^2,\\; \\operatorname{tr}(P) = k\\}$ or as involution matrices $\\operatorname{Gr}(k,\\mathbb{R}^n) \\cong \\{X \\in \\mathbb{R}^{n \\times n} : X^{\\scriptscriptstyle\\mathsf{T}} = X,\\; X^2 = I,\\; \\operatorname{tr}(X)=2k - n\\}$. We will determine an explicit expression for the degree of the Grassmannian with respect to these embeddings. In so doing, we resolved a conjecture of Devriendt--Friedman--Sturmfels about the degree $\\operatorname{Gr}(2, \\mathbb{R}^n)$ and in fact generalized it to $\\operatorname{Gr}(k, \\mathbb{R}^n)$. We also proved a set theoretic variant of another conjecture of Devriendt--Friedman--Sturmfels about the limit of $\\operatorname{Gr}(k,\\mathbb{R}^n)$ in the sense of Gr\\\"obner degneration.","sentences":["The degree of the Grassmannian with respect to the Pl\\\"ucker embedding is well-known.","However, the Pl\\\"ucker embedding, while ubiquitous in pure mathematics, is almost never used in applied mathematics.","In applied mathematics, the Grassmannian is usually embedded as projection matrices $\\operatorname{Gr}(k,\\mathbb{R}^n) \\cong \\{P \\in \\mathbb{R}^{n \\times n} : P^{\\scriptscriptstyle\\mathsf{T}} = P = P^2,\\; \\operatorname{tr}(P) = k\\}$ or as involution matrices $\\operatorname{Gr}(k,\\mathbb{R}^n) \\cong \\{X \\in \\mathbb{R}^{n \\times n} : X^{\\scriptscriptstyle\\mathsf{T}} = X,\\; X^2 = I,\\; \\operatorname{tr}(X)=2k - n\\}$. We will determine an explicit expression for the degree of the Grassmannian with respect to these embeddings.","In so doing, we resolved a conjecture of Devriendt--Friedman--Sturmfels about the degree $\\operatorname{Gr}(2, \\mathbb{R}^n)$ and in fact generalized it to $\\operatorname{Gr}(k, \\mathbb{R}^n)$. We also proved a set theoretic variant of another conjecture of Devriendt--Friedman--Sturmfels about the limit of $\\operatorname{Gr}(k,\\mathbb{R}^n)$ in the sense of Gr\\\"obner degneration."],"url":"http://arxiv.org/abs/2405.05128v1","category":"math.AG"}
{"created":"2024-05-08 15:19:50","title":"Knowledge Gaps and Research Needs for Modeling CO2 Mineralization in the Basalt-CO2-Water System: A Review of Laboratory Experiments","abstract":"Carbon capture and storage in basalt is being actively investigated as a scalable climate change mitigation option. Accurate geochemical modeling prediction of the extent and rate of CO2 mineralization is a critical component in assessing the local and global feasibility and efficacy of this strategy. In this study, we review basalt-CO2-water interaction experimental studies conducted during the last two decades to determine whether they provide useable information for geochemical modeling. Most of the cited experiments generate data on the temporal evolution of water composition, and a few provide identification of secondary precipitates and their compositions, offering empirical and semi-quantitative information about the reactivity of basalts and the likelihood of secondary carbonate mineralization at various temperatures, pHs, and pCO2 conditions. However, most experiments provide insufficient information on the properties and quantity of secondary minerals formed, prohibiting accurate mass balance calculations and hence more quantitative geochemical modeling studies. Primary Ca, Mg, and Fe-bearing minerals in basalt control the availability of major ions released into aqueous solution for carbonate precipitation, and many secondary minerals, i.e., smectites, Ca-Mg-Fe carbonates, and zeolites, provide sinks for the same major ions, some of which are difficult to quantify experimentally. Thus, we have a multi-source and multi-sink inverse mass balance problem with insufficient constraints on the bulk system in which the temporal evolution of major ions does not provide sufficient information on which mineral(s) dissolve or the sequence of dissolution and precipitation reactions. Going forward, we propose that future experimental work should focus on trace elements and multiple isotopic tracers and better characterize the solid reaction products with modern analytical instruments.","sentences":["Carbon capture and storage in basalt is being actively investigated as a scalable climate change mitigation option.","Accurate geochemical modeling prediction of the extent and rate of CO2 mineralization is a critical component in assessing the local and global feasibility and efficacy of this strategy.","In this study, we review basalt-CO2-water interaction experimental studies conducted during the last two decades to determine whether they provide useable information for geochemical modeling.","Most of the cited experiments generate data on the temporal evolution of water composition, and a few provide identification of secondary precipitates and their compositions, offering empirical and semi-quantitative information about the reactivity of basalts and the likelihood of secondary carbonate mineralization at various temperatures, pHs, and pCO2 conditions.","However, most experiments provide insufficient information on the properties and quantity of secondary minerals formed, prohibiting accurate mass balance calculations and hence more quantitative geochemical modeling studies.","Primary Ca, Mg, and Fe-bearing minerals in basalt control the availability of major ions released into aqueous solution for carbonate precipitation, and many secondary minerals, i.e., smectites, Ca-Mg-Fe carbonates, and zeolites, provide sinks for the same major ions, some of which are difficult to quantify experimentally.","Thus, we have a multi-source and multi-sink inverse mass balance problem with insufficient constraints on the bulk system in which the temporal evolution of major ions does not provide sufficient information on which mineral(s) dissolve or the sequence of dissolution and precipitation reactions.","Going forward, we propose that future experimental work should focus on trace elements and multiple isotopic tracers and better characterize the solid reaction products with modern analytical instruments."],"url":"http://arxiv.org/abs/2405.05122v1","category":"physics.geo-ph"}
{"created":"2024-05-08 15:16:02","title":"Full Version: (De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms","abstract":"We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.","sentences":["We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\".","Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures.","To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy.","We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts).","Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning."],"url":"http://arxiv.org/abs/2405.05118v1","category":"cs.PL"}
{"created":"2024-05-08 15:05:55","title":"QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs","abstract":"Table summarization is a crucial task aimed at condensing information from tabular data into concise and comprehensible textual summaries. However, existing approaches often fall short of adequately meeting users' information and quality requirements and tend to overlook the complexities of real-world queries. In this paper, we propose a novel method to address these limitations by introducing query-focused multi-table summarization. Our approach, which comprises a table serialization module, a summarization controller, and a large language model (LLM), utilizes textual queries and multiple tables to generate query-dependent table summaries tailored to users' information needs. To facilitate research in this area, we present a comprehensive dataset specifically tailored for this task, consisting of 4909 query-summary pairs, each associated with multiple tables. Through extensive experiments using our curated dataset, we demonstrate the effectiveness of our proposed method compared to baseline approaches. Our findings offer insights into the challenges of complex table reasoning for precise summarization, contributing to the advancement of research in query-focused multi-table summarization.","sentences":["Table summarization is a crucial task aimed at condensing information from tabular data into concise and comprehensible textual summaries.","However, existing approaches often fall short of adequately meeting users' information and quality requirements and tend to overlook the complexities of real-world queries.","In this paper, we propose a novel method to address these limitations by introducing query-focused multi-table summarization.","Our approach, which comprises a table serialization module, a summarization controller, and a large language model (LLM), utilizes textual queries and multiple tables to generate query-dependent table summaries tailored to users' information needs.","To facilitate research in this area, we present a comprehensive dataset specifically tailored for this task, consisting of 4909 query-summary pairs, each associated with multiple tables.","Through extensive experiments using our curated dataset, we demonstrate the effectiveness of our proposed method compared to baseline approaches.","Our findings offer insights into the challenges of complex table reasoning for precise summarization, contributing to the advancement of research in query-focused multi-table summarization."],"url":"http://arxiv.org/abs/2405.05109v1","category":"cs.CL"}
{"created":"2024-05-08 15:02:12","title":"Algebraic symmetries of the observables on the sky: Variable emitters and observers","abstract":"In this paper we prove a number of exact relations between optical observables, such as trigonometric parallax, position drift and the proper motion of a luminous source in addition to the variations of redshift and the viewing angle. These relations are valid in general relativity for any spacetime and they are of potential interest for astrometry and precise cosmology. They generalize the well-known Etherington's reciprocity relation between the angular diameter distance and the luminosity distance. Similar to the Etherington's relation, they hold independently of the spacetime metric, the positions and the motions of a light source or an observer. We show that those relations follow from the symplectic property of the bi-local geodesic operator, i.e., the geometric object that describes the light propagation between two distant regions of a spacetime. The set of relations we present is complete in the sense that no other relations between those observables should hold in general. In the meantime, we develop the mathematical machinery of the bi-local approach to light propagation in general relativity and its corresponding Hamiltonian formalism.","sentences":["In this paper we prove a number of exact relations between optical observables, such as trigonometric parallax, position drift and the proper motion of a luminous source in addition to the variations of redshift and the viewing angle.","These relations are valid in general relativity for any spacetime and they are of potential interest for astrometry and precise cosmology.","They generalize the well-known Etherington's reciprocity relation between the angular diameter distance and the luminosity distance.","Similar to the Etherington's relation, they hold independently of the spacetime metric, the positions and the motions of a light source or an observer.","We show that those relations follow from the symplectic property of the bi-local geodesic operator, i.e., the geometric object that describes the light propagation between two distant regions of a spacetime.","The set of relations we present is complete in the sense that no other relations between those observables should hold in general.","In the meantime, we develop the mathematical machinery of the bi-local approach to light propagation in general relativity and its corresponding Hamiltonian formalism."],"url":"http://arxiv.org/abs/2405.05105v1","category":"gr-qc"}
{"created":"2024-05-08 14:49:27","title":"Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks","abstract":"Popular artificial neural networks (ANN) optimize parameters for unidirectional value propagation, assuming some guessed parametrization type like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In contrast, for biological neurons e.g. \"it is not uncommon for axonal propagation of action potentials to happen in both directions\" \\cite{axon} - suggesting they are optimized to continuously operate in multidirectional way. Additionally, statistical dependencies a single neuron could model is not just (expected) value dependence, but entire joint distributions including also higher moments. Such agnostic joint distribution neuron would allow for multidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or $\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing. There will be discussed Hierarchical Correlation Reconstruction (HCR) for such neuron model: assuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type parametrization of joint distribution with polynomial basis $f_i$, which allows for flexible, inexpensive processing including nonlinearities, direct model estimation and update, trained through standard backpropagation or novel ways for such structure up to tensor decomposition. Using only pairwise (input-output) dependencies, its expected value prediction becomes KAN-like with trained activation functions as polynomials, can be extended by adding higher order dependencies through included products - in conscious interpretable way, allowing for multidirectional propagation of both values and probability densities.","sentences":["Popular artificial neural networks (ANN) optimize parameters for unidirectional value propagation, assuming some guessed parametrization type like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN).","In contrast, for biological neurons e.g. \"it is not uncommon for axonal propagation of action potentials to happen in both directions\" \\cite{axon} - suggesting they are optimized to continuously operate in multidirectional way.","Additionally, statistical dependencies a single neuron could model is not just (expected) value dependence, but entire joint distributions including also higher moments.","Such agnostic joint distribution neuron would allow for multidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or $\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing.","There will be discussed Hierarchical Correlation Reconstruction (HCR) for such neuron model: assuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type parametrization of joint distribution with polynomial basis $f_i$, which allows for flexible, inexpensive processing including nonlinearities, direct model estimation and update, trained through standard backpropagation or novel ways for such structure up to tensor decomposition.","Using only pairwise (input-output) dependencies, its expected value prediction becomes KAN-like with trained activation functions as polynomials, can be extended by adding higher order dependencies through included products - in conscious interpretable way, allowing for multidirectional propagation of both values and probability densities."],"url":"http://arxiv.org/abs/2405.05097v1","category":"cs.LG"}
{"created":"2024-05-08 14:43:26","title":"Mass function of stellar black holes as revealed by the LIGO-Virgo-KAGRA observations","abstract":"Ninety gravitational wave events have been detected by the LIGO-Virgo-KAGRA network and are released in the Gravitational-Wave Transient Catalog. Among these events, 83 cases are definitely binary black hole mergers since the masses of all the objects involved significantly exceed the upper limit of neutron stars. The black holes in these merger events naturally form two interesting samples, a pre-merger sample that includes all the black holes before the mergers and a post-merger sample that consists of the black holes generated during the merging processes. The former represents black holes that once existed in the Universe, while the latter represents newly born black holes. Here we present a statistical analysis on these two samples. The non-parametric $\\tau$ statistic method is adopted to correct for the observational selection effect. The Lynden-Bell's $C^{-}$ method is further applied to derive the mass distribution and density function of black holes. It is found that the mass distribution can be expressed as a broken power-law function. More interestingly, the power-law index in the high mass region is comparable for the two samples. The number density of black holes is found to depend on redshift as $\\rho(z) \\propto z^{-2.06}$-$z^{-2.12}$ based on the two samples. Implications of these findings on the origin of black holes are discussed.","sentences":["Ninety gravitational wave events have been detected by the LIGO-Virgo-KAGRA network and are released in the Gravitational-Wave Transient Catalog.","Among these events, 83 cases are definitely binary black hole mergers since the masses of all the objects involved significantly exceed the upper limit of neutron stars.","The black holes in these merger events naturally form two interesting samples, a pre-merger sample that includes all the black holes before the mergers and a post-merger sample that consists of the black holes generated during the merging processes.","The former represents black holes that once existed in the Universe, while the latter represents newly born black holes.","Here we present a statistical analysis on these two samples.","The non-parametric $\\tau$ statistic method is adopted to correct for the observational selection effect.","The Lynden-Bell's $C^{-}$ method is further applied to derive the mass distribution and density function of black holes.","It is found that the mass distribution can be expressed as a broken power-law function.","More interestingly, the power-law index in the high mass region is comparable for the two samples.","The number density of black holes is found to depend on redshift as $\\rho(z)","\\propto z^{-2.06}$-$z^{-2.12}$ based on the two samples.","Implications of these findings on the origin of black holes are discussed."],"url":"http://arxiv.org/abs/2405.05094v1","category":"gr-qc"}
{"created":"2024-05-08 14:42:35","title":"Understanding solid nitrogen through machine learning simulation","abstract":"We construct a fast, transferable, general purpose, machine-learning interatomic potential suitable for large-scale simulations of $N_2$. The potential is trained only on high quality quantum chemical molecule-molecule interactions, no condensed phase information is used. The potential reproduces the experimental phase diagram including the melt curve and the molecular solid phases of nitrogen up to 10 GPa. This demonstrates that many-molecule interactions are unnecessary to explain the condensed phases of $N_2$. With increased pressure, transitions are observed from cubic ($\\alpha-N_2$), which optimises quadrupole-quadrupole interactions, through tetragonal ($\\gamma-N_2$) which allows more efficient packing, through to monoclinic ($\\lambda-N_2$) which packs still more efficiently. On heating, we obtain the hcp 3D rotor phase ($\\beta-N_2$) and, at pressure, the cubic $\\delta-N_2$ phase which contains both 3D and 2D rotors, tetragonal $\\delta^\\star-N_2$ phase with 2D rotors and the rhombohedral $\\epsilon-N_2$. Molecular dynamics demonstrates where these phases are indeed rotors, rather than frustrated order. The model does not support the existence of the wide range of bondlengths reported for the complex $\\iota-N_2$ phase. The thermodynamic transitions involve both shifts of molecular centres and rotations of molecules. We simulate these phase transitions between finding that the onset of rotation is rapid whereas motion of molecular centres is inhibited and the cause of the observed sluggishness of transitions. Routine density functional theory calculations give a similar picture to the potential.","sentences":["We construct a fast, transferable, general purpose, machine-learning interatomic potential suitable for large-scale simulations of $N_2$. The potential is trained only on high quality quantum chemical molecule-molecule interactions, no condensed phase information is used.","The potential reproduces the experimental phase diagram including the melt curve and the molecular solid phases of nitrogen up to 10 GPa.","This demonstrates that many-molecule interactions are unnecessary to explain the condensed phases of $N_2$. With increased pressure, transitions are observed from cubic ($\\alpha-N_2$), which optimises quadrupole-quadrupole interactions, through tetragonal ($\\gamma-N_2$) which allows more efficient packing, through to monoclinic ($\\lambda-N_2$) which packs still more efficiently.","On heating, we obtain the hcp 3D rotor phase ($\\beta-N_2$) and, at pressure, the cubic $\\delta-N_2$ phase which contains both 3D and 2D rotors, tetragonal $\\delta^\\star-N_2$ phase with 2D rotors and the rhombohedral $\\epsilon-N_2$. Molecular dynamics demonstrates where these phases are indeed rotors, rather than frustrated order.","The model does not support the existence of the wide range of bondlengths reported for the complex $\\iota-N_2$ phase.","The thermodynamic transitions involve both shifts of molecular centres and rotations of molecules.","We simulate these phase transitions between finding that the onset of rotation is rapid whereas motion of molecular centres is inhibited and the cause of the observed sluggishness of transitions.","Routine density functional theory calculations give a similar picture to the potential."],"url":"http://arxiv.org/abs/2405.05092v1","category":"physics.comp-ph"}
{"created":"2024-05-08 14:33:32","title":"Electroweak Multiplets as Dark Matter candidates: A brief review","abstract":"I provide a thorough review of the theoretical and experimental status of ElectroWeak multiplets as Dark Matter candidates, serving as the prototype of Weakly Interacting Massive Particles (WIMPs) Dark Matter. Specifically, the examination includes both real SU(2) representations with zero hypercharge and complex ones with $Y\\neq 0$. For the first time, all calculable thermal masses for scalar and fermionic WIMPs are computed, incorporating significant non-perturbative non-relativistic effects such as Sommerfeld enhancement and the formation of WIMP bound states. WIMP masses of few hundred TeV are shown to be compatible both with $s$-wave unitarity of the annihilation cross-section, and perturbativity. Additionally, a strategy is outlined for probing these scenarios in the next generation of experiments.","sentences":["I provide a thorough review of the theoretical and experimental status of ElectroWeak multiplets as Dark Matter candidates, serving as the prototype of Weakly Interacting Massive Particles (WIMPs) Dark Matter.","Specifically, the examination includes both real SU(2) representations with zero hypercharge and complex ones with $Y\\neq 0$.","For the first time, all calculable thermal masses for scalar and fermionic WIMPs are computed, incorporating significant non-perturbative non-relativistic effects such as Sommerfeld enhancement and the formation of WIMP bound states.","WIMP masses of few hundred TeV are shown to be compatible both with $s$-wave unitarity of the annihilation cross-section, and perturbativity.","Additionally, a strategy is outlined for probing these scenarios in the next generation of experiments."],"url":"http://arxiv.org/abs/2405.05087v1","category":"hep-ph"}
{"created":"2024-05-08 14:29:40","title":"Transition frequencies, isotope shifts, and hyperfine structure in $4s \\rightarrow 4p$ transitions of Ti$^+$ ions","abstract":"We have measured transition frequencies, isotope shifts and hyperfine structure splittings in the $3 d^{2}\\left({ }^{3\\!}F\\right) 4 s\\,{ }^{4} F_J\\rightarrow 3 d^{2}\\left({ }^{3\\!} F\\right) 4 p \\,^{4} G_{J+1}$ transitions in Ti$^+$ ions for $J=\\frac{3}{2},\\, \\frac{5}{2},\\, \\frac{7}{2}$ using collinear laser spectroscopy. Ions were generated by laser ablation in a buffer gas atmosphere and extracted into vacuum through a nozzle and a pair of radiofrequency (RF) funnels. The obtained results are of interest as reference values for on-line measurements of short-lived titanium isotopes and for astrophysical searches for temporal or spatial variations of the fine structure constant $\\alpha$ using quasar absorption spectra.","sentences":["We have measured transition frequencies, isotope shifts and hyperfine structure splittings in the $3 d^{2}\\left({ }^{3\\!}F\\right) 4 s\\,{ }^{4} F_J\\rightarrow 3 d^{2}\\left({ }^{3\\!}","F\\right) 4 p \\,^{4} G_{J+1}$ transitions in Ti$^+$ ions for $J=\\frac{3}{2},\\, \\frac{5}{2},\\, \\frac{7}{2}$ using collinear laser spectroscopy.","Ions were generated by laser ablation in a buffer gas atmosphere and extracted into vacuum through a nozzle and a pair of radiofrequency (RF) funnels.","The obtained results are of interest as reference values for on-line measurements of short-lived titanium isotopes and for astrophysical searches for temporal or spatial variations of the fine structure constant $\\alpha$ using quasar absorption spectra."],"url":"http://arxiv.org/abs/2405.05084v1","category":"physics.atom-ph"}
{"created":"2024-05-08 14:29:39","title":"Committee Elections with Candidate Attribute Constraints","abstract":"In many real-world applications of committee elections, the candidates are associated with certain attributes and the chosen committee is required to satisfy some constraints posed on the candidate attributes. For instance, when dress collocation, it is generally acknowledged that when wearing a tie, you'd better wear a shirt, and wearing a suit, you'd better wear leather shoes. Here, dresses are categorized by upper garment, lower garment, shoes et.al, and upper garment is with the attribute tie and shirt, lower garment is with the attribute suit, and shoes is with the attribute leather. And two constraints \"tie infers shirt\" and \"suit infers leather shoes\" are proposed. We study this variant of committee elections from the computational complexity viewpoint. Given a set of candidates, each with some attributes and a profit, and a set of constraints, given as propositional logical expressions of the attributes, the task is to compute a set of k candidates, whose attributes satisfy all constraints and whose total profit achieves a given bound. We achieve a dichotomy concerning classical complexity with no length limit on constraints: the problem is polynomial-time solvable, if the following two conditions are fulfilled: 1) each candidate has only one attribute and 2) each attribute occurs at most once in the constraints. It becomes NP-hard if one of the two conditions is violated. Moreover, we examine its parameterized complexity. The parameterization with the number of constraints, the size of the committee, or the total profit bound as parameter leads to para-NP-hardness or W[1]-hardness, while with the number of attributes or the number of candidates as parameter, the problem turns out to be fixed-parameter tractable.","sentences":["In many real-world applications of committee elections, the candidates are associated with certain attributes and the chosen committee is required to satisfy some constraints posed on the candidate attributes.","For instance, when dress collocation, it is generally acknowledged that when wearing a tie, you'd better wear a shirt, and wearing a suit, you'd better wear leather shoes.","Here, dresses are categorized by upper garment, lower garment, shoes et.al, and upper garment is with the attribute tie and shirt, lower garment is with the attribute suit, and shoes is with the attribute leather.","And two constraints \"tie infers shirt\" and \"suit infers leather shoes\" are proposed.","We study this variant of committee elections from the computational complexity viewpoint.","Given a set of candidates, each with some attributes and a profit, and a set of constraints, given as propositional logical expressions of the attributes, the task is to compute a set of k candidates, whose attributes satisfy all constraints and whose total profit achieves a given bound.","We achieve a dichotomy concerning classical complexity with no length limit on constraints: the problem is polynomial-time solvable, if the following two conditions are fulfilled: 1) each candidate has only one attribute and 2) each attribute occurs at most once in the constraints.","It becomes NP-hard if one of the two conditions is violated.","Moreover, we examine its parameterized complexity.","The parameterization with the number of constraints, the size of the committee, or the total profit bound as parameter leads to para-NP-hardness or W[1]-hardness, while with the number of attributes or the number of candidates as parameter, the problem turns out to be fixed-parameter tractable."],"url":"http://arxiv.org/abs/2405.05083v1","category":"cs.CC"}
{"created":"2024-05-08 14:29:31","title":"On linear-combinatorial problems associated with subspaces spanned by $\\{\\pm 1\\}$-vectors","abstract":"A complete answer to the question about subspaces generated by $\\{\\pm 1\\}$-vectors, which arose in the work of I.Kanter and H.Sompolinsky on associative memories, is given. More precisely, let vectors $v_1, \\ldots , v_p,$ $p\\leq n-1,$ be chosen at random uniformly and independently from $\\{\\pm 1\\}^n \\subset {\\bf R}^n.$ Then the probability ${\\mathbb P}(p, n)$ that $$span \\ \\langle v_1, \\ldots , v_p \\rangle \\cap \\left\\{ \\{\\pm 1\\}^n \\setminus \\{\\pm v_1, \\ldots , \\pm v_p\\}\\right\\} \\ne \\emptyset \\ $$ is shown to be $$4{p \\choose 3}\\left(\\frac{3}{4}\\right)^n + O\\left(\\left(\\frac{5}{8} + o_n(1)\\right)^n\\right) \\quad \\mbox{as} \\quad n\\to \\infty,$$ where the constant implied by the $O$-notation does not depend on $p$. The main term in this estimate is the probability that some 3 vectors $v_{j_1}, v_{j_2}, v_{j_3}$ of $v_j$, $j= 1, \\ldots , p,$ have a linear combination that is a $\\{\\pm 1\\}$-vector different from $\\pm v_{j_1}, \\pm v_{j_2}, \\pm v_{j_3}. $","sentences":["A complete answer to the question about subspaces generated by $\\{\\pm 1\\}$-vectors, which arose in the work of I.Kanter and H.Sompolinsky on associative memories, is given.","More precisely, let vectors $v_1, \\ldots , v_p,$ $p\\leq n-1,$ be chosen at random uniformly and independently from $\\{\\pm 1\\}^n \\subset {\\bf R}^n.$","Then the probability ${\\mathbb P}(p, n)$ that $$span \\ \\langle v_1, \\ldots , v_p","\\rangle \\cap \\left\\{ \\{\\pm 1\\}^n \\setminus \\{\\pm v_1, \\ldots , \\pm v_p\\}\\right\\} \\ne \\emptyset \\ $$ is shown to be $$4{p \\choose 3}\\left(\\frac{3}{4}\\right)^n + O\\left(\\left(\\frac{5}{8} + o_n(1)\\right)^n\\right)","\\quad \\mbox{as} \\quad n\\to \\infty,$$ where the constant implied by the $O$-notation does not depend on $p$. The main term in this estimate is the probability that some 3 vectors $v_{j_1}, v_{j_2}, v_{j_3}$ of $v_j$, $j= 1, \\ldots , p,$ have a linear combination that is a $\\{\\pm 1\\}$-vector different from $\\pm v_{j_1}, \\pm v_{j_2}, \\pm v_{j_3}.","$"],"url":"http://arxiv.org/abs/2405.05082v1","category":"math.CO"}
{"created":"2024-05-08 14:24:11","title":"Concerns on Bias in Large Language Models when Creating Synthetic Personae","abstract":"This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs). These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them. The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research.","sentences":["This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs).","These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them.","The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research."],"url":"http://arxiv.org/abs/2405.05080v1","category":"cs.HC"}
{"created":"2024-05-08 14:22:39","title":"Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment","abstract":"Initialization-free bundle adjustment (BA) remains largely uncharted. While Levenberg-Marquardt algorithm is the golden method to solve the BA problem, it generally relies on a good initialization. In contrast, the under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization. Coupled with object space error formulation, recent works have shown its ability to solve (small-scale) initialization-free bundle adjustment problem. We introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series. Importantly, we link the power series expansion to Riemannian manifold optimization. This projective framework is crucial to solve large-scale bundle adjustment problem without initialization. Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy. In particular, our work is the first, to our knowledge, that addresses the scalability of BA without initialization and opens new venues for initialization-free Structure-from-Motion.","sentences":["Initialization-free bundle adjustment (BA) remains largely uncharted.","While Levenberg-Marquardt algorithm is the golden method to solve the BA problem, it generally relies on a good initialization.","In contrast, the under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization.","Coupled with object space error formulation, recent works have shown its ability to solve (small-scale) initialization-free bundle adjustment problem.","We introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series.","Importantly, we link the power series expansion to Riemannian manifold optimization.","This projective framework is crucial to solve large-scale bundle adjustment problem without initialization.","Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy.","In particular, our work is the first, to our knowledge, that addresses the scalability of BA without initialization and opens new venues for initialization-free Structure-from-Motion."],"url":"http://arxiv.org/abs/2405.05079v1","category":"cs.CV"}
{"created":"2024-05-08 14:21:46","title":"Analyzing the Influence of Geometrical Deformation on Photon Sphere and Shadow Radius: A New Analytical Approach -Stationary, and Axisymmetric Spacetime","abstract":"Black hole shadows and photon spheres offer valuable tools for investigating black hole properties. Recent observations by the Event Horizon Telescope Collaboration have confirmed the existence of rotating black holes. Black hole parameters influence the observed shadow size. This paper explores the impact of geometrical deformations on black hole shadow size using gravitational decoupling applied to axially-symmetric spacetime. We find the results are more complex than the spherically-symmetric case. We compare shadows in well-known models with those of an Kerr black hole. Our approach suggests that the influence of an accretion disc on the observed shadow shape can be accurately described despite negligible impact on the black hole geometry itself.","sentences":["Black hole shadows and photon spheres offer valuable tools for investigating black hole properties.","Recent observations by the Event Horizon Telescope Collaboration have confirmed the existence of rotating black holes.","Black hole parameters influence the observed shadow size.","This paper explores the impact of geometrical deformations on black hole shadow size using gravitational decoupling applied to axially-symmetric spacetime.","We find the results are more complex than the spherically-symmetric case.","We compare shadows in well-known models with those of an Kerr black hole.","Our approach suggests that the influence of an accretion disc on the observed shadow shape can be accurately described despite negligible impact on the black hole geometry itself."],"url":"http://arxiv.org/abs/2405.05077v1","category":"gr-qc"}
{"created":"2024-05-08 14:18:13","title":"Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations","abstract":"This work studies sparse adversarial perturbations bounded by $l_0$ norm. We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations. Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks. Codes are available at https://github.com/CityU-MLO/sPGD.","sentences":["This work studies sparse adversarial perturbations bounded by $l_0$ norm.","We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations.","Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations.","Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations.","Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios.","More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks.","Codes are available at https://github.com/CityU-MLO/sPGD."],"url":"http://arxiv.org/abs/2405.05075v1","category":"cs.LG"}
{"created":"2024-05-08 14:14:59","title":"gasmodel: An R Package for Generalized Autoregressive Score Models","abstract":"Generalized autoregressive score (GAS) models are a class of observation-driven time series models that employ the score to dynamically update time-varying parameters of the underlying probability distribution. GAS models have been extensively studied and numerous variants have been proposed in the literature to accommodate diverse data types and probability distributions. This paper introduces the gasmodel package, which has been designed to facilitate the estimation, forecasting, and simulation of a wide range of GAS models. The package provides a rich selection of distributions, offers flexible options for specifying dynamics, and allows to incorporate exogenous variables. Model estimation utilizes the maximum likelihood method.","sentences":["Generalized autoregressive score (GAS) models are a class of observation-driven time series models that employ the score to dynamically update time-varying parameters of the underlying probability distribution.","GAS models have been extensively studied and numerous variants have been proposed in the literature to accommodate diverse data types and probability distributions.","This paper introduces the gasmodel package, which has been designed to facilitate the estimation, forecasting, and simulation of a wide range of GAS models.","The package provides a rich selection of distributions, offers flexible options for specifying dynamics, and allows to incorporate exogenous variables.","Model estimation utilizes the maximum likelihood method."],"url":"http://arxiv.org/abs/2405.05073v1","category":"stat.CO"}
{"created":"2024-05-08 14:14:03","title":"Novel Actor-Critic Algorithm for Robust Decision Making of CAV under Delays and Loss of V2X Data","abstract":"Current autonomous driving systems heavily rely on V2X communication data to enhance situational awareness and the cooperation between vehicles. However, a major challenge when using V2X data is that it may not be available periodically because of unpredictable delays and data loss during wireless transmission between road stations and the receiver vehicle. This issue should be considered when designing control strategies for connected and autonomous vehicles. Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm that guarantees robust driving performance in V2X environment with delayed and/or lost data. The novel algorithm incorporates three key mechanisms: a virtual fixed sampling period, a combination of Temporal-Difference and Monte Carlo learning, and a numerical approximation of immediate reward values. To address the temporal aperiodicity problem of V2X data, we first illustrate this challenge. Then, we provide a detailed explanation of the Blind Actor-Critic algorithm where we highlight the proposed components to compensate for the temporal aperiodicity problem of V2X data. We evaluate the performance of our algorithm in a simulation environment and compare it to benchmark approaches. The results demonstrate that training metrics are improved compared to conventional actor-critic algorithms. Additionally, testing results show that our approach provides robust control, even under low V2X network reliability levels.","sentences":["Current autonomous driving systems heavily rely on V2X communication data to enhance situational awareness and the cooperation between vehicles.","However, a major challenge when using V2X data is that it may not be available periodically because of unpredictable delays and data loss during wireless transmission between road stations and the receiver vehicle.","This issue should be considered when designing control strategies for connected and autonomous vehicles.","Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm that guarantees robust driving performance in V2X environment with delayed and/or lost data.","The novel algorithm incorporates three key mechanisms: a virtual fixed sampling period, a combination of Temporal-Difference and Monte Carlo learning, and a numerical approximation of immediate reward values.","To address the temporal aperiodicity problem of V2X data, we first illustrate this challenge.","Then, we provide a detailed explanation of the Blind Actor-Critic algorithm where we highlight the proposed components to compensate for the temporal aperiodicity problem of V2X data.","We evaluate the performance of our algorithm in a simulation environment and compare it to benchmark approaches.","The results demonstrate that training metrics are improved compared to conventional actor-critic algorithms.","Additionally, testing results show that our approach provides robust control, even under low V2X network reliability levels."],"url":"http://arxiv.org/abs/2405.05072v1","category":"cs.LG"}
{"created":"2024-05-08 14:11:01","title":"Equivalence analysis between Quasi-coarse-grained and Atomistic Simulations","abstract":"In recent years, simulation methods based on the scaling of atomic potential functions, such as quasi-coarse-grained dynamics and coarse-grained dynamics, have shown promising results for modeling crystalline systems at multiple scales. However, this letter presents evidence suggesting that the spatiotemporal trajectories of coarse-grained systems generated by such simulation methods exhibit a complete correspondence with those of specific molecular dynamics systems. In essence, current coarse-grained simulation methods involve a direct amplification of the results obtained from molecular dynamics simulations across spatial and temporal scales, yet they may lack the capability to adequately capture authentic scale effects. Consequently, the findings of related studies warrant careful re-evaluation. Furthermore, this study underscores the importance of not only verifying the consistency of mesoscale simulation methods with microscopic simulations but also meticulously assessing their capability to accurately forecast mesoscale physical phenomena.","sentences":["In recent years, simulation methods based on the scaling of atomic potential functions, such as quasi-coarse-grained dynamics and coarse-grained dynamics, have shown promising results for modeling crystalline systems at multiple scales.","However, this letter presents evidence suggesting that the spatiotemporal trajectories of coarse-grained systems generated by such simulation methods exhibit a complete correspondence with those of specific molecular dynamics systems.","In essence, current coarse-grained simulation methods involve a direct amplification of the results obtained from molecular dynamics simulations across spatial and temporal scales, yet they may lack the capability to adequately capture authentic scale effects.","Consequently, the findings of related studies warrant careful re-evaluation.","Furthermore, this study underscores the importance of not only verifying the consistency of mesoscale simulation methods with microscopic simulations but also meticulously assessing their capability to accurately forecast mesoscale physical phenomena."],"url":"http://arxiv.org/abs/2405.05070v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 14:07:43","title":"Computing Chebyshev polynomials using the complex Remez algorithm","abstract":"We employ the generalized Remez algorithm, initially suggested by P. T. P. Tang, to perform an experimental study of Chebyshev polynomials in the complex plane. Our focus lies particularly on the examination of their norms and zeros. What sets our study apart is the breadth of examples considered, coupled with the fact that the degrees under investigation are substantially higher than those in previous studies where other methods have been applied. These computations of Chebyshev polynomials of high degrees reveal discernible patterns which allow for conjectures to be formulated based on abundant experimental evidence. The use of Tang's algorithm allows for computations executed with precision, maintaining accuracy within quantifiable margins of error. Additionally, as a result of our experimental study, we propose what we believe to be a fundamental relationship between Chebyshev and Faber polynomials associated with a compact set.","sentences":["We employ the generalized Remez algorithm, initially suggested by P. T. P. Tang, to perform an experimental study of Chebyshev polynomials in the complex plane.","Our focus lies particularly on the examination of their norms and zeros.","What sets our study apart is the breadth of examples considered, coupled with the fact that the degrees under investigation are substantially higher than those in previous studies where other methods have been applied.","These computations of Chebyshev polynomials of high degrees reveal discernible patterns which allow for conjectures to be formulated based on abundant experimental evidence.","The use of Tang's algorithm allows for computations executed with precision, maintaining accuracy within quantifiable margins of error.","Additionally, as a result of our experimental study, we propose what we believe to be a fundamental relationship between Chebyshev and Faber polynomials associated with a compact set."],"url":"http://arxiv.org/abs/2405.05067v1","category":"math.CV"}
{"created":"2024-05-08 14:04:35","title":"Designing Skill-Compatible AI: Methodologies and Frameworks in Chess","abstract":"Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility.","sentences":["Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power.","For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts.","We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities.","Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents.","We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners.","On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance.","Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility."],"url":"http://arxiv.org/abs/2405.05066v1","category":"cs.AI"}
{"created":"2024-05-08 13:55:52","title":"Impact of Tone-Aware Explanations in Recommender Systems","abstract":"In recommender systems, the presentation of explanations plays a crucial role in supporting users' decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience.","sentences":["In recommender systems, the presentation of explanations plays a crucial role in supporting users' decision-making processes.","Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked.","Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication.","However, studies on the impact of tone on explanations within the context of recommender systems are insufficient.","Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes.","We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products.","Collected data analysis reveals different perceived effects of tones depending on the domains.","Moreover, user attributes such as age and personality traits are found to influence the impact of tone.","This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience."],"url":"http://arxiv.org/abs/2405.05061v1","category":"cs.HC"}
{"created":"2024-05-08 13:55:25","title":"Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models","abstract":"Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems. In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals. The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model. We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model's output as synthetic labels for fine-tuning a large language model for the same task. Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design.","sentences":["Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems.","In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals.","The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model.","We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model's output as synthetic labels for fine-tuning a large language model for the same task.","Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design."],"url":"http://arxiv.org/abs/2405.05060v1","category":"cs.CL"}
{"created":"2024-05-08 13:40:59","title":"On the Euler characteristic of $S$-arithmetic groups","abstract":"We show that the sign of the Euler characteristic of an $S$-arithmetic subgroup of a simple $k$-group over a number field $k$ depends on the $S$-congruence completion only. Consequently, the sign is a profinite invariant for such $S$-arithmetic groups with the congruence subgroup property. This generalizes previous work of the first author with Kionke-Raimbault-Sauer.","sentences":["We show that the sign of the Euler characteristic of an $S$-arithmetic subgroup of a simple $k$-group over a number field $k$ depends on the $S$-congruence completion only.","Consequently, the sign is a profinite invariant for such $S$-arithmetic groups with the congruence subgroup property.","This generalizes previous work of the first author with Kionke-Raimbault-Sauer."],"url":"http://arxiv.org/abs/2405.05050v1","category":"math.GR"}
{"created":"2024-05-08 13:33:29","title":"High-mass star formation in the Large Magellanic Cloud triggered by colliding HI flows","abstract":"The galactic tidal interaction is a possible mechanism to trigger the active star formation in galaxies. The recent analyses using the HI data in the Large Magellanic Cloud (LMC) proposed that the tidally driven HI flow, the L-component, is colliding with the LMC disk, the D-component, and is triggering high-mass star formation toward the active star-forming regions R136 and N44. In order to explore the role of the collision over the entire LMC disk, we investigated the I-component, the collision-compressed gas between the L- and D-components, over the LMC disk, and found that 74% of the O/WR stars are located toward the I-component, suggesting their formation in the colliding gas. We compared four star-forming regions (R136, N44, N11, N77-N79-N83 complex). We found a positive correlation between the number of high-mass stars and the compressed gas pressure generated by collisions, suggesting that the pressure may be a key parameter in star formation.","sentences":["The galactic tidal interaction is a possible mechanism to trigger the active star formation in galaxies.","The recent analyses using the HI data in the Large Magellanic Cloud (LMC) proposed that the tidally driven HI flow, the L-component, is colliding with the LMC disk, the D-component, and is triggering high-mass star formation toward the active star-forming regions R136 and N44.","In order to explore the role of the collision over the entire LMC disk, we investigated the I-component, the collision-compressed gas between the L- and D-components, over the LMC disk, and found that 74% of the O/WR stars are located toward the I-component, suggesting their formation in the colliding gas.","We compared four star-forming regions (R136, N44, N11, N77-N79-N83 complex).","We found a positive correlation between the number of high-mass stars and the compressed gas pressure generated by collisions, suggesting that the pressure may be a key parameter in star formation."],"url":"http://arxiv.org/abs/2405.05046v1","category":"astro-ph.GA"}
{"created":"2024-05-08 13:28:01","title":"Photon Squeezing in Photonic Time Crystals","abstract":"Time-varying media offer a platform to realize novel and exotic wave effects, including photonic time crystals characterized by momentum band gaps with exponential wave amplification. Here we focus on the quantum electrodynamical properties of time-varying media, in particular vacuum amplification and squeezing. For that purpose, we present a theory of photon pair generation in photonic time crystals that unveils the link between the classical and quantum electrodynamical properties of these systems, that is, a direct relation between reflectivity and pair generation through the squeezing parameter. By working within an Hermitian framework, we are able to characterize quantum pair generation processes in photonic time crystals, showing how momentum bandgaps result in a non-resonant exponential enhancement of dynamical Casimir processes.","sentences":["Time-varying media offer a platform to realize novel and exotic wave effects, including photonic time crystals characterized by momentum band gaps with exponential wave amplification.","Here we focus on the quantum electrodynamical properties of time-varying media, in particular vacuum amplification and squeezing.","For that purpose, we present a theory of photon pair generation in photonic time crystals that unveils the link between the classical and quantum electrodynamical properties of these systems, that is, a direct relation between reflectivity and pair generation through the squeezing parameter.","By working within an Hermitian framework, we are able to characterize quantum pair generation processes in photonic time crystals, showing how momentum bandgaps result in a non-resonant exponential enhancement of dynamical Casimir processes."],"url":"http://arxiv.org/abs/2405.05043v1","category":"physics.optics"}
{"created":"2024-05-08 13:14:04","title":"Gr\u00f6bner Basis Cryptanalysis of Ciminion and Hydra","abstract":"Ciminion and Hydra are two recently introduced symmetric key Pseudo-Random Functions for Multi-Party Computation applications. For efficiency both primitives utilize quadratic permutations at round level. Therefore, polynomial system solving-based attacks pose a serious threat to these primitives. For Ciminion we construct a quadratic degree reverse lexicographic (DRL) Gr\\\"obner basis for the iterated polynomial model via affine transformations. For Hydra we provide a computer-aided proof in SageMath that a quadratic DRL Gr\\\"obner basis is already contained within the iterated polynomial system for the Hydra heads after affine transformations and a linear change of coordinates.   Our Ciminion DRL Gr\\\"obner basis simplifies cryptanalysis, since one does not need to impose genericity assumptions, like being regular or semi-regular, anymore to derive complexity estimates on key recovery attacks.   In the Hydra proposal it was claimed that $r_\\mathcal{H} = 31$ rounds for the heads are sufficient to achieve $128$ bits of security against Gr\\\"obner basis attacks for key recovery. However, for $r_\\mathcal{H} = 31$ standard term order conversion to a lexicographic (LEX) Gr\\\"obner basis for our Hydra DRL Gr\\\"obner basis requires just $126$ bits. Moreover, via the Eigenvalue Method up to $r_\\mathcal{H} = 33$ rounds can be attacked below $128$ bits.","sentences":["Ciminion and Hydra are two recently introduced symmetric key Pseudo-Random Functions for Multi-Party Computation applications.","For efficiency both primitives utilize quadratic permutations at round level.","Therefore, polynomial system solving-based attacks pose a serious threat to these primitives.","For Ciminion we construct a quadratic degree reverse lexicographic (DRL) Gr\\\"obner basis for the iterated polynomial model via affine transformations.","For Hydra we provide a computer-aided proof in SageMath that a quadratic DRL Gr\\\"obner basis is already contained within the iterated polynomial system for the Hydra heads after affine transformations and a linear change of coordinates.   ","Our Ciminion DRL Gr\\\"obner basis simplifies cryptanalysis, since one does not need to impose genericity assumptions, like being regular or semi-regular, anymore to derive complexity estimates on key recovery attacks.   ","In the Hydra proposal it was claimed that $r_\\mathcal{H} = 31$ rounds for the heads are sufficient to achieve $128$ bits of security against Gr\\\"obner basis attacks for key recovery.","However, for $r_\\mathcal{H} = 31$ standard term order conversion to a lexicographic (LEX) Gr\\\"obner basis for our Hydra DRL Gr\\\"obner basis requires just $126$ bits.","Moreover, via the Eigenvalue Method up to $r_\\mathcal{H} = 33$ rounds can be attacked below $128$ bits."],"url":"http://arxiv.org/abs/2405.05040v1","category":"cs.CR"}
{"created":"2024-05-08 13:13:02","title":"Reviewing Intelligent Cinematography: AI research for camera-based video production","abstract":"This paper offers a comprehensive review of artificial intelligence (AI) research in the context of real camera content acquisition for entertainment purposes and is aimed at both researchers and cinematographers. Considering the breadth of computer vision research and the lack of review papers tied to intelligent cinematography (IC), this review introduces a holistic view of the IC landscape while providing the technical insight for experts across across disciplines. We preface the main discussion with technical background on generative AI, object detection, automated camera calibration and 3-D content acquisition, and link explanatory articles to assist non-technical readers. The main discussion categorizes work by four production types: General Production, Virtual Production, Live Production and Aerial Production. Note that for Virtual Production we do not discuss research relating to virtual content acquisition, including work on automated video generation, like Stable Diffusion. Within each section, we (1) sub-classify work by the technical field of research - reflected by the subsections, and (2) evaluate the trends and challenge w.r.t to each type of production. In the final chapter, we present our concluding remarks on the greater scope of IC research and outline work that we believe has significant potential to influence the whole industry. We find that work relating to virtual production has the greatest potential to impact other mediums of production, driven by the growing interest in LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture for a virtual modelling of real world scenes and actors. This is the first piece of literature to offer a structured and comprehensive examination of IC research. Consequently, we address ethical and legal concerns regarding the use of creative AI involving artists, actors and the general public, in the...","sentences":["This paper offers a comprehensive review of artificial intelligence (AI) research in the context of real camera content acquisition for entertainment purposes and is aimed at both researchers and cinematographers.","Considering the breadth of computer vision research and the lack of review papers tied to intelligent cinematography (IC), this review introduces a holistic view of the IC landscape while providing the technical insight for experts across across disciplines.","We preface the main discussion with technical background on generative AI, object detection, automated camera calibration and 3-D content acquisition, and link explanatory articles to assist non-technical readers.","The main discussion categorizes work by four production types: General Production, Virtual Production, Live Production and Aerial Production.","Note that for Virtual Production we do not discuss research relating to virtual content acquisition, including work on automated video generation, like Stable Diffusion.","Within each section, we (1) sub-classify work by the technical field of research - reflected by the subsections, and (2) evaluate the trends and challenge w.r.t to each type of production.","In the final chapter, we present our concluding remarks on the greater scope of IC research and outline work that we believe has significant potential to influence the whole industry.","We find that work relating to virtual production has the greatest potential to impact other mediums of production, driven by the growing interest in LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture for a virtual modelling of real world scenes and actors.","This is the first piece of literature to offer a structured and comprehensive examination of IC research.","Consequently, we address ethical and legal concerns regarding the use of creative AI involving artists, actors and the general public, in the..."],"url":"http://arxiv.org/abs/2405.05039v1","category":"cs.CV"}
{"created":"2024-05-08 13:11:02","title":"Bounds on the charge of the graviton using gravitational wave observations","abstract":"If the graviton possesses a non-zero charge $q_g$, gravitational waves (GW) originating from astrophysical sources would experience an additional time delay due to intergalactic magnetic fields. This would result in a modification of the phase evolution of the observed GW signal similar to the effect induced by a massive graviton. As a result, we can reinterpret the most recent upper limits on the graviton's mass as constraints on the joint mass-charge parameter space, finding $|q_g|/{e} < 3\\times 10^{-34}$ where $e$ represents the charge of an electron. Additionally, we illustrate that a charged graviton would introduce a constant phase difference in the gravitational waves detected by two spatially separated GW detectors due to the Aharonov-Bohm effect. Using the non-observation of such a phase difference for the GW event GW190814, we establish a mass-independent constraint $|q_g|/e < 2\\times 10^{-26}$. To the best of our knowledge, our results constitute the first-ever bounds on the charge of the graviton. We also discuss various caveats involved in our measurements and prospects for strengthening these bounds with future GW observations.","sentences":["If the graviton possesses a non-zero charge $q_g$, gravitational waves (GW) originating from astrophysical sources would experience an additional time delay due to intergalactic magnetic fields.","This would result in a modification of the phase evolution of the observed GW signal similar to the effect induced by a massive graviton.","As a result, we can reinterpret the most recent upper limits on the graviton's mass as constraints on the joint mass-charge parameter space, finding $|q_g|/{e} < 3\\times 10^{-34}$ where $e$ represents the charge of an electron.","Additionally, we illustrate that a charged graviton would introduce a constant phase difference in the gravitational waves detected by two spatially separated GW detectors due to the Aharonov-Bohm effect.","Using the non-observation of such a phase difference for the GW event GW190814, we establish a mass-independent constraint $|q_g|/e < 2\\times 10^{-26}$. To the best of our knowledge, our results constitute the first-ever bounds on the charge of the graviton.","We also discuss various caveats involved in our measurements and prospects for strengthening these bounds with future GW observations."],"url":"http://arxiv.org/abs/2405.05038v1","category":"gr-qc"}
{"created":"2024-05-08 13:03:55","title":"Multi-fidelity Hamiltonian Monte Carlo","abstract":"Numerous applications in biology, statistics, science, and engineering require generating samples from high-dimensional probability distributions. In recent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a state-of-the-art Markov chain Monte Carlo technique, exploiting the shape of such high-dimensional target distributions to efficiently generate samples. Despite its impressive empirical success and increasing popularity, its wide-scale adoption remains limited due to the high computational cost of gradient calculation. Moreover, applying this method is impossible when the gradient of the posterior cannot be computed (for example, with black-box simulators). To overcome these challenges, we propose a novel two-stage Hamiltonian Monte Carlo algorithm with a surrogate model. In this multi-fidelity algorithm, the acceptance probability is computed in the first stage via a standard HMC proposal using an inexpensive differentiable surrogate model, and if the proposal is accepted, the posterior is evaluated in the second stage using the high-fidelity (HF) numerical solver. Splitting the standard HMC algorithm into these two stages allows for approximating the gradient of the posterior efficiently, while producing accurate posterior samples by using HF numerical solvers in the second stage. We demonstrate the effectiveness of this algorithm for a range of problems, including linear and nonlinear Bayesian inverse problems with in-silico data and experimental data. The proposed algorithm is shown to seamlessly integrate with various low-fidelity and HF models, priors, and datasets. Remarkably, our proposed method outperforms the traditional HMC algorithm in both computational and statistical efficiency by several orders of magnitude, all while retaining or improving the accuracy in computed posterior statistics.","sentences":["Numerous applications in biology, statistics, science, and engineering require generating samples from high-dimensional probability distributions.","In recent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a state-of-the-art Markov chain Monte Carlo technique, exploiting the shape of such high-dimensional target distributions to efficiently generate samples.","Despite its impressive empirical success and increasing popularity, its wide-scale adoption remains limited due to the high computational cost of gradient calculation.","Moreover, applying this method is impossible when the gradient of the posterior cannot be computed (for example, with black-box simulators).","To overcome these challenges, we propose a novel two-stage Hamiltonian Monte Carlo algorithm with a surrogate model.","In this multi-fidelity algorithm, the acceptance probability is computed in the first stage via a standard HMC proposal using an inexpensive differentiable surrogate model, and if the proposal is accepted, the posterior is evaluated in the second stage using the high-fidelity (HF) numerical solver.","Splitting the standard HMC algorithm into these two stages allows for approximating the gradient of the posterior efficiently, while producing accurate posterior samples by using HF numerical solvers in the second stage.","We demonstrate the effectiveness of this algorithm for a range of problems, including linear and nonlinear Bayesian inverse problems with in-silico data and experimental data.","The proposed algorithm is shown to seamlessly integrate with various low-fidelity and HF models, priors, and datasets.","Remarkably, our proposed method outperforms the traditional HMC algorithm in both computational and statistical efficiency by several orders of magnitude, all while retaining or improving the accuracy in computed posterior statistics."],"url":"http://arxiv.org/abs/2405.05033v1","category":"cs.CE"}
{"created":"2024-05-08 12:59:08","title":"Functional Specifications and Testing Requirements of Grid-Forming Type-IV Offshore Wind Power","abstract":"Throughout the past few years, various transmission system operators (TSOs) and research institutes have defined several functional specifications for grid-forming (GFM) converters via grid codes, white papers, and technical documents. These institutes and organisations also proposed testing requirements for general inverter-based resources (IBRs) and specific GFM converters. This paper initially reviews functional specifications and testing requirements from several sources to create an understanding of GFM capabilities in general. Furthermore, it proposes an outlook of the defined GFM capabilities, functional specifications, and testing requirements for offshore wind power plant (OF WPP) applications from an original equipment manufacturer (OEM) perspective. Finally, this paper briefly establishes the relevance of new testing methodologies for equipment-level certification and model validation, focusing on GFM functional specifications.","sentences":["Throughout the past few years, various transmission system operators (TSOs) and research institutes have defined several functional specifications for grid-forming (GFM) converters via grid codes, white papers, and technical documents.","These institutes and organisations also proposed testing requirements for general inverter-based resources (IBRs) and specific GFM converters.","This paper initially reviews functional specifications and testing requirements from several sources to create an understanding of GFM capabilities in general.","Furthermore, it proposes an outlook of the defined GFM capabilities, functional specifications, and testing requirements for offshore wind power plant (OF WPP) applications from an original equipment manufacturer (OEM) perspective.","Finally, this paper briefly establishes the relevance of new testing methodologies for equipment-level certification and model validation, focusing on GFM functional specifications."],"url":"http://arxiv.org/abs/2405.05030v1","category":"eess.SY"}
{"created":"2024-05-08 12:57:53","title":"StyleMamba : State Space Model for Efficient Text-driven Image Style Transfer","abstract":"We present StyleMamba, an efficient image style transfer framework that translates text prompts into corresponding visual styles while preserving the content integrity of the original images. Existing text-guided stylization requires hundreds of training iterations and takes a lot of computing resources. To speed up the process, we propose a conditional State Space Model for Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that sequentially aligns the image features to the target text prompts. To enhance the local and global style consistency between text and image, we propose masked and second-order directional losses to optimize the stylization direction to significantly reduce the training iterations by 5 times and the inference time by 3 times. Extensive experiments and qualitative evaluation confirm the robust and superior stylization performance of our methods compared to the existing baselines.","sentences":["We present StyleMamba, an efficient image style transfer framework that translates text prompts into corresponding visual styles while preserving the content integrity of the original images.","Existing text-guided stylization requires hundreds of training iterations and takes a lot of computing resources.","To speed up the process, we propose a conditional State Space Model for Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that sequentially aligns the image features to the target text prompts.","To enhance the local and global style consistency between text and image, we propose masked and second-order directional losses to optimize the stylization direction to significantly reduce the training iterations by 5 times and the inference time by 3 times.","Extensive experiments and qualitative evaluation confirm the robust and superior stylization performance of our methods compared to the existing baselines."],"url":"http://arxiv.org/abs/2405.05027v1","category":"cs.CV"}
{"created":"2024-05-08 12:57:46","title":"An anti-noise seismic inversion method based on diffusion model","abstract":"Seismic impedance inversion is one of the most important part of geophysical exploration. However, due to random noise, the traditional semi-supervised learning (SSL) methods lack generalization and stability. To solve this problem, some authors have proposed SSL methods with anti-noise function to improve noise robustness and inversion accuracy. However, such methods are often not ideal when faced with strong noise. In addition, Low-frequency impedance models can mitigate this problem, but creating accurate low-frequency models is difficult and error-prone when well-log data is sparse and subsurface structures are complex. To address those issues, we propose a novel deep learning inversion method called DSIM-USSL (Unsupervised and Semi-supervised joint Learning for Seismic Inversion based on diffusion model). Specifically, we are the first to introduce a diffusion model with strong noise tendency and construct a diffusion seismic inversion model (DSIM). In the reverse diffusion of DSIM, we design the encoder-decoder which combines CNN for capturing local features and GRU for global sequence modeling; and we choose U-net to learn the distribution of random noise, enhancing the generalization and stability of proposed method. Furthermore, to further improve generalization of the proposed method, a two-step training approach (USSL) is utilized. First, an unsupervised trained encoder-decoder is used as the initial network model in place of the traditional low-frequency wave impedance model that is difficult to accurately acquire. Then, the SSL is employed to further optimize the encoder-decoder model. Experimental results on the Marmousi2 model and field data demonstrate that the DSIM-USSL method achieves higher accuracy in the presence of seismic data with random noise, and maintains high stability even under strong noise conditions.","sentences":["Seismic impedance inversion is one of the most important part of geophysical exploration.","However, due to random noise, the traditional semi-supervised learning (SSL) methods lack generalization and stability.","To solve this problem, some authors have proposed SSL methods with anti-noise function to improve noise robustness and inversion accuracy.","However, such methods are often not ideal when faced with strong noise.","In addition, Low-frequency impedance models can mitigate this problem, but creating accurate low-frequency models is difficult and error-prone when well-log data is sparse and subsurface structures are complex.","To address those issues, we propose a novel deep learning inversion method called DSIM-USSL (Unsupervised and Semi-supervised joint Learning for Seismic Inversion based on diffusion model).","Specifically, we are the first to introduce a diffusion model with strong noise tendency and construct a diffusion seismic inversion model (DSIM).","In the reverse diffusion of DSIM, we design the encoder-decoder which combines CNN for capturing local features and GRU for global sequence modeling; and we choose U-net to learn the distribution of random noise, enhancing the generalization and stability of proposed method.","Furthermore, to further improve generalization of the proposed method, a two-step training approach (USSL) is utilized.","First, an unsupervised trained encoder-decoder is used as the initial network model in place of the traditional low-frequency wave impedance model that is difficult to accurately acquire.","Then, the SSL is employed to further optimize the encoder-decoder model.","Experimental results on the Marmousi2 model and field data demonstrate that the DSIM-USSL method achieves higher accuracy in the presence of seismic data with random noise, and maintains high stability even under strong noise conditions."],"url":"http://arxiv.org/abs/2405.05026v1","category":"physics.geo-ph"}
{"created":"2024-05-08 12:56:33","title":"Learning Structural Causal Models through Deep Generative Models: Methods, Guarantees, and Challenges","abstract":"This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures. It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries. Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling. It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs.","sentences":["This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures.","It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries.","Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling.","It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs."],"url":"http://arxiv.org/abs/2405.05025v1","category":"stat.ML"}
{"created":"2024-05-08 12:48:01","title":"HackCar: a test platform for attacks and defenses on a cost-contained automotive architecture","abstract":"In this paper, we introduce the design of HackCar, a testing platform for replicating attacks and defenses on a generic automotive system without requiring access to a complete vehicle. This platform empowers security researchers to illustrate the consequences of attacks targeting an automotive system on a realistic platform, facilitating the development and testing of security countermeasures against both existing and novel attacks. The HackCar platform is built upon an F1-10th model, to which various automotive-grade microcontrollers are connected through automotive communication protocols. This solution is crafted to be entirely modular, allowing for the creation of diverse test scenarios. Researchers and practitioners can thus develop innovative security solutions while adhering to the constraints of automotive-grade microcontrollers. We showcase our design by comparing it with a real, licensed, and unmodified vehicle. Additionally, we analyze the behavior of the HackCar in both an attack-free scenario and a scenario where an attack on in-vehicle communication is deployed.","sentences":["In this paper, we introduce the design of HackCar, a testing platform for replicating attacks and defenses on a generic automotive system without requiring access to a complete vehicle.","This platform empowers security researchers to illustrate the consequences of attacks targeting an automotive system on a realistic platform, facilitating the development and testing of security countermeasures against both existing and novel attacks.","The HackCar platform is built upon an F1-10th model, to which various automotive-grade microcontrollers are connected through automotive communication protocols.","This solution is crafted to be entirely modular, allowing for the creation of diverse test scenarios.","Researchers and practitioners can thus develop innovative security solutions while adhering to the constraints of automotive-grade microcontrollers.","We showcase our design by comparing it with a real, licensed, and unmodified vehicle.","Additionally, we analyze the behavior of the HackCar in both an attack-free scenario and a scenario where an attack on in-vehicle communication is deployed."],"url":"http://arxiv.org/abs/2405.05023v1","category":"cs.CR"}
{"created":"2024-05-08 12:46:18","title":"Adversarial Threats to Automatic Modulation Open Set Recognition in Wireless Networks","abstract":"Automatic Modulation Open Set Recognition (AMOSR) is a crucial technological approach for cognitive radio communications, wireless spectrum management, and interference monitoring within wireless networks. Numerous studies have shown that AMR is highly susceptible to minimal perturbations carefully designed by malicious attackers, leading to misclassification of signals. However, the adversarial security issue of AMOSR has not yet been explored. This paper adopts the perspective of attackers and proposes an Open Set Adversarial Attack (OSAttack), aiming at investigating the adversarial vulnerabilities of various AMOSR methods. Initially, an adversarial threat model for AMOSR scenarios is established. Subsequently, by analyzing the decision criteria of both discriminative and generative open set recognition, OSFGSM and OSPGD are proposed to reduce the performance of AMOSR. Finally, the influence of OSAttack on AMOSR is evaluated utilizing a range of qualitative and quantitative indicators. The results indicate that despite the increased resistance of AMOSR models to conventional interference signals, they remain vulnerable to attacks by adversarial examples.","sentences":["Automatic Modulation Open Set Recognition (AMOSR) is a crucial technological approach for cognitive radio communications, wireless spectrum management, and interference monitoring within wireless networks.","Numerous studies have shown that AMR is highly susceptible to minimal perturbations carefully designed by malicious attackers, leading to misclassification of signals.","However, the adversarial security issue of AMOSR has not yet been explored.","This paper adopts the perspective of attackers and proposes an Open Set Adversarial Attack (OSAttack), aiming at investigating the adversarial vulnerabilities of various AMOSR methods.","Initially, an adversarial threat model for AMOSR scenarios is established.","Subsequently, by analyzing the decision criteria of both discriminative and generative open set recognition, OSFGSM and OSPGD are proposed to reduce the performance of AMOSR.","Finally, the influence of OSAttack on AMOSR is evaluated utilizing a range of qualitative and quantitative indicators.","The results indicate that despite the increased resistance of AMOSR models to conventional interference signals, they remain vulnerable to attacks by adversarial examples."],"url":"http://arxiv.org/abs/2405.05022v1","category":"cs.CR"}
{"created":"2024-05-08 12:32:07","title":"TGTM: TinyML-based Global Tone Mapping for HDR Sensors","abstract":"Advanced driver assistance systems (ADAS) relying on multiple cameras are increasingly prevalent in vehicle technology. Yet, conventional imaging sensors struggle to capture clear images in conditions with intense illumination contrast, such as tunnel exits, due to their limited dynamic range. Introducing high dynamic range (HDR) sensors addresses this issue. However, the process of converting HDR content to a displayable range via tone mapping often leads to inefficient computations, when performed directly on pixel data. In this paper, we focus on HDR image tone mapping using a lightweight neural network applied on image histogram data. Our proposed TinyML-based global tone mapping method, termed as TGTM, operates at 9,000 FLOPS per RGB image of any resolution. Additionally, TGTM offers a generic approach that can be incorporated to any classical tone mapping method. Experimental results demonstrate that TGTM outperforms state-of-the-art methods on real HDR camera images by up to 5.85 dB higher PSNR with orders of magnitude less computations.","sentences":["Advanced driver assistance systems (ADAS) relying on multiple cameras are increasingly prevalent in vehicle technology.","Yet, conventional imaging sensors struggle to capture clear images in conditions with intense illumination contrast, such as tunnel exits, due to their limited dynamic range.","Introducing high dynamic range (HDR) sensors addresses this issue.","However, the process of converting HDR content to a displayable range via tone mapping often leads to inefficient computations, when performed directly on pixel data.","In this paper, we focus on HDR image tone mapping using a lightweight neural network applied on image histogram data.","Our proposed TinyML-based global tone mapping method, termed as TGTM, operates at 9,000 FLOPS per RGB image of any resolution.","Additionally, TGTM offers a generic approach that can be incorporated to any classical tone mapping method.","Experimental results demonstrate that TGTM outperforms state-of-the-art methods on real HDR camera images by up to 5.85 dB higher PSNR with orders of magnitude less computations."],"url":"http://arxiv.org/abs/2405.05016v1","category":"cs.CV"}
{"created":"2024-05-08 12:31:35","title":"Concrete Dense Network for Long-Sequence Time Series Clustering","abstract":"Time series clustering is fundamental in data analysis for discovering temporal patterns. Despite recent advancements, learning cluster-friendly representations is still challenging, particularly with long and complex time series. Deep temporal clustering methods have been trying to integrate the canonical k-means into end-to-end training of neural networks but fall back on surrogate losses due to the non-differentiability of the hard cluster assignment, yielding sub-optimal solutions. In addition, the autoregressive strategy used in the state-of-the-art RNNs is subject to error accumulation and slow training, while recent research findings have revealed that Transformers are less effective due to time points lacking semantic meaning, to the permutation invariance of attention that discards the chronological order and high computation cost. In light of these observations, we present LoSTer which is a novel dense autoencoder architecture for the long-sequence time series clustering problem (LSTC) capable of optimizing the k-means objective via the Gumbel-softmax reparameterization trick and designed specifically for accurate and fast clustering of long time series. Extensive experiments on numerous benchmark datasets and two real-world applications prove the effectiveness of LoSTer over state-of-the-art RNNs and Transformer-based deep clustering methods.","sentences":["Time series clustering is fundamental in data analysis for discovering temporal patterns.","Despite recent advancements, learning cluster-friendly representations is still challenging, particularly with long and complex time series.","Deep temporal clustering methods have been trying to integrate the canonical k-means into end-to-end training of neural networks but fall back on surrogate losses due to the non-differentiability of the hard cluster assignment, yielding sub-optimal solutions.","In addition, the autoregressive strategy used in the state-of-the-art RNNs is subject to error accumulation and slow training, while recent research findings have revealed that Transformers are less effective due to time points lacking semantic meaning, to the permutation invariance of attention that discards the chronological order and high computation cost.","In light of these observations, we present LoSTer which is a novel dense autoencoder architecture for the long-sequence time series clustering problem (LSTC) capable of optimizing the k-means objective via the Gumbel-softmax reparameterization trick and designed specifically for accurate and fast clustering of long time series.","Extensive experiments on numerous benchmark datasets and two real-world applications prove the effectiveness of LoSTer over state-of-the-art RNNs and Transformer-based deep clustering methods."],"url":"http://arxiv.org/abs/2405.05015v1","category":"cs.LG"}
{"created":"2024-05-08 12:26:25","title":"Potential Surface Ice Distribution on Close-in Terrestrial Exoplanets around M dwarfs","abstract":"Previous studies suggested that surface ice could be distributed on close-in terrestrial exoplanets around M-dwarfs if heat redistribution on the planets is very inefficient. In general, orbital and atmospheric parameters play an important role in the climate on terrestrial planets, including the cold-trap region where the permanent surface water reservoir can potentially be distributed. Here, we develop a simple coupled land-atmosphere model to explore the potential surface ice distribution on close-in terrestrial planets with various orbital and atmospheric parameters, assuming that the planets are airless or have a thin \\ce{N2} atmosphere. We find that the most significant factors in deciding the surface cold trap region are the spin-orbit ratio and obliquity. The incident stellar flux and the surface pressure play a limited role in the thin \\ce{N2} simulations for incident flux smaller than Mercury's and surface pressure lower than 10$^4$ Pa. Our result illustrates the possible distribution of surface ice on arid terrestrial planets and can help to understand the climate of these exoplanets.","sentences":["Previous studies suggested that surface ice could be distributed on close-in terrestrial exoplanets around M-dwarfs if heat redistribution on the planets is very inefficient.","In general, orbital and atmospheric parameters play an important role in the climate on terrestrial planets, including the cold-trap region where the permanent surface water reservoir can potentially be distributed.","Here, we develop a simple coupled land-atmosphere model to explore the potential surface ice distribution on close-in terrestrial planets with various orbital and atmospheric parameters, assuming that the planets are airless or have a thin \\ce{N2} atmosphere.","We find that the most significant factors in deciding the surface cold trap region are the spin-orbit ratio and obliquity.","The incident stellar flux and the surface pressure play a limited role in the thin \\ce{N2} simulations for incident flux smaller than Mercury's and surface pressure lower than 10$^4$ Pa.","Our result illustrates the possible distribution of surface ice on arid terrestrial planets and can help to understand the climate of these exoplanets."],"url":"http://arxiv.org/abs/2405.05013v1","category":"astro-ph.EP"}
{"created":"2024-05-08 12:25:53","title":"Is a photon ring invariably a closed structure?","abstract":"In this study, we investigate the image of a rotating compact object (CO) illuminated by a geometrically thin, optically thin disk on the equatorial plane. As the radius of the CO's surface fluctuates, the CO may partially or entirely obscure the photon region. We observe that the perceived photon ring may exhibit discontinuities, deviating from a closed structure, and may even disappear entirely. We find that the disruption and disappearance of the photon ring are dependent on the observational angle$-$a novel phenomenon not previously observed in black hole imaging studies. Our study reveals that while the factors influencing this unique photon ring phenomenon are diverse and the outcomes complex, we can provide a clear and comprehensive explanation of the physical essence and variation trends of this phenomenon. We do this by introducing and analyzing the properties and interrelationships of three characteristic functions, $\\tilde{\\eta}$, $\\eta_o$, and $\\eta_s$ related to the photon impact parameters. Additionally, our analysis of the intensity cuts and inner shadows of the images uncovers patterns that differ significantly from the shadow curve.","sentences":["In this study, we investigate the image of a rotating compact object (CO) illuminated by a geometrically thin, optically thin disk on the equatorial plane.","As the radius of the CO's surface fluctuates, the CO may partially or entirely obscure the photon region.","We observe that the perceived photon ring may exhibit discontinuities, deviating from a closed structure, and may even disappear entirely.","We find that the disruption and disappearance of the photon ring are dependent on the observational angle$-$a novel phenomenon not previously observed in black hole imaging studies.","Our study reveals that while the factors influencing this unique photon ring phenomenon are diverse and the outcomes complex, we can provide a clear and comprehensive explanation of the physical essence and variation trends of this phenomenon.","We do this by introducing and analyzing the properties and interrelationships of three characteristic functions, $\\tilde{\\eta}$, $\\eta_o$, and $\\eta_s$ related to the photon impact parameters.","Additionally, our analysis of the intensity cuts and inner shadows of the images uncovers patterns that differ significantly from the shadow curve."],"url":"http://arxiv.org/abs/2405.05011v1","category":"gr-qc"}
{"created":"2024-05-08 12:24:52","title":"ADELIE: Aligning Large Language Models on Information Extraction","abstract":"Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks. This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data. In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE. We first collect and construct a high-quality alignment corpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on IEInstruct. We further train ADELIE_SFT with direct preference optimization (DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO) achieve state-of-the-art (SoTA) performance among open-source models. We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline. We will release the code, data, and models to facilitate further research.","sentences":["Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks.","This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data.","In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE.","We first collect and construct a high-quality alignment corpus IEInstruct for IE.","Then we train ADELIE_SFT using instruction tuning on IEInstruct.","We further train ADELIE_SFT with direct preference optimization (DPO) objective, resulting in ADELIE_DPO.","Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO) achieve state-of-the-art (SoTA) performance among open-source models.","We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline.","We will release the code, data, and models to facilitate further research."],"url":"http://arxiv.org/abs/2405.05008v1","category":"cs.CL"}
{"created":"2024-05-08 12:11:10","title":"Small ball probability for multiple singular values of symmetric random matrices","abstract":"Let $A_n$ be an $n\\times n$ random symmetric matrix with $(A_{ij})_{i< j}$ i.i.d. mean $0$, variance 1, following a subGaussian distribution and diagonal elements i.i.d. following a subGaussian distribution with a fixed variance. We investigate the joint small ball probability that $A_n$ has eigenvalues near two fixed locations $\\lambda_1$ and $\\lambda_2$, where $\\lambda_1$ and $\\lambda_2$ are sufficiently separated and in the bulk of the semicircle law. More precisely we prove that for a wide class of entry distributions of $A_{ij}$ that involve all Gaussian convolutions (where $\\sigma_{min}(\\cdot)$ denotes the least singular value of a square matrix), $$\\mathbb{P}(\\sigma_{min}(A_n-\\lambda_1 I_n)\\leq\\delta_1n^{-1/2},\\sigma_{min}(A_n-\\lambda_2 I_n)\\leq\\delta_2n^{-1/2})\\leq c\\delta_1\\delta_2+e^{-cn}.$$ The given estimate approximately factorizes as the product of the estimates for the two individual events, which is an indication of quantitative independence. The estimate readily generalizes to $d$ distinct locations. As an application, we upper bound the probability that there exist $d$ eigenvalues of $A_n$ asymptotically satisfying any fixed linear equation, which in particular gives a lower bound of the distance to this linear relation from any possible eigenvalue pair that holds with probability $1-o(1)$, and rules out the existence of two equal singular values in generic regions of the spectrum.","sentences":["Let $A_n$ be an $n\\times n$ random symmetric matrix with $(A_{ij})_{i< j}$ i.i.d. mean $0$, variance 1, following a subGaussian distribution and diagonal elements i.i.d.","following a subGaussian distribution with a fixed variance.","We investigate the joint small ball probability that $A_n$ has eigenvalues near two fixed locations $\\lambda_1$ and $\\lambda_2$, where $\\lambda_1$ and $\\lambda_2$ are sufficiently separated and in the bulk of the semicircle law.","More precisely we prove that for a wide class of entry distributions of $A_{ij}$ that involve all Gaussian convolutions (where $\\sigma_{min}(\\cdot)$ denotes the least singular value of a square matrix), $$\\mathbb{P}(\\sigma_{min}(A_n-\\lambda_1 I_n)\\leq\\delta_1n^{-1/2},\\sigma_{min}(A_n-\\lambda_2 I_n)\\leq\\delta_2n^{-1/2})\\leq c\\delta_1\\delta_2+e^{-cn}.$$","The given estimate approximately factorizes as the product of the estimates for the two individual events, which is an indication of quantitative independence.","The estimate readily generalizes to $d$ distinct locations.","As an application, we upper bound the probability that there exist $d$ eigenvalues of $A_n$ asymptotically satisfying any fixed linear equation, which in particular gives a lower bound of the distance to this linear relation from any possible eigenvalue pair that holds with probability $1-o(1)$, and rules out the existence of two equal singular values in generic regions of the spectrum."],"url":"http://arxiv.org/abs/2405.04999v1","category":"math.PR"}
{"created":"2024-05-08 12:10:44","title":"Axiomatization of approximate exclusion","abstract":"We define and axiomatize approximate exclusion atoms in the team semantic setting. A team is a set of assignments, which can be seen as a mathematical model of a uni-relational database, and we say that an approximate exclusion atom is satisfied in a team if the corresponding usual exclusion atom is satisfied in a large enough subteam. We consider the implication problem for approximate exclusion atoms and show that it is axiomatizable for consequences with a degree of approximation that is not too large. We prove the completeness theorem for usual exclusion atoms, which is currently missing from the literature, and generalize it to approximate exclusion atoms. We also provide a polynomial time algorithm for the implication problems. The results also apply to exclusion dependencies in database theory.","sentences":["We define and axiomatize approximate exclusion atoms in the team semantic setting.","A team is a set of assignments, which can be seen as a mathematical model of a uni-relational database, and we say that an approximate exclusion atom is satisfied in a team if the corresponding usual exclusion atom is satisfied in a large enough subteam.","We consider the implication problem for approximate exclusion atoms and show that it is axiomatizable for consequences with a degree of approximation that is not too large.","We prove the completeness theorem for usual exclusion atoms, which is currently missing from the literature, and generalize it to approximate exclusion atoms.","We also provide a polynomial time algorithm for the implication problems.","The results also apply to exclusion dependencies in database theory."],"url":"http://arxiv.org/abs/2405.04998v1","category":"cs.LO"}
{"created":"2024-05-08 11:58:55","title":"NAVRepair: Node-type Aware C/C++ Code Vulnerability Repair","abstract":"The rapid advancement of deep learning has led to the development of Large Language Models (LLMs). In the field of vulnerability repair, previous research has leveraged rule-based fixing, pre-trained models, and LLM's prompt engineering. However, existing approaches have limitations in terms of the integration of code structure with error types. Besides, due to certain features of C/C++ language, vulnerability repair in C/C++ proves to be exceptionally challenging. To address these challenges, we propose NAVRepair, a novel framework that combines the node-type information extracted from Abstract Syntax Trees (ASTs) with error types, specifically targeting C/C++ vulnerabilities. Specifically, our approach employs type analysis to localize the minimum edit node (MEN) and customizes context information collection based on different error types. In the offline stage, NAVRepair parses code patches to locate MENs and designs rules to extract relevant contextual information for each MEN type. In the online repairing stage, it analyzes the suspicious code, combines it with vulnerability type templates derived from the Common Weakness Enumeration (CWE), and generates targeted repair prompts. We evaluate NAVRepair on multiple popular LLMs and demonstrate its effectiveness in improving the performance of code vulnerability repair. Notably, our framework is independent of any specific LLMs and can quickly adapt to new vulnerability types. Extensive experiments validate that NAVRepair achieves excellent results in assisting LLMs to accurately detect and fix C/C++ vulnerabilities. We achieve a 26% higher accuracy compared to an existing LLM-based C/C++ vulnerability repair method. We believe our node type-aware approach has promising application prospects for enhancing real-world C/C++ code security.","sentences":["The rapid advancement of deep learning has led to the development of Large Language Models (LLMs).","In the field of vulnerability repair, previous research has leveraged rule-based fixing, pre-trained models, and LLM's prompt engineering.","However, existing approaches have limitations in terms of the integration of code structure with error types.","Besides, due to certain features of C/C++ language, vulnerability repair in C/C++ proves to be exceptionally challenging.","To address these challenges, we propose NAVRepair, a novel framework that combines the node-type information extracted from Abstract Syntax Trees (ASTs) with error types, specifically targeting C/C++ vulnerabilities.","Specifically, our approach employs type analysis to localize the minimum edit node (MEN) and customizes context information collection based on different error types.","In the offline stage, NAVRepair parses code patches to locate MENs and designs rules to extract relevant contextual information for each MEN type.","In the online repairing stage, it analyzes the suspicious code, combines it with vulnerability type templates derived from the Common Weakness Enumeration (CWE), and generates targeted repair prompts.","We evaluate NAVRepair on multiple popular LLMs and demonstrate its effectiveness in improving the performance of code vulnerability repair.","Notably, our framework is independent of any specific LLMs and can quickly adapt to new vulnerability types.","Extensive experiments validate that NAVRepair achieves excellent results in assisting LLMs to accurately detect and fix C/C++ vulnerabilities.","We achieve a 26% higher accuracy compared to an existing LLM-based C/C++ vulnerability repair method.","We believe our node type-aware approach has promising application prospects for enhancing real-world C/C++ code security."],"url":"http://arxiv.org/abs/2405.04994v1","category":"cs.SE"}
{"created":"2024-05-08 11:54:39","title":"Optical characterization of size- and substrate-dependent performance of ultraviolet hybrid plasmonic nanowire lasers","abstract":"Nanowire-based plasmonic lasers are now established as nano-sources of coherent radiation, appearing as suitable candidates for integration into next-generation nanophotonic circuitry. However, compared to their photonic counterparts, their relatively high losses and large lasing thresholds still pose a burdening constraint on their scalability. In this study, the lasing characteristics of ZnO nanowires on Ag and Al substrates, operating as optically-pumped short-wavelength plasmonic nanolasers, are systematically investigated in combination with the size-dependent performance of the hybrid cavity. A hybrid nanomanipulation-assisted single nanowire optical characterization combined with high-throughput PL spectroscopy enables the correlation of the lasing characteristics to the metal substrate and the nanowire diameter. The results evidence that the coupling between excitons and surface plasmons is closely tied to the relationship between substrate dispersive behavior and nanowire diameter. Such coupling dictates the degree to which the lasing character, be it more plasmonic- or photonic-like, can define the stimulated emission features and, as a result, the device performance.","sentences":["Nanowire-based plasmonic lasers are now established as nano-sources of coherent radiation, appearing as suitable candidates for integration into next-generation nanophotonic circuitry.","However, compared to their photonic counterparts, their relatively high losses and large lasing thresholds still pose a burdening constraint on their scalability.","In this study, the lasing characteristics of ZnO nanowires on Ag and Al substrates, operating as optically-pumped short-wavelength plasmonic nanolasers, are systematically investigated in combination with the size-dependent performance of the hybrid cavity.","A hybrid nanomanipulation-assisted single nanowire optical characterization combined with high-throughput PL spectroscopy enables the correlation of the lasing characteristics to the metal substrate and the nanowire diameter.","The results evidence that the coupling between excitons and surface plasmons is closely tied to the relationship between substrate dispersive behavior and nanowire diameter.","Such coupling dictates the degree to which the lasing character, be it more plasmonic- or photonic-like, can define the stimulated emission features and, as a result, the device performance."],"url":"http://arxiv.org/abs/2405.04992v1","category":"physics.optics"}
{"created":"2024-05-08 11:54:15","title":"Health Index Estimation Through Integration of General Knowledge with Unsupervised Learning","abstract":"Accurately estimating a Health Index (HI) from condition monitoring data (CM) is essential for reliable and interpretable prognostics and health management (PHM) in complex systems. In most scenarios, complex systems operate under varying operating conditions and can exhibit different fault modes, making unsupervised inference of an HI from CM data a significant challenge. Hybrid models combining prior knowledge about degradation with deep learning models have been proposed to overcome this challenge. However, previously suggested hybrid models for HI estimation usually rely heavily on system-specific information, limiting their transferability to other systems. In this work, we propose an unsupervised hybrid method for HI estimation that integrates general knowledge about degradation into the convolutional autoencoder's model architecture and learning algorithm, enhancing its applicability across various systems. The effectiveness of the proposed method is demonstrated in two case studies from different domains: turbofan engines and lithium batteries. The results show that the proposed method outperforms other competitive alternatives, including residual-based methods, in terms of HI quality and their utility for Remaining Useful Life (RUL) predictions. The case studies also highlight the comparable performance of our proposed method with a supervised model trained with HI labels.","sentences":["Accurately estimating a Health Index (HI) from condition monitoring data (CM) is essential for reliable and interpretable prognostics and health management (PHM) in complex systems.","In most scenarios, complex systems operate under varying operating conditions and can exhibit different fault modes, making unsupervised inference of an HI from CM data a significant challenge.","Hybrid models combining prior knowledge about degradation with deep learning models have been proposed to overcome this challenge.","However, previously suggested hybrid models for HI estimation usually rely heavily on system-specific information, limiting their transferability to other systems.","In this work, we propose an unsupervised hybrid method for HI estimation that integrates general knowledge about degradation into the convolutional autoencoder's model architecture and learning algorithm, enhancing its applicability across various systems.","The effectiveness of the proposed method is demonstrated in two case studies from different domains: turbofan engines and lithium batteries.","The results show that the proposed method outperforms other competitive alternatives, including residual-based methods, in terms of HI quality and their utility for Remaining Useful Life (RUL) predictions.","The case studies also highlight the comparable performance of our proposed method with a supervised model trained with HI labels."],"url":"http://arxiv.org/abs/2405.04990v1","category":"cs.LG"}
{"created":"2024-05-08 11:53:43","title":"Paley-Wiener Type Theorems associated to Dirac Operators of Riesz-Feller type","abstract":"This paper systematically investigates Paley-Wiener-type theorems in the context of hypercomplex variables. To this end, we introduce and study the so-called generalized Bernstein spaces endowed by the fractional Dirac operator $D_{\\alpha}^{\\theta}$ - a space-fractional operator of order $\\alpha$ and skewness $\\theta$, encompassing the Dirac operator $D$. We will show that such family of function spaces seamlessly characterizes the interplay between Clifford-valued $L^p-$functions satisfying the support condition $\\mathrm{supp}\\ \\widehat{f}\\subseteq B(0,R)$, and the solutions of the Cauchy problems endowed by the space-time operator $\\partial_{x_0}+D_\\theta^\\alpha$ that are of exponential type $R^\\alpha$. Such construction allows us to generalize, in a meaningful way, the results obtained by Kou and Qian (2002) and Franklin, Hogan and Larkin (2017). Noteworthy, the exploitation of the well-known Kolmogorov-Stein inequalities to hypercomplex variables permits us to make the computation of the maximal radius $R$ for which $\\mathrm{supp}\\ \\widehat{f}$ is compactly supported in $B(0,R)$ rather explicit.","sentences":["This paper systematically investigates Paley-Wiener-type theorems in the context of hypercomplex variables.","To this end, we introduce and study the so-called generalized Bernstein spaces endowed by the fractional Dirac operator $D_{\\alpha}^{\\theta}$ - a space-fractional operator of order $\\alpha$ and skewness $\\theta$, encompassing the Dirac operator $D$. We will show that such family of function spaces seamlessly characterizes the interplay between Clifford-valued $L^p-$functions satisfying the support condition $\\mathrm{supp}\\ \\widehat{f}\\subseteq B(0,R)$, and the solutions of the Cauchy problems endowed by the space-time operator $\\partial_{x_0}+D_\\theta^\\alpha$ that are of exponential type $R^\\alpha$. Such construction allows us to generalize, in a meaningful way, the results obtained by Kou and Qian (2002) and Franklin, Hogan and Larkin (2017).","Noteworthy, the exploitation of the well-known Kolmogorov-Stein inequalities to hypercomplex variables permits us to make the computation of the maximal radius $R$ for which $\\mathrm{supp}\\ \\widehat{f}$ is compactly supported in $B(0,R)$ rather explicit."],"url":"http://arxiv.org/abs/2405.04989v1","category":"math.CV"}
{"created":"2024-05-08 11:53:13","title":"TeraPool-SDR: An 1.89TOPS 1024 RV-Cores 4MiB Shared-L1 Cluster for Next-Generation Open-Source Software-Defined Radios","abstract":"Radio Access Networks (RAN) workloads are rapidly scaling up in data processing intensity and throughput as the 5G (and beyond) standards grow in number of antennas and sub-carriers. Offering flexible Processing Elements (PEs), efficient memory access, and a productive parallel programming model, many-core clusters are a well-matched architecture for next-generation software-defined RANs, but staggering performance requirements demand a high number of PEs coupled with extreme Power, Performance and Area (PPA) efficiency. We present the architecture, design, and full physical implementation of Terapool-SDR, a cluster for Software Defined Radio (SDR) with 1024 latency-tolerant, compact RV32 PEs, sharing a global view of a 4MiB, 4096-banked, L1 memory. We report various feasible configurations of TeraPool-SDR featuring an ultra-high bandwidth PE-to-L1-memory interconnect, clocked at 730MHz, 880MHz, and 924MHz (TT/0.80 V/25 {\\deg}C) in 12nm FinFET technology. The TeraPool-SDR cluster achieves high energy efficiency on all SDR key kernels for 5G RANs: Fast Fourier Transform (93GOPS/W), Matrix-Multiplication (125GOPS/W), Channel Estimation (96GOPS/W), and Linear System Inversion (61GOPS/W). For all the kernels, it consumes less than 10W, in compliance with industry standards.","sentences":["Radio Access Networks (RAN) workloads are rapidly scaling up in data processing intensity and throughput as the 5G (and beyond) standards grow in number of antennas and sub-carriers.","Offering flexible Processing Elements (PEs), efficient memory access, and a productive parallel programming model, many-core clusters are a well-matched architecture for next-generation software-defined RANs, but staggering performance requirements demand a high number of PEs coupled with extreme Power, Performance and Area (PPA) efficiency.","We present the architecture, design, and full physical implementation of Terapool-SDR, a cluster for Software Defined Radio (SDR) with 1024 latency-tolerant, compact RV32 PEs, sharing a global view of a 4MiB, 4096-banked, L1 memory.","We report various feasible configurations of TeraPool-SDR featuring an ultra-high bandwidth PE-to-L1-memory interconnect, clocked at 730MHz, 880MHz, and 924MHz (TT/0.80 V/25 {\\deg}C) in 12nm FinFET technology.","The TeraPool-SDR cluster achieves high energy efficiency on all SDR key kernels for 5G RANs: Fast Fourier Transform (93GOPS/W), Matrix-Multiplication (125GOPS/W), Channel Estimation (96GOPS/W), and Linear System Inversion (61GOPS/W).","For all the kernels, it consumes less than 10W, in compliance with industry standards."],"url":"http://arxiv.org/abs/2405.04988v1","category":"cs.DC"}
{"created":"2024-05-08 11:47:32","title":"An Artificial Intelligence Approach for Interpreting Creative Combinational Designs","abstract":"Combinational creativity, a form of creativity involving the blending of familiar ideas, is pivotal in design innovation. While most research focuses on how combinational creativity in design is achieved through blending elements, this study focuses on the computational interpretation, specifically identifying the 'base' and 'additive' components that constitute a creative design. To achieve this goal, the authors propose a heuristic algorithm integrating computer vision and natural language processing technologies, and implement multiple approaches based on both discriminative and generative artificial intelligence architectures. A comprehensive evaluation was conducted on a dataset created for studying combinational creativity. Among the implementations of the proposed algorithm, the most effective approach demonstrated a high accuracy in interpretation, achieving 87.5% for identifying 'base' and 80% for 'additive'. We conduct a modular analysis and an ablation experiment to assess the performance of each part in our implementations. Additionally, the study includes an analysis of error cases and bottleneck issues, providing critical insights into the limitations and challenges inherent in the computational interpretation of creative designs.","sentences":["Combinational creativity, a form of creativity involving the blending of familiar ideas, is pivotal in design innovation.","While most research focuses on how combinational creativity in design is achieved through blending elements, this study focuses on the computational interpretation, specifically identifying the 'base' and 'additive' components that constitute a creative design.","To achieve this goal, the authors propose a heuristic algorithm integrating computer vision and natural language processing technologies, and implement multiple approaches based on both discriminative and generative artificial intelligence architectures.","A comprehensive evaluation was conducted on a dataset created for studying combinational creativity.","Among the implementations of the proposed algorithm, the most effective approach demonstrated a high accuracy in interpretation, achieving 87.5% for identifying 'base' and 80% for 'additive'.","We conduct a modular analysis and an ablation experiment to assess the performance of each part in our implementations.","Additionally, the study includes an analysis of error cases and bottleneck issues, providing critical insights into the limitations and challenges inherent in the computational interpretation of creative designs."],"url":"http://arxiv.org/abs/2405.04985v1","category":"cs.AI"}
{"created":"2024-05-08 11:39:07","title":"Formal Theory of Heavy Ion Double Charge Exchange Reactions","abstract":"The theory of heavy ion double charge exchange (DCE) reactions $A(Z,N)\\to A(Z\\pm 2,N\\mp 2)$ is recapitulated emphasizing the role of Double Single Charge Exchange (DSCE) and pion-nucleon Majorana DCE (MDCE) reactions. DSCE reactions are of second--order distorted wave character, mediated by isovector nucleon-nucleon (NN) interactions. The DSCE response functions resemble the nuclear matrix elements (NME) of $2\\nu 2\\beta$ decay. The MDCE process proceeds by a dynamically generated effective rank-2 isotensor interaction, defined by off--shell pion--nucleon DCE scattering. In closure approximation pion potentials and two--nucleon correlations are obtained, similar to the neutrino potentials and the intranuclear exchange of Majorana neutrinos in $0\\nu 2 \\beta$ Majorana double beta decay (MDBD).","sentences":["The theory of heavy ion double charge exchange (DCE) reactions $A(Z,N)\\to A(Z\\pm 2,N\\mp 2)$ is recapitulated emphasizing the role of Double Single Charge Exchange (DSCE) and pion-nucleon Majorana DCE (MDCE) reactions.","DSCE reactions are of second--order distorted wave character, mediated by isovector nucleon-nucleon (NN) interactions.","The DSCE response functions resemble the nuclear matrix elements (NME) of $2\\nu 2\\beta$ decay.","The MDCE process proceeds by a dynamically generated effective rank-2 isotensor interaction, defined by off--shell pion--nucleon DCE scattering.","In closure approximation pion potentials and two--nucleon correlations are obtained, similar to the neutrino potentials and the intranuclear exchange of Majorana neutrinos in $0\\nu 2 \\beta$ Majorana double beta decay (MDBD)."],"url":"http://arxiv.org/abs/2405.04978v1","category":"nucl-th"}
{"created":"2024-05-08 11:38:33","title":"Reconciling microscopic and macroscopic tests of the Compton-Schwarzschild correspondence","abstract":"We review the experimental constraints on the parameter $\\alpha$ associated with the Generalized Uncertainty Principle (GUP) and the parameter $\\beta$ associated with the Generalized Event Horizon (GEH). The Compton-Schwarzschild correspondence implies a relationship between $\\alpha$ and $\\beta$, with both parameters being of order 1. This presents a problem for our previous `$M+1/M$' model since the extra gravitational force at sub-Planckian masses contravenes observations. Various resolutions of this problem are discussed.","sentences":["We review the experimental constraints on the parameter $\\alpha$ associated with the Generalized Uncertainty Principle (GUP) and the parameter $\\beta$ associated with the Generalized Event Horizon (GEH).","The Compton-Schwarzschild correspondence implies a relationship between $\\alpha$ and $\\beta$, with both parameters being of order 1.","This presents a problem for our previous `$M+1/M$' model since the extra gravitational force at sub-Planckian masses contravenes observations.","Various resolutions of this problem are discussed."],"url":"http://arxiv.org/abs/2405.04977v1","category":"gr-qc"}
{"created":"2024-05-08 11:33:19","title":"RF-based Energy Harvesting: Nonlinear Models, Applications and Challenges","abstract":"So far, various aspects associated with wireless energy harvesting (EH) have been investigated from diverse perspectives, including energy sources and models, usage protocols, energy scheduling and optimization, and EH implementation in different wireless communication systems. However, a comprehensive survey specifically focusing on models of radio frequency (RF)-based EH behaviors has not yet been presented. To address this gap, this article provides an overview of the mainstream mathematical models that capture the nonlinear behavior of practical EH circuits, serving as a valuable handbook of mathematical models for EH application research. Moreover, we summarize the application of each nonlinear EH model, including the associated challenges and precautions. We also analyze the impact and advancements of each EH model on RF-based EH systems in wireless communication, utilizing artificial intelligence (AI) techniques. Additionally, we highlight emerging research directions in the context of nonlinear RF-based EH. This article aims to contribute to the future application of RF-based EH in novel communication research domains to a significant extent.","sentences":["So far, various aspects associated with wireless energy harvesting (EH) have been investigated from diverse perspectives, including energy sources and models, usage protocols, energy scheduling and optimization, and EH implementation in different wireless communication systems.","However, a comprehensive survey specifically focusing on models of radio frequency (RF)-based EH behaviors has not yet been presented.","To address this gap, this article provides an overview of the mainstream mathematical models that capture the nonlinear behavior of practical EH circuits, serving as a valuable handbook of mathematical models for EH application research.","Moreover, we summarize the application of each nonlinear EH model, including the associated challenges and precautions.","We also analyze the impact and advancements of each EH model on RF-based EH systems in wireless communication, utilizing artificial intelligence (AI) techniques.","Additionally, we highlight emerging research directions in the context of nonlinear RF-based EH.","This article aims to contribute to the future application of RF-based EH in novel communication research domains to a significant extent."],"url":"http://arxiv.org/abs/2405.04976v1","category":"cs.IT"}
{"created":"2024-05-08 11:32:50","title":"Prototype2Code: End-to-end Front-end Code Generation from UI Design Prototypes","abstract":"UI-to-code technology has streamlined the front-end development process, reducing repetitive tasks for engineers. prior research mainly use design prototypes as inputs, with the effectiveness of the generated code heavily dependent on these prototypes' quality, leading to compromised robustness. Moreover, these approaches also exhibit shortcomings in code quality, including issues such as disorganized UI structures and the inability to support responsive layouts. To address these challenges, we introduce Prototype2Code, which achieves end-to-end front-end code generation with business demands. For Prototype2Code, we incorporate design linting into the workflow, addressing the detection of fragmented elements and perceptual groups, enhancing the robustness of the generated outcomes. By optimizing the hierarchical structure and intelligently recognizing UI element types, Prototype2Code generates code that is more readable and structurally clearer. To meet responsive design requirements, Prototype2Code primarily supports flexbox layout model, ensuring code compatibility across various device sizes. To validate the efficacy, we compare Prototype2Code with the commercial code generation platform CodeFun and Screenshot-to-code based on GPT-4 with vision. Employing structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and mean squared error (MSE) for visual similarity assessment, Prototype2Code's rendered UI effects align most closely with the design prototypes, exhibiting the minimal errors. We also conduct a user study with five experienced front-end engineers, inviting them to review and revise code generated by the three methods. As a result, Prototype2Code surpasses other methods in readability, usability, and maintainability, better meeting the business needs of industrial development.","sentences":["UI-to-code technology has streamlined the front-end development process, reducing repetitive tasks for engineers.","prior research mainly use design prototypes as inputs, with the effectiveness of the generated code heavily dependent on these prototypes' quality, leading to compromised robustness.","Moreover, these approaches also exhibit shortcomings in code quality, including issues such as disorganized UI structures and the inability to support responsive layouts.","To address these challenges, we introduce Prototype2Code, which achieves end-to-end front-end code generation with business demands.","For Prototype2Code, we incorporate design linting into the workflow, addressing the detection of fragmented elements and perceptual groups, enhancing the robustness of the generated outcomes.","By optimizing the hierarchical structure and intelligently recognizing UI element types, Prototype2Code generates code that is more readable and structurally clearer.","To meet responsive design requirements, Prototype2Code primarily supports flexbox layout model, ensuring code compatibility across various device sizes.","To validate the efficacy, we compare Prototype2Code with the commercial code generation platform CodeFun and Screenshot-to-code based on GPT-4 with vision.","Employing structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and mean squared error (MSE) for visual similarity assessment, Prototype2Code's rendered UI effects align most closely with the design prototypes, exhibiting the minimal errors.","We also conduct a user study with five experienced front-end engineers, inviting them to review and revise code generated by the three methods.","As a result, Prototype2Code surpasses other methods in readability, usability, and maintainability, better meeting the business needs of industrial development."],"url":"http://arxiv.org/abs/2405.04975v1","category":"cs.SE"}
{"created":"2024-05-08 11:26:49","title":"Discrepancy-based Diffusion Models for Lesion Detection in Brain MRI","abstract":"Diffusion probabilistic models (DPMs) have exhibited significant effectiveness in computer vision tasks, particularly in image generation. However, their notable performance heavily relies on labelled datasets, which limits their application in medical images due to the associated high-cost annotations. Current DPM-related methods for lesion detection in medical imaging, which can be categorized into two distinct approaches, primarily rely on image-level annotations. The first approach, based on anomaly detection, involves learning reference healthy brain representations and identifying anomalies based on the difference in inference results. In contrast, the second approach, resembling a segmentation task, employs only the original brain multi-modalities as prior information for generating pixel-level annotations. In this paper, our proposed model - discrepancy distribution medical diffusion (DDMD) - for lesion detection in brain MRI introduces a novel framework by incorporating distinctive discrepancy features, deviating from the conventional direct reliance on image-level annotations or the original brain modalities. In our method, the inconsistency in image-level annotations is translated into distribution discrepancies among heterogeneous samples while preserving information within homogeneous samples. This property retains pixel-wise uncertainty and facilitates an implicit ensemble of segmentation, ultimately enhancing the overall detection performance. Thorough experiments conducted on the BRATS2020 benchmark dataset containing multimodal MRI scans for brain tumour detection demonstrate the great performance of our approach in comparison to state-of-the-art methods.","sentences":["Diffusion probabilistic models (DPMs) have exhibited significant effectiveness in computer vision tasks, particularly in image generation.","However, their notable performance heavily relies on labelled datasets, which limits their application in medical images due to the associated high-cost annotations.","Current DPM-related methods for lesion detection in medical imaging, which can be categorized into two distinct approaches, primarily rely on image-level annotations.","The first approach, based on anomaly detection, involves learning reference healthy brain representations and identifying anomalies based on the difference in inference results.","In contrast, the second approach, resembling a segmentation task, employs only the original brain multi-modalities as prior information for generating pixel-level annotations.","In this paper, our proposed model - discrepancy distribution medical diffusion (DDMD) - for lesion detection in brain MRI introduces a novel framework by incorporating distinctive discrepancy features, deviating from the conventional direct reliance on image-level annotations or the original brain modalities.","In our method, the inconsistency in image-level annotations is translated into distribution discrepancies among heterogeneous samples while preserving information within homogeneous samples.","This property retains pixel-wise uncertainty and facilitates an implicit ensemble of segmentation, ultimately enhancing the overall detection performance.","Thorough experiments conducted on the BRATS2020 benchmark dataset containing multimodal MRI scans for brain tumour detection demonstrate the great performance of our approach in comparison to state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04974v1","category":"cs.CV"}
{"created":"2024-05-08 11:25:04","title":"Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision Support","abstract":"Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals' decision-making (e.g., by manipulating purchase decisions). Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions. So far, the potential of these technological advances to overcome anchoring bias remains widely unclear. To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias. Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias. Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases.","sentences":["Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals' decision-making (e.g., by manipulating purchase decisions).","Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions.","So far, the potential of these technological advances to overcome anchoring bias remains widely unclear.","To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias.","Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias.","Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases."],"url":"http://arxiv.org/abs/2405.04972v1","category":"cs.CY"}
{"created":"2024-05-08 11:24:57","title":"End-to-End Semi-Supervised approach with Modulated Object Queries for Table Detection in Documents","abstract":"Table detection, a pivotal task in document analysis, aims to precisely recognize and locate tables within document images. Although deep learning has shown remarkable progress in this realm, it typically requires an extensive dataset of labeled data for proficient training. Current CNN-based semi-supervised table detection approaches use the anchor generation process and Non-Maximum Suppression (NMS) in their detection process, limiting training efficiency. Meanwhile, transformer-based semi-supervised techniques adopted a one-to-one match strategy that provides noisy pseudo-labels, limiting overall efficiency. This study presents an innovative transformer-based semi-supervised table detector. It improves the quality of pseudo-labels through a novel matching strategy combining one-to-one and one-to-many assignment techniques. This approach significantly enhances training efficiency during the early stages, ensuring superior pseudo-labels for further training. Our semi-supervised approach is comprehensively evaluated on benchmark datasets, including PubLayNet, ICADR-19, and TableBank. It achieves new state-of-the-art results, with a mAP of 95.7% and 97.9% on TableBank (word) and PubLaynet with 30% label data, marking a 7.4 and 7.6 point improvement over previous semi-supervised table detection approach, respectively. The results clearly show the superiority of our semi-supervised approach, surpassing all existing state-of-the-art methods by substantial margins. This research represents a significant advancement in semi-supervised table detection methods, offering a more efficient and accurate solution for practical document analysis tasks.","sentences":["Table detection, a pivotal task in document analysis, aims to precisely recognize and locate tables within document images.","Although deep learning has shown remarkable progress in this realm, it typically requires an extensive dataset of labeled data for proficient training.","Current CNN-based semi-supervised table detection approaches use the anchor generation process and Non-Maximum Suppression (NMS) in their detection process, limiting training efficiency.","Meanwhile, transformer-based semi-supervised techniques adopted a one-to-one match strategy that provides noisy pseudo-labels, limiting overall efficiency.","This study presents an innovative transformer-based semi-supervised table detector.","It improves the quality of pseudo-labels through a novel matching strategy combining one-to-one and one-to-many assignment techniques.","This approach significantly enhances training efficiency during the early stages, ensuring superior pseudo-labels for further training.","Our semi-supervised approach is comprehensively evaluated on benchmark datasets, including PubLayNet, ICADR-19, and TableBank.","It achieves new state-of-the-art results, with a mAP of 95.7% and 97.9% on TableBank (word) and PubLaynet with 30% label data, marking a 7.4 and 7.6 point improvement over previous semi-supervised table detection approach, respectively.","The results clearly show the superiority of our semi-supervised approach, surpassing all existing state-of-the-art methods by substantial margins.","This research represents a significant advancement in semi-supervised table detection methods, offering a more efficient and accurate solution for practical document analysis tasks."],"url":"http://arxiv.org/abs/2405.04971v1","category":"cs.CV"}
{"created":"2024-05-08 11:18:51","title":"Numerical analysis of small-strain elasto-plastic deformation using local Radial Basis Function approximation with Picard iteration","abstract":"This paper deals with a numerical analysis of plastic deformation under various conditions, utilizing Radial Basis Function (RBF) approximation. The focus is on the elasto-plastic von Mises problem under plane-strain assumption. Elastic deformation is modelled using the Navier-Cauchy equation. In regions where the von Mises stress surpasses the yield stress, corrections are applied locally through a return mapping algorithm. The non-linear deformation problem in the plastic domain is solved using the Picard iteration.   The solutions for the Navier-Cauchy equation are computed using the Radial Basis Function-Generated Finite Differences (RBF-FD) meshless method using only scattered nodes in a strong form. Verification of the method is performed through the analysis of an internally pressurized thick-walled cylinder subjected to varying loading conditions. These conditions induce states of elastic expansion, perfectly-plastic yielding, and plastic yielding with linear hardening. The results are benchmarked against analytical solutions and traditional Finite Element Method (FEM) solutions. The paper also showcases the robustness of this approach by solving case of thick-walled cylinder with cut-outs. The results affirm that the RBF-FD method produces results comparable to those obtained through FEM, while offering substantial benefits in managing complex geometries without the necessity for conventional meshing, along with other benefits of meshless methods.","sentences":["This paper deals with a numerical analysis of plastic deformation under various conditions, utilizing Radial Basis Function (RBF) approximation.","The focus is on the elasto-plastic von Mises problem under plane-strain assumption.","Elastic deformation is modelled using the Navier-Cauchy equation.","In regions where the von Mises stress surpasses the yield stress, corrections are applied locally through a return mapping algorithm.","The non-linear deformation problem in the plastic domain is solved using the Picard iteration.   ","The solutions for the Navier-Cauchy equation are computed using the Radial Basis Function-Generated Finite Differences (RBF-FD) meshless method using only scattered nodes in a strong form.","Verification of the method is performed through the analysis of an internally pressurized thick-walled cylinder subjected to varying loading conditions.","These conditions induce states of elastic expansion, perfectly-plastic yielding, and plastic yielding with linear hardening.","The results are benchmarked against analytical solutions and traditional Finite Element Method (FEM) solutions.","The paper also showcases the robustness of this approach by solving case of thick-walled cylinder with cut-outs.","The results affirm that the RBF-FD method produces results comparable to those obtained through FEM, while offering substantial benefits in managing complex geometries without the necessity for conventional meshing, along with other benefits of meshless methods."],"url":"http://arxiv.org/abs/2405.04970v1","category":"math.NA"}
{"created":"2024-05-08 11:15:20","title":"A review on discriminative self-supervised learning methods","abstract":"In the field of computer vision, self-supervised learning has emerged as a method to extract robust features from unlabeled data, where models derive labels autonomously from the data itself, without the need for manual annotation. This paper provides a comprehensive review of discriminative approaches of self-supervised learning within the domain of computer vision, examining their evolution and current status. Through an exploration of various methods including contrastive, self-distillation, knowledge distillation, feature decorrelation, and clustering techniques, we investigate how these approaches leverage the abundance of unlabeled data. Finally, we have comparison of self-supervised learning methods on the standard ImageNet classification benchmark.","sentences":["In the field of computer vision, self-supervised learning has emerged as a method to extract robust features from unlabeled data, where models derive labels autonomously from the data itself, without the need for manual annotation.","This paper provides a comprehensive review of discriminative approaches of self-supervised learning within the domain of computer vision, examining their evolution and current status.","Through an exploration of various methods including contrastive, self-distillation, knowledge distillation, feature decorrelation, and clustering techniques, we investigate how these approaches leverage the abundance of unlabeled data.","Finally, we have comparison of self-supervised learning methods on the standard ImageNet classification benchmark."],"url":"http://arxiv.org/abs/2405.04969v1","category":"cs.CV"}
{"created":"2024-05-08 11:14:14","title":"Predicting positon solutions of a family of Nonlinear Schr\u00f6dinger equations through Deep Learning algorithm","abstract":"We consider a hierarchy of nonlinear Schr\\\"{o}dinger equations (NLSEs) and forecast the evolution of positon solutions using a deep learning approach called Physics Informed Neural Networks (PINN). Notably, the PINN algorithm accurately predicts positon solutions not only in the standard NLSE but also in other higher order versions, including cubic, quartic and quintic NLSEs. The PINN approach also effectively handles two coupled NLSEs and two coupled Hirota equations. In addition to the above, we report exact second-order positon solutions of the sextic NLSE and coupled generalized NLSE. These solutions are not available in the existing literature and we construct them through generalized Darboux transformation method. Further, we utilize PINNs to forecast their behaviour as well. To validate PINN's accuracy, we compare the predicted solutions with exact solutions obtained from analytical methods. The results show high fidelity and low mean squared error in the predictions generated by our PINN model.","sentences":["We consider a hierarchy of nonlinear Schr\\\"{o}dinger equations (NLSEs) and forecast the evolution of positon solutions using a deep learning approach called Physics Informed Neural Networks (PINN).","Notably, the PINN algorithm accurately predicts positon solutions not only in the standard NLSE but also in other higher order versions, including cubic, quartic and quintic NLSEs.","The PINN approach also effectively handles two coupled NLSEs and two coupled Hirota equations.","In addition to the above, we report exact second-order positon solutions of the sextic NLSE and coupled generalized NLSE.","These solutions are not available in the existing literature and we construct them through generalized Darboux transformation method.","Further, we utilize PINNs to forecast their behaviour as well.","To validate PINN's accuracy, we compare the predicted solutions with exact solutions obtained from analytical methods.","The results show high fidelity and low mean squared error in the predictions generated by our PINN model."],"url":"http://arxiv.org/abs/2405.04968v1","category":"nlin.PS"}
{"created":"2024-05-08 11:11:04","title":"The evolution of expanding spacetime realizes approximate quantum cloning","abstract":"We investigate how quantum information, encoded in a quantum field, evolves during the expansion of spacetime. Due to information loss across the horizon, a local observer experiences this evolution as a nonunitary quantum channel. We obtain this channel in the case of de Sitter spacetime by assuming the initial global state encodes a signal state via fluctuations of the Bunch-Davies vacuum. Notably, de Sitter evolution exhibits intriguing cloning properties, establishing a connection between the curvature of spacetime and the propagation of quantum information.","sentences":["We investigate how quantum information, encoded in a quantum field, evolves during the expansion of spacetime.","Due to information loss across the horizon, a local observer experiences this evolution as a nonunitary quantum channel.","We obtain this channel in the case of de Sitter spacetime by assuming the initial global state encodes a signal state via fluctuations of the Bunch-Davies vacuum.","Notably, de Sitter evolution exhibits intriguing cloning properties, establishing a connection between the curvature of spacetime and the propagation of quantum information."],"url":"http://arxiv.org/abs/2405.04965v1","category":"gr-qc"}
{"created":"2024-05-08 10:49:39","title":"Improving Long Text Understanding with Knowledge Distilled from Summarization Model","abstract":"Long text understanding is important yet challenging for natural language processing. A long article or document usually contains many redundant words that are not pertinent to its gist and sometimes can be regarded as noise. With recent advances of abstractive summarization, we propose our \\emph{Gist Detector} to leverage the gist detection ability of a summarization model and integrate the extracted gist into downstream models to enhance their long text understanding ability. Specifically, Gist Detector first learns the gist detection knowledge distilled from a summarization model, and then produces gist-aware representations to augment downstream models. We evaluate our method on three different tasks: long document classification, distantly supervised open-domain question answering, and non-parallel text style transfer. The experimental results show that our method can significantly improve the performance of baseline models on all tasks.","sentences":["Long text understanding is important yet challenging for natural language processing.","A long article or document usually contains many redundant words that are not pertinent to its gist and sometimes can be regarded as noise.","With recent advances of abstractive summarization, we propose our \\emph{Gist Detector} to leverage the gist detection ability of a summarization model and integrate the extracted gist into downstream models to enhance their long text understanding ability.","Specifically, Gist Detector first learns the gist detection knowledge distilled from a summarization model, and then produces gist-aware representations to augment downstream models.","We evaluate our method on three different tasks: long document classification, distantly supervised open-domain question answering, and non-parallel text style transfer.","The experimental results show that our method can significantly improve the performance of baseline models on all tasks."],"url":"http://arxiv.org/abs/2405.04955v1","category":"cs.CL"}
{"created":"2024-05-08 10:48:28","title":"On vector parking functions and q-analogue","abstract":"In 2000, it was demonstrated that the set of $x$-parking functions of length $n$, where $x$=($a,b,...,b$) $\\in \\mathbbm{N}^n$, is equivalent to the set of rooted multicolored forests on [$n$]=\\{1,...,$n$\\}. In 2020, Yue Cai and Catherine H. Yan systematically investigated the properties of rational parking functions. Subsequently, a series of Context-free grammars possessing the requisite property were introduced by William Y.C. Chen and Harold R.L. Yang in 2021. %An Abelian-type identity is derived from a comparable methodology and grammatical framework. %Leveraging a comparable methodology and grammatical framework, an Abelian-type identity is derived herein. In this paper, I discuss generalized parking functions in terms of grammars. The primary result is to obtain the q-analogue about the number of '1's in certain vector parking functions with the assistance of grammars.","sentences":["In 2000, it was demonstrated that the set of $x$-parking functions of length $n$, where $x$=($a,b,...,b$) $\\in \\mathbbm{N}^n$, is equivalent to the set of rooted multicolored forests on [$n$]=\\{1,...,$n$\\}.","In 2020, Yue Cai and Catherine H. Yan systematically investigated the properties of rational parking functions.","Subsequently, a series of Context-free grammars possessing the requisite property were introduced by William Y.C. Chen and Harold R.L. Yang in 2021.","%An Abelian-type identity is derived from a comparable methodology and grammatical framework.","%Leveraging a comparable methodology and grammatical framework, an Abelian-type identity is derived herein.","In this paper, I discuss generalized parking functions in terms of grammars.","The primary result is to obtain the q-analogue about the number of '1's in certain vector parking functions with the assistance of grammars."],"url":"http://arxiv.org/abs/2405.04954v1","category":"math.CO"}
{"created":"2024-05-08 10:42:48","title":"VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context","abstract":"Large Multimodal Models (LMMs) have achieved impressive success in visual understanding and reasoning, remarkably improving the performance of mathematical reasoning in a visual context. Yet, a challenging type of visual math lies in the multimodal graph theory problem, which demands that LMMs understand the graphical structures accurately and perform multi-step reasoning on the visual graph. Additionally, exploring multimodal graph theory problems will lead to more effective strategies in fields like biology, transportation, and robotics planning. To step forward in this direction, we are the first to design a benchmark named VisionGraph, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems. It encompasses eight complex graph problem tasks, from connectivity to shortest path problems. Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning. Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance.","sentences":["Large Multimodal Models (LMMs) have achieved impressive success in visual understanding and reasoning, remarkably improving the performance of mathematical reasoning in a visual context.","Yet, a challenging type of visual math lies in the multimodal graph theory problem, which demands that LMMs understand the graphical structures accurately and perform multi-step reasoning on the visual graph.","Additionally, exploring multimodal graph theory problems will lead to more effective strategies in fields like biology, transportation, and robotics planning.","To step forward in this direction, we are the first to design a benchmark named VisionGraph, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems.","It encompasses eight complex graph problem tasks, from connectivity to shortest path problems.","Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning.","Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance."],"url":"http://arxiv.org/abs/2405.04950v1","category":"cs.CV"}
{"created":"2024-05-08 10:39:15","title":"Kiselman Minimum Principle and Rooftop Envelopes in Complex Hessian Equations","abstract":"We initiate the study of $m$-subharmonic functions with respect to a semipositive $(1,1)$-form in Euclidean domains, providing a significant element in understanding geodesics within the context of complex Hessian equations. Based on the foundational Perron envelope construction, we prove a decomposition of $m$-subharmonic solutions, and a general comparison principle that effectively manages singular Hessian measures. Additionally, we establish a rooftop equality and an analogue of the Kiselman minimum principle, which are crucial ingredients in establishing a criterion for geodesic connectivity among $m$-subharmonic functions, expressed in terms of their asymptotic envelopes.","sentences":["We initiate the study of $m$-subharmonic functions with respect to a semipositive $(1,1)$-form in Euclidean domains, providing a significant element in understanding geodesics within the context of complex Hessian equations.","Based on the foundational Perron envelope construction, we prove a decomposition of $m$-subharmonic solutions, and a general comparison principle that effectively manages singular Hessian measures.","Additionally, we establish a rooftop equality and an analogue of the Kiselman minimum principle, which are crucial ingredients in establishing a criterion for geodesic connectivity among $m$-subharmonic functions, expressed in terms of their asymptotic envelopes."],"url":"http://arxiv.org/abs/2405.04948v1","category":"math.CV"}
{"created":"2024-05-08 10:38:10","title":"The Spectral Gap of a Gaussian Quantum Markovian Generator","abstract":"Gaussian quantum Markov semigroups are the natural non-commutative extension of classical Ornstein-Uhlenbeck semigroups. They arise in open quantum systems of bosons where canonical non-commuting random variables of positions and momenta come into play. If there exits a faithful invariant density we explicitly compute the optimal exponential convergence rate, namely the spectral gap of the generator, in non-commutative $L^2$ spaces determined by the invariant density showing that the exact value is the lowest eigenvalue of a certain matrix determined by the diffusion and drift matrices. The spectral gap turns out to depend on the non-commutative $L^2$ space considered, whether the one determined by the so-called GNS or KMS multiplication by the square root of the invariant density. In the first case, it is strictly positive if and only if there is the maximum number of linearly independent noises. While, we exhibit explicit examples in which it is strictly positive only with KMS multiplication. We do not assume any symmetry or quantum detailed balance condition with respect to the invariant density.","sentences":["Gaussian quantum Markov semigroups are the natural non-commutative extension of classical Ornstein-Uhlenbeck semigroups.","They arise in open quantum systems of bosons where canonical non-commuting random variables of positions and momenta come into play.","If there exits a faithful invariant density we explicitly compute the optimal exponential convergence rate, namely the spectral gap of the generator, in non-commutative $L^2$ spaces determined by the invariant density showing that the exact value is the lowest eigenvalue of a certain matrix determined by the diffusion and drift matrices.","The spectral gap turns out to depend on the non-commutative $L^2$ space considered, whether the one determined by the so-called GNS or KMS multiplication by the square root of the invariant density.","In the first case, it is strictly positive if and only if there is the maximum number of linearly independent noises.","While, we exhibit explicit examples in which it is strictly positive only with KMS multiplication.","We do not assume any symmetry or quantum detailed balance condition with respect to the invariant density."],"url":"http://arxiv.org/abs/2405.04947v1","category":"math.FA"}
{"created":"2024-05-08 10:28:20","title":"A Sparse Tensor Generator with Efficient Feature Extraction","abstract":"Sparse tensor operations are gaining attention in emerging applications such as social networks, deep learning, diagnosis, crime, and review analysis. However, a major obstacle for research in sparse tensor operations is the deficiency of a broad-scale sparse tensor dataset. Another challenge in sparse tensor operations is examining the sparse tensor features, which are not only important for revealing its nonzero pattern but also have a significant impact on determining the best-suited storage format, the decomposition algorithm, and the reordering methods. However, due to the large sizes of real tensors, even extracting these features becomes costly without caution. To address these gaps in the literature, we have developed a smart sparse tensor generator that mimics the substantial features of real sparse tensors. Moreover, we propose various methods for efficiently extracting an extensive set of features for sparse tensors. The effectiveness of our generator is validated through the quality of features and the performance of decomposition in the generated tensors. Both the sparse tensor feature extractor and the tensor generator are open source with all the artifacts available at https://github.com/sparcityeu/feaTen and https://github.com/sparcityeu/genTen, respectively.","sentences":["Sparse tensor operations are gaining attention in emerging applications such as social networks, deep learning, diagnosis, crime, and review analysis.","However, a major obstacle for research in sparse tensor operations is the deficiency of a broad-scale sparse tensor dataset.","Another challenge in sparse tensor operations is examining the sparse tensor features, which are not only important for revealing its nonzero pattern but also have a significant impact on determining the best-suited storage format, the decomposition algorithm, and the reordering methods.","However, due to the large sizes of real tensors, even extracting these features becomes costly without caution.","To address these gaps in the literature, we have developed a smart sparse tensor generator that mimics the substantial features of real sparse tensors.","Moreover, we propose various methods for efficiently extracting an extensive set of features for sparse tensors.","The effectiveness of our generator is validated through the quality of features and the performance of decomposition in the generated tensors.","Both the sparse tensor feature extractor and the tensor generator are open source with all the artifacts available at https://github.com/sparcityeu/feaTen and https://github.com/sparcityeu/genTen, respectively."],"url":"http://arxiv.org/abs/2405.04944v1","category":"cs.MS"}
{"created":"2024-05-08 10:22:49","title":"Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs","abstract":"Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known. Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets. While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods. We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values. These novel semantics for RPOMDPS give access to results for the widely studied POSG model; concretely, we show the existence of a Nash equilibrium. Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate.","sentences":["Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known.","Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets.","While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods.","We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values.","These novel semantics for RPOMDPS give access to results for the widely studied POSG model; concretely, we show the existence of a Nash equilibrium.","Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate."],"url":"http://arxiv.org/abs/2405.04941v1","category":"cs.AI"}
{"created":"2024-05-08 10:15:04","title":"Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID","abstract":"Text-to-image person re-identification (ReID) retrieves pedestrian images according to textual descriptions. Manually annotating textual descriptions is time-consuming, restricting the scale of existing datasets and therefore the generalization ability of ReID models. As a result, we study the transferable text-to-image ReID problem, where we train a model on our proposed large-scale database and directly deploy it to various datasets for evaluation. We obtain substantial training data via Multi-modal Large Language Models (MLLMs). Moreover, we identify and address two key challenges in utilizing the obtained textual descriptions. First, an MLLM tends to generate descriptions with similar structures, causing the model to overfit specific sentence patterns. Thus, we propose a novel method that uses MLLMs to caption images according to various templates. These templates are obtained using a multi-turn dialogue with a Large Language Model (LLM). Therefore, we can build a large-scale dataset with diverse textual descriptions. Second, an MLLM may produce incorrect descriptions. Hence, we introduce a novel method that automatically identifies words in a description that do not correspond with the image. This method is based on the similarity between one text and all patch token embeddings in the image. Then, we mask these words with a larger probability in the subsequent training epoch, alleviating the impact of noisy textual descriptions. The experimental results demonstrate that our methods significantly boost the direct transfer text-to-image ReID performance. Benefiting from the pre-trained model weights, we also achieve state-of-the-art performance in the traditional evaluation settings.","sentences":["Text-to-image person re-identification (ReID) retrieves pedestrian images according to textual descriptions.","Manually annotating textual descriptions is time-consuming, restricting the scale of existing datasets and therefore the generalization ability of ReID models.","As a result, we study the transferable text-to-image ReID problem, where we train a model on our proposed large-scale database and directly deploy it to various datasets for evaluation.","We obtain substantial training data via Multi-modal Large Language Models (MLLMs).","Moreover, we identify and address two key challenges in utilizing the obtained textual descriptions.","First, an MLLM tends to generate descriptions with similar structures, causing the model to overfit specific sentence patterns.","Thus, we propose a novel method that uses MLLMs to caption images according to various templates.","These templates are obtained using a multi-turn dialogue with a Large Language Model (LLM).","Therefore, we can build a large-scale dataset with diverse textual descriptions.","Second, an MLLM may produce incorrect descriptions.","Hence, we introduce a novel method that automatically identifies words in a description that do not correspond with the image.","This method is based on the similarity between one text and all patch token embeddings in the image.","Then, we mask these words with a larger probability in the subsequent training epoch, alleviating the impact of noisy textual descriptions.","The experimental results demonstrate that our methods significantly boost the direct transfer text-to-image ReID performance.","Benefiting from the pre-trained model weights, we also achieve state-of-the-art performance in the traditional evaluation settings."],"url":"http://arxiv.org/abs/2405.04940v1","category":"cs.CV"}
{"created":"2024-05-08 10:10:24","title":"Fault Identification Enhancement with Reinforcement Learning (FIERL)","abstract":"This letter presents a novel approach in the field of Active Fault Detection (AFD), by explicitly separating the task into two parts: Passive Fault Detection (PFD) and control input design. This formulation is very general, and most existing AFD literature can be viewed through this lens. By recognizing this separation, PFD methods can be leveraged to provide components that make efficient use of the available information, while the control input is designed in order to optimize the gathering of information. The core contribution of this work is FIERL, a general simulation-based approach for the design of such control strategies, using Constrained Reinforcement Learning (CRL) to optimize the performance of arbitrary passive detectors. The control policy is learned without the need of knowing the passive detector inner workings, making FIERL broadly applicable. However, it is especially useful when paired with the design of an efficient passive component. Unlike most AFD approaches, FIERL can handle fairly complex scenarios such as continuous sets of fault modes. The effectiveness of FIERL is tested on a benchmark problem for actuator fault diagnosis, where FIERL is shown to be fairly robust, being able to generalize to fault dynamics not seen in training.","sentences":["This letter presents a novel approach in the field of Active Fault Detection (AFD), by explicitly separating the task into two parts: Passive Fault Detection (PFD) and control input design.","This formulation is very general, and most existing AFD literature can be viewed through this lens.","By recognizing this separation, PFD methods can be leveraged to provide components that make efficient use of the available information, while the control input is designed in order to optimize the gathering of information.","The core contribution of this work is FIERL, a general simulation-based approach for the design of such control strategies, using Constrained Reinforcement Learning (CRL) to optimize the performance of arbitrary passive detectors.","The control policy is learned without the need of knowing the passive detector inner workings, making FIERL broadly applicable.","However, it is especially useful when paired with the design of an efficient passive component.","Unlike most AFD approaches, FIERL can handle fairly complex scenarios such as continuous sets of fault modes.","The effectiveness of FIERL is tested on a benchmark problem for actuator fault diagnosis, where FIERL is shown to be fairly robust, being able to generalize to fault dynamics not seen in training."],"url":"http://arxiv.org/abs/2405.04938v1","category":"cs.LG"}
{"created":"2024-05-08 10:08:45","title":"Developing trustworthy AI applications with foundation models","abstract":"The trustworthiness of AI applications has been the subject of recent research and is also addressed in the EU's recently adopted AI Regulation. The currently emerging foundation models in the field of text, speech and image processing offer completely new possibilities for developing AI applications. This whitepaper shows how the trustworthiness of an AI application developed with foundation models can be evaluated and ensured. For this purpose, the application-specific, risk-based approach for testing and ensuring the trustworthiness of AI applications, as developed in the 'AI Assessment Catalog - Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is transferred to the context of foundation models. Special consideration is given to the fact that specific risks of foundation models can have an impact on the AI application and must also be taken into account when checking trustworthiness. Chapter 1 of the white paper explains the fundamental relationship between foundation models and AI applications based on them in terms of trustworthiness. Chapter 2 provides an introduction to the technical construction of foundation models and Chapter 3 shows how AI applications can be developed based on them. Chapter 4 provides an overview of the resulting risks regarding trustworthiness. Chapter 5 shows which requirements for AI applications and foundation models are to be expected according to the draft of the European Union's AI Regulation and Chapter 6 finally shows the system and procedure for meeting trustworthiness requirements.","sentences":["The trustworthiness of AI applications has been the subject of recent research and is also addressed in the EU's recently adopted AI Regulation.","The currently emerging foundation models in the field of text, speech and image processing offer completely new possibilities for developing AI applications.","This whitepaper shows how the trustworthiness of an AI application developed with foundation models can be evaluated and ensured.","For this purpose, the application-specific, risk-based approach for testing and ensuring the trustworthiness of AI applications, as developed in the 'AI Assessment Catalog - Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is transferred to the context of foundation models.","Special consideration is given to the fact that specific risks of foundation models can have an impact on the AI application and must also be taken into account when checking trustworthiness.","Chapter 1 of the white paper explains the fundamental relationship between foundation models and AI applications based on them in terms of trustworthiness.","Chapter 2 provides an introduction to the technical construction of foundation models and Chapter 3 shows how AI applications can be developed based on them.","Chapter 4 provides an overview of the resulting risks regarding trustworthiness.","Chapter 5 shows which requirements for AI applications and foundation models are to be expected according to the draft of the European Union's AI Regulation and Chapter 6 finally shows the system and procedure for meeting trustworthiness requirements."],"url":"http://arxiv.org/abs/2405.04937v1","category":"cs.AI"}
{"created":"2024-05-08 10:00:33","title":"The Importance of Being Symmetric: Flat Rotation Curves from Exact Axisymmetric Static Vacuum Spacetimes","abstract":"Starting from the vacuum Einstein Field Equations and a static axisymmetric ansatz, we find two new solutions describing an axisymmetric static vacuum spacetime with cylindrical symmetry: One of this exhibits an additional symmetry in $z$-direction and the other has $z$-coordinate dependent coefficients. In analogy to the Schwarzschild solution, these metrics describe a static vacuum spacetime and apply in similar settings except for the changed symmetry conditions. Analyzing the low-velocity limit corresponding to the Newtonian approximation of the Schwarzschild metric, we find an effective logarithmic potential. This yields flat rotation curves for test particles undergoing rotational motion within the spacetime described by the line elements, in contrast to Newtonian rotation curves. This analysis highlights how important the symmetry assumptions are for deriving general relativistic solutions.   One example of physical objects that are generally described in the static vacuum low-velocity limit (reducing to Newtonian gravity in the spherically symmetric case) and exhibit axial symmetry are disk galaxies. We show that symmetries and appropriate line elements that respect them are crucial to consider in such settings. In particular, the solutions presented here result in flat rotation curves without any need for dark matter. While these exact solutions are limited to static vacuum spacetimes, their application to physical galaxies relies on appropriate approximations. Nonetheless, they offer valuable insights into explanations for flat rotation curves in galaxies and their implications for dark matter.","sentences":["Starting from the vacuum Einstein Field Equations and a static axisymmetric ansatz, we find two new solutions describing an axisymmetric static vacuum spacetime with cylindrical symmetry: One of this exhibits an additional symmetry in $z$-direction and the other has $z$-coordinate dependent coefficients.","In analogy to the Schwarzschild solution, these metrics describe a static vacuum spacetime and apply in similar settings except for the changed symmetry conditions.","Analyzing the low-velocity limit corresponding to the Newtonian approximation of the Schwarzschild metric, we find an effective logarithmic potential.","This yields flat rotation curves for test particles undergoing rotational motion within the spacetime described by the line elements, in contrast to Newtonian rotation curves.","This analysis highlights how important the symmetry assumptions are for deriving general relativistic solutions.   ","One example of physical objects that are generally described in the static vacuum low-velocity limit (reducing to Newtonian gravity in the spherically symmetric case) and exhibit axial symmetry are disk galaxies.","We show that symmetries and appropriate line elements that respect them are crucial to consider in such settings.","In particular, the solutions presented here result in flat rotation curves without any need for dark matter.","While these exact solutions are limited to static vacuum spacetimes, their application to physical galaxies relies on appropriate approximations.","Nonetheless, they offer valuable insights into explanations for flat rotation curves in galaxies and their implications for dark matter."],"url":"http://arxiv.org/abs/2405.04933v1","category":"gr-qc"}
{"created":"2024-05-08 09:56:53","title":"FIGRET: Fine-Grained Robustness-Enhanced Traffic Engineering","abstract":"Traffic Engineering (TE) is critical for improving network performance and reliability. A key challenge in TE is the management of sudden traffic bursts. Existing TE schemes often struggle to accurately determine the extent of focus required for these surges, thereby facing difficulties in achieving a balance between performance under normal and peak traffic conditions. To address this issue, we introduce FIGRET, a Fine-Grained Robustness-Enhanced TE Scheme. FIGRET offers a novel approach to TE by providing varying levels of robustness enhancements, customized according to the distinct traffic characteristics of various source-destination pairs. By leveraging a sophisticated loss function and advanced deep learning techniques, FIGRET is capable of generating high-quality TE solutions efficiently. Our evaluations of real-world production networks, including Wide Area Networks and data centers, demonstrate that FIGRET significantly outperforms existing TE schemes. Compared to the TE scheme currently deployed in the Jupiter network of Google, FIGRET achieves a 9\\%-34\\% reduction in average Maximum Link Utilization and improves solution speed by $35\\times$-$1800 \\times$. Against DOTE, a state-of-the-art deep learning-based TE method, FIGRET substantially lowers the occurrence of significant congestion events triggered by traffic bursts by 41\\%-53.9\\% in topologies characterized by high traffic dynamics.","sentences":["Traffic Engineering (TE) is critical for improving network performance and reliability.","A key challenge in TE is the management of sudden traffic bursts.","Existing TE schemes often struggle to accurately determine the extent of focus required for these surges, thereby facing difficulties in achieving a balance between performance under normal and peak traffic conditions.","To address this issue, we introduce FIGRET, a Fine-Grained Robustness-Enhanced TE Scheme.","FIGRET offers a novel approach to TE by providing varying levels of robustness enhancements, customized according to the distinct traffic characteristics of various source-destination pairs.","By leveraging a sophisticated loss function and advanced deep learning techniques, FIGRET is capable of generating high-quality TE solutions efficiently.","Our evaluations of real-world production networks, including Wide Area Networks and data centers, demonstrate that FIGRET significantly outperforms existing TE schemes.","Compared to the TE scheme currently deployed in the Jupiter network of Google, FIGRET achieves a 9\\%-34\\% reduction in average Maximum Link Utilization and improves solution speed by $35\\times$-$1800 \\times$. Against DOTE, a state-of-the-art deep learning-based TE method, FIGRET substantially lowers the occurrence of significant congestion events triggered by traffic bursts by 41\\%-53.9\\% in topologies characterized by high traffic dynamics."],"url":"http://arxiv.org/abs/2405.04932v1","category":"cs.NI"}
{"created":"2024-05-08 09:54:46","title":"A joint model for DHS and MICS surveys: Spatial modeling with anonymized locations","abstract":"Anonymizing the GPS locations of observations can bias a spatial model's parameter estimates and attenuate spatial predictions when improperly accounted for, and is relevant in applications from public health to paleoseismology. In this work, we demonstrate that a newly introduced method for geostatistical modeling in the presence of anonymized point locations can be extended to account for more general kinds of positional uncertainty due to location anonymization, including both jittering (a form of random perturbations of GPS coordinates) and geomasking (reporting only the name of the area containing the true GPS coordinates). We further provide a numerical integration scheme that flexibly accounts for the positional uncertainty as well as spatial and covariate information.   We apply the method to women's secondary education completion data in the 2018 Nigeria demographic and health survey (NDHS) containing jittered point locations, and the 2016 Nigeria multiple indicator cluster survey (NMICS) containing geomasked locations. We show that accounting for the positional uncertainty in the surveys can improve predictions in terms of their continuous rank probability score.","sentences":["Anonymizing the GPS locations of observations can bias a spatial model's parameter estimates and attenuate spatial predictions when improperly accounted for, and is relevant in applications from public health to paleoseismology.","In this work, we demonstrate that a newly introduced method for geostatistical modeling in the presence of anonymized point locations can be extended to account for more general kinds of positional uncertainty due to location anonymization, including both jittering (a form of random perturbations of GPS coordinates) and geomasking (reporting only the name of the area containing the true GPS coordinates).","We further provide a numerical integration scheme that flexibly accounts for the positional uncertainty as well as spatial and covariate information.   ","We apply the method to women's secondary education completion data in the 2018 Nigeria demographic and health survey (NDHS) containing jittered point locations, and the 2016 Nigeria multiple indicator cluster survey (NMICS) containing geomasked locations.","We show that accounting for the positional uncertainty in the surveys can improve predictions in terms of their continuous rank probability score."],"url":"http://arxiv.org/abs/2405.04928v1","category":"stat.ME"}
{"created":"2024-05-08 09:51:32","title":"The many colors of the TNG100 simulation","abstract":"We apply the 3D dust radiative transfer code SKIRT to the low-redshift ($z\\leq0.1$) galaxy population in the TNG100 cosmological simulation, the fiducial run of the IllustrisTNG project. We compute global fluxes and spectral energy distributions (SEDs) from the far-ultraviolet to the sub-millimeter for $\\approx\\,$60 000 galaxies. Our post-processing methodology follows the study of Tr\\v{c}ka et al. (2022) of the higher-resolution TNG50 simulation. We verify that TNG100 reproduces observational luminosity functions at low redshifts to excellent precision, unlike TNG50. Additionally, we test the realism of our TNG100 plus SKIRT fluxes by comparing various flux and color relations to data from the GAMA survey. TNG100 broadly reproduces the observed distributions, but we predict ultraviolet colors that are too blue by $\\approx\\,$0.4 mag, possibly related to the extinction in the star-forming regions subgrid model not being selective enough. Furthermore, we find that the simulated galaxies exhibit mid-infrared fluxes elevated by up to $\\approx\\,$0.5 mag that we attribute to overly effective stochastic heating of the diffuse dust. All synthetic broadband fluxes and SEDs are made publicly available in three orientations and four apertures, and can readily be used to study TNG100 galaxies in a mock observational fashion.","sentences":["We apply the 3D dust radiative transfer code SKIRT to the low-redshift ($z\\leq0.1$) galaxy population in the TNG100 cosmological simulation, the fiducial run of the IllustrisTNG project.","We compute global fluxes and spectral energy distributions (SEDs) from the far-ultraviolet to the sub-millimeter for $\\approx\\,$60 000 galaxies.","Our post-processing methodology follows the study of Tr\\v{c}ka et al. (2022) of the higher-resolution TNG50 simulation.","We verify that TNG100 reproduces observational luminosity functions at low redshifts to excellent precision, unlike TNG50.","Additionally, we test the realism of our TNG100 plus SKIRT fluxes by comparing various flux and color relations to data from the GAMA survey.","TNG100 broadly reproduces the observed distributions, but we predict ultraviolet colors that are too blue by $\\approx\\,$0.4 mag, possibly related to the extinction in the star-forming regions subgrid model not being selective enough.","Furthermore, we find that the simulated galaxies exhibit mid-infrared fluxes elevated by up to $\\approx\\,$0.5 mag that we attribute to overly effective stochastic heating of the diffuse dust.","All synthetic broadband fluxes and SEDs are made publicly available in three orientations and four apertures, and can readily be used to study TNG100 galaxies in a mock observational fashion."],"url":"http://arxiv.org/abs/2405.04925v1","category":"astro-ph.GA"}
{"created":"2024-05-08 09:46:25","title":"Comparison of two different integration methods for the (1+1)-Dimensional Schro\u00f6dinger-Poisson Equation","abstract":"We compare two different numerical methods to integrate in time spatially delocalized initial densities using the Schr\\\"odinger-Poisson equation system as the evolution law. The basic equation is a nonlinear Schr\\\"odinger equation with an auto-gravitating potential created by the wave function density itself. The latter is determined as a solution of Poisson's equation modelling, e.g., non-relativistic gravity. For reasons of complexity, we treat a one-dimensional version of the problem whose numerical integration is still challenging because of the extreme long-range forces (being constant in the asymptotic limit). Both of our methods, a Strang splitting scheme and a basis function approach using B-splines, are compared in numerical convergence and effectivity. Overall, our Strang-splitting evolution compares favourably with the B-spline method. In particular, by using an adaptive time-stepper rather large one-dimensional boxes can be treated. These results give hope for extensions to two spatial dimensions for not too small boxes and large evolution times necessary for describing, for instance, dark matter formation over cosmologically relevant scales.","sentences":["We compare two different numerical methods to integrate in time spatially delocalized initial densities using the Schr\\\"odinger-Poisson equation system as the evolution law.","The basic equation is a nonlinear Schr\\\"odinger equation with an auto-gravitating potential created by the wave function density itself.","The latter is determined as a solution of Poisson's equation modelling, e.g., non-relativistic gravity.","For reasons of complexity, we treat a one-dimensional version of the problem whose numerical integration is still challenging because of the extreme long-range forces (being constant in the asymptotic limit).","Both of our methods, a Strang splitting scheme and a basis function approach using B-splines, are compared in numerical convergence and effectivity.","Overall, our Strang-splitting evolution compares favourably with the B-spline method.","In particular, by using an adaptive time-stepper rather large one-dimensional boxes can be treated.","These results give hope for extensions to two spatial dimensions for not too small boxes and large evolution times necessary for describing, for instance, dark matter formation over cosmologically relevant scales."],"url":"http://arxiv.org/abs/2405.04924v1","category":"gr-qc"}
{"created":"2024-05-08 09:45:54","title":"DataSP: A Differential All-to-All Shortest Path Algorithm for Learning Costs and Predicting Paths with Context","abstract":"Learning latent costs of transitions on graphs from trajectories demonstrations under various contextual features is challenging but useful for path planning. Yet, existing methods either oversimplify cost assumptions or scale poorly with the number of observed trajectories. This paper introduces DataSP, a differentiable all-to-all shortest path algorithm to facilitate learning latent costs from trajectories. It allows to learn from a large number of trajectories in each learning step without additional computation. Complex latent cost functions from contextual features can be represented in the algorithm through a neural network approximation. We further propose a method to sample paths from DataSP in order to reconstruct/mimic observed paths' distributions. We prove that the inferred distribution follows the maximum entropy principle. We show that DataSP outperforms state-of-the-art differentiable combinatorial solver and classical machine learning approaches in predicting paths on graphs.","sentences":["Learning latent costs of transitions on graphs from trajectories demonstrations under various contextual features is challenging but useful for path planning.","Yet, existing methods either oversimplify cost assumptions or scale poorly with the number of observed trajectories.","This paper introduces DataSP, a differentiable all-to-all shortest path algorithm to facilitate learning latent costs from trajectories.","It allows to learn from a large number of trajectories in each learning step without additional computation.","Complex latent cost functions from contextual features can be represented in the algorithm through a neural network approximation.","We further propose a method to sample paths from DataSP in order to reconstruct/mimic observed paths' distributions.","We prove that the inferred distribution follows the maximum entropy principle.","We show that DataSP outperforms state-of-the-art differentiable combinatorial solver and classical machine learning approaches in predicting paths on graphs."],"url":"http://arxiv.org/abs/2405.04923v1","category":"cs.LG"}
{"created":"2024-05-08 09:42:44","title":"The simplest model of a scalarized black hole in the Einstein-Klein-Gordon theory","abstract":"We investigate scalarized black holes in the Einstein-minimally coupled scalar theory with a negative potential $V(\\phi)=-\\alpha^2\\phi^6$. The tachyonic instability is absent from analyzing the linearized scalar equation, which could not allow for spontaneous scalarization. However, we obtain the black hole solutions with scalar hair by solving three full equations because this scalar potential violates the weak energy condition. This shows clearly that scalarized black holes can be obtained without introducing a non-minimal scalar coupling term. We perform the stability analysis for scalarized black holes by adopting radial perturbations, implying that all scalarized black holes belonging to a single branch are unstable.","sentences":["We investigate scalarized black holes in the Einstein-minimally coupled scalar theory with a negative potential $V(\\phi)=-\\alpha^2\\phi^6$.","The tachyonic instability is absent from analyzing the linearized scalar equation, which could not allow for spontaneous scalarization.","However, we obtain the black hole solutions with scalar hair by solving three full equations because this scalar potential violates the weak energy condition.","This shows clearly that scalarized black holes can be obtained without introducing a non-minimal scalar coupling term.","We perform the stability analysis for scalarized black holes by adopting radial perturbations, implying that all scalarized black holes belonging to a single branch are unstable."],"url":"http://arxiv.org/abs/2405.04921v1","category":"gr-qc"}
{"created":"2024-05-08 09:42:08","title":"Impact of phylogeny on the inference of functional sectors from protein sequence data","abstract":"Statistical analysis of multiple sequence alignments of homologous proteins has revealed groups of coevolving amino acids called sectors. These groups of amino-acid sites feature collective correlations in their amino-acid usage, and they are associated to functional properties. Modeling showed that natural selection on an additive functional trait of a protein is generically expected to give rise to a functional sector. These modeling results motivated a principled method, called ICOD, which is designed to identify functional sectors, as well as mutational effects, from sequence data. However, a challenge for all methods aiming to identify sectors from multiple sequence alignments is that correlations in amino-acid usage can also arise from the mere fact that homologous sequences share common ancestry, i.e. from phylogeny. Here, we generate controlled synthetic data from a minimal model comprising both phylogeny and functional sectors. We use this data to dissect the impact of phylogeny on sector identification and on mutational effect inference by different methods. We find that ICOD is most robust to phylogeny, but that conservation is also quite robust. Next, we consider natural multiple sequence alignments of protein families for which deep mutational scan experimental data is available. We show that in this natural data, conservation and ICOD best identify sites with strong functional roles, in agreement with our results on synthetic data. Importantly, these two methods have different premises, since they respectively focus on conservation and on correlations. Thus, their joint use can reveal complementary information.","sentences":["Statistical analysis of multiple sequence alignments of homologous proteins has revealed groups of coevolving amino acids called sectors.","These groups of amino-acid sites feature collective correlations in their amino-acid usage, and they are associated to functional properties.","Modeling showed that natural selection on an additive functional trait of a protein is generically expected to give rise to a functional sector.","These modeling results motivated a principled method, called ICOD, which is designed to identify functional sectors, as well as mutational effects, from sequence data.","However, a challenge for all methods aiming to identify sectors from multiple sequence alignments is that correlations in amino-acid usage can also arise from the mere fact that homologous sequences share common ancestry, i.e. from phylogeny.","Here, we generate controlled synthetic data from a minimal model comprising both phylogeny and functional sectors.","We use this data to dissect the impact of phylogeny on sector identification and on mutational effect inference by different methods.","We find that ICOD is most robust to phylogeny, but that conservation is also quite robust.","Next, we consider natural multiple sequence alignments of protein families for which deep mutational scan experimental data is available.","We show that in this natural data, conservation and ICOD best identify sites with strong functional roles, in agreement with our results on synthetic data.","Importantly, these two methods have different premises, since they respectively focus on conservation and on correlations.","Thus, their joint use can reveal complementary information."],"url":"http://arxiv.org/abs/2405.04920v1","category":"q-bio.PE"}
{"created":"2024-05-08 09:38:16","title":"Delve into Base-Novel Confusion: Redundancy Exploration for Few-Shot Class-Incremental Learning","abstract":"Few-shot class-incremental learning (FSCIL) aims to acquire knowledge from novel classes with limited samples while retaining information about base classes. Existing methods address catastrophic forgetting and overfitting by freezing the feature extractor during novel-class learning. However, these methods usually tend to cause the confusion between base and novel classes, i.e., classifying novel-class samples into base classes. In this paper, we delve into this phenomenon to study its cause and solution. We first interpret the confusion as the collision between the novel-class and the base-class region in the feature space. Then, we find the collision is caused by the label-irrelevant redundancies within the base-class feature and pixel space. Through qualitative and quantitative experiments, we identify this redundancy as the shortcut in the base-class training, which can be decoupled to alleviate the collision. Based on this analysis, to alleviate the collision between base and novel classes, we propose a method for FSCIL named Redundancy Decoupling and Integration (RDI). RDI first decouples redundancies from base-class space to shrink the intra-base-class feature space. Then, it integrates the redundancies as a dummy class to enlarge the inter-base-class feature space. This process effectively compresses the base-class feature space, creating buffer space for novel classes and alleviating the model's confusion between the base and novel classes. Extensive experiments across benchmark datasets, including CIFAR-100, miniImageNet, and CUB-200-2011 demonstrate that our method achieves state-of-the-art performance.","sentences":["Few-shot class-incremental learning (FSCIL) aims to acquire knowledge from novel classes with limited samples while retaining information about base classes.","Existing methods address catastrophic forgetting and overfitting by freezing the feature extractor during novel-class learning.","However, these methods usually tend to cause the confusion between base and novel classes, i.e., classifying novel-class samples into base classes.","In this paper, we delve into this phenomenon to study its cause and solution.","We first interpret the confusion as the collision between the novel-class and the base-class region in the feature space.","Then, we find the collision is caused by the label-irrelevant redundancies within the base-class feature and pixel space.","Through qualitative and quantitative experiments, we identify this redundancy as the shortcut in the base-class training, which can be decoupled to alleviate the collision.","Based on this analysis, to alleviate the collision between base and novel classes, we propose a method for FSCIL named Redundancy Decoupling and Integration (RDI).","RDI first decouples redundancies from base-class space to shrink the intra-base-class feature space.","Then, it integrates the redundancies as a dummy class to enlarge the inter-base-class feature space.","This process effectively compresses the base-class feature space, creating buffer space for novel classes and alleviating the model's confusion between the base and novel classes.","Extensive experiments across benchmark datasets, including CIFAR-100, miniImageNet, and CUB-200-2011 demonstrate that our method achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.04918v1","category":"cs.CV"}
{"created":"2024-05-08 09:36:34","title":"Predicting the binding of small molecules to proteins through invariant representation of the molecular structure","abstract":"We present a computational scheme for predicting the ligands that bind to a pocket of known structure. It is based on the generation of a general abstract representation of the molecules, which is invariant to rotations, translations and permutations of atoms, and has some degree of isometry with the space of conformations. We use these representations to train a non-deep machine learning algorithm to classify the binding between pockets and molecule pairs, and show that this approach has a better generalization capability than existing methods.","sentences":["We present a computational scheme for predicting the ligands that bind to a pocket of known structure.","It is based on the generation of a general abstract representation of the molecules, which is invariant to rotations, translations and permutations of atoms, and has some degree of isometry with the space of conformations.","We use these representations to train a non-deep machine learning algorithm to classify the binding between pockets and molecule pairs, and show that this approach has a better generalization capability than existing methods."],"url":"http://arxiv.org/abs/2405.04916v1","category":"physics.chem-ph"}
{"created":"2024-05-08 09:30:59","title":"Urban Boundary Delineation from Commuting Data with Bayesian Stochastic Blockmodeling: Scale, Contiguity, and Hierarchy","abstract":"A common method for delineating urban and suburban boundaries is to identify clusters of spatial units that are highly interconnected in a network of commuting flows, each cluster signaling a cohesive economic submarket. It is critical that the clustering methods employed for this task are principled and free of unnecessary tunable parameters to avoid unwanted inductive biases while remaining scalable for high resolution mobility networks. Here we systematically assess the benefits and limitations of a wide array of Stochastic Block Models (SBMs)$\\unicode{x2014}$a family of principled, nonparametric models for identifying clusters in networks$\\unicode{x2014}$for delineating urban spatial boundaries with commuting data. We find that the data compression capability and relative performance of different SBM variants heavily depends on the spatial extent of the commuting network, its aggregation scale, and the method used for weighting network edges. We also construct a new measure to assess the degree to which community detection algorithms find spatially contiguous partitions, finding that traditional SBMs may produce substantial spatial discontiguities that make them challenging to use in general for urban boundary delineation. We propose a fast nonparametric regionalization algorithm that can alleviate this issue, achieving data compression close to that of unconstrained SBM models while ensuring spatial contiguity, benefiting from a deterministic optimization procedure, and being generalizable to a wide range of community detection objective functions.","sentences":["A common method for delineating urban and suburban boundaries is to identify clusters of spatial units that are highly interconnected in a network of commuting flows, each cluster signaling a cohesive economic submarket.","It is critical that the clustering methods employed for this task are principled and free of unnecessary tunable parameters to avoid unwanted inductive biases while remaining scalable for high resolution mobility networks.","Here we systematically assess the benefits and limitations of a wide array of Stochastic Block Models (SBMs)$\\unicode{x2014}$a family of principled, nonparametric models for identifying clusters in networks$\\unicode{x2014}$for delineating urban spatial boundaries with commuting data.","We find that the data compression capability and relative performance of different SBM variants heavily depends on the spatial extent of the commuting network, its aggregation scale, and the method used for weighting network edges.","We also construct a new measure to assess the degree to which community detection algorithms find spatially contiguous partitions, finding that traditional SBMs may produce substantial spatial discontiguities that make them challenging to use in general for urban boundary delineation.","We propose a fast nonparametric regionalization algorithm that can alleviate this issue, achieving data compression close to that of unconstrained SBM models while ensuring spatial contiguity, benefiting from a deterministic optimization procedure, and being generalizable to a wide range of community detection objective functions."],"url":"http://arxiv.org/abs/2405.04911v1","category":"physics.soc-ph"}
{"created":"2024-05-08 09:28:26","title":"Learning with Posterior Sampling for Revenue Management under Time-varying Demand","abstract":"This paper discusses the revenue management (RM) problem to maximize revenue by pricing items or services. One challenge in this problem is that the demand distribution is unknown and varies over time in real applications such as airline and retail industries. In particular, the time-varying demand has not been well studied under scenarios of unknown demand due to the difficulty of jointly managing the remaining inventory and estimating the demand. To tackle this challenge, we first introduce an episodic generalization of the RM problem motivated by typical application scenarios. We then propose a computationally efficient algorithm based on posterior sampling, which effectively optimizes prices by solving linear programming. We derive a Bayesian regret upper bound of this algorithm for general models where demand parameters can be correlated between time periods, while also deriving a regret lower bound for generic algorithms. Our empirical study shows that the proposed algorithm performs better than other benchmark algorithms and comparably to the optimal policy in hindsight. We also propose a heuristic modification of the proposed algorithm, which further efficiently learns the pricing policy in the experiments.","sentences":["This paper discusses the revenue management (RM) problem to maximize revenue by pricing items or services.","One challenge in this problem is that the demand distribution is unknown and varies over time in real applications such as airline and retail industries.","In particular, the time-varying demand has not been well studied under scenarios of unknown demand due to the difficulty of jointly managing the remaining inventory and estimating the demand.","To tackle this challenge, we first introduce an episodic generalization of the RM problem motivated by typical application scenarios.","We then propose a computationally efficient algorithm based on posterior sampling, which effectively optimizes prices by solving linear programming.","We derive a Bayesian regret upper bound of this algorithm for general models where demand parameters can be correlated between time periods, while also deriving a regret lower bound for generic algorithms.","Our empirical study shows that the proposed algorithm performs better than other benchmark algorithms and comparably to the optimal policy in hindsight.","We also propose a heuristic modification of the proposed algorithm, which further efficiently learns the pricing policy in the experiments."],"url":"http://arxiv.org/abs/2405.04910v1","category":"cs.LG"}
{"created":"2024-05-08 09:28:04","title":"Traj-LLM: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models","abstract":"Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving. Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of the complex traffic semantics. This paper proposes Traj-LLM, the first to investigate the potential of using Large Language Models (LLMs) without explicit prompt engineering to generate future motion from agents' past/observed trajectories and scene semantics. Traj-LLM starts with sparse context joint coding to dissect the agent and scene features into a form that LLMs understand. On this basis, we innovatively explore LLMs' powerful comprehension abilities to capture a spectrum of high-level scene knowledge and interactive information. Emulating the human-like lane focus cognitive function and enhancing Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the pioneering Mamba module. Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant multi-modal predictions. Extensive experiments manifest that Traj-LLM, fortified by LLMs' strong prior knowledge and understanding prowess, together with lane-aware probability learning, outstrips state-of-the-art methods across evaluation metrics. Moreover, the few-shot analysis further substantiates Traj-LLM's performance, wherein with just 50% of the dataset, it outperforms the majority of benchmarks relying on complete data utilization. This study explores equipping the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent motion in a new way.","sentences":["Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving.","Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of the complex traffic semantics.","This paper proposes Traj-LLM, the first to investigate the potential of using Large Language Models (LLMs) without explicit prompt engineering to generate future motion from agents' past/observed trajectories and scene semantics.","Traj-LLM starts with sparse context joint coding to dissect the agent and scene features into a form that LLMs understand.","On this basis, we innovatively explore LLMs' powerful comprehension abilities to capture a spectrum of high-level scene knowledge and interactive information.","Emulating the human-like lane focus cognitive function and enhancing Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the pioneering Mamba module.","Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant multi-modal predictions.","Extensive experiments manifest that Traj-LLM, fortified by LLMs' strong prior knowledge and understanding prowess, together with lane-aware probability learning, outstrips state-of-the-art methods across evaluation metrics.","Moreover, the few-shot analysis further substantiates Traj-LLM's performance, wherein with just 50% of the dataset, it outperforms the majority of benchmarks relying on complete data utilization.","This study explores equipping the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent motion in a new way."],"url":"http://arxiv.org/abs/2405.04909v1","category":"cs.CV"}
{"created":"2024-05-08 09:24:51","title":"Empowering Wireless Networks with Artificial Intelligence Generated Graph","abstract":"In wireless communications, transforming network into graphs and processing them using deep learning models, such as Graph Neural Networks (GNNs), is one of the mainstream network optimization approaches. While effective, the generative AI (GAI) shows stronger capabilities in graph analysis, processing, and generation, than conventional methods such as GNN, offering a broader exploration space for graph-based network optimization. Therefore, this article proposes to use GAI-based graph generation to support wireless networks. Specifically, we first explore applications of graphs in wireless networks. Then, we introduce and analyze common GAI models from the perspective of graph generation. On this basis, we propose a framework that incorporates the conditional diffusion model and an evaluation network, which can be trained with reward functions and conditions customized by network designers and users. Once trained, the proposed framework can create graphs based on new conditions, helping to tackle problems specified by the user in wireless networks. Finally, using the link selection in integrated sensing and communication (ISAC) as an example, the effectiveness of the proposed framework is validated.","sentences":["In wireless communications, transforming network into graphs and processing them using deep learning models, such as Graph Neural Networks (GNNs), is one of the mainstream network optimization approaches.","While effective, the generative AI (GAI) shows stronger capabilities in graph analysis, processing, and generation, than conventional methods such as GNN, offering a broader exploration space for graph-based network optimization.","Therefore, this article proposes to use GAI-based graph generation to support wireless networks.","Specifically, we first explore applications of graphs in wireless networks.","Then, we introduce and analyze common GAI models from the perspective of graph generation.","On this basis, we propose a framework that incorporates the conditional diffusion model and an evaluation network, which can be trained with reward functions and conditions customized by network designers and users.","Once trained, the proposed framework can create graphs based on new conditions, helping to tackle problems specified by the user in wireless networks.","Finally, using the link selection in integrated sensing and communication (ISAC) as an example, the effectiveness of the proposed framework is validated."],"url":"http://arxiv.org/abs/2405.04907v1","category":"cs.NI"}
{"created":"2024-05-08 09:20:58","title":"Dependence-based fuzzy clustering of functional time series","abstract":"Time series clustering is an important data mining task with a wide variety of applications. While most methods focus on time series taking values on the real line, very few works consider functional time series. However, functional objects frequently arise in many fields, such as actuarial science, demography or finance. Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space. In this paper, the problem of clustering functional time series is addressed. To this aim, a distance between functional time series is introduced and used to construct a clustering procedure. The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting. Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning. Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient. Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach.","sentences":["Time series clustering is an important data mining task with a wide variety of applications.","While most methods focus on time series taking values on the real line, very few works consider functional time series.","However, functional objects frequently arise in many fields, such as actuarial science, demography or finance.","Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space.","In this paper, the problem of clustering functional time series is addressed.","To this aim, a distance between functional time series is introduced and used to construct a clustering procedure.","The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting.","Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees.","The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning.","Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient.","Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach."],"url":"http://arxiv.org/abs/2405.04904v1","category":"stat.ME"}
{"created":"2024-05-08 09:16:54","title":"Imbalanced Graph Classification with Multi-scale Oversampling Graph Neural Networks","abstract":"One main challenge in imbalanced graph classification is to learn expressive representations of the graphs in under-represented (minority) classes. Existing generic imbalanced learning methods, such as oversampling and imbalanced learning loss functions, can be adopted for enabling graph representation learning models to cope with this challenge. However, these methods often directly operate on the graph representations, ignoring rich discriminative information within the graphs and their interactions. To tackle this issue, we introduce a novel multi-scale oversampling graph neural network (MOSGNN) that learns expressive minority graph representations based on intra- and inter-graph semantics resulting from oversampled graphs at multiple scales - subgraph, graph, and pairwise graphs. It achieves this by jointly optimizing subgraph-level, graph-level, and pairwise-graph learning tasks to learn the discriminative information embedded within and between the minority graphs. Extensive experiments on 16 imbalanced graph datasets show that MOSGNN i) significantly outperforms five state-of-the-art models, and ii) offers a generic framework, in which different advanced imbalanced learning loss functions can be easily plugged in and obtain significantly improved classification performance.","sentences":["One main challenge in imbalanced graph classification is to learn expressive representations of the graphs in under-represented (minority) classes.","Existing generic imbalanced learning methods, such as oversampling and imbalanced learning loss functions, can be adopted for enabling graph representation learning models to cope with this challenge.","However, these methods often directly operate on the graph representations, ignoring rich discriminative information within the graphs and their interactions.","To tackle this issue, we introduce a novel multi-scale oversampling graph neural network (MOSGNN) that learns expressive minority graph representations based on intra- and inter-graph semantics resulting from oversampled graphs at multiple scales - subgraph, graph, and pairwise graphs.","It achieves this by jointly optimizing subgraph-level, graph-level, and pairwise-graph learning tasks to learn the discriminative information embedded within and between the minority graphs.","Extensive experiments on 16 imbalanced graph datasets show that MOSGNN i) significantly outperforms five state-of-the-art models, and ii) offers a generic framework, in which different advanced imbalanced learning loss functions can be easily plugged in and obtain significantly improved classification performance."],"url":"http://arxiv.org/abs/2405.04903v1","category":"cs.LG"}
{"created":"2024-05-08 09:13:42","title":"HAGAN: Hybrid Augmented Generative Adversarial Network for Medical Image Synthesis","abstract":"Medical Image Synthesis (MIS) plays an important role in the intelligent medical field, which greatly saves the economic and time costs of medical diagnosis. However, due to the complexity of medical images and similar characteristics of different tissue cells, existing methods face great challenges in meeting their biological consistency. To this end, we propose the Hybrid Augmented Generative Adversarial Network (HAGAN) to maintain the authenticity of structural texture and tissue cells. HAGAN contains Attention Mixed (AttnMix) Generator, Hierarchical Discriminator and Reverse Skip Connection between Discriminator and Generator. The AttnMix consistency differentiable regularization encourages the perception in structural and textural variations between real and fake images, which improves the pathological integrity of synthetic images and the accuracy of features in local areas. The Hierarchical Discriminator introduces pixel-by-pixel discriminant feedback to generator for enhancing the saliency and discriminance of global and local details simultaneously. The Reverse Skip Connection further improves the accuracy for fine details by fusing real and synthetic distribution features. Our experimental evaluations on three datasets of different scales, i.e., COVID-CT, ACDC and BraTS2018, demonstrate that HAGAN outperforms the existing methods and achieves state-of-the-art performance in both high-resolution and low-resolution.","sentences":["Medical Image Synthesis (MIS) plays an important role in the intelligent medical field, which greatly saves the economic and time costs of medical diagnosis.","However, due to the complexity of medical images and similar characteristics of different tissue cells, existing methods face great challenges in meeting their biological consistency.","To this end, we propose the Hybrid Augmented Generative Adversarial Network (HAGAN) to maintain the authenticity of structural texture and tissue cells.","HAGAN contains Attention Mixed (AttnMix) Generator, Hierarchical Discriminator and Reverse Skip Connection between Discriminator and Generator.","The AttnMix consistency differentiable regularization encourages the perception in structural and textural variations between real and fake images, which improves the pathological integrity of synthetic images and the accuracy of features in local areas.","The Hierarchical Discriminator introduces pixel-by-pixel discriminant feedback to generator for enhancing the saliency and discriminance of global and local details simultaneously.","The Reverse Skip Connection further improves the accuracy for fine details by fusing real and synthetic distribution features.","Our experimental evaluations on three datasets of different scales, i.e., COVID-CT, ACDC and BraTS2018, demonstrate that HAGAN outperforms the existing methods and achieves state-of-the-art performance in both high-resolution and low-resolution."],"url":"http://arxiv.org/abs/2405.04902v1","category":"eess.IV"}
{"created":"2024-05-08 09:13:10","title":"Self-supervised Gait-based Emotion Representation Learning from Selective Strongly Augmented Skeleton Sequences","abstract":"Emotion recognition is an important part of affective computing. Extracting emotional cues from human gaits yields benefits such as natural interaction, a nonintrusive nature, and remote detection. Recently, the introduction of self-supervised learning techniques offers a practical solution to the issues arising from the scarcity of labeled data in the field of gait-based emotion recognition. However, due to the limited diversity of gaits and the incompleteness of feature representations for skeletons, the existing contrastive learning methods are usually inefficient for the acquisition of gait emotions. In this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for self-supervised gait-based emotion representation, which aims to derive effective representations from limited labeled gait data. First, we propose an SSA method for the gait emotion recognition task, which includes upper body jitter and random spatiotemporal mask. The goal of SSA is to generate more diverse and targeted positive samples and prompt the model to learn more distinctive and robust feature representations. Then, we design a complementary feature fusion network (CFFN) that facilitates the integration of cross-domain information to acquire topological structural and global adaptive features. Finally, we implement the distributional divergence minimization loss to supervise the representation learning of the generally and strongly augmented queries. Our approach is validated on the Emotion-Gait (E-Gait) and Emilya datasets and outperforms the state-of-the-art methods under different evaluation protocols.","sentences":["Emotion recognition is an important part of affective computing.","Extracting emotional cues from human gaits yields benefits such as natural interaction, a nonintrusive nature, and remote detection.","Recently, the introduction of self-supervised learning techniques offers a practical solution to the issues arising from the scarcity of labeled data in the field of gait-based emotion recognition.","However, due to the limited diversity of gaits and the incompleteness of feature representations for skeletons, the existing contrastive learning methods are usually inefficient for the acquisition of gait emotions.","In this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for self-supervised gait-based emotion representation, which aims to derive effective representations from limited labeled gait data.","First, we propose an SSA method for the gait emotion recognition task, which includes upper body jitter and random spatiotemporal mask.","The goal of SSA is to generate more diverse and targeted positive samples and prompt the model to learn more distinctive and robust feature representations.","Then, we design a complementary feature fusion network (CFFN) that facilitates the integration of cross-domain information to acquire topological structural and global adaptive features.","Finally, we implement the distributional divergence minimization loss to supervise the representation learning of the generally and strongly augmented queries.","Our approach is validated on the Emotion-Gait (E-Gait) and Emilya datasets and outperforms the state-of-the-art methods under different evaluation protocols."],"url":"http://arxiv.org/abs/2405.04900v1","category":"cs.CV"}
{"created":"2024-05-08 09:05:02","title":"Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset","abstract":"Recent social media posts on the cholera outbreak in Hammanskraal have highlighted the diverse range of emotions people experienced in response to such an event. The extent of people's opinions varies greatly depending on their level of knowledge and information about the disease. The documented re-search about Cholera lacks investigations into the classification of emotions. This study aims to examine the emotions expressed in social media posts about Chol-era. A dataset of 23,000 posts was extracted and pre-processed. The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer library was applied to deter-mine the emotional significance of each text. Additionally, Machine Learning (ML) models were applied for emotion classification, including Long short-term memory (LSTM), Logistic regression, Decision trees, and the Bidirectional En-coder Representations from Transformers (BERT) model. The results of this study demonstrated that LSTM achieved the highest accuracy of 75%. Emotion classification presents a promising tool for gaining a deeper understanding of the impact of Cholera on society. The findings of this study might contribute to the development of effective interventions in public health strategies.","sentences":["Recent social media posts on the cholera outbreak in Hammanskraal have highlighted the diverse range of emotions people experienced in response to such an event.","The extent of people's opinions varies greatly depending on their level of knowledge and information about the disease.","The documented re-search about Cholera lacks investigations into the classification of emotions.","This study aims to examine the emotions expressed in social media posts about Chol-era.","A dataset of 23,000 posts was extracted and pre-processed.","The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer library was applied to deter-mine the emotional significance of each text.","Additionally, Machine Learning (ML) models were applied for emotion classification, including Long short-term memory (LSTM), Logistic regression, Decision trees, and the Bidirectional En-coder Representations from Transformers (BERT) model.","The results of this study demonstrated that LSTM achieved the highest accuracy of 75%.","Emotion classification presents a promising tool for gaining a deeper understanding of the impact of Cholera on society.","The findings of this study might contribute to the development of effective interventions in public health strategies."],"url":"http://arxiv.org/abs/2405.04897v1","category":"cs.CL"}
{"created":"2024-05-08 09:04:46","title":"Verified authors shape X/Twitter discursive communities","abstract":"Community detection algorithms try to extract a mesoscale structure from the available network data, generally avoiding any explicit assumption regarding the quantity and quality of information conveyed by specific sets of edges. In this paper, we show that the core of ideological/discursive communities on X/Twitter can be effectively identified by uncovering the most informative interactions in an authors-audience bipartite network through a maximum-entropy null model. The analysis is performed considering three X/Twitter datasets related to the main political events of 2022 in Italy, using as benchmarks four state-of-the-art algorithms - three descriptive, one inferential -, and manually annotating nearly 300 verified users based on their political affiliation. In terms of information content, the communities obtained with the entropy-based algorithm are comparable to those obtained with some of the benchmarks. However, such a methodology on the authors-audience bipartite network: uses just a small sample of the available data to identify the central users of each community; returns a neater partition of the user set in just a few, easy to interpret, communities; clusters well-known political figures in a way that better matches the political alliances when compared with the benchmarks. Our results provide an important insight into online debates, highlighting that online interaction networks are mostly shaped by the activity of a small set of users who enjoy public visibility even outside social media.","sentences":["Community detection algorithms try to extract a mesoscale structure from the available network data, generally avoiding any explicit assumption regarding the quantity and quality of information conveyed by specific sets of edges.","In this paper, we show that the core of ideological/discursive communities on X/Twitter can be effectively identified by uncovering the most informative interactions in an authors-audience bipartite network through a maximum-entropy null model.","The analysis is performed considering three X/Twitter datasets related to the main political events of 2022 in Italy, using as benchmarks four state-of-the-art algorithms - three descriptive, one inferential -, and manually annotating nearly 300 verified users based on their political affiliation.","In terms of information content, the communities obtained with the entropy-based algorithm are comparable to those obtained with some of the benchmarks.","However, such a methodology on the authors-audience bipartite network: uses just a small sample of the available data to identify the central users of each community; returns a neater partition of the user set in just a few, easy to interpret, communities; clusters well-known political figures in a way that better matches the political alliances when compared with the benchmarks.","Our results provide an important insight into online debates, highlighting that online interaction networks are mostly shaped by the activity of a small set of users who enjoy public visibility even outside social media."],"url":"http://arxiv.org/abs/2405.04896v1","category":"cs.SI"}
{"created":"2024-05-08 09:03:48","title":"On Correlation and Prediction Interval Reduction","abstract":"Pearson's correlation coefficient is a popular statistical measure to summarize the strength of association between two continuous variables. It is usually interpreted via its square as percentage of variance of one variable predicted by the other in a linear regression model. It can be generalized for multiple regression via the coefficient of determination, which is not straightforward to interpret in terms of prediction accuracy. In this paper, we propose to assess the prediction accuracy of a linear model via the prediction interval reduction (PIR) by comparing the width of the prediction interval derived from this model with the width of the prediction interval obtained without this model. At the population level, PIR is one-to-one related to the correlation and the coefficient of determination. In particular, a correlation of 0.5 corresponds to a PIR of only 13%. It is also the one's complement of the coefficient of alienation introduced at the beginning of last century. We argue that PIR is easily interpretable and useful to keep in mind how difficult it is to make accurate individual predictions, an important message in the era of precision medicine and artificial intelligence. Different estimates of PIR are compared in the context of a linear model and an extension of the PIR concept to non-linear models is outlined.","sentences":["Pearson's correlation coefficient is a popular statistical measure to summarize the strength of association between two continuous variables.","It is usually interpreted via its square as percentage of variance of one variable predicted by the other in a linear regression model.","It can be generalized for multiple regression via the coefficient of determination, which is not straightforward to interpret in terms of prediction accuracy.","In this paper, we propose to assess the prediction accuracy of a linear model via the prediction interval reduction (PIR) by comparing the width of the prediction interval derived from this model with the width of the prediction interval obtained without this model.","At the population level, PIR is one-to-one related to the correlation and the coefficient of determination.","In particular, a correlation of 0.5 corresponds to a PIR of only 13%.","It is also the one's complement of the coefficient of alienation introduced at the beginning of last century.","We argue that PIR is easily interpretable and useful to keep in mind how difficult it is to make accurate individual predictions, an important message in the era of precision medicine and artificial intelligence.","Different estimates of PIR are compared in the context of a linear model and an extension of the PIR concept to non-linear models is outlined."],"url":"http://arxiv.org/abs/2405.04895v1","category":"stat.ME"}
{"created":"2024-05-08 09:02:06","title":"Tunable nonlinear excitonic optical response in biased bilayer graphene","abstract":"Biased bilayer graphene (BBG) is an important system for studies of excitonic effects in graphene--based systems, with its easily tunable bandgap. This bandgap is governed by an external gate voltage, allowing one to tune the optical response of the system. In this paper, we study the excitonic linear and nonlinear optical response of Bernal stacked BBG as a function of the gate voltage, both for in--plane (IP) and out--of--plane (OOP) directions. Based on a semi-analytical model of the electronic structure of BBG describing the influence of gate voltage on excitonic binding energies, we focus our discussion on both the IP and OOP excitonic response. Both linear and second harmonic generation (SHG) nonlinear responses are shown to be very sensitive to the gate voltage, as both the interband momentum matrix elements and the bandgap of the system will vary greatly with bias potential.","sentences":["Biased bilayer graphene (BBG) is an important system for studies of excitonic effects in graphene--based systems, with its easily tunable bandgap.","This bandgap is governed by an external gate voltage, allowing one to tune the optical response of the system.","In this paper, we study the excitonic linear and nonlinear optical response of Bernal stacked BBG as a function of the gate voltage, both for in--plane (IP) and out--of--plane (OOP) directions.","Based on a semi-analytical model of the electronic structure of BBG describing the influence of gate voltage on excitonic binding energies, we focus our discussion on both the IP and OOP excitonic response.","Both linear and second harmonic generation (SHG) nonlinear responses are shown to be very sensitive to the gate voltage, as both the interband momentum matrix elements and the bandgap of the system will vary greatly with bias potential."],"url":"http://arxiv.org/abs/2405.04894v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 08:42:46","title":"Third Harmonic Enhancement Harnessing Photoexcitation Unveils New Nonlinearities in Zinc Oxide","abstract":"Nonlinear optical phenomena are at the heart of various technological domains such as high-speed data transfer, optical logic applications, and emerging fields such as non-reciprocal optics and photonic time crystal design. However, conventional nonlinear materials exhibit inherent limitations in the post-fabrication tailoring of their nonlinear optical properties. Achieving real-time control over optical nonlinearities remains a challenge. In this work, we demonstrate a method to switch third harmonic generation (THG), a commonly occurring nonlinear optical response. Third harmonic generation enhancements up to 50 times are demonstrated in zinc oxide films via the photoexcited state generation and tunable electric field enhancement. More interestingly, the enhanced third harmonic generation follows a quadratic scaling with incident power, as opposed to the conventional cubic scaling, which demonstrates a previously unreported mechanism of third harmonic generation. The THG can also be suppressed by modulating the optical losses in the film. This work shows that the photoexcitation of states can not only enhance nonlinearities, but can create new processes for third harmonic generation. Importantly, the proposed method enables real-time manipulation of the nonlinear response of a medium. The process is switchable and reversible, with the modulations occurring at picosecond timescale. Our study paves the way to boost or suppress the nonlinearities of solid-state media, enabling robust, switchable sources for nonlinear optical applications.","sentences":["Nonlinear optical phenomena are at the heart of various technological domains such as high-speed data transfer, optical logic applications, and emerging fields such as non-reciprocal optics and photonic time crystal design.","However, conventional nonlinear materials exhibit inherent limitations in the post-fabrication tailoring of their nonlinear optical properties.","Achieving real-time control over optical nonlinearities remains a challenge.","In this work, we demonstrate a method to switch third harmonic generation (THG), a commonly occurring nonlinear optical response.","Third harmonic generation enhancements up to 50 times are demonstrated in zinc oxide films via the photoexcited state generation and tunable electric field enhancement.","More interestingly, the enhanced third harmonic generation follows a quadratic scaling with incident power, as opposed to the conventional cubic scaling, which demonstrates a previously unreported mechanism of third harmonic generation.","The THG can also be suppressed by modulating the optical losses in the film.","This work shows that the photoexcitation of states can not only enhance nonlinearities, but can create new processes for third harmonic generation.","Importantly, the proposed method enables real-time manipulation of the nonlinear response of a medium.","The process is switchable and reversible, with the modulations occurring at picosecond timescale.","Our study paves the way to boost or suppress the nonlinearities of solid-state media, enabling robust, switchable sources for nonlinear optical applications."],"url":"http://arxiv.org/abs/2405.04891v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 08:39:25","title":"GISR: Geometric Initialization and Silhouette-based Refinement for Single-View Robot Pose and Configuration Estimation","abstract":"For autonomous robotics applications, it is crucial that robots are able to accurately measure their potential state and perceive their environment, including other agents within it (e.g., cobots interacting with humans). The redundancy of these measurements is important, as it allows for planning and execution of recovery protocols in the event of sensor failure or external disturbances. Visual estimation can provide this redundancy through the use of low-cost sensors and server as a standalone source of proprioception when no encoder-based sensing is available. Therefore, we estimate the configuration of the robot jointly with its pose, which provides a complete spatial understanding of the observed robot. We present GISR - a method for deep configuration and robot-to-camera pose estimation that prioritizes real-time execution. GISR is comprised of two modules: (i) a geometric initialization module, efficiently computing an approximate robot pose and configuration, and (ii) an iterative silhouette-based refinement module that refines the initial solution in only a few iterations. We evaluate our method on a publicly available dataset and show that GISR performs competitively with existing state-of-the-art approaches, while being significantly faster compared to existing methods of the same class. Our code is available at https://github.com/iwhitey/GISR-robot.","sentences":["For autonomous robotics applications, it is crucial that robots are able to accurately measure their potential state and perceive their environment, including other agents within it (e.g., cobots interacting with humans).","The redundancy of these measurements is important, as it allows for planning and execution of recovery protocols in the event of sensor failure or external disturbances.","Visual estimation can provide this redundancy through the use of low-cost sensors and server as a standalone source of proprioception when no encoder-based sensing is available.","Therefore, we estimate the configuration of the robot jointly with its pose, which provides a complete spatial understanding of the observed robot.","We present GISR - a method for deep configuration and robot-to-camera pose estimation that prioritizes real-time execution.","GISR is comprised of two modules: (i) a geometric initialization module, efficiently computing an approximate robot pose and configuration, and (ii) an iterative silhouette-based refinement module that refines the initial solution in only a few iterations.","We evaluate our method on a publicly available dataset and show that GISR performs competitively with existing state-of-the-art approaches, while being significantly faster compared to existing methods of the same class.","Our code is available at https://github.com/iwhitey/GISR-robot."],"url":"http://arxiv.org/abs/2405.04890v1","category":"cs.RO"}
{"created":"2024-05-08 08:38:28","title":"Fast LiDAR Upsampling using Conditional Diffusion Models","abstract":"The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments.","sentences":["The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods.","Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited.","These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction.","In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation.","Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks.","We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed.","This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset.","Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments."],"url":"http://arxiv.org/abs/2405.04889v1","category":"cs.CV"}
{"created":"2024-05-08 08:36:45","title":"Investigating the accelerated expansion of the Universe through updated constraints on viable $f(R)$ models within the Palatini formalism","abstract":"The observed accelerated expansion of the Universe at present epoch can be explained by some of the $f(R)$ models without invoking the existence of dark energy or any other such exotic component in cosmic fluid. The $f(R)$ models in Palatini formalism is relatively less explored in recent times with respect to their counterpart in metric formalism. We study seven $f(R)$ models in Palatini formalism: Hu-Sawicki (two cases), Starobinsky, exponential, Tsujikawa, $f(R) = R -\\beta /R^ n$, and $f(R)= R + \\alpha \\ln(R) - \\beta$. Following standard statistical procedure and utilizing data sets: type Ia supernovae data, cosmic chronometer observations, baryonic acoustic oscillations data, data from H \\textsc{ii} starburst galaxies, local measurements of the \\emph{Hubble} parameter ($H_{0}$), and distance priors of cosmic microwave background radiation data, we obtain constraints on the model parameters. When compared with the standard `lambda-cold dark matter model', for many data set combinations, the support for $f(R)$ models is significant. We obtain the relevant quantities for characterizing the accelerated expansion of the Universe, and these quantities are consistent with those obtained in a model-independent way by others. The curve of effective/total equation-of-state parameter, obtained from parameter constraints, clearly shows correct phases of the expansion history: the radiation-dominated epochs and the matter-dominated epochs, of the past, and the current accelerated expansion epoch eventually evolving to de-Sitter phase in the distant future. Overall, our results advocate in favour of pursuing $f(R)$ models in Palatini formalism as a potential alternative for explaining accelerated expansion of the Universe.","sentences":["The observed accelerated expansion of the Universe at present epoch can be explained by some of the $f(R)$ models without invoking the existence of dark energy or any other such exotic component in cosmic fluid.","The $f(R)$ models in Palatini formalism is relatively less explored in recent times with respect to their counterpart in metric formalism.","We study seven $f(R)$ models in Palatini formalism: Hu-Sawicki (two cases), Starobinsky, exponential, Tsujikawa, $f(R) = R -\\beta","/R^ n$, and $f(R)= R + \\alpha \\ln(R) - \\beta$. Following standard statistical procedure and utilizing data sets: type Ia supernovae data, cosmic chronometer observations, baryonic acoustic oscillations data, data from H \\textsc{ii} starburst galaxies, local measurements of the \\emph{Hubble} parameter ($H_{0}$), and distance priors of cosmic microwave background radiation data, we obtain constraints on the model parameters.","When compared with the standard `lambda-cold dark matter model', for many data set combinations, the support for $f(R)$ models is significant.","We obtain the relevant quantities for characterizing the accelerated expansion of the Universe, and these quantities are consistent with those obtained in a model-independent way by others.","The curve of effective/total equation-of-state parameter, obtained from parameter constraints, clearly shows correct phases of the expansion history: the radiation-dominated epochs and the matter-dominated epochs, of the past, and the current accelerated expansion epoch eventually evolving to de-Sitter phase in the distant future.","Overall, our results advocate in favour of pursuing $f(R)$ models in Palatini formalism as a potential alternative for explaining accelerated expansion of the Universe."],"url":"http://arxiv.org/abs/2405.04886v1","category":"astro-ph.CO"}
{"created":"2024-05-08 08:32:34","title":"Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion","abstract":"Unified multi-model representation spaces are the foundation of multimodal understanding and generation. However, the billions of model parameters and catastrophic forgetting problems make it challenging to further enhance pre-trained unified spaces. In this work, we propose Molecule-Space, an idea that treats multimodal representation spaces as \"molecules\", and augments pre-trained unified space by integrating knowledge from extra expert spaces via \"molecules space reactions\". Specifically, we introduce two kinds of basic space reactions: 1) Space Displacement Reaction and 2) Space Combination Reaction. Based on these defined basic reactions, we design Complex Sequential & Parallel Reactions to effectively integrate multiple spaces simultaneously. Benefiting from the modularization concept, we further propose a coarse-to-fine customized inference strategy to flexibly adjust the enhanced unified space for different purposes. Experimentally, we fuse the audio-image-text space of ImageBind with the image-text and audio-text expert spaces. The resulting space outperforms ImageBind on 5 downstream tasks across 9 datasets. Moreover, via customized inference, it even surpasses the used image-text and audio-text expert spaces.","sentences":["Unified multi-model representation spaces are the foundation of multimodal understanding and generation.","However, the billions of model parameters and catastrophic forgetting problems make it challenging to further enhance pre-trained unified spaces.","In this work, we propose Molecule-Space, an idea that treats multimodal representation spaces as \"molecules\", and augments pre-trained unified space by integrating knowledge from extra expert spaces via \"molecules space reactions\".","Specifically, we introduce two kinds of basic space reactions: 1) Space Displacement Reaction and 2) Space Combination Reaction.","Based on these defined basic reactions, we design Complex Sequential & Parallel Reactions to effectively integrate multiple spaces simultaneously.","Benefiting from the modularization concept, we further propose a coarse-to-fine customized inference strategy to flexibly adjust the enhanced unified space for different purposes.","Experimentally, we fuse the audio-image-text space of ImageBind with the image-text and audio-text expert spaces.","The resulting space outperforms ImageBind on 5 downstream tasks across 9 datasets.","Moreover, via customized inference, it even surpasses the used image-text and audio-text expert spaces."],"url":"http://arxiv.org/abs/2405.04883v1","category":"cs.CV"}
{"created":"2024-05-08 08:30:34","title":"G\u00f6del Number based Clustering Algorithm with Decimal First Degree Cellular Automata","abstract":"In this paper, a decimal first degree cellular automata (FDCA) based clustering algorithm is proposed where clusters are created based on reachability. Cyclic spaces are created and configurations which are in the same cycle are treated as the same cluster. Here, real-life data objects are encoded into decimal strings using G\\\"odel number based encoding. The benefits of the scheme is, it reduces the encoded string length while maintaining the features properties. Candidate CA rules are identified based on some theoretical criteria such as self-replication and information flow. An iterative algorithm is developed to generate the desired number of clusters over three stages. The results of the clustering are evaluated based on benchmark clustering metrics such as Silhouette score, Davis Bouldin, Calinski Harabasz and Dunn Index. In comparison with the existing state-of-the-art clustering algorithms, our proposed algorithm gives better performance.","sentences":["In this paper, a decimal first degree cellular automata (FDCA) based clustering algorithm is proposed where clusters are created based on reachability.","Cyclic spaces are created and configurations which are in the same cycle are treated as the same cluster.","Here, real-life data objects are encoded into decimal strings using G\\\"odel number based encoding.","The benefits of the scheme is, it reduces the encoded string length while maintaining the features properties.","Candidate CA rules are identified based on some theoretical criteria such as self-replication and information flow.","An iterative algorithm is developed to generate the desired number of clusters over three stages.","The results of the clustering are evaluated based on benchmark clustering metrics such as Silhouette score, Davis Bouldin, Calinski Harabasz and Dunn Index.","In comparison with the existing state-of-the-art clustering algorithms, our proposed algorithm gives better performance."],"url":"http://arxiv.org/abs/2405.04881v1","category":"cs.FL"}
{"created":"2024-05-08 08:28:40","title":"The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio","abstract":"With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for effective detection methods. Unlike traditional deepfake audio generation, which often involves multi-step processes culminating in vocoder usage, ALM directly utilizes neural codec methods to decode discrete codes into audio. Moreover, driven by large-scale data, ALMs exhibit remarkable robustness and versatility, posing a significant challenge to current audio deepfake detection (ADD) models. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially construct the Codecfake dataset, an open-source large-scale dataset, including two languages, millions of audio samples, and various test conditions, tailored for ALM-based audio detection. Additionally, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original SAM, we propose the CSAM strategy to learn a domain balanced and generalized minima. Experiment results demonstrate that co-training on Codecfake dataset and vocoded dataset with CSAM strategy yield the lowest average Equal Error Rate (EER) of 0.616% across all test conditions compared to baseline models.","sentences":["With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for effective detection methods.","Unlike traditional deepfake audio generation, which often involves multi-step processes culminating in vocoder usage, ALM directly utilizes neural codec methods to decode discrete codes into audio.","Moreover, driven by large-scale data, ALMs exhibit remarkable robustness and versatility, posing a significant challenge to current audio deepfake detection (ADD) models.","To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform.","We initially construct the Codecfake dataset, an open-source large-scale dataset, including two languages, millions of audio samples, and various test conditions, tailored for ALM-based audio detection.","Additionally, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original SAM, we propose the CSAM strategy to learn a domain balanced and generalized minima.","Experiment results demonstrate that co-training on Codecfake dataset and vocoded dataset with CSAM strategy yield the lowest average Equal Error Rate (EER) of 0.616% across all test conditions compared to baseline models."],"url":"http://arxiv.org/abs/2405.04880v1","category":"cs.SD"}
{"created":"2024-05-08 08:27:02","title":"Non-Abelian Braiding of Topological Edge Bands","abstract":"Braiding is a geometric concept that manifests itself in a variety of scientific contexts from biology to physics, and has been employed to classify bulk band topology in topological materials. Topological edge states can also form braiding structures, as demonstrated recently in a type of topological insulators known as M\\\"obius insulators, whose topological edge states form two braided bands exhibiting a M\\\"obius twist. While the formation of M\\\"obius twist is inspiring, it belongs to the simple Abelian braid group $\\mathbb{B}_2$. The most fascinating features about topological braids rely on the non-Abelianness in the higher-order braid group $\\mathbb{B}_N$ ($N \\geq 3$), which necessitates multiple edge bands, but so far it has not been discussed. Here, based on the gauge enriched symmetry, we develop a scheme to realize non-Abelian braiding of multiple topological edge bands. We propose tight-binding models of topological insulators that are able to generate topological edge states forming non-Abelian braiding structures. Experimental demonstrations are conducted in two acoustic crystals, which carry three and four braided acoustic edge bands, respectively. The observed braiding structure can correspond to the topological winding in the complex eigenvalue space of projective translation operator, akin to the previously established point-gap winding topology in the bulk of the Hatano-Nelson model. Our work also constitutes the realization of non-Abelian braiding topology on an actual crystal platform, but not based on the \"virtual\" synthetic dimensions.","sentences":["Braiding is a geometric concept that manifests itself in a variety of scientific contexts from biology to physics, and has been employed to classify bulk band topology in topological materials.","Topological edge states can also form braiding structures, as demonstrated recently in a type of topological insulators known as M\\\"obius insulators, whose topological edge states form two braided bands exhibiting a M\\\"obius twist.","While the formation of M\\\"obius twist is inspiring, it belongs to the simple Abelian braid group $\\mathbb{B}_2$. The most fascinating features about topological braids rely on the non-Abelianness in the higher-order braid group $\\mathbb{B}_N$ ($N \\geq 3$), which necessitates multiple edge bands, but so far it has not been discussed.","Here, based on the gauge enriched symmetry, we develop a scheme to realize non-Abelian braiding of multiple topological edge bands.","We propose tight-binding models of topological insulators that are able to generate topological edge states forming non-Abelian braiding structures.","Experimental demonstrations are conducted in two acoustic crystals, which carry three and four braided acoustic edge bands, respectively.","The observed braiding structure can correspond to the topological winding in the complex eigenvalue space of projective translation operator, akin to the previously established point-gap winding topology in the bulk of the Hatano-Nelson model.","Our work also constitutes the realization of non-Abelian braiding topology on an actual crystal platform, but not based on the \"virtual\" synthetic dimensions."],"url":"http://arxiv.org/abs/2405.04879v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 08:23:25","title":"The Need Of Trustworthy Announcements To Achieve Driving Comfort","abstract":"An Intelligent Transport System (ITS) is more demanding nowadays and it can be achieved through deploying Vehicular Ad Hoc Networks (VANETs). Vehicles and Roadside Units (RSUs) exchange traffic events. Malicious drivers generate false events. Thus, they need to be identified to maintain trustworthy communication. When an authorised user acts maliciously, the security scheme typically fails. However, a trust model can isolate false messages. In this paper, the significance of trustworthy announcements for VANETs is analysed. To this end, a series of experiments is conducted in Veins to illustrate how the trustworthiness of announcements affects travel time. A traffic scenario is created where vehicles detour to an alternate route with an announcement from the leading vehicle. Both true and false announcements are considered. Results confirm that false announcements and refraining from announcements increase travel time. However, the travel time is reduced with trustworthy announcements. From this analysis, it can be concluded that trustworthy announcements facilitate driver comfort.","sentences":["An Intelligent Transport System (ITS) is more demanding nowadays and it can be achieved through deploying Vehicular Ad Hoc Networks (VANETs).","Vehicles and Roadside Units (RSUs) exchange traffic events.","Malicious drivers generate false events.","Thus, they need to be identified to maintain trustworthy communication.","When an authorised user acts maliciously, the security scheme typically fails.","However, a trust model can isolate false messages.","In this paper, the significance of trustworthy announcements for VANETs is analysed.","To this end, a series of experiments is conducted in Veins to illustrate how the trustworthiness of announcements affects travel time.","A traffic scenario is created where vehicles detour to an alternate route with an announcement from the leading vehicle.","Both true and false announcements are considered.","Results confirm that false announcements and refraining from announcements increase travel time.","However, the travel time is reduced with trustworthy announcements.","From this analysis, it can be concluded that trustworthy announcements facilitate driver comfort."],"url":"http://arxiv.org/abs/2405.04878v1","category":"cs.CR"}
{"created":"2024-05-08 08:17:25","title":"Nonequilibrium entropy from density estimation","abstract":"Entropy is a central concept in physics, but can be challenging to calculate even for systems that are easily simulated. This is exacerbated out of equilibrium, where generally little is known about the distribution characterizing simulated configurations. However, modern machine learning algorithms can estimate the probability density characterizing an ensemble of images, given nothing more than sample images assumed to be drawn from this distribution. We show that by mapping system configurations to images, such approaches can be adapted to the efficient estimation of the density, and therefore the entropy, from simulated or experimental data. We then use this idea to obtain entropic limit cycles in a kinetic Ising model driven by an oscillating magnetic field. Despite being a global probe, we demonstrate that this allows us to identify and characterize stochastic dynamics at parameters near the dynamical phase transition.","sentences":["Entropy is a central concept in physics, but can be challenging to calculate even for systems that are easily simulated.","This is exacerbated out of equilibrium, where generally little is known about the distribution characterizing simulated configurations.","However, modern machine learning algorithms can estimate the probability density characterizing an ensemble of images, given nothing more than sample images assumed to be drawn from this distribution.","We show that by mapping system configurations to images, such approaches can be adapted to the efficient estimation of the density, and therefore the entropy, from simulated or experimental data.","We then use this idea to obtain entropic limit cycles in a kinetic Ising model driven by an oscillating magnetic field.","Despite being a global probe, we demonstrate that this allows us to identify and characterize stochastic dynamics at parameters near the dynamical phase transition."],"url":"http://arxiv.org/abs/2405.04877v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 08:15:19","title":"Accelerating the prediction of stacking fault energy by combining ab initio calculations and machine learning","abstract":"Stacking fault energies (SFEs) are vital parameters for understanding the deformation mechanisms in metals and alloys, with prior knowledge of SFEs from ab initio calculations being crucial for alloy design. Machine learning (ML) algorithms employed in the present work demonstrate approximately 80 times acceleration in predicting generalized stacking fault energy (GSFE), which is otherwise computationally expensive to obtain directly from density functional theory (DFT) calculations, particularly for alloys. The features used to train the ML algorithms stem from the physics-based Friedel model, revealing a connection between the physics of d-electrons and the deformation behavior of transition metals and alloys. Predictions based on the ML model are consistent with experimental data. This model could aid in accelerating alloy design by offering a rapid method for screening materials based on stacking fault energies.","sentences":["Stacking fault energies (SFEs) are vital parameters for understanding the deformation mechanisms in metals and alloys, with prior knowledge of SFEs from ab initio calculations being crucial for alloy design.","Machine learning (ML) algorithms employed in the present work demonstrate approximately 80 times acceleration in predicting generalized stacking fault energy (GSFE), which is otherwise computationally expensive to obtain directly from density functional theory (DFT) calculations, particularly for alloys.","The features used to train the ML algorithms stem from the physics-based Friedel model, revealing a connection between the physics of d-electrons and the deformation behavior of transition metals and alloys.","Predictions based on the ML model are consistent with experimental data.","This model could aid in accelerating alloy design by offering a rapid method for screening materials based on stacking fault energies."],"url":"http://arxiv.org/abs/2405.04876v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 08:12:21","title":"SCALA: Split Federated Learning with Concatenated Activations and Logit Adjustments","abstract":"Split Federated Learning (SFL) is a distributed machine learning framework which strategically divides the learning process between a server and clients and collaboratively trains a shared model by aggregating local models updated based on data from distributed clients. However, data heterogeneity and partial client participation result in label distribution skew, which severely degrades the learning performance. To address this issue, we propose SFL with Concatenated Activations and Logit Adjustments (SCALA). Specifically, the activations from the client-side models are concatenated as the input of the server-side model so as to centrally adjust label distribution across different clients, and logit adjustments of loss functions on both server-side and client-side models are performed to deal with the label distribution variation across different subsets of participating clients. Theoretical analysis and experimental results verify the superiority of the proposed SCALA on public datasets.","sentences":["Split Federated Learning (SFL) is a distributed machine learning framework which strategically divides the learning process between a server and clients and collaboratively trains a shared model by aggregating local models updated based on data from distributed clients.","However, data heterogeneity and partial client participation result in label distribution skew, which severely degrades the learning performance.","To address this issue, we propose SFL with Concatenated Activations and Logit Adjustments (SCALA).","Specifically, the activations from the client-side models are concatenated as the input of the server-side model so as to centrally adjust label distribution across different clients, and logit adjustments of loss functions on both server-side and client-side models are performed to deal with the label distribution variation across different subsets of participating clients.","Theoretical analysis and experimental results verify the superiority of the proposed SCALA on public datasets."],"url":"http://arxiv.org/abs/2405.04875v1","category":"cs.LG"}
{"created":"2024-05-08 08:08:50","title":"Critical Infrastructure Protection: Generative AI, Challenges, and Opportunities","abstract":"Critical National Infrastructure (CNI) encompasses a nation's essential assets that are fundamental to the operation of society and the economy, ensuring the provision of vital utilities such as energy, water, transportation, and communication. Nevertheless, growing cybersecurity threats targeting these infrastructures can potentially interfere with operations and seriously risk national security and public safety. In this paper, we examine the intricate issues raised by cybersecurity risks to vital infrastructure, highlighting these systems' vulnerability to different types of cyberattacks. We analyse the significance of trust, privacy, and resilience for Critical Infrastructure Protection (CIP), examining the diverse standards and regulations to manage these domains. We also scrutinise the co-analysis of safety and security, offering innovative approaches for their integration and emphasising the interdependence between these fields. Furthermore, we introduce a comprehensive method for CIP leveraging Generative AI and Large Language Models (LLMs), giving a tailored lifecycle and discussing specific applications across different critical infrastructure sectors. Lastly, we discuss potential future directions that promise to enhance the security and resilience of critical infrastructures. This paper proposes innovative strategies for CIP from evolving attacks and enhances comprehension of cybersecurity concerns related to critical infrastructure.","sentences":["Critical National Infrastructure (CNI) encompasses a nation's essential assets that are fundamental to the operation of society and the economy, ensuring the provision of vital utilities such as energy, water, transportation, and communication.","Nevertheless, growing cybersecurity threats targeting these infrastructures can potentially interfere with operations and seriously risk national security and public safety.","In this paper, we examine the intricate issues raised by cybersecurity risks to vital infrastructure, highlighting these systems' vulnerability to different types of cyberattacks.","We analyse the significance of trust, privacy, and resilience for Critical Infrastructure Protection (CIP), examining the diverse standards and regulations to manage these domains.","We also scrutinise the co-analysis of safety and security, offering innovative approaches for their integration and emphasising the interdependence between these fields.","Furthermore, we introduce a comprehensive method for CIP leveraging Generative AI and Large Language Models (LLMs), giving a tailored lifecycle and discussing specific applications across different critical infrastructure sectors.","Lastly, we discuss potential future directions that promise to enhance the security and resilience of critical infrastructures.","This paper proposes innovative strategies for CIP from evolving attacks and enhances comprehension of cybersecurity concerns related to critical infrastructure."],"url":"http://arxiv.org/abs/2405.04874v1","category":"cs.CR"}
{"created":"2024-05-08 08:05:47","title":"Logical Negation Augmenting and Debiasing for Prompt-based Methods","abstract":"Prompt-based methods have gained increasing attention on NLP and shown validity on many downstream tasks. Many works have focused on mining these methods' potential for knowledge extraction, but few explore their ability to make logical reasoning. In this work, we focus on the effectiveness of the prompt-based methods on first-order logical reasoning and find that the bottleneck lies in logical negation. Based on our analysis, logical negation tends to result in spurious correlations to negative answers, while propositions without logical negation correlate to positive answers. To solve the problem, we propose a simple but effective method, Negation Augmenting and Negation Debiasing (NAND), which introduces negative propositions to prompt-based methods without updating parameters. Specifically, these negative propositions can counteract spurious correlations by providing \"not\" for all instances so that models cannot make decisions only by whether expressions contain a logical negation. Experiments on three datasets show that NAND not only solves the problem of calibrating logical negation but also significantly enhances prompt-based methods of logical reasoning without model retraining.","sentences":["Prompt-based methods have gained increasing attention on NLP and shown validity on many downstream tasks.","Many works have focused on mining these methods' potential for knowledge extraction, but few explore their ability to make logical reasoning.","In this work, we focus on the effectiveness of the prompt-based methods on first-order logical reasoning and find that the bottleneck lies in logical negation.","Based on our analysis, logical negation tends to result in spurious correlations to negative answers, while propositions without logical negation correlate to positive answers.","To solve the problem, we propose a simple but effective method, Negation Augmenting and Negation Debiasing (NAND), which introduces negative propositions to prompt-based methods without updating parameters.","Specifically, these negative propositions can counteract spurious correlations by providing \"not\" for all instances so that models cannot make decisions only by whether expressions contain a logical negation.","Experiments on three datasets show that NAND not only solves the problem of calibrating logical negation but also significantly enhances prompt-based methods of logical reasoning without model retraining."],"url":"http://arxiv.org/abs/2405.04872v1","category":"cs.CL"}
{"created":"2024-05-08 08:01:06","title":"EMISSA: Exploring millimetre indicators of solar-stellar activity III. Comparison of Ca II indices and millimetre continua in a 3D model atmosphere","abstract":"The Ca II H & K lines are strong chromospheric diagnostics that can be used to determine the temperature stratification and magnetic structure of the solar atmosphere. The Atacama Large Millimetre/Submillimetre Array (ALMA) offers complementary information on the thermal structure of stellar atmospheres using mm continuum radiation. The overall aim is to establish more robust solar/stellar activity indicators using ALMA observations in comparison with classical diagnostics, such as the s index and infrared triplet (IRT) index. A study was conducted using 1.5D radiative transfer codes RH1.5D and advanced radiative transfer (ART), along with an enhanced network atmosphere model generated by the state-of-the-art 3D radiation magnetohydrodynamics (rMHD) Bifrost code, to compute synthetic spectra for both Ca II lines and mm continua. To account for the limited spatial resolution of ALMA, we simulated the effect using a Gaussian point spread function (PSF). Additionally, we analysed the correlations and slopes of scatter plots between the Ca II indices and mm continuum for the original and degraded resolutions, focusing on the entire simulation box, quiet Sun regions, and enhanced network patches separately. The activity indices generated from these lines could further be used to compare the spectra of Sun-like stars with the solar spectrum. The Ca II activity indices and mm brightness temperatures are weakly correlated at the high resolution, with the highest correlation observed at a wavelength of 0.3 mm, corresponding to ALMA band 10. As the resolution decreases, the correlation consistently increases. Conversely, the slopes exhibit a decreasing trend with increasing wavelength, while the degradation of resolution does not noticeably affect the calculated slopes. Consequently, these relationships could be valuable for calibrating the mm continuum maps obtained through ALMA observations.","sentences":["The Ca II H & K lines are strong chromospheric diagnostics that can be used to determine the temperature stratification and magnetic structure of the solar atmosphere.","The Atacama Large Millimetre/Submillimetre Array (ALMA) offers complementary information on the thermal structure of stellar atmospheres using mm continuum radiation.","The overall aim is to establish more robust solar/stellar activity indicators using ALMA observations in comparison with classical diagnostics, such as the s index and infrared triplet (IRT) index.","A study was conducted using 1.5D radiative transfer codes RH1.5D and advanced radiative transfer (ART), along with an enhanced network atmosphere model generated by the state-of-the-art 3D radiation magnetohydrodynamics (rMHD)","Bifrost code, to compute synthetic spectra for both Ca II lines and mm continua.","To account for the limited spatial resolution of ALMA, we simulated the effect using a Gaussian point spread function (PSF).","Additionally, we analysed the correlations and slopes of scatter plots between the Ca II indices and mm continuum for the original and degraded resolutions, focusing on the entire simulation box, quiet Sun regions, and enhanced network patches separately.","The activity indices generated from these lines could further be used to compare the spectra of Sun-like stars with the solar spectrum.","The Ca II activity indices and mm brightness temperatures are weakly correlated at the high resolution, with the highest correlation observed at a wavelength of 0.3 mm, corresponding to ALMA band 10.","As the resolution decreases, the correlation consistently increases.","Conversely, the slopes exhibit a decreasing trend with increasing wavelength, while the degradation of resolution does not noticeably affect the calculated slopes.","Consequently, these relationships could be valuable for calibrating the mm continuum maps obtained through ALMA observations."],"url":"http://arxiv.org/abs/2405.04871v1","category":"astro-ph.SR"}
{"created":"2024-05-08 07:50:21","title":"Enhancing Geometric Ontology Embeddings for $\\mathcal{EL}^{++}$ with Negative Sampling and Deductive Closure Filtering","abstract":"Ontology embeddings map classes, relations, and individuals in ontologies into $\\mathbb{R}^n$, and within $\\mathbb{R}^n$ similarity between entities can be computed or new axioms inferred. For ontologies in the Description Logic $\\mathcal{EL}^{++}$, several embedding methods have been developed that explicitly generate models of an ontology. However, these methods suffer from some limitations; they do not distinguish between statements that are unprovable and provably false, and therefore they may use entailed statements as negatives. Furthermore, they do not utilize the deductive closure of an ontology to identify statements that are inferred but not asserted. We evaluated a set of embedding methods for $\\mathcal{EL}^{++}$ ontologies based on high-dimensional ball representation of concept descriptions, incorporating several modifications that aim to make use of the ontology deductive closure. In particular, we designed novel negative losses that account both for the deductive closure and different types of negatives. We demonstrate that our embedding methods improve over the baseline ontology embedding in the task of knowledge base or ontology completion.","sentences":["Ontology embeddings map classes, relations, and individuals in ontologies into $\\mathbb{R}^n$, and within $\\mathbb{R}^n$ similarity between entities can be computed or new axioms inferred.","For ontologies in the Description Logic $\\mathcal{EL}^{++}$, several embedding methods have been developed that explicitly generate models of an ontology.","However, these methods suffer from some limitations; they do not distinguish between statements that are unprovable and provably false, and therefore they may use entailed statements as negatives.","Furthermore, they do not utilize the deductive closure of an ontology to identify statements that are inferred but not asserted.","We evaluated a set of embedding methods for $\\mathcal{EL}^{++}$ ontologies based on high-dimensional ball representation of concept descriptions, incorporating several modifications that aim to make use of the ontology deductive closure.","In particular, we designed novel negative losses that account both for the deductive closure and different types of negatives.","We demonstrate that our embedding methods improve over the baseline ontology embedding in the task of knowledge base or ontology completion."],"url":"http://arxiv.org/abs/2405.04868v1","category":"cs.AI"}
{"created":"2024-05-08 07:49:29","title":"MIPI 2024 Challenge on Demosaic for HybridEVS Camera: Methods and Results","abstract":"The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems. However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms. In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2024. In total, 170 participants were successfully registered, and 14 teams submitted results in the final testing phase. The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2024/.","sentences":["The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems.","However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI).","Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms.","In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2024.","In total, 170 participants were successfully registered, and 14 teams submitted results in the final testing phase.","The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal.","More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2024/."],"url":"http://arxiv.org/abs/2405.04867v1","category":"eess.IV"}
{"created":"2024-05-08 07:48:40","title":"Systematic review, analysis, and characterisation of malicious industrial network traffic datasets for aiding Machine Learning algorithm performance testing","abstract":"The adoption of the Industrial Internet of Things (IIoT) as a complementary technology to Operational Technology (OT) has enabled a new level of standardised data access and process visibility. This convergence of Information Technology (IT), OT, and IIoT has also created new cybersecurity vulnerabilities and risks that must be managed. Artificial Intelligence (AI) is emerging as a powerful tool to monitor OT/IIoT networks for malicious activity and is a highly active area of research. AI researchers are applying advanced Machine Learning (ML) and Deep Learning (DL) techniques to the detection of anomalous or malicious activity in network traffic. They typically use datasets derived from IoT/IIoT/OT network traffic captures to measure the performance of their proposed approaches. Therefore, there is a widespread need for datasets for algorithm testing. This work systematically reviews publicly available network traffic capture-based datasets, including categorisation of contained attack types, review of metadata, and statistical as well as complexity analysis. Each dataset is analysed to provide researchers with metadata that can be used to select the best dataset for their research question. This results in an added benefit to the community as researchers can select the best dataset for their research more easily and according to their specific Machine Learning goals.","sentences":["The adoption of the Industrial Internet of Things (IIoT) as a complementary technology to Operational Technology (OT) has enabled a new level of standardised data access and process visibility.","This convergence of Information Technology (IT), OT, and IIoT has also created new cybersecurity vulnerabilities and risks that must be managed.","Artificial Intelligence (AI) is emerging as a powerful tool to monitor OT/IIoT networks for malicious activity and is a highly active area of research.","AI researchers are applying advanced Machine Learning (ML) and Deep Learning (DL) techniques to the detection of anomalous or malicious activity in network traffic.","They typically use datasets derived from IoT/IIoT/OT network traffic captures to measure the performance of their proposed approaches.","Therefore, there is a widespread need for datasets for algorithm testing.","This work systematically reviews publicly available network traffic capture-based datasets, including categorisation of contained attack types, review of metadata, and statistical as well as complexity analysis.","Each dataset is analysed to provide researchers with metadata that can be used to select the best dataset for their research question.","This results in an added benefit to the community as researchers can select the best dataset for their research more easily and according to their specific Machine Learning goals."],"url":"http://arxiv.org/abs/2405.04866v1","category":"cs.CR"}
{"created":"2024-05-08 07:38:50","title":"Three-dimensional higher-order saddle points induced flat bands in Co-based kagome metals","abstract":"The saddle point (van Hove singularity) exhibits a divergent density of states in 2D systems, leading to fascinating phenomena like strong correlations and unconventional superconductivity, yet it is seldom observed in 3D systems. In this work, we have found two types of 3D higher-order saddle points (HOSPs) in emerging 3D kagome metals, YbCo$_6$Ge$_6$ and MgCo$_6$Ge$_6$. Both HOSPs exhibit a singularity in their density of states, which is significantly enhanced compared to the ordinary saddle point. The HOSP near the Fermi energy generates a flat band extending a large area in the Brillouin zone, potentially amplifying the correlation effect and fostering electronic instabilities. Two types of HOSPs exhibit distinct robustness upon element substitution and lattice distortions in these kagome compounds. Our work paves the way for engineering exotic band structures, such as saddle points and flat bands, and exploring interesting phenomena in Co-based kagome materials.","sentences":["The saddle point (van Hove singularity) exhibits a divergent density of states in 2D systems, leading to fascinating phenomena like strong correlations and unconventional superconductivity, yet it is seldom observed in 3D systems.","In this work, we have found two types of 3D higher-order saddle points (HOSPs) in emerging 3D kagome metals, YbCo$_6$Ge$_6$ and MgCo$_6$Ge$_6$. Both HOSPs exhibit a singularity in their density of states, which is significantly enhanced compared to the ordinary saddle point.","The HOSP near the Fermi energy generates a flat band extending a large area in the Brillouin zone, potentially amplifying the correlation effect and fostering electronic instabilities.","Two types of HOSPs exhibit distinct robustness upon element substitution and lattice distortions in these kagome compounds.","Our work paves the way for engineering exotic band structures, such as saddle points and flat bands, and exploring interesting phenomena in Co-based kagome materials."],"url":"http://arxiv.org/abs/2405.04863v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 07:32:19","title":"Concolic Testing of Quantum Programs","abstract":"This paper presents the first concolic testing framework specifically designed for quantum programs. The framework defines quantum conditional statements that quantify quantum states and presents a symbolization method for quantum variables. Utilizing this framework, we generate path constraints for each concrete execution path of a quantum program. These constraints guide the exploration of new paths, with a quantum constraint solver determining the outcomes to generate novel input samples and enhance branch coverage. We implemented this framework in Python and integrated it with Qiskit for practical evaluation. Experimental results demonstrate that our concolic testing framework significantly improves branch coverage and the quality of quantum input samples, demonstrating its effectiveness and efficiency in quantum software testing.","sentences":["This paper presents the first concolic testing framework specifically designed for quantum programs.","The framework defines quantum conditional statements that quantify quantum states and presents a symbolization method for quantum variables.","Utilizing this framework, we generate path constraints for each concrete execution path of a quantum program.","These constraints guide the exploration of new paths, with a quantum constraint solver determining the outcomes to generate novel input samples and enhance branch coverage.","We implemented this framework in Python and integrated it with Qiskit for practical evaluation.","Experimental results demonstrate that our concolic testing framework significantly improves branch coverage and the quality of quantum input samples, demonstrating its effectiveness and efficiency in quantum software testing."],"url":"http://arxiv.org/abs/2405.04860v1","category":"cs.SE"}
{"created":"2024-05-08 07:28:00","title":"Guarding Force: Safety-Critical Compliant Control for Robot-Environment Interaction","abstract":"In this study, we propose a safety-critical compliant control strategy designed to strictly enforce interaction force constraints during the physical interaction of robots with unknown environments. The interaction force constraint is interpreted as a new force-constrained control barrier function (FC-CBF) by exploiting the generalized contact model and the prior information of the environment, i.e., the prior stiffness and rest position, for robot kinematics. The difference between the real environment and the generalized contact model is approximated by constructing a tracking differentiator, and its estimation error is quantified based on Lyapunov theory. By interpreting strict interaction safety specification as a dynamic constraint, restricting the desired joint angular rates in kinematics, the proposed approach modifies nominal compliant controllers using quadratic programming, ensuring adherence to interaction force constraints in unknown environments. The strict force constraint and the stability of the closed-loop system are rigorously analyzed. Experimental tests using a UR3e industrial robot with different environments verify the effectiveness of the proposed method in achieving the force constraints in unknown environments.","sentences":["In this study, we propose a safety-critical compliant control strategy designed to strictly enforce interaction force constraints during the physical interaction of robots with unknown environments.","The interaction force constraint is interpreted as a new force-constrained control barrier function (FC-CBF) by exploiting the generalized contact model and the prior information of the environment, i.e., the prior stiffness and rest position, for robot kinematics.","The difference between the real environment and the generalized contact model is approximated by constructing a tracking differentiator, and its estimation error is quantified based on Lyapunov theory.","By interpreting strict interaction safety specification as a dynamic constraint, restricting the desired joint angular rates in kinematics, the proposed approach modifies nominal compliant controllers using quadratic programming, ensuring adherence to interaction force constraints in unknown environments.","The strict force constraint and the stability of the closed-loop system are rigorously analyzed.","Experimental tests using a UR3e industrial robot with different environments verify the effectiveness of the proposed method in achieving the force constraints in unknown environments."],"url":"http://arxiv.org/abs/2405.04859v1","category":"cs.RO"}
{"created":"2024-05-08 07:11:27","title":"Revisiting general dark matter-bound-electron interactions","abstract":"In this letter we revisit general dark matter (DM)-bound-electron interactions studied previously in the influential work of [Catena et al., Phys. Rev. Res. 2, 033195 (2020)]. We derive the DM-electron response functions and find a crucial minus sign was missed for the second atomic response function $W_2$ defined in that work. The minus sign has significant phenomenological consequences when explaining experimental bounds on specific DM scenarios. Furthermore, for the most general DM-electron nonrelativistic or relativistic interactions for DM with spin up to one, we find there are three DM response functions ($a_{0,1,2}$) whose corresponding atomic response functions ($\\widetilde W_{0,1,2}$) are linear combinations of the four response functions ($W_{1,2,3,4}$) given in that work, $$ \\widetilde W_0 = W_1, \\, \\widetilde W_2 = |\\mathbf{v}_0^\\perp|^2 W_1+ W_3 - 2 {m_e\\, \\mathbf{q}\\cdot \\mathbf{v}_0^\\perp \\over \\mathbf{q}^2} W_2,\\, \\widetilde W_3 = { (\\mathbf{q}\\cdot \\mathbf{v}_0^\\perp)^2 \\over \\mathbf{q}^2} W_1 + {m_e^2 \\over \\mathbf{q}^2}W_4 - 2 {m_e\\, \\mathbf{q}\\cdot \\mathbf{v}_0^\\perp \\over \\mathbf{q}^2} W_2. $$ Due to the minus sign correction for $W_2$, there can be significant cancellations between the $W_2$ and $W_{3,4}$ terms, so that $\\widetilde W_{2,3}$ are dominated by the usual response function $W_1$ in some cases. Ignoring the sign could thus result in misinterpretation of the experimental data in some DM scenarios. As an example, we show that the recent XENON1T constraint on the fermionic DM anapole moment is weakened by a factor of 2 or so. Many DM scenarios involving DM or electron axial-vector current can yield $W_2$ and thus are potentially affected by the sign.","sentences":["In this letter we revisit general dark matter (DM)-bound-electron interactions studied previously in the influential work of [Catena et al., Phys.","Rev. Res. 2, 033195 (2020)].","We derive the DM-electron response functions and find a crucial minus sign was missed for the second atomic response function $W_2$ defined in that work.","The minus sign has significant phenomenological consequences when explaining experimental bounds on specific DM scenarios.","Furthermore, for the most general DM-electron nonrelativistic or relativistic interactions for DM with spin up to one, we find there are three DM response functions ($a_{0,1,2}$) whose corresponding atomic response functions ($\\widetilde W_{0,1,2}$) are linear combinations of the four response functions ($W_{1,2,3,4}$) given in that work, $$ \\widetilde W_0 = W_1, \\, \\widetilde W_2","= |\\mathbf{v}_0^\\perp|^2 W_1+ W_3 - 2 {m_e\\, \\mathbf{q}\\cdot \\mathbf{v}_0^\\perp \\over \\mathbf{q}^2} W_2,\\, \\widetilde W_3 = { (\\mathbf{q}\\cdot \\mathbf{v}_0^\\perp)^2 \\over \\mathbf{q}^2} W_1 + {m_e^2 \\over \\mathbf{q}^2}W_4 - 2 {m_e\\, \\mathbf{q}\\cdot \\mathbf{v}_0^\\perp \\over \\mathbf{q}^2} W_2.","$$ Due to the minus sign correction for $W_2$, there can be significant cancellations between the $W_2$ and $W_{3,4}$ terms, so that $\\widetilde W_{2,3}$ are dominated by the usual response function $W_1$ in some cases.","Ignoring the sign could thus result in misinterpretation of the experimental data in some DM scenarios.","As an example, we show that the recent XENON1T constraint on the fermionic DM anapole moment is weakened by a factor of 2 or so.","Many DM scenarios involving DM or electron axial-vector current can yield $W_2$ and thus are potentially affected by the sign."],"url":"http://arxiv.org/abs/2405.04855v1","category":"hep-ph"}
{"created":"2024-05-08 07:09:43","title":"Explaining Clustering of Ecological Momentary Assessment Data Through Temporal and Feature Attention","abstract":"In the field of psychopathology, Ecological Momentary Assessment (EMA) studies offer rich individual data on psychopathology-relevant variables (e.g., affect, behavior, etc) in real-time. EMA data is collected dynamically, represented as complex multivariate time series (MTS). Such information is crucial for a better understanding of mental disorders at the individual- and group-level. More specifically, clustering individuals in EMA data facilitates uncovering and studying the commonalities as well as variations of groups in the population. Nevertheless, since clustering is an unsupervised task and true EMA grouping is not commonly available, the evaluation of clustering is quite challenging. An important aspect of evaluation is clustering explainability. Thus, this paper proposes an attention-based interpretable framework to identify the important time-points and variables that play primary roles in distinguishing between clusters. A key part of this study is to examine ways to analyze, summarize, and interpret the attention weights as well as evaluate the patterns underlying the important segments of the data that differentiate across clusters. To evaluate the proposed approach, an EMA dataset of 187 individuals grouped in 3 clusters is used for analyzing the derived attention-based importance attributes. More specifically, this analysis provides the distinct characteristics at the cluster-, feature- and individual level. Such clustering explanations could be beneficial for generalizing existing concepts of mental disorders, discovering new insights, and even enhancing our knowledge at an individual level.","sentences":["In the field of psychopathology, Ecological Momentary Assessment (EMA) studies offer rich individual data on psychopathology-relevant variables (e.g., affect, behavior, etc) in real-time.","EMA data is collected dynamically, represented as complex multivariate time series (MTS).","Such information is crucial for a better understanding of mental disorders at the individual- and group-level.","More specifically, clustering individuals in EMA data facilitates uncovering and studying the commonalities as well as variations of groups in the population.","Nevertheless, since clustering is an unsupervised task and true EMA grouping is not commonly available, the evaluation of clustering is quite challenging.","An important aspect of evaluation is clustering explainability.","Thus, this paper proposes an attention-based interpretable framework to identify the important time-points and variables that play primary roles in distinguishing between clusters.","A key part of this study is to examine ways to analyze, summarize, and interpret the attention weights as well as evaluate the patterns underlying the important segments of the data that differentiate across clusters.","To evaluate the proposed approach, an EMA dataset of 187 individuals grouped in 3 clusters is used for analyzing the derived attention-based importance attributes.","More specifically, this analysis provides the distinct characteristics at the cluster-, feature- and individual level.","Such clustering explanations could be beneficial for generalizing existing concepts of mental disorders, discovering new insights, and even enhancing our knowledge at an individual level."],"url":"http://arxiv.org/abs/2405.04854v1","category":"cs.LG"}
{"created":"2024-05-08 06:46:14","title":"Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration","abstract":"In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by Syring and Martin (2019) [Biometrika, Volume 106, Issue 2, pp. 479-486]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desired value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.","sentences":["In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies.","This paper explores two branches of robust Bayesian methods in response to this trend.","The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications.","The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models.","In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter.","The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by Syring and Martin (2019)","[Biometrika, Volume 106, Issue 2, pp. 479-486].","Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples.","We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers.","A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling.","Thus, the proposed algorithm can reach the desired value within a few iterations.","This improvement substantially reduces the computational cost of the GPC.","Its efficacy is demonstrated through synthetic and real data applications."],"url":"http://arxiv.org/abs/2405.04845v1","category":"stat.ME"}
{"created":"2024-05-08 06:35:04","title":"Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems","abstract":"The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems. The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility.   Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system. Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper). The \\emph{selection bias} inherent in the model of each stage significantly influences the results that are ultimately presented to users.   To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users.   We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank. Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best complies with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output.   We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms. The algorithm is proved to be effective in both retrieval and ranking stages. Since deployed, the algorithm has brought consistent and significant performance gain to the platform.","sentences":["The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems.","The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility.   ","Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system.","Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper).","The \\emph{selection bias} inherent in the model of each stage significantly influences the results that are ultimately presented to users.   ","To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users.   ","We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank.","Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best complies with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output.   ","We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms.","The algorithm is proved to be effective in both retrieval and ranking stages.","Since deployed, the algorithm has brought consistent and significant performance gain to the platform."],"url":"http://arxiv.org/abs/2405.04844v1","category":"cs.IR"}
{"created":"2024-05-08 06:32:18","title":"Tilings of Flat Tori by Congruent Hexagons","abstract":"Convex hexagons that can tile the plane have been classified into three types. For the generic cases (not necessarily convex) of the three types and two other special cases, we classify tilings of the plane under the assumption that all vertices have degree $3$. Then we use the classification to describe the corresponding hexagonal tilings of flat tori and their moduli spaces.","sentences":["Convex hexagons that can tile the plane have been classified into three types.","For the generic cases (not necessarily convex) of the three types and two other special cases, we classify tilings of the plane under the assumption that all vertices have degree $3$. Then we use the classification to describe the corresponding hexagonal tilings of flat tori and their moduli spaces."],"url":"http://arxiv.org/abs/2405.04843v1","category":"math.CO"}
{"created":"2024-05-08 06:29:26","title":"xMTrans: Temporal Attentive Cross-Modality Fusion Transformer for Long-Term Traffic Prediction","abstract":"Traffic predictions play a crucial role in intelligent transportation systems. The rapid development of IoT devices allows us to collect different kinds of data with high correlations to traffic predictions, fostering the development of efficient multi-modal traffic prediction models. Until now, there are few studies focusing on utilizing advantages of multi-modal data for traffic predictions. In this paper, we introduce a novel temporal attentive cross-modality transformer model for long-term traffic predictions, namely xMTrans, with capability of exploring the temporal correlations between the data of two modalities: one target modality (for prediction, e.g., traffic congestion) and one support modality (e.g., people flow). We conducted extensive experiments to evaluate our proposed model on traffic congestion and taxi demand predictions using real-world datasets. The results showed the superiority of xMTrans against recent state-of-the-art methods on long-term traffic predictions. In addition, we also conducted a comprehensive ablation study to further analyze the effectiveness of each module in xMTrans.","sentences":["Traffic predictions play a crucial role in intelligent transportation systems.","The rapid development of IoT devices allows us to collect different kinds of data with high correlations to traffic predictions, fostering the development of efficient multi-modal traffic prediction models.","Until now, there are few studies focusing on utilizing advantages of multi-modal data for traffic predictions.","In this paper, we introduce a novel temporal attentive cross-modality transformer model for long-term traffic predictions, namely xMTrans, with capability of exploring the temporal correlations between the data of two modalities: one target modality (for prediction, e.g., traffic congestion) and one support modality (e.g., people flow).","We conducted extensive experiments to evaluate our proposed model on traffic congestion and taxi demand predictions using real-world datasets.","The results showed the superiority of xMTrans against recent state-of-the-art methods on long-term traffic predictions.","In addition, we also conducted a comprehensive ablation study to further analyze the effectiveness of each module in xMTrans."],"url":"http://arxiv.org/abs/2405.04841v1","category":"cs.LG"}
{"created":"2024-05-08 06:27:07","title":"Federated Adaptation for Foundation Model-based Recommendations","abstract":"With the recent success of large language models, particularly foundation models with generalization abilities, applying foundation models for recommendations becomes a new paradigm to improve existing recommendation systems. It becomes a new open challenge to enable the foundation model to capture user preference changes in a timely manner with reasonable communication and computation costs while preserving privacy. This paper proposes a novel federated adaptation mechanism to enhance the foundation model-based recommendation system in a privacy-preserving manner. Specifically, each client will learn a lightweight personalized adapter using its private data. The adapter then collaborates with pre-trained foundation models to provide recommendation service efficiently with fine-grained manners. Importantly, users' private behavioral data remains secure as it is not shared with the server. This data localization-based privacy preservation is embodied via the federated learning framework. The model can ensure that shared knowledge is incorporated into all adapters while simultaneously preserving each user's personal preferences. Experimental results on four benchmark datasets demonstrate our method's superior performance. Implementation code is available to ease reproducibility.","sentences":["With the recent success of large language models, particularly foundation models with generalization abilities, applying foundation models for recommendations becomes a new paradigm to improve existing recommendation systems.","It becomes a new open challenge to enable the foundation model to capture user preference changes in a timely manner with reasonable communication and computation costs while preserving privacy.","This paper proposes a novel federated adaptation mechanism to enhance the foundation model-based recommendation system in a privacy-preserving manner.","Specifically, each client will learn a lightweight personalized adapter using its private data.","The adapter then collaborates with pre-trained foundation models to provide recommendation service efficiently with fine-grained manners.","Importantly, users' private behavioral data remains secure as it is not shared with the server.","This data localization-based privacy preservation is embodied via the federated learning framework.","The model can ensure that shared knowledge is incorporated into all adapters while simultaneously preserving each user's personal preferences.","Experimental results on four benchmark datasets demonstrate our method's superior performance.","Implementation code is available to ease reproducibility."],"url":"http://arxiv.org/abs/2405.04840v1","category":"cs.IR"}
{"created":"2024-05-08 06:09:11","title":"FlexEControl: Flexible and Efficient Multimodal Control for Text-to-Image Generation","abstract":"Controllable text-to-image (T2I) diffusion models generate images conditioned on both text prompts and semantic inputs of other modalities like edge maps. Nevertheless, current controllable T2I methods commonly face challenges related to efficiency and faithfulness, especially when conditioning on multiple inputs from either the same or diverse modalities. In this paper, we propose a novel Flexible and Efficient method, FlexEControl, for controllable T2I generation. At the core of FlexEControl is a unique weight decomposition strategy, which allows for streamlined integration of various input types. This approach not only enhances the faithfulness of the generated image to the control, but also significantly reduces the computational overhead typically associated with multimodal conditioning. Our approach achieves a reduction of 41% in trainable parameters and 30% in memory usage compared with Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate images under the guidance of multiple input conditions of various modalities.","sentences":["Controllable text-to-image (T2I) diffusion models generate images conditioned on both text prompts and semantic inputs of other modalities like edge maps.","Nevertheless, current controllable T2I methods commonly face challenges related to efficiency and faithfulness, especially when conditioning on multiple inputs from either the same or diverse modalities.","In this paper, we propose a novel Flexible and Efficient method, FlexEControl, for controllable T2I generation.","At the core of FlexEControl is a unique weight decomposition strategy, which allows for streamlined integration of various input types.","This approach not only enhances the faithfulness of the generated image to the control, but also significantly reduces the computational overhead typically associated with multimodal conditioning.","Our approach achieves a reduction of 41% in trainable parameters and 30% in memory usage compared with Uni-ControlNet.","Moreover, it doubles data efficiency and can flexibly generate images under the guidance of multiple input conditions of various modalities."],"url":"http://arxiv.org/abs/2405.04834v1","category":"cs.CV"}
{"created":"2024-05-08 06:08:09","title":"Thermal analysis of black hole in de Rham--Gabadadze--Tolley massive gravity in Barrow entropy framework","abstract":"This study examines a recently hypothesized black hole solution in de Rham--Gabadadze--Tolley massive gravity. Firstly, we consider the negative cosmological constant as a thermodynamic pressure. We extract the thermodynamical properties such as Hawking temperature, heat capacity and Gibbs free energy using the Barrow entropy. We also obtain a new pressure associated to the perfect fluid dark matter and discuss the first-order van der Waals-like phase transition. This black hole's stability is investigated through specific heat and Gibbs free energy. Also, we analyze the thermodynamic curvatures behavior of black hole through geometry methods (Weinhold, Ruppeiner, Hendi-Panahiyah-Eslam-Momennia (HPEM), and geometrothermodynamics (GTD)).","sentences":["This study examines a recently hypothesized black hole solution in de Rham--Gabadadze--Tolley massive gravity.","Firstly, we consider the negative cosmological constant as a thermodynamic pressure.","We extract the thermodynamical properties such as Hawking temperature, heat capacity and Gibbs free energy using the Barrow entropy.","We also obtain a new pressure associated to the perfect fluid dark matter and discuss the first-order van der Waals-like phase transition.","This black hole's stability is investigated through specific heat and Gibbs free energy.","Also, we analyze the thermodynamic curvatures behavior of black hole through geometry methods (Weinhold, Ruppeiner, Hendi-Panahiyah-Eslam-Momennia (HPEM), and geometrothermodynamics (GTD))."],"url":"http://arxiv.org/abs/2405.04833v1","category":"gr-qc"}
{"created":"2024-05-08 05:49:46","title":"Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution","abstract":"Ownership verification is currently the most critical and widely adopted post-hoc method to safeguard model copyright. In general, model owners exploit it to identify whether a given suspicious third-party model is stolen from them by examining whether it has particular properties `inherited' from their released models. Currently, backdoor-based model watermarks are the primary and cutting-edge methods to implant such properties in the released models. However, backdoor-based methods have two fatal drawbacks, including harmfulness and ambiguity. The former indicates that they introduce maliciously controllable misclassification behaviors ($i.e.$, backdoor) to the watermarked released models. The latter denotes that malicious users can easily pass the verification by finding other misclassified samples, leading to ownership ambiguity.   In this paper, we argue that both limitations stem from the `zero-bit' nature of existing watermarking schemes, where they exploit the status ($i.e.$, misclassified) of predictions for verification. Motivated by this understanding, we design a new watermarking paradigm, $i.e.$, Explanation as a Watermark (EaaW), that implants verification behaviors into the explanation of feature attribution instead of model predictions. Specifically, EaaW embeds a `multi-bit' watermark into the feature attribution explanation of specific trigger samples without changing the original prediction. We correspondingly design the watermark embedding and extraction algorithms inspired by explainable artificial intelligence. In particular, our approach can be used for different tasks ($e.g.$, image classification and text generation). Extensive experiments verify the effectiveness and harmlessness of our EaaW and its resistance to potential attacks.","sentences":["Ownership verification is currently the most critical and widely adopted post-hoc method to safeguard model copyright.","In general, model owners exploit it to identify whether a given suspicious third-party model is stolen from them by examining whether it has particular properties `inherited' from their released models.","Currently, backdoor-based model watermarks are the primary and cutting-edge methods to implant such properties in the released models.","However, backdoor-based methods have two fatal drawbacks, including harmfulness and ambiguity.","The former indicates that they introduce maliciously controllable misclassification behaviors ($i.e.$, backdoor) to the watermarked released models.","The latter denotes that malicious users can easily pass the verification by finding other misclassified samples, leading to ownership ambiguity.   ","In this paper, we argue that both limitations stem from the `zero-bit' nature of existing watermarking schemes, where they exploit the status ($i.e.$, misclassified) of predictions for verification.","Motivated by this understanding, we design a new watermarking paradigm, $i.e.$, Explanation as a Watermark (EaaW), that implants verification behaviors into the explanation of feature attribution instead of model predictions.","Specifically, EaaW embeds a `multi-bit' watermark into the feature attribution explanation of specific trigger samples without changing the original prediction.","We correspondingly design the watermark embedding and extraction algorithms inspired by explainable artificial intelligence.","In particular, our approach can be used for different tasks ($e.g.$, image classification and text generation).","Extensive experiments verify the effectiveness and harmlessness of our EaaW and its resistance to potential attacks."],"url":"http://arxiv.org/abs/2405.04825v1","category":"cs.CR"}
{"created":"2024-05-08 05:38:56","title":"APrompt4EM: Augmented Prompt Tuning for Generalized Entity Matching","abstract":"Generalized Entity Matching (GEM), which aims at judging whether two records represented in different formats refer to the same real-world entity, is an essential task in data management. The prompt tuning paradigm for pre-trained language models (PLMs), including the recent PromptEM model, effectively addresses the challenges of low-resource GEM in practical applications, offering a robust solution when labeled data is scarce. However, existing prompt tuning models for GEM face the challenges of prompt design and information gap. This paper introduces an augmented prompt tuning framework for the challenges, which consists of two main improvements. The first is an augmented contextualized soft token-based prompt tuning method that extracts a guiding soft token benefit for the PLMs' prompt tuning, and the second is a cost-effective information augmentation strategy leveraging large language models (LLMs). Our approach performs well on the low-resource GEM challenges. Extensive experiments show promising advancements of our basic model without information augmentation over existing methods based on moderate-size PLMs (average 5.24%+), and our model with information augmentation achieves comparable performance compared with fine-tuned LLMs, using less than 14% of the API fee.","sentences":["Generalized Entity Matching (GEM), which aims at judging whether two records represented in different formats refer to the same real-world entity, is an essential task in data management.","The prompt tuning paradigm for pre-trained language models (PLMs), including the recent PromptEM model, effectively addresses the challenges of low-resource GEM in practical applications, offering a robust solution when labeled data is scarce.","However, existing prompt tuning models for GEM face the challenges of prompt design and information gap.","This paper introduces an augmented prompt tuning framework for the challenges, which consists of two main improvements.","The first is an augmented contextualized soft token-based prompt tuning method that extracts a guiding soft token benefit for the PLMs' prompt tuning, and the second is a cost-effective information augmentation strategy leveraging large language models (LLMs).","Our approach performs well on the low-resource GEM challenges.","Extensive experiments show promising advancements of our basic model without information augmentation over existing methods based on moderate-size PLMs (average 5.24%+), and our model with information augmentation achieves comparable performance compared with fine-tuned LLMs, using less than 14% of the API fee."],"url":"http://arxiv.org/abs/2405.04820v1","category":"cs.CL"}
{"created":"2024-05-08 05:38:20","title":"DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature","abstract":"Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at https://github.com/David-Li0406/DALK.","sentences":["Recent advancements in large language models (LLMs) have achieved promising performances across various applications.","Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains.","In this work, we introduce DALK, a.k.a.","Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority.","With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities.","The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK.","Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM.","We will release the code and data at https://github.com/David-Li0406/DALK."],"url":"http://arxiv.org/abs/2405.04819v1","category":"cs.CL"}
{"created":"2024-05-08 05:36:52","title":"ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation","abstract":"Evaluating free-text explanations is a multifaceted, subjective, and labor-intensive task. Large language models (LLMs) present an appealing alternative due to their potential for consistency, scalability, and cost-efficiency. In this work, we present ACORN, a new dataset of 3,500 free-text explanations and aspect-wise quality ratings, and use it to gain insights into how LLMs evaluate explanations. We observed that replacing one of the human ratings sometimes maintained, but more often lowered the inter-annotator agreement across different settings and quality aspects, suggesting that their judgments are not always consistent with human raters. We further quantified this difference by comparing the correlation between LLM-generated ratings with majority-voted human ratings across different quality aspects. With the best system, Spearman's rank correlation ranged between 0.53 to 0.95, averaging 0.72 across aspects, indicating moderately high but imperfect alignment. Finally, we considered the alternative of using an LLM as an additional rater when human raters are scarce, and measured the correlation between majority-voted labels with a limited human pool and LLMs as an additional rater, compared to the original gold labels. While GPT-4 improved the outcome when there were only two human raters, in all other observed cases, LLMs were neutral to detrimental when there were three or more human raters. We publicly release the dataset to support future improvements in LLM-in-the-loop evaluation here: https://github.com/a-brassard/ACORN.","sentences":["Evaluating free-text explanations is a multifaceted, subjective, and labor-intensive task.","Large language models (LLMs) present an appealing alternative due to their potential for consistency, scalability, and cost-efficiency.","In this work, we present ACORN, a new dataset of 3,500 free-text explanations and aspect-wise quality ratings, and use it to gain insights into how LLMs evaluate explanations.","We observed that replacing one of the human ratings sometimes maintained, but more often lowered the inter-annotator agreement across different settings and quality aspects, suggesting that their judgments are not always consistent with human raters.","We further quantified this difference by comparing the correlation between LLM-generated ratings with majority-voted human ratings across different quality aspects.","With the best system, Spearman's rank correlation ranged between 0.53 to 0.95, averaging 0.72 across aspects, indicating moderately high but imperfect alignment.","Finally, we considered the alternative of using an LLM as an additional rater when human raters are scarce, and measured the correlation between majority-voted labels with a limited human pool and LLMs as an additional rater, compared to the original gold labels.","While GPT-4 improved the outcome when there were only two human raters, in all other observed cases, LLMs were neutral to detrimental when there were three or more human raters.","We publicly release the dataset to support future improvements in LLM-in-the-loop evaluation here: https://github.com/a-brassard/ACORN."],"url":"http://arxiv.org/abs/2405.04818v1","category":"cs.CL"}
{"created":"2024-05-08 04:59:59","title":"A Novel Technique for Query Plan Representation Based on Graph Neural Networks","abstract":"Learning representations for query plans play a pivotal role in machine learning-based query optimizers of database management systems. To this end, particular model architectures are proposed in the literature to convert the tree-structured query plans into representations with formats learnable by downstream machine learning models. However, existing research rarely compares and analyzes the query plan representation capabilities of these tree models and their direct impact on the performance of the overall optimizer. To address this problem, we perform a comparative study to explore the effect of using different state-of-the-art tree models on the optimizer's cost estimation and plan selection performance in relatively complex workloads. Additionally, we explore the possibility of using graph neural networks (GNN) in the query plan representation task. We propose a novel tree model combining directed GNN with Gated Recurrent Units (GRU) and demonstrate experimentally that the new tree model provides significant improvements to cost estimation tasks and relatively excellent plan selection performance compared to the state-of-the-art tree models.","sentences":["Learning representations for query plans play a pivotal role in machine learning-based query optimizers of database management systems.","To this end, particular model architectures are proposed in the literature to convert the tree-structured query plans into representations with formats learnable by downstream machine learning models.","However, existing research rarely compares and analyzes the query plan representation capabilities of these tree models and their direct impact on the performance of the overall optimizer.","To address this problem, we perform a comparative study to explore the effect of using different state-of-the-art tree models on the optimizer's cost estimation and plan selection performance in relatively complex workloads.","Additionally, we explore the possibility of using graph neural networks (GNN) in the query plan representation task.","We propose a novel tree model combining directed GNN with Gated Recurrent Units (GRU) and demonstrate experimentally that the new tree model provides significant improvements to cost estimation tasks and relatively excellent plan selection performance compared to the state-of-the-art tree models."],"url":"http://arxiv.org/abs/2405.04814v1","category":"cs.DB"}
{"created":"2024-05-08 04:54:48","title":"General Place Recognition Survey: Towards Real-World Autonomy","abstract":"In the realm of robotics, the quest for achieving real-world autonomy, capable of executing large-scale and long-term operations, has positioned place recognition (PR) as a cornerstone technology. Despite the PR community's remarkable strides over the past two decades, garnering attention from fields like computer vision and robotics, the development of PR methods that sufficiently support real-world robotic systems remains a challenge. This paper aims to bridge this gap by highlighting the crucial role of PR within the framework of Simultaneous Localization and Mapping (SLAM) 2.0. This new phase in robotic navigation calls for scalable, adaptable, and efficient PR solutions by integrating advanced artificial intelligence (AI) technologies. For this goal, we provide a comprehensive review of the current state-of-the-art (SOTA) advancements in PR, alongside the remaining challenges, and underscore its broad applications in robotics.   This paper begins with an exploration of PR's formulation and key research challenges. We extensively review literature, focusing on related methods on place representation and solutions to various PR challenges. Applications showcasing PR's potential in robotics, key PR datasets, and open-source libraries are discussed. We also emphasizes our open-source package, aimed at new development and benchmark for general PR. We conclude with a discussion on PR's future directions, accompanied by a summary of the literature covered and access to our open-source library, available to the robotics community at: https://github.com/MetaSLAM/GPRS.","sentences":["In the realm of robotics, the quest for achieving real-world autonomy, capable of executing large-scale and long-term operations, has positioned place recognition (PR) as a cornerstone technology.","Despite the PR community's remarkable strides over the past two decades, garnering attention from fields like computer vision and robotics, the development of PR methods that sufficiently support real-world robotic systems remains a challenge.","This paper aims to bridge this gap by highlighting the crucial role of PR within the framework of Simultaneous Localization and Mapping (SLAM) 2.0.","This new phase in robotic navigation calls for scalable, adaptable, and efficient PR solutions by integrating advanced artificial intelligence (AI) technologies.","For this goal, we provide a comprehensive review of the current state-of-the-art (SOTA) advancements in PR, alongside the remaining challenges, and underscore its broad applications in robotics.   ","This paper begins with an exploration of PR's formulation and key research challenges.","We extensively review literature, focusing on related methods on place representation and solutions to various PR challenges.","Applications showcasing PR's potential in robotics, key PR datasets, and open-source libraries are discussed.","We also emphasizes our open-source package, aimed at new development and benchmark for general PR.","We conclude with a discussion on PR's future directions, accompanied by a summary of the literature covered and access to our open-source library, available to the robotics community at: https://github.com/MetaSLAM/GPRS."],"url":"http://arxiv.org/abs/2405.04812v1","category":"cs.RO"}
{"created":"2024-05-08 04:51:56","title":"A general error analysis for randomized low-rank approximation with application to data assimilation","abstract":"Randomized algorithms have proven to perform well on a large class of numerical linear algebra problems. Their theoretical analysis is critical to provide guarantees on their behaviour, and in this sense, the stochastic analysis of the randomized low-rank approximation error plays a central role. Indeed, several randomized methods for the approximation of dominant eigen- or singular modes can be rewritten as low-rank approximation methods. However, despite the large variety of algorithms, the existing theoretical frameworks for their analysis rely on a specific structure for the covariance matrix that is not adapted to all the algorithms. We propose a general framework for the stochastic analysis of the low-rank approximation error in Frobenius norm for centered and non-standard Gaussian matrices. Under minimal assumptions on the covariance matrix, we derive accurate bounds both in expectation and probability. Our bounds have clear interpretations that enable us to derive properties and motivate practical choices for the covariance matrix resulting in efficient low-rank approximation algorithms. The most commonly used bounds in the literature have been demonstrated as a specific instance of the bounds proposed here, with the additional contribution of being tighter. Numerical experiments related to data assimilation further illustrate that exploiting the problem structure to select the covariance matrix improves the performance as suggested by our bounds.","sentences":["Randomized algorithms have proven to perform well on a large class of numerical linear algebra problems.","Their theoretical analysis is critical to provide guarantees on their behaviour, and in this sense, the stochastic analysis of the randomized low-rank approximation error plays a central role.","Indeed, several randomized methods for the approximation of dominant eigen- or singular modes can be rewritten as low-rank approximation methods.","However, despite the large variety of algorithms, the existing theoretical frameworks for their analysis rely on a specific structure for the covariance matrix that is not adapted to all the algorithms.","We propose a general framework for the stochastic analysis of the low-rank approximation error in Frobenius norm for centered and non-standard Gaussian matrices.","Under minimal assumptions on the covariance matrix, we derive accurate bounds both in expectation and probability.","Our bounds have clear interpretations that enable us to derive properties and motivate practical choices for the covariance matrix resulting in efficient low-rank approximation algorithms.","The most commonly used bounds in the literature have been demonstrated as a specific instance of the bounds proposed here, with the additional contribution of being tighter.","Numerical experiments related to data assimilation further illustrate that exploiting the problem structure to select the covariance matrix improves the performance as suggested by our bounds."],"url":"http://arxiv.org/abs/2405.04811v1","category":"math.NA"}
{"created":"2024-05-08 04:26:44","title":"Variational analysis of unbounded and discontinuous generalized eigenvalue functions with application to topology optimization","abstract":"The maximum (or minimum) generalized eigenvalue of symmetric positive semidefinite matrices that depend on optimization variables often appears as objective or constraint functions in structural topology optimization when we consider robustness, vibration, and buckling. It can be an unbounded or discontinuous function where matrices become singular (where a topological change of the structural design occurs). Based on variational analysis, we redefine the maximum (and minimum) generalized eigenvalue function as an extended real-valued function and propose a real-valued continuous approximation of it. Then, we show that the proposed approximation epi-converges to the original redefined function, which justifies solving problems with the approximation instead. We consider two specific topology optimization problems: robust compliance optimization and eigenfrequency optimization and conduct simple numerical experiments.","sentences":["The maximum (or minimum) generalized eigenvalue of symmetric positive semidefinite matrices that depend on optimization variables often appears as objective or constraint functions in structural topology optimization when we consider robustness, vibration, and buckling.","It can be an unbounded or discontinuous function where matrices become singular (where a topological change of the structural design occurs).","Based on variational analysis, we redefine the maximum (and minimum) generalized eigenvalue function as an extended real-valued function and propose a real-valued continuous approximation of it.","Then, we show that the proposed approximation epi-converges to the original redefined function, which justifies solving problems with the approximation instead.","We consider two specific topology optimization problems: robust compliance optimization and eigenfrequency optimization and conduct simple numerical experiments."],"url":"http://arxiv.org/abs/2405.04805v1","category":"math.OC"}
{"created":"2024-05-08 04:26:32","title":"WixUp: A General Data Augmentation Framework for Wireless Perception in Tracking of Humans","abstract":"Recent advancements in wireless perception technologies, including mmWave, WiFi, and acoustics, have expanded their application in human motion tracking and health monitoring. They are promising alternatives to traditional camera-based perception systems, thanks to their efficacy under diverse conditions or occlusions, and enhanced privacy. However, the integration of deep learning within this field introduces new challenges such as the need for extensive training data and poor model generalization, especially with sparse and noisy wireless point clouds. As a remedy, data augmentation is one solution well-explored in other deep learning fields, but they are not directly applicable to the unique characteristics of wireless signals. This motivates us to propose a custom data augmentation framework, WixUp, tailored for wireless perception. Moreover, we aim to make it a general framework supporting various datasets, model architectures, sensing modalities, and tasks; while previous wireless data augmentation or generative simulations do not exhibit this generalizability, only limited to certain use cases. More specifically, WixUp can reverse-transform lossy coordinates into dense range profiles using Gaussian mixture and probability tricks, making it capable of in-depth data diversity enhancement; and its mixing-based method enables unsupervised domain adaptation via self-training, allowing training of the model with no labels from new users or environments in practice. In summary, our extensive evaluation experiments show that WixUp provides consistent performance improvement across various scenarios and outperforms the baselines.","sentences":["Recent advancements in wireless perception technologies, including mmWave, WiFi, and acoustics, have expanded their application in human motion tracking and health monitoring.","They are promising alternatives to traditional camera-based perception systems, thanks to their efficacy under diverse conditions or occlusions, and enhanced privacy.","However, the integration of deep learning within this field introduces new challenges such as the need for extensive training data and poor model generalization, especially with sparse and noisy wireless point clouds.","As a remedy, data augmentation is one solution well-explored in other deep learning fields, but they are not directly applicable to the unique characteristics of wireless signals.","This motivates us to propose a custom data augmentation framework, WixUp, tailored for wireless perception.","Moreover, we aim to make it a general framework supporting various datasets, model architectures, sensing modalities, and tasks; while previous wireless data augmentation or generative simulations do not exhibit this generalizability, only limited to certain use cases.","More specifically, WixUp can reverse-transform lossy coordinates into dense range profiles using Gaussian mixture and probability tricks, making it capable of in-depth data diversity enhancement; and its mixing-based method enables unsupervised domain adaptation via self-training, allowing training of the model with no labels from new users or environments in practice.","In summary, our extensive evaluation experiments show that WixUp provides consistent performance improvement across various scenarios and outperforms the baselines."],"url":"http://arxiv.org/abs/2405.04804v1","category":"cs.NI"}
{"created":"2024-05-08 04:21:15","title":"Repdigits as difference of two balancing or Lucas-balancing numbers","abstract":"Repdigits are natural numbers formed by the repetition of a single digit. In this paper, we study the problem of writing repdigits as the difference of two balancing or Lucas-balancing numbers. The method of proof involves the application of Baker's theory for linear forms in logarithms of algebraic numbers and the Baker-Davenport reduction procedure. Computations are done with the help of a simple computer program in {\\it Mathematica}.","sentences":["Repdigits are natural numbers formed by the repetition of a single digit.","In this paper, we study the problem of writing repdigits as the difference of two balancing or Lucas-balancing numbers.","The method of proof involves the application of Baker's theory for linear forms in logarithms of algebraic numbers and the Baker-Davenport reduction procedure.","Computations are done with the help of a simple computer program in {\\it Mathematica}."],"url":"http://arxiv.org/abs/2405.04801v1","category":"math.GM"}
{"created":"2024-05-08 04:14:06","title":"From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control","abstract":"Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. \\method~uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that \\method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.","sentences":["Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies.","With the advent of LLMs, language has been emerging as a prospective interface layer.","However, this has several limitations.","Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine).","Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting.","We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations.","\\method~uses a learnable latent code to act as a bridge between LLMs and low-level policies.","This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations.","Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training.","Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that \\method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors."],"url":"http://arxiv.org/abs/2405.04798v1","category":"cs.RO"}
{"created":"2024-05-08 04:10:53","title":"Bouncing cosmological models in $f(R,L_m)$ gravity","abstract":"This article explores matter bounce non-singular cosmology in $f(R,L_m)$ gravity. We consider two non-linear $f(R,L_m)$ functional forms, specifically, $f(R,L_m) = \\frac{R}{2} + \\lambda R^2 + \\alpha L_m$ and $f(R,L_m) = \\frac{R}{2} + L_m ^\\beta + \\gamma$ representing a minimal coupling case. We derive the corresponding Friedmann-like equations for both the assumed models in the FLRW background, and then we present the impact of the model parameters along with the parameter of bouncing scale factor on the equation of state parameter, pressure, and the energy density. In addition, we examine the dynamical behavior of cosmographic parameters such as jerk, lerk, and snap parameters. Further, we find that the violation of the null energy condition along with the strong energy condition depicts the non-singular accelerating behavior, corresponding to both assumed non-linear $f(R,L_m)$ functions. Lastly, we present the behavior of the adiabatic speed of sound to examine the viability of the considered cosmological bouncing scenario.","sentences":["This article explores matter bounce non-singular cosmology in $f(R,L_m)$ gravity.","We consider two non-linear $f(R,L_m)$ functional forms, specifically, $f(R,L_m) = \\frac{R}{2} + \\lambda R^2 + \\alpha L_m$ and $f(R,L_m) = \\frac{R}{2} + L_m ^\\beta + \\gamma$ representing a minimal coupling case.","We derive the corresponding Friedmann-like equations for both the assumed models in the FLRW background, and then we present the impact of the model parameters along with the parameter of bouncing scale factor on the equation of state parameter, pressure, and the energy density.","In addition, we examine the dynamical behavior of cosmographic parameters such as jerk, lerk, and snap parameters.","Further, we find that the violation of the null energy condition along with the strong energy condition depicts the non-singular accelerating behavior, corresponding to both assumed non-linear $f(R,L_m)$ functions.","Lastly, we present the behavior of the adiabatic speed of sound to examine the viability of the considered cosmological bouncing scenario."],"url":"http://arxiv.org/abs/2405.04797v1","category":"gr-qc"}
{"created":"2024-05-08 04:01:40","title":"Variational Schr\u00f6dinger Diffusion Models","abstract":"Schr\\\"odinger bridge (SB) has emerged as the go-to method for optimizing transportation plans in diffusion models. However, SB requires estimating the intractable forward score functions, inevitably resulting in the costly implicit training loss based on simulated trajectories. To improve the scalability while preserving efficient transportation plans, we leverage variational inference to linearize the forward score functions (variational scores) of SB and restore simulation-free properties in training backward scores. We propose the variational Schr\\\"odinger diffusion model (VSDM), where the forward process is a multivariate diffusion and the variational scores are adaptively optimized for efficient transport. Theoretically, we use stochastic approximation to prove the convergence of the variational scores and show the convergence of the adaptively generated samples based on the optimal variational scores. Empirically, we test the algorithm in simulated examples and observe that VSDM is efficient in generations of anisotropic shapes and yields straighter sample trajectories compared to the single-variate diffusion. We also verify the scalability of the algorithm in real-world data and achieve competitive unconditional generation performance in CIFAR10 and conditional generation in time series modeling. Notably, VSDM no longer depends on warm-up initializations and has become tuning-friendly in training large-scale experiments.","sentences":["Schr\\\"odinger bridge (SB) has emerged as the go-to method for optimizing transportation plans in diffusion models.","However, SB requires estimating the intractable forward score functions, inevitably resulting in the costly implicit training loss based on simulated trajectories.","To improve the scalability while preserving efficient transportation plans, we leverage variational inference to linearize the forward score functions (variational scores) of SB and restore simulation-free properties in training backward scores.","We propose the variational Schr\\\"odinger diffusion model (VSDM), where the forward process is a multivariate diffusion and the variational scores are adaptively optimized for efficient transport.","Theoretically, we use stochastic approximation to prove the convergence of the variational scores and show the convergence of the adaptively generated samples based on the optimal variational scores.","Empirically, we test the algorithm in simulated examples and observe that VSDM is efficient in generations of anisotropic shapes and yields straighter sample trajectories compared to the single-variate diffusion.","We also verify the scalability of the algorithm in real-world data and achieve competitive unconditional generation performance in CIFAR10 and conditional generation in time series modeling.","Notably, VSDM no longer depends on warm-up initializations and has become tuning-friendly in training large-scale experiments."],"url":"http://arxiv.org/abs/2405.04795v1","category":"cs.LG"}
{"created":"2024-05-08 03:57:45","title":"Zero-shot LLM-guided Counterfactual Generation for Text","abstract":"Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets. Collecting and annotating such datasets for counterfactual generation is labor intensive and therefore, infeasible in practice. Therefore, in this work, we focus on a novel problem setting: \\textit{zero-shot counterfactual generation}. To this end, we propose a structured way to utilize large language models (LLMs) as general purpose counterfactual example generators. We hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero-shot manner, without requiring any training or fine-tuning. Through comprehensive experiments on various downstream tasks in natural language processing (NLP), we demonstrate the efficacy of LLMs as zero-shot counterfactual generators in evaluating and explaining black-box NLP models.","sentences":["Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks.","Although methods for automated counterfactual generation have been explored, such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets.","Collecting and annotating such datasets for counterfactual generation is labor intensive and therefore, infeasible in practice.","Therefore, in this work, we focus on a novel problem setting: \\textit{zero-shot counterfactual generation}.","To this end, we propose a structured way to utilize large language models (LLMs) as general purpose counterfactual example generators.","We hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero-shot manner, without requiring any training or fine-tuning.","Through comprehensive experiments on various downstream tasks in natural language processing (NLP), we demonstrate the efficacy of LLMs as zero-shot counterfactual generators in evaluating and explaining black-box NLP models."],"url":"http://arxiv.org/abs/2405.04793v1","category":"cs.CL"}
{"created":"2024-05-08 03:52:04","title":"Quantifying Smooth Muscles Regional Organization in the Rat Bladder Using Immunohistochemistry, Multiphoton Microscopy and Machine Learning","abstract":"The smooth muscle bundles (SMBs) in the bladder act as contractile elements which enable the bladder to void effectively. In contrast to skeletal muscles, these bundles are not highly aligned, rather they are oriented more heterogeneously throughout the bladder wall. In this work, for the first time, this regional orientation of the SMBs is quantified across the whole bladder, without the need for optical clearing or cryosectioning. Immunohistochemistry staining was utilized to visualize smooth muscle cell actin in multiphoton microscopy (MPM) images of bladder smooth muscle bundles (SMBs). Feature vectors for each pixel were generated using a range of filters, including Gaussian blur, Gaussian gradient magnitude, Laplacian of Gaussian, Hessian eigenvalues, structure tensor eigenvalues, Gabor, and Sobel gradients. A Random Forest classifier was subsequently trained to automate the segmentation of SMBs in the MPM images. Finally, the orientation of SMBs in each bladder region was quantified using the CT-FIRE package. This information is essential for biomechanical models of the bladder that include contractile elements.","sentences":["The smooth muscle bundles (SMBs) in the bladder act as contractile elements which enable the bladder to void effectively.","In contrast to skeletal muscles, these bundles are not highly aligned, rather they are oriented more heterogeneously throughout the bladder wall.","In this work, for the first time, this regional orientation of the SMBs is quantified across the whole bladder, without the need for optical clearing or cryosectioning.","Immunohistochemistry staining was utilized to visualize smooth muscle cell actin in multiphoton microscopy (MPM) images of bladder smooth muscle bundles (SMBs).","Feature vectors for each pixel were generated using a range of filters, including Gaussian blur, Gaussian gradient magnitude, Laplacian of Gaussian, Hessian eigenvalues, structure tensor eigenvalues, Gabor, and Sobel gradients.","A Random Forest classifier was subsequently trained to automate the segmentation of SMBs in the MPM images.","Finally, the orientation of SMBs in each bladder region was quantified using the CT-FIRE package.","This information is essential for biomechanical models of the bladder that include contractile elements."],"url":"http://arxiv.org/abs/2405.04790v1","category":"q-bio.TO"}
{"created":"2024-05-08 03:43:58","title":"DiffMatch: Visual-Language Guidance Makes Better Semi-supervised Change Detector","abstract":"Change Detection (CD) aims to identify pixels with semantic changes between images. However, annotating massive numbers of pixel-level images is labor-intensive and costly, especially for multi-temporal images, which require pixel-wise comparisons by human experts. Considering the excellent performance of visual language models (VLMs) for zero-shot, open-vocabulary, etc. with prompt-based reasoning, it is promising to utilize VLMs to make better CD under limited labeled data. In this paper, we propose a VLM guidance-based semi-supervised CD method, namely DiffMatch. The insight of DiffMatch is to synthesize free change labels using VLMs to provide additional supervision signals for unlabeled data. However, almost all current VLMs are designed for single-temporal images and cannot be directly applied to bi- or multi-temporal images. Motivated by this, we first propose a VLM-based mixed change event generation (CEG) strategy to yield pseudo labels for unlabeled CD data. Since the additional supervised signals provided by these VLM-driven pseudo labels may conflict with the pseudo labels from the consistency regularization paradigm (e.g. FixMatch), we propose the dual projection head for de-entangling different signal sources. Further, we explicitly decouple the bi-temporal images semantic representation through two auxiliary segmentation decoders, which are also guided by VLM. Finally, to make the model more adequately capture change representations, we introduce metric-aware supervision by feature-level contrastive loss in auxiliary branches. Extensive experiments show the advantage of DiffMatch. For instance, DiffMatch improves the FixMatch baseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In addition, our CEG strategy, in an un-supervised manner, can achieve performance far superior to state-of-the-art un-supervised CD methods.","sentences":["Change Detection (CD) aims to identify pixels with semantic changes between images.","However, annotating massive numbers of pixel-level images is labor-intensive and costly, especially for multi-temporal images, which require pixel-wise comparisons by human experts.","Considering the excellent performance of visual language models (VLMs) for zero-shot, open-vocabulary, etc. with prompt-based reasoning, it is promising to utilize VLMs to make better CD under limited labeled data.","In this paper, we propose a VLM guidance-based semi-supervised CD method, namely DiffMatch.","The insight of DiffMatch is to synthesize free change labels using VLMs to provide additional supervision signals for unlabeled data.","However, almost all current VLMs are designed for single-temporal images and cannot be directly applied to bi- or multi-temporal images.","Motivated by this, we first propose a VLM-based mixed change event generation (CEG) strategy to yield pseudo labels for unlabeled CD data.","Since the additional supervised signals provided by these VLM-driven pseudo labels may conflict with the pseudo labels from the consistency regularization paradigm (e.g. FixMatch), we propose the dual projection head for de-entangling different signal sources.","Further, we explicitly decouple the bi-temporal images semantic representation through two auxiliary segmentation decoders, which are also guided by VLM.","Finally, to make the model more adequately capture change representations, we introduce metric-aware supervision by feature-level contrastive loss in auxiliary branches.","Extensive experiments show the advantage of DiffMatch.","For instance, DiffMatch improves the FixMatch baseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels.","In addition, our CEG strategy, in an un-supervised manner, can achieve performance far superior to state-of-the-art un-supervised CD methods."],"url":"http://arxiv.org/abs/2405.04788v1","category":"cs.CV"}
{"created":"2024-05-08 03:16:59","title":"GoalGrasp: Grasping Goals in Partially Occluded Scenarios without Grasp Training","abstract":"We present GoalGrasp, a simple yet effective 6-DOF robot grasp pose detection method that does not rely on grasp pose annotations and grasp training. Our approach enables user-specified object grasping in partially occluded scenes. By combining 3D bounding boxes and simple human grasp priors, our method introduces a novel paradigm for robot grasp pose detection. First, we employ a 3D object detector named RCV, which requires no 3D annotations, to achieve rapid 3D detection in new scenes. Leveraging the 3D bounding box and human grasp priors, our method achieves dense grasp pose detection. The experimental evaluation involves 18 common objects categorized into 7 classes based on shape. Without grasp training, our method generates dense grasp poses for 1000 scenes. We compare our method's grasp poses to existing approaches using a novel stability metric, demonstrating significantly higher grasp pose stability. In user-specified robot grasping experiments, our approach achieves a 94% grasp success rate. Moreover, in user-specified grasping experiments under partial occlusion, the success rate reaches 92%.","sentences":["We present GoalGrasp, a simple yet effective 6-DOF robot grasp pose detection method that does not rely on grasp pose annotations and grasp training.","Our approach enables user-specified object grasping in partially occluded scenes.","By combining 3D bounding boxes and simple human grasp priors, our method introduces a novel paradigm for robot grasp pose detection.","First, we employ a 3D object detector named RCV, which requires no 3D annotations, to achieve rapid 3D detection in new scenes.","Leveraging the 3D bounding box and human grasp priors, our method achieves dense grasp pose detection.","The experimental evaluation involves 18 common objects categorized into 7 classes based on shape.","Without grasp training, our method generates dense grasp poses for 1000 scenes.","We compare our method's grasp poses to existing approaches using a novel stability metric, demonstrating significantly higher grasp pose stability.","In user-specified robot grasping experiments, our approach achieves a 94% grasp success rate.","Moreover, in user-specified grasping experiments under partial occlusion, the success rate reaches 92%."],"url":"http://arxiv.org/abs/2405.04783v1","category":"cs.RO"}
{"created":"2024-05-08 03:01:22","title":"Strictification of $\\infty$-Groupoids is Comonadic","abstract":"We investigate the universal strictification adjunction from weak $\\infty$-groupoids (modeled as simplicial sets) to strict $\\infty$-groupoids (modeled as simplicial T-complexes). We prove that any simplicial set can be recovered up to weak homotopy equivalence as the totalization of its canonical cosimplicial resolution induced by this adjunction. This generalizes the fact due to Bousfield and Kan that the homotopy type of a simply connected space can be recovered as the totalization of its canonical cosimplicial resolution induced by the free simplicial abelian group adjunction. Furthermore, we leverage this result to show that this strictification adjunction induces a comonadic adjunction between the quasicategories of simplicial sets and strict $\\infty$-groupoids.","sentences":["We investigate the universal strictification adjunction from weak $\\infty$-groupoids (modeled as simplicial sets) to strict $\\infty$-groupoids (modeled as simplicial T-complexes).","We prove that any simplicial set can be recovered up to weak homotopy equivalence as the totalization of its canonical cosimplicial resolution induced by this adjunction.","This generalizes the fact due to Bousfield and Kan that the homotopy type of a simply connected space can be recovered as the totalization of its canonical cosimplicial resolution induced by the free simplicial abelian group adjunction.","Furthermore, we leverage this result to show that this strictification adjunction induces a comonadic adjunction between the quasicategories of simplicial sets and strict $\\infty$-groupoids."],"url":"http://arxiv.org/abs/2405.04780v1","category":"math.AT"}
{"created":"2024-05-08 02:48:52","title":"Teacher-Student Network for Real-World Face Super-Resolution with Progressive Embedding of Edge Information","abstract":"Traditional face super-resolution (FSR) methods trained on synthetic datasets usually have poor generalization ability for real-world face images. Recent work has utilized complex degradation models or training networks to simulate the real degradation process, but this limits the performance of these methods due to the domain differences that still exist between the generated low-resolution images and the real low-resolution images. Moreover, because of the existence of a domain gap, the semantic feature information of the target domain may be affected when synthetic data and real data are utilized to train super-resolution models simultaneously. In this study, a real-world face super-resolution teacher-student model is proposed, which considers the domain gap between real and synthetic data and progressively includes diverse edge information by using the recurrent network's intermediate outputs. Extensive experiments demonstrate that our proposed approach surpasses state-of-the-art methods in obtaining high-quality face images for real-world FSR.","sentences":["Traditional face super-resolution (FSR) methods trained on synthetic datasets usually have poor generalization ability for real-world face images.","Recent work has utilized complex degradation models or training networks to simulate the real degradation process, but this limits the performance of these methods due to the domain differences that still exist between the generated low-resolution images and the real low-resolution images.","Moreover, because of the existence of a domain gap, the semantic feature information of the target domain may be affected when synthetic data and real data are utilized to train super-resolution models simultaneously.","In this study, a real-world face super-resolution teacher-student model is proposed, which considers the domain gap between real and synthetic data and progressively includes diverse edge information by using the recurrent network's intermediate outputs.","Extensive experiments demonstrate that our proposed approach surpasses state-of-the-art methods in obtaining high-quality face images for real-world FSR."],"url":"http://arxiv.org/abs/2405.04778v1","category":"eess.IV"}
{"created":"2024-05-08 02:48:29","title":"Empathy Through Multimodality in Conversational Interfaces","abstract":"Agents represent one of the most emerging applications of Large Language Models (LLMs) and Generative AI, with their effectiveness hinging on multimodal capabilities to navigate complex user environments. Conversational Health Agents (CHAs), a prime example of this, are redefining healthcare by offering nuanced support that transcends textual analysis to incorporate emotional intelligence. This paper introduces an LLM-based CHA engineered for rich, multimodal dialogue-especially in the realm of mental health support. It adeptly interprets and responds to users' emotional states by analyzing multimodal cues, thus delivering contextually aware and empathetically resonant verbal responses. Our implementation leverages the versatile openCHA framework, and our comprehensive evaluation involves neutral prompts expressed in diverse emotional tones: sadness, anger, and joy. We evaluate the consistency and repeatability of the planning capability of the proposed CHA. Furthermore, human evaluators critique the CHA's empathic delivery, with findings revealing a striking concordance between the CHA's outputs and evaluators' assessments. These results affirm the indispensable role of vocal (soon multimodal) emotion recognition in strengthening the empathetic connection built by CHAs, cementing their place at the forefront of interactive, compassionate digital health solutions.","sentences":["Agents represent one of the most emerging applications of Large Language Models (LLMs) and Generative AI, with their effectiveness hinging on multimodal capabilities to navigate complex user environments.","Conversational Health Agents (CHAs), a prime example of this, are redefining healthcare by offering nuanced support that transcends textual analysis to incorporate emotional intelligence.","This paper introduces an LLM-based CHA engineered for rich, multimodal dialogue-especially in the realm of mental health support.","It adeptly interprets and responds to users' emotional states by analyzing multimodal cues, thus delivering contextually aware and empathetically resonant verbal responses.","Our implementation leverages the versatile openCHA framework, and our comprehensive evaluation involves neutral prompts expressed in diverse emotional tones: sadness, anger, and joy.","We evaluate the consistency and repeatability of the planning capability of the proposed CHA.","Furthermore, human evaluators critique the CHA's empathic delivery, with findings revealing a striking concordance between the CHA's outputs and evaluators' assessments.","These results affirm the indispensable role of vocal (soon multimodal) emotion recognition in strengthening the empathetic connection built by CHAs, cementing their place at the forefront of interactive, compassionate digital health solutions."],"url":"http://arxiv.org/abs/2405.04777v1","category":"cs.CL"}
{"created":"2024-05-08 02:48:28","title":"Chain of Thoughtlessness: An Analysis of CoT in Planning","abstract":"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated by modifying prompts to include examples with chains of thought--demonstrations of solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examine the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations and depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially because of the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.","sentences":["Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution.","Previous work has claimed that this can be mitigated by modifying prompts to include examples with chains of thought--demonstrations of solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.","This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examine the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt.","While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.","Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations and depend on carefully engineering highly problem specific prompts.","This spotlights drawbacks of chain of thought, especially because of the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces."],"url":"http://arxiv.org/abs/2405.04776v1","category":"cs.AI"}
{"created":"2024-05-08 02:44:13","title":"Hypergraph-enhanced Dual Semi-supervised Graph Classification","abstract":"In this paper, we study semi-supervised graph classification, which aims at accurately predicting the categories of graphs in scenarios with limited labeled graphs and abundant unlabeled graphs. Despite the promising capability of graph neural networks (GNNs), they typically require a large number of costly labeled graphs, while a wealth of unlabeled graphs fail to be effectively utilized. Moreover, GNNs are inherently limited to encoding local neighborhood information using message-passing mechanisms, thus lacking the ability to model higher-order dependencies among nodes. To tackle these challenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classification, which captures graph semantics from the perspective of the hypergraph and the line graph, respectively. Specifically, to better explore the higher-order relationships among nodes, we design a hypergraph structure learning to adaptively learn complex node dependencies beyond pairwise relations. Meanwhile, based on the learned hypergraph, we introduce a line graph to capture the interaction between hyperedges, thereby better mining the underlying semantic structures. Finally, we develop a relational consistency learning to facilitate knowledge transfer between the two branches and provide better mutual guidance. Extensive experiments on real-world graph datasets verify the effectiveness of the proposed method against existing state-of-the-art methods.","sentences":["In this paper, we study semi-supervised graph classification, which aims at accurately predicting the categories of graphs in scenarios with limited labeled graphs and abundant unlabeled graphs.","Despite the promising capability of graph neural networks (GNNs), they typically require a large number of costly labeled graphs, while a wealth of unlabeled graphs fail to be effectively utilized.","Moreover, GNNs are inherently limited to encoding local neighborhood information using message-passing mechanisms, thus lacking the ability to model higher-order dependencies among nodes.","To tackle these challenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classification, which captures graph semantics from the perspective of the hypergraph and the line graph, respectively.","Specifically, to better explore the higher-order relationships among nodes, we design a hypergraph structure learning to adaptively learn complex node dependencies beyond pairwise relations.","Meanwhile, based on the learned hypergraph, we introduce a line graph to capture the interaction between hyperedges, thereby better mining the underlying semantic structures.","Finally, we develop a relational consistency learning to facilitate knowledge transfer between the two branches and provide better mutual guidance.","Extensive experiments on real-world graph datasets verify the effectiveness of the proposed method against existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04773v1","category":"cs.LG"}
{"created":"2024-05-08 02:43:47","title":"Cluster Alphabets from Generalized Worldsheets: A Geometric Approach to Finite Types","abstract":"We provide a systematic derivation of cluster alphabets of finite types. The construction is based on a geometric realization of the generalized worldsheets by gluing and folding a pair of polygons. The cross ratios of the worldsheet z variables are evolved using the Y-system equations. By a new gauge choice, we obtain a simpler set of cluster alphabets than the known ones.","sentences":["We provide a systematic derivation of cluster alphabets of finite types.","The construction is based on a geometric realization of the generalized worldsheets by gluing and folding a pair of polygons.","The cross ratios of the worldsheet z variables are evolved using the Y-system equations.","By a new gauge choice, we obtain a simpler set of cluster alphabets than the known ones."],"url":"http://arxiv.org/abs/2405.04772v1","category":"hep-th"}
{"created":"2024-05-08 02:33:35","title":"Inference With Combining Rules From Multiple Differentially Private Synthetic Datasets","abstract":"Differential privacy (DP) has been accepted as a rigorous criterion for measuring the privacy protection offered by random mechanisms used to obtain statistics or, as we will study here, synthetic datasets from confidential data. Methods to generate such datasets are increasingly numerous, using varied tools including Bayesian models, deep neural networks and copulas. However, little is still known about how to properly perform statistical inference with these differentially private synthetic (DIPS) datasets. The challenge is for the analyses to take into account the variability from the synthetic data generation in addition to the usual sampling variability. A similar challenge also occurs when missing data is imputed before analysis, and statisticians have developed appropriate inference procedures for this case, which we tend extended to the case of synthetic datasets for privacy. In this work, we study the applicability of these procedures, based on combining rules, to the analysis of DIPS datasets. Our empirical experiments show that the proposed combining rules may offer accurate inference in certain contexts, but not in all cases.","sentences":["Differential privacy (DP) has been accepted as a rigorous criterion for measuring the privacy protection offered by random mechanisms used to obtain statistics or, as we will study here, synthetic datasets from confidential data.","Methods to generate such datasets are increasingly numerous, using varied tools including Bayesian models, deep neural networks and copulas.","However, little is still known about how to properly perform statistical inference with these differentially private synthetic (DIPS) datasets.","The challenge is for the analyses to take into account the variability from the synthetic data generation in addition to the usual sampling variability.","A similar challenge also occurs when missing data is imputed before analysis, and statisticians have developed appropriate inference procedures for this case, which we tend extended to the case of synthetic datasets for privacy.","In this work, we study the applicability of these procedures, based on combining rules, to the analysis of DIPS datasets.","Our empirical experiments show that the proposed combining rules may offer accurate inference in certain contexts, but not in all cases."],"url":"http://arxiv.org/abs/2405.04769v1","category":"stat.ME"}
{"created":"2024-05-08 02:31:51","title":"Test-Time Augmentation for Traveling Salesperson Problem","abstract":"We propose Test-Time Augmentation (TTA) as an effective technique for addressing combinatorial optimization problems, including the Traveling Salesperson Problem. In general, deep learning models possessing the property of invariance, where the output is uniquely determined regardless of the node indices, have been proposed to learn graph structures efficiently. In contrast, we interpret the permutation of node indices, which exchanges the elements of the distance matrix, as a TTA scheme. The results demonstrate that our method is capable of obtaining shorter solutions than the latest models. Furthermore, we show that the probability of finding a solution closer to an exact solution increases depending on the augmentation size.","sentences":["We propose Test-Time Augmentation (TTA) as an effective technique for addressing combinatorial optimization problems, including the Traveling Salesperson Problem.","In general, deep learning models possessing the property of invariance, where the output is uniquely determined regardless of the node indices, have been proposed to learn graph structures efficiently.","In contrast, we interpret the permutation of node indices, which exchanges the elements of the distance matrix, as a TTA scheme.","The results demonstrate that our method is capable of obtaining shorter solutions than the latest models.","Furthermore, we show that the probability of finding a solution closer to an exact solution increases depending on the augmentation size."],"url":"http://arxiv.org/abs/2405.04767v1","category":"cs.LG"}
{"created":"2024-05-08 02:24:09","title":"When Foresight Pruning Meets Zeroth-Order Optimization: Efficient Federated Learning for Low-Memory Devices","abstract":"Although Federated Learning (FL) enables collaborative learning in Artificial Intelligence of Things (AIoT) design, it fails to work on low-memory AIoT devices due to its heavy memory usage. To address this problem, various federated pruning methods are proposed to reduce memory usage during inference. However, few of them can substantially mitigate the memory burdens during pruning and training. As an alternative, zeroth-order or backpropagation-free (BP-Free) methods can partially alleviate the memory consumption, but they suffer from scaling up and large computation overheads, since the gradient estimation error and floating point operations (FLOPs) increase as the dimensionality of the model parameters grows. In this paper, we propose a federated foresight pruning method based on Neural Tangent Kernel (NTK), which can seamlessly integrate with federated BP-Free training frameworks. We present an approximation to the computation of federated NTK by using the local NTK matrices. Moreover, we demonstrate that the data-free property of our method can substantially reduce the approximation error in extreme data heterogeneity scenarios. Since our approach improves the performance of the vanilla BP-Free method with fewer FLOPs and truly alleviates memory pressure during training and inference, it makes FL more friendly to low-memory devices. Comprehensive experimental results obtained from simulation- and real test-bed-based platforms show that our federated foresight-pruning method not only preserves the ability of the dense model with a memory reduction up to 9x but also boosts the performance of the vanilla BP-Free method with dramatically fewer FLOPs.","sentences":["Although Federated Learning (FL) enables collaborative learning in Artificial Intelligence of Things (AIoT) design, it fails to work on low-memory AIoT devices due to its heavy memory usage.","To address this problem, various federated pruning methods are proposed to reduce memory usage during inference.","However, few of them can substantially mitigate the memory burdens during pruning and training.","As an alternative, zeroth-order or backpropagation-free (BP-Free) methods can partially alleviate the memory consumption, but they suffer from scaling up and large computation overheads, since the gradient estimation error and floating point operations (FLOPs) increase as the dimensionality of the model parameters grows.","In this paper, we propose a federated foresight pruning method based on Neural Tangent Kernel (NTK), which can seamlessly integrate with federated BP-Free training frameworks.","We present an approximation to the computation of federated NTK by using the local NTK matrices.","Moreover, we demonstrate that the data-free property of our method can substantially reduce the approximation error in extreme data heterogeneity scenarios.","Since our approach improves the performance of the vanilla BP-Free method with fewer FLOPs and truly alleviates memory pressure during training and inference, it makes FL more friendly to low-memory devices.","Comprehensive experimental results obtained from simulation- and real test-bed-based platforms show that our federated foresight-pruning method not only preserves the ability of the dense model with a memory reduction up to 9x but also boosts the performance of the vanilla BP-Free method with dramatically fewer FLOPs."],"url":"http://arxiv.org/abs/2405.04765v1","category":"cs.LG"}
{"created":"2024-05-08 02:09:17","title":"Large Language Models for Cyber Security: A Systematic Literature Review","abstract":"The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in various domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity (LLM4Security). By comprehensively collecting over 30K relevant papers and systematically analyzing 127 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain. Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to a wide range of cybersecurity tasks, including vulnerability detection, malware analysis, network intrusion detection, and phishing detection. Second, we find that the datasets used for training and evaluating LLMs in these tasks are often limited in size and diversity, highlighting the need for more comprehensive and representative datasets. Third, we identify several promising techniques for adapting LLMs to specific cybersecurity domains, such as fine-tuning, transfer learning, and domain-specific pre-training. Finally, we discuss the main challenges and opportunities for future research in LLM4Security, including the need for more interpretable and explainable models, the importance of addressing data privacy and security concerns, and the potential for leveraging LLMs for proactive defense and threat hunting. Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research.","sentences":["The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in various domains, including cybersecurity.","As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks.","In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity (LLM4Security).","By comprehensively collecting over 30K relevant papers and systematically analyzing 127 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain.","Through our analysis, we identify several key findings.","First, we observe that LLMs are being applied to a wide range of cybersecurity tasks, including vulnerability detection, malware analysis, network intrusion detection, and phishing detection.","Second, we find that the datasets used for training and evaluating LLMs in these tasks are often limited in size and diversity, highlighting the need for more comprehensive and representative datasets.","Third, we identify several promising techniques for adapting LLMs to specific cybersecurity domains, such as fine-tuning, transfer learning, and domain-specific pre-training.","Finally, we discuss the main challenges and opportunities for future research in LLM4Security, including the need for more interpretable and explainable models, the importance of addressing data privacy and security concerns, and the potential for leveraging LLMs for proactive defense and threat hunting.","Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several promising directions for future research."],"url":"http://arxiv.org/abs/2405.04760v1","category":"cs.CR"}
{"created":"2024-05-08 02:05:38","title":"Multi-Label Out-of-Distribution Detection with Spectral Normalized Joint Energy","abstract":"In today's interconnected world, achieving reliable out-of-distribution (OOD) detection poses a significant challenge for machine learning models. While numerous studies have introduced improved approaches for multi-class OOD detection tasks, the investigation into multi-label OOD detection tasks has been notably limited. We introduce Spectral Normalized Joint Energy (SNoJoE), a method that consolidates label-specific information across multiple labels through the theoretically justified concept of an energy-based function. Throughout the training process, we employ spectral normalization to manage the model's feature space, thereby enhancing model efficacy and generalization, in addition to bolstering robustness. Our findings indicate that the application of spectral normalization to joint energy scores notably amplifies the model's capability for OOD detection. We perform OOD detection experiments utilizing PASCAL-VOC as the in-distribution dataset and ImageNet-22K or Texture as the out-of-distribution datasets. Our experimental results reveal that, in comparison to prior top performances, SNoJoE achieves 11% and 54% relative reductions in FPR95 on the respective OOD datasets, thereby defining the new state of the art in this field of study.","sentences":["In today's interconnected world, achieving reliable out-of-distribution (OOD) detection poses a significant challenge for machine learning models.","While numerous studies have introduced improved approaches for multi-class OOD detection tasks, the investigation into multi-label OOD detection tasks has been notably limited.","We introduce Spectral Normalized Joint Energy (SNoJoE), a method that consolidates label-specific information across multiple labels through the theoretically justified concept of an energy-based function.","Throughout the training process, we employ spectral normalization to manage the model's feature space, thereby enhancing model efficacy and generalization, in addition to bolstering robustness.","Our findings indicate that the application of spectral normalization to joint energy scores notably amplifies the model's capability for OOD detection.","We perform OOD detection experiments utilizing PASCAL-VOC as the in-distribution dataset and ImageNet-22K or Texture as the out-of-distribution datasets.","Our experimental results reveal that, in comparison to prior top performances, SNoJoE achieves 11% and 54% relative reductions in FPR95 on the respective OOD datasets, thereby defining the new state of the art in this field of study."],"url":"http://arxiv.org/abs/2405.04759v1","category":"cs.CV"}
{"created":"2024-05-08 02:01:17","title":"Honeyfile Camouflage: Hiding Fake Files in Plain Sight","abstract":"Honeyfiles are a particularly useful type of honeypot: fake files deployed to detect and infer information from malicious behaviour. This paper considers the challenge of naming honeyfiles so they are camouflaged when placed amongst real files in a file system. Based on cosine distances in semantic vector spaces, we develop two metrics for filename camouflage: one based on simple averaging and one on clustering with mixture fitting. We evaluate and compare the metrics, showing that both perform well on a publicly available GitHub software repository dataset.","sentences":["Honeyfiles are a particularly useful type of honeypot: fake files deployed to detect and infer information from malicious behaviour.","This paper considers the challenge of naming honeyfiles so they are camouflaged when placed amongst real files in a file system.","Based on cosine distances in semantic vector spaces, we develop two metrics for filename camouflage: one based on simple averaging and one on clustering with mixture fitting.","We evaluate and compare the metrics, showing that both perform well on a publicly available GitHub software repository dataset."],"url":"http://arxiv.org/abs/2405.04758v1","category":"cs.CR"}
{"created":"2024-05-08 01:52:49","title":"Communication-efficient and Differentially-private Distributed Nash Equilibrium Seeking with Linear Convergence","abstract":"The distributed computation of a Nash equilibrium (NE) for non-cooperative games is gaining increased attention recently. Due to the nature of distributed systems, privacy and communication efficiency are two critical concerns. Traditional approaches often address these critical concerns in isolation. This work introduces a unified framework, named CDP-NES, designed to improve communication efficiency in the privacy-preserving NE seeking algorithm for distributed non-cooperative games over directed graphs. Leveraging both general compression operators and the noise adding mechanism, CDP-NES perturbs local states with Laplacian noise and applies difference compression prior to their exchange among neighbors. We prove that CDP-NES not only achieves linear convergence to a neighborhood of the NE in games with restricted monotone mappings but also guarantees $\\epsilon$-differential privacy, addressing privacy and communication efficiency simultaneously. Finally, simulations are provided to illustrate the effectiveness of the proposed method.","sentences":["The distributed computation of a Nash equilibrium (NE) for non-cooperative games is gaining increased attention recently.","Due to the nature of distributed systems, privacy and communication efficiency are two critical concerns.","Traditional approaches often address these critical concerns in isolation.","This work introduces a unified framework, named CDP-NES, designed to improve communication efficiency in the privacy-preserving NE seeking algorithm for distributed non-cooperative games over directed graphs.","Leveraging both general compression operators and the noise adding mechanism, CDP-NES perturbs local states with Laplacian noise and applies difference compression prior to their exchange among neighbors.","We prove that CDP-NES not only achieves linear convergence to a neighborhood of the NE in games with restricted monotone mappings but also guarantees $\\epsilon$-differential privacy, addressing privacy and communication efficiency simultaneously.","Finally, simulations are provided to illustrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.04757v1","category":"eess.SY"}
{"created":"2024-05-08 01:51:29","title":"BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models","abstract":"Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also learn social biases, which has a significant potential for societal harm. There have been many mitigation strategies proposed for LLM safety, but it is unclear how effective they are for eliminating social biases. In this work, we propose a new methodology for attacking language models with knowledge graph augmented generation. We refactor natural language stereotypes into a knowledge graph, and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models. We find our method increases bias in all models, even those trained with safety guardrails. This demonstrates the need for further research in AI safety, and further work in this new adversarial space.","sentences":["Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly.","The language model can also learn social biases, which has a significant potential for societal harm.","There have been many mitigation strategies proposed for LLM safety, but it is unclear how effective they are for eliminating social biases.","In this work, we propose a new methodology for attacking language models with knowledge graph augmented generation.","We refactor natural language stereotypes into a knowledge graph, and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models.","We find our method increases bias in all models, even those trained with safety guardrails.","This demonstrates the need for further research in AI safety, and further work in this new adversarial space."],"url":"http://arxiv.org/abs/2405.04756v1","category":"cs.CL"}
{"created":"2024-05-08 17:37:57","title":"BenthicNet: A global compilation of seafloor images for deep learning applications","abstract":"Advances in underwater imaging enable the collection of extensive seafloor image datasets that are necessary for monitoring important benthic ecosystems. The ability to collect seafloor imagery has outpaced our capacity to analyze it, hindering expedient mobilization of this crucial environmental information. Recent machine learning approaches provide opportunities to increase the efficiency with which seafloor image datasets are analyzed, yet large and consistent datasets necessary to support development of such approaches are scarce. Here we present BenthicNet: a global compilation of seafloor imagery designed to support the training and evaluation of large-scale image recognition models. An initial set of over 11.4 million images was collected and curated to represent a diversity of seafloor environments using a representative subset of 1.3 million images. These are accompanied by 2.6 million annotations translated to the CATAMI scheme, which span 190,000 of the images. A large deep learning model was trained on this compilation and preliminary results suggest it has utility for automating large and small-scale image analysis tasks. The compilation and model are made openly available for use by the scientific community at https://doi.org/10.20383/103.0614.","sentences":["Advances in underwater imaging enable the collection of extensive seafloor image datasets that are necessary for monitoring important benthic ecosystems.","The ability to collect seafloor imagery has outpaced our capacity to analyze it, hindering expedient mobilization of this crucial environmental information.","Recent machine learning approaches provide opportunities to increase the efficiency with which seafloor image datasets are analyzed, yet large and consistent datasets necessary to support development of such approaches are scarce.","Here we present BenthicNet: a global compilation of seafloor imagery designed to support the training and evaluation of large-scale image recognition models.","An initial set of over 11.4 million images was collected and curated to represent a diversity of seafloor environments using a representative subset of 1.3 million images.","These are accompanied by 2.6 million annotations translated to the CATAMI scheme, which span 190,000 of the images.","A large deep learning model was trained on this compilation and preliminary results suggest it has utility for automating large and small-scale image analysis tasks.","The compilation and model are made openly available for use by the scientific community at https://doi.org/10.20383/103.0614."],"url":"http://arxiv.org/abs/2405.05241v1","category":"cs.CV"}
{"created":"2024-05-08 17:27:16","title":"How a table modulates the risk of airborne transmission between facing individuals","abstract":"Airborne transmission has been recognized as an important route of transmission for SARS-CoV-2, the virus responsible for the COVID-19 pandemic. While coughing and sneezing are spectacular sources of production of infected aerosols with far-reaching airflows, the prevalence of asymptomatic transmissions highlighted the importance of social activities. Gathering around a table, a common scenario of human interactions, not only fixes a typical distance between static interlocutors, but influences airborne transmission, by serving both as a flow diverter and a surface for droplet deposition. Here, we use high-fidelity large-eddy simulations to characterize short-range airborne transmission when two people face each other at a typical table. We show that compared to the natural distance travelled by free buoyant puffs and jets, the distance between the table and the emission constitutes a new length scale that modifies downward exhaled flows, common during nose breathing, speech, and laughter. When the table is close to the emitter, its main effect is to restrict the forward spread of emitted particles. However, if the distance between individuals is too short, particles reaching the recipient become more concentrated, rising transmission risks. Additionally, simulations of forceful exhalations, like laughter, demonstrate that the table acts as a filter that collects medium-sized particles that would have remained in the free jet otherwise, but can in that case be involved in the fomite transmission route. The table introduces a cut-off size for particles that depends on the inertia of the exhaled material, thereby modifying the size distribution of particles suspended in the air.","sentences":["Airborne transmission has been recognized as an important route of transmission for SARS-CoV-2, the virus responsible for the COVID-19 pandemic.","While coughing and sneezing are spectacular sources of production of infected aerosols with far-reaching airflows, the prevalence of asymptomatic transmissions highlighted the importance of social activities.","Gathering around a table, a common scenario of human interactions, not only fixes a typical distance between static interlocutors, but influences airborne transmission, by serving both as a flow diverter and a surface for droplet deposition.","Here, we use high-fidelity large-eddy simulations to characterize short-range airborne transmission when two people face each other at a typical table.","We show that compared to the natural distance travelled by free buoyant puffs and jets, the distance between the table and the emission constitutes a new length scale that modifies downward exhaled flows, common during nose breathing, speech, and laughter.","When the table is close to the emitter, its main effect is to restrict the forward spread of emitted particles.","However, if the distance between individuals is too short, particles reaching the recipient become more concentrated, rising transmission risks.","Additionally, simulations of forceful exhalations, like laughter, demonstrate that the table acts as a filter that collects medium-sized particles that would have remained in the free jet otherwise, but can in that case be involved in the fomite transmission route.","The table introduces a cut-off size for particles that depends on the inertia of the exhaled material, thereby modifying the size distribution of particles suspended in the air."],"url":"http://arxiv.org/abs/2405.05232v1","category":"physics.flu-dyn"}
{"created":"2024-05-08 17:24:24","title":"myAURA: Personalized health library for epilepsy management via knowledge graph sparsification and visualization","abstract":"Objective: We report the development of the patient-centered myAURA application and suite of methods designed to aid epilepsy patients, caregivers, and researchers in making decisions about care and self-management.   Materials and Methods: myAURA rests on the federation of an unprecedented collection of heterogeneous data resources relevant to epilepsy, such as biomedical databases, social media, and electronic health records. A generalizable, open-source methodology was developed to compute a multi-layer knowledge graph linking all this heterogeneous data via the terms of a human-centered biomedical dictionary.   Results: The power of the approach is first exemplified in the study of the drug-drug interaction phenomenon. Furthermore, we employ a novel network sparsification methodology using the metric backbone of weighted graphs, which reveals the most important edges for inference, recommendation, and visualization, such as pharmacology factors patients discuss on social media. The network sparsification approach also allows us to extract focused digital cohorts from social media whose discourse is more relevant to epilepsy or other biomedical problems. Finally, we present our patient-centered design and pilot-testing of myAURA, including its user interface, based on focus groups and other stakeholder input.   Discussion: The ability to search and explore myAURA's heterogeneous data sources via a sparsified multi-layer knowledge graph, as well as the combination of those layers in a single map, are useful features for integrating relevant information for epilepsy.   Conclusion: Our stakeholder-driven, scalable approach to integrate traditional and non-traditional data sources, enables biomedical discovery and data-powered patient self-management in epilepsy, and is generalizable to other chronic conditions.","sentences":["Objective: We report the development of the patient-centered myAURA application and suite of methods designed to aid epilepsy patients, caregivers, and researchers in making decisions about care and self-management.   ","Materials and Methods: myAURA rests on the federation of an unprecedented collection of heterogeneous data resources relevant to epilepsy, such as biomedical databases, social media, and electronic health records.","A generalizable, open-source methodology was developed to compute a multi-layer knowledge graph linking all this heterogeneous data via the terms of a human-centered biomedical dictionary.   ","Results:","The power of the approach is first exemplified in the study of the drug-drug interaction phenomenon.","Furthermore, we employ a novel network sparsification methodology using the metric backbone of weighted graphs, which reveals the most important edges for inference, recommendation, and visualization, such as pharmacology factors patients discuss on social media.","The network sparsification approach also allows us to extract focused digital cohorts from social media whose discourse is more relevant to epilepsy or other biomedical problems.","Finally, we present our patient-centered design and pilot-testing of myAURA, including its user interface, based on focus groups and other stakeholder input.   ","Discussion: The ability to search and explore myAURA's heterogeneous data sources via a sparsified multi-layer knowledge graph, as well as the combination of those layers in a single map, are useful features for integrating relevant information for epilepsy.   ","Conclusion: Our stakeholder-driven, scalable approach to integrate traditional and non-traditional data sources, enables biomedical discovery and data-powered patient self-management in epilepsy, and is generalizable to other chronic conditions."],"url":"http://arxiv.org/abs/2405.05229v1","category":"cs.IR"}
{"created":"2024-05-08 16:43:25","title":"Hybrid Quantum Graph Neural Network for Molecular Property Prediction","abstract":"To accelerate the process of materials design, materials science has increasingly used data driven techniques to extract information from collected data. Specially, machine learning (ML) algorithms, which span the ML discipline, have demonstrated ability to predict various properties of materials with the level of accuracy similar to explicit calculation of quantum mechanical theories, but with significantly reduced run time and computational resources. Within ML, graph neural networks have emerged as an important algorithm within the field of machine learning, since they are capable of predicting accurately a wide range of important physical, chemical and electronic properties due to their higher learning ability based on the graph representation of material and molecular descriptors through the aggregation of information embedded within the graph. In parallel with the development of state of the art classical machine learning applications, the fusion of quantum computing and machine learning have created a new paradigm where classical machine learning model can be augmented with quantum layers which are able to encode high dimensional data more efficiently. Leveraging the structure of existing algorithms, we developed a unique and novel gradient free hybrid quantum classical convoluted graph neural network (HyQCGNN) to predict formation energies of perovskite materials. The performance of our hybrid statistical model is competitive with the results obtained purely from a classical convoluted graph neural network, and other classical machine learning algorithms, such as XGBoost. Consequently, our study suggests a new pathway to explore how quantum feature encoding and parametric quantum circuits can yield drastic improvements of complex ML algorithm like graph neural network.","sentences":["To accelerate the process of materials design, materials science has increasingly used data driven techniques to extract information from collected data.","Specially, machine learning (ML) algorithms, which span the ML discipline, have demonstrated ability to predict various properties of materials with the level of accuracy similar to explicit calculation of quantum mechanical theories, but with significantly reduced run time and computational resources.","Within ML, graph neural networks have emerged as an important algorithm within the field of machine learning, since they are capable of predicting accurately a wide range of important physical, chemical and electronic properties due to their higher learning ability based on the graph representation of material and molecular descriptors through the aggregation of information embedded within the graph.","In parallel with the development of state of the art classical machine learning applications, the fusion of quantum computing and machine learning have created a new paradigm where classical machine learning model can be augmented with quantum layers which are able to encode high dimensional data more efficiently.","Leveraging the structure of existing algorithms, we developed a unique and novel gradient free hybrid quantum classical convoluted graph neural network (HyQCGNN) to predict formation energies of perovskite materials.","The performance of our hybrid statistical model is competitive with the results obtained purely from a classical convoluted graph neural network, and other classical machine learning algorithms, such as XGBoost.","Consequently, our study suggests a new pathway to explore how quantum feature encoding and parametric quantum circuits can yield drastic improvements of complex ML algorithm like graph neural network."],"url":"http://arxiv.org/abs/2405.05205v1","category":"quant-ph"}
{"created":"2024-05-08 16:40:15","title":"A multiple coupon collection process and its Markov embedding structure","abstract":"The embedding problem of Markov transition matrices into Markov semigroups is a classic problem that regained a lot of impetus and activities in recent years. We consider it here for the following generalisation of the well-known coupon collection process: from a finite set of distinct objects, a subset is drawn repeatedly according to some probability distribution, independently and with replacement, and each time united with the set of objects sampled so far. We derive and interpret properties and explicit conditions for the resulting discrete-time Markov chain to be representable within a semigroup or a flow of a continuous-time process of the same type.","sentences":["The embedding problem of Markov transition matrices into Markov semigroups is a classic problem that regained a lot of impetus and activities in recent years.","We consider it here for the following generalisation of the well-known coupon collection process: from a finite set of distinct objects, a subset is drawn repeatedly according to some probability distribution, independently and with replacement, and each time united with the set of objects sampled so far.","We derive and interpret properties and explicit conditions for the resulting discrete-time Markov chain to be representable within a semigroup or a flow of a continuous-time process of the same type."],"url":"http://arxiv.org/abs/2405.05203v1","category":"math.PR"}
{"created":"2024-05-08 15:54:57","title":"ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with Probability Map Guided Multi-Format Feature Fusion","abstract":"Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks. However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied. This has been a long-standing hindrance to the improvement of pose estimation accuracy. To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method. ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body. Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %. The emphasis of our study is focusing on the position information that is not exploited before in radar singal. This provides direction to investigate other potential non-redundant information from mmWave rader.","sentences":["Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks.","However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied.","This has been a long-standing hindrance to the improvement of pose estimation accuracy.","To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model,","ProbRadarM3F.","This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method.","ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body.","Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %.","The emphasis of our study is focusing on the position information that is not exploited before in radar singal.","This provides direction to investigate other potential non-redundant information from mmWave rader."],"url":"http://arxiv.org/abs/2405.05164v1","category":"cs.CV"}
{"created":"2024-05-08 15:40:42","title":"Boundary symmetry breaking of flocking systems","abstract":"We consider a flocking system confined transversally between two infinite reflecting parallel walls separated by a distance $L_\\perp$. Infinite or periodic boundary conditions are assumed longitudinally to the direction of collective motion, defining a ring geometry typical of experimental realizations with flocking active colloids. Such a confinement selects a flocking state with its mean direction aligned parallel to the wall, thus breaking explicitly the rotational symmetry locally by a boundary effect. Finite size scaling analysis and numerical simulations show that confinement induces an effective mass term ${M_c} \\sim L_\\perp^{-\\zeta}$ (with positive $\\zeta$ being the dynamical scaling exponent of the free theory) suppressing scale free correlations at small wave-numbers. However, due to the finite system size in the transversal direction, this effect can only be detected for large enough longitudinal system sizes (i.e. narrow ring geometries). Furthermore, in the longitudinal direction, density correlations are characterized by an anomalous effective mass term. The effective mass term also enhances the global scalar order parameter and suppresses fluctuations of the mean flocking direction. These results suggest an equivalence between transversal confinement and driving by an homogeneous external field, which breaks the rotational symmetry at the global level.","sentences":["We consider a flocking system confined transversally between two infinite reflecting parallel walls separated by a distance $L_\\perp$. Infinite or periodic boundary conditions are assumed longitudinally to the direction of collective motion, defining a ring geometry typical of experimental realizations with flocking active colloids.","Such a confinement selects a flocking state with its mean direction aligned parallel to the wall, thus breaking explicitly the rotational symmetry locally by a boundary effect.","Finite size scaling analysis and numerical simulations show that confinement induces an effective mass term ${M_c} \\sim L_\\perp^{-\\zeta}$ (with positive $\\zeta$ being the dynamical scaling exponent of the free theory) suppressing scale free correlations at small wave-numbers.","However, due to the finite system size in the transversal direction, this effect can only be detected for large enough longitudinal system sizes (i.e. narrow ring geometries).","Furthermore, in the longitudinal direction, density correlations are characterized by an anomalous effective mass term.","The effective mass term also enhances the global scalar order parameter and suppresses fluctuations of the mean flocking direction.","These results suggest an equivalence between transversal confinement and driving by an homogeneous external field, which breaks the rotational symmetry at the global level."],"url":"http://arxiv.org/abs/2405.05148v1","category":"cond-mat.soft"}
{"created":"2024-05-08 15:33:21","title":"Dynamic Size Counting in the Population Protocol Model","abstract":"The population protocol model describes collections of distributed agents that interact in pairs to solve a common task. We consider a dynamic variant of this prominent model, where we assume that an adversary may change the population size at an arbitrary point in time. In this model we tackle the problem of counting the population size: in the dynamic size counting problem the goal is to design an algorithm that computes an approximation of $\\log n$. This estimate can be used to turn static, non-uniform population protocols, i.e., protocols that depend on the population size $n$, into dynamic and loosely-stabilizing protocols.   Our contributions in this paper are three-fold. Starting from an arbitrary initial configuration, we first prove that the agents converge quickly to a valid configuration where each agent has a constant-factor approximation of $\\log n$, and once the agents reach such a valid configuration, they stay in it for a polynomial number of time steps. Second, we show how to use our protocol to define a uniform and loosely-stabilizing phase clock for the population protocol model. Finally, we support our theoretical findings by empirical simulations that show that our protocols work well in practice.","sentences":["The population protocol model describes collections of distributed agents that interact in pairs to solve a common task.","We consider a dynamic variant of this prominent model, where we assume that an adversary may change the population size at an arbitrary point in time.","In this model we tackle the problem of counting the population size: in the dynamic size counting problem the goal is to design an algorithm that computes an approximation of $\\log n$. This estimate can be used to turn static, non-uniform population protocols, i.e., protocols that depend on the population size $n$, into dynamic and loosely-stabilizing protocols.   ","Our contributions in this paper are three-fold.","Starting from an arbitrary initial configuration, we first prove that the agents converge quickly to a valid configuration where each agent has a constant-factor approximation of $\\log n$, and once the agents reach such a valid configuration, they stay in it for a polynomial number of time steps.","Second, we show how to use our protocol to define a uniform and loosely-stabilizing phase clock for the population protocol model.","Finally, we support our theoretical findings by empirical simulations that show that our protocols work well in practice."],"url":"http://arxiv.org/abs/2405.05137v1","category":"cs.DC"}
{"created":"2024-05-08 15:32:20","title":"Identifying every building's function in large-scale urban areas with multi-modality remote-sensing data","abstract":"Buildings, as fundamental man-made structures in urban environments, serve as crucial indicators for understanding various city function zones. Rapid urbanization has raised an urgent need for efficiently surveying building footprints and functions. In this study, we proposed a semi-supervised framework to identify every building's function in large-scale urban areas with multi-modality remote-sensing data. In detail, optical images, building height, and nighttime-light data are collected to describe the morphological attributes of buildings. Then, the area of interest (AOI) and building masks from the volunteered geographic information (VGI) data are collected to form sparsely labeled samples. Furthermore, the multi-modality data and weak labels are utilized to train a segmentation model with a semi-supervised strategy. Finally, results are evaluated by 20,000 validation points and statistical survey reports from the government. The evaluations reveal that the produced function maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings in Shanghai, China. This study has the potential to support large-scale urban management and sustainable urban development. All collected data and produced maps are open access at https://github.com/LiZhuoHong/BuildingMap.","sentences":["Buildings, as fundamental man-made structures in urban environments, serve as crucial indicators for understanding various city function zones.","Rapid urbanization has raised an urgent need for efficiently surveying building footprints and functions.","In this study, we proposed a semi-supervised framework to identify every building's function in large-scale urban areas with multi-modality remote-sensing data.","In detail, optical images, building height, and nighttime-light data are collected to describe the morphological attributes of buildings.","Then, the area of interest (AOI) and building masks from the volunteered geographic information (VGI) data are collected to form sparsely labeled samples.","Furthermore, the multi-modality data and weak labels are utilized to train a segmentation model with a semi-supervised strategy.","Finally, results are evaluated by 20,000 validation points and statistical survey reports from the government.","The evaluations reveal that the produced function maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings in Shanghai, China.","This study has the potential to support large-scale urban management and sustainable urban development.","All collected data and produced maps are open access at https://github.com/LiZhuoHong/BuildingMap."],"url":"http://arxiv.org/abs/2405.05133v1","category":"cs.CV"}
{"created":"2024-05-08 14:22:16","title":"Observation of $t\\bar{t}$ production in the lepton+jets and dilepton channels in $p$+Pb collisions at $\\sqrt{s_\\mathrm{NN}}=8.16$ TeV with the ATLAS detector","abstract":"This paper reports the observation of top-quark pair production in proton-lead collisions in the ATLAS experiment at the Large Hadron Collider. The measurement is performed using 165 nb$^{-1}$ of $p$+Pb data collected at $\\sqrt{s_\\mathrm{NN}}=8.16$ TeV in 2016. Events are categorised in two analysis channels, consisting of either events with exactly one lepton (electron or muon) and at least four jets, or events with two opposite-charge leptons and at least two jets. In both channels at least one $b$-tagged jet is also required. Top-quark pair production is observed with a significance over five standard deviations in each channel. The top-quark pair production cross-section is measured to be $\\sigma_{t\\bar{t}}= 58.1\\pm 2.0\\;\\mathrm{(stat.)\\;^{+4.8}_{-4.4} \\;\\mathrm{(syst.)}}\\;\\mathrm{nb}$, with a total uncertainty of 9%. In addition, the nuclear modification factor is measured to be $R_{p\\mathrm{A}} = 1.090\\pm0.039\\;(\\mathrm{stat.})\\;^{+0.094}_{-0.087}\\;(\\mathrm{syst.})$. The measurements are found to be in good agreement with theory predictions involving nuclear parton distribution functions.","sentences":["This paper reports the observation of top-quark pair production in proton-lead collisions in the ATLAS experiment at the Large Hadron Collider.","The measurement is performed using 165 nb$^{-1}$ of $p$+Pb data collected at $\\sqrt{s_\\mathrm{NN}}=8.16$ TeV in 2016.","Events are categorised in two analysis channels, consisting of either events with exactly one lepton (electron or muon) and at least four jets, or events with two opposite-charge leptons and at least two jets.","In both channels at least one $b$-tagged jet is also required.","Top-quark pair production is observed with a significance over five standard deviations in each channel.","The top-quark pair production cross-section is measured to be $\\sigma_{t\\bar{t}}= 58.1\\pm 2.0\\;\\mathrm{(stat.)\\;^{+4.8}_{-4.4} \\;\\mathrm{(syst.)}}\\;\\mathrm{nb}$, with a total uncertainty of 9%.","In addition, the nuclear modification factor is measured to be $R_{p\\mathrm{A}} = 1.090\\pm0.039\\;(\\mathrm{stat.})\\;^{+0.094}_{-0.087}\\;(\\mathrm{syst.})$.","The measurements are found to be in good agreement with theory predictions involving nuclear parton distribution functions."],"url":"http://arxiv.org/abs/2405.05078v1","category":"nucl-ex"}
{"created":"2024-05-08 14:13:09","title":"Search for production of a single vector-like quark decaying to tH or tZ in the all-hadronic final state in pp collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for electroweak production of a single vector-like T quark in association with a bottom (b) quark in the all-hadronic decay channel is presented. This search uses proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected by the CMS experiment at the CERN LHC during 2016-2018, corresponding to an integrated luminosity of 138 fb$^{-1}$ The T quark is assumed to have charge 2/3 and decay to a top (t) quark and a Higgs (H) or Z boson. Event kinematics and the presence of jets containing b hadrons are used to reconstruct the hadronic decays of the t quark and H or Z boson. No significant deviation from the standard model prediction is observed in the data. The 95% confidence level upper limits on the product of the production cross section and branching fraction of a T quark produced in association with a b quark and decaying via tH or tZ range from 1260 to 68 fb for T quark masses of 600-1200 GeV.","sentences":["A search for electroweak production of a single vector-like T quark in association with a bottom (b) quark in the all-hadronic decay channel is presented.","This search uses proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected by the CMS experiment at the CERN LHC during 2016-2018, corresponding to an integrated luminosity of 138 fb$^{-1}$","The T quark is assumed to have charge 2/3 and decay to a top (t) quark and a Higgs (H) or Z boson.","Event kinematics and the presence of jets containing b hadrons are used to reconstruct the hadronic decays of the t quark and H or Z boson.","No significant deviation from the standard model prediction is observed in the data.","The 95% confidence level upper limits on the product of the production cross section and branching fraction of a T quark produced in association with a b quark and decaying via tH or tZ range from 1260 to 68 fb for T quark masses of 600-1200 GeV."],"url":"http://arxiv.org/abs/2405.05071v1","category":"hep-ex"}
{"created":"2024-05-08 13:42:22","title":"Ab initio computations of strongly deformed nuclei around $^{80}$Zr","abstract":"Nuclei around $N\\approx Z\\approx 40$ are strongly deformed and exhibit coexistence of shapes. These phenomena have challenged nuclear models. Here we perform ab initio coupled-cluster computations of low-lying collective states and electromagnetic quadrupole transitions of the even-even nuclei $^{72}$Kr, $^{76,78}$Sr, $^{78,80}$Zr and $^{84}$Mo starting from chiral nucleon-nucleon and three-nucleon forces. Our calculations reproduce the coexistence of oblate and prolate shapes in these nuclei, yield rotational bands and strong electromagnetic transitions, but are not accurate for some observables and nuclei. These results highlight the advances and challenges of ab initio computations of heavy deformed nuclei.","sentences":["Nuclei around $N\\approx Z\\approx 40$ are strongly deformed and exhibit coexistence of shapes.","These phenomena have challenged nuclear models.","Here we perform ab initio coupled-cluster computations of low-lying collective states and electromagnetic quadrupole transitions of the even-even nuclei $^{72}$Kr, $^{76,78}$Sr, $^{78,80}$Zr and $^{84}$Mo starting from chiral nucleon-nucleon and three-nucleon forces.","Our calculations reproduce the coexistence of oblate and prolate shapes in these nuclei, yield rotational bands and strong electromagnetic transitions, but are not accurate for some observables and nuclei.","These results highlight the advances and challenges of ab initio computations of heavy deformed nuclei."],"url":"http://arxiv.org/abs/2405.05052v1","category":"nucl-th"}
{"created":"2024-05-08 13:29:18","title":"Unique continuation at the boundary for divergence form elliptic equations on quasiconvex domains","abstract":"Let $\\Omega \\subset \\mathbb{R}^d$ be a quasiconvex Lipschitz domain and $A(x)$ be a $d \\times d$ uniformly elliptic, symmetric matrix with Lipschitz coefficients. Assume a nontrivial $u$ solves $-\\nabla \\cdot (A(x) \\nabla u) = 0$ in $\\Omega$, and $u$ vanishes on $\\Sigma = \\partial \\Omega \\cap B$ for some ball $B$. The main contribution of this paper is to demonstrate the existence of a countable collection of open balls $(B_i)_i$ such that the restriction of $u$ to $B_i \\cap \\Omega$ maintains a consistent sign. Furthermore, for any compact subset $K$ of $\\Sigma$, the set difference $K \\setminus \\bigcup_i B_i$ is shown to possess a Minkowski dimension that is strictly less than $d - 1 - \\epsilon$. As a consequence, we prove Lin's conjecture in quasiconvex domains.","sentences":["Let $\\Omega \\subset \\mathbb{R}^d$ be a quasiconvex Lipschitz domain and $A(x)$ be a $d \\times d$ uniformly elliptic, symmetric matrix with Lipschitz coefficients.","Assume a nontrivial $u$ solves $-\\nabla \\cdot (A(x) \\nabla u) = 0$ in $\\Omega$, and $u$ vanishes on $\\Sigma = \\partial \\Omega \\cap B$ for some ball $B$. The main contribution of this paper is to demonstrate the existence of a countable collection of open balls $(B_i)_i$ such that the restriction of $u$ to $B_i \\cap \\Omega$ maintains a consistent sign.","Furthermore, for any compact subset $K$ of $\\Sigma$, the set difference $K \\setminus \\bigcup_i B_i$ is shown to possess a Minkowski dimension that is strictly less than $d - 1 - \\epsilon$. As a consequence, we prove Lin's conjecture in quasiconvex domains."],"url":"http://arxiv.org/abs/2405.05044v1","category":"math.AP"}
{"created":"2024-05-08 12:04:18","title":"Negative-energy spin waves in antiferromagnets for spin-current amplification and analogue gravity","abstract":"Magnonic black holes-analogue event horizons for the spin-wave collective excitations of ordered magnets-can be used for fundamental research, for example for investigating Hawking radiation, but also for technological applications of spin waves. Here we show how to engineer a horizon for spin waves in antiferromagnets, which have the attractive feature of fast magnetization dynamics and linear dispersion relation. We propose a set-up with spatially varying exchange interaction with spin transfer torque to implement the horizon and a second set-up for the amplification of spin waves consisting of an antiferromagnet subject to a spatially varying external magnetic field that is driven by spin orbit torque. We compute the values of parameters needed to implement the horizon and to have amplification of spin waves. We develop the corresponding Klein-Gordon equation and quantify the amplification. Our work paves the way for investigation of Hawking radiation of spin waves and for antiferromagnet-based spin-waves amplifiers.","sentences":["Magnonic black holes-analogue event horizons for the spin-wave collective excitations of ordered magnets-can be used for fundamental research, for example for investigating Hawking radiation, but also for technological applications of spin waves.","Here we show how to engineer a horizon for spin waves in antiferromagnets, which have the attractive feature of fast magnetization dynamics and linear dispersion relation.","We propose a set-up with spatially varying exchange interaction with spin transfer torque to implement the horizon and a second set-up for the amplification of spin waves consisting of an antiferromagnet subject to a spatially varying external magnetic field that is driven by spin orbit torque.","We compute the values of parameters needed to implement the horizon and to have amplification of spin waves.","We develop the corresponding Klein-Gordon equation and quantify the amplification.","Our work paves the way for investigation of Hawking radiation of spin waves and for antiferromagnet-based spin-waves amplifiers."],"url":"http://arxiv.org/abs/2405.04996v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 11:12:37","title":"Communication-Efficient Collaborative Perception via Information Filling with Codebook","abstract":"Collaborative perception empowers each agent to improve its perceptual ability through the exchange of perceptual messages with other agents. It inherently results in a fundamental trade-off between perception ability and communication cost. To address this bottleneck issue, our core idea is to optimize the collaborative messages from two key aspects: representation and selection. The proposed codebook-based message representation enables the transmission of integer codes, rather than high-dimensional feature maps. The proposed information-filling-driven message selection optimizes local messages to collectively fill each agent's information demand, preventing information overflow among multiple agents. By integrating these two designs, we propose CodeFilling, a novel communication-efficient collaborative perception system, which significantly advances the perception-communication trade-off and is inclusive to both homogeneous and heterogeneous collaboration settings. We evaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new simulation dataset, OPV2VH+. Results show that CodeFilling outperforms previous SOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication volume. Our code is available at https://github.com/PhyllisH/CodeFilling.","sentences":["Collaborative perception empowers each agent to improve its perceptual ability through the exchange of perceptual messages with other agents.","It inherently results in a fundamental trade-off between perception ability and communication cost.","To address this bottleneck issue, our core idea is to optimize the collaborative messages from two key aspects: representation and selection.","The proposed codebook-based message representation enables the transmission of integer codes, rather than high-dimensional feature maps.","The proposed information-filling-driven message selection optimizes local messages to collectively fill each agent's information demand, preventing information overflow among multiple agents.","By integrating these two designs, we propose CodeFilling, a novel communication-efficient collaborative perception system, which significantly advances the perception-communication trade-off and is inclusive to both homogeneous and heterogeneous collaboration settings.","We evaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new simulation dataset, OPV2VH+.","Results show that CodeFilling outperforms previous SOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication volume.","Our code is available at https://github.com/PhyllisH/CodeFilling."],"url":"http://arxiv.org/abs/2405.04966v1","category":"cs.IT"}
{"created":"2024-05-08 11:06:40","title":"Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture","abstract":"In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection.","sentences":["In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements.","To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances.","The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow.","Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses.","This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements.","We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues.","Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio.","We validate the proposed framework and conduct ablation studies to demonstrate its efficacy.","Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality.","To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection."],"url":"http://arxiv.org/abs/2405.04963v1","category":"cs.MM"}
{"created":"2024-05-08 09:35:33","title":"ATLAS searches for additional scalars and exotic Higgs boson decays with the LHC Run 2 dataset","abstract":"This report reviews the published results of searches for possible additional scalar particles and exotic decays of the Higgs boson performed by the ATLAS Collaboration using up to 140 fb$^{-1}$ of 13 TeV proton-proton collision data collected during Run 2 of the Large Hadron Collider. Key results are examined, and observed excesses, while never statistically compelling, are noted. Constraints are placed on parameters of several models which extend the Standard Model, for example by adding one or more singlet or doublet fields, or offering exotic Higgs boson decay channels. Summaries of new searches as well as extensions of previous searches are discussed. These new results have a wider reach or attain stronger exclusion limits. New experimental techniques that were developed for these searches are highlighted. Search channels which have not yet been examined are also listed, as these provide insight into possible future areas of exploration.","sentences":["This report reviews the published results of searches for possible additional scalar particles and exotic decays of the Higgs boson performed by the ATLAS Collaboration using up to 140 fb$^{-1}$ of 13 TeV proton-proton collision data collected during Run 2 of the Large Hadron Collider.","Key results are examined, and observed excesses, while never statistically compelling, are noted.","Constraints are placed on parameters of several models which extend the Standard Model, for example by adding one or more singlet or doublet fields, or offering exotic Higgs boson decay channels.","Summaries of new searches as well as extensions of previous searches are discussed.","These new results have a wider reach or attain stronger exclusion limits.","New experimental techniques that were developed for these searches are highlighted.","Search channels which have not yet been examined are also listed, as these provide insight into possible future areas of exploration."],"url":"http://arxiv.org/abs/2405.04914v1","category":"hep-ex"}
{"created":"2024-05-08 09:13:13","title":"Development of a method to measure trace level of uranium and thorium in scintillation films","abstract":"We have established a method to measure picograms-per-gram (pg g$^{-1}$) levels of $^{238}$U and $^{232}$Th in scintillation films by combining the dry ashing method and inductively coupled plasma mass spectrometry. Trace amounts of $^{238}$U and $^{232}$Th were measured in up to 2~g of the scintillation film with almost 100% collection efficiency. This paper details the experimental procedure, including the pretreatment of the samples and labware, detection limit of the method, collection efficiencies of $^{238}$U and $^{232}$Th, and measurement of $^{238}$U and $^{232}$Th in a polyethylene naphthalate film. This method is also applicable to $^{238}$U and $^{232}$Th measurements in other low-background organic materials for rare event search experiments.","sentences":["We have established a method to measure picograms-per-gram (pg g$^{-1}$) levels of $^{238}$U and $^{232}$Th in scintillation films by combining the dry ashing method and inductively coupled plasma mass spectrometry.","Trace amounts of $^{238}$U and $^{232}$Th were measured in up to 2~g of the scintillation film with almost 100% collection efficiency.","This paper details the experimental procedure, including the pretreatment of the samples and labware, detection limit of the method, collection efficiencies of $^{238}$U and $^{232}$Th, and measurement of $^{238}$U and $^{232}$Th in a polyethylene naphthalate film.","This method is also applicable to $^{238}$U and $^{232}$Th measurements in other low-background organic materials for rare event search experiments."],"url":"http://arxiv.org/abs/2405.04901v1","category":"physics.ins-det"}
{"created":"2024-05-08 05:29:38","title":"Proportion Estimation by Masked Learning from Label Proportion","abstract":"The PD-L1 rate, the number of PD-L1 positive tumor cells over the total number of all tumor cells, is an important metric for immunotherapy. This metric is recorded as diagnostic information with pathological images. In this paper, we propose a proportion estimation method with a small amount of cell-level annotation and proportion annotation, which can be easily collected. Since the PD-L1 rate is calculated from only `tumor cells' and not using `non-tumor cells', we first detect tumor cells with a detection model. Then, we estimate the PD-L1 proportion by introducing a masking technique to `learning from label proportion.' In addition, we propose a weighted focal proportion loss to address data imbalance problems. Experiments using clinical data demonstrate the effectiveness of our method. Our method achieved the best performance in comparisons.","sentences":["The PD-L1 rate, the number of PD-L1 positive tumor cells over the total number of all tumor cells, is an important metric for immunotherapy.","This metric is recorded as diagnostic information with pathological images.","In this paper, we propose a proportion estimation method with a small amount of cell-level annotation and proportion annotation, which can be easily collected.","Since the PD-L1 rate is calculated from only `tumor cells' and not using `non-tumor cells', we first detect tumor cells with a detection model.","Then, we estimate the PD-L1 proportion by introducing a masking technique to `learning from label proportion.'","In addition, we propose a weighted focal proportion loss to address data imbalance problems.","Experiments using clinical data demonstrate the effectiveness of our method.","Our method achieved the best performance in comparisons."],"url":"http://arxiv.org/abs/2405.04815v1","category":"cs.CV"}
{"created":"2024-05-08 04:41:39","title":"Collective-subspace requantization for sub-barrier fusion reactions: Inertial functions for collective motions","abstract":"The adiabatic self-consistent collective coordinate (ASCC) method is used to determine the optimum reaction path and to calculate the potential and the inertial functions of the reaction model. The properties of the inertial functions are investigated with the ASCC method, in comparison with those of the cranking formulae. In addition, the properties of the pair rotation are investigated in the BCS pair model. The moments of inertia for rotation in both the real and the gauge spaces may decrease as the deformation develops.","sentences":["The adiabatic self-consistent collective coordinate (ASCC) method is used to determine the optimum reaction path and to calculate the potential and the inertial functions of the reaction model.","The properties of the inertial functions are investigated with the ASCC method, in comparison with those of the cranking formulae.","In addition, the properties of the pair rotation are investigated in the BCS pair model.","The moments of inertia for rotation in both the real and the gauge spaces may decrease as the deformation develops."],"url":"http://arxiv.org/abs/2405.04809v1","category":"nucl-th"}
{"created":"2024-05-08 04:38:36","title":"Transformer Architecture for NetsDB","abstract":"HiRISE (High-Resolution Imaging Science Experiment) is a camera onboard the Mars Reconnaissance orbiter responsible for photographing vast areas of the Martian surface in unprecedented detail. It can capture millions of incredible closeup images in minutes. However, Mars suffers from frequent regional and local dust storms hampering this data-collection process, and pipeline, resulting in loss of effort and crucial flight time. Removing these images manually requires a large amount of manpower. I filter out these images obstructed by atmospheric dust automatically by using a Dust Image Classifier fine-tuned on Resnet-50 with an accuracy of 94.05%. To further facilitate the seamless filtering of Images I design a prediction pipeline that classifies and stores these dusty patches. I also denoise partially obstructed images using an Auto Encoder-based denoiser and Pix2Pix GAN with 0.75 and 0.99 SSIM Index respectively","sentences":["HiRISE (High-Resolution Imaging Science Experiment) is a camera onboard the Mars Reconnaissance orbiter responsible for photographing vast areas of the Martian surface in unprecedented detail.","It can capture millions of incredible closeup images in minutes.","However, Mars suffers from frequent regional and local dust storms hampering this data-collection process, and pipeline, resulting in loss of effort and crucial flight time.","Removing these images manually requires a large amount of manpower.","I filter out these images obstructed by atmospheric dust automatically by using a Dust Image Classifier fine-tuned on Resnet-50 with an accuracy of 94.05%.","To further facilitate the seamless filtering of Images I design a prediction pipeline that classifies and stores these dusty patches.","I also denoise partially obstructed images using an Auto Encoder-based denoiser and Pix2Pix GAN with 0.75 and 0.99 SSIM Index respectively"],"url":"http://arxiv.org/abs/2405.04807v1","category":"cs.CV"}
{"created":"2024-05-08 03:56:27","title":"Mass media competition and alternative ordering in social dynamics","abstract":"We investigate the collective behavior of a system of social agents subject to the competition between two mass media influences considered as external fields. We study under what conditions either of two mass media with different intensities can impose its message to the majority. In addition to a collective state dominated by the stronger mass media and a disordered phase, we characterize two nontrivial effects as the parameters of the system are varied: (i) the appearance of a majority sharing the state of the weaker mass media, and (ii) the emergence of an alternative ordering in a state different from those of either media. We explore the dependence of both phenomena on the topology of the network of interactions. We show that the presence of long-range interactions rather than random connections is essential for the occurrence of both effects. The model can be extended to include multiple mass media and we illustrate it by considering three mass media fields acting on the system. Nontrivial collective behaviors persist for some ranges of parameters: the weakest mass media can convince the majority, and the system can spontaneously order against all applied fields.","sentences":["We investigate the collective behavior of a system of social agents subject to the competition between two mass media influences considered as external fields.","We study under what conditions either of two mass media with different intensities can impose its message to the majority.","In addition to a collective state dominated by the stronger mass media and a disordered phase, we characterize two nontrivial effects as the parameters of the system are varied: (i) the appearance of a majority sharing the state of the weaker mass media, and (ii) the emergence of an alternative ordering in a state different from those of either media.","We explore the dependence of both phenomena on the topology of the network of interactions.","We show that the presence of long-range interactions rather than random connections is essential for the occurrence of both effects.","The model can be extended to include multiple mass media and we illustrate it by considering three mass media fields acting on the system.","Nontrivial collective behaviors persist for some ranges of parameters: the weakest mass media can convince the majority, and the system can spontaneously order against all applied fields."],"url":"http://arxiv.org/abs/2405.04792v1","category":"physics.soc-ph"}
{"created":"2024-05-08 03:11:12","title":"CourseGPT-zh: an Educational Large Language Model Based on Knowledge Distillation Incorporating Prompt Optimization","abstract":"Large language models (LLMs) have demonstrated astonishing capabilities in natural language processing (NLP) tasks, sparking interest in their application to professional domains with higher specialized requirements. However, restricted access to closed-source LLMs via APIs and the difficulty in collecting massive high-quality datasets pose obstacles to the development of large language models in education fields of various courses. Given these challenges, we propose CourseGPT-zh, a course-oriented education LLM that supports customization and low-cost deployment. To address the comprehensiveness and diversity requirements of course-specific corpora, we design a high-quality question-answering corpus distillation framework incorporating prompt optimization, which effectively mines textbook knowledge and enhances its diversity. Moreover, considering the alignment of LLM responses with user needs, a novel method for discrete prompt optimization based on LLM-as-Judge is introduced. During optimization, this framework leverages the LLM's ability to reflect on and exploit error feedback and patterns, allowing for prompts that meet user needs and preferences while saving response length. Lastly, we obtain CourseGPT-zh based on the open-source LLM using parameter-efficient fine-tuning. Experimental results show that our discrete prompt optimization framework effectively improves the response quality of ChatGPT, and CourseGPT-zh exhibits strong professional capabilities in specialized knowledge question-answering, significantly outperforming comparable open-source models.","sentences":["Large language models (LLMs) have demonstrated astonishing capabilities in natural language processing (NLP) tasks, sparking interest in their application to professional domains with higher specialized requirements.","However, restricted access to closed-source LLMs via APIs and the difficulty in collecting massive high-quality datasets pose obstacles to the development of large language models in education fields of various courses.","Given these challenges, we propose CourseGPT-zh, a course-oriented education LLM that supports customization and low-cost deployment.","To address the comprehensiveness and diversity requirements of course-specific corpora, we design a high-quality question-answering corpus distillation framework incorporating prompt optimization, which effectively mines textbook knowledge and enhances its diversity.","Moreover, considering the alignment of LLM responses with user needs, a novel method for discrete prompt optimization based on LLM-as-Judge is introduced.","During optimization, this framework leverages the LLM's ability to reflect on and exploit error feedback and patterns, allowing for prompts that meet user needs and preferences while saving response length.","Lastly, we obtain CourseGPT-zh based on the open-source LLM using parameter-efficient fine-tuning.","Experimental results show that our discrete prompt optimization framework effectively improves the response quality of ChatGPT, and CourseGPT-zh exhibits strong professional capabilities in specialized knowledge question-answering, significantly outperforming comparable open-source models."],"url":"http://arxiv.org/abs/2405.04781v1","category":"cs.CL"}
{"created":"2024-05-08 02:46:41","title":"Determining Recoverable Consensus Numbers","abstract":"Herlihy's wait-free consensus hierarchy classifies the power of object types in asynchronous shared memory systems where processes can permanently crash (i.e. stop taking steps). In this hierarchy, a type has consensus number $n$ if objects of that type can be used along with (read/write) registers to solve consensus among $n$ processes that can permanently crash, but not among $n+1$ or more processes. In systems where processes can recover after crashing, the power of an object type to solve consensus may be different. Golab's recoverable consensus hierarchy classifies the power of object types in such a system. In the recoverable consensus hierarchy, a type has recoverable consensus number $n$ if objects of that type can be used along with registers to solve consensus among $n$ processes that can recover after crashing, but not among $n+1$ or more processes. In this paper, we prove that the recoverable consensus hierarchy of deterministic, readable types is robust, i.e., if consensus can be solved among $n$ processes that can recover after crashing using a collection of objects of deterministic, readable types, then one of these types has recoverable consensus number at least $n$. This is important for comparing the relative computational power of different deterministic, readable types, because it implies that one cannot combine various objects to obtain an algorithm that is better at solving recoverable consensus than any of the individual object types. Our result can be used to show that, for all $n \\geq 4$, there exists a readable type with consensus number $n$ and recoverable consensus number $n-2$. We also show that, for all $n > n' \\geq 1$, there exists a non-readable type that has consensus number $n$ and recoverable consensus number $n'$.","sentences":["Herlihy's wait-free consensus hierarchy classifies the power of object types in asynchronous shared memory systems where processes can permanently crash (i.e. stop taking steps).","In this hierarchy, a type has consensus number $n$ if objects of that type can be used along with (read/write) registers to solve consensus among $n$ processes that can permanently crash, but not among $n+1$ or more processes.","In systems where processes can recover after crashing, the power of an object type to solve consensus may be different.","Golab's recoverable consensus hierarchy classifies the power of object types in such a system.","In the recoverable consensus hierarchy, a type has recoverable consensus number $n$ if objects of that type can be used along with registers to solve consensus among $n$ processes that can recover after crashing, but not among $n+1$ or more processes.","In this paper, we prove that the recoverable consensus hierarchy of deterministic, readable types is robust, i.e., if consensus can be solved among $n$ processes that can recover after crashing using a collection of objects of deterministic, readable types, then one of these types has recoverable consensus number at least $n$. This is important for comparing the relative computational power of different deterministic, readable types, because it implies that one cannot combine various objects to obtain an algorithm that is better at solving recoverable consensus than any of the individual object types.","Our result can be used to show that, for all $n \\geq 4$, there exists a readable type with consensus number $n$ and recoverable consensus number $n-2$. We also show that, for all $n > n' \\geq 1$, there exists a non-readable type that has consensus number $n$ and recoverable consensus number $n'$."],"url":"http://arxiv.org/abs/2405.04775v1","category":"cs.DC"}
{"created":"2024-05-08 02:22:12","title":"Predictive Enforcement","abstract":"We study law enforcement guided by data-informed predictions of \"hot spots\" for likely criminal offenses. Such \"predictive\" enforcement could lead to data being selectively and disproportionately collected from neighborhoods targeted for enforcement by the prediction. Predictive enforcement that fails to account for this endogenous \"datafication\" may lead to the over-policing of traditionally high-crime neighborhoods and performs poorly, in particular, in some cases as poorly as if no data were used. Endogenizing the incentives for criminal offenses identifies additional deterrence benefits from the informationally efficient use of data.","sentences":["We study law enforcement guided by data-informed predictions of \"hot spots\" for likely criminal offenses.","Such \"predictive\" enforcement could lead to data being selectively and disproportionately collected from neighborhoods targeted for enforcement by the prediction.","Predictive enforcement that fails to account for this endogenous \"datafication\" may lead to the over-policing of traditionally high-crime neighborhoods and performs poorly, in particular, in some cases as poorly as if no data were used.","Endogenizing the incentives for criminal offenses identifies additional deterrence benefits from the informationally efficient use of data."],"url":"http://arxiv.org/abs/2405.04764v1","category":"econ.TH"}
{"created":"2024-05-08 01:41:25","title":"AttacKG+:Boosting Attack Knowledge Graph Construction with Large Language Models","abstract":"Attack knowledge graph construction seeks to convert textual cyber threat intelligence (CTI) reports into structured representations, portraying the evolutionary traces of cyber attacks. Even though previous research has proposed various methods to construct attack knowledge graphs, they generally suffer from limited generalization capability to diverse knowledge types as well as requirement of expertise in model design and tuning. Addressing these limitations, we seek to utilize Large Language Models (LLMs), which have achieved enormous success in a broad range of tasks given exceptional capabilities in both language understanding and zero-shot task fulfillment. Thus, we propose a fully automatic LLM-based framework to construct attack knowledge graphs named: AttacKG+. Our framework consists of four consecutive modules: rewriter, parser, identifier, and summarizer, each of which is implemented by instruction prompting and in-context learning empowered by LLMs. Furthermore, we upgrade the existing attack knowledge schema and propose a comprehensive version. We represent a cyber attack as a temporally unfolding event, each temporal step of which encapsulates three layers of representation, including behavior graph, MITRE TTP labels, and state summary. Extensive evaluation demonstrates that: 1) our formulation seamlessly satisfies the information needs in threat event analysis, 2) our construction framework is effective in faithfully and accurately extracting the information defined by AttacKG+, and 3) our attack graph directly benefits downstream security practices such as attack reconstruction. All the code and datasets will be released upon acceptance.","sentences":["Attack knowledge graph construction seeks to convert textual cyber threat intelligence (CTI) reports into structured representations, portraying the evolutionary traces of cyber attacks.","Even though previous research has proposed various methods to construct attack knowledge graphs, they generally suffer from limited generalization capability to diverse knowledge types as well as requirement of expertise in model design and tuning.","Addressing these limitations, we seek to utilize Large Language Models (LLMs), which have achieved enormous success in a broad range of tasks given exceptional capabilities in both language understanding and zero-shot task fulfillment.","Thus, we propose a fully automatic LLM-based framework to construct attack knowledge graphs named: AttacKG+.","Our framework consists of four consecutive modules: rewriter, parser, identifier, and summarizer, each of which is implemented by instruction prompting and in-context learning empowered by LLMs.","Furthermore, we upgrade the existing attack knowledge schema and propose a comprehensive version.","We represent a cyber attack as a temporally unfolding event, each temporal step of which encapsulates three layers of representation, including behavior graph, MITRE TTP labels, and state summary.","Extensive evaluation demonstrates that: 1) our formulation seamlessly satisfies the information needs in threat event analysis, 2) our construction framework is effective in faithfully and accurately extracting the information defined by AttacKG+, and 3) our attack graph directly benefits downstream security practices such as attack reconstruction.","All the code and datasets will be released upon acceptance."],"url":"http://arxiv.org/abs/2405.04753v1","category":"cs.CR"}
{"created":"2024-05-08 01:35:57","title":"The Catalog of early-type Runaway stars from LAMOST-DR8","abstract":"Runaway stars are OB-type stars ejected from their birthplace with large peculiar velocities. The leading hypothesis addressed in their formation includes the supernova ejection mechanism and the dynamic ejection scenario. Identification of runaway populations is the first step to investigating their formation and evolution. Here we present our work of searching for Galactic runaway candidate stars from the LAMOST Medium-Resolution Survey DR8 database. After studying the kinematic properties for a collection of 4,432 early-type stars, predominantly B-type stars, using the radial velocity measurements from LAMOST DR8 and astrometric solutions made by Gaia DR3, we identified 229 runaway candidate stars. They span a wide distribution in projected rotational velocities. We investigated the Galactic spatial distribution of the runaway population and noticed that most of them likely reside within the Galactic thin disk. Based upon analyzing the Doppler shifts of the candidate stars, we found two binary runaway candidates displaying velocity variation with estimated orbital periods of 40 and 61 days.","sentences":["Runaway stars are OB-type stars ejected from their birthplace with large peculiar velocities.","The leading hypothesis addressed in their formation includes the supernova ejection mechanism and the dynamic ejection scenario.","Identification of runaway populations is the first step to investigating their formation and evolution.","Here we present our work of searching for Galactic runaway candidate stars from the LAMOST Medium-Resolution Survey DR8 database.","After studying the kinematic properties for a collection of 4,432 early-type stars, predominantly B-type stars, using the radial velocity measurements from LAMOST DR8 and astrometric solutions made by Gaia DR3, we identified 229 runaway candidate stars.","They span a wide distribution in projected rotational velocities.","We investigated the Galactic spatial distribution of the runaway population and noticed that most of them likely reside within the Galactic thin disk.","Based upon analyzing the Doppler shifts of the candidate stars, we found two binary runaway candidates displaying velocity variation with estimated orbital periods of 40 and 61 days."],"url":"http://arxiv.org/abs/2405.04750v1","category":"astro-ph.SR"}
{"created":"2024-05-08 01:22:47","title":"SVD-AE: Simple Autoencoders for Collaborative Filtering","abstract":"Collaborative filtering (CF) methods for recommendation systems have been extensively researched, ranging from matrix factorization and autoencoder-based to graph filtering-based methods. Recently, lightweight methods that require almost no training have been recently proposed to reduce overall computation. However, existing methods still have room to improve the trade-offs among accuracy, efficiency, and robustness. In particular, there are no well-designed closed-form studies for \\emph{balanced} CF in terms of the aforementioned trade-offs. In this paper, we design SVD-AE, a simple yet effective singular vector decomposition (SVD)-based linear autoencoder, whose closed-form solution can be defined based on SVD for CF. SVD-AE does not require iterative training processes as its closed-form solution can be calculated at once. Furthermore, given the noisy nature of the rating matrix, we explore the robustness against such noisy interactions of existing CF methods and our SVD-AE. As a result, we demonstrate that our simple design choice based on truncated SVD can be used to strengthen the noise robustness of the recommendation while improving efficiency. Code is available at https://github.com/seoyoungh/svd-ae.","sentences":["Collaborative filtering (CF) methods for recommendation systems have been extensively researched, ranging from matrix factorization and autoencoder-based to graph filtering-based methods.","Recently, lightweight methods that require almost no training have been recently proposed to reduce overall computation.","However, existing methods still have room to improve the trade-offs among accuracy, efficiency, and robustness.","In particular, there are no well-designed closed-form studies for \\emph{balanced} CF in terms of the aforementioned trade-offs.","In this paper, we design SVD-AE, a simple yet effective singular vector decomposition (SVD)-based linear autoencoder, whose closed-form solution can be defined based on SVD for CF.","SVD-AE does not require iterative training processes as its closed-form solution can be calculated at once.","Furthermore, given the noisy nature of the rating matrix, we explore the robustness against such noisy interactions of existing CF methods and our SVD-AE.","As a result, we demonstrate that our simple design choice based on truncated SVD can be used to strengthen the noise robustness of the recommendation while improving efficiency.","Code is available at https://github.com/seoyoungh/svd-ae."],"url":"http://arxiv.org/abs/2405.04746v1","category":"cs.IR"}
{"created":"2024-05-08 00:52:57","title":"Cryptanalysis of the SIMON Cypher Using Neo4j","abstract":"The exponential growth in the number of Internet of Things (IoT) devices has seen the introduction of several Lightweight Encryption Algorithms (LEA). While LEAs are designed to enhance the integrity, privacy and security of data collected and transmitted by IoT devices, it is hazardous to assume that all LEAs are secure and exhibit similar levels of protection. To improve encryption strength, cryptanalysts and algorithm designers routinely probe LEAs using various cryptanalysis techniques to identify vulnerabilities and limitations of LEAs. Despite recent improvements in the efficiency of cryptanalysis utilising heuristic methods and a Partial Difference Distribution Table (PDDT), the process remains inefficient, with the random nature of the heuristic inhibiting reproducible results. However, the use of a PDDT presents opportunities to identify relationships between differentials utilising knowledge graphs, leading to the identification of efficient paths throughout the PDDT. This paper introduces the novel use of knowledge graphs to identify intricate relationships between differentials in the SIMON LEA, allowing for the identification of optimal paths throughout the differentials, and increasing the effectiveness of the differential security analyses of SIMON.","sentences":["The exponential growth in the number of Internet of Things (IoT) devices has seen the introduction of several Lightweight Encryption Algorithms (LEA).","While LEAs are designed to enhance the integrity, privacy and security of data collected and transmitted by IoT devices, it is hazardous to assume that all LEAs are secure and exhibit similar levels of protection.","To improve encryption strength, cryptanalysts and algorithm designers routinely probe LEAs using various cryptanalysis techniques to identify vulnerabilities and limitations of LEAs.","Despite recent improvements in the efficiency of cryptanalysis utilising heuristic methods and a Partial Difference Distribution Table (PDDT), the process remains inefficient, with the random nature of the heuristic inhibiting reproducible results.","However, the use of a PDDT presents opportunities to identify relationships between differentials utilising knowledge graphs, leading to the identification of efficient paths throughout the PDDT.","This paper introduces the novel use of knowledge graphs to identify intricate relationships between differentials in the SIMON LEA, allowing for the identification of optimal paths throughout the differentials, and increasing the effectiveness of the differential security analyses of SIMON."],"url":"http://arxiv.org/abs/2405.04735v1","category":"cs.CR"}
{"created":"2024-05-08 00:45:20","title":"S-EQA: Tackling Situational Queries in Embodied Question Answering","abstract":"We present and tackle the problem of Embodied Question Answering (EQA) with Situational Queries (S-EQA) in a household environment. Unlike prior EQA work tackling simple queries that directly reference target objects and quantifiable properties pertaining them, EQA with situational queries (such as \"Is the bathroom clean and dry?\") is more challenging, as the agent needs to figure out not just what the target objects pertaining to the query are, but also requires a consensus on their states to be answerable. Towards this objective, we first introduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an LLM's output to create a dataset of unique situational queries, corresponding consensus object information, and predicted answers. PGE maintains uniqueness among the generated queries, using multiple forms of semantic similarity. We validate the generated dataset via a large scale user-study conducted on M-Turk, and introduce it as S-EQA, the first dataset tackling EQA with situational queries. Our user study establishes the authenticity of S-EQA with a high 97.26% of the generated queries being deemed answerable, given the consensus object data. Conversely, we observe a low correlation of 46.2% on the LLM-predicted answers to human-evaluated ones; indicating the LLM's poor capability in directly answering situational queries, while establishing S-EQA's usability in providing a human-validated consensus for an indirect solution. We evaluate S-EQA via Visual Question Answering (VQA) on VirtualHome, which unlike other simulators, contains several objects with modifiable states that also visually appear different upon modification -- enabling us to set a quantitative benchmark for S-EQA. To the best of our knowledge, this is the first work to introduce EQA with situational queries, and also the first to use a generative approach for query creation.","sentences":["We present and tackle the problem of Embodied Question Answering (EQA) with Situational Queries (S-EQA) in a household environment.","Unlike prior EQA work tackling simple queries that directly reference target objects and quantifiable properties pertaining them, EQA with situational queries (such as \"Is the bathroom clean and dry?\") is more challenging, as the agent needs to figure out not just what the target objects pertaining to the query are, but also requires a consensus on their states to be answerable.","Towards this objective, we first introduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an LLM's output to create a dataset of unique situational queries, corresponding consensus object information, and predicted answers.","PGE maintains uniqueness among the generated queries, using multiple forms of semantic similarity.","We validate the generated dataset via a large scale user-study conducted on M-Turk, and introduce it as S-EQA, the first dataset tackling EQA with situational queries.","Our user study establishes the authenticity of S-EQA with a high 97.26% of the generated queries being deemed answerable, given the consensus object data.","Conversely, we observe a low correlation of 46.2% on the LLM-predicted answers to human-evaluated ones; indicating the LLM's poor capability in directly answering situational queries, while establishing S-EQA's usability in providing a human-validated consensus for an indirect solution.","We evaluate S-EQA via Visual Question Answering (VQA) on VirtualHome, which unlike other simulators, contains several objects with modifiable states that also visually appear different upon modification -- enabling us to set a quantitative benchmark for S-EQA.","To the best of our knowledge, this is the first work to introduce EQA with situational queries, and also the first to use a generative approach for query creation."],"url":"http://arxiv.org/abs/2405.04732v1","category":"cs.RO"}
{"created":"2024-05-08 00:32:19","title":"LLMs Can Patch Up Missing Relevance Judgments in Evaluation","abstract":"Unjudged documents or holes in information retrieval benchmarks are considered non-relevant in evaluation, yielding no gains in measuring effectiveness. However, these missing judgments may inadvertently introduce biases into the evaluation as their prevalence for a retrieval model is heavily contingent on the pooling process. Thus, filling holes becomes crucial in ensuring reliable and accurate evaluation. Collecting human judgment for all documents is cumbersome and impractical. In this paper, we aim at leveraging large language models (LLMs) to automatically label unjudged documents. Our goal is to instruct an LLM using detailed instructions to assign fine-grained relevance judgments to holes. To this end, we systematically simulate scenarios with varying degrees of holes by randomly dropping relevant documents from the relevance judgment in TREC DL tracks. Our experiments reveal a strong correlation between our LLM-based method and ground-truth relevance judgments. Based on our simulation experiments conducted on three TREC DL datasets, in the extreme scenario of retaining only 10% of judgments, our method achieves a Kendall tau correlation of 0.87 and 0.92 on an average for Vicu\\~na-7B and GPT-3.5 Turbo respectively.","sentences":["Unjudged documents or holes in information retrieval benchmarks are considered non-relevant in evaluation, yielding no gains in measuring effectiveness.","However, these missing judgments may inadvertently introduce biases into the evaluation as their prevalence for a retrieval model is heavily contingent on the pooling process.","Thus, filling holes becomes crucial in ensuring reliable and accurate evaluation.","Collecting human judgment for all documents is cumbersome and impractical.","In this paper, we aim at leveraging large language models (LLMs) to automatically label unjudged documents.","Our goal is to instruct an LLM using detailed instructions to assign fine-grained relevance judgments to holes.","To this end, we systematically simulate scenarios with varying degrees of holes by randomly dropping relevant documents from the relevance judgment in TREC DL tracks.","Our experiments reveal a strong correlation between our LLM-based method and ground-truth relevance judgments.","Based on our simulation experiments conducted on three TREC DL datasets, in the extreme scenario of retaining only 10% of judgments, our method achieves a Kendall tau correlation of 0.87 and 0.92 on an average for Vicu\\~na-7B and GPT-3.5","Turbo respectively."],"url":"http://arxiv.org/abs/2405.04727v1","category":"cs.IR"}
{"created":"2024-05-08 00:03:23","title":"Detecting and Refining HiRISE Image Patches Obscured by Atmospheric Dust","abstract":"HiRISE (High-Resolution Imaging Science Experiment) is a camera onboard the Mars Reconnaissance orbiter responsible for photographing vast areas of the Martian surface in unprecedented detail. It can capture millions of incredible closeup images in minutes. However, Mars suffers from frequent regional and local dust storms hampering this data-collection process, and pipeline, resulting in loss of effort and crucial flight time. Removing these images manually requires a large amount of manpower. I filter out these images obstructed by atmospheric dust automatically by using a Dust Image Classifier fine-tuned on Resnet-50 with an accuracy of 94.05%. To further facilitate the seamless filtering of Images I design a prediction pipeline that classifies and stores these dusty patches. I also denoise partially obstructed images using an Auto Encoder-based denoiser and Pix2Pix GAN with 0.75 and 0.99 SSIM Index respectively.","sentences":["HiRISE (High-Resolution Imaging Science Experiment) is a camera onboard the Mars Reconnaissance orbiter responsible for photographing vast areas of the Martian surface in unprecedented detail.","It can capture millions of incredible closeup images in minutes.","However, Mars suffers from frequent regional and local dust storms hampering this data-collection process, and pipeline, resulting in loss of effort and crucial flight time.","Removing these images manually requires a large amount of manpower.","I filter out these images obstructed by atmospheric dust automatically by using a Dust Image Classifier fine-tuned on Resnet-50 with an accuracy of 94.05%.","To further facilitate the seamless filtering of Images I design a prediction pipeline that classifies and stores these dusty patches.","I also denoise partially obstructed images using an Auto Encoder-based denoiser and Pix2Pix GAN with 0.75 and 0.99 SSIM Index respectively."],"url":"http://arxiv.org/abs/2405.04722v1","category":"cs.CV"}
{"created":"2024-05-07 23:49:02","title":"Metaverse Survey & Tutorial: Exploring Key Requirements, Technologies, Standards, Applications, Challenges, and Perspectives","abstract":"In this paper, we present a comprehensive survey of the metaverse, envisioned as a transformative dimension of next-generation Internet technologies. This study not only outlines the structural components of our survey but also makes a substantial scientific contribution by elucidating the foundational concepts underlying the emergence of the metaverse. We analyze its architecture by defining key characteristics and requirements, thereby illuminating the nascent reality set to revolutionize digital interactions. Our analysis emphasizes the importance of collaborative efforts in developing metaverse standards, thereby fostering a unified understanding among industry stakeholders, organizations, and regulatory bodies. We extend our scrutiny to critical technologies integral to the metaverse, including interactive experiences, communication technologies, ubiquitous computing, digital twins, artificial intelligence, and cybersecurity measures. For each technological domain, we rigorously assess current contributions, principal techniques, and representative use cases, providing a nuanced perspective on their potential impacts. Furthermore, we delve into the metaverse's diverse applications across education, healthcare, business, social interactions, industrial sectors, defense, and mission-critical operations, highlighting its extensive utility. Each application is thoroughly analyzed, demonstrating its value and addressing associated challenges. The survey concludes with an overview of persistent challenges and future directions, offering insights into essential considerations and strategies necessary to harness the full potential of the metaverse. Through this detailed investigation, our goal is to articulate the scientific contributions of this survey paper, transcending a mere structural overview to highlight the transformative implications of the metaverse.","sentences":["In this paper, we present a comprehensive survey of the metaverse, envisioned as a transformative dimension of next-generation Internet technologies.","This study not only outlines the structural components of our survey but also makes a substantial scientific contribution by elucidating the foundational concepts underlying the emergence of the metaverse.","We analyze its architecture by defining key characteristics and requirements, thereby illuminating the nascent reality set to revolutionize digital interactions.","Our analysis emphasizes the importance of collaborative efforts in developing metaverse standards, thereby fostering a unified understanding among industry stakeholders, organizations, and regulatory bodies.","We extend our scrutiny to critical technologies integral to the metaverse, including interactive experiences, communication technologies, ubiquitous computing, digital twins, artificial intelligence, and cybersecurity measures.","For each technological domain, we rigorously assess current contributions, principal techniques, and representative use cases, providing a nuanced perspective on their potential impacts.","Furthermore, we delve into the metaverse's diverse applications across education, healthcare, business, social interactions, industrial sectors, defense, and mission-critical operations, highlighting its extensive utility.","Each application is thoroughly analyzed, demonstrating its value and addressing associated challenges.","The survey concludes with an overview of persistent challenges and future directions, offering insights into essential considerations and strategies necessary to harness the full potential of the metaverse.","Through this detailed investigation, our goal is to articulate the scientific contributions of this survey paper, transcending a mere structural overview to highlight the transformative implications of the metaverse."],"url":"http://arxiv.org/abs/2405.04718v1","category":"cs.HC"}
{"created":"2024-05-07 23:43:46","title":"Physics-based deep learning reveals rising heating demand heightens air pollution in Norwegian cities","abstract":"Policymakers frequently analyze air quality and climate change in isolation, disregarding their interactions. This study explores the influence of specific climate factors on air quality by contrasting a regression model with K-Means Clustering, Hierarchical Clustering, and Random Forest techniques. We employ Physics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine the air pollution predictions. Our analysis utilizes ten years (2009-2018) of daily traffic, weather, and air pollution data from three major cities in Norway. Findings from feature selection reveal a correlation between rising heating degree days and heightened air pollution levels, suggesting increased heating activities in Norway are a contributing factor to worsening air quality. PBDL demonstrates superior accuracy in air pollution predictions compared to LSTM. This paper contributes to the growing literature on PBDL methods for more accurate air pollution predictions using environmental variables, aiding policymakers in formulating effective data-driven climate policies.","sentences":["Policymakers frequently analyze air quality and climate change in isolation, disregarding their interactions.","This study explores the influence of specific climate factors on air quality by contrasting a regression model with K-Means Clustering, Hierarchical Clustering, and Random Forest techniques.","We employ Physics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine the air pollution predictions.","Our analysis utilizes ten years (2009-2018) of daily traffic, weather, and air pollution data from three major cities in Norway.","Findings from feature selection reveal a correlation between rising heating degree days and heightened air pollution levels, suggesting increased heating activities in Norway are a contributing factor to worsening air quality.","PBDL demonstrates superior accuracy in air pollution predictions compared to LSTM.","This paper contributes to the growing literature on PBDL methods for more accurate air pollution predictions using environmental variables, aiding policymakers in formulating effective data-driven climate policies."],"url":"http://arxiv.org/abs/2405.04716v1","category":"cs.CY"}
{"created":"2024-05-07 23:37:40","title":"Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning","abstract":"Statistics suffers from a fundamental problem, \"the curse of endogeneity\" -- the regression function, or more broadly the prediction risk minimizer with infinite data, may not be the target we wish to pursue. This is because when complex data are collected from multiple sources, the biases deviated from the interested (causal) association inherited in individuals or sub-populations are not expected to be canceled. Traditional remedies are of hindsight and restrictive in being tailored to prior knowledge like untestable cause-effect structures, resulting in methods that risk model misspecification and lack scalable applicability. This paper seeks to offer a purely data-driven and universally applicable method that only uses the heterogeneity of the biases in the data rather than following pre-offered commandments. Such an idea is formulated as a nonparametric invariance pursuit problem, whose goal is to unveil the invariant conditional expectation $m^\\star(x)\\equiv \\mathbb{E}[Y^{(e)}|X_{S^\\star}^{(e)}=x_{S^\\star}]$ with unknown important variable set $S^\\star$ across heterogeneous environments $e\\in \\mathcal{E}$. Under the structural causal model framework, $m^\\star$ can be interpreted as certain data-driven causality in general. The paper contributes to proposing a novel framework, called Focused Adversarial Invariance Regularization (FAIR), formulated as a single minimax optimization program that can solve the general invariance pursuit problem. As illustrated by the unified non-asymptotic analysis, our adversarial estimation framework can attain provable sample-efficient estimation akin to standard regression under a minimal identification condition for various tasks and models. As an application, the FAIR-NN estimator realized by two Neural Network classes is highlighted as the first approach to attain statistically efficient estimation in general nonparametric invariance learning.","sentences":["Statistics suffers from a fundamental problem, \"the curse of endogeneity\" -- the regression function, or more broadly the prediction risk minimizer with infinite data, may not be the target we wish to pursue.","This is because when complex data are collected from multiple sources, the biases deviated from the interested (causal) association inherited in individuals or sub-populations are not expected to be canceled.","Traditional remedies are of hindsight and restrictive in being tailored to prior knowledge like untestable cause-effect structures, resulting in methods that risk model misspecification and lack scalable applicability.","This paper seeks to offer a purely data-driven and universally applicable method that only uses the heterogeneity of the biases in the data rather than following pre-offered commandments.","Such an idea is formulated as a nonparametric invariance pursuit problem, whose goal is to unveil the invariant conditional expectation $m^\\star(x)\\equiv \\mathbb{E}[Y^{(e)}|X_{S^\\star}^{(e)}=x_{S^\\star}]$ with unknown important variable set $S^\\star$ across heterogeneous environments $e\\in \\mathcal{E}$. Under the structural causal model framework, $m^\\star$ can be interpreted as certain data-driven causality in general.","The paper contributes to proposing a novel framework, called Focused Adversarial Invariance Regularization (FAIR), formulated as a single minimax optimization program that can solve the general invariance pursuit problem.","As illustrated by the unified non-asymptotic analysis, our adversarial estimation framework can attain provable sample-efficient estimation akin to standard regression under a minimal identification condition for various tasks and models.","As an application, the FAIR-NN estimator realized by two Neural Network classes is highlighted as the first approach to attain statistically efficient estimation in general nonparametric invariance learning."],"url":"http://arxiv.org/abs/2405.04715v1","category":"math.ST"}
{"created":"2024-05-07 23:32:36","title":"RACER: Epistemic Risk-Sensitive RL Enables Fast Driving with Fewer Crashes","abstract":"Reinforcement learning provides an appealing framework for robotic control due to its ability to learn expressive policies purely through real-world interaction. However, this requires addressing real-world constraints and avoiding catastrophic failures during training, which might severely impede both learning progress and the performance of the final policy. In many robotics settings, this amounts to avoiding certain \"unsafe\" states. The high-speed off-road driving task represents a particularly challenging instantiation of this problem: a high-return policy should drive as aggressively and as quickly as possible, which often requires getting close to the edge of the set of \"safe\" states, and therefore places a particular burden on the method to avoid frequent failures.   To both learn highly performant policies and avoid excessive failures, we propose a reinforcement learning framework that combines risk-sensitive control with an adaptive action space curriculum.   Furthermore, we show that our risk-sensitive objective automatically avoids out-of-distribution states when equipped with an estimator for epistemic uncertainty.   We implement our algorithm on a small-scale rally car and show that it is capable of learning high-speed policies for a real-world off-road driving task. We show that our method greatly reduces the number of safety violations during the training process, and actually leads to higher-performance policies in both driving and non-driving simulation environments with similar challenges.","sentences":["Reinforcement learning provides an appealing framework for robotic control due to its ability to learn expressive policies purely through real-world interaction.","However, this requires addressing real-world constraints and avoiding catastrophic failures during training, which might severely impede both learning progress and the performance of the final policy.","In many robotics settings, this amounts to avoiding certain \"unsafe\" states.","The high-speed off-road driving task represents a particularly challenging instantiation of this problem: a high-return policy should drive as aggressively and as quickly as possible, which often requires getting close to the edge of the set of \"safe\" states, and therefore places a particular burden on the method to avoid frequent failures.   ","To both learn highly performant policies and avoid excessive failures, we propose a reinforcement learning framework that combines risk-sensitive control with an adaptive action space curriculum.   ","Furthermore, we show that our risk-sensitive objective automatically avoids out-of-distribution states when equipped with an estimator for epistemic uncertainty.   ","We implement our algorithm on a small-scale rally car and show that it is capable of learning high-speed policies for a real-world off-road driving task.","We show that our method greatly reduces the number of safety violations during the training process, and actually leads to higher-performance policies in both driving and non-driving simulation environments with similar challenges."],"url":"http://arxiv.org/abs/2405.04714v1","category":"cs.RO"}
{"created":"2024-05-07 22:47:56","title":"Guiding the Way: A Comprehensive Examination of AI Guidelines in Global Media","abstract":"With the increasing adoption of artificial intelligence (AI) technologies in the news industry, media organizations have begun publishing guidelines that aim to promote the responsible, ethical, and unbiased implementation of AI-based technologies. These guidelines are expected to serve journalists and media workers by establishing best practices and a framework that helps them navigate ever-evolving AI tools. Drawing on institutional theory and digital inequality concepts, this study analyzes 37 AI guidelines for media purposes in 17 countries. Our analysis reveals key thematic areas, such as transparency, accountability, fairness, privacy, and the preservation of journalistic values. Results highlight shared principles and best practices that emerge from these guidelines, including the importance of human oversight, explainability of AI systems, disclosure of automated content, and protection of user data. However, the geographical distribution of these guidelines, highlighting the dominance of Western nations, particularly North America and Europe, can further ongoing concerns about power asymmetries in AI adoption and consequently isomorphism outside these regions. Our results may serve as a resource for news organizations, policymakers, and stakeholders looking to navigate the complex AI development toward creating a more inclusive and equitable digital future for the media industry worldwide.","sentences":["With the increasing adoption of artificial intelligence (AI) technologies in the news industry, media organizations have begun publishing guidelines that aim to promote the responsible, ethical, and unbiased implementation of AI-based technologies.","These guidelines are expected to serve journalists and media workers by establishing best practices and a framework that helps them navigate ever-evolving AI tools.","Drawing on institutional theory and digital inequality concepts, this study analyzes 37 AI guidelines for media purposes in 17 countries.","Our analysis reveals key thematic areas, such as transparency, accountability, fairness, privacy, and the preservation of journalistic values.","Results highlight shared principles and best practices that emerge from these guidelines, including the importance of human oversight, explainability of AI systems, disclosure of automated content, and protection of user data.","However, the geographical distribution of these guidelines, highlighting the dominance of Western nations, particularly North America and Europe, can further ongoing concerns about power asymmetries in AI adoption and consequently isomorphism outside these regions.","Our results may serve as a resource for news organizations, policymakers, and stakeholders looking to navigate the complex AI development toward creating a more inclusive and equitable digital future for the media industry worldwide."],"url":"http://arxiv.org/abs/2405.04706v1","category":"cs.CY"}
{"created":"2024-05-07 22:31:50","title":"Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory Architectures","abstract":"Large Language Models (LLMs) deployed on edge devices learn through fine-tuning and updating a certain portion of their parameters. Although such learning methods can be optimized to reduce resource utilization, the overall required resources remain a heavy burden on edge devices. Instead, Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method, can improve the quality of the LLM-generated content without updating model parameters. However, the RAG-based LLM may involve repetitive searches on the profile data in every user-LLM interaction. This search can lead to significant latency along with the accumulation of user data. Conventional efforts to decrease latency result in restricting the size of saved user data, thus reducing the scalability of RAG as user data continuously grows. It remains an open question: how to free RAG from the constraints of latency and scalability on edge devices? In this paper, we propose a novel framework to accelerate RAG via Computing-in-Memory (CiM) architectures. It accelerates matrix multiplications by performing in-situ computation inside the memory while avoiding the expensive data transfer between the computing unit and memory. Our framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive learning-based training method and noise-aware training, can enable RAG to efficiently search profile data with CiM. To the best of our knowledge, this is the first work utilizing CiM to accelerate RAG.","sentences":["Large Language Models (LLMs) deployed on edge devices learn through fine-tuning and updating a certain portion of their parameters.","Although such learning methods can be optimized to reduce resource utilization, the overall required resources remain a heavy burden on edge devices.","Instead, Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method, can improve the quality of the LLM-generated content without updating model parameters.","However, the RAG-based LLM may involve repetitive searches on the profile data in every user-LLM interaction.","This search can lead to significant latency along with the accumulation of user data.","Conventional efforts to decrease latency result in restricting the size of saved user data, thus reducing the scalability of RAG as user data continuously grows.","It remains an open question: how to free RAG from the constraints of latency and scalability on edge devices?","In this paper, we propose a novel framework to accelerate RAG via Computing-in-Memory (CiM) architectures.","It accelerates matrix multiplications by performing in-situ computation inside the memory while avoiding the expensive data transfer between the computing unit and memory.","Our framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive learning-based training method and noise-aware training, can enable RAG to efficiently search profile data with CiM. To the best of our knowledge, this is the first work utilizing CiM to accelerate RAG."],"url":"http://arxiv.org/abs/2405.04700v1","category":"cs.LG"}
{"created":"2024-05-07 22:07:07","title":"Enhancing Organizational Performance: Harnessing AI and NLP for User Feedback Analysis in Product Development","abstract":"This paper explores the application of AI and NLP techniques for user feedback analysis in the context of heavy machine crane products. By leveraging AI and NLP, organizations can gain insights into customer perceptions, improve product development, enhance satisfaction and loyalty, inform decision-making, and gain a competitive advantage. The paper highlights the impact of user feedback analysis on organizational performance and emphasizes the reasons for using AI and NLP, including scalability, objectivity, improved accuracy, increased insights, and time savings. The methodology involves data collection, cleaning, text and rating analysis, interpretation, and feedback implementation. Results include sentiment analysis, word cloud visualizations, and radar charts comparing product attributes. These findings provide valuable information for understanding customer sentiment, identifying improvement areas, and making data-driven decisions to enhance the customer experience. In conclusion, promising AI and NLP techniques in user feedback analysis offer organizations a powerful tool to understand customers, improve product development, increase satisfaction, and drive business success","sentences":["This paper explores the application of AI and NLP techniques for user feedback analysis in the context of heavy machine crane products.","By leveraging AI and NLP, organizations can gain insights into customer perceptions, improve product development, enhance satisfaction and loyalty, inform decision-making, and gain a competitive advantage.","The paper highlights the impact of user feedback analysis on organizational performance and emphasizes the reasons for using AI and NLP, including scalability, objectivity, improved accuracy, increased insights, and time savings.","The methodology involves data collection, cleaning, text and rating analysis, interpretation, and feedback implementation.","Results include sentiment analysis, word cloud visualizations, and radar charts comparing product attributes.","These findings provide valuable information for understanding customer sentiment, identifying improvement areas, and making data-driven decisions to enhance the customer experience.","In conclusion, promising AI and NLP techniques in user feedback analysis offer organizations a powerful tool to understand customers, improve product development, increase satisfaction, and drive business success"],"url":"http://arxiv.org/abs/2405.04692v1","category":"econ.GN"}
{"created":"2024-05-07 21:59:57","title":"Towards Human-AI Mutual Learning: A New Research Paradigm","abstract":"This paper describes a new research paradigm for studying human-AI collaboration, named \"human-AI mutual learning\", defined as the process where humans and AI agents preserve, exchange, and improve knowledge during human-AI collaboration. We describe relevant methodologies, motivations, domain examples, benefits, challenges, and future research agenda under this paradigm.","sentences":["This paper describes a new research paradigm for studying human-AI collaboration, named \"human-AI mutual learning\", defined as the process where humans and AI agents preserve, exchange, and improve knowledge during human-AI collaboration.","We describe relevant methodologies, motivations, domain examples, benefits, challenges, and future research agenda under this paradigm."],"url":"http://arxiv.org/abs/2405.04687v1","category":"cs.HC"}
{"created":"2024-05-07 21:58:45","title":"Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking","abstract":"Large Language Models (LLMs) are becoming crucial across various fields, emphasizing the urgency for high-quality models in underrepresented languages. This study explores the unique challenges faced by low-resource languages, such as data scarcity, model selection, evaluation, and computational limitations, with a special focus on Turkish. We conduct an in-depth analysis to evaluate the impact of training strategies, model choices, and data availability on the performance of LLMs designed for underrepresented languages. Our approach includes two methodologies: (i) adapting existing LLMs originally pretrained in English to understand Turkish, and (ii) developing a model from the ground up using Turkish pretraining data, both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities. The relative performance of these methods is evaluated through the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that assess different reasoning and knowledge skills. Furthermore, we conducted experiments on data and model scaling, both during pretraining and fine-tuning, simultaneously emphasizing the capacity for knowledge transfer across languages and addressing the challenges of catastrophic forgetting encountered during fine-tuning on a different language. Our goal is to offer a detailed guide for advancing the LLM framework in low-resource linguistic contexts, thereby making natural language processing (NLP) benefits more globally accessible.","sentences":["Large Language Models (LLMs) are becoming crucial across various fields, emphasizing the urgency for high-quality models in underrepresented languages.","This study explores the unique challenges faced by low-resource languages, such as data scarcity, model selection, evaluation, and computational limitations, with a special focus on Turkish.","We conduct an in-depth analysis to evaluate the impact of training strategies, model choices, and data availability on the performance of LLMs designed for underrepresented languages.","Our approach includes two methodologies: (i) adapting existing LLMs originally pretrained in English to understand Turkish, and (ii) developing a model from the ground up using Turkish pretraining data, both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities.","The relative performance of these methods is evaluated through the creation of a new leaderboard for Turkish LLMs, featuring benchmarks that assess different reasoning and knowledge skills.","Furthermore, we conducted experiments on data and model scaling, both during pretraining and fine-tuning, simultaneously emphasizing the capacity for knowledge transfer across languages and addressing the challenges of catastrophic forgetting encountered during fine-tuning on a different language.","Our goal is to offer a detailed guide for advancing the LLM framework in low-resource linguistic contexts, thereby making natural language processing (NLP) benefits more globally accessible."],"url":"http://arxiv.org/abs/2405.04685v1","category":"cs.CL"}
{"created":"2024-05-07 21:52:39","title":"TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation","abstract":"Recent advances in diffusion-based generative modeling have led to the development of text-to-video (T2V) models that can generate high-quality videos conditioned on a text prompt. Most of these T2V models often produce single-scene video clips that depict an entity performing a particular action (e.g., `a red panda climbing a tree'). However, it is pertinent to generate multi-scene videos since they are ubiquitous in the real-world (e.g., `a red panda climbing a tree' followed by `the red panda sleeps on the top of the tree'). To generate multi-scene videos from the pretrained T2V model, we introduce Time-Aligned Captions (TALC) framework. Specifically, we enhance the text-conditioning mechanism in the T2V architecture to recognize the temporal alignment between the video scenes and scene descriptions. For instance, we condition the visual features of the earlier and later scenes of the generated video with the representations of the first scene description (e.g., `a red panda climbing a tree') and second scene description (e.g., `the red panda sleeps on the top of the tree'), respectively. As a result, we show that the T2V model can generate multi-scene videos that adhere to the multi-scene text descriptions and be visually consistent (e.g., entity and background). Further, we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework. We show that the TALC-finetuned model outperforms the baseline methods by 15.5 points in the overall score, which averages visual consistency and text adherence using human evaluation. The project website is https://talc-mst2v.github.io/.","sentences":["Recent advances in diffusion-based generative modeling have led to the development of text-to-video (T2V) models that can generate high-quality videos conditioned on a text prompt.","Most of these T2V models often produce single-scene video clips that depict an entity performing a particular action (e.g., `a red panda climbing a tree').","However, it is pertinent to generate multi-scene videos since they are ubiquitous in the real-world (e.g., `a red panda climbing a tree' followed by `the red panda sleeps on the top of the tree').","To generate multi-scene videos from the pretrained T2V model, we introduce Time-Aligned Captions (TALC) framework.","Specifically, we enhance the text-conditioning mechanism in the T2V architecture to recognize the temporal alignment between the video scenes and scene descriptions.","For instance, we condition the visual features of the earlier and later scenes of the generated video with the representations of the first scene description (e.g., `a red panda climbing a tree') and second scene description (e.g., `the red panda sleeps on the top of the tree'), respectively.","As a result, we show that the T2V model can generate multi-scene videos that adhere to the multi-scene text descriptions and be visually consistent (e.g., entity and background).","Further, we finetune the pretrained T2V model with multi-scene video-text data using the TALC framework.","We show that the TALC-finetuned model outperforms the baseline methods by 15.5 points in the overall score, which averages visual consistency and text adherence using human evaluation.","The project website is https://talc-mst2v.github.io/."],"url":"http://arxiv.org/abs/2405.04682v1","category":"cs.CV"}
{"created":"2024-05-07 21:34:10","title":"Responding to Generative AI Technologies with Research-through-Design: The Ryelands AI Lab as an Exploratory Study","abstract":"Generative AI technologies demand new practical and critical competencies, which call on design to respond to and foster these. We present an exploratory study guided by Research-through-Design, in which we partnered with a primary school to develop a constructionist curriculum centered on students interacting with a generative AI technology. We provide a detailed account of the design of and outputs from the curriculum and learning materials, finding centrally that the reflexive and prolonged `hands-on' approach led to a co-development of students' practical and critical competencies. From the study, we contribute guidance for designing constructionist approaches to generative AI technology education; further arguing to do so with `critical responsivity.' We then discuss how HCI researchers may leverage constructionist strategies in designing interactions with generative AI technologies; and suggest that Research-through-Design can play an important role as a `rapid response methodology' capable of reacting to fast-evolving, disruptive technologies such as generative AI.","sentences":["Generative AI technologies demand new practical and critical competencies, which call on design to respond to and foster these.","We present an exploratory study guided by Research-through-Design, in which we partnered with a primary school to develop a constructionist curriculum centered on students interacting with a generative AI technology.","We provide a detailed account of the design of and outputs from the curriculum and learning materials, finding centrally that the reflexive and prolonged `hands-on' approach led to a co-development of students' practical and critical competencies.","From the study, we contribute guidance for designing constructionist approaches to generative AI technology education; further arguing to do so with `critical responsivity.'","We then discuss how HCI researchers may leverage constructionist strategies in designing interactions with generative AI technologies; and suggest that Research-through-Design can play an important role as a `rapid response methodology' capable of reacting to fast-evolving, disruptive technologies such as generative AI."],"url":"http://arxiv.org/abs/2405.04677v1","category":"cs.HC"}
{"created":"2024-05-07 21:14:38","title":"Towards Accurate and Efficient Document Analytics with Large Language Models","abstract":"Unstructured data formats account for over 80% of the data currently stored, and extracting value from such formats remains a considerable challenge. In particular, current approaches for managing unstructured documents do not support ad-hoc analytical queries on document collections. Moreover, Large Language Models (LLMs) directly applied to the documents themselves, or on portions of documents through a process of Retrieval-Augmented Generation (RAG), fail to provide high accuracy query results, and in the LLM-only case, additionally incur high costs. Since many unstructured documents in a collection often follow similar templates that impart a common semantic structure, we introduce ZenDB, a document analytics system that leverages this semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document collections. ZenDB efficiently extracts semantic hierarchical structures from such templatized documents, and introduces a novel query engine that leverages these structures for accurate and cost-effective query execution. Users can impose a schema on their documents, and query it, all via SQL. Extensive experiments on three real-world document collections demonstrate ZenDB's benefits, achieving up to 30% cost savings compared to LLM-based baselines, while maintaining or improving accuracy, and surpassing RAG-based baselines by up to 61% in precision and 80% in recall, at a marginally higher cost.","sentences":["Unstructured data formats account for over 80% of the data currently stored, and extracting value from such formats remains a considerable challenge.","In particular, current approaches for managing unstructured documents do not support ad-hoc analytical queries on document collections.","Moreover, Large Language Models (LLMs) directly applied to the documents themselves, or on portions of documents through a process of Retrieval-Augmented Generation (RAG), fail to provide high accuracy query results, and in the LLM-only case, additionally incur high costs.","Since many unstructured documents in a collection often follow similar templates that impart a common semantic structure, we introduce ZenDB, a document analytics system that leverages this semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document collections.","ZenDB efficiently extracts semantic hierarchical structures from such templatized documents, and introduces a novel query engine that leverages these structures for accurate and cost-effective query execution.","Users can impose a schema on their documents, and query it, all via SQL.","Extensive experiments on three real-world document collections demonstrate ZenDB's benefits, achieving up to 30% cost savings compared to LLM-based baselines, while maintaining or improving accuracy, and surpassing RAG-based baselines by up to 61% in precision and 80% in recall, at a marginally higher cost."],"url":"http://arxiv.org/abs/2405.04674v1","category":"cs.DB"}
{"created":"2024-05-07 20:51:49","title":"Proximal Policy Optimization with Adaptive Exploration","abstract":"Proximal Policy Optimization with Adaptive Exploration (axPPO) is introduced as a novel learning algorithm. This paper investigates the exploration-exploitation tradeoff within the context of reinforcement learning and aims to contribute new insights into reinforcement learning algorithm design. The proposed adaptive exploration framework dynamically adjusts the exploration magnitude during training based on the recent performance of the agent. Our proposed method outperforms standard PPO algorithms in learning efficiency, particularly when significant exploratory behavior is needed at the beginning of the learning process.","sentences":["Proximal Policy Optimization with Adaptive Exploration (axPPO) is introduced as a novel learning algorithm.","This paper investigates the exploration-exploitation tradeoff within the context of reinforcement learning and aims to contribute new insights into reinforcement learning algorithm design.","The proposed adaptive exploration framework dynamically adjusts the exploration magnitude during training based on the recent performance of the agent.","Our proposed method outperforms standard PPO algorithms in learning efficiency, particularly when significant exploratory behavior is needed at the beginning of the learning process."],"url":"http://arxiv.org/abs/2405.04664v1","category":"cs.LG"}
{"created":"2024-05-07 20:30:14","title":"ACEGEN: Reinforcement learning of generative chemical agents for drug discovery","abstract":"In recent years, reinforcement learning (RL) has emerged as a valuable tool in drug design, offering the potential to propose and optimize molecules with desired properties. However, striking a balance between capability, flexibility, and reliability remains challenging due to the complexity of advanced RL algorithms and the significant reliance on specialized code. In this work, we introduce ACEGEN, a comprehensive and streamlined toolkit tailored for generative drug design, built using TorchRL, a modern decision-making library that offers efficient and thoroughly tested reusable components. ACEGEN provides a robust, flexible, and efficient platform for molecular design. We validate its effectiveness by benchmarking it across various algorithms and conducting multiple drug discovery case studies. ACEGEN is accessible at https://github.com/acellera/acegen-open.","sentences":["In recent years, reinforcement learning (RL) has emerged as a valuable tool in drug design, offering the potential to propose and optimize molecules with desired properties.","However, striking a balance between capability, flexibility, and reliability remains challenging due to the complexity of advanced RL algorithms and the significant reliance on specialized code.","In this work, we introduce ACEGEN, a comprehensive and streamlined toolkit tailored for generative drug design, built using TorchRL, a modern decision-making library that offers efficient and thoroughly tested reusable components.","ACEGEN provides a robust, flexible, and efficient platform for molecular design.","We validate its effectiveness by benchmarking it across various algorithms and conducting multiple drug discovery case studies.","ACEGEN is accessible at https://github.com/acellera/acegen-open."],"url":"http://arxiv.org/abs/2405.04657v1","category":"cs.LG"}
{"created":"2024-05-07 20:16:21","title":"Artificial Broadcasts as Galactic Populations: II. Comparing Individualist and Collective Bounds on Broadcast Populations in Single Galaxies","abstract":"The search for extraterrestrial intelligence includes efforts to constrain populations of artificial broadcasts in other galaxies. Previous efforts use individualist methods, searching for single broadcasts with high signal-to-noise ratio. These would be detected as observables with extreme values. This approach is limited to very bright broadcasts and also is subject to confusion, where a large number of broadcasts blend together to form a noise continuum. The mean value of the total emission provides an additional collective bound: the luminosity of the transmitters is no higher than the galaxy's observed luminosity. Using the framework developed in Paper I, I evaluate how confusion affects individualist searches. I then compare individualist and collective approaches for radio broadcasts from the Milky Way, M31, and three Virgo Cluster elliptical galaxies. For current observations, confusion blurs narrowband radio broadcasts together in the Virgo ellipticals when there is one broadcast per gigahertz per 1000 stars. The collective bound implies fewer than $\\sim 10^6 (\\overline{\\ell}/10^{13} W)^{-1}$ L-band broadcasts per star gigahertz GHz in the Milky Way and is about 10 and 400 times stronger in M31 and M59, respectively. Applying the collective bound to the far-infrared--radio correlation yields constraints on radio broadcast populations in star-forming galaxies throughout the Universe. The collective bound allows us to rule out large regions of broadcast population parameter space even for distant galaxies. It also imposes constraints on gamma-ray, neutrino, and gravitational-wave broadcasts in the nearest galaxies.","sentences":["The search for extraterrestrial intelligence includes efforts to constrain populations of artificial broadcasts in other galaxies.","Previous efforts use individualist methods, searching for single broadcasts with high signal-to-noise ratio.","These would be detected as observables with extreme values.","This approach is limited to very bright broadcasts and also is subject to confusion, where a large number of broadcasts blend together to form a noise continuum.","The mean value of the total emission provides an additional collective bound: the luminosity of the transmitters is no higher than the galaxy's observed luminosity.","Using the framework developed in Paper I, I evaluate how confusion affects individualist searches.","I then compare individualist and collective approaches for radio broadcasts from the Milky Way, M31, and three Virgo Cluster elliptical galaxies.","For current observations, confusion blurs narrowband radio broadcasts together in the Virgo ellipticals when there is one broadcast per gigahertz per 1000 stars.","The collective bound implies fewer than $\\sim 10^6 (\\overline{\\ell}/10^{13} W)^{-1}$ L-band broadcasts per star gigahertz GHz in the Milky Way and is about 10 and 400 times stronger in M31 and M59, respectively.","Applying the collective bound to the far-infrared--radio correlation yields constraints on radio broadcast populations in star-forming galaxies throughout the Universe.","The collective bound allows us to rule out large regions of broadcast population parameter space even for distant galaxies.","It also imposes constraints on gamma-ray, neutrino, and gravitational-wave broadcasts in the nearest galaxies."],"url":"http://arxiv.org/abs/2405.04651v1","category":"astro-ph.GA"}
{"created":"2024-05-07 20:11:07","title":"A Self-Supervised Method for Body Part Segmentation and Keypoint Detection of Rat Images","abstract":"Recognition of individual components and keypoint detection supported by instance segmentation is crucial to analyze the behavior of agents on the scene. Such systems could be used for surveillance, self-driving cars, and also for medical research, where behavior analysis of laboratory animals is used to confirm the aftereffects of a given medicine. A method capable of solving the aforementioned tasks usually requires a large amount of high-quality hand-annotated data, which takes time and money to produce. In this paper, we propose a method that alleviates the need for manual labeling of laboratory rats. To do so, first, we generate initial annotations with a computer vision-based approach, then through extensive augmentation, we train a deep neural network on the generated data. The final system is capable of instance segmentation, keypoint detection, and body part segmentation even when the objects are heavily occluded.","sentences":["Recognition of individual components and keypoint detection supported by instance segmentation is crucial to analyze the behavior of agents on the scene.","Such systems could be used for surveillance, self-driving cars, and also for medical research, where behavior analysis of laboratory animals is used to confirm the aftereffects of a given medicine.","A method capable of solving the aforementioned tasks usually requires a large amount of high-quality hand-annotated data, which takes time and money to produce.","In this paper, we propose a method that alleviates the need for manual labeling of laboratory rats.","To do so, first, we generate initial annotations with a computer vision-based approach, then through extensive augmentation, we train a deep neural network on the generated data.","The final system is capable of instance segmentation, keypoint detection, and body part segmentation even when the objects are heavily occluded."],"url":"http://arxiv.org/abs/2405.04650v1","category":"cs.CV"}
{"created":"2024-05-07 20:09:23","title":"Artificial Broadcasts as Galactic Populations: I. A Point Process Formalism for Extraterrestrial Intelligences and Their Broadcasts","abstract":"Artificial broadcasts from extraterrestrial intelligences (ETIs) are a hypothetical class of celestial phenomena. Unlike known astrophysical objects, the societies that generate them may be able to replicate on galactic scales through interstellar travel. Different galaxies could thus have drastically different populations, with abundance variations of many orders of magnitude. I present a probabilistic formalism to treat this shared history, in which societies and their broadcasts are described by distributions over basic properties like lifespan and energy released. The framework contains a hierarchy of objects related by a tree structure. Discrete societies, the sources of broadcasts, are organized into potentially interstellar \"metasocieties.\" The population of each type of object is represented by a random point process in an abstract parameter hyperspace, a \"haystack.\" When a selection like an observation draws a sample, the point process is thinned. Given assumptions of interchangeability and independence, observables are modeled with compound Poisson random variables. I present an example of how selection bias can favor sampling longer-lived objects. I rederive the Drake Equation for societies in the limit of no expansion. When interstellar replication is present, however, the mean number of detected broadcasts can depend quadratically on stellar mass, suggesting a search strategy favoring large galaxies.","sentences":["Artificial broadcasts from extraterrestrial intelligences (ETIs) are a hypothetical class of celestial phenomena.","Unlike known astrophysical objects, the societies that generate them may be able to replicate on galactic scales through interstellar travel.","Different galaxies could thus have drastically different populations, with abundance variations of many orders of magnitude.","I present a probabilistic formalism to treat this shared history, in which societies and their broadcasts are described by distributions over basic properties like lifespan and energy released.","The framework contains a hierarchy of objects related by a tree structure.","Discrete societies, the sources of broadcasts, are organized into potentially interstellar \"metasocieties.\"","The population of each type of object is represented by a random point process in an abstract parameter hyperspace, a \"haystack.\"","When a selection like an observation draws a sample, the point process is thinned.","Given assumptions of interchangeability and independence, observables are modeled with compound Poisson random variables.","I present an example of how selection bias can favor sampling longer-lived objects.","I rederive the Drake Equation for societies in the limit of no expansion.","When interstellar replication is present, however, the mean number of detected broadcasts can depend quadratically on stellar mass, suggesting a search strategy favoring large galaxies."],"url":"http://arxiv.org/abs/2405.04646v1","category":"astro-ph.GA"}
{"created":"2024-05-07 20:09:18","title":"Enhancing LLM-Based Feedback: Insights from Intelligent Tutoring Systems and the Learning Sciences","abstract":"The field of Artificial Intelligence in Education (AIED) focuses on the intersection of technology, education, and psychology, placing a strong emphasis on supporting learners' needs with compassion and understanding. The growing prominence of Large Language Models (LLMs) has led to the development of scalable solutions within educational settings, including generating different types of feedback in Intelligent Tutoring Systems. However, the approach to utilizing these models often involves directly formulating prompts to solicit specific information, lacking a solid theoretical foundation for prompt construction and empirical assessments of their impact on learning. This work advocates careful and caring AIED research by going through previous research on feedback generation in ITS, with emphasis on the theoretical frameworks they utilized and the efficacy of the corresponding design in empirical evaluations, and then suggesting opportunities to apply these evidence-based principles to the design, experiment, and evaluation phases of LLM-based feedback generation. The main contributions of this paper include: an avocation of applying more cautious, theoretically grounded methods in feedback generation in the era of generative AI; and practical suggestions on theory and evidence-based feedback design for LLM-powered ITS.","sentences":["The field of Artificial Intelligence in Education (AIED) focuses on the intersection of technology, education, and psychology, placing a strong emphasis on supporting learners' needs with compassion and understanding.","The growing prominence of Large Language Models (LLMs) has led to the development of scalable solutions within educational settings, including generating different types of feedback in Intelligent Tutoring Systems.","However, the approach to utilizing these models often involves directly formulating prompts to solicit specific information, lacking a solid theoretical foundation for prompt construction and empirical assessments of their impact on learning.","This work advocates careful and caring AIED research by going through previous research on feedback generation in ITS, with emphasis on the theoretical frameworks they utilized and the efficacy of the corresponding design in empirical evaluations, and then suggesting opportunities to apply these evidence-based principles to the design, experiment, and evaluation phases of LLM-based feedback generation.","The main contributions of this paper include: an avocation of applying more cautious, theoretically grounded methods in feedback generation in the era of generative AI; and practical suggestions on theory and evidence-based feedback design for LLM-powered ITS."],"url":"http://arxiv.org/abs/2405.04645v1","category":"cs.HC"}
{"created":"2024-05-07 19:20:32","title":"ResNCT: A Deep Learning Model for the Synthesis of Nephrographic Phase Images in CT Urography","abstract":"Purpose: To develop and evaluate a transformer-based deep learning model for the synthesis of nephrographic phase images in CT urography (CTU) examinations from the unenhanced and urographic phases.   Materials and Methods: This retrospective study was approved by the local Institutional Review Board. A dataset of 119 patients (mean $\\pm$ SD age, 65 $\\pm$ 12 years; 75/44 males/females) with three-phase CT urography studies was curated for deep learning model development. The three phases for each patient were aligned with an affine registration algorithm. A custom model, coined Residual transformer model for Nephrographic phase CT image synthesis (ResNCT), was developed and implemented with paired inputs of non-contrast and urographic sets of images trained to produce the nephrographic phase images, that were compared with the corresponding ground truth nephrographic phase images. The synthesized images were evaluated with multiple performance metrics, including peak signal to noise ratio (PSNR), structural similarity index (SSIM), normalized cross correlation coefficient (NCC), mean absolute error (MAE), and root mean squared error (RMSE).   Results: The ResNCT model successfully generated synthetic nephrographic images from non-contrast and urographic image inputs. With respect to ground truth nephrographic phase images, the images synthesized by the model achieved high PSNR (27.8 $\\pm$ 2.7 dB), SSIM (0.88 $\\pm$ 0.05), and NCC (0.98 $\\pm$ 0.02), and low MAE (0.02 $\\pm$ 0.005) and RMSE (0.042 $\\pm$ 0.016).   Conclusion: The ResNCT model synthesized nephrographic phase CT images with high similarity to ground truth images. The ResNCT model provides a means of eliminating the acquisition of the nephrographic phase with a resultant 33% reduction in radiation dose for CTU examinations.","sentences":["Purpose: To develop and evaluate a transformer-based deep learning model for the synthesis of nephrographic phase images in CT urography (CTU) examinations from the unenhanced and urographic phases.   ","Materials and Methods: This retrospective study was approved by the local Institutional Review Board.","A dataset of 119 patients (mean $\\pm$ SD age, 65 $\\pm$ 12 years; 75/44 males/females) with three-phase CT urography studies was curated for deep learning model development.","The three phases for each patient were aligned with an affine registration algorithm.","A custom model, coined Residual transformer model for Nephrographic phase CT image synthesis (ResNCT), was developed and implemented with paired inputs of non-contrast and urographic sets of images trained to produce the nephrographic phase images, that were compared with the corresponding ground truth nephrographic phase images.","The synthesized images were evaluated with multiple performance metrics, including peak signal to noise ratio (PSNR), structural similarity index (SSIM), normalized cross correlation coefficient (NCC), mean absolute error (MAE), and root mean squared error (RMSE).   ","Results:","The ResNCT model successfully generated synthetic nephrographic images from non-contrast and urographic image inputs.","With respect to ground truth nephrographic phase images, the images synthesized by the model achieved high PSNR (27.8 $\\pm$ 2.7 dB), SSIM (0.88 $\\pm$ 0.05), and NCC (0.98 $\\pm$ 0.02), and low MAE (0.02 $\\pm$ 0.005) and RMSE (0.042 $\\pm$ 0.016).   ","Conclusion: The ResNCT model synthesized nephrographic phase CT images with high similarity to ground truth images.","The ResNCT model provides a means of eliminating the acquisition of the nephrographic phase with a resultant 33% reduction in radiation dose for CTU examinations."],"url":"http://arxiv.org/abs/2405.04629v1","category":"eess.IV"}
{"created":"2024-05-07 19:05:26","title":"Folded context condensation in Path Integral formalism for infinite context transformers","abstract":"This short note is written for rapid communication of long context training and to share the idea of how to train it with low memory usage. In the note, we generalize the attention algorithm and neural network of Generative Pre-Trained Transformers and reinterpret it in Path integral formalism. First, the role of the transformer is understood as the time evolution of the token state and second, it is suggested that the all key-token states in the same time as the query-token can attend to the attention with the query token states. As a result of the repetitive time evolution, it is discussed that the token states in the past sequence meats the token states in the present sequence so that the attention between separated sequences becomes possible for maintaining infinite contextual information just by using low memory for limited size of sequence. For the experiment, the $12$ input token window size was taken and one GPU with $24$GB memory was used for the pre-training. It was confirmed that more than $150$ length context is preserved. The sampling result of the training, the code and the other details will be included in the revised version of this note later.","sentences":["This short note is written for rapid communication of long context training and to share the idea of how to train it with low memory usage.","In the note, we generalize the attention algorithm and neural network of Generative Pre-Trained Transformers and reinterpret it in Path integral formalism.","First, the role of the transformer is understood as the time evolution of the token state and second, it is suggested that the all key-token states in the same time as the query-token can attend to the attention with the query token states.","As a result of the repetitive time evolution, it is discussed that the token states in the past sequence meats the token states in the present sequence so that the attention between separated sequences becomes possible for maintaining infinite contextual information just by using low memory for limited size of sequence.","For the experiment, the $12$ input token window size was taken and one GPU with $24$GB memory was used for the pre-training.","It was confirmed that more than $150$ length context is preserved.","The sampling result of the training, the code and the other details will be included in the revised version of this note later."],"url":"http://arxiv.org/abs/2405.04620v1","category":"hep-ph"}
{"created":"2024-05-07 18:36:40","title":"AI in Lung Health: Benchmarking Detection and Diagnostic Models Across Multiple CT Scan Datasets","abstract":"BACKGROUND: Lung cancer's high mortality rate can be mitigated by early detection, which is increasingly reliant on artificial intelligence (AI) for diagnostic imaging. However, the performance of AI models is contingent upon the datasets used for their training and validation. METHODS: This study developed and validated the DLCSD-mD and LUNA16-mD models utilizing the Duke Lung Cancer Screening Dataset (DLCSD), encompassing over 2,000 CT scans with more than 3,000 annotations. These models were rigorously evaluated against the internal DLCSD and external LUNA16 and NLST datasets, aiming to establish a benchmark for imaging-based performance. The assessment focused on creating a standardized evaluation framework to facilitate consistent comparison with widely utilized datasets, ensuring a comprehensive validation of the model's efficacy. Diagnostic accuracy was assessed using free-response receiver operating characteristic (FROC) and area under the curve (AUC) analyses. RESULTS: On the internal DLCSD set, the DLCSD-mD model achieved an AUC of 0.93 (95% CI:0.91-0.94), demonstrating high accuracy. Its performance was sustained on the external datasets, with AUCs of 0.97 (95% CI: 0.96-0.98) on LUNA16 and 0.75 (95% CI: 0.73-0.76) on NLST. Similarly, the LUNA16-mD model recorded an AUC of 0.96 (95% CI: 0.95-0.97) on its native dataset and showed transferable diagnostic performance with AUCs of 0.91 (95% CI: 0.89-0.93) on DLCSD and 0.71 (95% CI: 0.70-0.72) on NLST. CONCLUSION: The DLCSD-mD model exhibits reliable performance across different datasets, establishing the DLCSD as a robust benchmark for lung cancer detection and diagnosis. Through the provision of our models and code to the public domain, we aim to accelerate the development of AI-based diagnostic tools and encourage reproducibility and collaborative advancements within the medical machine-learning (ML) field.","sentences":["BACKGROUND: Lung cancer's high mortality rate can be mitigated by early detection, which is increasingly reliant on artificial intelligence (AI) for diagnostic imaging.","However, the performance of AI models is contingent upon the datasets used for their training and validation.","METHODS:","This study developed and validated the DLCSD-mD and LUNA16-mD models utilizing the Duke Lung Cancer Screening Dataset (DLCSD), encompassing over 2,000 CT scans with more than 3,000 annotations.","These models were rigorously evaluated against the internal DLCSD and external LUNA16 and NLST datasets, aiming to establish a benchmark for imaging-based performance.","The assessment focused on creating a standardized evaluation framework to facilitate consistent comparison with widely utilized datasets, ensuring a comprehensive validation of the model's efficacy.","Diagnostic accuracy was assessed using free-response receiver operating characteristic (FROC) and area under the curve (AUC) analyses.","RESULTS:","On the internal DLCSD set, the DLCSD-mD model achieved an AUC of 0.93 (95% CI:0.91-0.94), demonstrating high accuracy.","Its performance was sustained on the external datasets, with AUCs of 0.97 (95% CI: 0.96-0.98) on LUNA16 and 0.75 (95% CI: 0.73-0.76) on NLST.","Similarly, the LUNA16-mD model recorded an AUC of 0.96 (95% CI: 0.95-0.97) on its native dataset and showed transferable diagnostic performance with AUCs of 0.91 (95% CI: 0.89-0.93) on DLCSD and 0.71 (95% CI: 0.70-0.72) on NLST.","CONCLUSION:","The DLCSD-mD model exhibits reliable performance across different datasets, establishing the DLCSD as a robust benchmark for lung cancer detection and diagnosis.","Through the provision of our models and code to the public domain, we aim to accelerate the development of AI-based diagnostic tools and encourage reproducibility and collaborative advancements within the medical machine-learning (ML) field."],"url":"http://arxiv.org/abs/2405.04605v1","category":"cs.CV"}
{"created":"2024-05-07 18:10:08","title":"Visually Guided Swarm Motion Coordination via Insect-inspired Small Target Motion Reactions","abstract":"Despite progress developing experimentally-consistent models of insect in-flight sensing and feedback for individual agents, a lack of systematic understanding of the multi-agent and group performance of the resulting bio-inspired sensing and feedback approaches remains a barrier to robotic swarm implementations. This study introduces the small-target motion reactive (STMR) swarming approach by designing a concise engineering model of the small target motion detector (STMD) neurons found in insect lobula complexes. The STMD neuron model identifies the bearing angle at which peak optic flow magnitude occurs, and this angle is used to design an output feedback switched control system. A theoretical stability analysis provides bi-agent stability and state boundedness in group contexts. The approach is simulated and implemented on ground vehicles for validation and behavioral studies. The results indicate despite having the lowest connectivity of contemporary approaches (each agent instantaneously regards only a single neighbor), collective group motion can be achieved. STMR group level metric analysis also highlights continuously varying polarization and decreasing heading variance.","sentences":["Despite progress developing experimentally-consistent models of insect in-flight sensing and feedback for individual agents, a lack of systematic understanding of the multi-agent and group performance of the resulting bio-inspired sensing and feedback approaches remains a barrier to robotic swarm implementations.","This study introduces the small-target motion reactive (STMR) swarming approach by designing a concise engineering model of the small target motion detector (STMD) neurons found in insect lobula complexes.","The STMD neuron model identifies the bearing angle at which peak optic flow magnitude occurs, and this angle is used to design an output feedback switched control system.","A theoretical stability analysis provides bi-agent stability and state boundedness in group contexts.","The approach is simulated and implemented on ground vehicles for validation and behavioral studies.","The results indicate despite having the lowest connectivity of contemporary approaches (each agent instantaneously regards only a single neighbor), collective group motion can be achieved.","STMR group level metric analysis also highlights continuously varying polarization and decreasing heading variance."],"url":"http://arxiv.org/abs/2405.04591v1","category":"eess.SY"}
{"created":"2024-05-07 18:00:01","title":"Kirkwood-Dirac representations beyond quantum states (and their relation to noncontextuality)","abstract":"Kirkwood-Dirac representations of quantum states are increasingly finding use in many areas within quantum theory. Usually, representations of this sort are only applied to provide a representation of quantum states (as complex functions over some set). We show how standard Kirkwood-Dirac representations can be extended to a fully compositional representation of all of quantum theory (including channels, measurements and so on), and prove that this extension satisfies the essential features of functoriality (namely, that the representation commutes with composition of channels), linearity, and quasistochasticity. Interestingly, the representation of a POVM element is uniquely picked out to be the collection of weak values for it relative to the bases defining the representation. We then prove that if one can find any Kirkwood-Dirac representation that is everywhere real and nonnegative for a given experimental scenario or fragment of quantum theory, then the scenario or fragment is consistent with the principle of generalized noncontextuality, a key notion of classicality in quantum foundations. We also show that the converse does not hold: even if one verifies that all Kirkwood-Dirac representations (as defined herein) of an experiment require negativity or imaginarity, one cannot generally conclude that the experiment witnesses contextuality.","sentences":["Kirkwood-Dirac representations of quantum states are increasingly finding use in many areas within quantum theory.","Usually, representations of this sort are only applied to provide a representation of quantum states (as complex functions over some set).","We show how standard Kirkwood-Dirac representations can be extended to a fully compositional representation of all of quantum theory (including channels, measurements and so on), and prove that this extension satisfies the essential features of functoriality (namely, that the representation commutes with composition of channels), linearity, and quasistochasticity.","Interestingly, the representation of a POVM element is uniquely picked out to be the collection of weak values for it relative to the bases defining the representation.","We then prove that if one can find any Kirkwood-Dirac representation that is everywhere real and nonnegative for a given experimental scenario or fragment of quantum theory, then the scenario or fragment is consistent with the principle of generalized noncontextuality, a key notion of classicality in quantum foundations.","We also show that the converse does not hold: even if one verifies that all Kirkwood-Dirac representations (as defined herein) of an experiment require negativity or imaginarity, one cannot generally conclude that the experiment witnesses contextuality."],"url":"http://arxiv.org/abs/2405.04573v1","category":"quant-ph"}
{"created":"2024-05-07 18:00:01","title":"The Orbit of NGC 5907 ULX-1","abstract":"We report on the orbit of the binary system powering the most extreme ultraluminous X-ray pulsar known to date: NGC 5907 ULX-1 (hereafter ULX1). ULX1 has been the target of a substantial multi-instrument campaign, mainly in the X-ray band, but no clear counterparts are known in other bands. Although ULX1 is highly variable and pulsations can be transient (regardless of the source flux), the timing data collected so far allow us to investigate the orbit of this system. We find an orbital period $P_{orb}=5.7^{+0.1}_{-0.6}\\text{ d}$ and a projected semi-axis $A_1 =3.1^{+0.8}_{-0.9}\\text{ lts}$. The most likely ephemeris is: $P_{orb}=5.6585(6)\\text{ d}$, $A_1 = 3.1(4)\\text{ lts}$, and the epoch of ascending nodes passage is: $T_{asc} = 57751.37(5)\\text{ MJD}$. However, there are 6 similar solutions, acceptable within $3\\,\\sigma$. We find further indications that ULX1 is a high-mass X-ray binary. This implies that we are observing its orbit face-on, with an inclination $<5\\text{ deg}$.","sentences":["We report on the orbit of the binary system powering the most extreme ultraluminous X-ray pulsar known to date: NGC 5907 ULX-1 (hereafter ULX1).","ULX1 has been the target of a substantial multi-instrument campaign, mainly in the X-ray band, but no clear counterparts are known in other bands.","Although ULX1 is highly variable and pulsations can be transient (regardless of the source flux), the timing data collected so far allow us to investigate the orbit of this system.","We find an orbital period $P_{orb}=5.7^{+0.1}_{-0.6}\\text{ d}$ and a projected semi-axis $A_1 =3.1^{+0.8}_{-0.9}\\text{ lts}$. The most likely ephemeris is: $P_{orb}=5.6585(6)\\text{ d}$, $A_1 = 3.1(4)\\text{ lts}$, and the epoch of ascending nodes passage is: $T_{asc} = 57751.37(5)\\text{ MJD}$.","However, there are 6 similar solutions, acceptable within $3\\,\\sigma$. We find further indications that ULX1 is a high-mass X-ray binary.","This implies that we are observing its orbit face-on, with an inclination $<5\\text{ deg}$."],"url":"http://arxiv.org/abs/2405.04574v1","category":"astro-ph.HE"}
{"created":"2024-05-07 17:59:50","title":"Tactile-Augmented Radiance Fields","abstract":"We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF","sentences":["We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space.","This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene.","We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes.","Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features.","We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal.","To evaluate our approach, we collect a dataset of TaRFs.","This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal.","We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks.","Project page: https://dou-yiming.github.io/TaRF"],"url":"http://arxiv.org/abs/2405.04534v1","category":"cs.CV"}
{"created":"2024-05-07 17:59:30","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving","abstract":"Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.","sentences":["Quantization can accelerate large language model (LLM) inference.","Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4.","Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving.","We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs.","To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache.","QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin.","QoQ is implemented by the QServe inference library that achieves measured speedup.","The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores.","Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM.","Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization.","In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency.","We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization.","As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM.","Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100.","Thus, QServe effectively reduces the dollar cost of LLM serving by 3x.","Code is available at https://github.com/mit-han-lab/qserve."],"url":"http://arxiv.org/abs/2405.04532v1","category":"cs.CL"}
{"created":"2024-05-07 17:50:21","title":"xLSTM: Extended Long Short-Term Memory","abstract":"In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.","sentences":["In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM).","Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs).","However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.","We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?","Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques.","Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.","Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures.","Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling."],"url":"http://arxiv.org/abs/2405.04517v1","category":"cs.LG"}
{"created":"2024-05-07 17:44:54","title":"Switchable Decision: Dynamic Neural Generation Networks","abstract":"Auto-regressive generation models achieve competitive performance across many different NLP tasks such as summarization, question answering, and classifications. However, they are also known for being slow in inference, which makes them challenging to deploy in real-time applications. We propose a switchable decision to accelerate inference by dynamically assigning computation resources for each data instance. Automatically making decisions on where to skip and how to balance quality and computation cost with constrained optimization, our dynamic neural generation networks enforce the efficient inference path and determine the optimized trade-off. Experiments across question answering, summarization, and classification benchmarks show that our method benefits from less computation cost during inference while keeping the same accuracy. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.","sentences":["Auto-regressive generation models achieve competitive performance across many different NLP tasks such as summarization, question answering, and classifications.","However, they are also known for being slow in inference, which makes them challenging to deploy in real-time applications.","We propose a switchable decision to accelerate inference by dynamically assigning computation resources for each data instance.","Automatically making decisions on where to skip and how to balance quality and computation cost with constrained optimization, our dynamic neural generation networks enforce the efficient inference path and determine the optimized trade-off.","Experiments across question answering, summarization, and classification benchmarks show that our method benefits from less computation cost during inference while keeping the same accuracy.","Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks."],"url":"http://arxiv.org/abs/2405.04513v1","category":"cs.CL"}
{"created":"2024-05-07 17:38:39","title":"New allometric models for the USA create a step-change in forest carbon estimation, modeling, and mapping","abstract":"The United States national forest inventory (NFI) serves as the foundation for forest aboveground biomass (AGB) and carbon accounting across the nation. These data enable design-based estimates of forest carbon stocks and stock-changes at state and regional levels, but also serve as inputs to model-based approaches for characterizing forest carbon stocks and stock-changes at finer resolutions. Although NFI tree and plot-level data are often treated as truth in these models, they are in fact estimates based on regional species-group models known collectively as the Component Ratio Method (CRM). In late 2023 the Forest Inventory and Analysis (FIA) program introduced a new National Scale Volume and Biomass Estimators (NSVB) system to replace CRM nationwide and offer more precise and accurate representations of forest AGB and carbon. Given the prevalence of model-based AGB studies relying on FIA, there is concern about the transferability of methods from CRM to NSVB models, as well as the comparability of existing CRM AGB products (e.g. maps) to new and forthcoming NSVB AGB products. To begin addressing these concerns we compared previously published CRM AGB maps to new maps produced using identical methods with NSVB AGB reference data. Our results suggest that models relying on passive satellite imagery (e.g. Landsat) provide acceptable estimates of point-in-time NSVB AGB and carbon stocks, but fail to accurately quantify growth in mature closed-canopy forests. We highlight that existing estimates, models, and maps based on FIA reference data are no longer compatible with NSVB, and recommend new methods as well as updated models and maps for accommodating this step-change. Our collective ability to adopt NSVB in our modeling and mapping workflows will help us provide the most accurate spatial forest carbon data possible in order to better inform local management and decision making.","sentences":["The United States national forest inventory (NFI) serves as the foundation for forest aboveground biomass (AGB) and carbon accounting across the nation.","These data enable design-based estimates of forest carbon stocks and stock-changes at state and regional levels, but also serve as inputs to model-based approaches for characterizing forest carbon stocks and stock-changes at finer resolutions.","Although NFI tree and plot-level data are often treated as truth in these models, they are in fact estimates based on regional species-group models known collectively as the Component Ratio Method (CRM).","In late 2023 the Forest Inventory and Analysis (FIA) program introduced a new National Scale Volume and Biomass Estimators (NSVB) system to replace CRM nationwide and offer more precise and accurate representations of forest AGB and carbon.","Given the prevalence of model-based AGB studies relying on FIA, there is concern about the transferability of methods from CRM to NSVB models, as well as the comparability of existing CRM AGB products (e.g. maps) to new and forthcoming NSVB AGB products.","To begin addressing these concerns we compared previously published CRM AGB maps to new maps produced using identical methods with NSVB AGB reference data.","Our results suggest that models relying on passive satellite imagery (e.g. Landsat) provide acceptable estimates of point-in-time NSVB AGB and carbon stocks, but fail to accurately quantify growth in mature closed-canopy forests.","We highlight that existing estimates, models, and maps based on FIA reference data are no longer compatible with NSVB, and recommend new methods as well as updated models and maps for accommodating this step-change.","Our collective ability to adopt NSVB in our modeling and mapping workflows will help us provide the most accurate spatial forest carbon data possible in order to better inform local management and decision making."],"url":"http://arxiv.org/abs/2405.04507v1","category":"stat.AP"}
{"created":"2024-05-07 17:05:27","title":"Toward In-Context Teaching: Adapting Examples to Students' Misconceptions","abstract":"When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill. Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students' changing state of knowledge. There is increasing interest in using computational models, particularly large language models, as pedagogical tools. As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples. But how effectively can these models adapt as teachers to students of different types? To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods. We additionally introduce (3) AToM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs. In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models. In human experiments, both AToM and LLMs outperform non-adaptive random example selection. Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it.","sentences":["When a teacher provides examples for a student to study, these examples must be informative, enabling a student to progress from their current state toward a target concept or skill.","Good teachers must therefore simultaneously infer what students already know and adapt their teaching to students' changing state of knowledge.","There is increasing interest in using computational models, particularly large language models, as pedagogical tools.","As students, language models in particular have shown a remarkable ability to adapt to new tasks given small numbers of examples.","But how effectively can these models adapt as teachers to students of different types?","To study this question, we introduce a suite of models and evaluation methods we call AdapT. AdapT has two components: (1) a collection of simulated Bayesian student models that can be used for evaluation of automated teaching methods; (2) a platform for evaluation with human students, to characterize the real-world effectiveness of these methods.","We additionally introduce (3) AToM, a new probabilistic model for adaptive teaching that jointly infers students' past beliefs and optimizes for the correctness of future beliefs.","In evaluations of simulated students across three learning domains (fraction arithmetic, English morphology, function learning), AToM systematically outperforms LLM-based and standard Bayesian teaching models.","In human experiments, both AToM and LLMs outperform non-adaptive random example selection.","Our results highlight both the difficulty of the adaptive teaching task and the potential of learned adaptive models for solving it."],"url":"http://arxiv.org/abs/2405.04495v1","category":"cs.CL"}
{"created":"2024-05-07 17:02:02","title":"TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters","abstract":"The training, testing, and deployment, of autonomous vehicles requires realistic and efficient simulators. Moreover, because of the high variability between different problems presented in different autonomous systems, these simulators need to be easy to use, and easy to modify. To address these problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv. TorchDriveEnv is a lightweight reinforcement learning benchmark programmed entirely in Python, which can be modified to test a number of different factors in learned vehicle behavior, including the effect of varying kinematic models, agent types, and traffic control patterns. Most importantly unlike many replay based simulation approaches, TorchDriveEnv is fully integrated with a state of the art behavioral simulation API. This allows users to train and evaluate driving models alongside data driven Non-Playable Characters (NPC) whose initializations and driving behavior are reactive, realistic, and diverse. We illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common reinforcement learning baselines in both training and validation environments. Our experiments show that TorchDriveEnv is easy to use, but difficult to solve.","sentences":["The training, testing, and deployment, of autonomous vehicles requires realistic and efficient simulators.","Moreover, because of the high variability between different problems presented in different autonomous systems, these simulators need to be easy to use, and easy to modify.","To address these problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv.","TorchDriveEnv is a lightweight reinforcement learning benchmark programmed entirely in Python, which can be modified to test a number of different factors in learned vehicle behavior, including the effect of varying kinematic models, agent types, and traffic control patterns.","Most importantly unlike many replay based simulation approaches, TorchDriveEnv is fully integrated with a state of the art behavioral simulation API.","This allows users to train and evaluate driving models alongside data driven Non-Playable Characters (NPC) whose initializations and driving behavior are reactive, realistic, and diverse.","We illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common reinforcement learning baselines in both training and validation environments.","Our experiments show that TorchDriveEnv is easy to use, but difficult to solve."],"url":"http://arxiv.org/abs/2405.04491v1","category":"cs.AI"}
{"created":"2024-05-07 16:45:15","title":"Concentration Tail-Bound Analysis of Coevolutionary and Bandit Learning Algorithms","abstract":"Runtime analysis, as a branch of the theory of AI, studies how the number of iterations algorithms take before finding a solution (its runtime) depends on the design of the algorithm and the problem structure. Drift analysis is a state-of-the-art tool for estimating the runtime of randomised algorithms, such as evolutionary and bandit algorithms. Drift refers roughly to the expected progress towards the optimum per iteration. This paper considers the problem of deriving concentration tail-bounds on the runtime/regret of algorithms. It provides a novel drift theorem that gives precise exponential tail-bounds given positive, weak, zero and even negative drift. Previously, such exponential tail bounds were missing in the case of weak, zero, or negative drift. Our drift theorem can be used to prove a strong concentration of the runtime/regret of algorithms in AI. For example, we prove that the regret of the \\rwab bandit algorithm is highly concentrated, while previous analyses only considered the expected regret. This means that the algorithm obtains the optimum within a given time frame with high probability, i.e. a form of algorithm reliability. Moreover, our theorem implies that the time needed by the co-evolutionary algorithm RLS-PD to obtain a Nash equilibrium in a \\bilinear max-min-benchmark problem is highly concentrated. However, we also prove that the algorithm forgets the Nash equilibrium, and the time until this occurs is highly concentrated. This highlights a weakness in the RLS-PD which should be addressed by future work.","sentences":["Runtime analysis, as a branch of the theory of AI, studies how the number of iterations algorithms take before finding a solution (its runtime) depends on the design of the algorithm and the problem structure.","Drift analysis is a state-of-the-art tool for estimating the runtime of randomised algorithms, such as evolutionary and bandit algorithms.","Drift refers roughly to the expected progress towards the optimum per iteration.","This paper considers the problem of deriving concentration tail-bounds on the runtime/regret of algorithms.","It provides a novel drift theorem that gives precise exponential tail-bounds given positive, weak, zero and even negative drift.","Previously, such exponential tail bounds were missing in the case of weak, zero, or negative drift.","Our drift theorem can be used to prove a strong concentration of the runtime/regret of algorithms in AI.","For example, we prove that the regret of the \\rwab bandit algorithm is highly concentrated, while previous analyses only considered the expected regret.","This means that the algorithm obtains the optimum within a given time frame with high probability, i.e. a form of algorithm reliability.","Moreover, our theorem implies that the time needed by the co-evolutionary algorithm RLS-PD to obtain a Nash equilibrium in a \\bilinear max-min-benchmark problem is highly concentrated.","However, we also prove that the algorithm forgets the Nash equilibrium, and the time until this occurs is highly concentrated.","This highlights a weakness in the RLS-PD which should be addressed by future work."],"url":"http://arxiv.org/abs/2405.04480v1","category":"cs.NE"}
{"created":"2024-05-07 16:24:03","title":"A Significantly Better Class of Activation Functions Than ReLU Like Activation Functions","abstract":"This paper introduces a significantly better class of activation functions than the almost universally used ReLU like and Sigmoidal class of activation functions. Two new activation functions referred to as the Cone and Parabolic-Cone that differ drastically from popular activation functions and significantly outperform these on the CIFAR-10 and Imagenette benchmmarks are proposed. The cone activation functions are positive only on a finite interval and are strictly negative except at the end-points of the interval, where they become zero. Thus the set of inputs that produce a positive output for a neuron with cone activation functions is a hyperstrip and not a half-space as is the usual case. Since a hyper strip is the region between two parallel hyper-planes, it allows neurons to more finely divide the input feature space into positive and negative classes than with infinitely wide half-spaces. In particular the XOR function can be learn by a single neuron with cone-like activation functions. Both the cone and parabolic-cone activation functions are shown to achieve higher accuracies with significantly fewer neurons on benchmarks. The results presented in this paper indicate that many nonlinear real-world datasets may be separated with fewer hyperstrips than half-spaces. The Cone and Parabolic-Cone activation functions have larger derivatives than ReLU and are shown to significantly speedup training.","sentences":["This paper introduces a significantly better class of activation functions than the almost universally used ReLU like and Sigmoidal class of activation functions.","Two new activation functions referred to as the Cone and Parabolic-Cone that differ drastically from popular activation functions and significantly outperform these on the CIFAR-10 and Imagenette benchmmarks are proposed.","The cone activation functions are positive only on a finite interval and are strictly negative except at the end-points of the interval, where they become zero.","Thus the set of inputs that produce a positive output for a neuron with cone activation functions is a hyperstrip and not a half-space as is the usual case.","Since a hyper strip is the region between two parallel hyper-planes, it allows neurons to more finely divide the input feature space into positive and negative classes than with infinitely wide half-spaces.","In particular the XOR function can be learn by a single neuron with cone-like activation functions.","Both the cone and parabolic-cone activation functions are shown to achieve higher accuracies with significantly fewer neurons on benchmarks.","The results presented in this paper indicate that many nonlinear real-world datasets may be separated with fewer hyperstrips than half-spaces.","The Cone and Parabolic-Cone activation functions have larger derivatives than ReLU and are shown to significantly speedup training."],"url":"http://arxiv.org/abs/2405.04459v1","category":"cs.AI"}
{"created":"2024-05-07 16:23:06","title":"Towards Geographic Inclusion in the Evaluation of Text-to-Image Models","abstract":"Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases. In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling. However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures. In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs. We collect over 65,000 image annotations and 20 survey responses. We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity. For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative. In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of \"appeal\" captured in reference datasets used to ground evaluations. We recommend steps for improved automatic and human evaluations.","sentences":["Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases.","In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling.","However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures.","In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs.","We collect over 65,000 image annotations and 20 survey responses.","We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity.","For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative.","In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of \"appeal\" captured in reference datasets used to ground evaluations.","We recommend steps for improved automatic and human evaluations."],"url":"http://arxiv.org/abs/2405.04457v1","category":"cs.CV"}
{"created":"2024-05-07 16:16:00","title":"Towards Continual Knowledge Graph Embedding via Incremental Distillation","abstract":"Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score.","sentences":["Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges.","To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge.","However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods.","On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs.","On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively.","In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs.","First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning.","By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features.","Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation.","Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge.","Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines.","Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score."],"url":"http://arxiv.org/abs/2405.04453v1","category":"cs.AI"}
{"created":"2024-05-07 16:11:51","title":"Causal Inference in the Multiverse of Hazard","abstract":"Hazard serves as a pivotal estimand in both practical applications and methodological frameworks. However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups. In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds. Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability. Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect. Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a \"multiverse of hazard.\" The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world's risk lying between the two. This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards.","sentences":["Hazard serves as a pivotal estimand in both practical applications and methodological frameworks.","However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups.","In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds.","Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability.","Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect.","Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a \"multiverse of hazard.\"","The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world's risk lying between the two.","This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards."],"url":"http://arxiv.org/abs/2405.04446v1","category":"stat.ME"}
{"created":"2024-05-07 16:07:29","title":"POV Learning: Individual Alignment of Multimodal Models using Human Perception","abstract":"Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback. This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data. However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably. Since perception differs for each person, the same situation is observed differently. Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ. We hypothesize that individual perception patterns can be used for improving the alignment on an individual level. We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments. For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer. Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment. It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values.","sentences":["Aligning machine learning systems with human expectations is mostly attempted by training with manually vetted human behavioral samples, typically explicit feedback.","This is done on a population level since the context that is capturing the subjective Point-Of-View (POV) of a concrete person in a specific situational context is not retained in the data.","However, we argue that alignment on an individual level can boost the subjective predictive performance for the individual user interacting with the system considerably.","Since perception differs for each person, the same situation is observed differently.","Consequently, the basis for decision making and the subsequent reasoning processes and observable reactions differ.","We hypothesize that individual perception patterns can be used for improving the alignment on an individual level.","We test this, by integrating perception information into machine learning systems and measuring their predictive performance wrt.~individual subjective assessments.","For our empirical study, we collect a novel data set of multimodal stimuli and corresponding eye tracking sequences for the novel task of Perception-Guided Crossmodal Entailment and tackle it with our Perception-Guided Multimodal Transformer.","Our findings suggest that exploiting individual perception signals for the machine learning of subjective human assessments provides a valuable cue for individual alignment.","It does not only improve the overall predictive performance from the point-of-view of the individual user but might also contribute to steering AI systems towards every person's individual expectations and values."],"url":"http://arxiv.org/abs/2405.04443v1","category":"cs.AI"}
{"created":"2024-05-07 16:07:05","title":"AugmenTory: A Fast and Flexible Polygon Augmentation Library","abstract":"Data augmentation is a key technique for addressing the challenge of limited datasets, which have become a major component in the training procedures of image processing. Techniques such as geometric transformations and color space adjustments have been thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes. Data augmentation is the most important key to addressing the challenge of limited datasets, which have become a major component of image processing training procedures. Data augmentation techniques, such as geometric transformations and color space adjustments, are thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes. Polygons play a crucial role in instance segmentation and have seen a surge in use across advanced models, such as YOLOv8. Despite their growing popularity, the lack of specialized libraries hampers the polygon-augmentation process. This paper introduces a novel solution to this challenge, embodied in the newly developed AugmenTory library. Notably, AugmenTory offers reduced computational demands in both time and space compared to existing methods. Additionally, the library includes a postprocessing thresholding feature. The AugmenTory package is publicly available on GitHub, where interested users can access the source code: https://github.com/Smartory/AugmenTory","sentences":["Data augmentation is a key technique for addressing the challenge of limited datasets, which have become a major component in the training procedures of image processing.","Techniques such as geometric transformations and color space adjustments have been thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes.","Data augmentation is the most important key to addressing the challenge of limited datasets, which have become a major component of image processing training procedures.","Data augmentation techniques, such as geometric transformations and color space adjustments, are thoroughly tested for their ability to artificially expand training datasets and generate semi-realistic data for training purposes.","Polygons play a crucial role in instance segmentation and have seen a surge in use across advanced models, such as YOLOv8.","Despite their growing popularity, the lack of specialized libraries hampers the polygon-augmentation process.","This paper introduces a novel solution to this challenge, embodied in the newly developed AugmenTory library.","Notably, AugmenTory offers reduced computational demands in both time and space compared to existing methods.","Additionally, the library includes a postprocessing thresholding feature.","The AugmenTory package is publicly available on GitHub, where interested users can access the source code:","https://github.com/Smartory/AugmenTory"],"url":"http://arxiv.org/abs/2405.04442v1","category":"cs.CV"}
{"created":"2024-05-07 16:05:06","title":"Designing, Developing, and Validating Network Intelligence for Scaling in Service-Based Architectures based on Deep Reinforcement Learning","abstract":"Automating network processes without human intervention is crucial for the complex 6G environment. This requires zero-touch management and orchestration, the integration of Network Intelligence (NI) into the network architecture, and the efficient lifecycle management of intelligent functions. Reinforcement Learning (RL) plays a key role in this context, offering intelligent decision-making capabilities suited to networks' dynamic nature. Despite its potential, integrating RL poses challenges in model development and application. To tackle those issues, we delve into designing, developing, and validating RL algorithms for scaling network functions in service-based network architectures such as Open Radio Access Network (O-RAN). It builds upon and expands previous research on RL lifecycle management by proposing several RL algorithms and Reward Functions (RFns). Our proposed methodology is anchored on a dual approach: firstly, it evaluates the training performance of these algorithms under varying RFns, and secondly, it validates their performance after being trained to discern the practical applicability in real-world settings. We show that, despite significant progress, the development stage of RL techniques for networking applications, particularly in scaling scenarios, still leaves room for significant improvements. This study underscores the importance of ongoing research and development to enhance the practicality and resilience of RL techniques in real-world networking environments.","sentences":["Automating network processes without human intervention is crucial for the complex 6G environment.","This requires zero-touch management and orchestration, the integration of Network Intelligence (NI) into the network architecture, and the efficient lifecycle management of intelligent functions.","Reinforcement Learning (RL) plays a key role in this context, offering intelligent decision-making capabilities suited to networks' dynamic nature.","Despite its potential, integrating RL poses challenges in model development and application.","To tackle those issues, we delve into designing, developing, and validating RL algorithms for scaling network functions in service-based network architectures such as Open Radio Access Network (O-RAN).","It builds upon and expands previous research on RL lifecycle management by proposing several RL algorithms and Reward Functions (RFns).","Our proposed methodology is anchored on a dual approach: firstly, it evaluates the training performance of these algorithms under varying RFns, and secondly, it validates their performance after being trained to discern the practical applicability in real-world settings.","We show that, despite significant progress, the development stage of RL techniques for networking applications, particularly in scaling scenarios, still leaves room for significant improvements.","This study underscores the importance of ongoing research and development to enhance the practicality and resilience of RL techniques in real-world networking environments."],"url":"http://arxiv.org/abs/2405.04441v1","category":"cs.NI"}
{"created":"2024-05-07 15:56:43","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model","abstract":"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.","sentences":["We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference.","It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens.","DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation.","Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.","We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential.","Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models."],"url":"http://arxiv.org/abs/2405.04434v2","category":"cs.CL"}
{"created":"2024-05-07 15:51:33","title":"Designing the Network Intelligence Stratum for 6G Networks","abstract":"As network complexity escalates, there is an increasing need for more sophisticated methods to manage and operate these networks, focusing on enhancing efficiency, reliability, and security. A wide range of Artificial Intelligence (AI)/Machine Learning (ML) models are being developed in response. These models are pivotal in automating decision-making, conducting predictive analyses, managing networks proactively, enhancing security, and optimizing network performance. They are foundational in shaping the future of networks, collectively forming what is known as Network Intelligence (NI). Prominent Standard-Defining Organizations (SDOs) are integrating NI into future network architectures, particularly emphasizing the closed-loop approach. However, existing methods for seamlessly integrating NI into network architectures are not yet fully effective. This paper introduces an in-depth architectural design for a Network Intelligence Stratum (NI Stratum). This stratum is supported by a novel end-to-end NI orchestrator that supports closed-loop NI operations across various network domains. The primary goal of this design is to streamline the deployment and coordination of NI throughout the entire network infrastructure, tackling issues related to scalability, conflict resolution, and effective data management. We detail exhaustive workflows for managing the NI lifecycle and demonstrate a reference implementation of the NI Stratum, focusing on its compatibility and integration with current network systems and open-source platforms such as Kubernetes and Kubeflow, as well as on its validation on real-world environments. The paper also outlines major challenges and open issues in deploying and managing NI.","sentences":["As network complexity escalates, there is an increasing need for more sophisticated methods to manage and operate these networks, focusing on enhancing efficiency, reliability, and security.","A wide range of Artificial Intelligence (AI)/Machine Learning (ML) models are being developed in response.","These models are pivotal in automating decision-making, conducting predictive analyses, managing networks proactively, enhancing security, and optimizing network performance.","They are foundational in shaping the future of networks, collectively forming what is known as Network Intelligence (NI).","Prominent Standard-Defining Organizations (SDOs) are integrating NI into future network architectures, particularly emphasizing the closed-loop approach.","However, existing methods for seamlessly integrating NI into network architectures are not yet fully effective.","This paper introduces an in-depth architectural design for a Network Intelligence Stratum (NI Stratum).","This stratum is supported by a novel end-to-end NI orchestrator that supports closed-loop NI operations across various network domains.","The primary goal of this design is to streamline the deployment and coordination of NI throughout the entire network infrastructure, tackling issues related to scalability, conflict resolution, and effective data management.","We detail exhaustive workflows for managing the NI lifecycle and demonstrate a reference implementation of the NI Stratum, focusing on its compatibility and integration with current network systems and open-source platforms such as Kubernetes and Kubeflow, as well as on its validation on real-world environments.","The paper also outlines major challenges and open issues in deploying and managing NI."],"url":"http://arxiv.org/abs/2405.04432v1","category":"cs.NI"}
{"created":"2024-05-07 15:35:30","title":"Super-Exponential Regret for UCT, AlphaGo and Variants","abstract":"We improve the proofs of the lower bounds of Coquelin and Munos (2007) that demonstrate that UCT can have $\\exp(\\dots\\exp(1)\\dots)$ regret (with $\\Omega(D)$ exp terms) on the $D$-chain environment, and that a `polynomial' UCT variant has $\\exp_2(\\exp_2(D - O(\\log D)))$ regret on the same environment -- the original proofs contain an oversight for rewards bounded in $[0, 1]$, which we fix in the present draft. We also adapt the proofs to AlphaGo's MCTS and its descendants (e.g., AlphaZero, Leela Zero) to also show $\\exp_2(\\exp_2(D - O(\\log D)))$ regret.","sentences":["We improve the proofs of the lower bounds of Coquelin and Munos (2007) that demonstrate that UCT can have $\\exp(\\dots\\exp(1)\\dots)$ regret (with $\\Omega(D)$ exp terms) on the $D$-chain environment, and that a `polynomial' UCT variant has $\\exp_2(\\exp_2(D - O(\\log D)))$ regret on the same environment -- the original proofs contain an oversight for rewards bounded in $[0, 1]$, which we fix in the present draft.","We also adapt the proofs to AlphaGo's MCTS and its descendants (e.g., AlphaZero, Leela Zero) to also show $\\exp_2(\\exp_2(D - O(\\log D)))$ regret."],"url":"http://arxiv.org/abs/2405.04407v1","category":"cs.LG"}
{"created":"2024-05-07 15:30:14","title":"Vision Mamba: A Comprehensive Survey and Taxonomy","abstract":"State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems. This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning. In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding. By mapping sequence data to state space, long-term dependencies in the data can be better captured. In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity. Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference. Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer. Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain. To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study. This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains. Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.","sentences":["State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems.","This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning.","In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding.","By mapping sequence data to state space, long-term dependencies in the data can be better captured.","In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity.","Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference.","Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer.","Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain.","To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study.","This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains.","Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy."],"url":"http://arxiv.org/abs/2405.04404v1","category":"cs.CV"}
{"created":"2024-05-07 15:18:10","title":"PACIFISTA: Conflict Evaluation and Management in Open RAN","abstract":"The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles. In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps. RICs enable for the first time truly intelligent and self-organizing cellular networks. However, enabling the execution of many Artificial Intelligence (AI) algorithms taking autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges. For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies. Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system. This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts. PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity. Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA's ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems. We demonstrate that even O-RAN applications with similar goals can result in 16% throughput loss, and show how applications with conflicting goals might cause severe instability and result in up to 30% performance degradation. We also show that PACIFISTA can help operators to identify coexisting applications and maintain performance degradation below a tolerable threshold.","sentences":["The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles.","In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps.","RICs enable for the first time truly intelligent and self-organizing cellular networks.","However, enabling the execution of many Artificial Intelligence (AI) algorithms taking autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges.","For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies.","Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system.","This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts.","PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity.","Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA's ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems.","We demonstrate that even O-RAN applications with similar goals can result in 16% throughput loss, and show how applications with conflicting goals might cause severe instability and result in up to 30% performance degradation.","We also show that PACIFISTA can help operators to identify coexisting applications and maintain performance degradation below a tolerable threshold."],"url":"http://arxiv.org/abs/2405.04395v1","category":"cs.NI"}
{"created":"2024-05-07 15:11:42","title":"Pragmatist Intelligence: Where the Principle of Usefulness Can Take ANNs","abstract":"Artificial neural networks (ANNs) perform extraordinarily on numerous tasks including classification or prediction, e.g., speech processing and image classification. These new functions are based on a computational model that is enabled to select freely all necessary internal model parameters as long as it eventually delivers the functionality it is supposed to exhibit. Here, we review the connection between the model parameter selection in machine learning (ML) algorithms running on ANNs and the epistemological theory of neopragmatism focusing on the theory's utility and anti-representationalist aspects. To understand the consequences of the model parameter selection of an ANN, we suggest using neopragmatist theories whose implications are well studied. Incidentally, neopragmatism's notion of optimization is also based on utility considerations. This means that applying this approach elegantly reveals the inherent connections between optimization in ML, using a numerical method during the learning phase, and optimization in the ethical theory of consequentialism, where it occurs as a maxim of action. We suggest that these connections originate from the way relevance is calculated in ML systems. This could ultimately reveal a tendency for specific actions in ML systems.","sentences":["Artificial neural networks (ANNs) perform extraordinarily on numerous tasks including classification or prediction, e.g., speech processing and image classification.","These new functions are based on a computational model that is enabled to select freely all necessary internal model parameters as long as it eventually delivers the functionality it is supposed to exhibit.","Here, we review the connection between the model parameter selection in machine learning (ML) algorithms running on ANNs and the epistemological theory of neopragmatism focusing on the theory's utility and anti-representationalist aspects.","To understand the consequences of the model parameter selection of an ANN, we suggest using neopragmatist theories whose implications are well studied.","Incidentally, neopragmatism's notion of optimization is also based on utility considerations.","This means that applying this approach elegantly reveals the inherent connections between optimization in ML, using a numerical method during the learning phase, and optimization in the ethical theory of consequentialism, where it occurs as a maxim of action.","We suggest that these connections originate from the way relevance is calculated in ML systems.","This could ultimately reveal a tendency for specific actions in ML systems."],"url":"http://arxiv.org/abs/2405.04386v1","category":"cs.AI"}
{"created":"2024-05-07 14:57:24","title":"Leveraging LSTM and GAN for Modern Malware Detection","abstract":"The malware booming is a cyberspace equal to the effect of climate change to ecosystems in terms of danger. In the case of significant investments in cybersecurity technologies and staff training, the global community has become locked up in the eternal war with cyber security threats. The multi-form and changing faces of malware are continuously pushing the boundaries of the cybersecurity practitioners employ various approaches like detection and mitigate in coping with this issue. Some old mannerisms like signature-based detection and behavioral analysis are slow to adapt to the speedy evolution of malware types. Consequently, this paper proposes the utilization of the Deep Learning Model, LSTM networks, and GANs to amplify malware detection accuracy and speed. A fast-growing, state-of-the-art technology that leverages raw bytestream-based data and deep learning architectures, the AI technology provides better accuracy and performance than the traditional methods. Integration of LSTM and GAN model is the technique that is used for the synthetic generation of data, leading to the expansion of the training datasets, and as a result, the detection accuracy is improved. The paper uses the VirusShare dataset which has more than one million unique samples of the malware as the training and evaluation set for the presented models. Through thorough data preparation including tokenization, augmentation, as well as model training, the LSTM and GAN models convey the better performance in the tasks compared to straight classifiers. The research outcomes come out with 98% accuracy that shows the efficiency of deep learning plays a decisive role in proactive cybersecurity defense. Aside from that, the paper studies the output of ensemble learning and model fusion methods as a way to reduce biases and lift model complexity.","sentences":["The malware booming is a cyberspace equal to the effect of climate change to ecosystems in terms of danger.","In the case of significant investments in cybersecurity technologies and staff training, the global community has become locked up in the eternal war with cyber security threats.","The multi-form and changing faces of malware are continuously pushing the boundaries of the cybersecurity practitioners employ various approaches like detection and mitigate in coping with this issue.","Some old mannerisms like signature-based detection and behavioral analysis are slow to adapt to the speedy evolution of malware types.","Consequently, this paper proposes the utilization of the Deep Learning Model, LSTM networks, and GANs to amplify malware detection accuracy and speed.","A fast-growing, state-of-the-art technology that leverages raw bytestream-based data and deep learning architectures, the AI technology provides better accuracy and performance than the traditional methods.","Integration of LSTM and GAN model is the technique that is used for the synthetic generation of data, leading to the expansion of the training datasets, and as a result, the detection accuracy is improved.","The paper uses the VirusShare dataset which has more than one million unique samples of the malware as the training and evaluation set for the presented models.","Through thorough data preparation including tokenization, augmentation, as well as model training, the LSTM and GAN models convey the better performance in the tasks compared to straight classifiers.","The research outcomes come out with 98% accuracy that shows the efficiency of deep learning plays a decisive role in proactive cybersecurity defense.","Aside from that, the paper studies the output of ensemble learning and model fusion methods as a way to reduce biases and lift model complexity."],"url":"http://arxiv.org/abs/2405.04373v1","category":"cs.CR"}
{"created":"2024-05-07 14:55:42","title":"Explainable machine learning for predicting shellfish toxicity in the Adriatic Sea using long-term monitoring data of HABs","abstract":"In this study, explainable machine learning techniques are applied to predict the toxicity of mussels in the Gulf of Trieste (Adriatic Sea) caused by harmful algal blooms. By analysing a newly created 28-year dataset containing records of toxic phytoplankton in mussel farming areas and toxin concentrations in mussels (Mytilus galloprovincialis), we train and evaluate the performance of ML models to accurately predict diarrhetic shellfish poisoning (DSP) events. The random forest model provided the best prediction of positive toxicity results based on the F1 score. Explainability methods such as permutation importance and SHAP identified key species (Dinophysis fortii and D. caudata) and environmental factors (salinity, river discharge and precipitation) as the best predictors of DSP outbreaks. These findings are important for improving early warning systems and supporting sustainable aquaculture practices.","sentences":["In this study, explainable machine learning techniques are applied to predict the toxicity of mussels in the Gulf of Trieste (Adriatic Sea) caused by harmful algal blooms.","By analysing a newly created 28-year dataset containing records of toxic phytoplankton in mussel farming areas and toxin concentrations in mussels (Mytilus galloprovincialis), we train and evaluate the performance of ML models to accurately predict diarrhetic shellfish poisoning (DSP) events.","The random forest model provided the best prediction of positive toxicity results based on the F1 score.","Explainability methods such as permutation importance and SHAP identified key species (Dinophysis fortii and D. caudata) and environmental factors (salinity, river discharge and precipitation) as the best predictors of DSP outbreaks.","These findings are important for improving early warning systems and supporting sustainable aquaculture practices."],"url":"http://arxiv.org/abs/2405.04372v1","category":"cs.LG"}
{"created":"2024-05-07 14:54:32","title":"Inferring Discussion Topics about Exploitation of Vulnerabilities from Underground Hacking Forums","abstract":"The increasing sophistication of cyber threats necessitates proactive measures to identify vulnerabilities and potential exploits. Underground hacking forums serve as breeding grounds for the exchange of hacking techniques and discussions related to exploitation. In this research, we propose an innovative approach using topic modeling to analyze and uncover key themes in vulnerabilities discussed within these forums. The objective of our study is to develop a machine learning-based model that can automatically detect and classify vulnerability-related discussions in underground hacking forums. By monitoring and analyzing the content of these forums, we aim to identify emerging vulnerabilities, exploit techniques, and potential threat actors. To achieve this, we collect a large-scale dataset consisting of posts and threads from multiple underground forums. We preprocess and clean the data to ensure accuracy and reliability. Leveraging topic modeling techniques, specifically Latent Dirichlet Allocation (LDA), we uncover latent topics and their associated keywords within the dataset. This enables us to identify recurring themes and prevalent discussions related to vulnerabilities, exploits, and potential targets.","sentences":["The increasing sophistication of cyber threats necessitates proactive measures to identify vulnerabilities and potential exploits.","Underground hacking forums serve as breeding grounds for the exchange of hacking techniques and discussions related to exploitation.","In this research, we propose an innovative approach using topic modeling to analyze and uncover key themes in vulnerabilities discussed within these forums.","The objective of our study is to develop a machine learning-based model that can automatically detect and classify vulnerability-related discussions in underground hacking forums.","By monitoring and analyzing the content of these forums, we aim to identify emerging vulnerabilities, exploit techniques, and potential threat actors.","To achieve this, we collect a large-scale dataset consisting of posts and threads from multiple underground forums.","We preprocess and clean the data to ensure accuracy and reliability.","Leveraging topic modeling techniques, specifically Latent Dirichlet Allocation (LDA), we uncover latent topics and their associated keywords within the dataset.","This enables us to identify recurring themes and prevalent discussions related to vulnerabilities, exploits, and potential targets."],"url":"http://arxiv.org/abs/2405.04561v1","category":"cs.CR"}
{"created":"2024-05-07 14:52:34","title":"Community Detection for Heterogeneous Multiple Social Networks","abstract":"The community plays a crucial role in understanding user behavior and network characteristics in social networks. Some users can use multiple social networks at once for a variety of objectives. These users are called overlapping users who bridge different social networks. Detecting communities across multiple social networks is vital for interaction mining, information diffusion, and behavior migration analysis among networks. This paper presents a community detection method based on nonnegative matrix tri-factorization for multiple heterogeneous social networks, which formulates a common consensus matrix to represent the global fused community. Specifically, the proposed method involves creating adjacency matrices based on network structure and content similarity, followed by alignment matrices which distinguish overlapping users in different social networks. With the generated alignment matrices, the method could enhance the fusion degree of the global community by detecting overlapping user communities across networks. The effectiveness of the proposed method is evaluated with new metrics on Twitter, Instagram, and Tumblr datasets. The results of the experiments demonstrate its superior performance in terms of community quality and community fusion.","sentences":["The community plays a crucial role in understanding user behavior and network characteristics in social networks.","Some users can use multiple social networks at once for a variety of objectives.","These users are called overlapping users who bridge different social networks.","Detecting communities across multiple social networks is vital for interaction mining, information diffusion, and behavior migration analysis among networks.","This paper presents a community detection method based on nonnegative matrix tri-factorization for multiple heterogeneous social networks, which formulates a common consensus matrix to represent the global fused community.","Specifically, the proposed method involves creating adjacency matrices based on network structure and content similarity, followed by alignment matrices which distinguish overlapping users in different social networks.","With the generated alignment matrices, the method could enhance the fusion degree of the global community by detecting overlapping user communities across networks.","The effectiveness of the proposed method is evaluated with new metrics on Twitter, Instagram, and Tumblr datasets.","The results of the experiments demonstrate its superior performance in terms of community quality and community fusion."],"url":"http://arxiv.org/abs/2405.04371v1","category":"cs.SI"}
{"created":"2024-05-07 14:33:45","title":"Global Scale Self-Supervised Channel Charting with Sensor Fusion","abstract":"The sensing and positioning capabilities foreseen in 6G have great potential for technology advancements in various domains, such as future smart cities and industrial use cases. Channel charting has emerged as a promising technology in recent years for radio frequency-based sensing and localization. However, the accuracy of these techniques is yet far behind the numbers envisioned in 6G. To reduce this gap, in this paper, we propose a novel channel charting technique capitalizing on the time of arrival measurements from surrounding Transmission Reception Points (TRPs) along with their locations and leveraging sensor fusion in channel charting by incorporating laser scanner data during the training phase of our algorithm. The proposed algorithm remains self-supervised during training and test phases, requiring no geometrical models or user position ground truth. Simulation results validate the achievement of a sub-meter level localization accuracy using our algorithm 90% of the time, outperforming the state-of-the-art channel charting techniques and the traditional triangulation-based approaches.","sentences":["The sensing and positioning capabilities foreseen in 6G have great potential for technology advancements in various domains, such as future smart cities and industrial use cases.","Channel charting has emerged as a promising technology in recent years for radio frequency-based sensing and localization.","However, the accuracy of these techniques is yet far behind the numbers envisioned in 6G. To reduce this gap, in this paper, we propose a novel channel charting technique capitalizing on the time of arrival measurements from surrounding Transmission Reception Points (TRPs) along with their locations and leveraging sensor fusion in channel charting by incorporating laser scanner data during the training phase of our algorithm.","The proposed algorithm remains self-supervised during training and test phases, requiring no geometrical models or user position ground truth.","Simulation results validate the achievement of a sub-meter level localization accuracy using our algorithm 90% of the time, outperforming the state-of-the-art channel charting techniques and the traditional triangulation-based approaches."],"url":"http://arxiv.org/abs/2405.04357v1","category":"cs.IT"}
{"created":"2024-05-07 14:23:22","title":"Revisiting character-level adversarial attacks","abstract":"Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples. While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend. Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples. Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in 4.84% points and the USE similarity in 8% points with respect to the previous art. Our implementation is available in https://github.com/LIONS-EPFL/Charmer.","sentences":["Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels.","Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples.","While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend.","Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples.","Our method successfully targets both small (BERT) and large (Llama 2) models.","Specifically, on BERT with SST-2, Charmer improves the ASR in 4.84% points and the USE similarity in 8% points with respect to the previous art.","Our implementation is available in https://github.com/LIONS-EPFL/Charmer."],"url":"http://arxiv.org/abs/2405.04346v1","category":"cs.LG"}
{"created":"2024-05-07 14:22:32","title":"Novel View Synthesis with Neural Radiance Fields for Industrial Robot Applications","abstract":"Neural Radiance Fields (NeRFs) have become a rapidly growing research field with the potential to revolutionize typical photogrammetric workflows, such as those used for 3D scene reconstruction. As input, NeRFs require multi-view images with corresponding camera poses as well as the interior orientation. In the typical NeRF workflow, the camera poses and the interior orientation are estimated in advance with Structure from Motion (SfM). But the quality of the resulting novel views, which depends on different parameters such as the number and distribution of available images, as well as the accuracy of the related camera poses and interior orientation, is difficult to predict. In addition, SfM is a time-consuming pre-processing step, and its quality strongly depends on the image content. Furthermore, the undefined scaling factor of SfM hinders subsequent steps in which metric information is required. In this paper, we evaluate the potential of NeRFs for industrial robot applications. We propose an alternative to SfM pre-processing: we capture the input images with a calibrated camera that is attached to the end effector of an industrial robot and determine accurate camera poses with metric scale based on the robot kinematics. We then investigate the quality of the novel views by comparing them to ground truth, and by computing an internal quality measure based on ensemble methods. For evaluation purposes, we acquire multiple datasets that pose challenges for reconstruction typical of industrial applications, like reflective objects, poor texture, and fine structures. We show that the robot-based pose determination reaches similar accuracy as SfM in non-demanding cases, while having clear advantages in more challenging scenarios. Finally, we present first results of applying the ensemble method to estimate the quality of the synthetic novel view in the absence of a ground truth.","sentences":["Neural Radiance Fields (NeRFs) have become a rapidly growing research field with the potential to revolutionize typical photogrammetric workflows, such as those used for 3D scene reconstruction.","As input, NeRFs require multi-view images with corresponding camera poses as well as the interior orientation.","In the typical NeRF workflow, the camera poses and the interior orientation are estimated in advance with Structure from Motion (SfM).","But the quality of the resulting novel views, which depends on different parameters such as the number and distribution of available images, as well as the accuracy of the related camera poses and interior orientation, is difficult to predict.","In addition, SfM is a time-consuming pre-processing step, and its quality strongly depends on the image content.","Furthermore, the undefined scaling factor of SfM hinders subsequent steps in which metric information is required.","In this paper, we evaluate the potential of NeRFs for industrial robot applications.","We propose an alternative to SfM pre-processing: we capture the input images with a calibrated camera that is attached to the end effector of an industrial robot and determine accurate camera poses with metric scale based on the robot kinematics.","We then investigate the quality of the novel views by comparing them to ground truth, and by computing an internal quality measure based on ensemble methods.","For evaluation purposes, we acquire multiple datasets that pose challenges for reconstruction typical of industrial applications, like reflective objects, poor texture, and fine structures.","We show that the robot-based pose determination reaches similar accuracy as SfM in non-demanding cases, while having clear advantages in more challenging scenarios.","Finally, we present first results of applying the ensemble method to estimate the quality of the synthetic novel view in the absence of a ground truth."],"url":"http://arxiv.org/abs/2405.04345v1","category":"cs.CV"}
{"created":"2024-05-07 14:19:09","title":"Enhancing Scalability of Metric Differential Privacy via Secret Dataset Partitioning and Benders Decomposition","abstract":"Metric Differential Privacy (mDP) extends the concept of Differential Privacy (DP) to serve as a new paradigm of data perturbation. It is designed to protect secret data represented in general metric space, such as text data encoded as word embeddings or geo-location data on the road network or grid maps. To derive an optimal data perturbation mechanism under mDP, a widely used method is linear programming (LP), which, however, might suffer from a polynomial explosion of decision variables, rendering it impractical in large-scale mDP.   In this paper, our objective is to develop a new computation framework to enhance the scalability of the LP-based mDP. Considering the connections established by the mDP constraints among the secret records, we partition the original secret dataset into various subsets. Building upon the partition, we reformulate the LP problem for mDP and solve it via Benders Decomposition, which is composed of two stages: (1) a master program to manage the perturbation calculation across subsets and (2) a set of subproblems, each managing the perturbation derivation within a subset. Our experimental results on multiple datasets, including geo-location data in the road network/grid maps, text data, and synthetic data, underscore our proposed mechanism's superior scalability and efficiency.","sentences":["Metric Differential Privacy (mDP) extends the concept of Differential Privacy (DP) to serve as a new paradigm of data perturbation.","It is designed to protect secret data represented in general metric space, such as text data encoded as word embeddings or geo-location data on the road network or grid maps.","To derive an optimal data perturbation mechanism under mDP, a widely used method is linear programming (LP), which, however, might suffer from a polynomial explosion of decision variables, rendering it impractical in large-scale mDP.   ","In this paper, our objective is to develop a new computation framework to enhance the scalability of the LP-based mDP.","Considering the connections established by the mDP constraints among the secret records, we partition the original secret dataset into various subsets.","Building upon the partition, we reformulate the LP problem for mDP and solve it via Benders Decomposition, which is composed of two stages: (1) a master program to manage the perturbation calculation across subsets and (2) a set of subproblems, each managing the perturbation derivation within a subset.","Our experimental results on multiple datasets, including geo-location data in the road network/grid maps, text data, and synthetic data, underscore our proposed mechanism's superior scalability and efficiency."],"url":"http://arxiv.org/abs/2405.04344v1","category":"cs.AI"}
{"created":"2024-05-07 14:08:57","title":"Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction","abstract":"Predicting Remaining Useful Life (RUL) plays a crucial role in the prognostics and health management of industrial systems that involve a variety of interrelated sensors. Given a constant stream of time series sensory data from such systems, deep learning models have risen to prominence at identifying complex, nonlinear temporal dependencies in these data. In addition to the temporal dependencies of individual sensors, spatial dependencies emerge as important correlations among these sensors, which can be naturally modelled by a temporal graph that describes time-varying spatial relationships. However, the majority of existing studies have relied on capturing discrete snapshots of this temporal graph, a coarse-grained approach that leads to loss of temporal information. Moreover, given the variety of heterogeneous sensors, it becomes vital that such inherent heterogeneity is leveraged for RUL prediction in temporal sensor graphs. To capture the nuances of the temporal and spatial relationships and heterogeneous characteristics in an interconnected graph of sensors, we introduce a novel model named Temporal and Heterogeneous Graph Neural Networks (THGNN). Specifically, THGNN aggregates historical data from neighboring nodes to accurately capture the temporal dynamics and spatial correlations within the stream of sensor data in a fine-grained manner. Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address the diversity of sensor types, significantly improving the model's capacity to learn the heterogeneity in the data sources. Finally, we have validated the effectiveness of our approach through comprehensive experiments. Our empirical findings demonstrate significant advancements on the N-CMAPSS dataset, achieving improvements of up to 19.2% and 31.6% in terms of two different evaluation metrics over state-of-the-art methods.","sentences":["Predicting Remaining Useful Life (RUL) plays a crucial role in the prognostics and health management of industrial systems that involve a variety of interrelated sensors.","Given a constant stream of time series sensory data from such systems, deep learning models have risen to prominence at identifying complex, nonlinear temporal dependencies in these data.","In addition to the temporal dependencies of individual sensors, spatial dependencies emerge as important correlations among these sensors, which can be naturally modelled by a temporal graph that describes time-varying spatial relationships.","However, the majority of existing studies have relied on capturing discrete snapshots of this temporal graph, a coarse-grained approach that leads to loss of temporal information.","Moreover, given the variety of heterogeneous sensors, it becomes vital that such inherent heterogeneity is leveraged for RUL prediction in temporal sensor graphs.","To capture the nuances of the temporal and spatial relationships and heterogeneous characteristics in an interconnected graph of sensors, we introduce a novel model named Temporal and Heterogeneous Graph Neural Networks (THGNN).","Specifically, THGNN aggregates historical data from neighboring nodes to accurately capture the temporal dynamics and spatial correlations within the stream of sensor data in a fine-grained manner.","Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address the diversity of sensor types, significantly improving the model's capacity to learn the heterogeneity in the data sources.","Finally, we have validated the effectiveness of our approach through comprehensive experiments.","Our empirical findings demonstrate significant advancements on the N-CMAPSS dataset, achieving improvements of up to 19.2% and 31.6% in terms of two different evaluation metrics over state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04336v1","category":"cs.AI"}
{"created":"2024-05-07 14:01:33","title":"A Fourth Wave of Open Data? Exploring the Spectrum of Scenarios for Open Data and Generative AI","abstract":"Since late 2022, generative AI has taken the world by storm, with widespread use of tools including ChatGPT, Gemini, and Claude. Generative AI and large language model (LLM) applications are transforming how individuals find and access data and knowledge. However, the intricate relationship between open data and generative AI, and the vast potential it holds for driving innovation in this field remain underexplored areas. This white paper seeks to unpack the relationship between open data and generative AI and explore possible components of a new Fourth Wave of Open Data: Is open data becoming AI ready? Is open data moving towards a data commons approach? Is generative AI making open data more conversational? Will generative AI improve open data quality and provenance? Towards this end, we provide a new Spectrum of Scenarios framework. This framework outlines a range of scenarios in which open data and generative AI could intersect and what is required from a data quality and provenance perspective to make open data ready for those specific scenarios. These scenarios include: pertaining, adaptation, inference and insight generation, data augmentation, and open-ended exploration. Through this process, we found that in order for data holders to embrace generative AI to improve open data access and develop greater insights from open data, they first must make progress around five key areas: enhance transparency and documentation, uphold quality and integrity, promote interoperability and standards, improve accessibility and useability, and address ethical considerations.","sentences":["Since late 2022, generative AI has taken the world by storm, with widespread use of tools including ChatGPT, Gemini, and Claude.","Generative AI and large language model (LLM) applications are transforming how individuals find and access data and knowledge.","However, the intricate relationship between open data and generative AI, and the vast potential it holds for driving innovation in this field remain underexplored areas.","This white paper seeks to unpack the relationship between open data and generative AI and explore possible components of a new Fourth Wave of Open Data: Is open data becoming AI ready?","Is open data moving towards a data commons approach?","Is generative AI making open data more conversational?","Will generative AI improve open data quality and provenance?","Towards this end, we provide a new Spectrum of Scenarios framework.","This framework outlines a range of scenarios in which open data and generative AI could intersect and what is required from a data quality and provenance perspective to make open data ready for those specific scenarios.","These scenarios include: pertaining, adaptation, inference and insight generation, data augmentation, and open-ended exploration.","Through this process, we found that in order for data holders to embrace generative AI to improve open data access and develop greater insights from open data, they first must make progress around five key areas: enhance transparency and documentation, uphold quality and integrity, promote interoperability and standards, improve accessibility and useability, and address ethical considerations."],"url":"http://arxiv.org/abs/2405.04333v1","category":"cs.AI"}
{"created":"2024-05-07 13:50:40","title":"Granite Code Models: A Family of Open Foundation Models for Code Intelligence","abstract":"Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.","sentences":["Large Language Models (LLMs) trained on code are revolutionizing the software development process.","Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously.","Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more.","In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages.","The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases.","Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs.","The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model.","We release all our Granite Code models under an Apache 2.0 license for both research and commercial use."],"url":"http://arxiv.org/abs/2405.04324v1","category":"cs.AI"}
{"created":"2024-05-07 13:49:59","title":"Beyond human subjectivity and error: a novel AI grading system","abstract":"The grading of open-ended questions is a high-effort, high-impact task in education. Automating this task promises a significant reduction in workload for education professionals, as well as more consistent grading outcomes for students, by circumventing human subjectivity and error. While recent breakthroughs in AI technology might facilitate such automation, this has not been demonstrated at scale. It this paper, we introduce a novel automatic short answer grading (ASAG) system. The system is based on a fine-tuned open-source transformer model which we trained on large set of exam data from university courses across a large range of disciplines. We evaluated the trained model's performance against held-out test data in a first experiment and found high accuracy levels across a broad spectrum of unseen questions, even in unseen courses. We further compared the performance of our model with that of certified human domain experts in a second experiment: we first assembled another test dataset from real historical exams - the historic grades contained in that data were awarded to students in a regulated, legally binding examination process; we therefore considered them as ground truth for our experiment. We then asked certified human domain experts and our model to grade the historic student answers again without disclosing the historic grades. Finally, we compared the hence obtained grades with the historic grades (our ground truth). We found that for the courses examined, the model deviated less from the official historic grades than the human re-graders - the model's median absolute error was 44 % smaller than the human re-graders', implying that the model is more consistent than humans in grading. These results suggest that leveraging AI enhanced grading can reduce human subjectivity, improve consistency and thus ultimately increase fairness.","sentences":["The grading of open-ended questions is a high-effort, high-impact task in education.","Automating this task promises a significant reduction in workload for education professionals, as well as more consistent grading outcomes for students, by circumventing human subjectivity and error.","While recent breakthroughs in AI technology might facilitate such automation, this has not been demonstrated at scale.","It this paper, we introduce a novel automatic short answer grading (ASAG) system.","The system is based on a fine-tuned open-source transformer model which we trained on large set of exam data from university courses across a large range of disciplines.","We evaluated the trained model's performance against held-out test data in a first experiment and found high accuracy levels across a broad spectrum of unseen questions, even in unseen courses.","We further compared the performance of our model with that of certified human domain experts in a second experiment: we first assembled another test dataset from real historical exams - the historic grades contained in that data were awarded to students in a regulated, legally binding examination process; we therefore considered them as ground truth for our experiment.","We then asked certified human domain experts and our model to grade the historic student answers again without disclosing the historic grades.","Finally, we compared the hence obtained grades with the historic grades (our ground truth).","We found that for the courses examined, the model deviated less from the official historic grades than the human re-graders - the model's median absolute error was 44 % smaller than the human re-graders', implying that the model is more consistent than humans in grading.","These results suggest that leveraging AI enhanced grading can reduce human subjectivity, improve consistency and thus ultimately increase fairness."],"url":"http://arxiv.org/abs/2405.04323v1","category":"cs.AI"}
{"created":"2024-05-07 13:35:51","title":"Cross-IQA: Unsupervised Learning for Image Quality Assessment","abstract":"Automatic perception of image quality is a challenging problem that impacts billions of Internet and social media users daily. To advance research in this field, we propose a no-reference image quality assessment (NR-IQA) method termed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA method can learn image quality features from unlabeled image data. We construct the pretext task of synthesized image reconstruction to unsupervised extract the image quality information based ViT block. The pretrained encoder of Cross-IQA is used to fine-tune a linear regression model for score prediction. Experimental results show that Cross-IQA can achieve state-of-the-art performance in assessing the low-frequency degradation information (e.g., color change, blurring, etc.) of images compared with the classical full-reference IQA and NR-IQA under the same datasets.","sentences":["Automatic perception of image quality is a challenging problem that impacts billions of Internet and social media users daily.","To advance research in this field, we propose a no-reference image quality assessment (NR-IQA) method termed Cross-IQA based on vision transformer(ViT) model.","The proposed Cross-IQA method can learn image quality features from unlabeled image data.","We construct the pretext task of synthesized image reconstruction to unsupervised extract the image quality information based ViT block.","The pretrained encoder of Cross-IQA is used to fine-tune a linear regression model for score prediction.","Experimental results show that Cross-IQA can achieve state-of-the-art performance in assessing the low-frequency degradation information (e.g., color change, blurring, etc.) of images compared with the classical full-reference IQA and NR-IQA under the same datasets."],"url":"http://arxiv.org/abs/2405.04311v1","category":"cs.CV"}
{"created":"2024-05-07 13:29:41","title":"Improving Offline Reinforcement Learning with Inaccurate Simulators","abstract":"Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment. However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process. In many robotic applications, an inaccurate simulator is often available. However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment. To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner. Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset. Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator. Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods.","sentences":["Offline reinforcement learning (RL) provides a promising approach to avoid costly online interaction with the real environment.","However, the performance of offline RL highly depends on the quality of the datasets, which may cause extrapolation error in the learning process.","In many robotic applications, an inaccurate simulator is often available.","However, the data directly collected from the inaccurate simulator cannot be directly used in offline RL due to the well-known exploration-exploitation dilemma and the dynamic gap between inaccurate simulation and the real environment.","To address these issues, we propose a novel approach to combine the offline dataset and the inaccurate simulation data in a better manner.","Specifically, we pre-train a generative adversarial network (GAN) model to fit the state distribution of the offline dataset.","Given this, we collect data from the inaccurate simulator starting from the distribution provided by the generator and reweight the simulated data using the discriminator.","Our experimental results in the D4RL benchmark and a real-world manipulation task confirm that our method can benefit more from both inaccurate simulator and limited offline datasets to achieve better performance than the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04307v1","category":"cs.RO"}
{"created":"2024-05-07 13:27:58","title":"A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields","abstract":"Aphid infestations are one of the primary causes of extensive damage to wheat and sorghum fields and are one of the most common vectors for plant viruses, resulting in significant agricultural yield losses. To address this problem, farmers often employ the inefficient use of harmful chemical pesticides that have negative health and environmental impacts. As a result, a large amount of pesticide is wasted on areas without significant pest infestation. This brings to attention the urgent need for an intelligent autonomous system that can locate and spray sufficiently large infestations selectively within the complex crop canopies. We have developed a large multi-scale dataset for aphid cluster detection and segmentation, collected from actual sorghum fields and meticulously annotated to include clusters of aphids. Our dataset comprises a total of 54,742 image patches, showcasing a variety of viewpoints, diverse lighting conditions, and multiple scales, highlighting its effectiveness for real-world applications. In this study, we trained and evaluated four real-time semantic segmentation models and three object detection models specifically for aphid cluster segmentation and detection. Considering the balance between accuracy and efficiency, Fast-SCNN delivered the most effective segmentation results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames per second (FPS). For object detection, RT-DETR exhibited the best overall performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and 72.55 on an NVIDIA V100 GPU. Our experiments further indicate that aphid cluster segmentation is more suitable for assessing aphid infestations than using detection models.","sentences":["Aphid infestations are one of the primary causes of extensive damage to wheat and sorghum fields and are one of the most common vectors for plant viruses, resulting in significant agricultural yield losses.","To address this problem, farmers often employ the inefficient use of harmful chemical pesticides that have negative health and environmental impacts.","As a result, a large amount of pesticide is wasted on areas without significant pest infestation.","This brings to attention the urgent need for an intelligent autonomous system that can locate and spray sufficiently large infestations selectively within the complex crop canopies.","We have developed a large multi-scale dataset for aphid cluster detection and segmentation, collected from actual sorghum fields and meticulously annotated to include clusters of aphids.","Our dataset comprises a total of 54,742 image patches, showcasing a variety of viewpoints, diverse lighting conditions, and multiple scales, highlighting its effectiveness for real-world applications.","In this study, we trained and evaluated four real-time semantic segmentation models and three object detection models specifically for aphid cluster segmentation and detection.","Considering the balance between accuracy and efficiency, Fast-SCNN delivered the most effective segmentation results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames per second (FPS).","For object detection, RT-DETR exhibited the best overall performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and 72.55 on an NVIDIA V100 GPU.","Our experiments further indicate that aphid cluster segmentation is more suitable for assessing aphid infestations than using detection models."],"url":"http://arxiv.org/abs/2405.04305v1","category":"cs.CV"}
{"created":"2024-05-07 13:18:22","title":"Behaviour Planning: A Toolkit for Diverse Planning","abstract":"Diverse planning is the problem of generating plans with distinct characteristics. This is valuable for many real-world scenarios, including applications related to plan recognition and business process automation. In this work, we introduce \\emph{Behaviour Planning}, a diverse planning toolkit that can characterise and generate diverse plans based on modular diversity models. We present a qualitative framework for describing diversity models, a planning approach for generating plans aligned with any given diversity model, and provide a practical implementation of an SMT-based behaviour planner. We showcase how the qualitative approach offered by Behaviour Planning allows it to overcome various challenges faced by previous approaches. Finally, the experimental evaluation shows the effectiveness of Behaviour Planning in generating diverse plans compared to state-of-the-art approaches.","sentences":["Diverse planning is the problem of generating plans with distinct characteristics.","This is valuable for many real-world scenarios, including applications related to plan recognition and business process automation.","In this work, we introduce \\emph{Behaviour Planning}, a diverse planning toolkit that can characterise and generate diverse plans based on modular diversity models.","We present a qualitative framework for describing diversity models, a planning approach for generating plans aligned with any given diversity model, and provide a practical implementation of an SMT-based behaviour planner.","We showcase how the qualitative approach offered by Behaviour Planning allows it to overcome various challenges faced by previous approaches.","Finally, the experimental evaluation shows the effectiveness of Behaviour Planning in generating diverse plans compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.04300v1","category":"cs.AI"}
{"created":"2024-05-07 13:09:49","title":"Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework","abstract":"Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges. This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance. Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively. While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative. Dual-agent systems further increase accuracy, though this comes with higher operational costs. This research highlights AI's potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management.","sentences":["Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges.","This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance.","Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively.","While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative.","Dual-agent systems further increase accuracy, though this comes with higher operational costs.","This research highlights AI's potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management."],"url":"http://arxiv.org/abs/2405.04294v1","category":"cs.AI"}
{"created":"2024-05-07 13:09:25","title":"Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask Learning","abstract":"This study introduces 'clickbait spoiling', a novel technique designed to detect, categorize, and generate spoilers as succinct text responses, countering the curiosity induced by clickbait content. By leveraging a multi-task learning framework, our model's generalization capabilities are significantly enhanced, effectively addressing the pervasive issue of clickbait. The crux of our research lies in generating appropriate spoilers, be it a phrase, an extended passage, or multiple, depending on the spoiler type required. Our methodology integrates two crucial techniques: a refined spoiler categorization method and a modified version of the Question Answering (QA) mechanism, incorporated within a multi-task learning paradigm for optimized spoiler extraction from context. Notably, we have included fine-tuning methods for models capable of handling longer sequences to accommodate the generation of extended spoilers. This research highlights the potential of sophisticated text processing techniques in tackling the omnipresent issue of clickbait, promising an enhanced user experience in the digital realm.","sentences":["This study introduces 'clickbait spoiling', a novel technique designed to detect, categorize, and generate spoilers as succinct text responses, countering the curiosity induced by clickbait content.","By leveraging a multi-task learning framework, our model's generalization capabilities are significantly enhanced, effectively addressing the pervasive issue of clickbait.","The crux of our research lies in generating appropriate spoilers, be it a phrase, an extended passage, or multiple, depending on the spoiler type required.","Our methodology integrates two crucial techniques: a refined spoiler categorization method and a modified version of the Question Answering (QA) mechanism, incorporated within a multi-task learning paradigm for optimized spoiler extraction from context.","Notably, we have included fine-tuning methods for models capable of handling longer sequences to accommodate the generation of extended spoilers.","This research highlights the potential of sophisticated text processing techniques in tackling the omnipresent issue of clickbait, promising an enhanced user experience in the digital realm."],"url":"http://arxiv.org/abs/2405.04292v1","category":"cs.CL"}
{"created":"2024-05-07 13:06:14","title":"Spiral Attractors in a Reduced Mean-Field Model of Neuron-Glial Interaction","abstract":"It is well known that bursting activity plays an important role in the processes of transmission of neural signals. In terms of population dynamics, macroscopic bursting can be described using a mean-field approach. Mean field theory provides a useful tool for analysis of collective behavior of a large populations of interacting units, allowing to reduce the description of corresponding dynamics to just a few equations. Recently a new phenomenological model was proposed that describes bursting population activity of a big group of excitatory neurons, taking into account short-term synaptic plasticity and the astrocytic modulation of the synaptic dynamics [1]. The purpose of the present study is to investigate various bifurcation scenarios of the appearance of bursting activity in the phenomenological model. We show that the birth of bursting population pattern can be connected both with the cascade of period doubling bifurcations and further development of chaos according to the Shilnikov scenario, which leads to the appearance of a homoclinic attractor containing a homoclinic loop of a saddle-focus equilibrium with the two-dimensional unstable invariant manifold. We also show that the homoclinic spiral attractors observed in the system under study generate several types of bursting activity with different properties.","sentences":["It is well known that bursting activity plays an important role in the processes of transmission of neural signals.","In terms of population dynamics, macroscopic bursting can be described using a mean-field approach.","Mean field theory provides a useful tool for analysis of collective behavior of a large populations of interacting units, allowing to reduce the description of corresponding dynamics to just a few equations.","Recently a new phenomenological model was proposed that describes bursting population activity of a big group of excitatory neurons, taking into account short-term synaptic plasticity and the astrocytic modulation of the synaptic dynamics","[1].","The purpose of the present study is to investigate various bifurcation scenarios of the appearance of bursting activity in the phenomenological model.","We show that the birth of bursting population pattern can be connected both with the cascade of period doubling bifurcations and further development of chaos according to the Shilnikov scenario, which leads to the appearance of a homoclinic attractor containing a homoclinic loop of a saddle-focus equilibrium with the two-dimensional unstable invariant manifold.","We also show that the homoclinic spiral attractors observed in the system under study generate several types of bursting activity with different properties."],"url":"http://arxiv.org/abs/2405.04291v1","category":"nlin.CD"}
{"created":"2024-05-07 13:04:29","title":"Bayesian Simultaneous Localization and Multi-Lane Tracking Using Onboard Sensors and a SD Map","abstract":"High-definition map with accurate lane-level information is crucial for autonomous driving, but the creation of these maps is a resource-intensive process. To this end, we present a cost-effective solution to create lane-level roadmaps using only the global navigation satellite system (GNSS) and a camera on customer vehicles. Our proposed solution utilizes a prior standard-definition (SD) map, GNSS measurements, visual odometry, and lane marking edge detection points, to simultaneously estimate the vehicle's 6D pose, its position within a SD map, and also the 3D geometry of traffic lines. This is achieved using a Bayesian simultaneous localization and multi-object tracking filter, where the estimation of traffic lines is formulated as a multiple extended object tracking problem, solved using a trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. In TPMBM filtering, traffic lines are modeled using B-spline trajectories, and each trajectory is parameterized by a sequence of control points. The proposed solution has been evaluated using experimental data collected by a test vehicle driving on highway. Preliminary results show that the traffic line estimates, overlaid on the satellite image, generally align with the lane markings up to some lateral offsets.","sentences":["High-definition map with accurate lane-level information is crucial for autonomous driving, but the creation of these maps is a resource-intensive process.","To this end, we present a cost-effective solution to create lane-level roadmaps using only the global navigation satellite system (GNSS) and a camera on customer vehicles.","Our proposed solution utilizes a prior standard-definition (SD) map, GNSS measurements, visual odometry, and lane marking edge detection points, to simultaneously estimate the vehicle's 6D pose, its position within a SD map, and also the 3D geometry of traffic lines.","This is achieved using a Bayesian simultaneous localization and multi-object tracking filter, where the estimation of traffic lines is formulated as a multiple extended object tracking problem, solved using a trajectory Poisson multi-Bernoulli mixture (TPMBM) filter.","In TPMBM filtering, traffic lines are modeled using B-spline trajectories, and each trajectory is parameterized by a sequence of control points.","The proposed solution has been evaluated using experimental data collected by a test vehicle driving on highway.","Preliminary results show that the traffic line estimates, overlaid on the satellite image, generally align with the lane markings up to some lateral offsets."],"url":"http://arxiv.org/abs/2405.04290v1","category":"cs.RO"}
{"created":"2024-05-07 12:54:54","title":"On the Foundations of Earth and Climate Foundation Models","abstract":"Foundation models have enormous potential in advancing Earth and climate sciences, however, current approaches may not be optimal as they focus on a few basic features of a desirable Earth and climate foundation model. Crafting the ideal Earth foundation model, we define eleven features which would allow such a foundation model to be beneficial for any geoscientific downstream application in an environmental- and human-centric manner.We further shed light on the way forward to achieve the ideal model and to evaluate Earth foundation models. What comes after foundation models? Energy efficient adaptation, adversarial defenses, and interpretability are among the emerging directions.","sentences":["Foundation models have enormous potential in advancing Earth and climate sciences, however, current approaches may not be optimal as they focus on a few basic features of a desirable Earth and climate foundation model.","Crafting the ideal Earth foundation model, we define eleven features which would allow such a foundation model to be beneficial for any geoscientific downstream application in an environmental- and human-centric manner.","We further shed light on the way forward to achieve the ideal model and to evaluate Earth foundation models.","What comes after foundation models?","Energy efficient adaptation, adversarial defenses, and interpretability are among the emerging directions."],"url":"http://arxiv.org/abs/2405.04285v1","category":"cs.AI"}
{"created":"2024-05-07 12:50:28","title":"CoqPyt: Proof Navigation in Python in the Era of LLMs","abstract":"Proof assistants enable users to develop machine-checked proofs regarding software-related properties. Unfortunately, the interactive nature of these proof assistants imposes most of the proof burden on the user, making formal verification a complex, and time-consuming endeavor. Recent automation techniques based on neural methods address this issue, but require good programmatic support for collecting data and interacting with proof assistants. This paper presents CoqPyt, a Python tool for interacting with the Coq proof assistant. CoqPyt improves on other Coq-related tools by providing novel features, such as the extraction of rich premise data. We expect our work to aid development of tools and techniques, especially LLM-based, designed for proof synthesis and repair. A video describing and demonstrating CoqPyt is available at: https://youtu.be/fk74o0rePM8.","sentences":["Proof assistants enable users to develop machine-checked proofs regarding software-related properties.","Unfortunately, the interactive nature of these proof assistants imposes most of the proof burden on the user, making formal verification a complex, and time-consuming endeavor.","Recent automation techniques based on neural methods address this issue, but require good programmatic support for collecting data and interacting with proof assistants.","This paper presents CoqPyt, a Python tool for interacting with the Coq proof assistant.","CoqPyt improves on other Coq-related tools by providing novel features, such as the extraction of rich premise data.","We expect our work to aid development of tools and techniques, especially LLM-based, designed for proof synthesis and repair.","A video describing and demonstrating CoqPyt is available at: https://youtu.be/fk74o0rePM8."],"url":"http://arxiv.org/abs/2405.04282v1","category":"cs.SE"}
{"created":"2024-05-07 12:40:59","title":"Generating Feature Vectors from Phonetic Transcriptions in Cross-Linguistic Data Formats","abstract":"When comparing speech sounds across languages, scholars often make use of feature representations of individual sounds in order to determine fine-grained sound similarities. Although binary feature systems for large numbers of speech sounds have been proposed, large-scale computational applications often face the challenges that the proposed feature systems -- even if they list features for several thousand sounds -- only cover a smaller part of the numerous speech sounds reflected in actual cross-linguistic data. In order to address the problem of missing data for attested speech sounds, we propose a new approach that can create binary feature vectors dynamically for all sounds that can be represented in the the standardized version of the International Phonetic Alphabet proposed by the Cross-Linguistic Transcription Systems (CLTS) reference catalog. Since CLTS is actively used in large data collections, covering more than 2,000 distinct language varieties, our procedure for the generation of binary feature vectors provides immediate access to a very large collection of multilingual wordlists. Testing our feature system in different ways on different datasets proves that the system is not only useful to provide a straightforward means to compare the similarity of speech sounds, but also illustrates its potential to be used in future cross-linguistic machine learning applications.","sentences":["When comparing speech sounds across languages, scholars often make use of feature representations of individual sounds in order to determine fine-grained sound similarities.","Although binary feature systems for large numbers of speech sounds have been proposed, large-scale computational applications often face the challenges that the proposed feature systems -- even if they list features for several thousand sounds -- only cover a smaller part of the numerous speech sounds reflected in actual cross-linguistic data.","In order to address the problem of missing data for attested speech sounds, we propose a new approach that can create binary feature vectors dynamically for all sounds that can be represented in the the standardized version of the International Phonetic Alphabet proposed by the Cross-Linguistic Transcription Systems (CLTS) reference catalog.","Since CLTS is actively used in large data collections, covering more than 2,000 distinct language varieties, our procedure for the generation of binary feature vectors provides immediate access to a very large collection of multilingual wordlists.","Testing our feature system in different ways on different datasets proves that the system is not only useful to provide a straightforward means to compare the similarity of speech sounds, but also illustrates its potential to be used in future cross-linguistic machine learning applications."],"url":"http://arxiv.org/abs/2405.04271v1","category":"cs.CL"}
{"created":"2024-05-07 12:20:12","title":"Verified Neural Compressed Sensing","abstract":"We develop the first (to the best of our knowledge) provably correct neural networks for a precise computational task, with the proof of correctness generated by an automated verification algorithm without any human input. Prior work on neural network verification has focused on partial specifications that, even when satisfied, are not sufficient to ensure that a neural network never makes errors. We focus on applying neural network verification to computational tasks with a precise notion of correctness, where a verifiably correct neural network provably solves the task at hand with no caveats. In particular, we develop an approach to train and verify the first provably correct neural networks for compressed sensing, i.e., recovering sparse vectors from a number of measurements smaller than the dimension of the vector. We show that for modest problem dimensions (up to 50), we can train neural networks that provably recover a sparse vector from linear and binarized linear measurements. Furthermore, we show that the complexity of the network (number of neurons/layers) can be adapted to the problem difficulty and solve problems where traditional compressed sensing methods are not known to provably work.","sentences":["We develop the first (to the best of our knowledge) provably correct neural networks for a precise computational task, with the proof of correctness generated by an automated verification algorithm without any human input.","Prior work on neural network verification has focused on partial specifications that, even when satisfied, are not sufficient to ensure that a neural network never makes errors.","We focus on applying neural network verification to computational tasks with a precise notion of correctness, where a verifiably correct neural network provably solves the task at hand with no caveats.","In particular, we develop an approach to train and verify the first provably correct neural networks for compressed sensing, i.e., recovering sparse vectors from a number of measurements smaller than the dimension of the vector.","We show that for modest problem dimensions (up to 50), we can train neural networks that provably recover a sparse vector from linear and binarized linear measurements.","Furthermore, we show that the complexity of the network (number of neurons/layers) can be adapted to the problem difficulty and solve problems where traditional compressed sensing methods are not known to provably work."],"url":"http://arxiv.org/abs/2405.04260v2","category":"cs.LG"}
{"created":"2024-05-07 12:18:23","title":"A Weighted Least-Squares Method for Non-Asymptotic Identification of Markov Parameters from Multiple Trajectories","abstract":"Markov parameters play a key role in system identification. There exists many algorithms where these parameters are estimated using least-squares in a first, pre-processing, step, including subspace identification and multi-step least-squares algorithms, such as Weighted Null-Space Fitting. Recently, there has been an increasing interest in non-asymptotic analysis of estimation algorithms. In this contribution we identify the Markov parameters using weighted least-squares and present non-asymptotic analysis for such estimator. To cover both stable and unstable systems, multiple trajectories are collected. We show that with the optimal weighting matrix, weighted least-squares gives a tighter error bound than ordinary least-squares for the case of non-uniformly distributed measurement errors. Moreover, as the optimal weighting matrix depends on the system's true parameters, we introduce two methods to consistently estimate the optimal weighting matrix, where the convergence rate of these estimates is also provided. Numerical experiments demonstrate improvements of weighted least-squares over ordinary least-squares in finite sample settings.","sentences":["Markov parameters play a key role in system identification.","There exists many algorithms where these parameters are estimated using least-squares in a first, pre-processing, step, including subspace identification and multi-step least-squares algorithms, such as Weighted Null-Space Fitting.","Recently, there has been an increasing interest in non-asymptotic analysis of estimation algorithms.","In this contribution we identify the Markov parameters using weighted least-squares and present non-asymptotic analysis for such estimator.","To cover both stable and unstable systems, multiple trajectories are collected.","We show that with the optimal weighting matrix, weighted least-squares gives a tighter error bound than ordinary least-squares for the case of non-uniformly distributed measurement errors.","Moreover, as the optimal weighting matrix depends on the system's true parameters, we introduce two methods to consistently estimate the optimal weighting matrix, where the convergence rate of these estimates is also provided.","Numerical experiments demonstrate improvements of weighted least-squares over ordinary least-squares in finite sample settings."],"url":"http://arxiv.org/abs/2405.04258v1","category":"eess.SY"}
{"created":"2024-05-07 12:13:11","title":"VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting","abstract":"This paper presents VAEneu, an innovative autoregressive method for multistep ahead univariate probabilistic time series forecasting. We employ the conditional VAE framework and optimize the lower bound of the predictive distribution likelihood function by adopting the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function. This novel pipeline results in forecasting sharp and well-calibrated predictive distribution. Through a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets. The results unequivocally demonstrate VAEneu's remarkable forecasting performance. VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting.","sentences":["This paper presents VAEneu, an innovative autoregressive method for multistep ahead univariate probabilistic time series forecasting.","We employ the conditional VAE framework and optimize the lower bound of the predictive distribution likelihood function by adopting the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule, as the loss function.","This novel pipeline results in forecasting sharp and well-calibrated predictive distribution.","Through a comprehensive empirical study, VAEneu is rigorously benchmarked against 12 baseline models across 12 datasets.","The results unequivocally demonstrate VAEneu's remarkable forecasting performance.","VAEneu provides a valuable tool for quantifying future uncertainties, and our extensive empirical study lays the foundation for future comparative studies for univariate multistep ahead probabilistic forecasting."],"url":"http://arxiv.org/abs/2405.04252v1","category":"cs.LG"}
{"created":"2024-05-07 12:07:06","title":"Federated Learning for Cooperative Inference Systems: The Case of Early Exit Networks","abstract":"As Internet of Things (IoT) technology advances, end devices like sensors and smartphones are progressively equipped with AI models tailored to their local memory and computational constraints. Local inference reduces communication costs and latency; however, these smaller models typically underperform compared to more sophisticated models deployed on edge servers or in the cloud. Cooperative Inference Systems (CISs) address this performance trade-off by enabling smaller devices to offload part of their inference tasks to more capable devices. These systems often deploy hierarchical models that share numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize strategies like early exits or ordered dropout. In such instances, Federated Learning (FL) may be employed to jointly train the models within a CIS. Yet, traditional training methods have overlooked the operational dynamics of CISs during inference, particularly the potential high heterogeneity in serving rates across clients. To address this gap, we propose a novel FL approach designed explicitly for use in CISs that accounts for these variations in serving rates. Our framework not only offers rigorous theoretical guarantees, but also surpasses state-of-the-art (SOTA) training algorithms for CISs, especially in scenarios where inference request rates or data availability are uneven among clients.","sentences":["As Internet of Things (IoT) technology advances, end devices like sensors and smartphones are progressively equipped with AI models tailored to their local memory and computational constraints.","Local inference reduces communication costs and latency; however, these smaller models typically underperform compared to more sophisticated models deployed on edge servers or in the cloud.","Cooperative Inference Systems (CISs) address this performance trade-off by enabling smaller devices to offload part of their inference tasks to more capable devices.","These systems often deploy hierarchical models that share numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize strategies like early exits or ordered dropout.","In such instances, Federated Learning (FL) may be employed to jointly train the models within a CIS.","Yet, traditional training methods have overlooked the operational dynamics of CISs during inference, particularly the potential high heterogeneity in serving rates across clients.","To address this gap, we propose a novel FL approach designed explicitly for use in CISs that accounts for these variations in serving rates.","Our framework not only offers rigorous theoretical guarantees, but also surpasses state-of-the-art (SOTA) training algorithms for CISs, especially in scenarios where inference request rates or data availability are uneven among clients."],"url":"http://arxiv.org/abs/2405.04249v1","category":"cs.LG"}
{"created":"2024-05-07 12:06:12","title":"Neurocomputational Phenotypes in Female and Male Autistic Individuals","abstract":"Autism Spectrum Disorder (ASD) is characterized by an altered phenotype in social interaction and communication. Additionally, autism typically manifests differently in females as opposed to males: a phenomenon that has likely led to long-term problems in diagnostics of autism in females. These sex-based differences in communicative behavior may originate from differences in neurocomputational properties of brain organization. The present study looked to examine the relationship between one neurocomputational measure of brain organization, the local power-law exponent, in autistic vs. neurotypical, as well as male vs. female participants. To investigate the autistic phenotype in neural organization based on biological sex, we collected continuous resting-state EEG data for 19 autistic young adults (10 F), and 23 controls (14 F), using a 64-channel Net Station EEG acquisition system. The data was analyzed to quantify the 1/f power spectrum. Correlations between power-law exponent and behavioral measures were calculated in a between-group (female vs. male; autistic vs. neurotypical) design. On average, the power-law exponent was significantly greater in the male ASD group than in the female ASD group in fronto-central regions. The differences were more pronounced over the left hemisphere, suggesting neural organization differences in regions responsible for language complexity. These differences provide a potential explanation for behavioral variances in female vs. male autistic young adults.","sentences":["Autism Spectrum Disorder (ASD) is characterized by an altered phenotype in social interaction and communication.","Additionally, autism typically manifests differently in females as opposed to males: a phenomenon that has likely led to long-term problems in diagnostics of autism in females.","These sex-based differences in communicative behavior may originate from differences in neurocomputational properties of brain organization.","The present study looked to examine the relationship between one neurocomputational measure of brain organization, the local power-law exponent, in autistic vs. neurotypical, as well as male vs. female participants.","To investigate the autistic phenotype in neural organization based on biological sex, we collected continuous resting-state EEG data for 19 autistic young adults (10 F), and 23 controls (14 F), using a 64-channel Net Station EEG acquisition system.","The data was analyzed to quantify the 1/f power spectrum.","Correlations between power-law exponent and behavioral measures were calculated in a between-group (female vs. male; autistic vs. neurotypical) design.","On average, the power-law exponent was significantly greater in the male ASD group than in the female ASD group in fronto-central regions.","The differences were more pronounced over the left hemisphere, suggesting neural organization differences in regions responsible for language complexity.","These differences provide a potential explanation for behavioral variances in female vs. male autistic young adults."],"url":"http://arxiv.org/abs/2405.04248v1","category":"q-bio.NC"}
{"created":"2024-05-07 12:02:23","title":"Exploring Correlations of Self-supervised Tasks for Graphs","abstract":"Graph self-supervised learning has sparked a research surge in training informative representations without accessing any labeled data. However, our understanding of graph self-supervised learning remains limited, and the inherent relationships between various self-supervised tasks are still unexplored. Our paper aims to provide a fresh understanding of graph self-supervised learning based on task correlations. Specifically, we evaluate the performance of the representations trained by one specific task on other tasks and define correlation values to quantify task correlations. Through this process, we unveil the task correlations between various self-supervised tasks and can measure their expressive capabilities, which are closely related to downstream performance. By analyzing the correlation values between tasks across various datasets, we reveal the complexity of task correlations and the limitations of existing multi-task learning methods. To obtain more capable representations, we propose Graph Task Correlation Modeling (GraphTCM) to illustrate the task correlations and utilize it to enhance graph self-supervised training. The experimental results indicate that our method significantly outperforms existing methods across various downstream tasks.","sentences":["Graph self-supervised learning has sparked a research surge in training informative representations without accessing any labeled data.","However, our understanding of graph self-supervised learning remains limited, and the inherent relationships between various self-supervised tasks are still unexplored.","Our paper aims to provide a fresh understanding of graph self-supervised learning based on task correlations.","Specifically, we evaluate the performance of the representations trained by one specific task on other tasks and define correlation values to quantify task correlations.","Through this process, we unveil the task correlations between various self-supervised tasks and can measure their expressive capabilities, which are closely related to downstream performance.","By analyzing the correlation values between tasks across various datasets, we reveal the complexity of task correlations and the limitations of existing multi-task learning methods.","To obtain more capable representations, we propose Graph Task Correlation Modeling (GraphTCM) to illustrate the task correlations and utilize it to enhance graph self-supervised training.","The experimental results indicate that our method significantly outperforms existing methods across various downstream tasks."],"url":"http://arxiv.org/abs/2405.04245v1","category":"cs.LG"}
{"created":"2024-05-07 11:58:34","title":"Exploring the Potential of Robot-Collected Data for Training Gesture Classification Systems","abstract":"Sensors and Artificial Intelligence (AI) have revolutionized the analysis of human movement, but the scarcity of specific samples presents a significant challenge in training intelligent systems, particularly in the context of diagnosing neurodegenerative diseases. This study investigates the feasibility of utilizing robot-collected data to train classification systems traditionally trained with human-collected data. As a proof of concept, we recorded a database of numeric characters using an ABB robotic arm and an Apple Watch. We compare the classification performance of the trained systems using both human-recorded and robot-recorded data. Our primary objective is to determine the potential for accurate identification of human numeric characters wearing a smartwatch using robotic movement as training data. The findings of this study offer valuable insights into the feasibility of using robot-collected data for training classification systems. This research holds broad implications across various domains that require reliable identification, particularly in scenarios where access to human-specific data is limited.","sentences":["Sensors and Artificial Intelligence (AI) have revolutionized the analysis of human movement, but the scarcity of specific samples presents a significant challenge in training intelligent systems, particularly in the context of diagnosing neurodegenerative diseases.","This study investigates the feasibility of utilizing robot-collected data to train classification systems traditionally trained with human-collected data.","As a proof of concept, we recorded a database of numeric characters using an ABB robotic arm and an Apple Watch.","We compare the classification performance of the trained systems using both human-recorded and robot-recorded data.","Our primary objective is to determine the potential for accurate identification of human numeric characters wearing a smartwatch using robotic movement as training data.","The findings of this study offer valuable insights into the feasibility of using robot-collected data for training classification systems.","This research holds broad implications across various domains that require reliable identification, particularly in scenarios where access to human-specific data is limited."],"url":"http://arxiv.org/abs/2405.04241v1","category":"cs.RO"}
{"created":"2024-05-07 11:50:25","title":"Unveiling the optimization process of Physics Informed Neural Networks: How accurate and competitive can PINNs be?","abstract":"This study investigates the potential accuracy boundaries of physics-informed neural networks, contrasting their approach with previous similar works and traditional numerical methods. We find that selecting improved optimization algorithms significantly enhances the accuracy of the results. Simple modifications to the loss function may also improve precision, offering an additional avenue for enhancement. Despite optimization algorithms having a greater impact on convergence than adjustments to the loss function, practical considerations often favor tweaking the latter due to ease of implementation. On a global scale, the integration of an enhanced optimizer and a marginally adjusted loss function enables a reduction in the loss function by several orders of magnitude across diverse physical problems. Consequently, our results obtained using compact networks (typically comprising 2 or 3 layers of 20-30 neurons) achieve accuracies comparable to finite difference schemes employing thousands of grid points. This study encourages the continued advancement of PINNs and associated optimization techniques for broader applications across various fields.","sentences":["This study investigates the potential accuracy boundaries of physics-informed neural networks, contrasting their approach with previous similar works and traditional numerical methods.","We find that selecting improved optimization algorithms significantly enhances the accuracy of the results.","Simple modifications to the loss function may also improve precision, offering an additional avenue for enhancement.","Despite optimization algorithms having a greater impact on convergence than adjustments to the loss function, practical considerations often favor tweaking the latter due to ease of implementation.","On a global scale, the integration of an enhanced optimizer and a marginally adjusted loss function enables a reduction in the loss function by several orders of magnitude across diverse physical problems.","Consequently, our results obtained using compact networks (typically comprising 2 or 3 layers of 20-30 neurons) achieve accuracies comparable to finite difference schemes employing thousands of grid points.","This study encourages the continued advancement of PINNs and associated optimization techniques for broader applications across various fields."],"url":"http://arxiv.org/abs/2405.04230v1","category":"physics.comp-ph"}
{"created":"2024-05-07 11:33:49","title":"Iterative Experience Refinement of Software-Developing Agents","abstract":"Autonomous agents powered by large language models (LLMs) show significant potential for achieving high autonomy in various scenarios such as software development. Recent research has shown that LLM agents can leverage past experiences to reduce errors and enhance efficiency. However, the static experience paradigm, reliant on a fixed collection of past experiences acquired heuristically, lacks iterative refinement and thus hampers agents' adaptability. In this paper, we introduce the Iterative Experience Refinement framework, enabling LLM agents to refine experiences iteratively during task execution. We propose two fundamental patterns: the successive pattern, refining based on nearest experiences within a task batch, and the cumulative pattern, acquiring experiences across all previous task batches. Augmented with our heuristic experience elimination, the method prioritizes high-quality and frequently-used experiences, effectively managing the experience space and enhancing efficiency. Extensive experiments show that while the successive pattern may yield superior results, the cumulative pattern provides more stable performance. Moreover, experience elimination facilitates achieving better performance using just 11.54% of a high-quality subset.","sentences":["Autonomous agents powered by large language models (LLMs) show significant potential for achieving high autonomy in various scenarios such as software development.","Recent research has shown that LLM agents can leverage past experiences to reduce errors and enhance efficiency.","However, the static experience paradigm, reliant on a fixed collection of past experiences acquired heuristically, lacks iterative refinement and thus hampers agents' adaptability.","In this paper, we introduce the Iterative Experience Refinement framework, enabling LLM agents to refine experiences iteratively during task execution.","We propose two fundamental patterns: the successive pattern, refining based on nearest experiences within a task batch, and the cumulative pattern, acquiring experiences across all previous task batches.","Augmented with our heuristic experience elimination, the method prioritizes high-quality and frequently-used experiences, effectively managing the experience space and enhancing efficiency.","Extensive experiments show that while the successive pattern may yield superior results, the cumulative pattern provides more stable performance.","Moreover, experience elimination facilitates achieving better performance using just 11.54% of a high-quality subset."],"url":"http://arxiv.org/abs/2405.04219v1","category":"cs.CL"}
{"created":"2024-05-07 11:27:13","title":"NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions","abstract":"Today's classical planners are powerful, but modeling input tasks in formats such as PDDL is tedious and error-prone. In contrast, planning with Large Language Models (LLMs) allows for almost any input text, but offers no guarantees on plan quality or even soundness. In an attempt to merge the best of these two approaches, some work has begun to use LLMs to automate parts of the PDDL creation process. However, these methods still require various degrees of expert input. We present NL2Plan, the first domain-agnostic offline LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the necessary information from a short text prompt before creating a complete PDDL description of both the domain and the problem, which is finally solved by a classical planner. We evaluate NL2Plan on four planning domains and find that it solves 10 out of 15 tasks - a clear improvement over a plain chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover, in two out of the five failure cases, instead of returning an invalid plan, NL2Plan reports that it failed to solve the task. In addition to using NL2Plan in end-to-end mode, users can inspect and correct all of its intermediate results, such as the PDDL representation, increasing explainability and making it an assistive tool for PDDL creation.","sentences":["Today's classical planners are powerful, but modeling input tasks in formats such as PDDL is tedious and error-prone.","In contrast, planning with Large Language Models (LLMs) allows for almost any input text, but offers no guarantees on plan quality or even soundness.","In an attempt to merge the best of these two approaches, some work has begun to use LLMs to automate parts of the PDDL creation process.","However, these methods still require various degrees of expert input.","We present NL2Plan, the first domain-agnostic offline LLM-driven planning system.","NL2Plan uses an LLM to incrementally extract the necessary information from a short text prompt before creating a complete PDDL description of both the domain and the problem, which is finally solved by a classical planner.","We evaluate NL2Plan on four planning domains and find that it solves 10 out of 15 tasks - a clear improvement over a plain chain-of-thought reasoning LLM approach, which only solves 2 tasks.","Moreover, in two out of the five failure cases, instead of returning an invalid plan, NL2Plan reports that it failed to solve the task.","In addition to using NL2Plan in end-to-end mode, users can inspect and correct all of its intermediate results, such as the PDDL representation, increasing explainability and making it an assistive tool for PDDL creation."],"url":"http://arxiv.org/abs/2405.04215v1","category":"cs.AI"}
{"created":"2024-05-07 11:24:56","title":"Green Tsetlin Redefining Efficiency in Tsetlin Machine Frameworks","abstract":"Green Tsetlin (GT) is a Tsetlin Machine (TM) framework developed to solve real-world problems using TMs. Several frameworks already exist that provide access to TM implementations. However, these either lack features or have a research-first focus. GT is an easy-to-use framework that aims to lower the complexity and provide a production-ready TM implementation that is great for experienced practitioners and beginners. To this end, GT establishes a clear separation between training and inference. A C++ backend with a Python interface provides competitive training and inference performance, with the option of running in pure Python. It also integrates support for critical components such as exporting trained models, hyper-parameter search, and cross-validation out-of-the-box.","sentences":["Green Tsetlin (GT) is a Tsetlin Machine (TM) framework developed to solve real-world problems using TMs.","Several frameworks already exist that provide access to TM implementations.","However, these either lack features or have a research-first focus.","GT is an easy-to-use framework that aims to lower the complexity and provide a production-ready TM implementation that is great for experienced practitioners and beginners.","To this end, GT establishes a clear separation between training and inference.","A C++ backend with a Python interface provides competitive training and inference performance, with the option of running in pure Python.","It also integrates support for critical components such as exporting trained models, hyper-parameter search, and cross-validation out-of-the-box."],"url":"http://arxiv.org/abs/2405.04212v1","category":"cs.AI"}
{"created":"2024-05-07 11:20:10","title":"NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator","abstract":"Attention mechanisms are becoming increasingly popular, being used in neural network models in multiple domains such as natural language processing (NLP) and vision applications, especially at the edge. However, attention layers are difficult to map onto existing neuro accelerators since they have a much higher density of non-linear operations, which lead to inefficient utilization of today's vector units. This work introduces NOVA, a NoC-based Vector Unit that can perform non-linear operations within the NoC of the accelerators, and can be overlaid onto existing neuro accelerators to map attention layers at the edge. Our results show that the NOVA architecture is up to 37.8x more power-efficient than state-of-the-art hardware approximators when running existing attention-based neural networks.","sentences":["Attention mechanisms are becoming increasingly popular, being used in neural network models in multiple domains such as natural language processing (NLP) and vision applications, especially at the edge.","However, attention layers are difficult to map onto existing neuro accelerators since they have a much higher density of non-linear operations, which lead to inefficient utilization of today's vector units.","This work introduces NOVA, a NoC-based Vector Unit that can perform non-linear operations within the NoC of the accelerators, and can be overlaid onto existing neuro accelerators to map attention layers at the edge.","Our results show that the NOVA architecture is up to 37.8x more power-efficient than state-of-the-art hardware approximators when running existing attention-based neural networks."],"url":"http://arxiv.org/abs/2405.04206v1","category":"cs.AR"}
{"created":"2024-05-07 11:13:17","title":"Enhancing Physical Layer Communication Security through Generative AI with Mixture of Experts","abstract":"AI technologies have become more widely adopted in wireless communications. As an emerging type of AI technologies, the generative artificial intelligence (GAI) gains lots of attention in communication security. Due to its powerful learning ability, GAI models have demonstrated superiority over conventional AI methods. However, GAI still has several limitations, including high computational complexity and limited adaptability. Mixture of Experts (MoE), which uses multiple expert models for prediction through a gate mechanism, proposes possible solutions. Firstly, we review GAI model's applications in physical layer communication security, discuss limitations, and explore how MoE can help GAI overcome these limitations. Furthermore, we propose an MoE-enabled GAI framework for network optimization problems for communication security. To demonstrate the framework's effectiveness, we provide a case study in a cooperative friendly jamming scenario. The experimental results show that the MoE-enabled framework effectively assists the GAI algorithm, solves its limitations, and enhances communication security.","sentences":["AI technologies have become more widely adopted in wireless communications.","As an emerging type of AI technologies, the generative artificial intelligence (GAI) gains lots of attention in communication security.","Due to its powerful learning ability, GAI models have demonstrated superiority over conventional AI methods.","However, GAI still has several limitations, including high computational complexity and limited adaptability.","Mixture of Experts (MoE), which uses multiple expert models for prediction through a gate mechanism, proposes possible solutions.","Firstly, we review GAI model's applications in physical layer communication security, discuss limitations, and explore how MoE can help GAI overcome these limitations.","Furthermore, we propose an MoE-enabled GAI framework for network optimization problems for communication security.","To demonstrate the framework's effectiveness, we provide a case study in a cooperative friendly jamming scenario.","The experimental results show that the MoE-enabled framework effectively assists the GAI algorithm, solves its limitations, and enhances communication security."],"url":"http://arxiv.org/abs/2405.04198v1","category":"cs.CR"}
{"created":"2024-05-07 11:13:12","title":"Resonant structure for improved directionality and extraction of single photons","abstract":"Fluorescent atomic defects, especially in dielectric materials, such as diamond are quite promising for several emerging quantum applications. However, efficient light extraction, directional emission, and narrow spectral emission are key challenges. We have designed dielectric metasurface exploiting Mie-resonance and the Kerker condition to address these issues. Our designed diamond metasurface, tailored for nitrogen-vacancy (NV) defect centers in diamond, predicts up to 500x improvement in the collection of 637 nm (zero phonon line) photons over that from the bare diamond. Our design achieves highly directional emission, predominantly emitting in a 20 degree lobe in the forward direction. This makes light collection more efficient, including for fiber-based collection. The predicted results are stable against the position of the emitter placed in the metaelement, thus alleviating the challenging fabrication requirement of precise positioning of the defect center. Equally importantly, our design approach can be applied to enhance single photon emission also from other defects such as SiV, other materials such as hBN, and other sources such as quantum dots.","sentences":["Fluorescent atomic defects, especially in dielectric materials, such as diamond are quite promising for several emerging quantum applications.","However, efficient light extraction, directional emission, and narrow spectral emission are key challenges.","We have designed dielectric metasurface exploiting Mie-resonance and the Kerker condition to address these issues.","Our designed diamond metasurface, tailored for nitrogen-vacancy (NV) defect centers in diamond, predicts up to 500x improvement in the collection of 637 nm (zero phonon line) photons over that from the bare diamond.","Our design achieves highly directional emission, predominantly emitting in a 20 degree lobe in the forward direction.","This makes light collection more efficient, including for fiber-based collection.","The predicted results are stable against the position of the emitter placed in the metaelement, thus alleviating the challenging fabrication requirement of precise positioning of the defect center.","Equally importantly, our design approach can be applied to enhance single photon emission also from other defects such as SiV, other materials such as hBN, and other sources such as quantum dots."],"url":"http://arxiv.org/abs/2405.04197v2","category":"physics.optics"}
{"created":"2024-05-07 10:49:10","title":"Artificial Intelligence-powered fossil shark tooth identification: Unleashing the potential of Convolutional Neural Networks","abstract":"All fields of knowledge are being impacted by Artificial Intelligence. In particular, the Deep Learning paradigm enables the development of data analysis tools that support subject matter experts in a variety of sectors, from physics up to the recognition of ancient languages. Palaeontology is now observing this trend as well. This study explores the capability of Convolutional Neural Networks (CNNs), a particular class of Deep Learning algorithms specifically crafted for computer vision tasks, to classify images of isolated fossil shark teeth gathered from online datasets as well as from the authors$'$ experience on Peruvian Miocene and Italian Pliocene fossil assemblages. The shark taxa that are included in the final, composite dataset (which consists of more than one thousand images) are representative of both extinct and extant genera, namely, Carcharhinus, Carcharias, Carcharocles, Chlamydoselachus, Cosmopolitodus, Galeocerdo, Hemipristis, Notorynchus, Prionace and Squatina. We developed a CNN, named SharkNet-X, specifically tailored on our recognition task, reaching a 5-fold cross validated mean accuracy of 0.85 to identify images containing a single shark tooth. Furthermore, we elaborated a visualization of the features extracted from images using the last dense layer of the CNN, achieved through the application of the clustering technique t-SNE. In addition, in order to understand and explain the behaviour of the CNN while giving a paleontological point of view on the results, we introduced the explainability method SHAP. To the best of our knowledge, this is the first instance in which this method is applied to the field of palaeontology. The main goal of this work is to showcase how Deep Learning techniques can aid in identifying isolated fossil shark teeth, paving the way for developing new information tools for automating the recognition and classification of fossils.","sentences":["All fields of knowledge are being impacted by Artificial Intelligence.","In particular, the Deep Learning paradigm enables the development of data analysis tools that support subject matter experts in a variety of sectors, from physics up to the recognition of ancient languages.","Palaeontology is now observing this trend as well.","This study explores the capability of Convolutional Neural Networks (CNNs), a particular class of Deep Learning algorithms specifically crafted for computer vision tasks, to classify images of isolated fossil shark teeth gathered from online datasets as well as from the authors$'$ experience on Peruvian Miocene and Italian Pliocene fossil assemblages.","The shark taxa that are included in the final, composite dataset (which consists of more than one thousand images) are representative of both extinct and extant genera, namely, Carcharhinus, Carcharias, Carcharocles, Chlamydoselachus, Cosmopolitodus, Galeocerdo, Hemipristis, Notorynchus, Prionace and Squatina.","We developed a CNN, named SharkNet-X, specifically tailored on our recognition task, reaching a 5-fold cross validated mean accuracy of 0.85 to identify images containing a single shark tooth.","Furthermore, we elaborated a visualization of the features extracted from images using the last dense layer of the CNN, achieved through the application of the clustering technique t-SNE.","In addition, in order to understand and explain the behaviour of the CNN while giving a paleontological point of view on the results, we introduced the explainability method SHAP.","To the best of our knowledge, this is the first instance in which this method is applied to the field of palaeontology.","The main goal of this work is to showcase how Deep Learning techniques can aid in identifying isolated fossil shark teeth, paving the way for developing new information tools for automating the recognition and classification of fossils."],"url":"http://arxiv.org/abs/2405.04189v1","category":"cs.CV"}
{"created":"2024-05-07 10:45:52","title":"Research on signalized intersection mixed traffic flow platoon control method considering Backward-looking effect","abstract":"Connected and Autonomous Vehicles (CAVs) technology facilitates the advancement of intelligent transportation. However, intelligent control techniques for mixed traffic flow at signalized intersections involving both CAVs and Human-Driven Vehicles (HDVs) require further investigation into the impact of backward-looking effect. This paper proposes the concept of 1+n+1 mixed platoon considering the backward-looking effect, consisting of one leading CAV, n following HDVs, and one trailing CAV. The leading and trailing CAVs collectively guide the movement of intermediate HDVs at intersections, forming an optimal control framework for platoon-based CAVs at signalized intersections. Initially, a linearized dynamic model for the 1+n+1 mixed platoon is established and compared with a benchmark model focusing solely on controlling the lead vehicle. Subsequently, constraints are formulated for the optimal control framework, aiming to enhance overall intersection traffic efficiency and fuel economy by directly controlling the leading and trailing CAVs in the platoon. Finally, extensive numerical simulations compare vehicle throughput and fuel consumption at signalized intersections under different mixed platoon control methods, validating that considering both front and backward-looking effects in the mixed platoon control method outperforms traditional methods focusing solely on the lead CAV.","sentences":["Connected and Autonomous Vehicles (CAVs) technology facilitates the advancement of intelligent transportation.","However, intelligent control techniques for mixed traffic flow at signalized intersections involving both CAVs and Human-Driven Vehicles (HDVs) require further investigation into the impact of backward-looking effect.","This paper proposes the concept of 1+n+1 mixed platoon considering the backward-looking effect, consisting of one leading CAV, n following HDVs, and one trailing CAV.","The leading and trailing CAVs collectively guide the movement of intermediate HDVs at intersections, forming an optimal control framework for platoon-based CAVs at signalized intersections.","Initially, a linearized dynamic model for the 1+n+1 mixed platoon is established and compared with a benchmark model focusing solely on controlling the lead vehicle.","Subsequently, constraints are formulated for the optimal control framework, aiming to enhance overall intersection traffic efficiency and fuel economy by directly controlling the leading and trailing CAVs in the platoon.","Finally, extensive numerical simulations compare vehicle throughput and fuel consumption at signalized intersections under different mixed platoon control methods, validating that considering both front and backward-looking effects in the mixed platoon control method outperforms traditional methods focusing solely on the lead CAV."],"url":"http://arxiv.org/abs/2405.04185v1","category":"physics.app-ph"}
{"created":"2024-05-07 10:18:28","title":"\u03c3/\u03c0 character of H-bonding in water clusters","abstract":"Hydrogen bonds are typically treated as sufficiently localized directional intermolecular bonds, in which dispersion and electrostatic contributions can be distinguished. However, being formed chiefly due to the overlapping of p orbitals of electronegative atoms, the corresponding electronic bonds are characterized by both {\\sigma}- and {\\pi}-kind binding, the former determining the directionality of bonds, while the latter, the coupling of molecules and the collective effects in H-bond networks. The latter contribution was never considered previously and is predetermined by overlapping pre-lone pair orbitals of oxygen atoms. This is manifested in the peculiarities of the electron density distribution, which are quantified based on the analysis of magnetic shielding tensors of oxygen and bridge hydrogen nuclei and illustrated by the shapes of cluster orbitals of water aggregates.","sentences":["Hydrogen bonds are typically treated as sufficiently localized directional intermolecular bonds, in which dispersion and electrostatic contributions can be distinguished.","However, being formed chiefly due to the overlapping of p orbitals of electronegative atoms, the corresponding electronic bonds are characterized by both {\\sigma}- and {\\pi}-kind binding, the former determining the directionality of bonds, while the latter, the coupling of molecules and the collective effects in H-bond networks.","The latter contribution was never considered previously and is predetermined by overlapping pre-lone pair orbitals of oxygen atoms.","This is manifested in the peculiarities of the electron density distribution, which are quantified based on the analysis of magnetic shielding tensors of oxygen and bridge hydrogen nuclei and illustrated by the shapes of cluster orbitals of water aggregates."],"url":"http://arxiv.org/abs/2405.04173v1","category":"physics.chem-ph"}
{"created":"2024-05-07 10:11:42","title":"FedStale: leveraging stale client updates in federated learning","abstract":"Federated learning algorithms, such as FedAvg, are negatively affected by data heterogeneity and partial client participation. To mitigate the latter problem, global variance reduction methods, like FedVARP, leverage stale model updates for non-participating clients. These methods are effective under homogeneous client participation. Yet, this paper shows that, when some clients participate much less than others, aggregating updates with different levels of staleness can detrimentally affect the training process. Motivated by this observation, we introduce FedStale, a novel algorithm that updates the global model in each round through a convex combination of \"fresh\" updates from participating clients and \"stale\" updates from non-participating ones. By adjusting the weight in the convex combination, FedStale interpolates between FedAvg, which only uses fresh updates, and FedVARP, which treats fresh and stale updates equally. Our analysis of FedStale convergence yields the following novel findings: i) it integrates and extends previous FedAvg and FedVARP analyses to heterogeneous client participation; ii) it underscores how the least participating client influences convergence error; iii) it provides practical guidelines to best exploit stale updates, showing that their usefulness diminishes as data heterogeneity decreases and participation heterogeneity increases. Extensive experiments featuring diverse levels of client data and participation heterogeneity not only confirm these findings but also show that FedStale outperforms both FedAvg and FedVARP in many settings.","sentences":["Federated learning algorithms, such as FedAvg, are negatively affected by data heterogeneity and partial client participation.","To mitigate the latter problem, global variance reduction methods, like FedVARP, leverage stale model updates for non-participating clients.","These methods are effective under homogeneous client participation.","Yet, this paper shows that, when some clients participate much less than others, aggregating updates with different levels of staleness can detrimentally affect the training process.","Motivated by this observation, we introduce FedStale, a novel algorithm that updates the global model in each round through a convex combination of \"fresh\" updates from participating clients and \"stale\" updates from non-participating ones.","By adjusting the weight in the convex combination, FedStale interpolates between FedAvg, which only uses fresh updates, and FedVARP, which treats fresh and stale updates equally.","Our analysis of FedStale convergence yields the following novel findings: i) it integrates and extends previous FedAvg and FedVARP analyses to heterogeneous client participation; ii) it underscores how the least participating client influences convergence error; iii) it provides practical guidelines to best exploit stale updates, showing that their usefulness diminishes as data heterogeneity decreases and participation heterogeneity increases.","Extensive experiments featuring diverse levels of client data and participation heterogeneity not only confirm these findings but also show that FedStale outperforms both FedAvg and FedVARP in many settings."],"url":"http://arxiv.org/abs/2405.04171v1","category":"cs.LG"}
{"created":"2024-05-07 10:00:00","title":"MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization","abstract":"This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine-tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization. In contrast to existing domain adaptation approaches in summarization, MEDVOC treats vocabulary as an optimizable parameter and optimizes the PLM vocabulary based on fragment score conditioned only on the downstream task's reference summaries. Unlike previous works on vocabulary adaptation (limited only to classification tasks), optimizing vocabulary based on summarization tasks requires an extremely costly intermediate fine-tuning step on large summarization datasets. To that end, our novel fragment score-based hyperparameter search very significantly reduces this fine-tuning time -- from 450 days to less than 2 days on average. Furthermore, while previous works on vocabulary adaptation are often primarily tied to single PLMs, MEDVOC is designed to be deployable across multiple PLMs (with varying model vocabulary sizes, pre-training objectives, and model sizes) -- bridging the limited vocabulary overlap between the biomedical literature domain and PLMs. MEDVOC outperforms baselines by 15.74% in terms of Rouge-L in zero-shot setting and shows gains of 17.29% in high Out-Of-Vocabulary (OOV) concentrations. Our human evaluation shows MEDVOC generates more faithful medical summaries (88% compared to 59% in baselines). We make the codebase publicly available at https://github.com/gb-kgp/MEDVOC.","sentences":["This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for fine-tuning pre-trained language models (PLMs) like BertSumAbs, BART, and PEGASUS for improved medical text summarization.","In contrast to existing domain adaptation approaches in summarization, MEDVOC treats vocabulary as an optimizable parameter and optimizes the PLM vocabulary based on fragment score conditioned only on the downstream task's reference summaries.","Unlike previous works on vocabulary adaptation (limited only to classification tasks), optimizing vocabulary based on summarization tasks requires an extremely costly intermediate fine-tuning step on large summarization datasets.","To that end, our novel fragment score-based hyperparameter search very significantly reduces this fine-tuning time -- from 450 days to less than 2 days on average.","Furthermore, while previous works on vocabulary adaptation are often primarily tied to single PLMs, MEDVOC is designed to be deployable across multiple PLMs (with varying model vocabulary sizes, pre-training objectives, and model sizes) -- bridging the limited vocabulary overlap between the biomedical literature domain and PLMs.","MEDVOC outperforms baselines by 15.74% in terms of Rouge-L in zero-shot setting and shows gains of 17.29% in high Out-Of-Vocabulary (OOV) concentrations.","Our human evaluation shows MEDVOC generates more faithful medical summaries (88% compared to 59% in baselines).","We make the codebase publicly available at https://github.com/gb-kgp/MEDVOC."],"url":"http://arxiv.org/abs/2405.04163v1","category":"cs.CL"}
{"created":"2024-05-07 09:58:02","title":"Opportunities for machine learning in scientific discovery","abstract":"Technological advancements have substantially increased computational power and data availability, enabling the application of powerful machine-learning (ML) techniques across various fields. However, our ability to leverage ML methods for scientific discovery, {\\it i.e.} to obtain fundamental and formalized knowledge about natural processes, is still in its infancy. In this review, we explore how the scientific community can increasingly leverage ML techniques to achieve scientific discoveries. We observe that the applicability and opportunity of ML depends strongly on the nature of the problem domain, and whether we have full ({\\it e.g.}, turbulence), partial ({\\it e.g.}, computational biochemistry), or no ({\\it e.g.}, neuroscience) {\\it a-priori} knowledge about the governing equations and physical properties of the system. Although challenges remain, principled use of ML is opening up new avenues for fundamental scientific discoveries. Throughout these diverse fields, there is a theme that ML is enabling researchers to embrace complexity in observational data that was previously intractable to classic analysis and numerical investigations.","sentences":["Technological advancements have substantially increased computational power and data availability, enabling the application of powerful machine-learning (ML) techniques across various fields.","However, our ability to leverage ML methods for scientific discovery, {\\it i.e.} to obtain fundamental and formalized knowledge about natural processes, is still in its infancy.","In this review, we explore how the scientific community can increasingly leverage ML techniques to achieve scientific discoveries.","We observe that the applicability and opportunity of ML depends strongly on the nature of the problem domain, and whether we have full ({\\it e.g.}, turbulence), partial ({\\it e.g.}, computational biochemistry), or no ({\\it e.g.}, neuroscience) {\\it a-priori} knowledge about the governing equations and physical properties of the system.","Although challenges remain, principled use of ML is opening up new avenues for fundamental scientific discoveries.","Throughout these diverse fields, there is a theme that ML is enabling researchers to embrace complexity in observational data that was previously intractable to classic analysis and numerical investigations."],"url":"http://arxiv.org/abs/2405.04161v1","category":"cs.LG"}
{"created":"2024-05-07 09:50:57","title":"How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability","abstract":"Transformer-based language models are treated as black-boxes because of their large number of parameters and complex internal interactions, which is a serious safety concern. Mechanistic Interpretability (MI) intends to reverse-engineer neural network behaviors in terms of human-understandable components. In this work, we focus on understanding how GPT-2 Small performs the task of predicting three-letter acronyms. Previous works in the MI field have focused so far on tasks that predict a single token. To the best of our knowledge, this is the first work that tries to mechanistically understand a behavior involving the prediction of multiple consecutive tokens. We discover that the prediction is performed by a circuit composed of 8 attention heads (~5% of the total heads) which we classified in three groups according to their role. We also demonstrate that these heads concentrate the acronym prediction functionality. In addition, we mechanistically interpret the most relevant heads of the circuit and find out that they use positional information which is propagated via the causal mask mechanism. We expect this work to lay the foundation for understanding more complex behaviors involving multiple-token predictions.","sentences":["Transformer-based language models are treated as black-boxes because of their large number of parameters and complex internal interactions, which is a serious safety concern.","Mechanistic Interpretability (MI) intends to reverse-engineer neural network behaviors in terms of human-understandable components.","In this work, we focus on understanding how GPT-2 Small performs the task of predicting three-letter acronyms.","Previous works in the MI field have focused so far on tasks that predict a single token.","To the best of our knowledge, this is the first work that tries to mechanistically understand a behavior involving the prediction of multiple consecutive tokens.","We discover that the prediction is performed by a circuit composed of 8 attention heads (~5% of the total heads) which we classified in three groups according to their role.","We also demonstrate that these heads concentrate the acronym prediction functionality.","In addition, we mechanistically interpret the most relevant heads of the circuit and find out that they use positional information which is propagated via the causal mask mechanism.","We expect this work to lay the foundation for understanding more complex behaviors involving multiple-token predictions."],"url":"http://arxiv.org/abs/2405.04156v1","category":"cs.LG"}
{"created":"2024-05-07 09:41:39","title":"Gas Source Localization Using physics Guided Neural Networks","abstract":"This work discusses a novel method for estimating the location of a gas source based on spatially distributed concentration measurements taken, e.g., by a mobile robot or flying platform that follows a predefined trajectory to collect samples. The proposed approach uses a Physics-Guided Neural Network to approximate the gas dispersion with the source location as an additional network input. After an initial offline training phase, the neural network can be used to efficiently solve the inverse problem of localizing the gas source based on measurements. The proposed approach allows avoiding rather costly numerical simulations of gas physics needed for solving inverse problems. Our experiments show that the method localizes the source well, even when dealing with measurements affected by noise.","sentences":["This work discusses a novel method for estimating the location of a gas source based on spatially distributed concentration measurements taken, e.g., by a mobile robot or flying platform that follows a predefined trajectory to collect samples.","The proposed approach uses a Physics-Guided Neural Network to approximate the gas dispersion with the source location as an additional network input.","After an initial offline training phase, the neural network can be used to efficiently solve the inverse problem of localizing the gas source based on measurements.","The proposed approach allows avoiding rather costly numerical simulations of gas physics needed for solving inverse problems.","Our experiments show that the method localizes the source well, even when dealing with measurements affected by noise."],"url":"http://arxiv.org/abs/2405.04151v1","category":"cs.LG"}
{"created":"2024-05-07 09:33:08","title":"Reconstructing the spacetime dual to a free matrix","abstract":"In this paper we consider the collective field theory description of the singlet sector of a free matrix field in 2+1 dimensions. This necessarily involves the study of $k$-local collective fields, which are functions of $2k+1$ coordinates. We argue that these coordinates have a natural interpretation: the $k$-local collective field is a field defined on an AdS$_4\\times$S$^{k-2}\\times$S$^{k-1}$ spacetime. The modes of a harmonic expansion on the S$^{k-2}\\times$S$^{k-1}$ portion of the spacetime leads to the spinning bulk fields of the dual gravity theory.","sentences":["In this paper we consider the collective field theory description of the singlet sector of a free matrix field in 2+1 dimensions.","This necessarily involves the study of $k$-local collective fields, which are functions of $2k+1$ coordinates.","We argue that these coordinates have a natural interpretation: the $k$-local collective field is a field defined on an AdS$_4\\times$S$^{k-2}\\times$S$^{k-1}$ spacetime.","The modes of a harmonic expansion on the S$^{k-2}\\times$S$^{k-1}$ portion of the spacetime leads to the spinning bulk fields of the dual gravity theory."],"url":"http://arxiv.org/abs/2405.04148v1","category":"hep-th"}
{"created":"2024-05-07 09:08:00","title":"GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness","abstract":"This study explores the limitations of traditional Cybersecurity Awareness and Training (CSAT) programs and proposes an innovative solution using Generative Pre-Trained Transformers (GPT) to address these shortcomings. Traditional approaches lack personalization and adaptability to individual learning styles. To overcome these challenges, the study integrates GPT models to deliver highly tailored and dynamic cybersecurity learning expe-riences. Leveraging natural language processing capabilities, the proposed approach personalizes training modules based on individual trainee pro-files, helping to ensure engagement and effectiveness. An experiment using a GPT model to provide a real-time and adaptive CSAT experience through generating customized training content. The findings have demonstrated a significant improvement over traditional programs, addressing issues of en-gagement, dynamicity, and relevance. GPT-powered CSAT programs offer a scalable and effective solution to enhance cybersecurity awareness, provid-ing personalized training content that better prepares individuals to miti-gate cybersecurity risks in their specific roles within the organization.","sentences":["This study explores the limitations of traditional Cybersecurity Awareness and Training (CSAT) programs and proposes an innovative solution using Generative Pre-Trained Transformers (GPT) to address these shortcomings.","Traditional approaches lack personalization and adaptability to individual learning styles.","To overcome these challenges, the study integrates GPT models to deliver highly tailored and dynamic cybersecurity learning expe-riences.","Leveraging natural language processing capabilities, the proposed approach personalizes training modules based on individual trainee pro-files, helping to ensure engagement and effectiveness.","An experiment using a GPT model to provide a real-time and adaptive CSAT experience through generating customized training content.","The findings have demonstrated a significant improvement over traditional programs, addressing issues of en-gagement, dynamicity, and relevance.","GPT-powered CSAT programs offer a scalable and effective solution to enhance cybersecurity awareness, provid-ing personalized training content that better prepares individuals to miti-gate cybersecurity risks in their specific roles within the organization."],"url":"http://arxiv.org/abs/2405.04138v1","category":"cs.CR"}
{"created":"2024-05-07 09:05:20","title":"Enriched BERT Embeddings for Scholarly Publication Classification","abstract":"With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses this challenge organized as a competition. The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph (ORKG) taxonomy of research fields for a given article.This paper presents our results. Initially, we enrich the dataset (containing English scholarly articles sourced from ORKG and arXiv), then leverage different pre-trained language Models (PLMs), specifically BERT, and explore their efficacy in transfer learning for this downstream task. Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and SPECTER2. We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as OpenAlex, Semantic Scholar, and Crossref. Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with SPECTER2 emerging as the most accurate model. Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, OpenAlex and Crossref. Our best-performing approach achieves a weighted F1-score of 0.7415. Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources.","sentences":["With the rapid expansion of academic literature and the proliferation of preprints, researchers face growing challenges in manually organizing and labeling large volumes of articles.","The NSLP 2024 FoRC Shared Task I addresses this challenge organized as a competition.","The goal is to develop a classifier capable of predicting one of 123 predefined classes from the Open Research Knowledge Graph (ORKG) taxonomy of research fields for a given article.","This paper presents our results.","Initially, we enrich the dataset (containing English scholarly articles sourced from ORKG and arXiv), then leverage different pre-trained language Models (PLMs), specifically BERT, and explore their efficacy in transfer learning for this downstream task.","Our experiments encompass feature-based and fine-tuned transfer learning approaches using diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and SPECTER2.","We conduct hyperparameter tuning and investigate the impact of data augmentation from bibliographic databases such as OpenAlex, Semantic Scholar, and Crossref.","Our results demonstrate that fine-tuning pre-trained models substantially enhances classification performance, with SPECTER2 emerging as the most accurate model.","Moreover, enriching the dataset with additional metadata improves classification outcomes significantly, especially when integrating information from S2AG, OpenAlex and Crossref.","Our best-performing approach achieves a weighted F1-score of 0.7415.","Overall, our study contributes to the advancement of reliable automated systems for scholarly publication categorization, offering a potential solution to the laborious manual curation process, thereby facilitating researchers in efficiently locating relevant resources."],"url":"http://arxiv.org/abs/2405.04136v1","category":"cs.AI"}
{"created":"2024-05-07 09:04:52","title":"In-context Learning for Automated Driving Scenarios","abstract":"One of the key challenges in current Reinforcement Learning (RL)-based Automated Driving (AD) agents is achieving flexible, precise, and human-like behavior cost-effectively. This paper introduces an innovative approach utilizing Large Language Models (LLMs) to intuitively and effectively optimize RL reward functions in a human-centric way. We developed a framework where instructions and dynamic environment descriptions are input into the LLM. The LLM then utilizes this information to assist in generating rewards, thereby steering the behavior of RL agents towards patterns that more closely resemble human driving. The experimental results demonstrate that this approach not only makes RL agents more anthropomorphic but also reaches better performance. Additionally, various strategies for reward-proxy and reward-shaping are investigated, revealing the significant impact of prompt design on shaping an AD vehicle's behavior. These findings offer a promising direction for the development of more advanced and human-like automated driving systems. Our experimental data and source code can be found here.","sentences":["One of the key challenges in current Reinforcement Learning (RL)-based Automated Driving (AD) agents is achieving flexible, precise, and human-like behavior cost-effectively.","This paper introduces an innovative approach utilizing Large Language Models (LLMs) to intuitively and effectively optimize RL reward functions in a human-centric way.","We developed a framework where instructions and dynamic environment descriptions are input into the LLM.","The LLM then utilizes this information to assist in generating rewards, thereby steering the behavior of RL agents towards patterns that more closely resemble human driving.","The experimental results demonstrate that this approach not only makes RL agents more anthropomorphic but also reaches better performance.","Additionally, various strategies for reward-proxy and reward-shaping are investigated, revealing the significant impact of prompt design on shaping an AD vehicle's behavior.","These findings offer a promising direction for the development of more advanced and human-like automated driving systems.","Our experimental data and source code can be found here."],"url":"http://arxiv.org/abs/2405.04135v1","category":"cs.AI"}
{"created":"2024-05-07 08:47:40","title":"Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling","abstract":"Analog electronic circuits are at the core of an important category of musical devices. The nonlinear features of their electronic components give analog musical devices a distinctive timbre and sound quality, making them highly desirable. Artificial neural networks have rapidly gained popularity for the emulation of analog audio effects circuits, particularly recurrent networks. While neural approaches have been successful in accurately modeling distortion circuits, they require architectural improvements that account for parameter conditioning and low latency response. In this article, we explore the application of recent machine learning advancements for virtual analog modeling. We compare State Space models and Linear Recurrent Units against the more common Long Short Term Memory networks. These have shown promising ability in sequence to sequence modeling tasks, showing a notable improvement in signal history encoding. Our comparative study uses these black box neural modeling techniques with a variety of audio effects. We evaluate the performance and limitations using multiple metrics aiming to assess the models' ability to accurately replicate energy envelopes, frequency contents, and transients in the audio signal. To incorporate control parameters we employ the Feature wise Linear Modulation method. Long Short Term Memory networks exhibit better accuracy in emulating distortions and equalizers, while the State Space model, followed by Long Short Term Memory networks when integrated in an encoder decoder structure, outperforms others in emulating saturation and compression. When considering long time variant characteristics, the State Space model demonstrates the greatest accuracy. The Long Short Term Memory and, in particular, Linear Recurrent Unit networks present more tendency to introduce audio artifacts.","sentences":["Analog electronic circuits are at the core of an important category of musical devices.","The nonlinear features of their electronic components give analog musical devices a distinctive timbre and sound quality, making them highly desirable.","Artificial neural networks have rapidly gained popularity for the emulation of analog audio effects circuits, particularly recurrent networks.","While neural approaches have been successful in accurately modeling distortion circuits, they require architectural improvements that account for parameter conditioning and low latency response.","In this article, we explore the application of recent machine learning advancements for virtual analog modeling.","We compare State Space models and Linear Recurrent Units against the more common Long Short Term Memory networks.","These have shown promising ability in sequence to sequence modeling tasks, showing a notable improvement in signal history encoding.","Our comparative study uses these black box neural modeling techniques with a variety of audio effects.","We evaluate the performance and limitations using multiple metrics aiming to assess the models' ability to accurately replicate energy envelopes, frequency contents, and transients in the audio signal.","To incorporate control parameters we employ the Feature wise Linear Modulation method.","Long Short Term Memory networks exhibit better accuracy in emulating distortions and equalizers, while the State Space model, followed by Long Short Term Memory networks when integrated in an encoder decoder structure, outperforms others in emulating saturation and compression.","When considering long time variant characteristics, the State Space model demonstrates the greatest accuracy.","The Long Short Term Memory and, in particular, Linear Recurrent Unit networks present more tendency to introduce audio artifacts."],"url":"http://arxiv.org/abs/2405.04124v2","category":"cs.SD"}
{"created":"2024-05-08 17:59:31","title":"Raman-phonon-polariton condensation in a transversely pumped cavity","abstract":"Phonon polaritons are hybrid states of light and matter that are typically realised when optically active phonons couple strongly to photons. We suggest a new approach to realising phonon polaritons, by employing a transverse-pumping Raman scheme, as used in experiments on cold atoms in optical cavities. This approach allows hybridisation between an optical cavity mode and any Raman-active phonon mode. Moreover, this approach enables one to tune the effective phonon-photon coupling by changing the strength of the transverse pumping light. We show that such a system may realise a phonon-polariton condensate. To do this, we find the stationary states and use Floquet theory to determine their stability. We thus identify distinct superradiant and lasing states in which the polariton modes are macroscopically populated. We map out the phase diagram of these states as a function of pump frequencies and strengths. Using parameters for transition metal dichalcogenides, we show that realisation of these phases may be practicably obtainable. The ability to manipulate phonon mode frequencies and attain steady-state populations of selected phonon modes provides a new tool for engineering correlated states of electrons.","sentences":["Phonon polaritons are hybrid states of light and matter that are typically realised when optically active phonons couple strongly to photons.","We suggest a new approach to realising phonon polaritons, by employing a transverse-pumping Raman scheme, as used in experiments on cold atoms in optical cavities.","This approach allows hybridisation between an optical cavity mode and any Raman-active phonon mode.","Moreover, this approach enables one to tune the effective phonon-photon coupling by changing the strength of the transverse pumping light.","We show that such a system may realise a phonon-polariton condensate.","To do this, we find the stationary states and use Floquet theory to determine their stability.","We thus identify distinct superradiant and lasing states in which the polariton modes are macroscopically populated.","We map out the phase diagram of these states as a function of pump frequencies and strengths.","Using parameters for transition metal dichalcogenides, we show that realisation of these phases may be practicably obtainable.","The ability to manipulate phonon mode frequencies and attain steady-state populations of selected phonon modes provides a new tool for engineering correlated states of electrons."],"url":"http://arxiv.org/abs/2405.05257v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 17:45:57","title":"Semi-infinite particle systems with exclusion interaction and heterogeneous jump rates","abstract":"We study semi-infinite particle systems on the one-dimensional integer lattice, where each particle performs a continuous-time nearest-neighbour random walk, with jump rates intrinsic to each particle, subject to an exclusion interaction which suppresses jumps that would lead to more than one particle occupying any site. Under appropriate hypotheses on the jump rates (uniformly bounded rates is sufficient) and started from an initial condition that is a finite perturbation of the close-packed configuration, we give conditions under which the particles evolve as a single, semi-infinite \"stable cloud\". More precisely, we show that inter-particle separations converge to a product-geometric stationary distribution, and that the location of every particle obeys a strong law of large numbers with the same characteristic speed.","sentences":["We study semi-infinite particle systems on the one-dimensional integer lattice, where each particle performs a continuous-time nearest-neighbour random walk, with jump rates intrinsic to each particle, subject to an exclusion interaction which suppresses jumps that would lead to more than one particle occupying any site.","Under appropriate hypotheses on the jump rates (uniformly bounded rates is sufficient) and started from an initial condition that is a finite perturbation of the close-packed configuration, we give conditions under which the particles evolve as a single, semi-infinite \"stable cloud\".","More precisely, we show that inter-particle separations converge to a product-geometric stationary distribution, and that the location of every particle obeys a strong law of large numbers with the same characteristic speed."],"url":"http://arxiv.org/abs/2405.05246v1","category":"math.PR"}
{"created":"2024-05-08 17:42:49","title":"Advancing Blockchain Scalability: A Linear Optimization Framework for Diversified Node Allocation in Shards","abstract":"Blockchain technology, while revolutionary in enabling decentralized transactions, faces scalability challenges as the ledger must be replicated across all nodes of the chain, limiting throughput and efficiency. Sharding, which divides the chain into smaller segments, called shards, offers a solution by enabling parallel transaction processing. However, sharding introduces new complexities, notably how to allocate nodes to shards without compromising the network's security.   This paper introduces a novel linear optimization framework for node allocation to shards that addresses decentralization constraints while minimizing resource consumption. In contrast to traditional methods that depend on random or trust-based assignments, our approach evaluates node characteristics, including ownership, hardware, and geographical distribution, and requires an explicit specification of decentralization targets with respect to these characteristics. By employing linear optimization, the framework identifies a resource-efficient node set meeting these targets. Adopted by the Internet Computer Protocol (ICP) community, this framework proves its utility in real-world blockchain applications. It provides a quantitative tool for node onboarding and offboarding decisions, balancing decentralization and resource considerations.","sentences":["Blockchain technology, while revolutionary in enabling decentralized transactions, faces scalability challenges as the ledger must be replicated across all nodes of the chain, limiting throughput and efficiency.","Sharding, which divides the chain into smaller segments, called shards, offers a solution by enabling parallel transaction processing.","However, sharding introduces new complexities, notably how to allocate nodes to shards without compromising the network's security.   ","This paper introduces a novel linear optimization framework for node allocation to shards that addresses decentralization constraints while minimizing resource consumption.","In contrast to traditional methods that depend on random or trust-based assignments, our approach evaluates node characteristics, including ownership, hardware, and geographical distribution, and requires an explicit specification of decentralization targets with respect to these characteristics.","By employing linear optimization, the framework identifies a resource-efficient node set meeting these targets.","Adopted by the Internet Computer Protocol (ICP) community, this framework proves its utility in real-world blockchain applications.","It provides a quantitative tool for node onboarding and offboarding decisions, balancing decentralization and resource considerations."],"url":"http://arxiv.org/abs/2405.05245v1","category":"cs.DC"}
{"created":"2024-05-08 17:36:14","title":"Cellular Traffic Prediction Using Online Prediction Algorithms","abstract":"The advent of 5G technology promises a paradigm shift in the realm of telecommunications, offering unprecedented speeds and connectivity. However, the efficient management of traffic in 5G networks remains a critical challenge. It is due to the dynamic and heterogeneous nature of network traffic, varying user behaviors, extended network size, and diverse applications, all of which demand highly accurate and adaptable prediction models to optimize network resource allocation and management. This paper investigates the efficacy of live prediction algorithms for forecasting cellular network traffic in real-time scenarios. We apply two live prediction algorithms on machine learning models, one of which is recently proposed Fast LiveStream Prediction (FLSP) algorithm. We examine the performance of these algorithms under two distinct data gathering methodologies: synchronous, where all network cells report statistics simultaneously, and asynchronous, where reporting occurs across consecutive time slots. Our study delves into the impact of these gathering scenarios on the predictive performance of traffic models. Our study reveals that the FLSP algorithm can halve the required bandwidth for asynchronous data reporting compared to conventional online prediction algorithms, while simultaneously enhancing prediction accuracy and reducing processing load. Additionally, we conduct a thorough analysis of algorithmic complexity and memory requirements across various machine learning models. Through empirical evaluation, we provide insights into the trade-offs inherent in different prediction strategies, offering valuable guidance for network optimization and resource allocation in dynamic environments.","sentences":["The advent of 5G technology promises a paradigm shift in the realm of telecommunications, offering unprecedented speeds and connectivity.","However, the efficient management of traffic in 5G networks remains a critical challenge.","It is due to the dynamic and heterogeneous nature of network traffic, varying user behaviors, extended network size, and diverse applications, all of which demand highly accurate and adaptable prediction models to optimize network resource allocation and management.","This paper investigates the efficacy of live prediction algorithms for forecasting cellular network traffic in real-time scenarios.","We apply two live prediction algorithms on machine learning models, one of which is recently proposed Fast LiveStream Prediction (FLSP) algorithm.","We examine the performance of these algorithms under two distinct data gathering methodologies: synchronous, where all network cells report statistics simultaneously, and asynchronous, where reporting occurs across consecutive time slots.","Our study delves into the impact of these gathering scenarios on the predictive performance of traffic models.","Our study reveals that the FLSP algorithm can halve the required bandwidth for asynchronous data reporting compared to conventional online prediction algorithms, while simultaneously enhancing prediction accuracy and reducing processing load.","Additionally, we conduct a thorough analysis of algorithmic complexity and memory requirements across various machine learning models.","Through empirical evaluation, we provide insights into the trade-offs inherent in different prediction strategies, offering valuable guidance for network optimization and resource allocation in dynamic environments."],"url":"http://arxiv.org/abs/2405.05239v1","category":"eess.SY"}
{"created":"2024-05-08 17:27:48","title":"Performance Bounds for Velocity Estimation with Large Antenna Arrays","abstract":"Joint communication and sensing (JCS) is envisioned as an enabler of future 6G networks. One of the key features of these networks will be the use of extremely large aperture arrays (ELAAs) and high operating frequencies, which will result in significant near-field propagation effects. This unique property can be harnessed to improve sensing capabilities. In this paper, we focus on velocity sensing, as using ELAAs allows the estimation of not just the radial component but also the transverse component. We derive analytical performance bounds for both velocity components, demonstrating how they are affected by the different system parameters and geometries. These insights offer a foundational understanding of how near-field effects play in velocity sensing differently from the far field and from position estimate.","sentences":["Joint communication and sensing (JCS) is envisioned as an enabler of future 6G networks.","One of the key features of these networks will be the use of extremely large aperture arrays (ELAAs) and high operating frequencies, which will result in significant near-field propagation effects.","This unique property can be harnessed to improve sensing capabilities.","In this paper, we focus on velocity sensing, as using ELAAs allows the estimation of not just the radial component but also the transverse component.","We derive analytical performance bounds for both velocity components, demonstrating how they are affected by the different system parameters and geometries.","These insights offer a foundational understanding of how near-field effects play in velocity sensing differently from the far field and from position estimate."],"url":"http://arxiv.org/abs/2405.05234v1","category":"eess.SP"}
{"created":"2024-05-08 17:27:11","title":"DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN Training","abstract":"Graph neural networks (GNNs) are machine learning models specialized for graph data and widely used in many applications. To train GNNs on large graphs that exceed CPU memory, several systems store data on disk and conduct out-of-core processing. However, these systems suffer from either read amplification when reading node features that are usually smaller than a disk page or degraded model accuracy by treating the graph as disconnected partitions. To close this gap, we build a system called DiskGNN, which achieves high I/O efficiency and thus fast training without hurting model accuracy. The key technique used by DiskGNN is offline sampling, which helps decouple graph sampling from model computation. In particular, by conducting graph sampling beforehand, DiskGNN acquires the node features that will be accessed by model computation, and such information is utilized to pack the target node features contiguously on disk to avoid read amplification. Besides, \\name{} also adopts designs including four-level feature store to fully utilize the memory hierarchy to cache node features and reduce disk access, batched packing to accelerate the feature packing process, and pipelined training to overlap disk access with other operations. We compare DiskGNN with Ginex and MariusGNN, which are state-of-the-art systems for out-of-core GNN training. The results show that DiskGNN can speed up the baselines by over 8x while matching their best model accuracy.","sentences":["Graph neural networks (GNNs) are machine learning models specialized for graph data and widely used in many applications.","To train GNNs on large graphs that exceed CPU memory, several systems store data on disk and conduct out-of-core processing.","However, these systems suffer from either read amplification when reading node features that are usually smaller than a disk page or degraded model accuracy by treating the graph as disconnected partitions.","To close this gap, we build a system called DiskGNN, which achieves high I/O efficiency and thus fast training without hurting model accuracy.","The key technique used by DiskGNN is offline sampling, which helps decouple graph sampling from model computation.","In particular, by conducting graph sampling beforehand, DiskGNN acquires the node features that will be accessed by model computation, and such information is utilized to pack the target node features contiguously on disk to avoid read amplification.","Besides, \\name{} also adopts designs including four-level feature store to fully utilize the memory hierarchy to cache node features and reduce disk access, batched packing to accelerate the feature packing process, and pipelined training to overlap disk access with other operations.","We compare DiskGNN with Ginex and MariusGNN, which are state-of-the-art systems for out-of-core GNN training.","The results show that DiskGNN can speed up the baselines by over 8x while matching their best model accuracy."],"url":"http://arxiv.org/abs/2405.05231v1","category":"cs.LG"}
{"created":"2024-05-08 17:14:07","title":"XMM-Newton observations of the extragalactic microquasar S26 and their implications for PeV cosmic rays","abstract":"The extragalactic microquasar S26 has the most powerful jets observed in accreting binaries, with a kinetic luminosity of $L_{\\rm jet}\\sim10^{40}\\,{\\rm erg\\,s^{-1}}$. According to the jet-disk symbiosis model, this implies that the accretion power to the stellar black hole at the core of the system should be very super-Eddington, on the order of $L_{\\rm acc}\\sim L_{\\rm jet}$. However, the observed X-ray flux of this system, measured by the \\textit{Chandra} and \\textit{XMM-Newton} telescopes, indicates an apparent very sub-Eddington accretion luminosity of $L_{\\rm X}\\approx 10^{37}\\,{\\rm erg\\,s^{-1}}$, orders of magnitude smaller than the jet power. We present here a preliminary investigation of the relationship between jet and disk power, analyze an X-ray observation of S26 obtained with \\textit{XMM-Newton}, and propose an explanation for the emission. We also examine the acceleration and distribution of the particles to discuss the feasibility of microquasars as potential PeVatron sources, exploring their ability to produce cosmic rays with energies of about 1 PeV or higher.","sentences":["The extragalactic microquasar S26 has the most powerful jets observed in accreting binaries, with a kinetic luminosity of $L_{\\rm jet}\\sim10^{40}\\,{\\rm erg\\,s^{-1}}$.","According to the jet-disk symbiosis model, this implies that the accretion power to the stellar black hole at the core of the system should be very super-Eddington, on the order of $L_{\\rm acc}\\sim L_{\\rm jet}$.","However, the observed X-ray flux of this system, measured by the \\textit{Chandra} and \\textit{XMM-Newton} telescopes, indicates an apparent very sub-Eddington accretion luminosity of $L_{\\rm X}\\approx 10^{37}\\,{\\rm","erg\\,s^{-1}}$, orders of magnitude smaller than the jet power.","We present here a preliminary investigation of the relationship between jet and disk power, analyze an X-ray observation of S26 obtained with \\textit{XMM-Newton}, and propose an explanation for the emission.","We also examine the acceleration and distribution of the particles to discuss the feasibility of microquasars as potential PeVatron sources, exploring their ability to produce cosmic rays with energies of about 1 PeV or higher."],"url":"http://arxiv.org/abs/2405.05221v1","category":"astro-ph.HE"}
{"created":"2024-05-08 17:09:58","title":"Disorder-resilient transport through dopant arrays in silicon","abstract":"Chains and arrays of phosphorus donors in silicon have recently been used to demonstrate dopant-based quantum simulators. The dopant disorder present in fabricated devices must be accounted for. Here, we theoretically study transport through disordered donor-based $3\\times 3$ arrays that model recent experimental results. We employ a theory that combines the exact diagonalization of an extended Hubbard model of the array with a non-equilibrium Green's function formalism to model transport in interacting systems. We show that current flow through the array and features of measured stability diagrams are highly resilient to disorder. We interpret this as an emergence of uncomplicated behavior in the multi-electron system dominated by strong correlations, regardless of array filling, where the current follows the shortest paths between source and drain sites that avoid possible obstacles. The reference $3\\times 3$ array has transport properties very similar to three parallel 3-site chains coupled only by interchain Coulomb interaction, which indicates a challenge in characterizing such devices.","sentences":["Chains and arrays of phosphorus donors in silicon have recently been used to demonstrate dopant-based quantum simulators.","The dopant disorder present in fabricated devices must be accounted for.","Here, we theoretically study transport through disordered donor-based $3\\times 3$ arrays that model recent experimental results.","We employ a theory that combines the exact diagonalization of an extended Hubbard model of the array with a non-equilibrium Green's function formalism to model transport in interacting systems.","We show that current flow through the array and features of measured stability diagrams are highly resilient to disorder.","We interpret this as an emergence of uncomplicated behavior in the multi-electron system dominated by strong correlations, regardless of array filling, where the current follows the shortest paths between source and drain sites that avoid possible obstacles.","The reference $3\\times 3$ array has transport properties very similar to three parallel 3-site chains coupled only by interchain Coulomb interaction, which indicates a challenge in characterizing such devices."],"url":"http://arxiv.org/abs/2405.05217v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 17:09:03","title":"FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models","abstract":"The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \\textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.","sentences":["The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space.","Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task.","Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts.","To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \\textbf{FinePOSE}.","It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance.","(2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality.","(3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step.","Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods.","We further extend FinePOSE to multi-human pose estimation.","Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios.","Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024."],"url":"http://arxiv.org/abs/2405.05216v1","category":"cs.CV"}
{"created":"2024-05-08 17:07:56","title":"Restricted Randomized Benchmarking with Universal Gates of Fixed Sequence Length","abstract":"The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible. Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation. We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one. This makes our protocol highly resource efficient and practical for small qubit numbers. We exemplify our protocol for creating Haar-randomness in the case of single and two qubits. Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique. We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise.","sentences":["The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible.","Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation.","We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one.","This makes our protocol highly resource efficient and practical for small qubit numbers.","We exemplify our protocol for creating Haar-randomness in the case of single and two qubits.","Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique.","We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise."],"url":"http://arxiv.org/abs/2405.05215v1","category":"quant-ph"}
{"created":"2024-05-08 17:01:41","title":"Exponential time propagators for elastodynamics","abstract":"We propose a computationally efficient and systematically convergent approach for elastodynamics simulations. We recast the second-order dynamical equation of elastodynamics into an equivalent first-order system of coupled equations, so as to express the solution in the form of a Magnus expansion. With any spatial discretization, it entails computing the exponential of a matrix acting upon a vector. We employ an adaptive Krylov subspace approach to inexpensively and and accurately evaluate the action of the exponential matrix on a vector. In particular, we use an apriori error estimate to predict the optimal Kyrlov subspace size required for each time-step size. We show that the Magnus expansion truncated after its first term provides quadratic and superquadratic convergence in the time-step for nonlinear and linear elastodynamics, respectively. We demonstrate the accuracy and efficiency of the proposed method for one linear (linear cantilever beam) and three nonlinear (nonlinear cantilever beam, soft tissue elastomer, and hyperelastic rubber) benchmark systems. For a desired accuracy in energy, displacement, and velocity, our method allows for $10-100\\times$ larger time-steps than conventional time-marching schemes such as Newmark-$\\beta$ method. Computationally, it translates to a $\\sim$$1000\\times$ and $\\sim$$10-100\\times$ speed-up over conventional time-marching schemes for linear and nonlinear elastodynamics, respectively.","sentences":["We propose a computationally efficient and systematically convergent approach for elastodynamics simulations.","We recast the second-order dynamical equation of elastodynamics into an equivalent first-order system of coupled equations, so as to express the solution in the form of a Magnus expansion.","With any spatial discretization, it entails computing the exponential of a matrix acting upon a vector.","We employ an adaptive Krylov subspace approach to inexpensively and and accurately evaluate the action of the exponential matrix on a vector.","In particular, we use an apriori error estimate to predict the optimal Kyrlov subspace size required for each time-step size.","We show that the Magnus expansion truncated after its first term provides quadratic and superquadratic convergence in the time-step for nonlinear and linear elastodynamics, respectively.","We demonstrate the accuracy and efficiency of the proposed method for one linear (linear cantilever beam) and three nonlinear (nonlinear cantilever beam, soft tissue elastomer, and hyperelastic rubber) benchmark systems.","For a desired accuracy in energy, displacement, and velocity, our method allows for $10-100\\times$ larger time-steps than conventional time-marching schemes such as Newmark-$\\beta$ method.","Computationally, it translates to a $\\sim$$1000\\times$ and $\\sim$$10-100\\times$ speed-up over conventional time-marching schemes for linear and nonlinear elastodynamics, respectively."],"url":"http://arxiv.org/abs/2405.05213v1","category":"math.NA"}
{"created":"2024-05-08 16:58:22","title":"MOTLEE: Collaborative Multi-Object Tracking Using Temporal Consistency for Neighboring Robot Frame Alignment","abstract":"Knowing the locations of nearby moving objects is important for a mobile robot to operate safely in a dynamic environment. Dynamic object tracking performance can be improved if robots share observations of tracked objects with nearby team members in real-time. To share observations, a robot must make up-to-date estimates of the transformation from its coordinate frame to the frame of each neighbor, which can be challenging because of odometry drift. We present Multiple Object Tracking with Localization Error Elimination (MOTLEE), a complete system for a multi-robot team to accurately estimate frame transformations and collaboratively track dynamic objects. To accomplish this, robots use open-set image-segmentation methods to build object maps of their environment and then use our Temporally Consistent Alignment of Frames Filter (TCAFF) to align maps and estimate coordinate frame transformations without any initial knowledge of neighboring robot poses. We show that our method for aligning frames enables a team of four robots to collaboratively track six pedestrians with accuracy similar to that of a system with ground truth localization in a challenging hardware demonstration. The code and hardware dataset are available at https://github.com/mit-acl/motlee.","sentences":["Knowing the locations of nearby moving objects is important for a mobile robot to operate safely in a dynamic environment.","Dynamic object tracking performance can be improved if robots share observations of tracked objects with nearby team members in real-time.","To share observations, a robot must make up-to-date estimates of the transformation from its coordinate frame to the frame of each neighbor, which can be challenging because of odometry drift.","We present Multiple Object Tracking with Localization Error Elimination (MOTLEE), a complete system for a multi-robot team to accurately estimate frame transformations and collaboratively track dynamic objects.","To accomplish this, robots use open-set image-segmentation methods to build object maps of their environment and then use our Temporally Consistent Alignment of Frames Filter (TCAFF) to align maps and estimate coordinate frame transformations without any initial knowledge of neighboring robot poses.","We show that our method for aligning frames enables a team of four robots to collaboratively track six pedestrians with accuracy similar to that of a system with ground truth localization in a challenging hardware demonstration.","The code and hardware dataset are available at https://github.com/mit-acl/motlee."],"url":"http://arxiv.org/abs/2405.05210v1","category":"cs.RO"}
{"created":"2024-05-08 16:40:18","title":"CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation","abstract":"Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs. The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation. These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers. Results: Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of .84 and .79, positive-label recall and precision values ranging from .71 to .86, and accuracies aligning closely with human annotator agreement (.87). Discussion: This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text, and identified trends in stigmatizing language use in an EHR setting. Additional labeled data may help improve lower scare quote model performance. Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems.","sentences":["Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques.","Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs.","The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation.","These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset.","For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers.","Results:","Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions.","Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of .84 and .79, positive-label recall and precision values ranging from .71 to .86, and accuracies aligning closely with human annotator agreement (.87).","Discussion:","This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text, and identified trends in stigmatizing language use in an EHR setting.","Additional labeled data may help improve lower scare quote model performance.","Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems."],"url":"http://arxiv.org/abs/2405.05204v1","category":"cs.CL"}
{"created":"2024-05-08 16:21:48","title":"Exploring the limits of the law of mass action in the mean field description of epidemics on Erd\u00f6s-R\u00e9nyi networks","abstract":"The manner epidemics occurs in a social network depends on various elements, with two of the most influential being the relationships among individuals in the population and the mechanism of transmission. In this paper, we assume that the social network has a homogeneous random topology of Erd\\\"os-R\\'enyi type. Regarding the contagion process, we assume that the probability of infection is proportional to the proportion of infected neighbours.   We consider a constant population, whose individuals are the nodes of the social network, formed by two variable subpopulations: Susceptible and Infected (SI model). We simulate the epidemics on this random network and study whether the average dynamics can be described using a mean field approach in terms of Differential Equations, employing the law of mass action. We show that a macroscopic description could be applied for low average connectivity, adjusting the value of the contagion rate in a precise function. This dependence is illustrated by calculating the transient times for each connectivity.   This study contributes valuable insights into the interplay between network connectivity, contagion dynamics, and the applicability of mean-field approximations. The delineation of critical thresholds and the distinctive behaviour at lower connectivity enable a deeper understanding of epidemic dynamics.","sentences":["The manner epidemics occurs in a social network depends on various elements, with two of the most influential being the relationships among individuals in the population and the mechanism of transmission.","In this paper, we assume that the social network has a homogeneous random topology of Erd\\\"os-R\\'enyi type.","Regarding the contagion process, we assume that the probability of infection is proportional to the proportion of infected neighbours.   ","We consider a constant population, whose individuals are the nodes of the social network, formed by two variable subpopulations: Susceptible and Infected (SI model).","We simulate the epidemics on this random network and study whether the average dynamics can be described using a mean field approach in terms of Differential Equations, employing the law of mass action.","We show that a macroscopic description could be applied for low average connectivity, adjusting the value of the contagion rate in a precise function.","This dependence is illustrated by calculating the transient times for each connectivity.   ","This study contributes valuable insights into the interplay between network connectivity, contagion dynamics, and the applicability of mean-field approximations.","The delineation of critical thresholds and the distinctive behaviour at lower connectivity enable a deeper understanding of epidemic dynamics."],"url":"http://arxiv.org/abs/2405.05186v1","category":"math.DS"}
{"created":"2024-05-08 16:20:47","title":"Machine Learning Assisted Dynamical Classification of Trans-Neptunian Objects","abstract":"Trans-Neptunian objects (TNOs) are small, icy bodies in the outer solar system. They are observed to have a complex orbital distribution that was shaped by the early dynamical history and migration of the giant planets. Comparisons between the different dynamical classes of modeled and observed TNOs can help constrain the history of the outer solar system. Because of the complex dynamics of TNOs, particularly those in and near mean motion resonances with Neptune, classification has traditionally been done by human inspection of plots of the time evolution of orbital parameters. This is very inefficient. The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to increase the number of known TNOs by a factor of $\\sim$10, necessitating a much more automated process. In this chapter we present an improved supervised machine learning classifier for TNOs. Using a large and diverse training set as well as carefully chosen, dynamically motivated data features calculated from numerical integrations of TNO orbits, our classifier returns results that match those of a human classifier 98% of the time, and dynamically relevant classifications 99.7% of the time. This classifier is dramatically more efficient than human classification, and it will improve classification of both observed and modeled TNO data.","sentences":["Trans-Neptunian objects (TNOs) are small, icy bodies in the outer solar system.","They are observed to have a complex orbital distribution that was shaped by the early dynamical history and migration of the giant planets.","Comparisons between the different dynamical classes of modeled and observed TNOs can help constrain the history of the outer solar system.","Because of the complex dynamics of TNOs, particularly those in and near mean motion resonances with Neptune, classification has traditionally been done by human inspection of plots of the time evolution of orbital parameters.","This is very inefficient.","The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to increase the number of known TNOs by a factor of $\\sim$10, necessitating a much more automated process.","In this chapter we present an improved supervised machine learning classifier for TNOs.","Using a large and diverse training set as well as carefully chosen, dynamically motivated data features calculated from numerical integrations of TNO orbits, our classifier returns results that match those of a human classifier 98% of the time, and dynamically relevant classifications 99.7% of the time.","This classifier is dramatically more efficient than human classification, and it will improve classification of both observed and modeled TNO data."],"url":"http://arxiv.org/abs/2405.05185v1","category":"astro-ph.EP"}
{"created":"2024-05-08 16:15:19","title":"Unclocklike biological oscillators with frequency memory","abstract":"Entrainment experiments on the vertebrate segmentation clock have revealed that embryonic oscillators actively change their internal frequency to adapt to the driving signal. This is neither consistent with a one-dimensional clock model nor with a limit-cycle model, but rather suggests a new \"unclocklike\" behavior. In this work, we propose simple biologically realistic descriptions of such internal frequency adaptation, where a phase oscillator activates a memory variable controlling the oscillator's frequency. We study two opposite limits for the control of the memory variable, one with a smooth phase-averaging memory field, and the other with a pulsatile, phase-dependent activation. Both models recapitulate intriguing properties of the entrained segmentation clock, such as very broad Arnold tongues and an entrainment phase plateauing with detuning. We compute analytically multiple properties of such systems, such as the entrainment phases and cycle shapes. We further describe new phenomena, including hysteresis in entrainment, bistability in the frequency of the entrained oscillator, and probabilistic entrainment. Our work shows oscillators with frequency memory can exhibit new classes of unclocklike properties, that can be tested experimentally.","sentences":["Entrainment experiments on the vertebrate segmentation clock have revealed that embryonic oscillators actively change their internal frequency to adapt to the driving signal.","This is neither consistent with a one-dimensional clock model nor with a limit-cycle model, but rather suggests a new \"unclocklike\" behavior.","In this work, we propose simple biologically realistic descriptions of such internal frequency adaptation, where a phase oscillator activates a memory variable controlling the oscillator's frequency.","We study two opposite limits for the control of the memory variable, one with a smooth phase-averaging memory field, and the other with a pulsatile, phase-dependent activation.","Both models recapitulate intriguing properties of the entrained segmentation clock, such as very broad Arnold tongues and an entrainment phase plateauing with detuning.","We compute analytically multiple properties of such systems, such as the entrainment phases and cycle shapes.","We further describe new phenomena, including hysteresis in entrainment, bistability in the frequency of the entrained oscillator, and probabilistic entrainment.","Our work shows oscillators with frequency memory can exhibit new classes of unclocklike properties, that can be tested experimentally."],"url":"http://arxiv.org/abs/2405.05180v1","category":"physics.bio-ph"}
{"created":"2024-05-08 16:13:49","title":"Network mutual information measures for graph similarity","abstract":"A wide range of tasks in exploratory network analysis and machine learning, such as clustering network populations or identifying anomalies in temporal graph streams, require a measure of the similarity between two graphs. To provide a meaningful data summary for downstream scientific analyses, the graph similarity measures used in these unsupervised settings must be principled, interpretable, and capable of distinguishing meaningful overlapping network structure from statistical noise at different scales of interest. Here we derive a family of graph mutual information measures that satisfy these criteria and are constructed using only fundamental information theoretic principles. Our measures capture the information shared among networks according to different encodings of their structural information, with our mesoscale mutual information measure allowing for network comparison under any specified network coarse-graining. We test our measures in a range of applications on real and synthetic network data, finding that they effectively highlight intuitive aspects of network similarity across scales in a variety of systems.","sentences":["A wide range of tasks in exploratory network analysis and machine learning, such as clustering network populations or identifying anomalies in temporal graph streams, require a measure of the similarity between two graphs.","To provide a meaningful data summary for downstream scientific analyses, the graph similarity measures used in these unsupervised settings must be principled, interpretable, and capable of distinguishing meaningful overlapping network structure from statistical noise at different scales of interest.","Here we derive a family of graph mutual information measures that satisfy these criteria and are constructed using only fundamental information theoretic principles.","Our measures capture the information shared among networks according to different encodings of their structural information, with our mesoscale mutual information measure allowing for network comparison under any specified network coarse-graining.","We test our measures in a range of applications on real and synthetic network data, finding that they effectively highlight intuitive aspects of network similarity across scales in a variety of systems."],"url":"http://arxiv.org/abs/2405.05177v1","category":"physics.soc-ph"}
{"created":"2024-05-08 16:12:04","title":"The local cohomology of vector fields","abstract":"We compute the local cohomology of vector fields on a manifold. In the smooth case this recovers the diagonal cohomology studied in work of Losik, Guillemin, Fuks and others. In the holomorphic case this cohomology has recently appeared in work of Hennion and Kapranov in their study of the Lie algebra cohomology of vector fields on a complex manifold. Additionally, we construct explicit representatives for cocycles in Gelfand--Fuks cohomology via descent.","sentences":["We compute the local cohomology of vector fields on a manifold.","In the smooth case this recovers the diagonal cohomology studied in work of Losik, Guillemin, Fuks and others.","In the holomorphic case this cohomology has recently appeared in work of Hennion and Kapranov in their study of the Lie algebra cohomology of vector fields on a complex manifold.","Additionally, we construct explicit representatives for cocycles in Gelfand--Fuks cohomology via descent."],"url":"http://arxiv.org/abs/2405.05174v1","category":"math.DG"}
{"created":"2024-05-08 16:04:47","title":"Riemann problem for polychromatic soliton gases: a testbed for the spectral kinetic theory","abstract":"We use Riemann problem for soliton gas as a benchmark for a detailed numerical validation of the spectral kinetic theory for the Korteweg-de Vries (KdV) and the focusing nonlinear Schr\\\"odinger (fNLS) equations. We construct weak solutions to the kinetic equation for soliton gas describing collision of two dense \"polychromatic\" soliton gases composed of a finite number of \"monochromatic\" components, each consisting of solitons with nearly identical spectral parameters of the scattering operator in the Lax pair. The interaction between the gas components plays the key role in the emergent, large-scale hydrodynamic evolution. We then use the solutions of the spectral kinetic equation to evaluate macroscopic physical observables in KdV and fNLS soliton gases and compare them with the respective ensemble averages extracted from the \"exact\" soliton gas numerical solutions of the KdV and fNLS equations. To numerically synthesise dense polychromatic soliton gases we develop a new method which combines recent advances in the spectral theory of the so-called soliton condensates and the effective algorithms for the numerical realisation of $n$-soliton solutions with large $n$.","sentences":["We use Riemann problem for soliton gas as a benchmark for a detailed numerical validation of the spectral kinetic theory for the Korteweg-de Vries (KdV) and the focusing nonlinear Schr\\\"odinger (fNLS) equations.","We construct weak solutions to the kinetic equation for soliton gas describing collision of two dense \"polychromatic\" soliton gases composed of a finite number of \"monochromatic\" components, each consisting of solitons with nearly identical spectral parameters of the scattering operator in the Lax pair.","The interaction between the gas components plays the key role in the emergent, large-scale hydrodynamic evolution.","We then use the solutions of the spectral kinetic equation to evaluate macroscopic physical observables in KdV and fNLS soliton gases and compare them with the respective ensemble averages extracted from the \"exact\" soliton gas numerical solutions of the KdV and fNLS equations.","To numerically synthesise dense polychromatic soliton gases we develop a new method which combines recent advances in the spectral theory of the so-called soliton condensates and the effective algorithms for the numerical realisation of $n$-soliton solutions with large $n$."],"url":"http://arxiv.org/abs/2405.05166v1","category":"nlin.PS"}
{"created":"2024-05-08 16:02:29","title":"Discovery of T center-like quantum defects in silicon","abstract":"Quantum technologies would benefit from the development of high performance quantum defects acting as single-photon emitters or spin-photon interface. Finding such a quantum defect in silicon is especially appealing in view of its favorable spin bath and high processability. While some color centers in silicon have been emerging in quantum applications, there is still a need to search and develop new high performance quantum emitters. Searching a high-throughput computational database of more than 22,000 charged complex defects in silicon, we identify a series of defects formed by a group III element combined with carbon ((A-C)$\\rm _{Si}$ with A=B,Al,Ga,In,Tl) and substituting on a silicon site. These defects are analogous structurally, electronically and chemically to the well-known T center in silicon ((C-C-H)$\\rm_{Si}$) and their optical properties are mainly driven by an unpaired electron in a carbon $p$ orbital. They all emit in the telecom and some of these color centers show improved properties compared to the T center in terms of computed radiative lifetime or emission efficiency. We also show that the synthesis of hydrogenated T center-like defects followed by a dehydrogenation annealing step could be an efficient way of synthesis. All the T center-like defects show a higher symmetry than the T center making them easier to align with magnetic fields. Our work motivates further studies on the synthesis and control of this new family of quantum defects, and also demonstrates the use of high-throughput computational screening to detect new complex quantum defects.","sentences":["Quantum technologies would benefit from the development of high performance quantum defects acting as single-photon emitters or spin-photon interface.","Finding such a quantum defect in silicon is especially appealing in view of its favorable spin bath and high processability.","While some color centers in silicon have been emerging in quantum applications, there is still a need to search and develop new high performance quantum emitters.","Searching a high-throughput computational database of more than 22,000 charged complex defects in silicon, we identify a series of defects formed by a group III element combined with carbon ((A-C)$\\rm _{Si}$ with A=B,Al,Ga,In,Tl) and substituting on a silicon site.","These defects are analogous structurally, electronically and chemically to the well-known T center in silicon ((C-C-H)$\\rm_{Si}$) and their optical properties are mainly driven by an unpaired electron in a carbon $p$ orbital.","They all emit in the telecom and some of these color centers show improved properties compared to the T center in terms of computed radiative lifetime or emission efficiency.","We also show that the synthesis of hydrogenated T center-like defects followed by a dehydrogenation annealing step could be an efficient way of synthesis.","All the T center-like defects show a higher symmetry than the T center making them easier to align with magnetic fields.","Our work motivates further studies on the synthesis and control of this new family of quantum defects, and also demonstrates the use of high-throughput computational screening to detect new complex quantum defects."],"url":"http://arxiv.org/abs/2405.05165v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 15:54:35","title":"Fast Fourier transforms and fast Wigner and Weyl functions in large quantum systems","abstract":"Two methods for fast Fourier transforms are used in a quantum context. The first method is for systems with dimension of the Hilbert space $D=d^n$ with $d$ an odd integer, and is inspired by the Cooley-Tukey formalism. The `large Fourier transform' is expressed as a sequence of $n$ `small Fourier transforms' (together with some other transforms) in quantum systems with $d$-dimensional Hilbert space. Limitations of the method are discussed. In some special cases, the $n$ Fourier transforms can be performed in parallel. The second method is for systems with dimension of the Hilbert space $D=d_0...d_{n-1}$ with $d_0,...,d_{n-1}$ odd integers coprime to each other. It is inspired by the Good formalism, which in turn is based on the Chinese reminder theorem. In this case also the `large Fourier transform' is expressed as a sequence of $n$ `small Fourier transforms' (that involve some constants related to the number theory that describes the formalism). The `small Fourier transforms' can be performed in a classical computer or in a quantum computer (in which case we have the additional well known advantages of quantum Fourier transform circuits). In the case that the small Fourier transforms are performed with a classical computer, complexity arguments for both methods show the reduction in computational time from ${\\cal O}(D^2)$ to ${\\cal O}(D\\log D)$. The second method is also used for the fast calculation of Wigner and Weyl functions, in quantum systems with large finite dimension of the Hilbert space.","sentences":["Two methods for fast Fourier transforms are used in a quantum context.","The first method is for systems with dimension of the Hilbert space $D=d^n$ with $d$ an odd integer, and is inspired by the Cooley-Tukey formalism.","The `large Fourier transform' is expressed as a sequence of $n$ `small Fourier transforms' (together with some other transforms) in quantum systems with $d$-dimensional Hilbert space.","Limitations of the method are discussed.","In some special cases, the $n$ Fourier transforms can be performed in parallel.","The second method is for systems with dimension of the Hilbert space $D=d_0...","d_{n-1}$ with $d_0,...,d_{n-1}$ odd integers coprime to each other.","It is inspired by the Good formalism, which in turn is based on the Chinese reminder theorem.","In this case also the `large Fourier transform' is expressed as a sequence of $n$ `small Fourier transforms' (that involve some constants related to the number theory that describes the formalism).","The `small Fourier transforms' can be performed in a classical computer or in a quantum computer (in which case we have the additional well known advantages of quantum Fourier transform circuits).","In the case that the small Fourier transforms are performed with a classical computer, complexity arguments for both methods show the reduction in computational time from ${\\cal O}(D^2)$ to ${\\cal O}(D\\log D)$. The second method is also used for the fast calculation of Wigner and Weyl functions, in quantum systems with large finite dimension of the Hilbert space."],"url":"http://arxiv.org/abs/2405.05163v1","category":"quant-ph"}
{"created":"2024-05-08 15:54:19","title":"A Dual-Motor Actuator for Ceiling Robots with High Force and High Speed Capabilities","abstract":"Patient transfer devices allow to move patients passively in hospitals and care centers. Instead of hoisting the patient, it would be beneficial in some cases to assist their movement, enabling them to move by themselves. However, patient assistance requires devices capable of precisely controlling output forces at significantly higher speeds than those used for patient transfers alone, and a single motor solution would be over-sized and show poor efficiency to do both functions. This paper presents a dual-motor actuator and control schemes adapted for a patient mobility equipment that can be used to transfer patients, assist patient in their movement, and help prevent falls. The prototype is shown to be able to lift patients weighing up to 318 kg, to assist a patient with a desired force of up to 100 kg with a precision of 7.8%. Also, a smart control scheme to manage falls is shown to be able to stop a patient who is falling by applying a desired deceleration.","sentences":["Patient transfer devices allow to move patients passively in hospitals and care centers.","Instead of hoisting the patient, it would be beneficial in some cases to assist their movement, enabling them to move by themselves.","However, patient assistance requires devices capable of precisely controlling output forces at significantly higher speeds than those used for patient transfers alone, and a single motor solution would be over-sized and show poor efficiency to do both functions.","This paper presents a dual-motor actuator and control schemes adapted for a patient mobility equipment that can be used to transfer patients, assist patient in their movement, and help prevent falls.","The prototype is shown to be able to lift patients weighing up to 318 kg, to assist a patient with a desired force of up to 100 kg with a precision of 7.8%.","Also, a smart control scheme to manage falls is shown to be able to stop a patient who is falling by applying a desired deceleration."],"url":"http://arxiv.org/abs/2405.05162v1","category":"cs.RO"}
{"created":"2024-05-08 15:47:38","title":"Crystal structure identification with 3D convolutional neural networks with application to high-pressure phase transitions in SiO$_2$","abstract":"Efficient, reliable and easy-to-use structure recognition of atomic environments is essential for the analysis of atomic scale computer simulations. In this work, we train two neuronal network (NN) architectures, namely PointNet and dynamic graph convolutional NN (DG-CNN) using different hyperparameters and training regimes to assess their performance in structure identification tasks of atomistic structure data. We show benchmarks on simple crystal structures, where we can compare against established methods. The approach is subsequently extended to structurally more complex SiO$_2$ phases. By making use of this structure recognition tool, we are able to achieve a deeper understanding of the crystallization process in amorphous SiO$_2$ under shock compression. Lastly, we show how the NN based structure identification workflows can be integrated into OVITO using its python interface.","sentences":["Efficient, reliable and easy-to-use structure recognition of atomic environments is essential for the analysis of atomic scale computer simulations.","In this work, we train two neuronal network (NN) architectures, namely PointNet and dynamic graph convolutional NN (DG-CNN) using different hyperparameters and training regimes to assess their performance in structure identification tasks of atomistic structure data.","We show benchmarks on simple crystal structures, where we can compare against established methods.","The approach is subsequently extended to structurally more complex SiO$_2$ phases.","By making use of this structure recognition tool, we are able to achieve a deeper understanding of the crystallization process in amorphous SiO$_2$ under shock compression.","Lastly, we show how the NN based structure identification workflows can be integrated into OVITO using its python interface."],"url":"http://arxiv.org/abs/2405.05156v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 15:41:40","title":"Principal Component Analysis for Spatial Phase Reconstruction in Atom Interferometry","abstract":"Atom interferometers are sensitive to a wide range of forces by encoding their signals in interference patterns of matter waves. To estimate the magnitude of these forces, the underlying phase shifts they imprint on the atoms must be extracted. Up until now, extraction algorithms typically rely on a fixed model of the patterns' spatial structure, which if inaccurate can lead to systematic errors caused by, for example, wavefront aberrations of the used lasers. In this paper we employ an algorithm based on Principal Component Analysis, which is capable of characterizing the spatial phase structure and per image phase offsets of an atom interferometer from a set of images. The algorithm does so without any prior knowledge about the specific spatial pattern as long as this pattern is the same for all images in the set. On simulated images with atom projection noise we show the algorithm's reconstruction performance follows distinct scaling laws, i.e., it is inversely-proportional to the square-root of the number atoms or the number of images respectively, which allows a projection of its performance for experiments. We also successfully extract the spatial phase patterns of two experimental data sets from an atom gravimeter. This algorithm is a first step towards a better understanding and complex spatial phase patterns, e.g., caused by inhomogeneous laser fields in atom interferometry.","sentences":["Atom interferometers are sensitive to a wide range of forces by encoding their signals in interference patterns of matter waves.","To estimate the magnitude of these forces, the underlying phase shifts they imprint on the atoms must be extracted.","Up until now, extraction algorithms typically rely on a fixed model of the patterns' spatial structure, which if inaccurate can lead to systematic errors caused by, for example, wavefront aberrations of the used lasers.","In this paper we employ an algorithm based on Principal Component Analysis, which is capable of characterizing the spatial phase structure and per image phase offsets of an atom interferometer from a set of images.","The algorithm does so without any prior knowledge about the specific spatial pattern as long as this pattern is the same for all images in the set.","On simulated images with atom projection noise we show the algorithm's reconstruction performance follows distinct scaling laws, i.e., it is inversely-proportional to the square-root of the number atoms or the number of images respectively, which allows a projection of its performance for experiments.","We also successfully extract the spatial phase patterns of two experimental data sets from an atom gravimeter.","This algorithm is a first step towards a better understanding and complex spatial phase patterns, e.g., caused by inhomogeneous laser fields in atom interferometry."],"url":"http://arxiv.org/abs/2405.05150v1","category":"physics.atom-ph"}
{"created":"2024-05-08 15:27:58","title":"DenserRadar: A 4D millimeter-wave radar point cloud detector based on dense LiDAR point clouds","abstract":"The 4D millimeter-wave (mmWave) radar, with its robustness in extreme environments, extensive detection range, and capabilities for measuring velocity and elevation, has demonstrated significant potential for enhancing the perception abilities of autonomous driving systems in corner-case scenarios. Nevertheless, the inherent sparsity and noise of 4D mmWave radar point clouds restrict its further development and practical application. In this paper, we introduce a novel 4D mmWave radar point cloud detector, which leverages high-resolution dense LiDAR point clouds. Our approach constructs dense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a specially designed network named DenserRadar. The proposed method surpasses existing probability-based and learning-based radar point cloud detectors in terms of both point cloud density and accuracy on the K-Radar dataset.","sentences":["The 4D millimeter-wave (mmWave) radar, with its robustness in extreme environments, extensive detection range, and capabilities for measuring velocity and elevation, has demonstrated significant potential for enhancing the perception abilities of autonomous driving systems in corner-case scenarios.","Nevertheless, the inherent sparsity and noise of 4D mmWave radar point clouds restrict its further development and practical application.","In this paper, we introduce a novel 4D mmWave radar point cloud detector, which leverages high-resolution dense LiDAR point clouds.","Our approach constructs dense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a specially designed network named DenserRadar.","The proposed method surpasses existing probability-based and learning-based radar point cloud detectors in terms of both point cloud density and accuracy on the K-Radar dataset."],"url":"http://arxiv.org/abs/2405.05131v1","category":"cs.RO"}
{"created":"2024-05-08 15:21:44","title":"Correlation and Autocorrelation of Data on Complex Networks","abstract":"Networks where each node has one or more associated numerical values are common in applications. This work studies how summary statistics used for the analysis of spatial data can be applied to non-spatial networks for the purposes of exploratory data analysis. We focus primarily on Moran-type statistics and discuss measures of global autocorrelation, local autocorrelation and global correlation. We introduce null models based on fixing edges and permuting the data or fixing the data and permuting the edges. We demonstrate the use of these statistics on real and synthetic node-valued networks.","sentences":["Networks where each node has one or more associated numerical values are common in applications.","This work studies how summary statistics used for the analysis of spatial data can be applied to non-spatial networks for the purposes of exploratory data analysis.","We focus primarily on Moran-type statistics and discuss measures of global autocorrelation, local autocorrelation and global correlation.","We introduce null models based on fixing edges and permuting the data or fixing the data and permuting the edges.","We demonstrate the use of these statistics on real and synthetic node-valued networks."],"url":"http://arxiv.org/abs/2405.05125v1","category":"cs.SI"}
{"created":"2024-05-08 15:21:02","title":"A Gauss-Newton Method for ODE Optimal Tracking Control","abstract":"This paper introduces and analyses a continuous optimization approach to solve optimal control problems involving ordinary differential equations (ODEs) and tracking type objectives. Our aim is to determine control or input functions, and potentially uncertain model parameters, for a dynamical system described by an ODE. We establish the mathematical framework and define the optimal control problem with a tracking functional, incorporating regularization terms and box-constraints for model parameters and input functions. Treating the problem as an infinite-dimensional optimization problem, we employ a Gauss-Newton method within a suitable function space framework. This leads to an iterative process where, at each step, we solve a linearization of the problem by considering a linear surrogate model around the current solution estimate. The resulting linear auxiliary problem resembles a linear-quadratic ODE optimal tracking control problem, which we tackle using either a gradient descent method in function spaces or a Riccati-based approach. Finally, we present and analyze the efficacy of our method through numerical experiments.","sentences":["This paper introduces and analyses a continuous optimization approach to solve optimal control problems involving ordinary differential equations (ODEs) and tracking type objectives.","Our aim is to determine control or input functions, and potentially uncertain model parameters, for a dynamical system described by an ODE.","We establish the mathematical framework and define the optimal control problem with a tracking functional, incorporating regularization terms and box-constraints for model parameters and input functions.","Treating the problem as an infinite-dimensional optimization problem, we employ a Gauss-Newton method within a suitable function space framework.","This leads to an iterative process where, at each step, we solve a linearization of the problem by considering a linear surrogate model around the current solution estimate.","The resulting linear auxiliary problem resembles a linear-quadratic ODE optimal tracking control problem, which we tackle using either a gradient descent method in function spaces or a Riccati-based approach.","Finally, we present and analyze the efficacy of our method through numerical experiments."],"url":"http://arxiv.org/abs/2405.05124v1","category":"math.OC"}
{"created":"2024-05-08 15:20:37","title":"An Imprecise Maxwell's Demon with Feedback Delay: An Exactly Solvable Information Engine Model","abstract":"A finite cycle time information engine based on a two-level system in contact with a thermal reservoir is studied analytically. The model for the engine incorporates an error in measuring the system's state and time delay between the measurement and the feedback process. The efficiency and power of the engine in steady state are derived as a function of level spacing, feedback delay time, engine cycle time, and measurement error. For a fixed value of level spacing and feedback delay, there is an upper bound on measurement error such that the engine can extract positive work. This threshold value of error is found to be independent of the cycle time. For a range of values of level spacing and feedback delay time, efficiency has a non-monotonic dependence on the measurement error, implying that there is an optimal measurement error for the information engine to operate efficiently. At high temperatures and with precise measurement, the engine's ability to extract positive work is extended over a larger range of feedback delay time.","sentences":["A finite cycle time information engine based on a two-level system in contact with a thermal reservoir is studied analytically.","The model for the engine incorporates an error in measuring the system's state and time delay between the measurement and the feedback process.","The efficiency and power of the engine in steady state are derived as a function of level spacing, feedback delay time, engine cycle time, and measurement error.","For a fixed value of level spacing and feedback delay, there is an upper bound on measurement error such that the engine can extract positive work.","This threshold value of error is found to be independent of the cycle time.","For a range of values of level spacing and feedback delay time, efficiency has a non-monotonic dependence on the measurement error, implying that there is an optimal measurement error for the information engine to operate efficiently.","At high temperatures and with precise measurement, the engine's ability to extract positive work is extended over a larger range of feedback delay time."],"url":"http://arxiv.org/abs/2405.05123v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 15:12:46","title":"Likelihood and appearance of life beyond the Earth: An astronomical perspective","abstract":"As of 2023, over 5500 planets are known to orbit stars other than our Sun. We can measure their sizes and orbital periods, infer their masses and temperatures, and constrain their compositions. Based on these data, about 1% of extrasolar planets are potentially habitable for life as we know it, implying that of the billions of planets in our Galaxy, some may actually be inhabited, at least by microbes. However, recognizing signs of alien life forms is a major challenge for current technology, because of the wide range of conditions on extrasolar planets, and because of the wide range of forms that life may take. This chapter reviews observations of exoplanets and discusses astrobiological definitions of habitability and the likelihood of finding life beyond the Earth, both within and outside the Solar system.","sentences":["As of 2023, over 5500 planets are known to orbit stars other than our Sun.","We can measure their sizes and orbital periods, infer their masses and temperatures, and constrain their compositions.","Based on these data, about 1% of extrasolar planets are potentially habitable for life as we know it, implying that of the billions of planets in our Galaxy, some may actually be inhabited, at least by microbes.","However, recognizing signs of alien life forms is a major challenge for current technology, because of the wide range of conditions on extrasolar planets, and because of the wide range of forms that life may take.","This chapter reviews observations of exoplanets and discusses astrobiological definitions of habitability and the likelihood of finding life beyond the Earth, both within and outside the Solar system."],"url":"http://arxiv.org/abs/2405.05115v1","category":"astro-ph.GA"}
{"created":"2024-05-08 15:11:42","title":"Universality of opinions disappearing in sociophysical models of opinion dynamics: From initial multitude of opinions to ultimate consensus","abstract":"Possibility of reaching a consensus in social systems with strong initial fragmentation is one of the most interesting issues in sociopysics. It is also intriguing what the dynamics of such processes is. To address those problems, we performed computer simulations using well-established models of social opinion formation, namely the voter, Sznajd, and Latan\\'e models. We investigated opinion dynamics in cases where the initial number of opinions is very large, equal to the number of actors (the voter and Latan\\'e models) or when every second actor has their own opinion (Sznajd model), with some variations on the update schemes, lattice topologies, effective ranges of interaction, and information noise levels. For all considered models, the number of opinions assumed by the actors is finally almost always reduced to only one. However, while the voter and Latan\\'e models exhibit a power-law time decrease in the number of opinions, the Sznajd model follows a complex three-stage behavior. We also demonstrated that the mean/median time of reaching the consensus scales with system size according to a power law for voter and Sznajd models, while for the Latan\\'e model this increase is even faster. Our results show that in the studied models the consensus is possible, provided that a long enough and model-dependent time to reach this state is available.","sentences":["Possibility of reaching a consensus in social systems with strong initial fragmentation is one of the most interesting issues in sociopysics.","It is also intriguing what the dynamics of such processes is.","To address those problems, we performed computer simulations using well-established models of social opinion formation, namely the voter, Sznajd, and Latan\\'e models.","We investigated opinion dynamics in cases where the initial number of opinions is very large, equal to the number of actors (the voter and Latan\\'e models) or when every second actor has their own opinion (Sznajd model), with some variations on the update schemes, lattice topologies, effective ranges of interaction, and information noise levels.","For all considered models, the number of opinions assumed by the actors is finally almost always reduced to only one.","However, while the voter and Latan\\'e models exhibit a power-law time decrease in the number of opinions, the Sznajd model follows a complex three-stage behavior.","We also demonstrated that the mean/median time of reaching the consensus scales with system size according to a power law for voter and Sznajd models, while for the Latan\\'e model this increase is even faster.","Our results show that in the studied models the consensus is possible, provided that a long enough and model-dependent time to reach this state is available."],"url":"http://arxiv.org/abs/2405.05114v1","category":"physics.soc-ph"}
{"created":"2024-05-08 15:06:02","title":"Uncertainty quantification in metric spaces","abstract":"This paper introduces a novel uncertainty quantification framework for regression models where the response takes values in a separable metric space, and the predictors are in a Euclidean space. The proposed algorithms can efficiently handle large datasets and are agnostic to the predictive base model used. Furthermore, the algorithms possess asymptotic consistency guarantees and, in some special homoscedastic cases, we provide non-asymptotic guarantees. To illustrate the effectiveness of the proposed uncertainty quantification framework, we use a linear regression model for metric responses (known as the global Fr\\'echet model) in various clinical applications related to precision and digital medicine. The different clinical outcomes analyzed are represented as complex statistical objects, including multivariate Euclidean data, Laplacian graphs, and probability distributions.","sentences":["This paper introduces a novel uncertainty quantification framework for regression models where the response takes values in a separable metric space, and the predictors are in a Euclidean space.","The proposed algorithms can efficiently handle large datasets and are agnostic to the predictive base model used.","Furthermore, the algorithms possess asymptotic consistency guarantees and, in some special homoscedastic cases, we provide non-asymptotic guarantees.","To illustrate the effectiveness of the proposed uncertainty quantification framework, we use a linear regression model for metric responses (known as the global Fr\\'echet model) in various clinical applications related to precision and digital medicine.","The different clinical outcomes analyzed are represented as complex statistical objects, including multivariate Euclidean data, Laplacian graphs, and probability distributions."],"url":"http://arxiv.org/abs/2405.05110v1","category":"math.ST"}
{"created":"2024-05-08 15:04:22","title":"Leveraging AES Padding: dBs for Nothing and FEC for Free in IoT Systems","abstract":"The Internet of Things (IoT) represents a significant advancement in digital technology, with its rapidly growing network of interconnected devices. This expansion, however, brings forth critical challenges in data security and reliability, especially under the threat of increasing cyber vulnerabilities. Addressing the security concerns, the Advanced Encryption Standard (AES) is commonly employed for secure encryption in IoT systems. Our study explores an innovative use of AES, by repurposing AES padding bits for error correction and thus introducing a dual-functional method that seamlessly integrates error-correcting capabilities into the standard encryption process. The integration of the state-of-the-art Guessing Random Additive Noise Decoder (GRAND) in the receiver's architecture facilitates the joint decoding and decryption process. This strategic approach not only preserves the existing structure of the transmitter but also significantly enhances communication reliability in noisy environments, achieving a notable over 3 dB gain in Block Error Rate (BLER). Remarkably, this enhanced performance comes with a minimal power overhead at the receiver - less than 15% compared to the traditional decryption-only process, underscoring the efficiency of our hardware design for IoT applications. This paper discusses a comprehensive analysis of our approach, particularly in energy efficiency and system performance, presenting a novel and practical solution for reliable IoT communications.","sentences":["The Internet of Things (IoT) represents a significant advancement in digital technology, with its rapidly growing network of interconnected devices.","This expansion, however, brings forth critical challenges in data security and reliability, especially under the threat of increasing cyber vulnerabilities.","Addressing the security concerns, the Advanced Encryption Standard (AES) is commonly employed for secure encryption in IoT systems.","Our study explores an innovative use of AES, by repurposing AES padding bits for error correction and thus introducing a dual-functional method that seamlessly integrates error-correcting capabilities into the standard encryption process.","The integration of the state-of-the-art Guessing Random Additive Noise Decoder (GRAND) in the receiver's architecture facilitates the joint decoding and decryption process.","This strategic approach not only preserves the existing structure of the transmitter but also significantly enhances communication reliability in noisy environments, achieving a notable over 3 dB gain in Block Error Rate (BLER).","Remarkably, this enhanced performance comes with a minimal power overhead at the receiver - less than 15% compared to the traditional decryption-only process, underscoring the efficiency of our hardware design for IoT applications.","This paper discusses a comprehensive analysis of our approach, particularly in energy efficiency and system performance, presenting a novel and practical solution for reliable IoT communications."],"url":"http://arxiv.org/abs/2405.05107v1","category":"cs.ET"}
{"created":"2024-05-08 14:59:33","title":"Multistability of Bi-Reaction Networks","abstract":"We provide a sufficient and necessary condition in terms of the stoichiometric coefficients for a bi-reaction network to admit multistability. Also, this result completely characterizes the bi-reaction networks according to if they admit multistability.","sentences":["We provide a sufficient and necessary condition in terms of the stoichiometric coefficients for a bi-reaction network to admit multistability.","Also, this result completely characterizes the bi-reaction networks according to if they admit multistability."],"url":"http://arxiv.org/abs/2405.05103v1","category":"math.DS"}
{"created":"2024-05-08 14:54:00","title":"Phase-induced vortex pinning in rotating supersolid dipolar systems","abstract":"We analyze the pinning of vortices for a stationary rotating dipolar supersolid along the low-density paths between droplets as a function of the rotation frequency. We restrict ourselves to the stationary configurations of vortices with the same symmetry as that of the array of droplets. Our approach exploits the fact that the wave function of each droplet acquires a linear phase on the coordinates, and hence the relative phases between neighboring droplets allows us to predict the position of the vortices. For a confined system, the estimate accurately reproduces the Gross-Pitaevskii results in the spatial regions where the neighboring droplets are well defined.","sentences":["We analyze the pinning of vortices for a stationary rotating dipolar supersolid along the low-density paths between droplets as a function of the rotation frequency.","We restrict ourselves to the stationary configurations of vortices with the same symmetry as that of the array of droplets.","Our approach exploits the fact that the wave function of each droplet acquires a linear phase on the coordinates, and hence the relative phases between neighboring droplets allows us to predict the position of the vortices.","For a confined system, the estimate accurately reproduces the Gross-Pitaevskii results in the spatial regions where the neighboring droplets are well defined."],"url":"http://arxiv.org/abs/2405.05099v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-08 14:43:20","title":"A Hierarchical Approach to Quantum Many-Body Systems in Structured Environments","abstract":"Cavity quantum materials combine the rich many-body physics of condensed matter systems with strong coupling to the surrounding electromagnetic field, which presents both novel prospects and intricate challenges. One is often interested in the properties of one specific aspect of the material, e.g. the electronic many-body dynamics, subject to a structured bath of phononic and photonic modes. Open quantum systems featuring non-Markovian dynamics are routinely solved using techniques such as the Hierarchical Equations of Motion (HEOM) but their usage of the system density-matrix renders them intractable for many-body systems. Here, we combine the HEOM with the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy to reach a consistent and rigorous description of open many-body systems and their quantum dynamics. We demonstrate first the strength and limitations of this stacked hierarchy for superradiant emission and spin-squeezing of established quantum optical models before presenting its full potential for quantum many-body systems. In particular, we explicitly simulate the impact of charge noise on the dynamic of the Fermi-Hubbard model subject to a structured bath comprising cavity and vibro-phononic environment. Strong optical coupling not only modifies the dynamic of the many-body system but serves furthermore as measurement channel providing information about the correlated motion imprinted by charge noise. Our work establishes an accessible, yet rigorous, route between condensed matter and quantum optics, fostering the growth of a new domain at their interface.","sentences":["Cavity quantum materials combine the rich many-body physics of condensed matter systems with strong coupling to the surrounding electromagnetic field, which presents both novel prospects and intricate challenges.","One is often interested in the properties of one specific aspect of the material, e.g. the electronic many-body dynamics, subject to a structured bath of phononic and photonic modes.","Open quantum systems featuring non-Markovian dynamics are routinely solved using techniques such as the Hierarchical Equations of Motion (HEOM) but their usage of the system density-matrix renders them intractable for many-body systems.","Here, we combine the HEOM with the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy to reach a consistent and rigorous description of open many-body systems and their quantum dynamics.","We demonstrate first the strength and limitations of this stacked hierarchy for superradiant emission and spin-squeezing of established quantum optical models before presenting its full potential for quantum many-body systems.","In particular, we explicitly simulate the impact of charge noise on the dynamic of the Fermi-Hubbard model subject to a structured bath comprising cavity and vibro-phononic environment.","Strong optical coupling not only modifies the dynamic of the many-body system but serves furthermore as measurement channel providing information about the correlated motion imprinted by charge noise.","Our work establishes an accessible, yet rigorous, route between condensed matter and quantum optics, fostering the growth of a new domain at their interface."],"url":"http://arxiv.org/abs/2405.05093v1","category":"quant-ph"}
{"created":"2024-05-08 14:32:09","title":"Fair Voting Outcomes with Impact and Novelty Compromises? Unraveling Biases of Equal Shares in Participatory Budgeting","abstract":"Participatory budgeting, as a paradigm for democratic innovations, engages citizens in the distribution of a public budget to projects, which they propose and vote for implementation. So far, voting algorithms have been devised and studied in social choice literature to elect projects that are popular, while others prioritize on a proportional representation of voters' preferences, for instance, equal shares. However, the anticipated impact and novelty in the broader society by the winning projects, as selected by different algorithms, remains totally under-explored, lacking both a universal theory of impact for voting and a rigorous framework for impact and novelty assessments. This papers tackles this grand challenge towards new axiomatic foundations for designing effective and fair voting methods. This is via new and striking insights derived from a large-scale analysis of biases over 345 real-world voting outcomes, characterized for the first time by a novel portfolio of impact and novelty metrics. We find strong causal evidence that equal shares comes with impact loss in several infrastructural projects of different cost levels that have been so far over-represented. However, it also comes with a novel, yet over-represented, impact gain in welfare, education and culture. We discuss broader implications of these results and how impact loss can be mitigated at the stage of campaign design and project ideation.","sentences":["Participatory budgeting, as a paradigm for democratic innovations, engages citizens in the distribution of a public budget to projects, which they propose and vote for implementation.","So far, voting algorithms have been devised and studied in social choice literature to elect projects that are popular, while others prioritize on a proportional representation of voters' preferences, for instance, equal shares.","However, the anticipated impact and novelty in the broader society by the winning projects, as selected by different algorithms, remains totally under-explored, lacking both a universal theory of impact for voting and a rigorous framework for impact and novelty assessments.","This papers tackles this grand challenge towards new axiomatic foundations for designing effective and fair voting methods.","This is via new and striking insights derived from a large-scale analysis of biases over 345 real-world voting outcomes, characterized for the first time by a novel portfolio of impact and novelty metrics.","We find strong causal evidence that equal shares comes with impact loss in several infrastructural projects of different cost levels that have been so far over-represented.","However, it also comes with a novel, yet over-represented, impact gain in welfare, education and culture.","We discuss broader implications of these results and how impact loss can be mitigated at the stage of campaign design and project ideation."],"url":"http://arxiv.org/abs/2405.05085v1","category":"cs.MA"}
{"created":"2024-05-08 14:18:36","title":"Subsystem Information Capacity in Random Circuits and Hamiltonian Dynamics","abstract":"In this study, we explore the information capacity of open quantum systems, focusing on the effective channels formed by the subsystem of random quantum circuits and quantum Hamiltonian evolution. By analyzing the subsystem information capacity, which is closely linked to quantum coherent information of these effective quantum channels, we uncover a diverse range of dynamical and steady behaviors depending on the types of evolution. Therefore, the subsystem information capacity serves as a valuable tool for studying the intrinsic nature of various dynamical phases, such as integrable, localized, thermalized, and topological systems. We also reveal the impact of different initial information encoding schemes on information dynamics including one-to-one, one-to-many, and many-to-many. To support our findings, we provide representative examples for numerical simulations, including random quantum circuits with or without mid-circuit measurements, random Clifford Floquet circuits, free and interacting Aubry-Andr\\'e models, and Su-Schrieffer-Heeger models. Those numerical results are further quantitatively explained using the effective statistical model mapping and the quasiparticle picture in the cases of random circuits and non-interacting Hamiltonian dynamics, respectively.","sentences":["In this study, we explore the information capacity of open quantum systems, focusing on the effective channels formed by the subsystem of random quantum circuits and quantum Hamiltonian evolution.","By analyzing the subsystem information capacity, which is closely linked to quantum coherent information of these effective quantum channels, we uncover a diverse range of dynamical and steady behaviors depending on the types of evolution.","Therefore, the subsystem information capacity serves as a valuable tool for studying the intrinsic nature of various dynamical phases, such as integrable, localized, thermalized, and topological systems.","We also reveal the impact of different initial information encoding schemes on information dynamics including one-to-one, one-to-many, and many-to-many.","To support our findings, we provide representative examples for numerical simulations, including random quantum circuits with or without mid-circuit measurements, random Clifford Floquet circuits, free and interacting Aubry-Andr\\'e models, and Su-Schrieffer-Heeger models.","Those numerical results are further quantitatively explained using the effective statistical model mapping and the quasiparticle picture in the cases of random circuits and non-interacting Hamiltonian dynamics, respectively."],"url":"http://arxiv.org/abs/2405.05076v1","category":"quant-ph"}
{"created":"2024-05-08 14:08:07","title":"Chemistry Beyond Exact Solutions on a Quantum-Centric Supercomputer","abstract":"A universal quantum computer can be used as a simulator capable of predicting properties of diverse quantum systems. Electronic structure problems in chemistry offer practical use cases around the hundred-qubit mark. This appears promising since current quantum processors have reached these sizes. However, mapping these use cases onto quantum computers yields deep circuits, and for for pre-fault-tolerant quantum processors, the large number of measurements to estimate molecular energies leads to prohibitive runtimes. As a result, realistic chemistry is out of reach of current quantum computers in isolation. A natural question is whether classical distributed computation can relieve quantum processors from parsing all but a core, intrinsically quantum component of a chemistry workflow. Here, we incorporate quantum computations of chemistry in a quantum-centric supercomputing architecture, using up to 6400 nodes of the supercomputer Fugaku to assist a Heron superconducting quantum processor. We simulate the N$_2$ triple bond breaking in a correlation-consistent cc-pVDZ basis set, and the active-space electronic structure of [2Fe-2S] and [4Fe-4S] clusters, using 58, 45 and 77 qubits respectively, with quantum circuits of up to 10570 (3590 2-qubit) quantum gates. We obtain our results using a class of quantum circuits that approximates molecular eigenstates, and a hybrid estimator. The estimator processes quantum samples, produces upper bounds to the ground-state energy and wavefunctions supported on a polynomial number of states. This guarantees an unconditional quality metric for quantum advantage, certifiable by classical computers at polynomial cost. For current error rates, our results show that classical distributed computing coupled to quantum processors can produce good approximate solutions for practical problems beyond sizes amenable to exact diagonalization.","sentences":["A universal quantum computer can be used as a simulator capable of predicting properties of diverse quantum systems.","Electronic structure problems in chemistry offer practical use cases around the hundred-qubit mark.","This appears promising since current quantum processors have reached these sizes.","However, mapping these use cases onto quantum computers yields deep circuits, and for for pre-fault-tolerant quantum processors, the large number of measurements to estimate molecular energies leads to prohibitive runtimes.","As a result, realistic chemistry is out of reach of current quantum computers in isolation.","A natural question is whether classical distributed computation can relieve quantum processors from parsing all but a core, intrinsically quantum component of a chemistry workflow.","Here, we incorporate quantum computations of chemistry in a quantum-centric supercomputing architecture, using up to 6400 nodes of the supercomputer Fugaku to assist a Heron superconducting quantum processor.","We simulate the N$_2$ triple bond breaking in a correlation-consistent cc-pVDZ basis set, and the active-space electronic structure of [2Fe-2S] and [4Fe-4S] clusters, using 58, 45 and 77 qubits respectively, with quantum circuits of up to 10570 (3590 2-qubit) quantum gates.","We obtain our results using a class of quantum circuits that approximates molecular eigenstates, and a hybrid estimator.","The estimator processes quantum samples, produces upper bounds to the ground-state energy and wavefunctions supported on a polynomial number of states.","This guarantees an unconditional quality metric for quantum advantage, certifiable by classical computers at polynomial cost.","For current error rates, our results show that classical distributed computing coupled to quantum processors can produce good approximate solutions for practical problems beyond sizes amenable to exact diagonalization."],"url":"http://arxiv.org/abs/2405.05068v1","category":"quant-ph"}
{"created":"2024-05-08 13:58:08","title":"Controlling Borda Elections by Adding or Deleting either Votes or Candidates: Complete and Top-Truncated Votes","abstract":"An election is defined as a pair of a set of candidates C=\\{c_1,\\cdots,c_m\\} and a multiset of votes V=\\{v_1,\\cdots,v_n\\}, where each vote is a linear order of the candidates. The Borda election rule is characterized by a vector \\langle m-1,m-2,\\cdots,0\\rangle, which means that the candidate ranked at the i-th position of a vote v receives a score m-i from v, and the candidate receiving the most score from all votes wins the election. Here, we consider the control problems of a Borda election, where the chair of the election attempts to influence the election outcome by adding or deleting either votes or candidates with the intention to make a special candidate win (constructive control) or lose (destructive control) the election. Control problems have been extensively studied for Borda elections from both classical and parameterized complexity viewpoints. We complete the parameterized complexity picture for Borda control problems by showing W[2]-hardness with the number of additions/deletions as parameter for constructive control by deleting votes, adding candidates, or deleting candidates. The hardness result for deleting votes settles an open problem posed by Liu and Zhu. Following the suggestion by Menon and Larson, we also investigate the impact of introducing top-truncated votes, where each voter ranks only t out of the given m candidates, on the classical and parameterized complexity of Borda control problems. Constructive Borda control problems remain NP-hard even with t being a small constant. Moreover, we prove that in the top-truncated case, constructive control by adding/deleting votes problems are FPT with the number \\ell of additions/deletions and t as parameters, while for every constant t\\geq 2, constructive control by adding/deleting candidates problems are W[2]-hard with respect to \\ell.","sentences":["An election is defined as a pair of a set of candidates C=\\{c_1,\\cdots,c_m\\} and a multiset of votes V=\\{v_1,\\cdots,v_n\\}, where each vote is a linear order of the candidates.","The Borda election rule is characterized by a vector \\langle m-1,m-2,\\cdots,0\\rangle, which means that the candidate ranked at the i-th position of a vote v receives a score m-i from v, and the candidate receiving the most score from all votes wins the election.","Here, we consider the control problems of a Borda election, where the chair of the election attempts to influence the election outcome by adding or deleting either votes or candidates with the intention to make a special candidate win (constructive control) or lose (destructive control) the election.","Control problems have been extensively studied for Borda elections from both classical and parameterized complexity viewpoints.","We complete the parameterized complexity picture for Borda control problems by showing W[2]-hardness with the number of additions/deletions as parameter for constructive control by deleting votes, adding candidates, or deleting candidates.","The hardness result for deleting votes settles an open problem posed by Liu and Zhu.","Following the suggestion by Menon and Larson, we also investigate the impact of introducing top-truncated votes, where each voter ranks only t out of the given m candidates, on the classical and parameterized complexity of Borda control problems.","Constructive Borda control problems remain NP-hard even with t being a small constant.","Moreover, we prove that in the top-truncated case, constructive control by adding/deleting votes problems are FPT with the number \\ell of additions/deletions and t as parameters, while for every constant t\\geq 2, constructive control by adding/deleting candidates problems are W[2]-hard with respect to \\ell."],"url":"http://arxiv.org/abs/2405.05062v1","category":"cs.CC"}
{"created":"2024-05-08 13:54:16","title":"G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric Information","abstract":"Localization in already mapped environments is a critical component in many robotics and automotive applications, where previously acquired information can be exploited along with sensor fusion to provide robust and accurate localization estimates. In this work, we offer a new perspective on map-based localization by reusing prior topological and metric information. Thus, we reformulate this long-studied problem to go beyond the mere use of metric maps. Our framework seamlessly integrates LiDAR, iner\\-tial and GNSS measurements, and scan-to-map registrations in a sliding window graph fashion, which allows to accommodate the uncertainty of each observation. The modularity of our framework allows it to work with different sensor configurations (\\textit{e.g.,} LiDAR resolutions, GNSS denial) and environmental conditions (\\textit{e.g.,} map-less regions, large environments). We have conducted different validation experiments, including deployment in a real-world automotive application, demonstrating the accuracy, efficiency, and versatility of our system in online localization.","sentences":["Localization in already mapped environments is a critical component in many robotics and automotive applications, where previously acquired information can be exploited along with sensor fusion to provide robust and accurate localization estimates.","In this work, we offer a new perspective on map-based localization by reusing prior topological and metric information.","Thus, we reformulate this long-studied problem to go beyond the mere use of metric maps.","Our framework seamlessly integrates LiDAR, iner\\-tial and GNSS measurements, and scan-to-map registrations in a sliding window graph fashion, which allows to accommodate the uncertainty of each observation.","The modularity of our framework allows it to work with different sensor configurations (\\textit{e.g.,} LiDAR resolutions, GNSS denial) and environmental conditions (\\textit{e.g.,} map-less regions, large environments).","We have conducted different validation experiments, including deployment in a real-world automotive application, demonstrating the accuracy, efficiency, and versatility of our system in online localization."],"url":"http://arxiv.org/abs/2405.05059v1","category":"cs.RO"}
{"created":"2024-05-08 13:52:14","title":"Real-Time Motion Detection Using Dynamic Mode Decomposition","abstract":"Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit timeseries data to a linear dynamical system. In doing so, DMD decomposes dynamic data into spatially coherent modes that evolve in time according to exponential growth/decay or with a fixed frequency of oscillation. A prolific application of DMD has been to video, where one interprets the high-dimensional pixel space evolving through time as the video plays. In this work, we propose a simple and interpretable motion detection algorithm for streaming video data rooted in DMD. Our method leverages the fact that there exists a correspondence between the evolution of important video features, such as foreground motion, and the eigenvalues of the matrix which results from applying DMD to segments of video. We apply the method to a database of test videos which emulate security footage under varying realistic conditions. Effectiveness is analyzed using receiver operating characteristic curves, while we use cross-validation to optimize the threshold parameter that identifies movement.","sentences":["Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit timeseries data to a linear dynamical system.","In doing so, DMD decomposes dynamic data into spatially coherent modes that evolve in time according to exponential growth/decay or with a fixed frequency of oscillation.","A prolific application of DMD has been to video, where one interprets the high-dimensional pixel space evolving through time as the video plays.","In this work, we propose a simple and interpretable motion detection algorithm for streaming video data rooted in DMD.","Our method leverages the fact that there exists a correspondence between the evolution of important video features, such as foreground motion, and the eigenvalues of the matrix which results from applying DMD to segments of video.","We apply the method to a database of test videos which emulate security footage under varying realistic conditions.","Effectiveness is analyzed using receiver operating characteristic curves, while we use cross-validation to optimize the threshold parameter that identifies movement."],"url":"http://arxiv.org/abs/2405.05057v1","category":"cs.CV"}
{"created":"2024-05-08 13:48:21","title":"More than 200 Globular Clusters in the Milky Way, and still no super-Solar metallicity ones","abstract":"Many globular clusters (GCs) in the Milky Way (MW) have been studied in recent years, especially in hidden regions such as those of the Galactic bulge. Our main goal is to understand what we can learn if we include these new objects into the MWGC system that we know today. We catalogue 37 recently discovered GCs. We use different distributions for investigating the MWGC system: metallicity distribution (MD), luminosity function (LF), and age distribution. We first treat separately the new GCs sample from the known and well-characterised GCs. We merge these two samples, upgrading the MWGC system. We performed a comparison between our clusters sample and field star (FS) population. We find a double peaked distribution for the LF, which shows an elongated faint end tail. Considering the \"merged\" sample, the LF and the MDs display a bimodality trend. We construct the MD for the FS sample, and comparing this with that one of the GCs, we learn that a high percentage of FS show [Fe/H]$>0$, whereas we do not detect any GCs in the same metallicity range. In order to understand this inconsistency, we construct the age-metallicity diagram for both samples, noting that the old and metal-poor population (age$\\geq8$ Gyr and [Fe/H]$\\leq -1.0$) is represented by GCs, while the young and metal-rich population (age$<8$ Gyr and [Fe/H]$>-1.0$) corresponds to FS. From the analysis of the GC LF and MD, we can conclude that many GCs, probably those very faint, have survived strong dynamical processes, typical of the Bulge regions. We cannot exclude the possibility that some of them have been accreted during past merging events, especially the metal-poor component, whereas the metal-rich population may be related to the formation of the bulge and/or disk. Finally, the difference that we notice between the GC and FS samples should be sought in the evolutionary difference between these two stellar populations.","sentences":["Many globular clusters (GCs) in the Milky Way (MW) have been studied in recent years, especially in hidden regions such as those of the Galactic bulge.","Our main goal is to understand what we can learn if we include these new objects into the MWGC system that we know today.","We catalogue 37 recently discovered GCs.","We use different distributions for investigating the MWGC system: metallicity distribution (MD), luminosity function (LF), and age distribution.","We first treat separately the new GCs sample from the known and well-characterised GCs.","We merge these two samples, upgrading the MWGC system.","We performed a comparison between our clusters sample and field star (FS) population.","We find a double peaked distribution for the LF, which shows an elongated faint end tail.","Considering the \"merged\" sample, the LF and the MDs display a bimodality trend.","We construct the MD for the FS sample, and comparing this with that one of the GCs, we learn that a high percentage of FS show [Fe/H]$>0$, whereas we do not detect any GCs in the same metallicity range.","In order to understand this inconsistency, we construct the age-metallicity diagram for both samples, noting that the old and metal-poor population (age$\\geq8$ Gyr and [Fe/H]$\\leq -1.0$) is represented by GCs, while the young and metal-rich population (age$<8$ Gyr and [Fe/H]$>-1.0$) corresponds to FS.","From the analysis of the GC LF and MD, we can conclude that many GCs, probably those very faint, have survived strong dynamical processes, typical of the Bulge regions.","We cannot exclude the possibility that some of them have been accreted during past merging events, especially the metal-poor component, whereas the metal-rich population may be related to the formation of the bulge and/or disk.","Finally, the difference that we notice between the GC and FS samples should be sought in the evolutionary difference between these two stellar populations."],"url":"http://arxiv.org/abs/2405.05055v1","category":"astro-ph.GA"}
{"created":"2024-05-08 13:41:15","title":"Variational simulation of $d$-level systems on qubit-based quantum simulators","abstract":"Current quantum simulators are primarily qubit-based, making them naturally suitable for simulating 2-level quantum systems. However, many systems in nature are inherently $d$-level, including higher spins, bosons, vibrational modes, and itinerant electrons. To simulate $d$-level systems on qubit-based quantum simulators, an encoding method is required to map the $d$-level system onto a qubit basis. Such mapping may introduce illegitimate states in the Hilbert space which makes the simulation more sophisticated. In this paper, we develop a systematic method to address the illegitimate states. In addition, we compare two different mappings, namely binary and symmetry encoding methods, and compare their performance through variational simulation of the ground state and time evolution of various many-body systems. While binary encoding is very efficient with respect to the number of qubits it cannot easily incorporate the symmetries of the original Hamiltonian in its circuit design. On the other hand, the symmetry encoding facilitates the implementation of symmetries in the circuit design, though it comes with an overhead for the number of qubits. Our analysis shows that the symmetry encoding significantly outperforms the binary encoding, despite requiring extra qubits. Their advantage is indicated by requiring fewer two-qubit gates, converging faster, and being far more resilient to Barren plateaus. We have performed variational ground state simulations of spin-1, spin-3/2, and bosonic systems as well as variational time evolution of spin-1 systems. Our proposal can be implemented on existing quantum simulators and its potential is extendable to a broad class of physical models.","sentences":["Current quantum simulators are primarily qubit-based, making them naturally suitable for simulating 2-level quantum systems.","However, many systems in nature are inherently $d$-level, including higher spins, bosons, vibrational modes, and itinerant electrons.","To simulate $d$-level systems on qubit-based quantum simulators, an encoding method is required to map the $d$-level system onto a qubit basis.","Such mapping may introduce illegitimate states in the Hilbert space which makes the simulation more sophisticated.","In this paper, we develop a systematic method to address the illegitimate states.","In addition, we compare two different mappings, namely binary and symmetry encoding methods, and compare their performance through variational simulation of the ground state and time evolution of various many-body systems.","While binary encoding is very efficient with respect to the number of qubits it cannot easily incorporate the symmetries of the original Hamiltonian in its circuit design.","On the other hand, the symmetry encoding facilitates the implementation of symmetries in the circuit design, though it comes with an overhead for the number of qubits.","Our analysis shows that the symmetry encoding significantly outperforms the binary encoding, despite requiring extra qubits.","Their advantage is indicated by requiring fewer two-qubit gates, converging faster, and being far more resilient to Barren plateaus.","We have performed variational ground state simulations of spin-1, spin-3/2, and bosonic systems as well as variational time evolution of spin-1 systems.","Our proposal can be implemented on existing quantum simulators and its potential is extendable to a broad class of physical models."],"url":"http://arxiv.org/abs/2405.05051v1","category":"quant-ph"}
{"created":"2024-05-08 13:35:01","title":"An adaptive finite element multigrid solver using GPU acceleration","abstract":"Adaptive finite elements combined with geometric multigrid solvers are one of the most efficient numerical methods for problems such as the instationary Navier-Stokes equations. Yet despite their efficiency, computations remain expensive and the simulation of, for example, complex flow problems can take many hours or days. GPUs provide an interesting avenue to speed up the calculations due to their very large theoretical peak performance. However, the large degree of parallelism and non-standard API make the use of GPUs in scientific computing challenging. In this work, we develop a GPU acceleration for the adaptive finite element library Gascoigne and study its effectiveness for different systems of partial differential equations. Through the systematic formulation of all computations as linear algebra operations, we can employ GPU-accelerated linear algebra libraries, which simplifies the implementation and ensures the maintainability of the code while achieving very efficient GPU utilizations. Our results for a transport-diffusion equation, linear elasticity, and the instationary Navier-Stokes equations show substantial speedups of up to 20X compared to multi-core CPU implementations.","sentences":["Adaptive finite elements combined with geometric multigrid solvers are one of the most efficient numerical methods for problems such as the instationary Navier-Stokes equations.","Yet despite their efficiency, computations remain expensive and the simulation of, for example, complex flow problems can take many hours or days.","GPUs provide an interesting avenue to speed up the calculations due to their very large theoretical peak performance.","However, the large degree of parallelism and non-standard API make the use of GPUs in scientific computing challenging.","In this work, we develop a GPU acceleration for the adaptive finite element library Gascoigne and study its effectiveness for different systems of partial differential equations.","Through the systematic formulation of all computations as linear algebra operations, we can employ GPU-accelerated linear algebra libraries, which simplifies the implementation and ensures the maintainability of the code while achieving very efficient GPU utilizations.","Our results for a transport-diffusion equation, linear elasticity, and the instationary Navier-Stokes equations show substantial speedups of up to 20X compared to multi-core CPU implementations."],"url":"http://arxiv.org/abs/2405.05047v1","category":"math.NA"}
{"created":"2024-05-08 13:33:20","title":"Maximum of the Characteristic Polynomial of I.I.D. Matrices","abstract":"We compute the leading order asymptotic of the maximum of the characteristic polynomial for i.i.d. matrices with real or complex entries. In particular, this result is new even for real Ginibre matrices, which was left as an open problem in [arXiv:2303.09912]; the complex Ginibre case was covered in [arXiv:1902.01983]. These are the first universality results for the non--Hermitian analog of the first order term of the Fyodorov--Hiary--Keating conjecture. Our methods are based on constructing a coupling to the branching random walk via Dyson Brownian motion. In particular, we find a new connection between real i.i.d. matrices and inhomogeneous branching random walk.","sentences":["We compute the leading order asymptotic of the maximum of the characteristic polynomial for i.i.d. matrices with real or complex entries.","In particular, this result is new even for real Ginibre matrices, which was left as an open problem in [arXiv:2303.09912]; the complex Ginibre case was covered in [arXiv:1902.01983].","These are the first universality results for the non--Hermitian analog of the first order term of the Fyodorov--Hiary--Keating conjecture.","Our methods are based on constructing a coupling to the branching random walk via Dyson Brownian motion.","In particular, we find a new connection between real i.i.d. matrices and inhomogeneous branching random walk."],"url":"http://arxiv.org/abs/2405.05045v1","category":"math.PR"}
{"created":"2024-05-08 13:26:35","title":"Range separation of the interaction potential in intermolecular and intramolecular symmetry-adapted perturbation theory","abstract":"Symmetry-adapted perturbation theory (SAPT) is a popular and versatile tool to compute and decompose noncovalent interaction energies between molecules. The intramolecular SAPT (ISAPT) variant provides a similar energy decomposition between two nonbonded fragments of the same molecule, covalently connected by a third fragment. In this work, we explore an alternative approach where the noncovalent interaction is singled out by a range separation of the Coulomb potential. We investigate two common splittings of the $1/r$ potential into long-range and short-range parts based on the Gaussian and error functions, and approximate either the entire intermolecular/interfragment interaction or only its attractive terms by the long-range contribution. These range separation schemes are tested for a number of intermolecular and intramolecular complexes. We find that the energy corrections from range-separated SAPT or ISAPT are in reasonable agreement with complete SAPT/ISAPT data. This result should be contrasted with the inability of the long-range multipole expansion to describe crucial short-range charge penetration and exchange effects; it shows that the long-range interaction potential does not just recover the asymptotic interaction energy but also provides a useful account of short-range terms. The best consistency is attained for the error-function separation applied to all interaction terms, both attractive and repulsive. This study is the first step towards a fragmentation-free decomposition of intramolecular nonbonded energy.","sentences":["Symmetry-adapted perturbation theory (SAPT) is a popular and versatile tool to compute and decompose noncovalent interaction energies between molecules.","The intramolecular SAPT (ISAPT) variant provides a similar energy decomposition between two nonbonded fragments of the same molecule, covalently connected by a third fragment.","In this work, we explore an alternative approach where the noncovalent interaction is singled out by a range separation of the Coulomb potential.","We investigate two common splittings of the $1/r$ potential into long-range and short-range parts based on the Gaussian and error functions, and approximate either the entire intermolecular/interfragment interaction or only its attractive terms by the long-range contribution.","These range separation schemes are tested for a number of intermolecular and intramolecular complexes.","We find that the energy corrections from range-separated SAPT or ISAPT are in reasonable agreement with complete SAPT/ISAPT data.","This result should be contrasted with the inability of the long-range multipole expansion to describe crucial short-range charge penetration and exchange effects; it shows that the long-range interaction potential does not just recover the asymptotic interaction energy but also provides a useful account of short-range terms.","The best consistency is attained for the error-function separation applied to all interaction terms, both attractive and repulsive.","This study is the first step towards a fragmentation-free decomposition of intramolecular nonbonded energy."],"url":"http://arxiv.org/abs/2405.05041v1","category":"physics.chem-ph"}
{"created":"2024-05-08 13:07:34","title":"Dissipativity Conditions for Maximum Dynamic Loadability","abstract":"In this paper we consider a possibility of stabilizing very fast electromagnetic interactions between Inverter Based Resources (IBRs), known as the Control Induced System Stability problems. We propose that when these oscillatory interactions are controlled the ability of the grid to deliver power to loads at high rates will be greatly increased. We refer to this grid property as the dynamic grid loadability. The approach is to start by modeling the dynamical behavior of all components. Next, to avoid excessive complexity, interactions between components are captured in terms of unified technology-agnostic aggregate variables, instantaneous power and rate of change of instantaneous reactive power. Sufficient dissipativity conditions in terms of rate of change of energy conversion in components themselves and bounds on their rate of change of interactions are derived in support of achieving the maximum system loadability. These physically intuitive conditions are then used to derive methods to increase loadability using high switching frequency reactive power sources. Numerical simulations confirm the theoretical calculations, and shows dynamic load-side reactive power support increases stable dynamic loadability regions.","sentences":["In this paper we consider a possibility of stabilizing very fast electromagnetic interactions between Inverter Based Resources (IBRs), known as the Control Induced System Stability problems.","We propose that when these oscillatory interactions are controlled the ability of the grid to deliver power to loads at high rates will be greatly increased.","We refer to this grid property as the dynamic grid loadability.","The approach is to start by modeling the dynamical behavior of all components.","Next, to avoid excessive complexity, interactions between components are captured in terms of unified technology-agnostic aggregate variables, instantaneous power and rate of change of instantaneous reactive power.","Sufficient dissipativity conditions in terms of rate of change of energy conversion in components themselves and bounds on their rate of change of interactions are derived in support of achieving the maximum system loadability.","These physically intuitive conditions are then used to derive methods to increase loadability using high switching frequency reactive power sources.","Numerical simulations confirm the theoretical calculations, and shows dynamic load-side reactive power support increases stable dynamic loadability regions."],"url":"http://arxiv.org/abs/2405.05036v1","category":"eess.SY"}
{"created":"2024-05-08 13:05:08","title":"Spin pumping into quantum spin chains","abstract":"We theoretically investigate spin pumping into a quantum easy-plane ferromagnetic spin chain system. This quantum spin chain is effectively described by the Tomonaga-Luttinger (TL) liquid despite the ferromagnetic exchange interaction because of the easy-plane magnetic anisotropy. This TL liquid state has an extremely strong interaction that is hardly realized in other quantum antiferromagnetic chain systems or weakly interacting electron systems. We show how the strongly interacting TL liquid affects the ferromagnetic resonance that occurs in the ferromagnetic insulator. In particular, we discuss the dependence of the Gilbert damping on the temperature and the junction length. The Gilbert damping allows us to extract information about the above-mentioned strong interaction within the quantum ferromagnetic spin chain. We also point out that a well-known compound CsCuCl$_3$ will be suitable for the realization of our setup.","sentences":["We theoretically investigate spin pumping into a quantum easy-plane ferromagnetic spin chain system.","This quantum spin chain is effectively described by the Tomonaga-Luttinger (TL) liquid despite the ferromagnetic exchange interaction because of the easy-plane magnetic anisotropy.","This TL liquid state has an extremely strong interaction that is hardly realized in other quantum antiferromagnetic chain systems or weakly interacting electron systems.","We show how the strongly interacting TL liquid affects the ferromagnetic resonance that occurs in the ferromagnetic insulator.","In particular, we discuss the dependence of the Gilbert damping on the temperature and the junction length.","The Gilbert damping allows us to extract information about the above-mentioned strong interaction within the quantum ferromagnetic spin chain.","We also point out that a well-known compound CsCuCl$_3$ will be suitable for the realization of our setup."],"url":"http://arxiv.org/abs/2405.05034v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 13:03:27","title":"A dusty rain falls on the nova V959 Monocerotis","abstract":"We present archival and ground-based infrared observations of the gamma-ray-emitting nova V959 Mon, covering the period 100-4205 days after the 2012 eruption. We use these data to determine that the secondary in the nova system is a G5 main sequence star. Data from the NEOWISE survey reveal a significant increase in the emission at 3.4 microns and 4.6 microns at late (>~600 days) times, which we interpret as emission by dust. Other interpretations are considered but cannot be reconciled with the data. The presence of such late dust emission, and in particular its variation with time, are unprecedented in the context of novae. The behaviour of the dust emission suggests a qualitative interpretation in which ejecta from the 2012 eruption encounter denser pre-eruption circumbinary material, giving rise to Rayleigh-Taylor instabilities that cause clumps of dust-bearing material to fall back towards the central binary, the dust undergoing destruction by chemisputtering as it does so. The observed rise in the dust temperature, the decline in the nova-dust distance and in the dust mass, are consistent with this interpretation. Not all novae are expected to show this behaviour, but inspection of resources such as NEOWISE might reveal other novae post-eruption that do.","sentences":["We present archival and ground-based infrared observations of the gamma-ray-emitting nova V959 Mon, covering the period 100-4205 days after the 2012 eruption.","We use these data to determine that the secondary in the nova system is a G5 main sequence star.","Data from the NEOWISE survey reveal a significant increase in the emission at 3.4 microns and 4.6 microns at late (>~600 days) times, which we interpret as emission by dust.","Other interpretations are considered but cannot be reconciled with the data.","The presence of such late dust emission, and in particular its variation with time, are unprecedented in the context of novae.","The behaviour of the dust emission suggests a qualitative interpretation in which ejecta from the 2012 eruption encounter denser pre-eruption circumbinary material, giving rise to Rayleigh-Taylor instabilities that cause clumps of dust-bearing material to fall back towards the central binary, the dust undergoing destruction by chemisputtering as it does so.","The observed rise in the dust temperature, the decline in the nova-dust distance and in the dust mass, are consistent with this interpretation.","Not all novae are expected to show this behaviour, but inspection of resources such as NEOWISE might reveal other novae post-eruption that do."],"url":"http://arxiv.org/abs/2405.05032v1","category":"astro-ph.SR"}
{"created":"2024-05-08 12:58:39","title":"Stability And Uncertainty Propagation In Power Networks: A Lyapunov-based Approach With Applications To Renewable Resources Allocation","abstract":"The rapid increase in the integration of intermittent and stochastic renewable energy resources (RER) introduces challenging issues related to power system stability. Interestingly, identifying grid nodes that can best support stochastic loads from RER, has gained recent interest. Methods based on Lyapunov stability are commonly exploited to assess the stability of power networks. These strategies approach quantifying system stability while considering: (i) simplified reduced order power system models that do not model power flow constraints, or (ii) datadriven methods that are prone to measurement noise and hence can inaccurately depict stochastic loads as system instability. In this paper, while considering a nonlinear differential algebraic equation (NL-DAE) model, we introduce a new method for assessing the impact of uncertain renewable power injections on the stability of power system nodes/buses. The identification of stable nodes informs the operator/utility on how renewables injections affect the stability of the grid. The proposed method is based on optimizing metrics equivalent to the Lyapunov spectrum of exponents; its underlying properties result in a computationally efficient and scalable stable node identification algorithm for renewable energy resources allocation. The proposed method is validated on the IEEE 9-bus and 200-bus networks","sentences":["The rapid increase in the integration of intermittent and stochastic renewable energy resources (RER) introduces challenging issues related to power system stability.","Interestingly, identifying grid nodes that can best support stochastic loads from RER, has gained recent interest.","Methods based on Lyapunov stability are commonly exploited to assess the stability of power networks.","These strategies approach quantifying system stability while considering: (i) simplified reduced order power system models that do not model power flow constraints, or (ii) datadriven methods that are prone to measurement noise and hence can inaccurately depict stochastic loads as system instability.","In this paper, while considering a nonlinear differential algebraic equation (NL-DAE) model, we introduce a new method for assessing the impact of uncertain renewable power injections on the stability of power system nodes/buses.","The identification of stable nodes informs the operator/utility on how renewables injections affect the stability of the grid.","The proposed method is based on optimizing metrics equivalent to the Lyapunov spectrum of exponents; its underlying properties result in a computationally efficient and scalable stable node identification algorithm for renewable energy resources allocation.","The proposed method is validated on the IEEE 9-bus and 200-bus networks"],"url":"http://arxiv.org/abs/2405.05028v1","category":"eess.SY"}
{"created":"2024-05-08 12:44:37","title":"Quantum Circuit Ansatz: Abstraction and Reuse of Quantum Algorithm Design","abstract":"Quantum computing holds the potential to revolutionize various fields by efficiently tackling complex problems. At its core are quantum circuits, sequences of quantum gates manipulating quantum states. The selection of the right quantum circuit ansatz, which defines initial circuit structures and serves as the basis for optimization techniques, is crucial in quantum algorithm design.This paper presents a categorized catalog of quantum circuit ansatzes aimed at supporting quantum algorithm design and implementation. Each ansatz is described with details such as intent, motivation, applicability, circuit diagram, implementation, example, and see also. Practical examples are provided to illustrate their application in quantum algorithm design.The catalog aims to assist quantum algorithm designers by offering insights into the strengths and limitations of different ansatzes, thereby facilitating decision-making for specific tasks.","sentences":["Quantum computing holds the potential to revolutionize various fields by efficiently tackling complex problems.","At its core are quantum circuits, sequences of quantum gates manipulating quantum states.","The selection of the right quantum circuit ansatz, which defines initial circuit structures and serves as the basis for optimization techniques, is crucial in quantum algorithm design.","This paper presents a categorized catalog of quantum circuit ansatzes aimed at supporting quantum algorithm design and implementation.","Each ansatz is described with details such as intent, motivation, applicability, circuit diagram, implementation, example, and see also.","Practical examples are provided to illustrate their application in quantum algorithm design.","The catalog aims to assist quantum algorithm designers by offering insights into the strengths and limitations of different ansatzes, thereby facilitating decision-making for specific tasks."],"url":"http://arxiv.org/abs/2405.05021v1","category":"cs.SE"}
{"created":"2024-05-08 12:43:38","title":"First-principles study of structural, electronic and magnetic properties at the \\ce{(0001)Cr2O3-(111)Pt} interface","abstract":"We perform first-principles density functional calculations to elucidate structural, electronic and magnetic properties at the interface of \\ce{(0001)Cr2O3-(111)Pt} bilayers. This investigation is motivated by the fact that, despite the promise of \\ce{Cr2O3-Pt} heterostructures in a variety of antiferromagnetic spintronic applications, many key structural, electronic, and magnetic properties at the \\ce{Cr2O3-Pt} interface are poorly understood. We first analyze all inequivalent lateral interface alignments to determine the lowest energy interfacial structure. For all lateral alignments including the lowest-energy one, we observe an accumulation of electrons at the interface between \\ce{Cr2O3} and Pt. We find an unexpected reversal of the magnetic moments of the interface Cr ions in the presence of Pt compared to surface Cr moments in vacuum-terminated \\ce{(0001)Cr2O3}. We also find that the heterostructure exhibits a magnetic proximity effect in the first three Pt layers at the interface with \\ce{Cr2O3}, providing a mechanism by which the anomalous Hall effect can occur in \\ce{(0001)Cr2O3-(111)Pt} bilayers. Our results provide the basis for a more nuanced interpretation of magnetotransport experiments on \\ce{(0001)Cr2O3-(111)Pt} bilayers and should inform future development of improved antiferromagnetic spintronic devices based on the \\ce{Cr2O3-Pt} material system.","sentences":["We perform first-principles density functional calculations to elucidate structural, electronic and magnetic properties at the interface of \\ce{(0001)Cr2O3-(111)Pt} bilayers.","This investigation is motivated by the fact that, despite the promise of \\ce{Cr2O3-Pt} heterostructures in a variety of antiferromagnetic spintronic applications, many key structural, electronic, and magnetic properties at the \\ce{Cr2O3-Pt} interface are poorly understood.","We first analyze all inequivalent lateral interface alignments to determine the lowest energy interfacial structure.","For all lateral alignments including the lowest-energy one, we observe an accumulation of electrons at the interface between \\ce{Cr2O3} and Pt.","We find an unexpected reversal of the magnetic moments of the interface Cr ions in the presence of Pt compared to surface Cr moments in vacuum-terminated \\ce{(0001)Cr2O3}.","We also find that the heterostructure exhibits a magnetic proximity effect in the first three Pt layers at the interface with \\ce{Cr2O3}, providing a mechanism by which the anomalous Hall effect can occur in \\ce{(0001)Cr2O3-(111)Pt} bilayers.","Our results provide the basis for a more nuanced interpretation of magnetotransport experiments on \\ce{(0001)Cr2O3-(111)Pt} bilayers and should inform future development of improved antiferromagnetic spintronic devices based on the \\ce{Cr2O3-Pt} material system."],"url":"http://arxiv.org/abs/2405.05020v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 12:42:47","title":"Kerr nonlinearity effect on the stability of Wannier-Stark states in active optical systems","abstract":"The paper provides an analytical and numerical investigation of the dynamics of a one-dimensional chain of coupled optical resonators with conservative cubic nonlinearity and the gain saturated by nonlinear losses. The linear dependency of the resonator eigenfrequencies on their indexes makes it possible to use Wannier-Stark states as lasing modes. Numerical simulations have shown that the dependency of the resonant frequencies on the light intensity strongly affects the stability of Wannier-Stark states. To explain the observed destabilization of monochromatic lasing based on Wannier-Stark states a simple perturbation theory has been developed and compared with the data obtained in the numerical simulations.","sentences":["The paper provides an analytical and numerical investigation of the dynamics of a one-dimensional chain of coupled optical resonators with conservative cubic nonlinearity and the gain saturated by nonlinear losses.","The linear dependency of the resonator eigenfrequencies on their indexes makes it possible to use Wannier-Stark states as lasing modes.","Numerical simulations have shown that the dependency of the resonant frequencies on the light intensity strongly affects the stability of Wannier-Stark states.","To explain the observed destabilization of monochromatic lasing based on Wannier-Stark states a simple perturbation theory has been developed and compared with the data obtained in the numerical simulations."],"url":"http://arxiv.org/abs/2405.05018v1","category":"physics.optics"}
{"created":"2024-05-08 12:34:33","title":"6G Software Engineering: A Systematic Mapping Study","abstract":"6G will revolutionize the software world allowing faster cellular communications and a massive number of connected devices. 6G will enable a shift towards a continuous edge-to-cloud architecture. Current cloud solutions, where all the data is transferred and computed in the cloud, are not sustainable in such a large network of devices. Current technologies, including development methods, software architectures, and orchestration and offloading systems, still need to be prepared to cope with such requirements. In this paper, we conduct a Systematic Mapping Study to investigate the current research status of 6G Software Engineering. Results show that 18 research papers have been proposed in software process, software architecture, orchestration and offloading methods. Of these, software architecture and software-defined networks are respectively areas and topics that have received the most attention in 6G Software Engineering. In addition, the main types of results of these papers are methods, architectures, platforms, frameworks and algorithms. For the five tools/frameworks proposed, they are new and not currently studied by other researchers. The authors of these findings are mainly from China, India and Saudi Arabia. The results will enable researchers and practitioners to further research and extend for 6G Software Engineering.","sentences":["6G will revolutionize the software world allowing faster cellular communications and a massive number of connected devices.","6G will enable a shift towards a continuous edge-to-cloud architecture.","Current cloud solutions, where all the data is transferred and computed in the cloud, are not sustainable in such a large network of devices.","Current technologies, including development methods, software architectures, and orchestration and offloading systems, still need to be prepared to cope with such requirements.","In this paper, we conduct a Systematic Mapping Study to investigate the current research status of 6G Software Engineering.","Results show that 18 research papers have been proposed in software process, software architecture, orchestration and offloading methods.","Of these, software architecture and software-defined networks are respectively areas and topics that have received the most attention in 6G Software Engineering.","In addition, the main types of results of these papers are methods, architectures, platforms, frameworks and algorithms.","For the five tools/frameworks proposed, they are new and not currently studied by other researchers.","The authors of these findings are mainly from China, India and Saudi Arabia.","The results will enable researchers and practitioners to further research and extend for 6G Software Engineering."],"url":"http://arxiv.org/abs/2405.05017v1","category":"cs.SE"}
{"created":"2024-05-08 12:25:20","title":"On Solutions of Systems of Differential Equations on Half-Line with Summable Coefficients","abstract":"We consider a system of differential equations and obtain its solutions with exponential asymptotics and analyticity with respect to the spectral parameter. Solutions of such type have importance in studying spectral properties of differential operators. Here, we consider the system of first-order differential equations on a half-line with summable coefficients, containing a nonlinear dependence on the spectral parameter. We obtain fundamental systems of solutions with analyticity in certain sectors, in which it is possible to apply the method of successive approximations. We also construct non-fundamental systems of solutions with analyticity in a large sector, including two previously considered neighboring sectors. The obtained results admit applications in studying inverse spectral problems for the higher-order differential operators with distribution coefficients.","sentences":["We consider a system of differential equations and obtain its solutions with exponential asymptotics and analyticity with respect to the spectral parameter.","Solutions of such type have importance in studying spectral properties of differential operators.","Here, we consider the system of first-order differential equations on a half-line with summable coefficients, containing a nonlinear dependence on the spectral parameter.","We obtain fundamental systems of solutions with analyticity in certain sectors, in which it is possible to apply the method of successive approximations.","We also construct non-fundamental systems of solutions with analyticity in a large sector, including two previously considered neighboring sectors.","The obtained results admit applications in studying inverse spectral problems for the higher-order differential operators with distribution coefficients."],"url":"http://arxiv.org/abs/2405.05009v1","category":"math.CA"}
{"created":"2024-05-08 12:24:50","title":"HC-Mamba: Vision MAMBA with Hybrid Convolutional Techniques for Medical Image Segmentation","abstract":"Automatic medical image segmentation technology has the potential to expedite pathological diagnoses, thereby enhancing the efficiency of patient care. However, medical images often have complex textures and structures, and the models often face the problem of reduced image resolution and information loss due to downsampling. To address this issue, we propose HC-Mamba, a new medical image segmentation model based on the modern state space model Mamba. Specifically, we introduce the technique of dilated convolution in the HC-Mamba model to capture a more extensive range of contextual information without increasing the computational cost by extending the perceptual field of the convolution kernel. In addition, the HC-Mamba model employs depthwise separable convolutions, significantly reducing the number of parameters and the computational power of the model. By combining dilated convolution and depthwise separable convolutions, HC-Mamba is able to process large-scale medical image data at a much lower computational cost while maintaining a high level of performance. We conduct comprehensive experiments on segmentation tasks including skin lesion, and conduct extensive experiments on ISIC17 and ISIC18 to demonstrate the potential of the HC-Mamba model in medical image segmentation. The experimental results show that HC-Mamba exhibits competitive performance on all these datasets, thereby proving its effectiveness and usefulness in medical image segmentation.","sentences":["Automatic medical image segmentation technology has the potential to expedite pathological diagnoses, thereby enhancing the efficiency of patient care.","However, medical images often have complex textures and structures, and the models often face the problem of reduced image resolution and information loss due to downsampling.","To address this issue, we propose HC-Mamba, a new medical image segmentation model based on the modern state space model Mamba.","Specifically, we introduce the technique of dilated convolution in the HC-Mamba model to capture a more extensive range of contextual information without increasing the computational cost by extending the perceptual field of the convolution kernel.","In addition, the HC-Mamba model employs depthwise separable convolutions, significantly reducing the number of parameters and the computational power of the model.","By combining dilated convolution and depthwise separable convolutions, HC-Mamba is able to process large-scale medical image data at a much lower computational cost while maintaining a high level of performance.","We conduct comprehensive experiments on segmentation tasks including skin lesion, and conduct extensive experiments on ISIC17 and ISIC18 to demonstrate the potential of the HC-Mamba model in medical image segmentation.","The experimental results show that HC-Mamba exhibits competitive performance on all these datasets, thereby proving its effectiveness and usefulness in medical image segmentation."],"url":"http://arxiv.org/abs/2405.05007v1","category":"eess.IV"}
{"created":"2024-05-08 12:24:03","title":"Stochastic spatial Lotka-Volterra predator-prey models","abstract":"Stochastic, spatially extended models for predator-prey interaction display spatio-temporal structures that are not captured by the Lotka-Volterra mean-field rate equations. These spreading activity fronts reflect persistent correlations between predators and prey that can be analyzed through field-theoretic methods. Introducing local restrictions on the prey population induces a predator extinction threshold, with the critical dynamics at this continuous active-to-absorbing state transition governed by the scaling exponents of directed percolation. Novel features in biologically motivated model variants include the stabilizing effect of a periodically varying carrying capacity that describes seasonally oscillating resource availability; enhanced mean species densities and local fluctuations caused by spatially varying reaction rates; and intriguing evolutionary dynamics emerging when variable interaction rates are affixed to individuals combined with trait inheritance to their offspring. The basic susceptible-infected-susceptible and susceptible-infected-recovered models for infectious disease spreading near their epidemic thresholds are respectively captured by the directed and dynamic isotropic percolation universality classes. Systems with three cyclically competing species akin to spatial rock-paper-scissors games may display striking spiral patterns, yet conservation laws can prevent such noise-induced structure formation. In diffusively coupled inhomogeneous settings, one may observe the stabilization of vulnerable ecologies prone to finite-size extinction or fixation due to immigration waves emanating from the interfaces.","sentences":["Stochastic, spatially extended models for predator-prey interaction display spatio-temporal structures that are not captured by the Lotka-Volterra mean-field rate equations.","These spreading activity fronts reflect persistent correlations between predators and prey that can be analyzed through field-theoretic methods.","Introducing local restrictions on the prey population induces a predator extinction threshold, with the critical dynamics at this continuous active-to-absorbing state transition governed by the scaling exponents of directed percolation.","Novel features in biologically motivated model variants include the stabilizing effect of a periodically varying carrying capacity that describes seasonally oscillating resource availability; enhanced mean species densities and local fluctuations caused by spatially varying reaction rates; and intriguing evolutionary dynamics emerging when variable interaction rates are affixed to individuals combined with trait inheritance to their offspring.","The basic susceptible-infected-susceptible and susceptible-infected-recovered models for infectious disease spreading near their epidemic thresholds are respectively captured by the directed and dynamic isotropic percolation universality classes.","Systems with three cyclically competing species akin to spatial rock-paper-scissors games may display striking spiral patterns, yet conservation laws can prevent such noise-induced structure formation.","In diffusively coupled inhomogeneous settings, one may observe the stabilization of vulnerable ecologies prone to finite-size extinction or fixation due to immigration waves emanating from the interfaces."],"url":"http://arxiv.org/abs/2405.05006v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 12:17:21","title":"Nilpotent structures of oriented neutral vector bundles","abstract":"In this paper, we study nilpotent structures of an oriented vector bundle $E$ of rank $4n$ with a neutral metric $h$ and an $h$-connection $\\nabla$. We define $H$-nilpotent structures of $(E, h, \\nabla )$ for a Lie subgroup $H$ of $SO(2n, 2n)$ related to neutral hyperK\\\"{a}hler structures. We observe that there exist a complex structure $I$ and paracomplex structures $J_1$, $J_2$ of $E$ such that $h$, $\\nabla$, $I$, $J_1$, $J_2$ form a neutral hyperK\\\"{a}hler structure of $E$ if and only if there exists an $H$-nilpotent structure of $(E, h, \\nabla )$.","sentences":["In this paper, we study nilpotent structures of an oriented vector bundle $E$ of rank $4n$ with a neutral metric $h$ and an $h$-connection $\\nabla$. We define $H$-nilpotent structures of $(E, h, \\nabla )$ for a Lie subgroup $H$ of $SO(2n, 2n)$ related to neutral hyperK\\\"{a}hler structures.","We observe that there exist a complex structure $I$ and paracomplex structures $J_1$, $J_2$ of $E$ such that $h$, $\\nabla$, $I$, $J_1$, $J_2$ form a neutral hyperK\\\"{a}hler structure of $E$ if and only if there exists an $H$-nilpotent structure of $(E, h, \\nabla )$."],"url":"http://arxiv.org/abs/2405.05003v1","category":"math.DG"}
{"created":"2024-05-08 12:17:17","title":"Tunable Localisation in Parity-Time-Symmetric Resonator Arrays with Imaginary Gauge Potentials","abstract":"The aim of this paper is to illustrate both analytically and numerically the interplay of two fundamentally distinct non-Hermitian mechanisms in a deep subwavelength regime. Considering a parity-time symmetric system of one-dimensional subwavelength resonators equipped with two kinds of non-Hermiticity - an imaginary gauge potential and on-site gain and loss - we prove that all but two eigenmodes of the system decouple when going through an exceptional point. By tuning the gain-to-loss ratio, the system changes from a phase with unbroken parity-time symmetry to a phase with broken parity-time symmetry. At the macroscopic level, this is observed as a transition from symmetrical eigenmodes to condensated eigenmodes at one edge of the structure. Mathematically, it arises from a topological state change. The results of this paper open the door to the justification of a variety of phenomena arising from the interplay between non-Hermitian reciprocal and non-reciprocal mechanisms not only in subwavelength wave physics but also in quantum mechanics where the tight binding model coupled with the nearest neighbour approximation can be analysed with the same tools as those developed here.","sentences":["The aim of this paper is to illustrate both analytically and numerically the interplay of two fundamentally distinct non-Hermitian mechanisms in a deep subwavelength regime.","Considering a parity-time symmetric system of one-dimensional subwavelength resonators equipped with two kinds of non-Hermiticity - an imaginary gauge potential and on-site gain and loss - we prove that all but two eigenmodes of the system decouple when going through an exceptional point.","By tuning the gain-to-loss ratio, the system changes from a phase with unbroken parity-time symmetry to a phase with broken parity-time symmetry.","At the macroscopic level, this is observed as a transition from symmetrical eigenmodes to condensated eigenmodes at one edge of the structure.","Mathematically, it arises from a topological state change.","The results of this paper open the door to the justification of a variety of phenomena arising from the interplay between non-Hermitian reciprocal and non-reciprocal mechanisms not only in subwavelength wave physics but also in quantum mechanics where the tight binding model coupled with the nearest neighbour approximation can be analysed with the same tools as those developed here."],"url":"http://arxiv.org/abs/2405.05002v1","category":"physics.optics"}
{"created":"2024-05-08 12:04:43","title":"Bridging the Gap Between Saliency Prediction and Image Quality Assessment","abstract":"Over the past few years, deep neural models have made considerable advances in image quality assessment (IQA). However, the underlying reasons for their success remain unclear, owing to the complex nature of deep neural networks. IQA aims to describe how the human visual system (HVS) works and to create its efficient approximations. On the other hand, Saliency Prediction task aims to emulate HVS via determining areas of visual interest. Thus, we believe that saliency plays a crucial role in human perception. In this work, we conduct an empirical study that reveals the relation between IQA and Saliency Prediction tasks, demonstrating that the former incorporates knowledge of the latter. Moreover, we introduce a novel SACID dataset of saliency-aware compressed images and conduct a large-scale comparison of classic and neural-based IQA methods. All supplementary code and data will be available at the time of publication.","sentences":["Over the past few years, deep neural models have made considerable advances in image quality assessment (IQA).","However, the underlying reasons for their success remain unclear, owing to the complex nature of deep neural networks.","IQA aims to describe how the human visual system (HVS) works and to create its efficient approximations.","On the other hand, Saliency Prediction task aims to emulate HVS via determining areas of visual interest.","Thus, we believe that saliency plays a crucial role in human perception.","In this work, we conduct an empirical study that reveals the relation between IQA and Saliency Prediction tasks, demonstrating that the former incorporates knowledge of the latter.","Moreover, we introduce a novel SACID dataset of saliency-aware compressed images and conduct a large-scale comparison of classic and neural-based IQA methods.","All supplementary code and data will be available at the time of publication."],"url":"http://arxiv.org/abs/2405.04997v1","category":"cs.CV"}
{"created":"2024-05-08 11:59:31","title":"Spatial distribution of local elastic moduli in nanocrystalline metals","abstract":"Elastoplastic properties of nanocrystalline metals are non-uniform on the scale of the grain size, and this non-uniformity affects macroscopic quantities as, in these systems, a significant part of the material is at or adjacent to a grain boundary.   We use molecular dynamics simulations to study the spatial distributions of local elastic moduli in nano-grained pure metals and analyze their dependence on grain size.   Calculations are performed for copper and tantalum with grain sizes ranging from 5-20nm. Shear modulus distributions for grain and grain-boundary atoms were calculated. It is shown that the non-crystalline grain boundary has a wide shear-modulus distribution, which is grain-size independent, while grains have a peaked distribution, which becomes sharper with increasing grain size. Average elastic moduli of the bulk, grains, and grain boundary are calculated as a function of grain size. The atomistic simulations show that the reduction of total elastic moduli with decreasing grain size is mainly due to a resulting larger grain-boundary atoms fraction, and that the total elastic moduli can be approximated by a simple weighted average of larger grains elastic moduli and a lower grain-boundary elastic moduli.","sentences":["Elastoplastic properties of nanocrystalline metals are non-uniform on the scale of the grain size, and this non-uniformity affects macroscopic quantities as, in these systems, a significant part of the material is at or adjacent to a grain boundary.   ","We use molecular dynamics simulations to study the spatial distributions of local elastic moduli in nano-grained pure metals and analyze their dependence on grain size.   ","Calculations are performed for copper and tantalum with grain sizes ranging from 5-20nm.","Shear modulus distributions for grain and grain-boundary atoms were calculated.","It is shown that the non-crystalline grain boundary has a wide shear-modulus distribution, which is grain-size independent, while grains have a peaked distribution, which becomes sharper with increasing grain size.","Average elastic moduli of the bulk, grains, and grain boundary are calculated as a function of grain size.","The atomistic simulations show that the reduction of total elastic moduli with decreasing grain size is mainly due to a resulting larger grain-boundary atoms fraction, and that the total elastic moduli can be approximated by a simple weighted average of larger grains elastic moduli and a lower grain-boundary elastic moduli."],"url":"http://arxiv.org/abs/2405.04995v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 11:56:20","title":"On Stochastic Fundamental Limits in a Downlink Integrated Sensing and Communication Network","abstract":"This paper aims to analyze the stochastic performance of a multiple input multiple output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario, where a base station (BS) transmits a dual-functional radar-communication (DFRC) signal matrix, serving the purpose of transmitting communication data to the user while simultaneously sensing the angular location of a target. The channel between the BS and the user is modeled as a random channel with Rayleigh fading distribution, and the azimuth angle of the target is assumed to follow a uniform distribution. Due to the randomness inherent in the network, the challenge is to consider suitable performance metrics for this randomness. To address this issue, for users, we employ the user's rate outage probability (OP) and ergodic rate, while for target, we propose using the OP of the Cram\\'er-Rao lower bound (CRLB) for the angle of arrival and the ergodic CRLB. We have obtained the expressions of these metrics for scenarios where the BS employs two different beamforming methods. Our approach to deriving these metrics involves computing the probability density function (PDF) of the signal-to-noise ratio for users and the CRLB for the target. We have demonstrated that the central limit theorem provides a viable approach for deriving these PDFs. In our numerical results, we demonstrate the trade-off between sensing and communication (S \\& C) by characterizing the region of S \\& C metrics and by obtaining the Pareto optimal boundary points, confirmed with simulations.","sentences":["This paper aims to analyze the stochastic performance of a multiple input multiple output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario, where a base station (BS) transmits a dual-functional radar-communication (DFRC) signal matrix, serving the purpose of transmitting communication data to the user while simultaneously sensing the angular location of a target.","The channel between the BS and the user is modeled as a random channel with Rayleigh fading distribution, and the azimuth angle of the target is assumed to follow a uniform distribution.","Due to the randomness inherent in the network, the challenge is to consider suitable performance metrics for this randomness.","To address this issue, for users, we employ the user's rate outage probability (OP) and ergodic rate, while for target, we propose using the OP of the Cram\\'er-Rao lower bound (CRLB) for the angle of arrival and the ergodic CRLB.","We have obtained the expressions of these metrics for scenarios where the BS employs two different beamforming methods.","Our approach to deriving these metrics involves computing the probability density function (PDF) of the signal-to-noise ratio for users and the CRLB for the target.","We have demonstrated that the central limit theorem provides a viable approach for deriving these PDFs.","In our numerical results, we demonstrate the trade-off between sensing and communication (S \\& C) by characterizing the region of S \\& C metrics and by obtaining the Pareto optimal boundary points, confirmed with simulations."],"url":"http://arxiv.org/abs/2405.04993v1","category":"cs.IT"}
{"created":"2024-05-08 11:46:00","title":"Dynamic Data Layout Optimization with Worst-case Guarantees","abstract":"Many data analytics systems store and process large datasets in partitions containing millions of rows. By mapping rows to partitions in an optimized way, it is possible to improve query performance by skipping over large numbers of irrelevant partitions during query processing. This mapping is referred to as a data layout. Recent works have shown that customizing the data layout to the anticipated query workload greatly improves query performance, but the performance benefits may disappear if the workload changes. Reorganizing data layouts to accommodate workload drift can resolve this issue, but reorganization costs could exceed query savings if not done carefully.   In this paper, we present an algorithmic framework OReO that makes online reorganization decisions to balance the benefits of improved query performance with the costs of reorganization. Our framework extends results from Metrical Task Systems to provide a tight bound on the worst-case performance guarantee for online reorganization, without prior knowledge of the query workload. Through evaluation on real-world datasets and query workloads, our experiments demonstrate that online reorganization with OReO can lead to an up to 32% improvement in combined query and reorganization time compared to using a single, optimized data layout for the entire workload.","sentences":["Many data analytics systems store and process large datasets in partitions containing millions of rows.","By mapping rows to partitions in an optimized way, it is possible to improve query performance by skipping over large numbers of irrelevant partitions during query processing.","This mapping is referred to as a data layout.","Recent works have shown that customizing the data layout to the anticipated query workload greatly improves query performance, but the performance benefits may disappear if the workload changes.","Reorganizing data layouts to accommodate workload drift can resolve this issue, but reorganization costs could exceed query savings if not done carefully.   ","In this paper, we present an algorithmic framework OReO that makes online reorganization decisions to balance the benefits of improved query performance with the costs of reorganization.","Our framework extends results from Metrical Task Systems to provide a tight bound on the worst-case performance guarantee for online reorganization, without prior knowledge of the query workload.","Through evaluation on real-world datasets and query workloads, our experiments demonstrate that online reorganization with OReO can lead to an up to 32% improvement in combined query and reorganization time compared to using a single, optimized data layout for the entire workload."],"url":"http://arxiv.org/abs/2405.04984v1","category":"cs.DB"}
{"created":"2024-05-08 11:41:22","title":"A unified theory of the self-similar supersonic Marshak wave problem","abstract":"We present a systematic study of the similarity solutions for the Marshak wave problem, in the local thermodynamic equilibrium (LTE) diffusion approximation and in the supersonic regime. Self-similar solutions exist for a temporal power law surface temperature drive and a material model with power law temperature dependent opacity and energy density. The properties of the solutions in both linear and nonlinear conduction regimes are studied as a function of the temporal drive, opacity and energy density exponents. We show that there exists a range of the temporal exponent for which the total energy in the system decreases, and the solution has a local maxima. For nonlinear conduction, we specify the conditions on the opacity and energy density exponents under which the heat front is linear or even flat, and does posses its common sharp character; this character is independent of the drive exponent. We specify the values of the temporal exponents for which analytical solutions exist and employ the Hammer-Rosen perturbation theory to obtain highly accurate approximate solutions, which are parameterized using only two numerically fitted quantities. The solutions are used to construct a set of benchmarks for supersonic LTE radiative heat transfer, including some with unusual and interesting properties such as local maxima and non sharp fronts. The solutions are compared in detail to implicit Monte-Carlo and discrete-ordinate transport simulations as well gray diffusion simulations, showing a good agreement, which highlights their usefulness as a verification test problem for radiative transfer simulations.","sentences":["We present a systematic study of the similarity solutions for the Marshak wave problem, in the local thermodynamic equilibrium (LTE) diffusion approximation and in the supersonic regime.","Self-similar solutions exist for a temporal power law surface temperature drive and a material model with power law temperature dependent opacity and energy density.","The properties of the solutions in both linear and nonlinear conduction regimes are studied as a function of the temporal drive, opacity and energy density exponents.","We show that there exists a range of the temporal exponent for which the total energy in the system decreases, and the solution has a local maxima.","For nonlinear conduction, we specify the conditions on the opacity and energy density exponents under which the heat front is linear or even flat, and does posses its common sharp character; this character is independent of the drive exponent.","We specify the values of the temporal exponents for which analytical solutions exist and employ the Hammer-Rosen perturbation theory to obtain highly accurate approximate solutions, which are parameterized using only two numerically fitted quantities.","The solutions are used to construct a set of benchmarks for supersonic LTE radiative heat transfer, including some with unusual and interesting properties such as local maxima and non sharp fronts.","The solutions are compared in detail to implicit Monte-Carlo and discrete-ordinate transport simulations as well gray diffusion simulations, showing a good agreement, which highlights their usefulness as a verification test problem for radiative transfer simulations."],"url":"http://arxiv.org/abs/2405.04981v1","category":"astro-ph.HE"}
{"created":"2024-05-08 11:39:16","title":"Predictive Mapping of Spectral Signatures from RGB Imagery for Off-Road Terrain Analysis","abstract":"Accurate identification of complex terrain characteristics, such as soil composition and coefficient of friction, is essential for model-based planning and control of mobile robots in off-road environments. Spectral signatures leverage distinct patterns of light absorption and reflection to identify various materials, enabling precise characterization of their inherent properties. Recent research in robotics has explored the adoption of spectroscopy to enhance perception and interaction with environments. However, the significant cost and elaborate setup required for mounting these sensors present formidable barriers to widespread adoption. In this study, we introduce RS-Net (RGB to Spectral Network), a deep neural network architecture designed to map RGB images to corresponding spectral signatures. We illustrate how RS-Net can be synergistically combined with Co-Learning techniques for terrain property estimation. Initial results demonstrate the effectiveness of this approach in characterizing spectral signatures across an extensive off-road real-world dataset. These findings highlight the feasibility of terrain property estimation using only RGB cameras.","sentences":["Accurate identification of complex terrain characteristics, such as soil composition and coefficient of friction, is essential for model-based planning and control of mobile robots in off-road environments.","Spectral signatures leverage distinct patterns of light absorption and reflection to identify various materials, enabling precise characterization of their inherent properties.","Recent research in robotics has explored the adoption of spectroscopy to enhance perception and interaction with environments.","However, the significant cost and elaborate setup required for mounting these sensors present formidable barriers to widespread adoption.","In this study, we introduce RS-Net (RGB to Spectral Network), a deep neural network architecture designed to map RGB images to corresponding spectral signatures.","We illustrate how RS-Net can be synergistically combined with Co-Learning techniques for terrain property estimation.","Initial results demonstrate the effectiveness of this approach in characterizing spectral signatures across an extensive off-road real-world dataset.","These findings highlight the feasibility of terrain property estimation using only RGB cameras."],"url":"http://arxiv.org/abs/2405.04979v1","category":"cs.RO"}
{"created":"2024-05-08 11:09:24","title":"Frequency-Assisted Mamba for Remote Sensing Image Super-Resolution","abstract":"Recent progress in remote sensing image (RSI) super-resolution (SR) has exhibited remarkable performance using deep neural networks, e.g., Convolutional Neural Networks and Transformers. However, existing SR methods often suffer from either a limited receptive field or quadratic computational overhead, resulting in sub-optimal global representation and unacceptable computational costs in large-scale RSI. To alleviate these issues, we develop the first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR, which specializes in processing large-scale RSI by capturing long-range dependency with linear complexity. To achieve better SR reconstruction, building upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed FMSR, to explore the spatial and frequent correlations. In particular, our FMSR features a multi-level fusion architecture equipped with the Frequency Selection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate Module (HGM) to grasp their merits for effective spatial-frequency fusion. Recognizing that global and local dependencies are complementary and both beneficial for SR, we further recalibrate these multi-level features for accurate feature fusion via learnable scaling adaptors. Extensive experiments on AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms state-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on average, while consuming only 28.05% and 19.08% of its memory consumption and complexity, respectively.","sentences":["Recent progress in remote sensing image (RSI) super-resolution (SR) has exhibited remarkable performance using deep neural networks, e.g., Convolutional Neural Networks and Transformers.","However, existing SR methods often suffer from either a limited receptive field or quadratic computational overhead, resulting in sub-optimal global representation and unacceptable computational costs in large-scale RSI.","To alleviate these issues, we develop the first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR, which specializes in processing large-scale RSI by capturing long-range dependency with linear complexity.","To achieve better SR reconstruction, building upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed FMSR, to explore the spatial and frequent correlations.","In particular, our FMSR features a multi-level fusion architecture equipped with the Frequency Selection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate Module (HGM) to grasp their merits for effective spatial-frequency fusion.","Recognizing that global and local dependencies are complementary and both beneficial for SR, we further recalibrate these multi-level features for accurate feature fusion via learnable scaling adaptors.","Extensive experiments on AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms state-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on average, while consuming only 28.05% and 19.08% of its memory consumption and complexity, respectively."],"url":"http://arxiv.org/abs/2405.04964v1","category":"cs.CV"}
{"created":"2024-05-08 11:03:44","title":"Bistatic OFDM-based ISAC with Over-the-Air Synchronization: System Concept and Performance Analysis","abstract":"Integrated sensing and communication (ISAC) has been defined as one goal for 6G mobile communication systems. In this context, this article introduces a bistatic ISAC system based on orthogonal frequency-division multiplexing (OFDM). While the bistatic architecture brings advantages such as not demanding full duplex operation with respect to the monostatic one, the need for synchronizing transmitter and receiver is imposed. In this context, this article introuces a bistatic ISAC signal processing framework where an incoming OFDM-based ISAC signal undergoes over-the-air synchronization based on preamble symbols and pilots. Afterwards, bistatic radar processing is performed using either only pilot subcarriers or the full OFDM frame. The latter approach requires estimation of the originally transmitted frame based on communication processing and therefore error-free communication, which can be achieved via appropriate channel coding. The performance and limitations of the introduced system based on both aforementioned approaches are assessed via an analysis of the impact of residual synchronization mismatches and data decoding failures on both communication and radar performances. Finally, the performed analyses are validated by proof-of-concept measurement results.","sentences":["Integrated sensing and communication (ISAC) has been defined as one goal for 6G mobile communication systems.","In this context, this article introduces a bistatic ISAC system based on orthogonal frequency-division multiplexing (OFDM).","While the bistatic architecture brings advantages such as not demanding full duplex operation with respect to the monostatic one, the need for synchronizing transmitter and receiver is imposed.","In this context, this article introuces a bistatic ISAC signal processing framework where an incoming OFDM-based ISAC signal undergoes over-the-air synchronization based on preamble symbols and pilots.","Afterwards, bistatic radar processing is performed using either only pilot subcarriers or the full OFDM frame.","The latter approach requires estimation of the originally transmitted frame based on communication processing and therefore error-free communication, which can be achieved via appropriate channel coding.","The performance and limitations of the introduced system based on both aforementioned approaches are assessed via an analysis of the impact of residual synchronization mismatches and data decoding failures on both communication and radar performances.","Finally, the performed analyses are validated by proof-of-concept measurement results."],"url":"http://arxiv.org/abs/2405.04962v1","category":"eess.SP"}
{"created":"2024-05-08 10:47:28","title":"Supervised Anomaly Detection for Complex Industrial Images","abstract":"Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentation-based Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available.","sentences":["Automating visual inspection in industrial production lines is essential for increasing product quality across various industries.","Anomaly detection (AD) methods serve as robust tools for this purpose.","However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings.","To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses.","Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentation-based Anomaly Detector (SegAD).","First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics.","Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score.","Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC).","The code and the models are publicly available."],"url":"http://arxiv.org/abs/2405.04953v1","category":"cs.CV"}
{"created":"2024-05-08 10:42:36","title":"Trapping and extreme clustering of finitely-dense inertial particles near a rotating vortex pair","abstract":"Small heavy particles cannot get attracted into a region of closed streamlines in a non-accelerating frame (Sapsis & Haller 2010). In a rotating system, however, particles can get trapped (Angilella 2010) near vortices. We perform numerical simulations examining trapping of inertial particles in a prototypical rotating flow: an identical pair of rotating Lamb-Oseen vortices, without gravity. Our parameter space includes the particle Stokes number $St$, measuring the particle's inertia, and a density parameter $R$, measuring the particle-to-fluid relative density. We focus on inertial particles that are finitely denser than the fluid. Particles can get indefinitely trapped near the vortices and display extreme clustering into smaller dimensional objects: attracting fixed-points, limit cycles and chaotic attractors. As $St$ increases for a given $R$, we may have an incomplete or complete period-doubling route to chaos, as well as an unusual period-halving route back to a fixed-point attractor. The fraction of trapped particles can vary non-monotonically with $St$. We may even have windows in $St$ for which no particle trapping occurs. At $St$ larger than a critical value, beyond no trapping occurs, significant fractions of particles can spend long but finite times in the vortex vicinity. The inclusion of the Basset-Boussinesq history (BBH) force is imperative in our study due to particle's finite density. BBH force significantly increases the basin of attraction as well as the range of $St$ where trapping can occur. Extreme clustering can be physically significant in planetesimal formation by dust aggregation in protoplanetary disks, phytoplankton aggregation in oceans, etc.","sentences":["Small heavy particles cannot get attracted into a region of closed streamlines in a non-accelerating frame (Sapsis & Haller 2010).","In a rotating system, however, particles can get trapped (Angilella 2010) near vortices.","We perform numerical simulations examining trapping of inertial particles in a prototypical rotating flow: an identical pair of rotating Lamb-Oseen vortices, without gravity.","Our parameter space includes the particle Stokes number $St$, measuring the particle's inertia, and a density parameter $R$, measuring the particle-to-fluid relative density.","We focus on inertial particles that are finitely denser than the fluid.","Particles can get indefinitely trapped near the vortices and display extreme clustering into smaller dimensional objects: attracting fixed-points, limit cycles and chaotic attractors.","As $St$ increases for a given $R$, we may have an incomplete or complete period-doubling route to chaos, as well as an unusual period-halving route back to a fixed-point attractor.","The fraction of trapped particles can vary non-monotonically with $St$. We may even have windows in $St$ for which no particle trapping occurs.","At $St$ larger than a critical value, beyond no trapping occurs, significant fractions of particles can spend long but finite times in the vortex vicinity.","The inclusion of the Basset-Boussinesq history (BBH) force is imperative in our study due to particle's finite density.","BBH force significantly increases the basin of attraction as well as the range of $St$ where trapping can occur.","Extreme clustering can be physically significant in planetesimal formation by dust aggregation in protoplanetary disks, phytoplankton aggregation in oceans, etc."],"url":"http://arxiv.org/abs/2405.04949v1","category":"physics.flu-dyn"}
{"created":"2024-05-08 10:34:55","title":"Isospin symmetry breaking in atomic nuclei","abstract":"The importance of the isospin symmetry and its breaking in elucidating the properties of atomic nuclei is reviewed. The quark mass splitting and the electromagnetic origin of the isospin symmetry breaking (ISB) for nuclear many-body problem is discussed. The experimental data on isobaric analogue states cannot be described only with the Coulomb interaction, and ISB terms in the nucleon-nucleon interaction are needed to discern the observed properties. In the present work, the ISB terms are explicitly considered in nuclear energy density functional and spherical shell model approaches, and a detailed investigation of the analogue states and other properties of nuclei is performed. It is observed that isospin mixing is largest for the $N=Z$ system in the density functional approach.","sentences":["The importance of the isospin symmetry and its breaking in elucidating the properties of atomic nuclei is reviewed.","The quark mass splitting and the electromagnetic origin of the isospin symmetry breaking (ISB) for nuclear many-body problem is discussed.","The experimental data on isobaric analogue states cannot be described only with the Coulomb interaction, and ISB terms in the nucleon-nucleon interaction are needed to discern the observed properties.","In the present work, the ISB terms are explicitly considered in nuclear energy density functional and spherical shell model approaches, and a detailed investigation of the analogue states and other properties of nuclei is performed.","It is observed that isospin mixing is largest for the $N=Z$ system in the density functional approach."],"url":"http://arxiv.org/abs/2405.04946v1","category":"nucl-th"}
{"created":"2024-05-08 10:33:19","title":"On quadrirational pentagon maps","abstract":"We classify rational solutions of a specific type of the set theoretical version of the pentagon equation. That is, we find all quadrirational maps $R:(x,y)\\mapsto (u(x,y),v(x,y)),$ where $u, v$ are two rational functions on two arguments, that serve as solutions of the pentagon equation. Furthermore, provided a pentagon map that admits a partial inverse, we obtain genuine entwining pentagon set theoretical solutions.","sentences":["We classify rational solutions of a specific type of the set theoretical version of the pentagon equation.","That is, we find all quadrirational maps $R:(x,y)\\mapsto (u(x,y),v(x,y)),$ where $u, v$ are two rational functions on two arguments, that serve as solutions of the pentagon equation.","Furthermore, provided a pentagon map that admits a partial inverse, we obtain genuine entwining pentagon set theoretical solutions."],"url":"http://arxiv.org/abs/2405.04945v1","category":"nlin.SI"}
{"created":"2024-05-08 10:23:08","title":"Dual-domain Collaborative Denoising for Social Recommendation","abstract":"Social recommendation leverages social network to complement user-item interaction data for recommendation task, aiming to mitigate the data sparsity issue in recommender systems. However, existing social recommendation methods encounter the following challenge: both social network and interaction data contain substaintial noise, and the propagation of such noise through Graph Neural Networks (GNNs) not only fails to enhance recommendation performance but may also interfere with the model's normal training. Despite the importance of denoising for social network and interaction data, only a limited number of studies have considered the denoising for social network and all of them overlook that for interaction data, hindering the denoising effect and recommendation performance. Based on this, we propose a novel model called Dual-domain Collaborative Denoising for Social Recommendation ($\\textbf{DCDSR}$). DCDSR comprises two primary modules: the structure-level collaborative denoising module and the embedding-space collaborative denoising module. In the structure-level collaborative denoising module, information from interaction domain is first employed to guide social network denoising. Subsequently, the denoised social network is used to supervise the denoising for interaction data. The embedding-space collaborative denoising module devotes to resisting the noise cross-domain diffusion problem through contrastive learning with dual-domain embedding collaborative perturbation. Additionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is introduced to better harness the denoising capability of contrastive learning. Evaluating our model on three real-world datasets verifies that DCDSR has a considerable denoising effect, thus outperforms the state-of-the-art social recommendation methods.","sentences":["Social recommendation leverages social network to complement user-item interaction data for recommendation task, aiming to mitigate the data sparsity issue in recommender systems.","However, existing social recommendation methods encounter the following challenge: both social network and interaction data contain substaintial noise, and the propagation of such noise through Graph Neural Networks (GNNs) not only fails to enhance recommendation performance but may also interfere with the model's normal training.","Despite the importance of denoising for social network and interaction data, only a limited number of studies have considered the denoising for social network and all of them overlook that for interaction data, hindering the denoising effect and recommendation performance.","Based on this, we propose a novel model called Dual-domain Collaborative Denoising for Social Recommendation ($\\textbf{DCDSR}$).","DCDSR comprises two primary modules: the structure-level collaborative denoising module and the embedding-space collaborative denoising module.","In the structure-level collaborative denoising module, information from interaction domain is first employed to guide social network denoising.","Subsequently, the denoised social network is used to supervise the denoising for interaction data.","The embedding-space collaborative denoising module devotes to resisting the noise cross-domain diffusion problem through contrastive learning with dual-domain embedding collaborative perturbation.","Additionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is introduced to better harness the denoising capability of contrastive learning.","Evaluating our model on three real-world datasets verifies that DCDSR has a considerable denoising effect, thus outperforms the state-of-the-art social recommendation methods."],"url":"http://arxiv.org/abs/2405.04942v1","category":"cs.IR"}
{"created":"2024-05-08 10:10:59","title":"Phases Transition Mechanism in the Growth of WS2 and MoS2 Layers: Ab Initio Data Driving Machine Learning Molecular Dynamics","abstract":"Accurate and large-scale atomic simulation is crucial for understanding growth mechanism and precise synthesis control of crystals. However, it is beyond the capacities of both ab initio and classical molecular dynamics (MD). This study takes a feasible way by developing ab initio data to accurate machine learning interatomic potentials (MLIPs) to explore the complex growth processes. It successfully simulated the growth of monolayer, bilayer MoS$_2$/WS$_2$ and MoS$_2$/WS$_2$ heterostructures under variable conditions. Importantly, a SMMS (M is Mo/W atom) is disclosed as a new 2D structure with high stability during the growth. The two-step vapor-deposition can be regarded as the alternating between MS$_2$ and SMMS structures. Besides, the intermediate SMMS structure easy forms alloys can result in production of Mo$_x$W$_{1-x}$S$_2$ alloy. Furthermore, metallic SMMS is an ideal self-intercalating electrode for transition metal dichalcogenides based FET. This research provides a mode that ab initio data driving MLIPs to efficient and accurate atomic growth simulation and design of materials.","sentences":["Accurate and large-scale atomic simulation is crucial for understanding growth mechanism and precise synthesis control of crystals.","However, it is beyond the capacities of both ab initio and classical molecular dynamics (MD).","This study takes a feasible way by developing ab initio data to accurate machine learning interatomic potentials (MLIPs) to explore the complex growth processes.","It successfully simulated the growth of monolayer, bilayer MoS$_2$/WS$_2$ and MoS$_2$/WS$_2$ heterostructures under variable conditions.","Importantly, a SMMS (M is Mo/W atom) is disclosed as a new 2D structure with high stability during the growth.","The two-step vapor-deposition can be regarded as the alternating between MS$_2$ and SMMS structures.","Besides, the intermediate SMMS structure easy forms alloys can result in production of Mo$_x$W$_{1-x}$S$_2$ alloy.","Furthermore, metallic SMMS is an ideal self-intercalating electrode for transition metal dichalcogenides based FET.","This research provides a mode that ab initio data driving MLIPs to efficient and accurate atomic growth simulation and design of materials."],"url":"http://arxiv.org/abs/2405.04939v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 09:55:59","title":"Mapping reaction mechanism during overcharge of a LiNiO2/Graphite-silicon lithium-ion battery: a correlative operando approach by simultaneous gas analysis and synchrotron scattering techniques","abstract":"Li-ion battery degradation processes are multi-scale, heterogeneous, dynamic and involve multiple cell components through cross talk mechanisms. Correlated operando characterization capable of measuring several key parameters are needed to accelerate understanding on these complex degradation processes. In particular, degradation mechanisms during overcharge of LiNiO2/Graphite-Silicon is well known at the material level featuring O2 gas release and concomitant surface reconstruction of LiNiO2. However, there are still debates regarding the role of high voltage O1 phase formation on gas production and no information on the effect of produced gases on the cell components (anode or sensors), or effect of overcharge on electrode level behavior. In this work, we simultaneously measured the gas produced using operando mass spectrometry while spatially resolving nanostructure and lattice changes using operando micro SAXS/WAXS mapping during the formation and over charge of a LiNiO2/Gr-Si pouch cell. This new correlated operando characterization experiment allowed to (1) confirm the absence of O1 phase even with substantial gas produced at end of charge, (2) unveil the effect of gases on reference and negative electrodes, (3) show that overcharge increases in-plane reaction heterogeneities by creating local degraded spots lagging behind the ensemble electrochemistry. These findings will be important to optimize ageing of devices based on similar chemistries, in particular Ni-rich NMC, while showing the strength of correlated characterization leading to more efficient and robust information on complex mechanisms.","sentences":["Li-ion battery degradation processes are multi-scale, heterogeneous, dynamic and involve multiple cell components through cross talk mechanisms.","Correlated operando characterization capable of measuring several key parameters are needed to accelerate understanding on these complex degradation processes.","In particular, degradation mechanisms during overcharge of LiNiO2/Graphite-Silicon is well known at the material level featuring O2 gas release and concomitant surface reconstruction of LiNiO2.","However, there are still debates regarding the role of high voltage O1 phase formation on gas production and no information on the effect of produced gases on the cell components (anode or sensors), or effect of overcharge on electrode level behavior.","In this work, we simultaneously measured the gas produced using operando mass spectrometry while spatially resolving nanostructure and lattice changes using operando micro SAXS/WAXS mapping during the formation and over charge of a LiNiO2/Gr-Si pouch cell.","This new correlated operando characterization experiment allowed to (1) confirm the absence of O1 phase even with substantial gas produced at end of charge, (2) unveil the effect of gases on reference and negative electrodes, (3) show that overcharge increases in-plane reaction heterogeneities by creating local degraded spots lagging behind the ensemble electrochemistry.","These findings will be important to optimize ageing of devices based on similar chemistries, in particular Ni-rich NMC, while showing the strength of correlated characterization leading to more efficient and robust information on complex mechanisms."],"url":"http://arxiv.org/abs/2405.04931v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 09:53:53","title":"$C^\\infty$ well-posedness of higher order hyperbolic pseudo-differential equations with multiplicities","abstract":"In this paper, we study higher order hyperbolic pseudo-differential equations with variable multiplicities. We work in arbitrary space dimension and we assume that the principal part is time-dependent only. We identify sufficient conditions on the roots and the lower order terms (Levi conditions) under which the corresponding Cauchy problem is $C^\\infty$ well-posed. This is achieved via transformation into a first order system, reduction into upper-triangular form and application of suitable Fourier integral operator methods previously developed for hyperbolic non-diagonalisable systems. We also discuss how our result compares with the literature on second and third order hyperbolic equations.","sentences":["In this paper, we study higher order hyperbolic pseudo-differential equations with variable multiplicities.","We work in arbitrary space dimension and we assume that the principal part is time-dependent only.","We identify sufficient conditions on the roots and the lower order terms (Levi conditions) under which the corresponding Cauchy problem is $C^\\infty$ well-posed.","This is achieved via transformation into a first order system, reduction into upper-triangular form and application of suitable Fourier integral operator methods previously developed for hyperbolic non-diagonalisable systems.","We also discuss how our result compares with the literature on second and third order hyperbolic equations."],"url":"http://arxiv.org/abs/2405.04927v1","category":"math.AP"}
{"created":"2024-05-08 09:52:05","title":"Power-Domain Interference Graph Estimation for Full-Duplex Millimeter-Wave Backhauling","abstract":"Traditional wisdom for network resource management allocates separate frequency-time resources for measurement and data transmission tasks. As a result, the two types of tasks have to compete for resources, and a heavy measurement task inevitably reduces available resources for data transmission. This prevents interference graph estimation (IGE), a heavy yet important measurement task, from being widely used in practice. To resolve this issue, we propose to use power as a new dimension for interference measurement in full-duplex millimeter-wave backhaul networks, such that data transmission and measurement can be done simultaneously using the same frequency-time resources. Our core insight is to consider the mmWave network as a linear system, where the received power of a node is a linear combination of the channel gains. By controlling the powers of transmitters, we can find unique solutions for the channel gains of interference links and use them to estimate the interference. To accomplish resource allocation and IGE simultaneously, we jointly optimize resource allocation and IGE with power control. Extensive simulations show that significant links in the interference graph can be accurately estimated with minimal extra power consumption, independent of the time and carrier frequency offsets between nodes.","sentences":["Traditional wisdom for network resource management allocates separate frequency-time resources for measurement and data transmission tasks.","As a result, the two types of tasks have to compete for resources, and a heavy measurement task inevitably reduces available resources for data transmission.","This prevents interference graph estimation (IGE), a heavy yet important measurement task, from being widely used in practice.","To resolve this issue, we propose to use power as a new dimension for interference measurement in full-duplex millimeter-wave backhaul networks, such that data transmission and measurement can be done simultaneously using the same frequency-time resources.","Our core insight is to consider the mmWave network as a linear system, where the received power of a node is a linear combination of the channel gains.","By controlling the powers of transmitters, we can find unique solutions for the channel gains of interference links and use them to estimate the interference.","To accomplish resource allocation and IGE simultaneously, we jointly optimize resource allocation and IGE with power control.","Extensive simulations show that significant links in the interference graph can be accurately estimated with minimal extra power consumption, independent of the time and carrier frequency offsets between nodes."],"url":"http://arxiv.org/abs/2405.04926v1","category":"cs.NI"}
{"created":"2024-05-08 09:45:23","title":"The dynamical age of the LMC globular cluster NGC 1835 using the \"dynamical clock\"","abstract":"In the context of the study of the size-age relationship observed in star clusters in the Large Magellanic Cloud and the investigation of its origin, here we present the determination of the structural parameters and the dynamical age of the massive cluster NGC 1835. We have used a powerful combination of optical and near-ultraviolet images acquired with the WFC3 onboard the HST to construct the star density profile from resolved star counts, determining the values of the core, half-mass and tidal radii through the comparison with the King model family. The same data also allowed us to evaluate the dynamical age of the cluster by using the 'dynamical clock'. This is an empirical method that quantifies the level of central segregation of blue stragglers stars (BSSs) within the cluster half-mass radius by means of the A+ parameter, which is defined as the area enclosed between the cumulative radial distribution of BSSs and that of a reference (lighter) population. The results confirm that NGC 1835 is a very compact cluster with a core radius of only 0.84 pc. The estimated value of A+ ($0.30\\pm 0.04$) is the largest measured so far in the LMC clusters, providing evidence of a highly dynamically evolved stellar system. NGC 1835 nicely fits into the correlation between A+ and the central relaxation time and in the anti-correlation between A+ and the core radius defined by the Galactic and the Magellanic Cloud clusters investigated to date.","sentences":["In the context of the study of the size-age relationship observed in star clusters in the Large Magellanic Cloud and the investigation of its origin, here we present the determination of the structural parameters and the dynamical age of the massive cluster NGC 1835.","We have used a powerful combination of optical and near-ultraviolet images acquired with the WFC3 onboard the HST to construct the star density profile from resolved star counts, determining the values of the core, half-mass and tidal radii through the comparison with the King model family.","The same data also allowed us to evaluate the dynamical age of the cluster by using the 'dynamical clock'.","This is an empirical method that quantifies the level of central segregation of blue stragglers stars (BSSs) within the cluster half-mass radius by means of the A+ parameter, which is defined as the area enclosed between the cumulative radial distribution of BSSs and that of a reference (lighter) population.","The results confirm that NGC 1835 is a very compact cluster with a core radius of only 0.84 pc.","The estimated value of A+ ($0.30\\pm 0.04$) is the largest measured so far in the LMC clusters, providing evidence of a highly dynamically evolved stellar system.","NGC 1835 nicely fits into the correlation between A+ and the central relaxation time and in the anti-correlation between A+ and the core radius defined by the Galactic and the Magellanic Cloud clusters investigated to date."],"url":"http://arxiv.org/abs/2405.04922v1","category":"astro-ph.GA"}
{"created":"2024-05-08 09:24:10","title":"A progressive data-augmented RANS model for enhanced wind-farm simulations","abstract":"The development of advanced simulation tools is essential, both presently and in the future, for improving wind-energy design strategies, paving the way for a complete transition to sustainable solutions. The Reynolds-averaged Navier-Stokes (RANS) models are pivotal in enhancing our comprehension of the complex flow within and around wind farms and, hence, improving their capacity to accurately model turbulence within this context is a vital research goal. The enhancement is essential for a precise prediction of wake recovery and for capturing intricate flow phenomena such as secondary flows of Prandtl's second kind behind the turbines. To reach these objectives, here, we propose a progressive data-augmentation approach. We first incorporate the turbine-induced forces in the turbulent kinetic energy equation of the widely used $k-\\omega\\text{SST}$ model. Afterward, we utilize data from large-eddy simulations to progressively enhance the Reynolds-stress prediction of this baseline model, accurately capturing the evolution of eddy viscosity in the wake, as well as the emergence of secondary flows. We then apply the optimized model to two unseen cases with distinct layouts and conduct a comparative analysis focusing on the obtained quantities such as normalized streamwise velocity deficit, turbulence intensity, and power output. We also examine the success rate of the augmented model in predicting the secondary flows in the wake region. Our comparisons and validations demonstrate the superior performance of the progressive data-augmented model over the standard version in all cases considered in this study.","sentences":["The development of advanced simulation tools is essential, both presently and in the future, for improving wind-energy design strategies, paving the way for a complete transition to sustainable solutions.","The Reynolds-averaged Navier-Stokes (RANS) models are pivotal in enhancing our comprehension of the complex flow within and around wind farms and, hence, improving their capacity to accurately model turbulence within this context is a vital research goal.","The enhancement is essential for a precise prediction of wake recovery and for capturing intricate flow phenomena such as secondary flows of Prandtl's second kind behind the turbines.","To reach these objectives, here, we propose a progressive data-augmentation approach.","We first incorporate the turbine-induced forces in the turbulent kinetic energy equation of the widely used $k-\\omega\\text{SST}$ model.","Afterward, we utilize data from large-eddy simulations to progressively enhance the Reynolds-stress prediction of this baseline model, accurately capturing the evolution of eddy viscosity in the wake, as well as the emergence of secondary flows.","We then apply the optimized model to two unseen cases with distinct layouts and conduct a comparative analysis focusing on the obtained quantities such as normalized streamwise velocity deficit, turbulence intensity, and power output.","We also examine the success rate of the augmented model in predicting the secondary flows in the wake region.","Our comparisons and validations demonstrate the superior performance of the progressive data-augmented model over the standard version in all cases considered in this study."],"url":"http://arxiv.org/abs/2405.04906v1","category":"physics.flu-dyn"}
{"created":"2024-05-08 09:23:53","title":"Shadowing of actions of hyperbolic groups on their boundaries","abstract":"We prove that the canonical action of every hyperbolic group on its Gromov boundary has the shadowing (aka pseudo-orbit tracing) property. In particular, this recovers the results of Mann et al. that such actions are topologically stable.","sentences":["We prove that the canonical action of every hyperbolic group on its Gromov boundary has the shadowing (aka pseudo-orbit tracing) property.","In particular, this recovers the results of Mann et al. that such actions are topologically stable."],"url":"http://arxiv.org/abs/2405.04905v1","category":"math.GR"}
{"created":"2024-05-08 09:08:43","title":"MoveTouch: Robotic Motion Capturing System with Wearable Tactile Display to Achieve Safe HRI","abstract":"The collaborative robot market is flourishing as there is a trend towards simplification, modularity, and increased flexibility on the production line. But when humans and robots are collaborating in a shared environment, the safety of humans should be a priority. We introduce a novel wearable robotic system to enhance safety during Human Robot Interaction (HRI). The proposed wearable robot is designed to hold a fiducial marker and maintain its visibility to the tracking system, which, in turn, localizes the user's hand with good accuracy and low latency and provides haptic feedback on the user's wrist. The haptic feedback guides the user's hand movement during collaborative tasks in order to increase safety and enhance collaboration efficiency. A user study was conducted to assess the recognition and discriminability of ten designed haptic patterns applied to the volar and dorsal parts of the user's wrist. As a result, four patterns with a high recognition rate were chosen to be incorporated into our system. A second experiment was carried out to evaluate the system integration into real-world collaborative tasks.","sentences":["The collaborative robot market is flourishing as there is a trend towards simplification, modularity, and increased flexibility on the production line.","But when humans and robots are collaborating in a shared environment, the safety of humans should be a priority.","We introduce a novel wearable robotic system to enhance safety during Human Robot Interaction (HRI).","The proposed wearable robot is designed to hold a fiducial marker and maintain its visibility to the tracking system, which, in turn, localizes the user's hand with good accuracy and low latency and provides haptic feedback on the user's wrist.","The haptic feedback guides the user's hand movement during collaborative tasks in order to increase safety and enhance collaboration efficiency.","A user study was conducted to assess the recognition and discriminability of ten designed haptic patterns applied to the volar and dorsal parts of the user's wrist.","As a result, four patterns with a high recognition rate were chosen to be incorporated into our system.","A second experiment was carried out to evaluate the system integration into real-world collaborative tasks."],"url":"http://arxiv.org/abs/2405.04899v1","category":"cs.RO"}
{"created":"2024-05-08 09:06:59","title":"A Gamma-ray Emitting Collisional Ring Galaxy System in our Galactic Neighborhood","abstract":"The astrophysical $\\gamma$-ray photons carry the signatures of the violent phenomena happening on various astronomical scales in our Universe. This includes supernova remnants, pulsars, and pulsar wind nebulae in the Galactic environment and extragalactic relativistic jets associated with active galactic nuclei (AGN). However, $\\sim$30\\% of the \\gm-ray sources detected with the Fermi Large Area Telescope lack multiwavelength counterpart association, precluding us from characterizing their origin. Here we report, for the first time, the association of a collisional ring galaxy system in our Galactic neighborhood (distance $\\sim$10 Mpc), formed as a consequence of a smaller `bullet' galaxy piercing through a larger galaxy, as the multi-frequency counterpart of an unassociated $\\gamma$-ray source 4FGL~J1647.5$-$5724. The system, also known as \"Kathryn's Wheel\", contains two dwarf irregular galaxies and an edge-on, late-type, spiral galaxy surrounded by a ring of star-forming knots. We utilized observations taken from the Neil Gehrels Swift observatory, Rapid ASKAP Continuum Survey, SuperCOSMOS H$\\alpha$ Survey, Dark Energy Survey, and Visible MultiObject Spectrograph at Very Large Telescope to ascertain the association with 4FGL~J1647.5$-$5724 and to explore the connection between the star-forming activities and the observed $\\gamma$-ray emission. We found that star-formation alone cannot explain the observed $\\gamma$-ray emission, and additional contribution likely from the pulsars/supernova remnants or buried AGN is required. We conclude that arcsecond/sub-arcsecond-scale observations of this extraordinary $\\gamma$-ray emitting galaxy collision will be needed to resolve the environment and explore the origin of cosmic rays.","sentences":["The astrophysical $\\gamma$-ray photons carry the signatures of the violent phenomena happening on various astronomical scales in our Universe.","This includes supernova remnants, pulsars, and pulsar wind nebulae in the Galactic environment and extragalactic relativistic jets associated with active galactic nuclei (AGN).","However, $\\sim$30\\% of the \\gm-ray sources detected with the Fermi Large Area Telescope lack multiwavelength counterpart association, precluding us from characterizing their origin.","Here we report, for the first time, the association of a collisional ring galaxy system in our Galactic neighborhood (distance $\\sim$10 Mpc), formed as a consequence of a smaller `bullet' galaxy piercing through a larger galaxy, as the multi-frequency counterpart of an unassociated $\\gamma$-ray source 4FGL~J1647.5$-$5724.","The system, also known as \"Kathryn's Wheel\", contains two dwarf irregular galaxies and an edge-on, late-type, spiral galaxy surrounded by a ring of star-forming knots.","We utilized observations taken from the Neil Gehrels Swift observatory, Rapid ASKAP Continuum Survey, SuperCOSMOS H$\\alpha$ Survey, Dark Energy Survey, and Visible MultiObject Spectrograph at Very Large Telescope to ascertain the association with 4FGL~J1647.5$-$5724 and to explore the connection between the star-forming activities and the observed $\\gamma$-ray emission.","We found that star-formation alone cannot explain the observed $\\gamma$-ray emission, and additional contribution likely from the pulsars/supernova remnants or buried AGN is required.","We conclude that arcsecond/sub-arcsecond-scale observations of this extraordinary $\\gamma$-ray emitting galaxy collision will be needed to resolve the environment and explore the origin of cosmic rays."],"url":"http://arxiv.org/abs/2405.04898v1","category":"astro-ph.HE"}
{"created":"2024-05-08 08:57:38","title":"Cepheids as distance indicators and stellar tracers","abstract":"We review the phenomenology of classical Cepheids (CCs), Anomalous Cepheids (ACs) and type II Cepheids (TIICs) in the Milky Way (MW) and in the Magellanic Clouds (MCs).   We also examine the Hertzsprung progression in different stellar systems by using the shape of I-band light curves (Fourier parameters) and observables based on the difference in magnitude and in phase between the bump and the minimum in luminosity.   The distribution of Cepheids in optical and in optical-near infrared (NIR) color--magnitude diagrams is investigated to constrain the topology of the instability strip. The use of Cepheids as tracers of young (CCs), intermediate (ACs) and old (TIICs) stellar populations are brought forward by the comparison between observations (MCs) and cluster isochrones covering a broad range in stellar ages and in chemical compositions.   The different diagnostics adopted to estimate individual distances (period--luminosity, period--Wesenheit, period--luminosity--color relations) are reviewed together with pros and cons in the use of fundamental and overtones, optical and NIR photometric bands, and reddening free pseudo magnitudes (Wesenheit). We also discuss the use of CCs as stellar tracers and the radial gradients among the different groups of elements (iron, alpha, neutron-capture) together with their age-dependence.   Finally, we briefly outline the role that near-future space and ground-based facilities will play in the astrophysical and cosmological use of Cepheids.","sentences":["We review the phenomenology of classical Cepheids (CCs), Anomalous Cepheids (ACs) and type II Cepheids (TIICs) in the Milky Way (MW) and in the Magellanic Clouds (MCs).   ","We also examine the Hertzsprung progression in different stellar systems by using the shape of I-band light curves (Fourier parameters) and observables based on the difference in magnitude and in phase between the bump and the minimum in luminosity.   ","The distribution of Cepheids in optical and in optical-near infrared (NIR) color--magnitude diagrams is investigated to constrain the topology of the instability strip.","The use of Cepheids as tracers of young (CCs), intermediate (ACs) and old (TIICs) stellar populations are brought forward by the comparison between observations (MCs) and cluster isochrones covering a broad range in stellar ages and in chemical compositions.   ","The different diagnostics adopted to estimate individual distances (period--luminosity, period--Wesenheit, period--luminosity--color relations) are reviewed together with pros and cons in the use of fundamental and overtones, optical and NIR photometric bands, and reddening free pseudo magnitudes (Wesenheit).","We also discuss the use of CCs as stellar tracers and the radial gradients among the different groups of elements (iron, alpha, neutron-capture) together with their age-dependence.   ","Finally, we briefly outline the role that near-future space and ground-based facilities will play in the astrophysical and cosmological use of Cepheids."],"url":"http://arxiv.org/abs/2405.04893v1","category":"astro-ph.SR"}
{"created":"2024-05-08 08:35:48","title":"A trust management framework for vehicular ad hoc networks","abstract":"Vehicular Ad Hoc Networks (VANETs) enable road users and public infrastructure to share information that improves the operation of roads and driver experience. However, these are vulnerable to poorly behaved authorized users. Trust management is used to address attacks from authorized users in accordance with their trust score. By removing the dissemination of trust metrics in the validation process, communication overhead and response time are lowered. In this paper, we propose a new Tamper-Proof Device (TPD) based trust management framework for controlling trust at the sender side vehicle that regulates driver behaviour. Moreover, the dissemination of feedback is only required when there is conflicting information in the VANET. If a conflict arises, the Road-Side Unit (RSU) decides, using the weighted voting system, whether the originator is to be believed, or not. The framework is evaluated against a centralized reputation approach and the results demonstrate that it outperforms the latter.","sentences":["Vehicular Ad Hoc Networks (VANETs) enable road users and public infrastructure to share information that improves the operation of roads and driver experience.","However, these are vulnerable to poorly behaved authorized users.","Trust management is used to address attacks from authorized users in accordance with their trust score.","By removing the dissemination of trust metrics in the validation process, communication overhead and response time are lowered.","In this paper, we propose a new Tamper-Proof Device (TPD) based trust management framework for controlling trust at the sender side vehicle that regulates driver behaviour.","Moreover, the dissemination of feedback is only required when there is conflicting information in the VANET.","If a conflict arises, the Road-Side Unit (RSU) decides, using the weighted voting system, whether the originator is to be believed, or not.","The framework is evaluated against a centralized reputation approach and the results demonstrate that it outperforms the latter."],"url":"http://arxiv.org/abs/2405.04885v1","category":"cs.CR"}
{"created":"2024-05-08 08:06:22","title":"Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration","abstract":"Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.","sentences":["Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication.","Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned.","Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration.","To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration.","These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session.","The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships.","As a result, this work creates a blueprint for scaling collaboration across distributed spaces."],"url":"http://arxiv.org/abs/2405.04873v1","category":"cs.HC"}
{"created":"2024-05-08 07:43:43","title":"Regime Learning for Differentiable Particle Filters","abstract":"Differentiable particle filters are an emerging class of models that combine sequential Monte Carlo techniques with the flexibility of neural networks to perform state space inference. This paper concerns the case where the system may switch between a finite set of state-space models, i.e. regimes. No prior approaches effectively learn both the individual regimes and the switching process simultaneously. In this paper, we propose the neural network based regime learning differentiable particle filter (RLPF) to address this problem. We further design a training procedure for the RLPF and other related algorithms. We demonstrate competitive performance compared to the previous state-of-the-art algorithms on a pair of numerical experiments.","sentences":["Differentiable particle filters are an emerging class of models that combine sequential Monte Carlo techniques with the flexibility of neural networks to perform state space inference.","This paper concerns the case where the system may switch between a finite set of state-space models, i.e. regimes.","No prior approaches effectively learn both the individual regimes and the switching process simultaneously.","In this paper, we propose the neural network based regime learning differentiable particle filter (RLPF) to address this problem.","We further design a training procedure for the RLPF and other related algorithms.","We demonstrate competitive performance compared to the previous state-of-the-art algorithms on a pair of numerical experiments."],"url":"http://arxiv.org/abs/2405.04865v1","category":"cs.LG"}
{"created":"2024-05-08 07:35:14","title":"Insights into Deep Learning Refactoring: Bridging the Gap Between Practices and Expectations","abstract":"With the rapid development of deep learning, the implementation of intricate algorithms and substantial data processing have become standard elements of deep learning projects. As a result, the code has become progressively complex as the software evolves, which is difficult to maintain and understand. Existing studies have investigated the impact of refactoring on software quality within traditional software. However, the insight of code refactoring in the context of deep learning is still unclear. This study endeavors to fill this knowledge gap by empirically examining the current state of code refactoring in deep learning realm, and practitioners' views on refactoring. We first manually analyzed the commit history of five popular and well-maintained deep learning projects (e.g., PyTorch). We mined 4,921 refactoring practices in historical commits and measured how different types and elements of refactoring operations are distributed and found that refactoring operation types' distribution in deep learning projects is different from it in traditional Java software. We then surveyed 159 practitioners about their views of code refactoring in deep learning projects and their expectations of current refactoring tools. The result of the survey showed that refactoring research and the development of related tools in the field of deep learning are crucial for improving project maintainability and code quality, and that current refactoring tools do not adequately meet the needs of practitioners. Lastly, we provided our perspective on the future advancement of refactoring tools and offered suggestions for developers' development practices.","sentences":["With the rapid development of deep learning, the implementation of intricate algorithms and substantial data processing have become standard elements of deep learning projects.","As a result, the code has become progressively complex as the software evolves, which is difficult to maintain and understand.","Existing studies have investigated the impact of refactoring on software quality within traditional software.","However, the insight of code refactoring in the context of deep learning is still unclear.","This study endeavors to fill this knowledge gap by empirically examining the current state of code refactoring in deep learning realm, and practitioners' views on refactoring.","We first manually analyzed the commit history of five popular and well-maintained deep learning projects (e.g., PyTorch).","We mined 4,921 refactoring practices in historical commits and measured how different types and elements of refactoring operations are distributed and found that refactoring operation types' distribution in deep learning projects is different from it in traditional Java software.","We then surveyed 159 practitioners about their views of code refactoring in deep learning projects and their expectations of current refactoring tools.","The result of the survey showed that refactoring research and the development of related tools in the field of deep learning are crucial for improving project maintainability and code quality, and that current refactoring tools do not adequately meet the needs of practitioners.","Lastly, we provided our perspective on the future advancement of refactoring tools and offered suggestions for developers' development practices."],"url":"http://arxiv.org/abs/2405.04861v1","category":"cs.SE"}
{"created":"2024-05-08 07:00:52","title":"Mack modes in supersonic boundary layer","abstract":"Understanding the transition mechanism of boundary layer flows is of great significance in physics and engineering, especially due to the current development of supersonic and hypersonic aircraft. In this paper, we construct multiple unstable acoustic modes so-called Mack modes, which play a crucial role during the early stage of transition in the supersonic boundary layer. To this end, we develop an inner-outer gluing iteration to solve a hyperbolic-elliptic mixed type and singular system.","sentences":["Understanding the transition mechanism of boundary layer flows is of great significance in physics and engineering, especially due to the current development of supersonic and hypersonic aircraft.","In this paper, we construct multiple unstable acoustic modes so-called Mack modes, which play a crucial role during the early stage of transition in the supersonic boundary layer.","To this end, we develop an inner-outer gluing iteration to solve a hyperbolic-elliptic mixed type and singular system."],"url":"http://arxiv.org/abs/2405.04853v1","category":"math.AP"}
{"created":"2024-05-08 06:49:16","title":"Sorting multibay block stacking storage systems","abstract":"Autonomous mobile robots (AMRs) are increasingly used to automate operations in intralogistics. One crucial feature of AMRs is their availability, allowing them to operate 24/7. This work addresses the multibay unit load pre-marshalling problem, which extends pre-marshalling from a single bay to larger warehouse configurations with multiple bays. Pre-marshalling leverages off-peak time intervals to sort a block stacking warehouse in anticipation of future orders. These larger warehouse configurations require not only the minimization of the number of moves but also the consideration of distance or time when making sorting decisions. Our proposed solution for the multibay unit load pre-marshalling problem is based on our two-step approach that first determines the access direction for each stack and then finds a sequence of moves to sort the warehouse. In addition to adapting the existing approach that integrates a network flow model and an extended A* algorithm, we additionally present an exact constraint programming approach for the second stage of the problem-solving process. The results demonstrate that the presented solution approach effectively enhances the access time of unit loads and reduces the sorting effort for block stacking warehouses with multiple bays.","sentences":["Autonomous mobile robots (AMRs) are increasingly used to automate operations in intralogistics.","One crucial feature of AMRs is their availability, allowing them to operate 24/7.","This work addresses the multibay unit load pre-marshalling problem, which extends pre-marshalling from a single bay to larger warehouse configurations with multiple bays.","Pre-marshalling leverages off-peak time intervals to sort a block stacking warehouse in anticipation of future orders.","These larger warehouse configurations require not only the minimization of the number of moves but also the consideration of distance or time when making sorting decisions.","Our proposed solution for the multibay unit load pre-marshalling problem is based on our two-step approach that first determines the access direction for each stack and then finds a sequence of moves to sort the warehouse.","In addition to adapting the existing approach that integrates a network flow model and an extended A* algorithm, we additionally present an exact constraint programming approach for the second stage of the problem-solving process.","The results demonstrate that the presented solution approach effectively enhances the access time of unit loads and reduces the sorting effort for block stacking warehouses with multiple bays."],"url":"http://arxiv.org/abs/2405.04847v1","category":"cs.DS"}
{"created":"2024-05-08 06:31:14","title":"Effective alpha theory certification using interval arithmetic: alpha theory over regions","abstract":"We reexamine Smale's alpha theory as a way to certify a numerical solution to an analytic system. For a given point and a system, Smale's alpha theory determines whether Newton's method applied to this point shows the quadratic convergence to an exact solution. We introduce the alpha theory computation using interval arithmetic to avoid costly exact arithmetic. As a straightforward variation of the alpha theory, our work improves computational efficiency compared to software employing the traditional alpha theory.","sentences":["We reexamine Smale's alpha theory as a way to certify a numerical solution to an analytic system.","For a given point and a system, Smale's alpha theory determines whether Newton's method applied to this point shows the quadratic convergence to an exact solution.","We introduce the alpha theory computation using interval arithmetic to avoid costly exact arithmetic.","As a straightforward variation of the alpha theory, our work improves computational efficiency compared to software employing the traditional alpha theory."],"url":"http://arxiv.org/abs/2405.04842v1","category":"cs.SC"}
{"created":"2024-05-08 06:22:37","title":"Enhancing Data Integrity and Traceability in Industry Cyber Physical Systems (ICPS) through Blockchain Technology: A Comprehensive Approach","abstract":"Blockchain technology, heralded as a transformative innovation, has far-reaching implications beyond its initial application in cryptocurrencies. This study explores the potential of blockchain in enhancing data integrity and traceability within Industry Cyber-Physical Systems (ICPS), a crucial aspect in the era of Industry 4.0. ICPS, integrating computational and physical components, is pivotal in managing critical infrastructure like manufacturing, power grids, and transportation networks. However, they face challenges in security, privacy, and reliability. With its inherent immutability, transparency, and distributed consensus, blockchain presents a groundbreaking approach to address these challenges. It ensures robust data reliability and traceability across ICPS, enhancing transaction transparency and facilitating secure data sharing. This research unearths various blockchain applications in ICPS, including supply chain management, quality control, contract management, and data sharing. Each application demonstrates blockchain's capacity to streamline processes, reduce fraud, and enhance system efficiency. In supply chain management, blockchain provides real-time auditing and compliance. For quality control, it establishes tamper-proof records, boosting consumer confidence. In contract management, smart contracts automate execution, enhancing efficiency. Blockchain also fosters secure collaboration in ICPS, which is crucial for system stability and safety. This study emphasizes the need for further research on blockchain's practical implementation in ICPS, focusing on challenges like scalability, system integration, and security vulnerabilities. It also suggests examining blockchain's economic and organizational impacts in ICPS to understand its feasibility and long-term advantages.","sentences":["Blockchain technology, heralded as a transformative innovation, has far-reaching implications beyond its initial application in cryptocurrencies.","This study explores the potential of blockchain in enhancing data integrity and traceability within Industry Cyber-Physical Systems (ICPS), a crucial aspect in the era of Industry 4.0.","ICPS, integrating computational and physical components, is pivotal in managing critical infrastructure like manufacturing, power grids, and transportation networks.","However, they face challenges in security, privacy, and reliability.","With its inherent immutability, transparency, and distributed consensus, blockchain presents a groundbreaking approach to address these challenges.","It ensures robust data reliability and traceability across ICPS, enhancing transaction transparency and facilitating secure data sharing.","This research unearths various blockchain applications in ICPS, including supply chain management, quality control, contract management, and data sharing.","Each application demonstrates blockchain's capacity to streamline processes, reduce fraud, and enhance system efficiency.","In supply chain management, blockchain provides real-time auditing and compliance.","For quality control, it establishes tamper-proof records, boosting consumer confidence.","In contract management, smart contracts automate execution, enhancing efficiency.","Blockchain also fosters secure collaboration in ICPS, which is crucial for system stability and safety.","This study emphasizes the need for further research on blockchain's practical implementation in ICPS, focusing on challenges like scalability, system integration, and security vulnerabilities.","It also suggests examining blockchain's economic and organizational impacts in ICPS to understand its feasibility and long-term advantages."],"url":"http://arxiv.org/abs/2405.04837v1","category":"cs.CR"}
{"created":"2024-05-08 06:13:43","title":"On-ground calibration of the X-ray, gamma-ray, and relativistic electron detector onboard TARANIS","abstract":"We developed the X-ray, Gamma-ray and Relativistic Electron detector (XGRE) onboard the TARANIS satellite, to investigate high-energy phenomena associated with lightning discharges such as terrestrial gamma-ray flashes and terrestrial electron beams. XGRE consisted of three sensors. Each sensor has one layer of LaBr$_{3}$ crystals for X-ray/gamma-ray detections, and two layers of plastic scintillators for electron and charged-particle discrimination. Since 2018, the flight model of XGRE was developed, and validation and calibration tests, such as a thermal cycle test and a calibration test with the sensors onboard the satellite were performed before the launch of TARANIS on 17 November 2020. The energy range of the LaBr$_{3}$ crystals sensitive to X-rays and gamma rays was determined to be 0.04-11.6 MeV, 0.08-11.0 MeV, and 0.08-11.3 MeV for XGRE1, 2, and 3, respectively. The energy resolution at 0.662 MeV (full width at half maximum) was to be 20.5%, 25.9%, and 28.6%, respectively. Results from the calibration test were then used to validate a simulation model of XGRE and TARANIS. By performing Monte Carlo simulations with the verified model, we calculated effective areas of XGRE to X-rays, gamma rays, electrons, and detector responses to incident photons and electrons coming from various elevation and azimuth angles.","sentences":["We developed the X-ray, Gamma-ray and Relativistic Electron detector (XGRE) onboard the TARANIS satellite, to investigate high-energy phenomena associated with lightning discharges such as terrestrial gamma-ray flashes and terrestrial electron beams.","XGRE consisted of three sensors.","Each sensor has one layer of LaBr$_{3}$ crystals for X-ray/gamma-ray detections, and two layers of plastic scintillators for electron and charged-particle discrimination.","Since 2018, the flight model of XGRE was developed, and validation and calibration tests, such as a thermal cycle test and a calibration test with the sensors onboard the satellite were performed before the launch of TARANIS on 17 November 2020.","The energy range of the LaBr$_{3}$ crystals sensitive to X-rays and gamma rays was determined to be 0.04-11.6 MeV, 0.08-11.0 MeV, and 0.08-11.3 MeV for XGRE1, 2, and 3, respectively.","The energy resolution at 0.662 MeV (full width at half maximum) was to be 20.5%, 25.9%, and 28.6%, respectively.","Results from the calibration test were then used to validate a simulation model of XGRE and TARANIS.","By performing Monte Carlo simulations with the verified model, we calculated effective areas of XGRE to X-rays, gamma rays, electrons, and detector responses to incident photons and electrons coming from various elevation and azimuth angles."],"url":"http://arxiv.org/abs/2405.04836v1","category":"astro-ph.IM"}
{"created":"2024-05-08 06:02:37","title":"A Method of Measuring TES Complex ETF Response in Frequency-domain Multiplexed Readout by Single Sideband Power Modulation","abstract":"The digital frequency domain multiplexing (DfMux) technique is widely used for astrophysical instruments with large detector arrays. Detailed detector characterization is required for instrument calibration and systematics control. We conduct the TES complex electrothermal-feedback (ETF) response measurement with the DfMux readout system as follows. By injecting a single sideband signal, we induce modulation in TES power dissipation over a frequency range encompassing the detector response. The modulated current signal induced by TES heating effect is measured, allowing for the ETF response characterization of the detector. With the injection of an upper sideband, the TES readout current shows both an upper and a lower sideband. We model the upper and lower sideband complex ETF response and verify the model by fitting to experimental data. The model not only can fit for certain physical parameters of the detector, such as loop gain, temperature sensitivity, current sensitivity, and time constant, but also enables us to estimate the systematic effect introduced by the multiplexed readout. The method is therefore useful for in-situ detector calibration and for estimating systematic effects during astronomical telescope observations, such as those performed by the upcoming LiteBIRD satellite.","sentences":["The digital frequency domain multiplexing (DfMux) technique is widely used for astrophysical instruments with large detector arrays.","Detailed detector characterization is required for instrument calibration and systematics control.","We conduct the TES complex electrothermal-feedback (ETF) response measurement with the DfMux readout system as follows.","By injecting a single sideband signal, we induce modulation in TES power dissipation over a frequency range encompassing the detector response.","The modulated current signal induced by TES heating effect is measured, allowing for the ETF response characterization of the detector.","With the injection of an upper sideband, the TES readout current shows both an upper and a lower sideband.","We model the upper and lower sideband complex ETF response and verify the model by fitting to experimental data.","The model not only can fit for certain physical parameters of the detector, such as loop gain, temperature sensitivity, current sensitivity, and time constant, but also enables us to estimate the systematic effect introduced by the multiplexed readout.","The method is therefore useful for in-situ detector calibration and for estimating systematic effects during astronomical telescope observations, such as those performed by the upcoming LiteBIRD satellite."],"url":"http://arxiv.org/abs/2405.04830v1","category":"astro-ph.IM"}
{"created":"2024-05-08 05:54:54","title":"Fine-tuning Pre-trained Named Entity Recognition Models For Indian Languages","abstract":"Named Entity Recognition (NER) is a useful component in Natural Language Processing (NLP) applications. It is used in various tasks such as Machine Translation, Summarization, Information Retrieval, and Question-Answering systems. The research on NER is centered around English and some other major languages, whereas limited attention has been given to Indian languages. We analyze the challenges and propose techniques that can be tailored for Multilingual Named Entity Recognition for Indian Languages. We present a human annotated named entity corpora of 40K sentences for 4 Indian languages from two of the major Indian language families. Additionally,we present a multilingual model fine-tuned on our dataset, which achieves an F1 score of 0.80 on our dataset on average. We achieve comparable performance on completely unseen benchmark datasets for Indian languages which affirms the usability of our model.","sentences":["Named Entity Recognition (NER) is a useful component in Natural Language Processing (NLP) applications.","It is used in various tasks such as Machine Translation, Summarization, Information Retrieval, and Question-Answering systems.","The research on NER is centered around English and some other major languages, whereas limited attention has been given to Indian languages.","We analyze the challenges and propose techniques that can be tailored for Multilingual Named Entity Recognition for Indian Languages.","We present a human annotated named entity corpora of 40K sentences for 4 Indian languages from two of the major Indian language families.","Additionally,we present a multilingual model fine-tuned on our dataset, which achieves an F1 score of 0.80 on our dataset on average.","We achieve comparable performance on completely unseen benchmark datasets for Indian languages which affirms the usability of our model."],"url":"http://arxiv.org/abs/2405.04829v1","category":"cs.CL"}
{"created":"2024-05-08 05:52:28","title":"The Geometry of Three-Forms on Symplectic Six-Manifolds","abstract":"In this paper, we investigate the geometry associated with 3-forms of various orbital type on a symplectic 6-manifold. We show that there are extremely rich geometric structures attached to certain unstable 3-forms arising naturally from degeneration of Calabi-Yau structures, which in turn provides us a new perspective towards the SYZ conjecture. We give concrete examples and demonstrate that the limiting behavior of the Type IIA flow can be used to detect canonical geometric structures on symplectic manifolds.","sentences":["In this paper, we investigate the geometry associated with 3-forms of various orbital type on a symplectic 6-manifold.","We show that there are extremely rich geometric structures attached to certain unstable 3-forms arising naturally from degeneration of Calabi-Yau structures, which in turn provides us a new perspective towards the SYZ conjecture.","We give concrete examples and demonstrate that the limiting behavior of the Type IIA flow can be used to detect canonical geometric structures on symplectic manifolds."],"url":"http://arxiv.org/abs/2405.04827v1","category":"math.DG"}
{"created":"2024-05-08 05:51:01","title":"Adaptive Whole-body Robotic Tool-use Learning on Low-rigidity Plastic-made Humanoids Using Vision and Tactile Sensors","abstract":"Various robots have been developed so far; however, we face challenges in modeling the low-rigidity bodies of some robots. In particular, the deflection of the body changes during tool-use due to object grasping, resulting in significant shifts in the tool-tip position and the body's center of gravity. Moreover, this deflection varies depending on the weight and length of the tool, making these models exceptionally complex. However, there is currently no control or learning method that takes all of these effects into account. In this study, we propose a method for constructing a neural network that describes the mutual relationship among joint angle, visual information, and tactile information from the feet. We aim to train this network using the actual robot data and utilize it for tool-tip control. Additionally, we employ Parametric Bias to capture changes in this mutual relationship caused by variations in the weight and length of tools, enabling us to understand the characteristics of the grasped tool from the current sensor information. We apply this approach to the whole-body tool-use on KXR, a low-rigidity plastic-made humanoid robot, to validate its effectiveness.","sentences":["Various robots have been developed so far; however, we face challenges in modeling the low-rigidity bodies of some robots.","In particular, the deflection of the body changes during tool-use due to object grasping, resulting in significant shifts in the tool-tip position and the body's center of gravity.","Moreover, this deflection varies depending on the weight and length of the tool, making these models exceptionally complex.","However, there is currently no control or learning method that takes all of these effects into account.","In this study, we propose a method for constructing a neural network that describes the mutual relationship among joint angle, visual information, and tactile information from the feet.","We aim to train this network using the actual robot data and utilize it for tool-tip control.","Additionally, we employ Parametric Bias to capture changes in this mutual relationship caused by variations in the weight and length of tools, enabling us to understand the characteristics of the grasped tool from the current sensor information.","We apply this approach to the whole-body tool-use on KXR, a low-rigidity plastic-made humanoid robot, to validate its effectiveness."],"url":"http://arxiv.org/abs/2405.04826v1","category":"cs.RO"}
{"created":"2024-05-08 05:40:12","title":"ATDM:An Anthropomorphic Aerial Tendon-driven Manipulator with Low-Inertia and High-Stiffness","abstract":"Aerial Manipulator Systems (AMS) have garnered significant interest for their utility in aerial operations. Nonetheless, challenges related to the manipulator's limited stiffness and the coupling disturbance with manipulator movement persist. This paper introduces the Aerial Tendon-Driven Manipulator (ATDM), an innovative AMS that integrates a hexrotor Unmanned Aerial Vehicle (UAV) with a 4-degree-of-freedom (4-DOF) anthropomorphic tendon-driven manipulator. The design of the manipulator is anatomically inspired, emulating the human arm anatomy from the shoulder joint downward. To enhance the structural integrity and performance, finite element topology optimization and lattice optimization are employed on the links to replicate the radially graded structure characteristic of bone, this approach effectively reduces weight and inertia while simultaneously maximizing stiffness. A novel tensioning mechanism with adjustable tension is introduced to address cable relaxation, and a Tension-amplification tendon mechanism is implemented to increase the manipulator's overall stiffness and output. The paper presents a kinematic model based on virtual coupled joints, a comprehensive workspace analysis, and detailed calculations of output torques and stiffness for individual arm joints.   The prototype arm has a total weight of 2.7 kg, with the end effector contributing only 0.818 kg. By positioning all actuators at the base, coupling disturbance are minimized. The paper includes a detailed mechanical design and validates the system's performance through semi-physical multi-body dynamics simulations, confirming the efficacy of the proposed design.","sentences":["Aerial Manipulator Systems (AMS) have garnered significant interest for their utility in aerial operations.","Nonetheless, challenges related to the manipulator's limited stiffness and the coupling disturbance with manipulator movement persist.","This paper introduces the Aerial Tendon-Driven Manipulator (ATDM), an innovative AMS that integrates a hexrotor Unmanned Aerial Vehicle (UAV) with a 4-degree-of-freedom (4-DOF) anthropomorphic tendon-driven manipulator.","The design of the manipulator is anatomically inspired, emulating the human arm anatomy from the shoulder joint downward.","To enhance the structural integrity and performance, finite element topology optimization and lattice optimization are employed on the links to replicate the radially graded structure characteristic of bone, this approach effectively reduces weight and inertia while simultaneously maximizing stiffness.","A novel tensioning mechanism with adjustable tension is introduced to address cable relaxation, and a Tension-amplification tendon mechanism is implemented to increase the manipulator's overall stiffness and output.","The paper presents a kinematic model based on virtual coupled joints, a comprehensive workspace analysis, and detailed calculations of output torques and stiffness for individual arm joints.   ","The prototype arm has a total weight of 2.7 kg, with the end effector contributing only 0.818 kg.","By positioning all actuators at the base, coupling disturbance are minimized.","The paper includes a detailed mechanical design and validates the system's performance through semi-physical multi-body dynamics simulations, confirming the efficacy of the proposed design."],"url":"http://arxiv.org/abs/2405.04821v1","category":"cs.RO"}
{"created":"2024-05-08 04:38:41","title":"Multigrid-in-time preconditioners for KKT systems","abstract":"We develop multigrid-in-time preconditioners for Karush-Kuhn-Tucker (KKT) systems that arise in the solution of time-dependent optimization problems. We focus on a specific instance of KKT systems, known as augmented systems, which underpin the composite-step sequential quadratic programming framework [1]. To enable time-domain decomposition, our approach introduces virtual state variables and continuity constraints at each discrete time interval. The virtual state variables not only facilitate a decoupling in time but also give rise to fixed-point iterations that aid the solution of KKT systems. These fixed-point schemes can be used either as preconditioners for Krylov subspace methods or as smoothers for multigrid-in-time schemes. For the latter, we develop a block-Jacobi scheme that parallelizes trivially in the time domain. To complete the multigrid construction, we use simple prolongation and restriction operators based on geometric multigrid ideas, and a coarse-grid solver based on a GMRES iteration preconditioned with the symmetric block Gauss-Seidel scheme. We present two optimal control examples, involving the viscous Burgers' and van der Pol oscillator equations, respectively, and demonstrate algorithmic scalability.","sentences":["We develop multigrid-in-time preconditioners for Karush-Kuhn-Tucker (KKT) systems that arise in the solution of time-dependent optimization problems.","We focus on a specific instance of KKT systems, known as augmented systems, which underpin the composite-step sequential quadratic programming framework [1].","To enable time-domain decomposition, our approach introduces virtual state variables and continuity constraints at each discrete time interval.","The virtual state variables not only facilitate a decoupling in time but also give rise to fixed-point iterations that aid the solution of KKT systems.","These fixed-point schemes can be used either as preconditioners for Krylov subspace methods or as smoothers for multigrid-in-time schemes.","For the latter, we develop a block-Jacobi scheme that parallelizes trivially in the time domain.","To complete the multigrid construction, we use simple prolongation and restriction operators based on geometric multigrid ideas, and a coarse-grid solver based on a GMRES iteration preconditioned with the symmetric block Gauss-Seidel scheme.","We present two optimal control examples, involving the viscous Burgers' and van der Pol oscillator equations, respectively, and demonstrate algorithmic scalability."],"url":"http://arxiv.org/abs/2405.04808v1","category":"math.OC"}
{"created":"2024-05-08 04:31:09","title":"A leadless power transfer and wireless telemetry solutions for an endovascular electrocorticography","abstract":"Endovascular brain-computer interfaces (eBCIs) offer a minimally invasive way to connect the brain to external devices, merging neuroscience, engineering, and medical technology. Achieving wireless data and power transmission is crucial for the clinical viability of these implantable devices. Typically, solutions for endovascular electrocorticography (ECoG) include a sensing stent with multiple electrodes (e.g. in the superior sagittal sinus) in the brain, a subcutaneous chest implant for wireless energy harvesting and data telemetry, and a long (tens of centimetres) cable with a set of wires in between. This long cable presents risks and limitations, especially for younger patients or those with fragile vasculature. This work introduces a wireless and leadless telemetry and power transfer solution for endovascular ECoG. The proposed solution includes an optical telemetry module and a focused ultrasound (FUS) power transfer system. The proposed system can be miniaturised to fit in an endovascular stent. Our solution uses optical telemetry for high-speed data transmission (over 2 Mbit/s, capable of transmitting 41 ECoG channels at a 2 kHz sampling rate and 24-bit resolution) and the proposed power transferring scheme provides up to 10mW power budget into the site of the endovascular implants under the safety limit. Tests on bovine tissues confirmed the system's effectiveness, suggesting that future custom circuit designs could further enhance eBCI applications by removing wires and auxiliary implants, minimising complications.","sentences":["Endovascular brain-computer interfaces (eBCIs) offer a minimally invasive way to connect the brain to external devices, merging neuroscience, engineering, and medical technology.","Achieving wireless data and power transmission is crucial for the clinical viability of these implantable devices.","Typically, solutions for endovascular electrocorticography (ECoG) include a sensing stent with multiple electrodes (e.g. in the superior sagittal sinus) in the brain, a subcutaneous chest implant for wireless energy harvesting and data telemetry, and a long (tens of centimetres) cable with a set of wires in between.","This long cable presents risks and limitations, especially for younger patients or those with fragile vasculature.","This work introduces a wireless and leadless telemetry and power transfer solution for endovascular ECoG.","The proposed solution includes an optical telemetry module and a focused ultrasound (FUS) power transfer system.","The proposed system can be miniaturised to fit in an endovascular stent.","Our solution uses optical telemetry for high-speed data transmission (over 2 Mbit/s, capable of transmitting 41 ECoG channels at a 2 kHz sampling rate and 24-bit resolution) and the proposed power transferring scheme provides up to 10mW power budget into the site of the endovascular implants under the safety limit.","Tests on bovine tissues confirmed the system's effectiveness, suggesting that future custom circuit designs could further enhance eBCI applications by removing wires and auxiliary implants, minimising complications."],"url":"http://arxiv.org/abs/2405.04806v1","category":"eess.SY"}
{"created":"2024-05-08 04:25:57","title":"Blockchains for Internet of Things: Fundamentals, Applications, and Challenges","abstract":"Internet of Things (IoT) services necessitate the storage, transmission, and analysis of diverse data for inference, autonomy, and control. Blockchains, with their inherent properties of decentralization and security, offer efficient database solutions for these devices through consensus-based data sharing. However, it's essential to recognize that not every blockchain system is suitable for specific IoT applications, and some might be more beneficial when excluded with privacy concerns. For example, public blockchains are not suitable for storing sensitive data. This paper presents a detailed review of three distinct blockchains tailored for enhancing IoT applications. We initially delve into the foundational aspects of three blockchain systems, highlighting their strengths, limitations, and implementation needs. Additionally, we discuss the security issues in different blockchains. Subsequently, we explore the blockchain's application in three pivotal IoT areas: edge AI, communications, and healthcare. We underscore potential challenges and the future directions for integrating different blockchains in IoT. Ultimately, this paper aims to offer a comprehensive perspective on the synergies between blockchains and the IoT ecosystem, highlighting the opportunities and complexities involved.","sentences":["Internet of Things (IoT) services necessitate the storage, transmission, and analysis of diverse data for inference, autonomy, and control.","Blockchains, with their inherent properties of decentralization and security, offer efficient database solutions for these devices through consensus-based data sharing.","However, it's essential to recognize that not every blockchain system is suitable for specific IoT applications, and some might be more beneficial when excluded with privacy concerns.","For example, public blockchains are not suitable for storing sensitive data.","This paper presents a detailed review of three distinct blockchains tailored for enhancing IoT applications.","We initially delve into the foundational aspects of three blockchain systems, highlighting their strengths, limitations, and implementation needs.","Additionally, we discuss the security issues in different blockchains.","Subsequently, we explore the blockchain's application in three pivotal IoT areas: edge AI, communications, and healthcare.","We underscore potential challenges and the future directions for integrating different blockchains in IoT. Ultimately, this paper aims to offer a comprehensive perspective on the synergies between blockchains and the IoT ecosystem, highlighting the opportunities and complexities involved."],"url":"http://arxiv.org/abs/2405.04803v1","category":"cs.CR"}
{"created":"2024-05-08 04:14:31","title":"Practical and Scalable Quantum Reservoir Computing","abstract":"Quantum Reservoir Computing leverages quantum systems to solve complex computational tasks with unprecedented efficiency and reduced energy consumption. This paper presents a novel QRC framework utilizing a quantum optical reservoir composed of two-level atoms within a single-mode optical cavity. Employing the Jaynes-Cummings and Tavis-Cummings models, we introduce a scalable and practically measurable reservoir that outperforms traditional classical reservoir computing in both memory retention and nonlinear data processing. We evaluate the reservoir's performance through two primary tasks: the prediction of time-series data via the Mackey-Glass task and the classification of sine-square waveforms. Our results demonstrate significant enhancements in performance with increased numbers of atoms, supported by non-destructive, continuous quantum measurements and polynomial regression techniques. This study confirms the potential of QRC to offer a scalable and efficient solution for advanced computational challenges, marking a significant step forward in the integration of quantum physics with machine learning technology.","sentences":["Quantum Reservoir Computing leverages quantum systems to solve complex computational tasks with unprecedented efficiency and reduced energy consumption.","This paper presents a novel QRC framework utilizing a quantum optical reservoir composed of two-level atoms within a single-mode optical cavity.","Employing the Jaynes-Cummings and Tavis-Cummings models, we introduce a scalable and practically measurable reservoir that outperforms traditional classical reservoir computing in both memory retention and nonlinear data processing.","We evaluate the reservoir's performance through two primary tasks: the prediction of time-series data via the Mackey-Glass task and the classification of sine-square waveforms.","Our results demonstrate significant enhancements in performance with increased numbers of atoms, supported by non-destructive, continuous quantum measurements and polynomial regression techniques.","This study confirms the potential of QRC to offer a scalable and efficient solution for advanced computational challenges, marking a significant step forward in the integration of quantum physics with machine learning technology."],"url":"http://arxiv.org/abs/2405.04799v1","category":"quant-ph"}
{"created":"2024-05-08 03:29:38","title":"Dynamic Workforce Scheduling and Relocation in Hyperconnected Parcel Logistic Hubs","abstract":"With the development of e-commerce during the Covid-19 pandemic, one of the major challenges for many parcel logistics companies is to design reliable and flexible scheduling algorithms to meet uncertainties of parcel arrivals as well as manpower supplies in logistic hubs, especially for those depending on workforce greatly. Currently, most labor scheduling is periodic and limited to single facility, thus the number of required workers in each hub is constrained to meet the peak demand with high variance. We approach this challenge, recognizing that not only workforce schedules but also working locations could be dynamically optimized by developing a dynamic workforce scheduling and relocation system, fed from updated data with sensors and dynamically updated hub arrival demand predictions. In this paper, we propose novel reactive scheduling heuristics to dynamically match predicted arrivals with shifts at hyperconnected parcel logistics hubs. Dynamic scheduling and allocation mechanisms are carried out dynamically during delivery periods to spatiotemporally adjust the available workforce. We also include penalty costs to keep parcels sorted in time and scheduling adjustments are made in advance to allow sufficient time for crew planning. To assess the proposed methods, we conduct comprehensive case studies based on real-world parcel logistic networks of a logistic company in China. The results show that our proposed approach can significantly outperform traditional workforce scheduling strategies in hubs with limited computation time.","sentences":["With the development of e-commerce during the Covid-19 pandemic, one of the major challenges for many parcel logistics companies is to design reliable and flexible scheduling algorithms to meet uncertainties of parcel arrivals as well as manpower supplies in logistic hubs, especially for those depending on workforce greatly.","Currently, most labor scheduling is periodic and limited to single facility, thus the number of required workers in each hub is constrained to meet the peak demand with high variance.","We approach this challenge, recognizing that not only workforce schedules but also working locations could be dynamically optimized by developing a dynamic workforce scheduling and relocation system, fed from updated data with sensors and dynamically updated hub arrival demand predictions.","In this paper, we propose novel reactive scheduling heuristics to dynamically match predicted arrivals with shifts at hyperconnected parcel logistics hubs.","Dynamic scheduling and allocation mechanisms are carried out dynamically during delivery periods to spatiotemporally adjust the available workforce.","We also include penalty costs to keep parcels sorted in time and scheduling adjustments are made in advance to allow sufficient time for crew planning.","To assess the proposed methods, we conduct comprehensive case studies based on real-world parcel logistic networks of a logistic company in China.","The results show that our proposed approach can significantly outperform traditional workforce scheduling strategies in hubs with limited computation time."],"url":"http://arxiv.org/abs/2405.04785v1","category":"math.OC"}
{"created":"2024-05-08 03:13:20","title":"Dual-Image Enhanced CLIP for Zero-Shot Anomaly Detection","abstract":"Image Anomaly Detection has been a challenging task in Computer Vision field. The advent of Vision-Language models, particularly the rise of CLIP-based frameworks, has opened new avenues for zero-shot anomaly detection. Recent studies have explored the use of CLIP by aligning images with normal and prompt descriptions. However, the exclusive dependence on textual guidance often falls short, highlighting the critical importance of additional visual references. In this work, we introduce a Dual-Image Enhanced CLIP approach, leveraging a joint vision-language scoring system. Our methods process pairs of images, utilizing each as a visual reference for the other, thereby enriching the inference process with visual context. This dual-image strategy markedly enhanced both anomaly classification and localization performances. Furthermore, we have strengthened our model with a test-time adaptation module that incorporates synthesized anomalies to refine localization capabilities. Our approach significantly exploits the potential of vision-language joint anomaly detection and demonstrates comparable performance with current SOTA methods across various datasets.","sentences":["Image Anomaly Detection has been a challenging task in Computer Vision field.","The advent of Vision-Language models, particularly the rise of CLIP-based frameworks, has opened new avenues for zero-shot anomaly detection.","Recent studies have explored the use of CLIP by aligning images with normal and prompt descriptions.","However, the exclusive dependence on textual guidance often falls short, highlighting the critical importance of additional visual references.","In this work, we introduce a Dual-Image Enhanced CLIP approach, leveraging a joint vision-language scoring system.","Our methods process pairs of images, utilizing each as a visual reference for the other, thereby enriching the inference process with visual context.","This dual-image strategy markedly enhanced both anomaly classification and localization performances.","Furthermore, we have strengthened our model with a test-time adaptation module that incorporates synthesized anomalies to refine localization capabilities.","Our approach significantly exploits the potential of vision-language joint anomaly detection and demonstrates comparable performance with current SOTA methods across various datasets."],"url":"http://arxiv.org/abs/2405.04782v1","category":"cs.CV"}
{"created":"2024-05-08 02:52:55","title":"Time Delay Anomalies of Fuzzy Gravitational Lenses","abstract":"Fuzzy dark matter is a promising alternative to the standard cold dark matter. It has quite recently been noticed, that they can not only successfully explain the large-scale structure in the Universe, but can also solve problems of position and flux anomalies in galaxy strong lensing systems. In this paper we focus on the perturbation of time delays in strong lensing systems caused by fuzzy dark matter, thus making an important extension of previous works. We select a specific system HS 0810+2554 for the study of time delay anomalies. Then, based on the nature of the fuzzy dark matter fluctuations, we obtain theoretical relationship between the magnitude of the perturbation caused by fuzzy dark matter, its content in the galaxy, and its de Broglie wavelength $\\lambda _{\\mathrm{dB}}$. It turns out that, the perturbation of strong lensing time delays due to fuzzy dark matter quantified as standard deviation is $\\sigma_{\\Delta t} \\propto \\lambda _{\\mathrm{dB}}^{1.5}$. We also verify our results through simulations. Relatively strong fuzzy dark matter fluctuations in a lensing galaxy make it possible to to destroy the topological structure of the lens system and change the arrival order between the saddle point and the minimum point in the time delays surface. Finally, we stress the unique opportunity for studying properties of fuzzy dark matter created by possible precise time delay measurements from strongly lensed transients like fast radio bursts, supernovae or gravitational wave signals.","sentences":["Fuzzy dark matter is a promising alternative to the standard cold dark matter.","It has quite recently been noticed, that they can not only successfully explain the large-scale structure in the Universe, but can also solve problems of position and flux anomalies in galaxy strong lensing systems.","In this paper we focus on the perturbation of time delays in strong lensing systems caused by fuzzy dark matter, thus making an important extension of previous works.","We select a specific system HS 0810+2554 for the study of time delay anomalies.","Then, based on the nature of the fuzzy dark matter fluctuations, we obtain theoretical relationship between the magnitude of the perturbation caused by fuzzy dark matter, its content in the galaxy, and its de Broglie wavelength $\\lambda _{\\mathrm{dB}}$.","It turns out that, the perturbation of strong lensing time delays due to fuzzy dark matter quantified as standard deviation is $\\sigma_{\\Delta t} \\propto \\lambda _{\\mathrm{dB}}^{1.5}$. We also verify our results through simulations.","Relatively strong fuzzy dark matter fluctuations in a lensing galaxy make it possible to to destroy the topological structure of the lens system and change the arrival order between the saddle point and the minimum point in the time delays surface.","Finally, we stress the unique opportunity for studying properties of fuzzy dark matter created by possible precise time delay measurements from strongly lensed transients like fast radio bursts, supernovae or gravitational wave signals."],"url":"http://arxiv.org/abs/2405.04779v1","category":"astro-ph.GA"}
{"created":"2024-05-08 02:44:19","title":"Quench dynamics of Wannier-Stark states in an active synthetic photonic lattice","abstract":"Photonic emulators have facilitated the investigation of numerous solid-state phenomena and have contributed to the development of optical devices inspired by quantum mechanics. Although current photonic emulators are constrained to bosonic behavior with local interactions, the utilization of active synthetic lattices holds promise for surpassing these limitations. In this study, we propose employing the modulated ring fast-gain laser as a foundation for emulating quench dynamics within a synthetic lattice that conforms to equal density filling of its reciprocal space. To illustrate the effectiveness of this emulation platform, we subject a dispersed Wannier-Stark ladder to quenching and directly observe oscillations, enabled by the fast-gain, along with their coherent stabilization to a single Wannier stark state. These coherent dynamics stem directly from our lasers liquid state of light, a characteristic resulting from fast-gain and explained by the rapid decay of fluctuations occurring on the system's shortest timescale. Additionally, by adequately biasing the lattice through detuning the modulation from the cavity resonance, this process supports oscillatory dynamics within the synthetic space.","sentences":["Photonic emulators have facilitated the investigation of numerous solid-state phenomena and have contributed to the development of optical devices inspired by quantum mechanics.","Although current photonic emulators are constrained to bosonic behavior with local interactions, the utilization of active synthetic lattices holds promise for surpassing these limitations.","In this study, we propose employing the modulated ring fast-gain laser as a foundation for emulating quench dynamics within a synthetic lattice that conforms to equal density filling of its reciprocal space.","To illustrate the effectiveness of this emulation platform, we subject a dispersed Wannier-Stark ladder to quenching and directly observe oscillations, enabled by the fast-gain, along with their coherent stabilization to a single Wannier stark state.","These coherent dynamics stem directly from our lasers liquid state of light, a characteristic resulting from fast-gain and explained by the rapid decay of fluctuations occurring on the system's shortest timescale.","Additionally, by adequately biasing the lattice through detuning the modulation from the cavity resonance, this process supports oscillatory dynamics within the synthetic space."],"url":"http://arxiv.org/abs/2405.04774v1","category":"physics.optics"}
{"created":"2024-05-08 02:33:13","title":"Circularly polarized light irradiated ferromagnetic MnBi$_2$Te$_4$: the long-sought ideal Weyl semimetal","abstract":"The interaction between light and non-trivial energy band topology allows for the precise manipulation of topological quantum states, which has attracted intensive interest in condensed matter physics. In this work, using first-principles calculations, we studied the topological transition of ferromagnetic (FM) MnBi$_2$Te$_4$ upon irradiation with circularly polarized light (CPL). We revealed that the MnBi$_2$Te$_4$ can be driven from an FM insulator to a Weyl semimetal with a minimum number of Weyl points, i.e., two Weyl points in systems without time-reversal symmetry. More importantly, in FM MnBi$_2$Te$_4$ with out-of-plane easy magnetization axis, we found that the band dispersion of the WP evolves from Type-II to Type-III and finally to Type-I when the light intensity increases. Moreover, we show that the profile of the characteristic Fermi arc of Weyl semimetal phase is sensitive to changes in light intensity, which enables efficient manipulation of the Fermi arc length of FM MnBi$_2$Te$_4$ in experiments. In addition, for FM MnBi$_2$Te$_4$ with in-plane easy magnetization axis, the system becomes a type I Weyl semimetal under CPL irradiation. With controllable band dispersion, length of Fermi arc, and minimum number of WPs, our results indicate that CPL-irradiated FM MnBi$_2$Te$_4$ is an ideal platform to study novel transport phenomena in Weyl semimetals with distinct band dispersion.","sentences":["The interaction between light and non-trivial energy band topology allows for the precise manipulation of topological quantum states, which has attracted intensive interest in condensed matter physics.","In this work, using first-principles calculations, we studied the topological transition of ferromagnetic (FM) MnBi$_2$Te$_4$ upon irradiation with circularly polarized light (CPL).","We revealed that the MnBi$_2$Te$_4$ can be driven from an FM insulator to a Weyl semimetal with a minimum number of Weyl points, i.e., two Weyl points in systems without time-reversal symmetry.","More importantly, in FM MnBi$_2$Te$_4$ with out-of-plane easy magnetization axis, we found that the band dispersion of the WP evolves from Type-II to Type-III and finally to Type-I when the light intensity increases.","Moreover, we show that the profile of the characteristic Fermi arc of Weyl semimetal phase is sensitive to changes in light intensity, which enables efficient manipulation of the Fermi arc length of FM MnBi$_2$Te$_4$ in experiments.","In addition, for FM MnBi$_2$Te$_4$ with in-plane easy magnetization axis, the system becomes a type I Weyl semimetal under CPL irradiation.","With controllable band dispersion, length of Fermi arc, and minimum number of WPs, our results indicate that CPL-irradiated FM MnBi$_2$Te$_4$ is an ideal platform to study novel transport phenomena in Weyl semimetals with distinct band dispersion."],"url":"http://arxiv.org/abs/2405.04768v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 02:17:10","title":"Nearly-Optimal Consensus Tolerating Adaptive Omissions: Why is a Lot of Randomness is Needed?","abstract":"We study the problem of reaching agreement in a synchronous distributed system by $n$ autonomous parties, when the communication links from/to faulty parties can omit messages. The faulty parties are selected and controlled by an adaptive, full-information, computationally unbounded adversary. We design a randomized algorithm that works in $O(\\sqrt{n}\\log^2 n)$ rounds and sends $O(n^2\\log^3 n)$ communication bits, where the number of faulty parties is $\\Theta(n)$. Our result is simultaneously tight for both these measures within polylogarithmic factors: due to the $\\Omega(n^2)$ lower bound on communication by Abraham et al. (PODC'19) and $\\Omega(\\sqrt{n/\\log n})$ lower bound on the number of rounds by Bar-Joseph and Ben-Or (PODC'98). We also quantify how much randomness is necessary and sufficient to reduce time complexity to a certain value, while keeping the communication complexity (nearly) optimal. We prove that no MC algorithm can work in less than $\\Omega(\\frac{n^2}{\\max\\{R,n\\}\\log n})$ rounds if it uses less than $O(R)$ calls to a random source, assuming a constant fraction of faulty parties. This can be contrasted with a long line of work on consensus against an {\\em adversary limited to polynomial computation time}, thus unable to break cryptographic primitives, culminating in a work by Ghinea et al. (EUROCRYPT'22), where an optimal $O(r)$-round solution with probability $1-(cr)^{-r}$ is given. Our lower bound strictly separates these two regimes, by excluding such results if the adversary is computationally unbounded. On the upper bound side, we show that for $R\\in\\tilde{O}(n^{3/2})$ there exists an algorithm solving consensus in $\\tilde{O}(\\frac{n^2}{R})$ rounds with high probability, where tilde notation hides a polylogarithmic factor. The communication complexity of the algorithm does not depend on the amount of randomness $R$ and stays optimal within polylogarithmic factor.","sentences":["We study the problem of reaching agreement in a synchronous distributed system by $n$ autonomous parties, when the communication links from/to faulty parties can omit messages.","The faulty parties are selected and controlled by an adaptive, full-information, computationally unbounded adversary.","We design a randomized algorithm that works in $O(\\sqrt{n}\\log^2 n)$ rounds and sends $O(n^2\\log^3 n)$ communication bits, where the number of faulty parties is $\\Theta(n)$. Our result is simultaneously tight for both these measures within polylogarithmic factors: due to the $\\Omega(n^2)$ lower bound on communication by Abraham et al.","(PODC'19) and $\\Omega(\\sqrt{n/\\log n})$ lower bound on the number of rounds by Bar-Joseph and Ben-Or (PODC'98).","We also quantify how much randomness is necessary and sufficient to reduce time complexity to a certain value, while keeping the communication complexity (nearly) optimal.","We prove that no MC algorithm can work in less than $\\Omega(\\frac{n^2}{\\max\\{R,n\\}\\log n})$ rounds if it uses less than $O(R)$ calls to a random source, assuming a constant fraction of faulty parties.","This can be contrasted with a long line of work on consensus against an {\\em adversary limited to polynomial computation time}, thus unable to break cryptographic primitives, culminating in a work by Ghinea et al.","(EUROCRYPT'22), where an optimal $O(r)$-round solution with probability $1-(cr)^{-r}$ is given.","Our lower bound strictly separates these two regimes, by excluding such results if the adversary is computationally unbounded.","On the upper bound side, we show that for $R\\in\\tilde{O}(n^{3/2})$ there exists an algorithm solving consensus in $\\tilde{O}(\\frac{n^2}{R})$ rounds with high probability, where tilde notation hides a polylogarithmic factor.","The communication complexity of the algorithm does not depend on the amount of randomness $R$ and stays optimal within polylogarithmic factor."],"url":"http://arxiv.org/abs/2405.04762v1","category":"cs.DC"}
{"created":"2024-05-08 01:40:13","title":"HILCodec: High Fidelity and Lightweight Neural Audio Codec","abstract":"The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of Wave-U-Net does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, \\textit{HILCodec}, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.","sentences":["The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity.","Nonetheless, such improvements often come at the cost of increased model complexity.","In this paper, we identify and address the problems of existing neural audio codecs.","We show that the performance of Wave-U-Net does not increase consistently as the network depth increases.","We analyze the root cause of such a phenomenon and suggest a variance-constrained design.","Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator.","The resulting model, \\textit{HILCodec}, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types."],"url":"http://arxiv.org/abs/2405.04752v1","category":"eess.AS"}
{"created":"2024-05-08 01:29:57","title":"On diagonal digraphs, Koszul algebras and triangulations of homology spheres","abstract":"We present the magnitude homology of a finite digraph $G$ as a certain subquotient of its path algebra. We use this to prove that the second magnitude homology group ${\\rm MH}_{2,\\ell}(G,\\mathbb{Z})$ is a free abelian group for any $\\ell$, and to describe its rank. This allows us to give a condition, denoted by $(\\mathcal{V}_2)$, equivalent to vanishing of ${\\rm MH}_{2,\\ell}(G,\\mathbb{Z})$ for $\\ell>2.$ Recall that a digraph is called diagonal, if its magnitude homology is concentrated in diagonal degrees. Using the condition $(\\mathcal V_2),$ we show that the GLMY-fundamental group of a diagonal (undirected) graph is trivial. In other words, the two-dimensional CW-complex obtained from a diagonal graph by attaching 2-cells to all squares and triangles of the graph is simply connected. We also give an interpretation of diagonality in terms of Koszul algebras: a digraph $G$ is diagonal if and only if the distance algebra $\\sigma G$ is Koszul for any ground field; and if and only if $G$ satisfies $(\\mathcal{V}_2)$ and the path cochain algebra $\\Omega^\\bullet(G)$ is Koszul for any ground field. Besides, we show that the path cochain algebra $\\Omega^\\bullet(G)$ is quadratic for any $G.$ To obtain a source of examples of (non-)diagonal digraphs, we study the extended Hasse diagram $\\hat G_K$ of a simplicial complex $K$. For a combinatorial triangulation $K$ of a piecewise-linear manifold $M,$ we express the non-diagonal part of the magnitude homology of $\\hat G_K$ via the homology of $M$. As a corollary we obtain that, if $K$ is a combinatorial triangulation of a closed piecewise-linear manifold $M$, then $\\hat G_K$ is diagonal if and only if $M$ is a homology sphere.","sentences":["We present the magnitude homology of a finite digraph $G$ as a certain subquotient of its path algebra.","We use this to prove that the second magnitude homology group ${\\rm MH}_{2,\\ell}(G,\\mathbb{Z})$ is a free abelian group for any $\\ell$, and to describe its rank.","This allows us to give a condition, denoted by $(\\mathcal{V}_2)$, equivalent to vanishing of ${\\rm MH}_{2,\\ell}(G,\\mathbb{Z})$ for $\\ell>2.$ Recall that a digraph is called diagonal, if its magnitude homology is concentrated in diagonal degrees.","Using the condition $(\\mathcal V_2),$ we show that the GLMY-fundamental group of a diagonal (undirected) graph is trivial.","In other words, the two-dimensional CW-complex obtained from a diagonal graph by attaching 2-cells to all squares and triangles of the graph is simply connected.","We also give an interpretation of diagonality in terms of Koszul algebras: a digraph $G$ is diagonal if and only if the distance algebra $\\sigma G$ is Koszul for any ground field; and if and only if $G$ satisfies $(\\mathcal{V}_2)$ and the path cochain algebra $\\Omega^\\bullet(G)$ is Koszul for any ground field.","Besides, we show that the path cochain algebra $\\Omega^\\bullet(G)$ is quadratic for any $G.$ To obtain a source of examples of (non-)diagonal digraphs, we study the extended Hasse diagram $\\hat G_K$ of a simplicial complex $K$. For a combinatorial triangulation $K$ of a piecewise-linear manifold $M,$ we express the non-diagonal part of the magnitude homology of $\\hat G_K$ via the homology of $M$. As a corollary we obtain that, if $K$ is a combinatorial triangulation of a closed piecewise-linear manifold $M$, then $\\hat G_K$ is diagonal if and only if $M$ is a homology sphere."],"url":"http://arxiv.org/abs/2405.04748v1","category":"math.KT"}
{"created":"2024-05-08 01:14:24","title":"Off-Road Autonomy Validation Using Scalable Digital Twin Simulations Within High-Performance Computing Clusters","abstract":"Off-road autonomy validation presents unique challenges due to the unpredictable and dynamic nature of off-road environments. Traditional methods focusing on sequentially sweeping across the parameter space for variability analysis struggle to comprehensively assess the performance and safety of off-road autonomous systems within the imposed time constraints. This paper proposes leveraging scalable digital twin simulations within high-performance computing (HPC) clusters to address this challenge. By harnessing the computational power of HPC clusters, our approach aims to provide a scalable and efficient means to validate off-road autonomy algorithms, enabling rapid iteration and testing of autonomy algorithms under various conditions. We demonstrate the effectiveness of our framework through performance evaluations of the HPC cluster in terms of simulation parallelization and present the systematic variability analysis of a candidate off-road autonomy algorithm to identify potential vulnerabilities in the autonomy stack's perception, planning and control modules.","sentences":["Off-road autonomy validation presents unique challenges due to the unpredictable and dynamic nature of off-road environments.","Traditional methods focusing on sequentially sweeping across the parameter space for variability analysis struggle to comprehensively assess the performance and safety of off-road autonomous systems within the imposed time constraints.","This paper proposes leveraging scalable digital twin simulations within high-performance computing (HPC) clusters to address this challenge.","By harnessing the computational power of HPC clusters, our approach aims to provide a scalable and efficient means to validate off-road autonomy algorithms, enabling rapid iteration and testing of autonomy algorithms under various conditions.","We demonstrate the effectiveness of our framework through performance evaluations of the HPC cluster in terms of simulation parallelization and present the systematic variability analysis of a candidate off-road autonomy algorithm to identify potential vulnerabilities in the autonomy stack's perception, planning and control modules."],"url":"http://arxiv.org/abs/2405.04743v1","category":"cs.RO"}
{"created":"2024-05-08 01:13:07","title":"Sensing Out-of-Equilibrium and Quantum Non-Gaussian environments via induced Time-Reversal Symmetry Breaking on the quantum-probe dynamics","abstract":"Advancing quantum sensing tools for investigating systems at atomic and nanoscales is crucial for the progress of quantum technologies. While numerous protocols employ quantum probes to extract information from stationary or weakly coupled environments, the challenges intensify at atomic- and nano-scales where the environment is inherently out-of-equilibrium or strongly coupled with the sensor. We here prove that the time-reversal symmetry in the quantum-sensor control dynamics is broken, when partial information is probed from an environment that is out-of-equilibrium with non stationary fluctuations or is described by quantum non-Gaussian, strongly coupled environmental correlations. We exploit this phenomenon as a quantum sensing paradigm with proof-of principle experimental quantum simulations using solid-state nuclear magnetic resonance (NMR). This introduces a signal contrast on a qubit-probe that quantifies how far the sensed environment is from equilibrium or its quantum non-Gaussian nature. Protocols are also presented to discern and filter a variety of environmental properties including stationary, non-stationary and non-Gaussian quantum noise fluctuations as a step toward sensing the ubiquitous environments of a quantum-sensor at atomic and nanoscales.","sentences":["Advancing quantum sensing tools for investigating systems at atomic and nanoscales is crucial for the progress of quantum technologies.","While numerous protocols employ quantum probes to extract information from stationary or weakly coupled environments, the challenges intensify at atomic- and nano-scales where the environment is inherently out-of-equilibrium or strongly coupled with the sensor.","We here prove that the time-reversal symmetry in the quantum-sensor control dynamics is broken, when partial information is probed from an environment that is out-of-equilibrium with non stationary fluctuations or is described by quantum non-Gaussian, strongly coupled environmental correlations.","We exploit this phenomenon as a quantum sensing paradigm with proof-of principle experimental quantum simulations using solid-state nuclear magnetic resonance (NMR).","This introduces a signal contrast on a qubit-probe that quantifies how far the sensed environment is from equilibrium or its quantum non-Gaussian nature.","Protocols are also presented to discern and filter a variety of environmental properties including stationary, non-stationary and non-Gaussian quantum noise fluctuations as a step toward sensing the ubiquitous environments of a quantum-sensor at atomic and nanoscales."],"url":"http://arxiv.org/abs/2405.04742v1","category":"quant-ph"}
{"created":"2024-05-08 00:46:00","title":"One-Bit Phase Retrieval: Optimal Rates and Efficient Algorithms","abstract":"In this paper, we study the sample complexity and develop efficient optimal algorithms for 1-bit phase retrieval: recovering a signal $\\mathbf{x}\\in\\mathbb{R}^n$ from $m$ phaseless bits $\\{\\mathrm{sign}(|\\mathbf{a}_i^\\top\\mathbf{x}|-\\tau)\\}_{i=1}^m$ generated by standard Gaussian $\\mathbf{a}_i$s. By investigating a phaseless version of random hyperplane tessellation, we show that (constrained) hamming distance minimization uniformly recovers all unstructured signals with Euclidean norm bounded away from zero and infinity to the error $\\mathcal{O}((n/m)\\log(m/n))$, and $\\mathcal{O}((k/m)\\log(mn/k^2))$ when restricting to $k$-sparse signals. Both error rates are shown to be information-theoretically optimal, up to a logarithmic factor. Intriguingly, the optimal rate for sparse recovery matches that of 1-bit compressed sensing, suggesting that the phase information is non-essential for 1-bit compressed sensing. We also develop efficient algorithms for 1-bit (sparse) phase retrieval that can achieve these error rates. Specifically, we prove that (thresholded) gradient descent with respect to the one-sided $\\ell_1$-loss, when initialized via spectral methods, converges linearly and attains the near optimal reconstruction error, with sample complexity $\\mathcal{O}(n)$ for unstructured signals and $\\mathcal{O}(k^2\\log(n)\\log^2(m/k))$ for $k$-sparse signals. Our proof is based upon the observation that a certain local (restricted) approximate invertibility condition is respected by Gaussian measurements. To show this, we utilize a delicate covering argument and derive tight concentration bounds for the directional gradients by properly conditioning on the index set of phaseless hyperplane separations, which may be of independent interests and useful for other related problems.","sentences":["In this paper, we study the sample complexity and develop efficient optimal algorithms for 1-bit phase retrieval: recovering a signal $\\mathbf{x}\\in\\mathbb{R}^n$ from $m$ phaseless bits $\\{\\mathrm{sign}(|\\mathbf{a}_i^\\top\\mathbf{x}|-\\tau)\\}_{i=1}^m$ generated by standard Gaussian $\\mathbf{a}_i$s.","By investigating a phaseless version of random hyperplane tessellation, we show that (constrained) hamming distance minimization uniformly recovers all unstructured signals with Euclidean norm bounded away from zero and infinity to the error $\\mathcal{O}((n/m)\\log(m/n))$, and $\\mathcal{O}((k/m)\\log(mn/k^2))$ when restricting to $k$-sparse signals.","Both error rates are shown to be information-theoretically optimal, up to a logarithmic factor.","Intriguingly, the optimal rate for sparse recovery matches that of 1-bit compressed sensing, suggesting that the phase information is non-essential for 1-bit compressed sensing.","We also develop efficient algorithms for 1-bit (sparse) phase retrieval that can achieve these error rates.","Specifically, we prove that (thresholded) gradient descent with respect to the one-sided $\\ell_1$-loss, when initialized via spectral methods, converges linearly and attains the near optimal reconstruction error, with sample complexity $\\mathcal{O}(n)$ for unstructured signals and $\\mathcal{O}(k^2\\log(n)\\log^2(m/k))$ for $k$-sparse signals.","Our proof is based upon the observation that a certain local (restricted) approximate invertibility condition is respected by Gaussian measurements.","To show this, we utilize a delicate covering argument and derive tight concentration bounds for the directional gradients by properly conditioning on the index set of phaseless hyperplane separations, which may be of independent interests and useful for other related problems."],"url":"http://arxiv.org/abs/2405.04733v1","category":"cs.IT"}
{"created":"2024-05-08 00:37:53","title":"A rigidity property for a type of wave-Klein-Gordon system","abstract":"In this paper we investigate the rigidity property of a wave component coupled in a wave-Klein-Gordon system. We prove that when the radiation field of the wave component vanishes at the null infinity, the initial data of this component also vanish, therefor there is no wave in the whole spacetime","sentences":["In this paper we investigate the rigidity property of a wave component coupled in a wave-Klein-Gordon system.","We prove that when the radiation field of the wave component vanishes at the null infinity, the initial data of this component also vanish, therefor there is no wave in the whole spacetime"],"url":"http://arxiv.org/abs/2405.04730v1","category":"math.AP"}
{"created":"2024-05-08 00:36:38","title":"Machine learning aided parameter analysis in Perovskite X-ray Detector","abstract":"Many factors in perovskite X-ray detectors, such as crystal lattice and carrier dynamics, determine the final device performance (e.g., sensitivity and detection limit). However, the relationship between these factors remains unknown due to the complexity of the material. In this study, we employ machine learning to reveal the relationship between 15 intrinsic properties of halide perovskite materials and their device performance. We construct a database of X-ray detectors for the training of machine learning. The results show that the band gap is mainly influenced by the atomic number of the B-site metal, and the lattice length parameter b has the greatest impact on the carrier mobility-lifetime product ({\\mu}{\\tau}). An X-ray detector (m-F-PEA)2PbI4 were generated in the experiment and it further verified the accuracy of our ML models. We suggest further study on random forest regression for X-ray detector applications.","sentences":["Many factors in perovskite X-ray detectors, such as crystal lattice and carrier dynamics, determine the final device performance (e.g., sensitivity and detection limit).","However, the relationship between these factors remains unknown due to the complexity of the material.","In this study, we employ machine learning to reveal the relationship between 15 intrinsic properties of halide perovskite materials and their device performance.","We construct a database of X-ray detectors for the training of machine learning.","The results show that the band gap is mainly influenced by the atomic number of the B-site metal, and the lattice length parameter b has the greatest impact on the carrier mobility-lifetime product ({\\mu}{\\tau}).","An X-ray detector (m-F-PEA)2PbI4 were generated in the experiment and it further verified the accuracy of our ML models.","We suggest further study on random forest regression for X-ray detector applications."],"url":"http://arxiv.org/abs/2405.04729v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 17:22:25","title":"SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants","abstract":"In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions. Project website: orbit-surgical.github.io/sufia","sentences":["In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants.","SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution.","This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives.","SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks.","We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions.","Project website: orbit-surgical.github.io/sufia"],"url":"http://arxiv.org/abs/2405.05226v1","category":"cs.RO"}
{"created":"2024-05-08 17:11:01","title":"Clustering Retail Products Based on Customer Behaviour","abstract":"The categorization of retail products is essential for the business decision-making process. It is a common practice to classify products based on their quantitative and qualitative characteristics. In this paper we use a purely data-driven approach. Our clustering of products is based exclusively on the customer behaviour. We propose a method for clustering retail products using market basket data. Our model is formulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on simulated data how our method behaves in different settings. The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization.","sentences":["The categorization of retail products is essential for the business decision-making process.","It is a common practice to classify products based on their quantitative and qualitative characteristics.","In this paper we use a purely data-driven approach.","Our clustering of products is based exclusively on the customer behaviour.","We propose a method for clustering retail products using market basket data.","Our model is formulated as an optimization problem which is solved by a genetic algorithm.","It is demonstrated on simulated data how our method behaves in different settings.","The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts.","The number of clusters is a parameter of our algorithm.","We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization."],"url":"http://arxiv.org/abs/2405.05218v1","category":"stat.AP"}
{"created":"2024-05-08 17:06:32","title":"SPIDER: Improved Succinct Rank and Select Performance","abstract":"Rank and select data structures seek to preprocess a bit vector to quickly answer two kinds of queries: rank(i) gives the number of 1 bits in slots 0 through i, and select(j) gives the first slot s with rank(s) = j. A succinct data structure can answer these queries while using space much smaller than the size of the original bit vector.   State of the art succinct rank and select data structures use as little as 4% extra space while answering rank and select queries quickly. Rank queries can be answered using only a handful of array accesses. Select queries can be answered by starting with similar array accesses, followed by a linear scan.   Despite these strong results, a tradeoff remains: data structures that use under 4% space are significantly slower at answering rank and select queries than less-space-efficient data structures (using, say, > 20% extra space).   In this paper we make significant progress towards closing this gap. We give a new data structure, SPIDER, which uses 3.82% extra space. SPIDER gives the best rank query time for data sets of 8 billion or more bits, even compared to less space-efficient data structures. For select queries, SPIDER outperforms all data structures that use less than 4% space, and significantly closes the gap in select performance between data structures with less than 4% space, and those that use more (over 20%) space.   SPIDER makes two main technical contributions. For rank queries, it improves performance by interleaving the metadata with the bit vector to improve cache efficiency. For select queries, it uses predictions to almost eliminate the cost of the linear scan. These predictions are inspired by recent results on data structures with machine-learned predictions, adapted to the succinct data structure setting. Our results hold on both real and synthetic data, showing that these predictions are effective in practice.","sentences":["Rank and select data structures seek to preprocess a bit vector to quickly answer two kinds of queries: rank(i) gives the number of 1 bits in slots 0","through i, and select(j) gives the first slot s with rank(s) = j.","A succinct data structure can answer these queries while using space much smaller than the size of the original bit vector.   ","State of the art succinct rank and select data structures use as little as 4% extra space while answering rank and select queries quickly.","Rank queries can be answered using only a handful of array accesses.","Select queries can be answered by starting with similar array accesses, followed by a linear scan.   ","Despite these strong results, a tradeoff remains: data structures that use under 4% space are significantly slower at answering rank and select queries than less-space-efficient data structures (using, say, > 20% extra space).   ","In this paper we make significant progress towards closing this gap.","We give a new data structure, SPIDER, which uses 3.82% extra space.","SPIDER gives the best rank query time for data sets of 8 billion or more bits, even compared to less space-efficient data structures.","For select queries, SPIDER outperforms all data structures that use less than 4% space, and significantly closes the gap in select performance between data structures with less than 4% space, and those that use more (over 20%) space.   ","SPIDER makes two main technical contributions.","For rank queries, it improves performance by interleaving the metadata with the bit vector to improve cache efficiency.","For select queries, it uses predictions to almost eliminate the cost of the linear scan.","These predictions are inspired by recent results on data structures with machine-learned predictions, adapted to the succinct data structure setting.","Our results hold on both real and synthetic data, showing that these predictions are effective in practice."],"url":"http://arxiv.org/abs/2405.05214v1","category":"cs.DS"}
{"created":"2024-05-08 16:47:04","title":"In-depth analysis of LISA Pathfinder performance results: time evolution, noise projection, physical models, and implications for LISA","abstract":"We present an analysis of the LISA Pathfinder differential acceleration performance over the entire mission . We show that the Brownian noise level, detected for frequencies $f\\gtrsim \\SI{1}{mHz}$, has been evolving consistently with the outgassing of a single gaseous species, with an activation temperature of $(7.0\\pm 0.2)\\,\\text{kK}$. In excess to the Brownian noise, the acceleration amplitude spectral density (ASD) always shows a sub-mHz tail which is reasonably well fit, between $f=\\SI{36}{\\micro\\hertz}$ and $\\SI{1}{\\milli\\hertz}$, to $\\widetilde{S}_{\\Delta g}^{1/2}(1\\, \\text{mHz}/f)$. A Bayesian estimate of $\\widetilde{S}_{\\Delta g}^{1/2}$ on a partition of the entire set of measurements in 27 data stretches, each 2.75\\,d long, gives $\\widetilde{S}_{\\Delta g}^{1/2}=(1.1\\pm0.3)\\,\\si{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no particular time pattern over the course of the mission. The width the posterior contains, in excess of the statistical uncertainty, a true physical fluctuation of $\\widetilde{S}_{\\Delta g}^{1/2}$ from run to run, of about $\\SI{0.2}{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no correlation with specific operating conditions. At the lowest considered frequency of $f=\\SI{18}{\\micro\\hertz}$, the ASD significantly deviates from the $1/f$ behavior, because of temperature fluctuations that appear to modulate a quasi-static pressure gradient, sustained by the asymmetries of outgassing . We also present a projection of acceleration noise on the sources for which we had either a correlation measurement, or an estimate from dedicated experiments.These sources account for about 40\\% of the noise power the $1/f$ tail. We discuss the possible sources of the unaccounted-for fraction, present a series of analyses that rule many of them out, and identify the possible measures that may be taken to keep the remaining ones under control in LISA.","sentences":["We present an analysis of the LISA Pathfinder differential acceleration performance over the entire mission .","We show that the Brownian noise level, detected for frequencies $f\\gtrsim \\SI{1}{mHz}$, has been evolving consistently with the outgassing of a single gaseous species, with an activation temperature of $(7.0\\pm 0.2)\\,\\text{kK}$. In excess to the Brownian noise, the acceleration amplitude spectral density (ASD) always shows a sub-mHz tail which is reasonably well fit, between $f=\\SI{36}{\\micro\\hertz}$ and $\\SI{1}{\\milli\\hertz}$, to $\\widetilde{S}_{\\Delta g}^{1/2}(1\\, \\text{mHz}/f)$. A Bayesian estimate of $\\widetilde{S}_{\\Delta g}^{1/2}$ on a partition of the entire set of measurements in 27 data stretches, each 2.75\\,d long, gives $\\widetilde{S}_{\\Delta g}^{1/2}=(1.1\\pm0.3)\\,\\si{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no particular time pattern over the course of the mission.","The width the posterior contains, in excess of the statistical uncertainty, a true physical fluctuation of $\\widetilde{S}_{\\Delta g}^{1/2}$ from run to run, of about $\\SI{0.2}{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no correlation with specific operating conditions.","At the lowest considered frequency of $f=\\SI{18}{\\micro\\hertz}$, the ASD significantly deviates from the $1/f$ behavior, because of temperature fluctuations that appear to modulate a quasi-static pressure gradient, sustained by the asymmetries of outgassing .","We also present a projection of acceleration noise on the sources for which we had either a correlation measurement, or an estimate from dedicated experiments.","These sources account for about 40\\% of the noise power the $1/f$ tail.","We discuss the possible sources of the unaccounted-for fraction, present a series of analyses that rule many of them out, and identify the possible measures that may be taken to keep the remaining ones under control in LISA."],"url":"http://arxiv.org/abs/2405.05207v1","category":"astro-ph.IM"}
{"created":"2024-05-08 16:30:45","title":"Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs with infinite activity","abstract":"In this paper, we present a randomized extension of the deep splitting algorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)] using random neural networks suitable to approximately solve both high-dimensional nonlinear parabolic PDEs and PIDEs with jumps having (possibly) infinite activity. We provide a full error analysis of our so-called random deep splitting method. In particular, we prove that our random deep splitting method converges to the (unique viscosity) solution of the nonlinear PDE or PIDE under consideration. Moreover, we empirically analyze our random deep splitting method by considering several numerical examples including both nonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of financial derivatives under default risk. In particular, we empirically demonstrate in all examples that our random deep splitting method can approximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within seconds.","sentences":["In this paper, we present a randomized extension of the deep splitting algorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)] using random neural networks suitable to approximately solve both high-dimensional nonlinear parabolic PDEs and PIDEs with jumps having (possibly) infinite activity.","We provide a full error analysis of our so-called random deep splitting method.","In particular, we prove that our random deep splitting method converges to the (unique viscosity) solution of the nonlinear PDE or PIDE under consideration.","Moreover, we empirically analyze our random deep splitting method by considering several numerical examples including both nonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of financial derivatives under default risk.","In particular, we empirically demonstrate in all examples that our random deep splitting method can approximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within seconds."],"url":"http://arxiv.org/abs/2405.05192v1","category":"math.NA"}
{"created":"2024-05-08 16:30:37","title":"Bell's and Mermin's inequalities, entangled coherent states and unitary operators","abstract":"We elaborate on the recent proposal of employing unitary operators in Quantum Mechanics. The Bell and Mermin inequalities for entangled coherent states are scrutinized by making use of the unitary displacement operators. A violation of the Mermin inequality close to the maximum allowed value is reported, in agreement with the existing literature.","sentences":["We elaborate on the recent proposal of employing unitary operators in Quantum Mechanics.","The Bell and Mermin inequalities for entangled coherent states are scrutinized by making use of the unitary displacement operators.","A violation of the Mermin inequality close to the maximum allowed value is reported, in agreement with the existing literature."],"url":"http://arxiv.org/abs/2405.05191v1","category":"quant-ph"}
{"created":"2024-05-08 16:17:14","title":"Randomized quasi-Monte Carlo and Owen's boundary growth condition: A spectral analysis","abstract":"In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen's boundary growth condition [Owen, 2006] via spectral analysis. Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol' sequence, applying the Fourier transform and Walsh--Fourier transform, respectively, for this analysis. Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator's variance closely aligns with the exponent specified in Owen's boundary growth condition for both sequence types. We also provide guidance on choosing the importance sampling density to minimize RQMC estimator variance.","sentences":["In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen's boundary growth condition [Owen, 2006] via spectral analysis.","Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol' sequence, applying the Fourier transform and Walsh--Fourier transform, respectively, for this analysis.","Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator's variance closely aligns with the exponent specified in Owen's boundary growth condition for both sequence types.","We also provide guidance on choosing the importance sampling density to minimize RQMC estimator variance."],"url":"http://arxiv.org/abs/2405.05181v1","category":"math.NA"}
{"created":"2024-05-08 16:06:57","title":"Picking watermarks from noise (PWFN): an improved robust watermarking model against intensive distortions","abstract":"Digital watermarking is the process of embedding secret information by altering images in a way that is undetectable to the human eye. To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-decoder architecture by adding different noises to the noise layer. The decoder then extracts the watermarked information from the distorted image. However, this method can only resist weak noise attacks. To improve the robustness of the algorithm against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder. The module is aimed at reducing noise and recovering some of the information lost during an attack. Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency. Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities. In addition, ablation experiments show the superiority of our proposed module.","sentences":["Digital watermarking is the process of embedding secret information by altering images in a way that is undetectable to the human eye.","To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-decoder architecture by adding different noises to the noise layer.","The decoder then extracts the watermarked information from the distorted image.","However, this method can only resist weak noise attacks.","To improve the robustness of the algorithm against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder.","The module is aimed at reducing noise and recovering some of the information lost during an attack.","Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency.","Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities.","In addition, ablation experiments show the superiority of our proposed module."],"url":"http://arxiv.org/abs/2405.05170v1","category":"cs.MM"}
{"created":"2024-05-08 15:46:47","title":"An efficient truncation scheme for Eulerian and total Lagrangian SPH methods","abstract":"In smoothed particle hydrodynamics (SPH) method, the particle-based approximations are implemented via kernel functions, and the evaluation of performance involves two key criteria: numerical accuracy and computational efficiency. In the SPH community, the Wendland kernel reigns as the prevailing choice due to its commendable accuracy and reasonable computational efficiency. Nevertheless, there exists an urgent need to enhance the computational efficiency of numerical methods while upholding accuracy. In this paper, we employ a truncation approach to limit the compact support of the Wendland kernel to 1.6h. This decision is based on the observation that particles within the range of 1.6h to 2h make negligible contributions, practically approaching zero, to the SPH approximation. To address integration errors stemming from the truncation, we incorporate the Laguerre-Gauss kernel for particle relaxation due to the fact that this kernel has been demonstrated to enable the attainment of particle distributions with reduced residue and integration errors \\cite{wang2023fourth}. Furthermore, we introduce the kernel gradient correction to rectify numerical errors from the SPH approximation of kernel gradient and the truncated compact support. A comprehensive set of numerical examples including fluid dynamics in Eulerian formulation and solid dynamics in total Lagrangian formulation are tested and have demonstrated that truncated and standard Wendland kernels enable achieve the same level accuracy but the former significantly increase the computational efficiency.","sentences":["In smoothed particle hydrodynamics (SPH) method, the particle-based approximations are implemented via kernel functions, and the evaluation of performance involves two key criteria: numerical accuracy and computational efficiency.","In the SPH community, the Wendland kernel reigns as the prevailing choice due to its commendable accuracy and reasonable computational efficiency.","Nevertheless, there exists an urgent need to enhance the computational efficiency of numerical methods while upholding accuracy.","In this paper, we employ a truncation approach to limit the compact support of the Wendland kernel to 1.6h.","This decision is based on the observation that particles within the range of 1.6h to 2h make negligible contributions, practically approaching zero, to the SPH approximation.","To address integration errors stemming from the truncation, we incorporate the Laguerre-Gauss kernel for particle relaxation due to the fact that this kernel has been demonstrated to enable the attainment of particle distributions with reduced residue and integration errors \\cite{wang2023fourth}.","Furthermore, we introduce the kernel gradient correction to rectify numerical errors from the SPH approximation of kernel gradient and the truncated compact support.","A comprehensive set of numerical examples including fluid dynamics in Eulerian formulation and solid dynamics in total Lagrangian formulation are tested and have demonstrated that truncated and standard Wendland kernels enable achieve the same level accuracy but the former significantly increase the computational efficiency."],"url":"http://arxiv.org/abs/2405.05155v1","category":"cs.CE"}
{"created":"2024-05-08 15:16:12","title":"Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference","abstract":"Estimating causal effects under interference is pertinent to many real-world settings. However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information. A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information. However, their use of polynomial extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population. We analyze the bias and variance of an interpolation-style estimator under this experimental design. Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator. Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small.","sentences":["Estimating causal effects under interference is pertinent to many real-world settings.","However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information.","A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information.","However, their use of polynomial extrapolation can lead to prohibitively high variance.","To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population.","We analyze the bias and variance of an interpolation-style estimator under this experimental design.","Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator.","Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small."],"url":"http://arxiv.org/abs/2405.05119v1","category":"stat.ME"}
{"created":"2024-05-08 15:09:00","title":"High Voltage Determination and Stabilization for Collinear Laser Spectroscopy Applications","abstract":"Fast beam collinear laser spectroscopy is the established method to investigate nuclear ground state properties such as the spin, the electromagnetic moments, and the charge radius of exotic nuclei. These are extracted with high precision from atomic observables, i.e., the hyperfine splitting and its the isotope shift, which becomes possible due to a large reduction of the Doppler broadening by compressing the velocity width of the ion beam through electrostatic acceleration. With the advancement of the experimental methods and applied devices, e.g., to measure and stabilize the laser frequency, the acceleration potential became the dominant systematic uncertainty contribution. To overcome this, we present a custom-built high-voltage divider, which was developed and tested at the German metrology institute (PTB), and a feedback loop that enabled collinear laser spectroscopy to be performed at a 100-kHz level. Furthermore, we describe the impact of field penetration into the laser-ion-interaction region. This strongly affects the determined isotope shifts and hyperfine splittings, if Doppler tuning is applied, i.e., the ion beam energy is altered instead of scanning the laser frequency. Using different laser frequencies that were referenced to a frequency comb, the field penetration was extracted laser spectroscopically. This allowed us to define an effective scanning potential to still apply the faster and easier Doppler tuning without introducing systematic deviations.","sentences":["Fast beam collinear laser spectroscopy is the established method to investigate nuclear ground state properties such as the spin, the electromagnetic moments, and the charge radius of exotic nuclei.","These are extracted with high precision from atomic observables, i.e., the hyperfine splitting and its the isotope shift, which becomes possible due to a large reduction of the Doppler broadening by compressing the velocity width of the ion beam through electrostatic acceleration.","With the advancement of the experimental methods and applied devices, e.g., to measure and stabilize the laser frequency, the acceleration potential became the dominant systematic uncertainty contribution.","To overcome this, we present a custom-built high-voltage divider, which was developed and tested at the German metrology institute (PTB), and a feedback loop that enabled collinear laser spectroscopy to be performed at a 100-kHz level.","Furthermore, we describe the impact of field penetration into the laser-ion-interaction region.","This strongly affects the determined isotope shifts and hyperfine splittings, if Doppler tuning is applied, i.e., the ion beam energy is altered instead of scanning the laser frequency.","Using different laser frequencies that were referenced to a frequency comb, the field penetration was extracted laser spectroscopically.","This allowed us to define an effective scanning potential to still apply the faster and easier Doppler tuning without introducing systematic deviations."],"url":"http://arxiv.org/abs/2405.05112v1","category":"physics.ins-det"}
{"created":"2024-05-08 14:40:45","title":"What doesn't kill Gaia makes her stronger","abstract":"Life on Earth has experienced numerous upheavals over its approximately 4 billion year history. In previous work we have discussed how interruptions to stability lead, on average, to increases in habitability over time, a tendency we called Entropic Gaia. Here we continue this exploration, working with the Tangled Nature Model of co-evolution, to understand how the evolutionary history of life is shaped by periods of acute environmental stress. We find that while these periods of stress pose a risk of complete extinction, they also create opportunities for evolutionary exploration which would otherwise be impossible, leading to more populous and stable states among the survivors than in alternative histories without a stress period. We also study how the duration, repetition and number of refugia into which life escapes during the perturbation affects the final outcome. The model results are discussed in relation to both Earth history and the search for alien life.","sentences":["Life on Earth has experienced numerous upheavals over its approximately 4 billion year history.","In previous work we have discussed how interruptions to stability lead, on average, to increases in habitability over time, a tendency we called Entropic Gaia.","Here we continue this exploration, working with the Tangled Nature Model of co-evolution, to understand how the evolutionary history of life is shaped by periods of acute environmental stress.","We find that while these periods of stress pose a risk of complete extinction, they also create opportunities for evolutionary exploration which would otherwise be impossible, leading to more populous and stable states among the survivors than in alternative histories without a stress period.","We also study how the duration, repetition and number of refugia into which life escapes during the perturbation affects the final outcome.","The model results are discussed in relation to both Earth history and the search for alien life."],"url":"http://arxiv.org/abs/2405.05091v1","category":"q-bio.PE"}
{"created":"2024-05-08 14:33:29","title":"Cryptocurrency Risk, Trust, and Acceptance in Thailand: A Comparative Study with Switzerland","abstract":"The adoption of the Pao Tang digital wallet in Thailand, promoted under the Khon la Krueng (50-50 Co-Payment) Scheme, illustrates Thailand's receptiveness to digital financial instruments, amassing over 40 million users in just three years during the COVID-19 social distancing era. Nevertheless, acceptance of this platform does not confirm a broad understanding of cryptocurrencies and Web 3.0 technologies in the region. Through a mix of documentary research, online surveys and a targeted interview with the Pao Tang app's founder, this study evaluates the factors behind the Pao Tang platform's success and contrasts it with digital practices in Switzerland. Preliminary outcomes reveal a pronounced knowledge gap in Thailand regarding decentralized technologies. With regulatory frameworks for Web 3.0 and digital currencies still nascent, this research underscores the need for further exploration, serving as a blueprint for shaping strategies, policies, and awareness campaigns in both countries.","sentences":["The adoption of the Pao Tang digital wallet in Thailand, promoted under the Khon la Krueng (50-50 Co-Payment) Scheme, illustrates Thailand's receptiveness to digital financial instruments, amassing over 40 million users in just three years during the COVID-19 social distancing era.","Nevertheless, acceptance of this platform does not confirm a broad understanding of cryptocurrencies and Web 3.0 technologies in the region.","Through a mix of documentary research, online surveys and a targeted interview with the Pao Tang app's founder, this study evaluates the factors behind the Pao Tang platform's success and contrasts it with digital practices in Switzerland.","Preliminary outcomes reveal a pronounced knowledge gap in Thailand regarding decentralized technologies.","With regulatory frameworks for Web 3.0 and digital currencies still nascent, this research underscores the need for further exploration, serving as a blueprint for shaping strategies, policies, and awareness campaigns in both countries."],"url":"http://arxiv.org/abs/2405.05086v1","category":"cs.CE"}
{"created":"2024-05-08 14:25:40","title":"Robust deep learning from weakly dependent data","abstract":"Recent developments on deep learning established some theoretical properties of deep neural networks estimators. However, most of the existing works on this topic are restricted to bounded loss functions or (sub)-Gaussian or bounded input. This paper considers robust deep learning from weakly dependent observations, with unbounded loss function and unbounded input/output. It is only assumed that the output variable has a finite $r$ order moment, with $r >1$. Non asymptotic bounds for the expected excess risk of the deep neural network estimator are established under strong mixing, and $\\psi$-weak dependence assumptions on the observations. We derive a relationship between these bounds and $r$, and when the data have moments of any order (that is $r=\\infty$), the convergence rate is close to some well-known results. When the target predictor belongs to the class of H\\\"older smooth functions with sufficiently large smoothness index, the rate of the expected excess risk for exponentially strongly mixing data is close to or as same as those for obtained with i.i.d. samples. Application to robust nonparametric regression and robust nonparametric autoregression are considered. The simulation study for models with heavy-tailed errors shows that, robust estimators with absolute loss and Huber loss function outperform the least squares method.","sentences":["Recent developments on deep learning established some theoretical properties of deep neural networks estimators.","However, most of the existing works on this topic are restricted to bounded loss functions or (sub)-Gaussian or bounded input.","This paper considers robust deep learning from weakly dependent observations, with unbounded loss function and unbounded input/output.","It is only assumed that the output variable has a finite $r$ order moment, with $r >1$.","Non asymptotic bounds for the expected excess risk of the deep neural network estimator are established under strong mixing, and $\\psi$-weak dependence assumptions on the observations.","We derive a relationship between these bounds and $r$, and when the data have moments of any order (that is $r=\\infty$), the convergence rate is close to some well-known results.","When the target predictor belongs to the class of H\\\"older smooth functions with sufficiently large smoothness index, the rate of the expected excess risk for exponentially strongly mixing data is close to or as same as those for obtained with i.i.d. samples.","Application to robust nonparametric regression and robust nonparametric autoregression are considered.","The simulation study for models with heavy-tailed errors shows that, robust estimators with absolute loss and Huber loss function outperform the least squares method."],"url":"http://arxiv.org/abs/2405.05081v1","category":"stat.ML"}
{"created":"2024-05-08 13:54:00","title":"Quality assurance of actuators for the Medium-Sized Telescopes of the Cherenkov Telescope Array","abstract":"The Cherenkov Telescope Array (CTA) is a future ground-based observatory for gamma-ray astronomy providing unparalleled sensitivity in the energy range from 20 GeV up to 300 TeV. CTA will consist of telescopes with three different sizes. The Medium-Sized Telescopes (MSTs) will have 12 m reflectors with a tessellated mirror design of 86 mirror facets each. Each mirror facet is mounted on the mirror support structure with two actuators that are adjustable in length to align the mirrors, and a freely rotating fixpoint. Image resolution and pointing accuracy constraints impose limits on the backlash and deformation of the actuators and the fixpoint under various weight and wind loads. In this contribution, the test stand to measure the backlash and deformation behaviour of actuators and fixpoints is described and the measurement procedure is explained.","sentences":["The Cherenkov Telescope Array (CTA) is a future ground-based observatory for gamma-ray astronomy providing unparalleled sensitivity in the energy range from 20 GeV up to 300 TeV. CTA will consist of telescopes with three different sizes.","The Medium-Sized Telescopes (MSTs) will have 12 m reflectors with a tessellated mirror design of 86 mirror facets each.","Each mirror facet is mounted on the mirror support structure with two actuators that are adjustable in length to align the mirrors, and a freely rotating fixpoint.","Image resolution and pointing accuracy constraints impose limits on the backlash and deformation of the actuators and the fixpoint under various weight and wind loads.","In this contribution, the test stand to measure the backlash and deformation behaviour of actuators and fixpoints is described and the measurement procedure is explained."],"url":"http://arxiv.org/abs/2405.05058v1","category":"astro-ph.IM"}
{"created":"2024-05-08 12:26:15","title":"The Entropy Enigma: Success and Failure of Entropy Minimization","abstract":"Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time. EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes. In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy. After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\\%$, an improvement of $29.62\\%$ over the previous SoTA on this task. Our code is available at https://github.com/oripress/EntropyEnigma","sentences":["Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time.","EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes.","In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps.","We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy.","After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy.","Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels.","Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy.","Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\\%$, an improvement of $29.62\\%$ over the previous SoTA on this task.","Our code is available at https://github.com/oripress/EntropyEnigma"],"url":"http://arxiv.org/abs/2405.05012v1","category":"cs.CV"}
{"created":"2024-05-08 12:14:21","title":"Neutrino Oscillations as a Gravitational Wave Detector?","abstract":"Gravitational Waves (GWs) can alter the neutrino propagation distance and thus affect neutrino oscillations. This can result in a complete disappearance of the oscillatory behavior that competes with other sources of neutrino decoherence. We develop a set of criteria that determines under which conditions neutrino oscillations are sensitive to this effect, and discuss three concrete scenarios for neutrinos from astrophysical sources. We find that neutrino oscillations may probe so far unexplored regions of the GW parameter space.","sentences":["Gravitational Waves (GWs) can alter the neutrino propagation distance and thus affect neutrino oscillations.","This can result in a complete disappearance of the oscillatory behavior that competes with other sources of neutrino decoherence.","We develop a set of criteria that determines under which conditions neutrino oscillations are sensitive to this effect, and discuss three concrete scenarios for neutrinos from astrophysical sources.","We find that neutrino oscillations may probe so far unexplored regions of the GW parameter space."],"url":"http://arxiv.org/abs/2405.05000v1","category":"hep-ph"}
{"created":"2024-05-08 11:41:20","title":"Accurate estimation of the normalized mutual information of multidimensional data","abstract":"While the linear Pearson correlation coefficient represents a well-established normalized measure to quantify the interrelation of two stochastic variables $X$ and $Y$, it fails for multidimensional variables such as Cartesian coordinates. Avoiding any assumption about the underlying data, the mutual information $I(X, Y)$ does account for multidimensional correlations. However, unlike the normalized Pearson correlation, it has no upper bound ($I \\in [0, \\infty)$), i.e., it is not clear if say, $I = 0.4$ corresponds to a low or a high correlation. Moreover, the mutual information (MI) involves the estimation of high-dimensional probability densities (e.g., six-dimensional for Cartesian coordinates), which requires a k-nearest neighbor algorithm, such as the estimator by Kraskov et al. [Phys. Rev. E 69, 066138 (2004)]. As existing methods to normalize the MI cannot be used in connection with this estimator, a new approach is presented, which uses an entropy estimation method that is invariant under variable transformations. The algorithm is numerically efficient and does not require more effort than the calculation of the (un-normalized) MI. After validating the method by applying it to various toy models, the normalized MI between the $C_{\\alpha}$ -coordinates of T4 lysozyme is considered and compared to a correlation analysis of inter-residue contacts.","sentences":["While the linear Pearson correlation coefficient represents a well-established normalized measure to quantify the interrelation of two stochastic variables $X$ and $Y$, it fails for multidimensional variables such as Cartesian coordinates.","Avoiding any assumption about the underlying data, the mutual information $I(X, Y)$ does account for multidimensional correlations.","However, unlike the normalized Pearson correlation, it has no upper bound ($I \\in [0, \\infty)$), i.e., it is not clear if say, $I = 0.4$ corresponds to a low or a high correlation.","Moreover, the mutual information (MI) involves the estimation of high-dimensional probability densities (e.g., six-dimensional for Cartesian coordinates), which requires a k-nearest neighbor algorithm, such as the estimator by Kraskov et al.","[Phys. Rev. E 69, 066138 (2004)].","As existing methods to normalize the MI cannot be used in connection with this estimator, a new approach is presented, which uses an entropy estimation method that is invariant under variable transformations.","The algorithm is numerically efficient and does not require more effort than the calculation of the (un-normalized) MI.","After validating the method by applying it to various toy models, the normalized MI between the $C_{\\alpha}$ -coordinates of T4 lysozyme is considered and compared to a correlation analysis of inter-residue contacts."],"url":"http://arxiv.org/abs/2405.04980v1","category":"physics.data-an"}
{"created":"2024-05-08 11:13:30","title":"MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures","abstract":"Accurate and fast prediction of materials properties is central to the digital transformation of materials design. However, the vast design space and diverse operating conditions pose significant challenges for accurately modeling arbitrary material candidates and forecasting their properties. We present MatterSim, a deep learning model actively learned from large-scale first-principles computations, for efficient atomistic simulations at first-principles level and accurate prediction of broad material properties across the periodic table, spanning temperatures from 0 to 5000 K and pressures up to 1000 GPa. Out-of-the-box, the model serves as a machine learning force field, and shows remarkable capabilities not only in predicting ground-state material structures and energetics, but also in simulating their behavior under realistic temperatures and pressures, signifying an up to ten-fold enhancement in precision compared to the prior best-in-class. This enables MatterSim to compute materials' lattice dynamics, mechanical and thermodynamic properties, and beyond, to an accuracy comparable with first-principles methods. Specifically, MatterSim predicts Gibbs free energies for a wide range of inorganic solids with near-first-principles accuracy and achieves a 15 meV/atom resolution for temperatures up to 1000K compared with experiments. This opens an opportunity to predict experimental phase diagrams of materials at minimal computational cost. Moreover, MatterSim also serves as a platform for continuous learning and customization by integrating domain-specific data. The model can be fine-tuned for atomistic simulations at a desired level of theory or for direct structure-to-property predictions, achieving high data efficiency with a reduction in data requirements by up to 97%.","sentences":["Accurate and fast prediction of materials properties is central to the digital transformation of materials design.","However, the vast design space and diverse operating conditions pose significant challenges for accurately modeling arbitrary material candidates and forecasting their properties.","We present MatterSim, a deep learning model actively learned from large-scale first-principles computations, for efficient atomistic simulations at first-principles level and accurate prediction of broad material properties across the periodic table, spanning temperatures from 0 to 5000 K and pressures up to 1000 GPa.","Out-of-the-box, the model serves as a machine learning force field, and shows remarkable capabilities not only in predicting ground-state material structures and energetics, but also in simulating their behavior under realistic temperatures and pressures, signifying an up to ten-fold enhancement in precision compared to the prior best-in-class.","This enables MatterSim to compute materials' lattice dynamics, mechanical and thermodynamic properties, and beyond, to an accuracy comparable with first-principles methods.","Specifically, MatterSim predicts Gibbs free energies for a wide range of inorganic solids with near-first-principles accuracy and achieves a 15 meV/atom resolution for temperatures up to 1000K compared with experiments.","This opens an opportunity to predict experimental phase diagrams of materials at minimal computational cost.","Moreover, MatterSim also serves as a platform for continuous learning and customization by integrating domain-specific data.","The model can be fine-tuned for atomistic simulations at a desired level of theory or for direct structure-to-property predictions, achieving high data efficiency with a reduction in data requirements by up to 97%."],"url":"http://arxiv.org/abs/2405.04967v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 10:43:53","title":"Gaussian consensus processes and their Lyapunov exponents","abstract":"We introduce a simple dynamic model of opinion formation, in which a finite population of individuals hold vector-valued opinions. At each time step, each individual's opinion moves towards the mean opinion but is then perturbed independently by a centred multivariate Gaussian random variable, with covariance proportional to the covariance matrix of the opinions of the population. We establish precise necessary and sufficient conditions on the parameters of the model, under which all opinions converge to a common limiting value. Asymptotically perfect correlation emerges between opinions on different topics. Our results are rigorous and based on properties of the partial products of an i.i.d. sequence of random matrices. Each matrix is a fixed linear combination of the identity matrix and a real Ginibre matrix. We derive an analytic expression for the maximal Lyapunov exponent of this product sequence. We also analyze a continuous-time analogue of our model.","sentences":["We introduce a simple dynamic model of opinion formation, in which a finite population of individuals hold vector-valued opinions.","At each time step, each individual's opinion moves towards the mean opinion but is then perturbed independently by a centred multivariate Gaussian random variable, with covariance proportional to the covariance matrix of the opinions of the population.","We establish precise necessary and sufficient conditions on the parameters of the model, under which all opinions converge to a common limiting value.","Asymptotically perfect correlation emerges between opinions on different topics.","Our results are rigorous and based on properties of the partial products of an i.i.d. sequence of random matrices.","Each matrix is a fixed linear combination of the identity matrix and a real Ginibre matrix.","We derive an analytic expression for the maximal Lyapunov exponent of this product sequence.","We also analyze a continuous-time analogue of our model."],"url":"http://arxiv.org/abs/2405.04951v1","category":"math.PR"}
{"created":"2024-05-08 10:27:05","title":"Unsupervised Skin Feature Tracking with Deep Neural Networks","abstract":"Facial feature tracking is essential in imaging ballistocardiography for accurate heart rate estimation and enables motor degradation quantification in Parkinson's disease through skin feature tracking. While deep convolutional neural networks have shown remarkable accuracy in tracking tasks, they typically require extensive labeled data for supervised training. Our proposed pipeline employs a convolutional stacked autoencoder to match image crops with a reference crop containing the target feature, learning deep feature encodings specific to the object category in an unsupervised manner, thus reducing data requirements. To overcome edge effects making the performance dependent on crop size, we introduced a Gaussian weight on the residual errors of the pixels when calculating the loss function. Training the autoencoder on facial images and validating its performance on manually labeled face and hand videos, our Deep Feature Encodings (DFE) method demonstrated superior tracking accuracy with a mean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods like SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and CoTracker. Overall, our unsupervised learning approach excels in tracking various skin features under significant motion conditions, providing superior feature descriptors for tracking, matching, and image registration compared to both traditional and state-of-the-art supervised learning methods.","sentences":["Facial feature tracking is essential in imaging ballistocardiography for accurate heart rate estimation and enables motor degradation quantification in Parkinson's disease through skin feature tracking.","While deep convolutional neural networks have shown remarkable accuracy in tracking tasks, they typically require extensive labeled data for supervised training.","Our proposed pipeline employs a convolutional stacked autoencoder to match image crops with a reference crop containing the target feature, learning deep feature encodings specific to the object category in an unsupervised manner, thus reducing data requirements.","To overcome edge effects making the performance dependent on crop size, we introduced a Gaussian weight on the residual errors of the pixels when calculating the loss function.","Training the autoencoder on facial images and validating its performance on manually labeled face and hand videos, our Deep Feature Encodings (DFE) method demonstrated superior tracking accuracy with a mean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods like SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and CoTracker.","Overall, our unsupervised learning approach excels in tracking various skin features under significant motion conditions, providing superior feature descriptors for tracking, matching, and image registration compared to both traditional and state-of-the-art supervised learning methods."],"url":"http://arxiv.org/abs/2405.04943v1","category":"cs.CV"}
{"created":"2024-05-08 09:41:25","title":"Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression","abstract":"We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data. Numerical experiments confirm the validity of the fast computation method.","sentences":["We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression.","We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data.","Numerical experiments confirm the validity of the fast computation method."],"url":"http://arxiv.org/abs/2405.04919v1","category":"stat.ML"}
{"created":"2024-05-08 09:25:39","title":"Sedimentary rocks from Mediterranean drought in the Messinian age as a probe of the past cosmic ray flux","abstract":"We propose the use of natural minerals as detectors to study the past flux of cosmic rays. This novel application of the \\textit{paleo-detector} technique requires a specific approach as it needs samples that have been exposed to secondary cosmic rays for a well defined period of time. We suggest here the use of the evaporites formed during the desiccation of the Mediterranean sea ${\\sim}6$ Myr ago. These minerals have been created and exposed to the air or under a shallow water basin for ${\\sim}500$ kyr before being quickly submerged again by a km-scale overburden of water. We show that, by looking at the damages left in the minerals by muons in cosmic ray showers, we could detect differences in the primary cosmic ray flux during that period, as the ones expected from nearby supernova explosions, below the percent-level. We show also that little to no background from radioactive contamination and other astroparticles is expected for this kind of analysis.","sentences":["We propose the use of natural minerals as detectors to study the past flux of cosmic rays.","This novel application of the \\textit{paleo-detector} technique requires a specific approach as it needs samples that have been exposed to secondary cosmic rays for a well defined period of time.","We suggest here the use of the evaporites formed during the desiccation of the Mediterranean sea","${\\sim}6$ Myr ago.","These minerals have been created and exposed to the air or under a shallow water basin for ${\\sim}500$ kyr before being quickly submerged again by a km-scale overburden of water.","We show that, by looking at the damages left in the minerals by muons in cosmic ray showers, we could detect differences in the primary cosmic ray flux during that period, as the ones expected from nearby supernova explosions, below the percent-level.","We show also that little to no background from radioactive contamination and other astroparticles is expected for this kind of analysis."],"url":"http://arxiv.org/abs/2405.04908v1","category":"astro-ph.HE"}
{"created":"2024-05-08 08:35:31","title":"Cost of Locally Approximating High-Dimensional Ground States of Contextual Quantum Models","abstract":"Contextuality, one of the strongest forms of quantum correlations, delineates the quantum world and the classical one. It has been shown recently that some quantum models, in the form of infinite one-dimensional translation-invariant Hamiltonians with nearest- and next-to-nearest-neighbor interactions, have the lowest ground state energy density allowed in quantum physics. However, these models all have local Hilbert space dimension larger than two, making the study of their ground state behavior difficult on current qubit-based variational quantum simulation platforms. In this work, we focus on the cost of simulating the local approximations of ground states of these models using qubit-based parameterized quantum circuits. The local approximations, which are 3-site reduced density matrices with local Hilbert space dimension three, are purified then encoded into permutation-symmetric qubits. We develop a universal set of permutation-symmetry preserving qubit-based gates, using them as an ansatz to simulate parameterized quantum circuits designed for qutrits. These techniques allow us to assess the accuracy of simulating the purified local ground states with respect to a fixed amount of classical and quantum resources. We found that given the same quantum circuit and the number of iterations, more contextual ground states with lower energy density are easier to simulate.","sentences":["Contextuality, one of the strongest forms of quantum correlations, delineates the quantum world and the classical one.","It has been shown recently that some quantum models, in the form of infinite one-dimensional translation-invariant Hamiltonians with nearest- and next-to-nearest-neighbor interactions, have the lowest ground state energy density allowed in quantum physics.","However, these models all have local Hilbert space dimension larger than two, making the study of their ground state behavior difficult on current qubit-based variational quantum simulation platforms.","In this work, we focus on the cost of simulating the local approximations of ground states of these models using qubit-based parameterized quantum circuits.","The local approximations, which are 3-site reduced density matrices with local Hilbert space dimension three, are purified then encoded into permutation-symmetric qubits.","We develop a universal set of permutation-symmetry preserving qubit-based gates, using them as an ansatz to simulate parameterized quantum circuits designed for qutrits.","These techniques allow us to assess the accuracy of simulating the purified local ground states with respect to a fixed amount of classical and quantum resources.","We found that given the same quantum circuit and the number of iterations, more contextual ground states with lower energy density are easier to simulate."],"url":"http://arxiv.org/abs/2405.04884v1","category":"quant-ph"}
{"created":"2024-05-08 07:27:15","title":"Pedestrian Attribute Recognition as Label-balanced Multi-label Learning","abstract":"Rooting in the scarcity of most attributes, realistic pedestrian attribute datasets exhibit unduly skewed data distribution, from which two types of model failures are delivered: (1) label imbalance: model predictions lean greatly towards the side of majority labels; (2) semantics imbalance: model is easily overfitted on the under-represented attributes due to their insufficient semantic diversity. To render perfect label balancing, we propose a novel framework that successfully decouples label-balanced data re-sampling from the curse of attributes co-occurrence, i.e., we equalize the sampling prior of an attribute while not biasing that of the co-occurred others. To diversify the attributes semantics and mitigate the feature noise, we propose a Bayesian feature augmentation method to introduce true in-distribution novelty. Handling both imbalances jointly, our work achieves best accuracy on various popular benchmarks, and importantly, with minimal computational budget.","sentences":["Rooting in the scarcity of most attributes, realistic pedestrian attribute datasets exhibit unduly skewed data distribution, from which two types of model failures are delivered: (1) label imbalance: model predictions lean greatly towards the side of majority labels; (2) semantics imbalance: model is easily overfitted on the under-represented attributes due to their insufficient semantic diversity.","To render perfect label balancing, we propose a novel framework that successfully decouples label-balanced data re-sampling from the curse of attributes co-occurrence, i.e., we equalize the sampling prior of an attribute while not biasing that of the co-occurred others.","To diversify the attributes semantics and mitigate the feature noise, we propose a Bayesian feature augmentation method to introduce true in-distribution novelty.","Handling both imbalances jointly, our work achieves best accuracy on various popular benchmarks, and importantly, with minimal computational budget."],"url":"http://arxiv.org/abs/2405.04858v1","category":"cs.CV"}
{"created":"2024-05-08 06:49:19","title":"Convergence of Random Products of Countably Infinitely Many Projections","abstract":"Let $r \\in \\mathbb{N}\\cup\\{\\infty\\}$ be a fixed number and let $P_j\\,\\, (1 \\leq j\\leq r )$ be the projection onto the closed subspace $\\mathcal{M}_j$ of $\\mathscr{H}$. We are interested in studying the sequence $P_{i_1}, P_{i_2}, \\ldots \\in\\{P_1, \\ldots, P_r\\}$. A significant problem is to demonstrate conditions under which the sequence $\\{P_{i_n}\\cdots P_{i_2}P_{i_1}x\\}_{n=1}^\\infty$ converges strongly or weakly to $Px$ for any $x\\in\\mathscr{H}$, where $P$ is the projection onto the intersection $\\mathcal{M}=\\mathcal{M}_1\\cap \\ldots \\cap \\mathcal{M}_r$. Several mathematicians have presented their insights on this matter since von Neumann established his result in the case of $r=2$. In this paper, we give an affirmative answer to a question posed by M. Sakai. We present a result concerning random products of countably infinitely many projections (the case $r=\\infty$) incorporating the notion of pseudo-periodic function.","sentences":["Let $r \\in \\mathbb{N}\\cup\\{\\infty\\}$ be a fixed number and let $P_j\\,\\, (1 \\leq j\\leq r )","$ be the projection onto the closed subspace $\\mathcal{M}_j$ of $\\mathscr{H}$. We are interested in studying the sequence $P_{i_1}, P_{i_2}, \\ldots \\in\\{P_1, \\ldots, P_r\\}$.","A significant problem is to demonstrate conditions under which the sequence $\\{P_{i_n}\\cdots P_{i_2}P_{i_1}x\\}_{n=1}^\\infty$ converges strongly or weakly to $Px$ for any $x\\in\\mathscr{H}$, where $P$ is the projection onto the intersection $\\mathcal{M}=\\mathcal{M}_1\\cap \\ldots \\cap \\mathcal{M}_r$.","Several mathematicians have presented their insights on this matter since von Neumann established his result in the case of $r=2$. In this paper, we give an affirmative answer to a question posed by M. Sakai.","We present a result concerning random products of countably infinitely many projections (the case $r=\\infty$) incorporating the notion of pseudo-periodic function."],"url":"http://arxiv.org/abs/2405.04848v1","category":"math.FA"}
{"created":"2024-05-08 06:25:44","title":"Synthesis and stability of biomolecules under Earth's upper mantle conditions","abstract":"How life started on Earth is a long-time unsolved mystery. There are various hypotheses ranging from outer space to seabed. Here, we applied extensive ab initio molecular dynamics (AIMD) simulations to study chemical reactions of NH3, H2O, H2, and CO at pressures (P) and temperatures (T) approximating the conditions of Earth's upper mantle (i.e. 10 - 13 GPa, 1000 -1400 K). Contrary to the previous assumption that larger organic molecules might readily dissociate in aqueous solutions at extreme P-T conditions, we found that many organic compounds formed and persisted in C-H-O-N fluids under these extreme conditions, including glycine, ribose, and uracil-like molecules. Particularly, our free energy calculations showed that the C-N bond is thermodynamically stable at 10 GPa and 1400 K. Moreover, our findings support the \"RNA world\" hypothesis, as we observed the exclusive formation of the 5-membered-ring form of ribose. By exploring the depths of Earth's interior, we have uncovered a previously unexplored pathway through which life may have originated. These findings have contributed to our evolving understanding of the fundamental conditions necessary for life to arise on our planet.","sentences":["How life started on Earth is a long-time unsolved mystery.","There are various hypotheses ranging from outer space to seabed.","Here, we applied extensive ab initio molecular dynamics (AIMD) simulations to study chemical reactions of NH3, H2O, H2, and CO at pressures (P) and temperatures (T) approximating the conditions of Earth's upper mantle (i.e. 10 - 13 GPa, 1000 -1400 K).","Contrary to the previous assumption that larger organic molecules might readily dissociate in aqueous solutions at extreme P-T conditions, we found that many organic compounds formed and persisted in C-H-O-N fluids under these extreme conditions, including glycine, ribose, and uracil-like molecules.","Particularly, our free energy calculations showed that the C-N bond is thermodynamically stable at 10 GPa and 1400 K. Moreover, our findings support the \"RNA world\" hypothesis, as we observed the exclusive formation of the 5-membered-ring form of ribose.","By exploring the depths of Earth's interior, we have uncovered a previously unexplored pathway through which life may have originated.","These findings have contributed to our evolving understanding of the fundamental conditions necessary for life to arise on our planet."],"url":"http://arxiv.org/abs/2405.04839v1","category":"physics.bio-ph"}
{"created":"2024-05-08 06:12:18","title":"Precise large deviation for stationary sequence of branching process with immigration","abstract":"It is known that there exists the stationary sequence of branching process with immigration $\\{X_{n}\\}_{n\\in\\mathbb{Z}}$ under some conditions (Foster and Williamson (1971)), when the offspring is critical or subcritical. A precise large deviation probability for the partial sum $S_{n}=X_{1}+\\cdots+X_{n}$ is specified, the significant difference is revealed for the critical and subcritical cases.","sentences":["It is known that there exists the stationary sequence of branching process with immigration $\\{X_{n}\\}_{n\\in\\mathbb{Z}}$ under some conditions (Foster and Williamson (1971)), when the offspring is critical or subcritical.","A precise large deviation probability for the partial sum $S_{n}=X_{1}+\\cdots+X_{n}$ is specified, the significant difference is revealed for the critical and subcritical cases."],"url":"http://arxiv.org/abs/2405.04835v1","category":"math.PR"}
{"created":"2024-05-08 06:05:52","title":"Influence of Ga-Ge Doping on the Structural and Electrical Properties of Li7La3Zr2O12 Solid Electrolytes For Li ion Battery","abstract":"In order to replace the conventional liquid electrolytes by solid electrolytes, high room temperature ionic conductivity is required. To achieve such high ionic conductivity, the series Li7-3xGaxLa3Zr1.9Ge0.1O12 was prepared by solid-state reaction method. The content of Ge was kept constant at 0.10 a.p.f.u, and the Ga has been varied from 0 to 0.40 a.p.f.u. The conducting cubic phase was identified using X-ray diffraction study, whereas the physical and structural studies were perfomed using density measurements and scanning electron microscopy (SEM) analysis, respectively. Electrical conductivity results reveal that the 0.20 Ga ceramic sample possessed the highest room temperature Li ion conductivity of 5.09 x 10-4 S/cm and minimum activation energy of 0.25 eV. Predominant ionic conduction in 0.20 Ga ceramic sample was confirmed by the DC polarization method. The high room temperature ionic conductivity makes the 0.20 Ga ceramic sample a suitable candidate as a solid electrolyte for all-solid-state lithium-ion batteries (ASSLIBs).","sentences":["In order to replace the conventional liquid electrolytes by solid electrolytes, high room temperature ionic conductivity is required.","To achieve such high ionic conductivity, the series Li7-3xGaxLa3Zr1.9Ge0.1O12 was prepared by solid-state reaction method.","The content of Ge was kept constant at 0.10 a.p.f.u, and the Ga has been varied from 0 to 0.40 a.p.f.u.","The conducting cubic phase was identified using X-ray diffraction study, whereas the physical and structural studies were perfomed using density measurements and scanning electron microscopy (SEM) analysis, respectively.","Electrical conductivity results reveal that the 0.20 Ga ceramic sample possessed the highest room temperature Li ion conductivity of 5.09 x 10-4 S/cm and minimum activation energy of 0.25 eV. Predominant ionic conduction in 0.20 Ga ceramic sample was confirmed by the DC polarization method.","The high room temperature ionic conductivity makes the 0.20 Ga ceramic sample a suitable candidate as a solid electrolyte for all-solid-state lithium-ion batteries (ASSLIBs)."],"url":"http://arxiv.org/abs/2405.04831v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 05:41:53","title":"Counting Cohesive Subgraphs with Hereditary Properties","abstract":"Counting small cohesive subgraphs in a graph is a fundamental operation with numerous applications in graph analysis. Previous studies on cohesive subgraph counting are mainly based on the clique model, which aim to count the number of $k$-cliques in a graph with a small $k$. However, the clique model often proves too restrictive for practical use. To address this issue, we investigate a new problem of counting cohesive subgraphs that adhere to the hereditary property. Here the hereditary property means that if a graph $G$ has a property $\\mathcal{P}$, then any induced subgraph of $G$ also has a property $\\mathcal{P}$. To count these hereditary cohesive subgraphs (\\hcss), we propose a new listing-based framework called \\hcslist, which employs a backtracking enumeration procedure to count all \\hcss. A notable limitation of \\hcslist is that it requires enumerating all \\hcss, making it intractable for large and dense graphs due to the exponential growth in the number of \\hcss with respect to graph size. To overcome this limitation, we propose a novel pivot-based framework called \\hcspivot, which can count most \\hcss in a combinatorial manner without explicitly listing them. Two additional noteworthy features of \\hcspivot is its ability to (1) simultaneously count \\hcss of any size and (2) simultaneously count \\hcss for each vertex or each edge, while \\hcslist is only capable of counting a specific size of \\hcs and obtaining a total count of \\hcss in a graph. We focus specifically on two \\hcs: $s$-defective clique and $s$-plex, with several non-trivial pruning techniques to enhance the efficiency. We conduct extensive experiments on 8 large real-world graphs, and the results demonstrate the high efficiency and effectiveness of our solutions.","sentences":["Counting small cohesive subgraphs in a graph is a fundamental operation with numerous applications in graph analysis.","Previous studies on cohesive subgraph counting are mainly based on the clique model, which aim to count the number of $k$-cliques in a graph with a small $k$. However, the clique model often proves too restrictive for practical use.","To address this issue, we investigate a new problem of counting cohesive subgraphs that adhere to the hereditary property.","Here the hereditary property means that if a graph $G$ has a property $\\mathcal{P}$, then any induced subgraph of $G$ also has a property $\\mathcal{P}$. To count these hereditary cohesive subgraphs (\\hcss), we propose a new listing-based framework called \\hcslist, which employs a backtracking enumeration procedure to count all \\hcss.","A notable limitation of \\hcslist is that it requires enumerating all \\hcss, making it intractable for large and dense graphs due to the exponential growth in the number of \\hcss with respect to graph size.","To overcome this limitation, we propose a novel pivot-based framework called \\hcspivot, which can count most \\hcss in a combinatorial manner without explicitly listing them.","Two additional noteworthy features of \\hcspivot is its ability to (1) simultaneously count \\hcss of any size and (2) simultaneously count \\hcss for each vertex or each edge, while \\hcslist is only capable of counting a specific size of \\hcs and obtaining a total count of \\hcss in a graph.","We focus specifically on two \\hcs: $s$-defective clique and $s$-plex, with several non-trivial pruning techniques to enhance the efficiency.","We conduct extensive experiments on 8 large real-world graphs, and the results demonstrate the high efficiency and effectiveness of our solutions."],"url":"http://arxiv.org/abs/2405.04823v1","category":"cs.DS"}
{"created":"2024-05-08 05:34:18","title":"Testing the Fairness-Improvability of Algorithms","abstract":"Many algorithms have a disparate impact in that their benefits or harms fall disproportionately on certain social groups. Addressing an algorithm's disparate impact can be challenging, however, because it is not always clear whether there exists an alternative more-fair algorithm that does not compromise on other key objectives such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can incorporate any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and demonstrate its use empirically by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this demonstration, we find strong statistically significant evidence that it is possible to reduce the algorithm's disparate impact without compromising on the accuracy of its predictions.","sentences":["Many algorithms have a disparate impact in that their benefits or harms fall disproportionately on certain social groups.","Addressing an algorithm's disparate impact can be challenging, however, because it is not always clear whether there exists an alternative more-fair algorithm that does not compromise on other key objectives such as accuracy or profit.","Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest.","The question is how a policy maker can formally substantiate, or refute, this necessity defense.","In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives.","Our proposed test is simple to implement and can incorporate any exogenous constraint on the algorithm space.","We establish the large-sample validity and consistency of our test, and demonstrate its use empirically by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019).","In this demonstration, we find strong statistically significant evidence that it is possible to reduce the algorithm's disparate impact without compromising on the accuracy of its predictions."],"url":"http://arxiv.org/abs/2405.04816v1","category":"econ.EM"}
{"created":"2024-05-08 04:45:11","title":"Gradient sensing limit of a cell when controlling the elongating direction","abstract":"Eukaryotic cells perform chemotaxis by determining the direction of chemical gradients based on stochastic sensing of concentrations at the cell surface. To examine the efficiency of this process, previous studies have investigated the limit of estimation accuracy for gradients. However, these studies assume that the cell shape and gradient are constant, and do not consider how adaptive regulation of cell shape affects the estimation limit. Dynamics of cell shape during gradient sensing is biologically ubiquitous and can influence the estimation by altering the way the concentration is measured, and cells may strategically regulate their shape to improve estimation accuracy. To address this gap, we investigate the estimation limits in dynamic situations where cells change shape adaptively depending on the sensed signal. We approach this problem by analyzing the stationary solution of the Bayesian nonlinear filtering equation. By applying diffusion approximation to the ligand-receptor binding process and the Laplace method for the posterior expectation under a high signal-to-noise ratio regime, we obtain an analytical expression for the estimation limit. This expression indicates that estimation accuracy can be improved by elongating perpendicular to the estimated direction, which is also confirmed by numerical simulations. Our analysis provides a basis for clarifying the interplay between estimation and control in gradient sensing and sheds light on how cells optimize their shape to enhance chemotactic efficiency.","sentences":["Eukaryotic cells perform chemotaxis by determining the direction of chemical gradients based on stochastic sensing of concentrations at the cell surface.","To examine the efficiency of this process, previous studies have investigated the limit of estimation accuracy for gradients.","However, these studies assume that the cell shape and gradient are constant, and do not consider how adaptive regulation of cell shape affects the estimation limit.","Dynamics of cell shape during gradient sensing is biologically ubiquitous and can influence the estimation by altering the way the concentration is measured, and cells may strategically regulate their shape to improve estimation accuracy.","To address this gap, we investigate the estimation limits in dynamic situations where cells change shape adaptively depending on the sensed signal.","We approach this problem by analyzing the stationary solution of the Bayesian nonlinear filtering equation.","By applying diffusion approximation to the ligand-receptor binding process and the Laplace method for the posterior expectation under a high signal-to-noise ratio regime, we obtain an analytical expression for the estimation limit.","This expression indicates that estimation accuracy can be improved by elongating perpendicular to the estimated direction, which is also confirmed by numerical simulations.","Our analysis provides a basis for clarifying the interplay between estimation and control in gradient sensing and sheds light on how cells optimize their shape to enhance chemotactic efficiency."],"url":"http://arxiv.org/abs/2405.04810v1","category":"q-bio.CB"}
{"created":"2024-05-08 04:04:03","title":"Persistent homology of featured time series data and its applications","abstract":"Recent studies have actively employed persistent homology (PH), a topological data analysis technique, to analyze the topological information in time series data. Many successful studies have utilized graph representations of time series data for PH calculation. Given the diverse nature of time series data, it is crucial to have mechanisms that can adjust the PH calculations by incorporating domain-specific knowledge. In this context, we introduce a methodology that allows the adjustment of PH calculations by reflecting relevant domain knowledge in specific fields. We introduce the concept of featured time series, which is the pair of a time series augmented with specific features such as domain knowledge, and an influence vector that assigns a value to each feature to fine-tune the results of the PH. We then prove the stability theorem of the proposed method, which states that adjusting the influence vectors grants stability to the PH calculations. The proposed approach enables the tailored analysis of a time series based on the graph representation methodology, which makes it applicable to real-world domains. We consider two examples to verify the proposed method's advantages: anomaly detection of stock data and topological analysis of music data.","sentences":["Recent studies have actively employed persistent homology (PH), a topological data analysis technique, to analyze the topological information in time series data.","Many successful studies have utilized graph representations of time series data for PH calculation.","Given the diverse nature of time series data, it is crucial to have mechanisms that can adjust the PH calculations by incorporating domain-specific knowledge.","In this context, we introduce a methodology that allows the adjustment of PH calculations by reflecting relevant domain knowledge in specific fields.","We introduce the concept of featured time series, which is the pair of a time series augmented with specific features such as domain knowledge, and an influence vector that assigns a value to each feature to fine-tune the results of the PH.","We then prove the stability theorem of the proposed method, which states that adjusting the influence vectors grants stability to the PH calculations.","The proposed approach enables the tailored analysis of a time series based on the graph representation methodology, which makes it applicable to real-world domains.","We consider two examples to verify the proposed method's advantages: anomaly detection of stock data and topological analysis of music data."],"url":"http://arxiv.org/abs/2405.04796v1","category":"math.AT"}
{"created":"2024-05-08 03:56:19","title":"The Impact of Perceived Tone, Age, and Gender on Voice Assistant Persuasiveness in the Context of Product Recommendations","abstract":"Voice Assistants (VAs) can assist users in various everyday tasks, but many users are reluctant to rely on VAs for intricate tasks like online shopping. This study aims to examine whether the vocal characteristics of VAs can serve as an effective tool to persuade users and increase user engagement with VAs in online shopping. Prior studies have demonstrated that the perceived tone, age, and gender of a voice influence the perceived persuasiveness of the speaker in interpersonal interactions. Furthermore, persuasion in product communication has been shown to affect purchase decisions in online shopping. We investigate whether variations in a VA voice's perceived tone, age, and gender characteristics can persuade users, and ultimately affect their purchase decisions. Our experimental study showed that participants were more persuaded to make purchase decisions by VA voices having positive or neutral tones as well as middle-aged male or younger female voices. Our results suggest that VA designers should offer users the ability to easily customize VA voices with a range of tones, ages, and genders. This customization can enhance user comfort and enjoyment, potentially leading to higher engagement with VAs. Additionally, we discuss the boundaries of ethical persuasion, emphasizing the importance of safeguarding users' interests against unwarranted manipulation.","sentences":["Voice Assistants (VAs) can assist users in various everyday tasks, but many users are reluctant to rely on VAs for intricate tasks like online shopping.","This study aims to examine whether the vocal characteristics of VAs can serve as an effective tool to persuade users and increase user engagement with VAs in online shopping.","Prior studies have demonstrated that the perceived tone, age, and gender of a voice influence the perceived persuasiveness of the speaker in interpersonal interactions.","Furthermore, persuasion in product communication has been shown to affect purchase decisions in online shopping.","We investigate whether variations in a VA voice's perceived tone, age, and gender characteristics can persuade users, and ultimately affect their purchase decisions.","Our experimental study showed that participants were more persuaded to make purchase decisions by VA voices having positive or neutral tones as well as middle-aged male or younger female voices.","Our results suggest that VA designers should offer users the ability to easily customize VA voices with a range of tones, ages, and genders.","This customization can enhance user comfort and enjoyment, potentially leading to higher engagement with VAs.","Additionally, we discuss the boundaries of ethical persuasion, emphasizing the importance of safeguarding users' interests against unwarranted manipulation."],"url":"http://arxiv.org/abs/2405.04791v1","category":"cs.HC"}
{"created":"2024-05-08 01:51:19","title":"Conditional Local Feature Encoding for Graph Neural Networks","abstract":"Graph neural networks (GNNs) have shown great success in learning from graph-based data. The key mechanism of current GNNs is message passing, where a node's feature is updated based on the information passing from its local neighbourhood. A limitation of this mechanism is that node features become increasingly dominated by the information aggregated from the neighbourhood as we use more rounds of message passing. Consequently, as the GNN layers become deeper, adjacent node features tends to be similar, making it more difficult for GNNs to distinguish adjacent nodes, thereby, limiting the performance of GNNs. In this paper, we propose conditional local feature encoding (CLFE) to help prevent the problem of node features being dominated by the information from local neighbourhood. The idea of our method is to extract the node hidden state embedding from message passing process and concatenate it with the nodes feature from previous stage, then we utilise linear transformation to form a CLFE based on the concatenated vector. The CLFE will form the layer output to better preserve node-specific information, thus help to improve the performance of GNN models. To verify the feasibility of our method, we conducted extensive experiments on seven benchmark datasets for four graph domain tasks: super-pixel graph classification, node classification, link prediction, and graph regression. The experimental results consistently demonstrate that our method improves model performance across a variety of baseline GNN models for all four tasks.","sentences":["Graph neural networks (GNNs) have shown great success in learning from graph-based data.","The key mechanism of current GNNs is message passing, where a node's feature is updated based on the information passing from its local neighbourhood.","A limitation of this mechanism is that node features become increasingly dominated by the information aggregated from the neighbourhood as we use more rounds of message passing.","Consequently, as the GNN layers become deeper, adjacent node features tends to be similar, making it more difficult for GNNs to distinguish adjacent nodes, thereby, limiting the performance of GNNs.","In this paper, we propose conditional local feature encoding (CLFE) to help prevent the problem of node features being dominated by the information from local neighbourhood.","The idea of our method is to extract the node hidden state embedding from message passing process and concatenate it with the nodes feature from previous stage, then we utilise linear transformation to form a CLFE based on the concatenated vector.","The CLFE will form the layer output to better preserve node-specific information, thus help to improve the performance of GNN models.","To verify the feasibility of our method, we conducted extensive experiments on seven benchmark datasets for four graph domain tasks: super-pixel graph classification, node classification, link prediction, and graph regression.","The experimental results consistently demonstrate that our method improves model performance across a variety of baseline GNN models for all four tasks."],"url":"http://arxiv.org/abs/2405.04755v1","category":"cs.LG"}
{"created":"2024-05-08 00:58:29","title":"Pressure induced metallization and loss of surface magnetism in FeSi","abstract":"Single crystalline FeSi samples with a conducting surface state (CSS) were studied under high pressure ($\\textit{P}$) and magnetic field ($\\textit{B}$) by means of electrical resistance ($\\textit{R}$) measurements to explore how the bulk semiconducting state and the surface state are tuned by the application of pressure. We found that the energy gap ($\\Delta$) associated with the semiconducting bulk phase begins to close abruptly at a critical pressure ($P_{cr}$) of ~10 GPa and the bulk material becomes metallic with no obvious sign of any emergent phases or non-Fermi liquid behavior in $\\textit{R}$($\\textit{T}$) in the neighborhood of $P_{cr}$ above 3 K. Moreover, the metallic phase appears to remain at near-ambient pressure upon release of the pressure. Interestingly, the hysteresis in the $\\textit{R}$($\\textit{T}$) curve associated with the magnetically ordered CSS decreases with pressure and vanishes at $P_{cr}$, while the slope of the $\\textit{R}$($\\textit{B}$) curve, d$\\textit{R}$/d$\\textit{B}$, which has a negative value for $\\textit{P}$ < $P_{cr}$, decreases in magnitude with $\\textit{P}$ and changes sign at $P_{cr}$. Thus, the CSS and the corresponding two-dimensional magnetic order collapse at $P_{cr}$ where the energy gap $\\Delta$ of the bulk material starts to close abruptly, revealing the connection between the CSS and the semiconducting bulk state in FeSi.","sentences":["Single crystalline FeSi samples with a conducting surface state (CSS) were studied under high pressure ($\\textit{P}$) and magnetic field ($\\textit{B}$) by means of electrical resistance ($\\textit{R}$) measurements to explore how the bulk semiconducting state and the surface state are tuned by the application of pressure.","We found that the energy gap ($\\Delta$) associated with the semiconducting bulk phase begins to close abruptly at a critical pressure ($P_{cr}$) of ~10 GPa and the bulk material becomes metallic with no obvious sign of any emergent phases or non-Fermi liquid behavior in $\\textit{R}$($\\textit{T}$) in the neighborhood of $P_{cr}$ above 3 K. Moreover, the metallic phase appears to remain at near-ambient pressure upon release of the pressure.","Interestingly, the hysteresis in the $\\textit{R}$($\\textit{T}$) curve associated with the magnetically ordered CSS decreases with pressure and vanishes at $P_{cr}$, while the slope of the $\\textit{R}$($\\textit{B}$) curve, d$\\textit{R}$/d$\\textit{B}$, which has a negative value for $\\textit{P}$ < $P_{cr}$, decreases in magnitude with $\\textit{P}$ and changes sign at $P_{cr}$. Thus, the CSS and the corresponding two-dimensional magnetic order collapse at $P_{cr}$ where the energy gap $\\Delta$ of the bulk material starts to close abruptly, revealing the connection between the CSS and the semiconducting bulk state in FeSi."],"url":"http://arxiv.org/abs/2405.04739v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 00:08:07","title":"Astrometric and photometric characterization of $\u03b7$ Tel B combining two decades of observations","abstract":"$\\eta$ Tel is an 18 Myr system with a 2.09 M$_{\\odot}$ A-type star and an M7-M8 brown dwarf companion, $\\eta$ Tel B, separated by 4.2'' (208 au). High-contrast imaging campaigns over 20 years have enabled orbital and photometric characterization. $\\eta$ Tel B, bright and on a wide orbit, is ideal for detailed examination.   We analyzed three new SPHERE/IRDIS coronagraphic observations to explore $\\eta$ Tel B's orbital parameters, contrast, and surroundings, aiming to detect a circumplanetary disk or close companion. Reduced IRDIS data achieved a contrast of 1.0$\\times 10^{-5}$, enabling astrometric measurements with uncertainties of 4 mas in separation and 0.2 degrees in position angle, the smallest so far.   With a contrast of 6.8 magnitudes in the H band, $\\eta$ Tel B's separation and position angle were measured as 4.218'' and 167.3 degrees, respectively. Orbital analysis using Orvara code, considering Gaia-Hipparcos acceleration, revealed a low eccentric orbit (e $\\sim$ 0.34), inclination of 81.9 degrees, and semi-major axis of 218 au. $\\eta$ Tel B's mass was determined to be 48 \\MJup, consistent with previous calculations.   No significant residual indicating a satellite or disk around $\\eta$ Tel B was detected. Detection limits ruled out massive objects around $\\eta$ Tel B with masses down to 1.6 \\MJup at a separation of 33 au.","sentences":["$\\eta$ Tel is an 18 Myr system with a 2.09 M$_{\\odot}$ A-type star and an M7-M8 brown dwarf companion, $\\eta$ Tel B, separated by 4.2'' (208 au).","High-contrast imaging campaigns over 20 years have enabled orbital and photometric characterization.","$\\eta$ Tel B, bright and on a wide orbit, is ideal for detailed examination.   ","We analyzed three new SPHERE/IRDIS coronagraphic observations to explore $\\eta$ Tel B's orbital parameters, contrast, and surroundings, aiming to detect a circumplanetary disk or close companion.","Reduced IRDIS data achieved a contrast of 1.0$\\times 10^{-5}$, enabling astrometric measurements with uncertainties of 4 mas in separation and 0.2 degrees in position angle, the smallest so far.   ","With a contrast of 6.8 magnitudes in the H band, $\\eta$ Tel B's separation and position angle were measured as 4.218'' and 167.3 degrees, respectively.","Orbital analysis using Orvara code, considering Gaia-Hipparcos acceleration, revealed a low eccentric orbit (e $\\sim$ 0.34), inclination of 81.9 degrees, and semi-major axis of 218 au.","$\\eta$ Tel B's mass was determined to be 48 \\MJup, consistent with previous calculations.   ","No significant residual indicating a satellite or disk around $\\eta$ Tel B was detected.","Detection limits ruled out massive objects around $\\eta$ Tel B with masses down to 1.6 \\MJup at a separation of 33 au."],"url":"http://arxiv.org/abs/2405.04723v1","category":"astro-ph.SR"}
{"created":"2024-05-07 22:42:04","title":"Mitigating Negative Side Effects in Multi-Agent Systems Using Blame Assignment","abstract":"When agents that are independently trained (or designed) to complete their individual tasks are deployed in a shared environment, their joint actions may produce negative side effects (NSEs). As their training does not account for the behavior of other agents or their joint action effects on the environment, the agents have no prior knowledge of the NSEs of their actions. We model the problem of mitigating NSEs in a cooperative multi-agent system as a Lexicographic Decentralized Markov Decision Process with two objectives. The agents must optimize the completion of their assigned tasks while mitigating NSEs. We assume independence of transitions and rewards with respect to the agents' tasks but the joint NSE penalty creates a form of dependence in this setting. To improve scalability, the joint NSE penalty is decomposed into individual penalties for each agent using credit assignment, which facilitates decentralized policy computation. Our results in simulation on three domains demonstrate the effectiveness and scalability of our approach in mitigating NSEs by updating the policies of a subset of agents in the system.","sentences":["When agents that are independently trained (or designed) to complete their individual tasks are deployed in a shared environment, their joint actions may produce negative side effects (NSEs).","As their training does not account for the behavior of other agents or their joint action effects on the environment, the agents have no prior knowledge of the NSEs of their actions.","We model the problem of mitigating NSEs in a cooperative multi-agent system as a Lexicographic Decentralized Markov Decision Process with two objectives.","The agents must optimize the completion of their assigned tasks while mitigating NSEs.","We assume independence of transitions and rewards with respect to the agents' tasks but the joint NSE penalty creates a form of dependence in this setting.","To improve scalability, the joint NSE penalty is decomposed into individual penalties for each agent using credit assignment, which facilitates decentralized policy computation.","Our results in simulation on three domains demonstrate the effectiveness and scalability of our approach in mitigating NSEs by updating the policies of a subset of agents in the system."],"url":"http://arxiv.org/abs/2405.04702v1","category":"cs.MA"}
{"created":"2024-05-07 22:15:58","title":"Relationship of temperature changes in the mesopause region with the climate changes at the surface from observations in 1960-2024","abstract":"The results of an analysis of temperature variations in the mesopause region based on long-term measurements of hydroxyl airglow at the Zvenigorod Scientific Station of the A.M. Obukhov Institute of Atmospheric Physics RAS (ZSS IAP RAS) in 1960-2024 in comparison with variations of surface temperature characterizing global-scale climate changes are presented. Along with temperature variations in the mesopause region, two versions of temperature variations in the mesopause region, normalized to the same level of solar activity, were analyzed. Quantitative estimates of a strong decrease in temperature in the mesopause region over the past decades in winter against the background of a global increase in surface temperature have been obtained. It was noted that significant coherence of long-term variations for temperature in the mesopause region with the surface temperature in the Northern Hemisphere with the use of cross-wavelet analysis, what was not previously evident in data for a shorter time interval. The possibility of such coherence was predicted in (Mokhov et al., 2017) under the continuation of global warming based on the results of model simulations for the 20-21 centuries, taking into account anthropogenic forcing. It was not previously manifested from observational data for a shorter time interval. Along with long-term trends, features of a sharp decrease in temperature in the mesopause region in the 1970s with its synchronicity with the known shift in surface climate regimes associated with El Ni\\~no events were analyzed. The results of cross-wavelet analysis using data obtained at the ZSS IAP RAS for the time interval 1960-2024 indicate a more significant connection between temperature variations in the mesopause region and El Nino indices in recent decades.","sentences":["The results of an analysis of temperature variations in the mesopause region based on long-term measurements of hydroxyl airglow at the Zvenigorod Scientific Station of the A.M. Obukhov Institute of Atmospheric Physics RAS (ZSS IAP RAS) in 1960-2024 in comparison with variations of surface temperature characterizing global-scale climate changes are presented.","Along with temperature variations in the mesopause region, two versions of temperature variations in the mesopause region, normalized to the same level of solar activity, were analyzed.","Quantitative estimates of a strong decrease in temperature in the mesopause region over the past decades in winter against the background of a global increase in surface temperature have been obtained.","It was noted that significant coherence of long-term variations for temperature in the mesopause region with the surface temperature in the Northern Hemisphere with the use of cross-wavelet analysis, what was not previously evident in data for a shorter time interval.","The possibility of such coherence was predicted in (Mokhov et al., 2017) under the continuation of global warming based on the results of model simulations for the 20-21 centuries, taking into account anthropogenic forcing.","It was not previously manifested from observational data for a shorter time interval.","Along with long-term trends, features of a sharp decrease in temperature in the mesopause region in the 1970s with its synchronicity with the known shift in surface climate regimes associated with El Ni\\~no events were analyzed.","The results of cross-wavelet analysis using data obtained at the ZSS IAP RAS for the time interval 1960-2024 indicate a more significant connection between temperature variations in the mesopause region and El Nino indices in recent decades."],"url":"http://arxiv.org/abs/2405.04695v1","category":"physics.ao-ph"}
{"created":"2024-05-07 22:07:45","title":"Generalization of the Alpha-Stable Distribution with the Degree of Freedom","abstract":"A Wright function based framework is proposed to combine and extend several distribution families. The $\\alpha$-stable distribution is generalized by adding the degree of freedom parameter. The PDF of this two-sided super distribution family subsumes those of the original $\\alpha$-stable, Student's t distributions, as well as the exponential power distribution and the modified Bessel function of the second kind. Its CDF leads to a fractional extension of the Gauss hypergeometric function. The degree of freedom makes possible for valid variance, skewness, and kurtosis, just like Student's t. The original $\\alpha$-stable distribution is viewed as having one degree of freedom, that explains why it lacks most of the moments. A skew-Gaussian kernel is derived from the characteristic function of the $\\alpha$-stable law, which maximally preserves the law in the new framework. To facilitate such framework, the stable count distribution is generalized as the fractional extension of the generalized gamma distribution. It provides rich subordination capabilities, one of which is the fractional $\\chi$ distribution that supplies the needed 'degree of freedom' parameter. Hence, the \"new\" $\\alpha$-stable distribution is a \"ratio distribution\" of the skew-Gaussian kernel and the fractional $\\chi$ distribution. Mathematically, it is a new form of higher transcendental function under the Wright function family. Last, the new univariate symmetric distribution is extended to the multivariate elliptical distribution successfully.","sentences":["A Wright function based framework is proposed to combine and extend several distribution families.","The $\\alpha$-stable distribution is generalized by adding the degree of freedom parameter.","The PDF of this two-sided super distribution family subsumes those of the original $\\alpha$-stable, Student's t distributions, as well as the exponential power distribution and the modified Bessel function of the second kind.","Its CDF leads to a fractional extension of the Gauss hypergeometric function.","The degree of freedom makes possible for valid variance, skewness, and kurtosis, just like Student's t. The original $\\alpha$-stable distribution is viewed as having one degree of freedom, that explains why it lacks most of the moments.","A skew-Gaussian kernel is derived from the characteristic function of the $\\alpha$-stable law, which maximally preserves the law in the new framework.","To facilitate such framework, the stable count distribution is generalized as the fractional extension of the generalized gamma distribution.","It provides rich subordination capabilities, one of which is the fractional $\\chi$ distribution that supplies the needed 'degree of freedom' parameter.","Hence, the \"new\" $\\alpha$-stable distribution is a \"ratio distribution\" of the skew-Gaussian kernel and the fractional $\\chi$ distribution.","Mathematically, it is a new form of higher transcendental function under the Wright function family.","Last, the new univariate symmetric distribution is extended to the multivariate elliptical distribution successfully."],"url":"http://arxiv.org/abs/2405.04693v1","category":"q-fin.ST"}
{"created":"2024-05-07 21:53:39","title":"Multicomplex Ideals, Modules and Hilbert Spaces","abstract":"In this article we study some algebraic aspects of multicomplex numbers $\\mathbb M_n$. A canonical idempotent representation defined in terms of $n$ multicomplex conjugates is introduced. This representation facilitates computations in this algebra and makes it possible to introduce a generalized conjugacy, i.e. a composition of the $n$ multicomplex conjugates, as well as a multicomplex norm. The ideals of the ring of multicomplex numbers are then studied. Multicomplex free modules and their linear operators are introduced. Finally, we develop multicomplex Hilbert spaces.","sentences":["In this article we study some algebraic aspects of multicomplex numbers $\\mathbb M_n$. A canonical idempotent representation defined in terms of $n$ multicomplex conjugates is introduced.","This representation facilitates computations in this algebra and makes it possible to introduce a generalized conjugacy, i.e. a composition of the $n$ multicomplex conjugates, as well as a multicomplex norm.","The ideals of the ring of multicomplex numbers are then studied.","Multicomplex free modules and their linear operators are introduced.","Finally, we develop multicomplex Hilbert spaces."],"url":"http://arxiv.org/abs/2405.04683v1","category":"math-ph"}
{"created":"2024-05-07 21:47:14","title":"Axi-Higgs portal Dark Matter via Wess-Zumino mechanism","abstract":"We study the axion portal between the visible and the dark sector, where the Dark Matter is charged under a simple abelian extension of the Standard Model. In general, such models are anomalous and are rendered gauge invariant by a St\\\"ukelberg axion through Wess-Zumino/Green-Schwarz mechanism. This axion mixes with other Goldstone bosons in the model to give a physical axi-Higgs which becomes massive upon breaking the anomalous gauge group. Such axi-Higgs fields charged under the anomalous symmetry act as mediators for the Dark Matter annihilation to Standard Model particles and can lead to an efficient freeze-out mechanism. Here, we show that the St{\\\"u}kelberg axion and the resultant axi-Higgs, with its appropriate shift symmetry cancels the quantum anomalies and also generate the observed relic density for the Dark Matter. Moreover, we show that the relevant parameter space in our model, where photon production dominates, is safe from FermiLAT, Cherenkov Telescope Array, and H.E.S.S. indirect detection experiments.","sentences":["We study the axion portal between the visible and the dark sector, where the Dark Matter is charged under a simple abelian extension of the Standard Model.","In general, such models are anomalous and are rendered gauge invariant by a St\\\"ukelberg axion through Wess-Zumino/Green-Schwarz mechanism.","This axion mixes with other Goldstone bosons in the model to give a physical axi-Higgs which becomes massive upon breaking the anomalous gauge group.","Such axi-Higgs fields charged under the anomalous symmetry act as mediators for the Dark Matter annihilation to Standard Model particles and can lead to an efficient freeze-out mechanism.","Here, we show that the St{\\\"u}kelberg axion and the resultant axi-Higgs, with its appropriate shift symmetry cancels the quantum anomalies and also generate the observed relic density for the Dark Matter.","Moreover, we show that the relevant parameter space in our model, where photon production dominates, is safe from FermiLAT, Cherenkov Telescope Array, and H.E.S.S. indirect detection experiments."],"url":"http://arxiv.org/abs/2405.04680v1","category":"hep-ph"}
{"created":"2024-05-07 21:06:40","title":"Enhanced Lieb-Robinson bounds for a class of Bose-Hubbard type Hamiltonians","abstract":"Several recent works have considered Lieb-Robinson bounds (LRBs) for Bose-Hubbard-type Hamiltonians. For certain special classes of initial states (e.g., states with particle-free regions or perturbations of stationary states), the velocity of information propagation was bounded by a constant in time, $v\\leq C$, similarly to quantum spin systems. However, for the more general class of bounded-density initial states, the first-named author together with Vu and Saito derived the velocity bound $v\\leq C t^{D-1}$, where $D$ is the spatial lattice dimension. For $D\\geq 2$, this bound allows for accelerated information propagation. It has been known since the work of Eisert and Gross that some systems of lattice bosons are capable of accelerated information propagation. It is therefore a central question to understand under what conditions the bound $v\\leq C t^{D-1}$ can be enhanced. Here, we prove that additional physical constraints, translation-invariance and a $p$-body repulsion of the form $n_x^p$ with $p>D+1$, lead to a LRB with $v\\leq C t^{\\frac{D}{p-D-1}}$ for any initial state of bounded energy density. We also identify examples of quantum states which show that no further enhancement is possible without using additional dynamical constraints.","sentences":["Several recent works have considered Lieb-Robinson bounds (LRBs) for Bose-Hubbard-type Hamiltonians.","For certain special classes of initial states (e.g., states with particle-free regions or perturbations of stationary states), the velocity of information propagation was bounded by a constant in time, $v\\leq C$, similarly to quantum spin systems.","However, for the more general class of bounded-density initial states, the first-named author together with Vu and Saito derived the velocity bound $v\\leq C t^{D-1}$, where $D$ is the spatial lattice dimension.","For $D\\geq 2$, this bound allows for accelerated information propagation.","It has been known since the work of Eisert and Gross that some systems of lattice bosons are capable of accelerated information propagation.","It is therefore a central question to understand under what conditions the bound $v\\leq C t^{D-1}$ can be enhanced.","Here, we prove that additional physical constraints, translation-invariance and a $p$-body repulsion of the form $n_x^p$ with $p>D+1$, lead to a LRB with $v\\leq C t^{\\frac{D}{p-D-1}}$ for any initial state of bounded energy density.","We also identify examples of quantum states which show that no further enhancement is possible without using additional dynamical constraints."],"url":"http://arxiv.org/abs/2405.04672v1","category":"math-ph"}
{"created":"2024-05-07 21:03:51","title":"Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics","abstract":"Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on ''A is B'', LLM fails to directly conclude ''B is A'' during inference, which is known as the ''reversal curse'' (Berglund et al., 2023). In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers using the framework of Tian et al. (2023a). Our analysis reveals a core reason why the reversal curse happens: the (effective) weights of both auto-regressive models show asymmetry, i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT) (Wei et al., 2022b). We show the necessity of COT, i.e., a model trained on ''$A \\to B$'' and ''$B \\to C$'' fails to directly conclude ''$A \\to C$'' without COT (also empirically observed by Allen-Zhu and Li (2023)), for one-layer transformers via training dynamics, which provides a new perspective different from previous work (Feng et al., 2024) that focuses on expressivity. Finally, we also conduct experiments to validate our theory on multi-layer transformers under different settings.","sentences":["Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on ''A is B'', LLM fails to directly conclude ''B is A'' during inference, which is known as the ''reversal curse'' (Berglund et al., 2023).","In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers using the framework of Tian et al. (2023a).","Our analysis reveals a core reason why the reversal curse happens: the (effective) weights of both auto-regressive models show asymmetry, i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$.","Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT) (Wei et al., 2022b).","We show the necessity of COT, i.e., a model trained on ''$A \\to B$'' and ''$B \\to C$'' fails to directly conclude ''$A \\to C$'' without COT (also empirically observed by Allen-Zhu and Li (2023)), for one-layer transformers via training dynamics, which provides a new perspective different from previous work (Feng et al., 2024) that focuses on expressivity.","Finally, we also conduct experiments to validate our theory on multi-layer transformers under different settings."],"url":"http://arxiv.org/abs/2405.04669v1","category":"cs.LG"}
{"created":"2024-05-07 20:58:46","title":"Boundedness in a chemotaxis system with weakly singular sensitivity in dimension two with arbitrary sub-quadratic degradation sources","abstract":"We study the global existence and boundedness of solutions to a chemotaxis system with weakly singular sensitivity and sub-logistic sources in a two dimensional domain. X. Zhao (Nonlinearity; 2023; 36; 3909-3938 ) showed that the logistic degradation, $-\\mu u^2$, can prevent blow-up under the largeness assumption on $\\mu$. In this paper, we improve the result by replacing the quadratic degradation by sub-logistic one, $-\\frac{\\mu u^2}{\\ln^\\beta(u+e)}$ with $\\beta \\in (0,1)$, and removing the largeness assumption on $\\mu$.","sentences":["We study the global existence and boundedness of solutions to a chemotaxis system with weakly singular sensitivity and sub-logistic sources in a two dimensional domain.","X. Zhao (Nonlinearity; 2023; 36; 3909-3938 ) showed that the logistic degradation, $-\\mu u^2$, can prevent blow-up under the largeness assumption on $\\mu$. In this paper, we improve the result by replacing the quadratic degradation by sub-logistic one, $-\\frac{\\mu u^2}{\\ln^\\beta(u+e)}$ with $\\beta \\in (0,1)$, and removing the largeness assumption on $\\mu$."],"url":"http://arxiv.org/abs/2405.04666v1","category":"math.AP"}
{"created":"2024-05-07 20:44:30","title":"Relativistic Dips in Entangling Power of Gravity","abstract":"The salient feature of both classical and quantum gravity is its universal and attractive character. However, less is known about the behaviour and build-up of quantum correlations when quantum systems interact via graviton exchange. In this work, we show that quantum correlations can remain strongly suppressed for certain choices of parameters even when considering two adjacent quantum systems in delocalized states. Using the framework of linearized quantum gravity with post-Newtonian contributions, we find that there are special values of delocalization where gravitationally induced entanglement drops to negligible values, albeit non-vanishing. We find a pronounced cancellation point far from the Planck scale, where the system tends towards classicalization. In addition, we show that quantum correlations begin to reemerge for large and tiny delocalizations due to Heisenberg's uncertainty principle and the universal coupling of gravity to the energy-momentum tensor, forming a valley of gravitational entanglement.","sentences":["The salient feature of both classical and quantum gravity is its universal and attractive character.","However, less is known about the behaviour and build-up of quantum correlations when quantum systems interact via graviton exchange.","In this work, we show that quantum correlations can remain strongly suppressed for certain choices of parameters even when considering two adjacent quantum systems in delocalized states.","Using the framework of linearized quantum gravity with post-Newtonian contributions, we find that there are special values of delocalization where gravitationally induced entanglement drops to negligible values, albeit non-vanishing.","We find a pronounced cancellation point far from the Planck scale, where the system tends towards classicalization.","In addition, we show that quantum correlations begin to reemerge for large and tiny delocalizations due to Heisenberg's uncertainty principle and the universal coupling of gravity to the energy-momentum tensor, forming a valley of gravitational entanglement."],"url":"http://arxiv.org/abs/2405.04661v1","category":"quant-ph"}
{"created":"2024-05-07 20:25:37","title":"Nonradial instabilities in anisotropic neutron stars","abstract":"Non-radial oscillation modes of a neutron star possess valuable information about its internal structure and nuclear physics. Starting from the quadrupolar order, such modes under general relativity are known as quasi-normal modes since they dissipate energy through gravitational radiation and their frequencies are complex. The stability of these modes is governed by the sign of the imaginary part of the frequency, which determines whether the mode would decay or grow over time. In this Letter, we develop a fully consistent framework in general relativity to study quasi-normal modes of neutron stars with anisotropic pressure, whose motivation includes strong internal magnetic fields and non-vanishing shear or viscosity. We employ parametrized models for the anisotropy and solve the perturbed Einstein field equations numerically. We find that, unlike the case for isotropic neutron stars, the imaginary parts of some of the pressure ($p$-)modes flip signs as the degree of anisotropy deviates from zero, depicting a transition from stable modes to unstable modes. This finding indicates that some anisotropic neutron star models are unstable, potentially restricting the form of sustained anisotropy.","sentences":["Non-radial oscillation modes of a neutron star possess valuable information about its internal structure and nuclear physics.","Starting from the quadrupolar order, such modes under general relativity are known as quasi-normal modes since they dissipate energy through gravitational radiation and their frequencies are complex.","The stability of these modes is governed by the sign of the imaginary part of the frequency, which determines whether the mode would decay or grow over time.","In this Letter, we develop a fully consistent framework in general relativity to study quasi-normal modes of neutron stars with anisotropic pressure, whose motivation includes strong internal magnetic fields and non-vanishing shear or viscosity.","We employ parametrized models for the anisotropy and solve the perturbed Einstein field equations numerically.","We find that, unlike the case for isotropic neutron stars, the imaginary parts of some of the pressure ($p$-)modes flip signs as the degree of anisotropy deviates from zero, depicting a transition from stable modes to unstable modes.","This finding indicates that some anisotropic neutron star models are unstable, potentially restricting the form of sustained anisotropy."],"url":"http://arxiv.org/abs/2405.04653v1","category":"gr-qc"}
{"created":"2024-05-07 20:10:09","title":"Simpler and More General Distributed Coloring Based on Simple List Defective Coloring Algorithms","abstract":"In this paper, we give list coloring variants of simple iterative defective coloring algorithms. Formally, in a list defective coloring instance, each node $v$ of a graph is given a list $L_v$ of colors and a list of allowed defects $d_v(x)$ for the colors. Each node $v$ needs to be colored with a color $x\\in L_v$ such that at most $d_v(x)$ neighbors of $v$ also pick the same color $x$. For a defect parameter $d$, it is known that by making two sweeps in opposite order over the nodes of an edge-oriented graph with maximum outdegree $\\beta$, one can compute a coloring with $O(\\beta^2/d^2)$ colors such that every node has at most $d$ outneighbors of the same color. We generalize this and show that if all nodes have lists of size $p^2$ and $\\forall v:\\sum_{x\\in L_v}(d_v(x)+1)>p\\cdot\\beta$, we can make two sweeps of the nodes such that at the end, each node $v$ has chosen a color $x\\in L_v$ for which at most $d_v(x)$ outneighbors of $v$ are colored with color $x$. Our algorithm is simpler and computationally significantly more efficient than existing algorithms for similar list defective coloring problems. We show that the above result can in particular be used to obtain an alternative $\\tilde{O}(\\sqrt{\\Delta})+O(\\log^* n)$-round algorithm for the $(\\Delta+1)$-coloring problem in the CONGEST model. The neighborhood independence $\\theta$ of a graph is the maximum number of pairwise non-adjacent neighbors of some node of the graph. It is known that by doing a single sweep over the nodes of a graph of neighborhood independence $\\theta$, one can compute a $d$-defective coloring with $O(\\theta\\cdot \\Delta/d)$ colors. We extend this approach to the list defective coloring setting and use it to obtain an efficient recursive coloring algorithm for graphs of neighborhood independence $\\theta$. In particular, if $\\theta=O(1)$, we get an $(\\log\\Delta)^{O(\\log\\log\\Delta)}+O(\\log^* n)$-round algorithm.","sentences":["In this paper, we give list coloring variants of simple iterative defective coloring algorithms.","Formally, in a list defective coloring instance, each node $v$ of a graph is given a list $L_v$ of colors and a list of allowed defects $d_v(x)$ for the colors.","Each node $v$ needs to be colored with a color $x\\in L_v$ such that at most $d_v(x)$ neighbors of $v$ also pick the same color $x$.","For a defect parameter $d$, it is known that by making two sweeps in opposite order over the nodes of an edge-oriented graph with maximum outdegree $\\beta$, one can compute a coloring with $O(\\beta^2/d^2)$ colors such that every node has at most $d$ outneighbors of the same color.","We generalize this and show that if all nodes have lists of size $p^2$ and $\\forall v:\\sum_{x\\in L_v}(d_v(x)+1)>p\\cdot\\beta$, we can make two sweeps of the nodes such that at the end, each node $v$ has chosen a color $x\\in L_v$ for which at most $d_v(x)$ outneighbors of $v$ are colored with color $x$.","Our algorithm is simpler and computationally significantly more efficient than existing algorithms for similar list defective coloring problems.","We show that the above result can in particular be used to obtain an alternative $\\tilde{O}(\\sqrt{\\Delta})+O(\\log^* n)$-round algorithm for the $(\\Delta+1)$-coloring problem in the CONGEST model.","The neighborhood independence $\\theta$ of a graph is the maximum number of pairwise non-adjacent neighbors of some node of the graph.","It is known that by doing a single sweep over the nodes of a graph of neighborhood independence $\\theta$, one can compute a $d$-defective coloring with $O(\\theta\\cdot \\Delta/d)$ colors.","We extend this approach to the list defective coloring setting and use it to obtain an efficient recursive coloring algorithm for graphs of neighborhood independence $\\theta$. In particular, if $\\theta=O(1)$, we get an $(\\log\\Delta)^{O(\\log\\log\\Delta)}+O(\\log^* n)$-round algorithm."],"url":"http://arxiv.org/abs/2405.04648v1","category":"cs.DS"}
{"created":"2024-05-07 19:38:26","title":"Data-driven Error Estimation: Upper Bounding Multiple Errors with No Technical Debt","abstract":"We formulate the problem of constructing multiple simultaneously valid confidence intervals (CIs) as estimating a high probability upper bound on the maximum error for a class/set of estimate-estimand-error tuples, and refer to this as the error estimation problem. For a single such tuple, data-driven confidence intervals can often be used to bound the error in our estimate. However, for a class of estimate-estimand-error tuples, nontrivial high probability upper bounds on the maximum error often require class complexity as input -- limiting the practicality of such methods and often resulting in loose bounds. Rather than deriving theoretical class complexity-based bounds, we propose a completely data-driven approach to estimate an upper bound on the maximum error. The simple and general nature of our solution to this fundamental challenge lends itself to several applications including: multiple CI construction, multiple hypothesis testing, estimating excess risk bounds (a fundamental measure of uncertainty in machine learning) for any training/fine-tuning algorithm, and enabling the development of a contextual bandit pipeline that can leverage any reward model estimation procedure as input (without additional mathematical analysis).","sentences":["We formulate the problem of constructing multiple simultaneously valid confidence intervals (CIs) as estimating a high probability upper bound on the maximum error for a class/set of estimate-estimand-error tuples, and refer to this as the error estimation problem.","For a single such tuple, data-driven confidence intervals can often be used to bound the error in our estimate.","However, for a class of estimate-estimand-error tuples, nontrivial high probability upper bounds on the maximum error often require class complexity as input -- limiting the practicality of such methods and often resulting in loose bounds.","Rather than deriving theoretical class complexity-based bounds, we propose a completely data-driven approach to estimate an upper bound on the maximum error.","The simple and general nature of our solution to this fundamental challenge lends itself to several applications including: multiple CI construction, multiple hypothesis testing, estimating excess risk bounds (a fundamental measure of uncertainty in machine learning) for any training/fine-tuning algorithm, and enabling the development of a contextual bandit pipeline that can leverage any reward model estimation procedure as input (without additional mathematical analysis)."],"url":"http://arxiv.org/abs/2405.04636v1","category":"cs.LG"}
{"created":"2024-05-07 19:33:26","title":"The structure and migration of heavily irradiated grain boundaries and dislocations in Ni in the athermal limit","abstract":"The microstructural evolution at and near pre-existing grain boundaries (GBs) and dislocations in materials under high radiation doses is still poorly understood. In this work, we use the creation relaxation algorithm (CRA) developed for atomistic modeling of high-dose irradiation in bulk materials to probe the athermal limit of saturation of GB and dislocation core regions under irradiation in FCC Ni. We find that, upon continuously subjecting a single dislocation or GB to Frenkel pair creation in the athermal limit, a local steady state disordered defect structure is reached with excess properties that fluctuate around constant values. Case studies are given for a straight screw dislocation which elongates into a helix under irradiation and several types of low and high angle GBs, which exhibit coupled responses such as absorption of extrinsic dislocations, roughening and migration. A positive correlation is found between initial GB energy and the local steady state GB energy under irradiation across a wide variety of GB types. Metastable GB structures with similar density in the defect core region but different initial configurations are found to converge to the same limiting structure under CRA. The mechanical responses of pristine and irradiated dislocations and GB structures are compared under an applied shear stress. Irradiated screw and edge dislocations are found to exhibit a hardening response, migrating at larger flow stresses than their pristine counterparts. Mobile GBs are found to exhibit softening or hardening responses depending on GB character. Although some GBs recover their initial pristine structures upon migration outside of the radiation zone, many GBs sustain different flow stresses corresponding to altered mobile core structures.","sentences":["The microstructural evolution at and near pre-existing grain boundaries (GBs) and dislocations in materials under high radiation doses is still poorly understood.","In this work, we use the creation relaxation algorithm (CRA) developed for atomistic modeling of high-dose irradiation in bulk materials to probe the athermal limit of saturation of GB and dislocation core regions under irradiation in FCC Ni.","We find that, upon continuously subjecting a single dislocation or GB to Frenkel pair creation in the athermal limit, a local steady state disordered defect structure is reached with excess properties that fluctuate around constant values.","Case studies are given for a straight screw dislocation which elongates into a helix under irradiation and several types of low and high angle GBs, which exhibit coupled responses such as absorption of extrinsic dislocations, roughening and migration.","A positive correlation is found between initial GB energy and the local steady state GB energy under irradiation across a wide variety of GB types.","Metastable GB structures with similar density in the defect core region but different initial configurations are found to converge to the same limiting structure under CRA.","The mechanical responses of pristine and irradiated dislocations and GB structures are compared under an applied shear stress.","Irradiated screw and edge dislocations are found to exhibit a hardening response, migrating at larger flow stresses than their pristine counterparts.","Mobile GBs are found to exhibit softening or hardening responses depending on GB character.","Although some GBs recover their initial pristine structures upon migration outside of the radiation zone, many GBs sustain different flow stresses corresponding to altered mobile core structures."],"url":"http://arxiv.org/abs/2405.04633v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 19:20:13","title":"Wasserstein Proximal Coordinate Gradient Algorithms","abstract":"Motivated by approximation Bayesian computation using mean-field variational approximation and the computation of equilibrium in multi-species systems with cross-interaction, this paper investigates the composite geodesically convex optimization problem over multiple distributions. The objective functional under consideration is composed of a convex potential energy on a product of Wasserstein spaces and a sum of convex self-interaction and internal energies associated with each distribution. To efficiently solve this problem, we introduce the Wasserstein Proximal Coordinate Gradient (WPCG) algorithms with parallel, sequential and random update schemes. Under a quadratic growth (QC) condition that is weaker than the usual strong convexity requirement on the objective functional, we show that WPCG converges exponentially fast to the unique global optimum. In the absence of the QG condition, WPCG is still demonstrated to converge to the global optimal solution, albeit at a slower polynomial rate. Numerical results for both motivating examples are consistent with our theoretical findings.","sentences":["Motivated by approximation Bayesian computation using mean-field variational approximation and the computation of equilibrium in multi-species systems with cross-interaction, this paper investigates the composite geodesically convex optimization problem over multiple distributions.","The objective functional under consideration is composed of a convex potential energy on a product of Wasserstein spaces and a sum of convex self-interaction and internal energies associated with each distribution.","To efficiently solve this problem, we introduce the Wasserstein Proximal Coordinate Gradient (WPCG) algorithms with parallel, sequential and random update schemes.","Under a quadratic growth (QC) condition that is weaker than the usual strong convexity requirement on the objective functional, we show that WPCG converges exponentially fast to the unique global optimum.","In the absence of the QG condition, WPCG is still demonstrated to converge to the global optimal solution, albeit at a slower polynomial rate.","Numerical results for both motivating examples are consistent with our theoretical findings."],"url":"http://arxiv.org/abs/2405.04628v1","category":"math.OC"}
{"created":"2024-05-07 19:11:45","title":"On Geometric Implications","abstract":"It is a well-known fact that although the poset of open sets of a topological space is a Heyting algebra, its Heyting implication is not necessarily stable under the inverse image of continuous functions and hence is not a geometric concept. This leaves us wondering if there is any stable family of implications that can be safely called geometric. In this paper, we will first recall the abstract notion of implication as a binary modality introduced in [1]. Then, we will use a weaker version of categorical fibrations to define the geometricity of a category of pairs of spaces and implications over a given category of spaces. We will identify the greatest geometric category over the subcategories of open-irreducible (closed-irreducible) maps as a generalization of the usual injective open (closed) maps. Using this identification, we will then characterize all geometric categories over a given category S, provided that S has some basic closure properties. Specially, we will show that there is no non-trivial geometric category over the full category of spaces. Finally, as the implications we identified are also interesting in their own right, we will spend some time to investigate their algebraic properties. We will first use a Yoneda-type argument to provide a representation theorem, making the implications a part of an adjunction-style pair. Then, we will use this result to provide a Kripke-style representation for any arbitrary implication.","sentences":["It is a well-known fact that although the poset of open sets of a topological space is a Heyting algebra, its Heyting implication is not necessarily stable under the inverse image of continuous functions and hence is not a geometric concept.","This leaves us wondering if there is any stable family of implications that can be safely called geometric.","In this paper, we will first recall the abstract notion of implication as a binary modality introduced in [1].","Then, we will use a weaker version of categorical fibrations to define the geometricity of a category of pairs of spaces and implications over a given category of spaces.","We will identify the greatest geometric category over the subcategories of open-irreducible (closed-irreducible) maps as a generalization of the usual injective open (closed) maps.","Using this identification, we will then characterize all geometric categories over a given category S, provided that S has some basic closure properties.","Specially, we will show that there is no non-trivial geometric category over the full category of spaces.","Finally, as the implications we identified are also interesting in their own right, we will spend some time to investigate their algebraic properties.","We will first use a Yoneda-type argument to provide a representation theorem, making the implications a part of an adjunction-style pair.","Then, we will use this result to provide a Kripke-style representation for any arbitrary implication."],"url":"http://arxiv.org/abs/2405.04625v1","category":"math.LO"}
{"created":"2024-05-07 19:11:10","title":"The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models","abstract":"Scale the model, scale the data, scale the GPU farms is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. Content warning: This article contains racially dehumanising and offensive descriptions.","sentences":["Scale the model, scale the data, scale the GPU farms is the reigning sentiment in the world of generative AI today.","While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored.","This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks.","In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe.","Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased.","Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models.","Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples.","We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward.","Content warning: This article contains racially dehumanising and offensive descriptions."],"url":"http://arxiv.org/abs/2405.04623v1","category":"cs.CY"}
{"created":"2024-05-07 19:09:35","title":"Bounds on the Statistical Leakage-Resilience of Shamir's Secret Sharing","abstract":"Secret sharing is an instrumental tool for sharing secret keys in distributed systems. In a classical threshold setting, this involves a dealer who has a secret/key, a set of parties/users to which shares of the secret are sent, and a threshold on the number of users whose presence is needed in order to recover the secret. In secret sharing, secure links with no leakage are often assumed between the involved parties. However, when the users are nodes in a communication network and all the links are physical links, e.g., wireless, such assumptions are not valid anymore. In order to study this critical problem, we propose a statistical leakage model of secret sharing, where some noisy versions of all the secret shares might be independently leaked to an adversary. We then study the resilience of the seminal Shamir's secret sharing scheme with statistical leakage, and bound certain measures of security (i.e., semantic security, mutual information security), given other parameters of the system including the amount of leakage from each secret share. We show that for an extreme scenario of Shamir's scheme, in particular when the underlying field characteristic is $2$, the security of each bit of the secret against leakage improves exponentially with the number of users. To the best of our knowledge, this is the first attempt towards understanding secret sharing under general statistical noisy leakage.","sentences":["Secret sharing is an instrumental tool for sharing secret keys in distributed systems.","In a classical threshold setting, this involves a dealer who has a secret/key, a set of parties/users to which shares of the secret are sent, and a threshold on the number of users whose presence is needed in order to recover the secret.","In secret sharing, secure links with no leakage are often assumed between the involved parties.","However, when the users are nodes in a communication network and all the links are physical links, e.g., wireless, such assumptions are not valid anymore.","In order to study this critical problem, we propose a statistical leakage model of secret sharing, where some noisy versions of all the secret shares might be independently leaked to an adversary.","We then study the resilience of the seminal Shamir's secret sharing scheme with statistical leakage, and bound certain measures of security (i.e., semantic security, mutual information security), given other parameters of the system including the amount of leakage from each secret share.","We show that for an extreme scenario of Shamir's scheme, in particular when the underlying field characteristic is $2$, the security of each bit of the secret against leakage improves exponentially with the number of users.","To the best of our knowledge, this is the first attempt towards understanding secret sharing under general statistical noisy leakage."],"url":"http://arxiv.org/abs/2405.04622v1","category":"cs.IT"}
{"created":"2024-05-07 18:58:42","title":"Unique continuation for the wave equation based on a discontinuous Galerkin time discretization","abstract":"We consider a stable unique continuation problem for the wave equation where the initial data is lacking and the solution is reconstructed using measurements in some subset of the bulk domain. Typically fairly sophisticated space-time methods have been used in previous work to obtain stable and accurate solutions to this reconstruction problem. Here we propose to solve the problem using a standard discontinuous Galerkin method for the temporal discretization and continuous finite elements for the space discretization. Error estimates are established under a geometric control condition. We also investigate two preconditioning strategies which can be used to solve the arising globally coupled space-time system by means of simple time-stepping procedures. Our numerical experiments test the performance of these strategies and highlight the importance of the geometric control condition for reconstructing the solution beyond the data domain.","sentences":["We consider a stable unique continuation problem for the wave equation where the initial data is lacking and the solution is reconstructed using measurements in some subset of the bulk domain.","Typically fairly sophisticated space-time methods have been used in previous work to obtain stable and accurate solutions to this reconstruction problem.","Here we propose to solve the problem using a standard discontinuous Galerkin method for the temporal discretization and continuous finite elements for the space discretization.","Error estimates are established under a geometric control condition.","We also investigate two preconditioning strategies which can be used to solve the arising globally coupled space-time system by means of simple time-stepping procedures.","Our numerical experiments test the performance of these strategies and highlight the importance of the geometric control condition for reconstructing the solution beyond the data domain."],"url":"http://arxiv.org/abs/2405.04615v1","category":"math.NA"}
{"created":"2024-05-07 18:58:32","title":"Multi-Margin Loss: Proposal and Application in Recommender Systems","abstract":"Recommender systems guide users through vast amounts of information by suggesting items based on their predicted preferences. Collaborative filtering-based deep learning techniques have regained popularity due to their straightforward nature, relying only on user-item interactions. Typically, these systems consist of three main components: an interaction module, a loss function, and a negative sampling strategy. Initially, researchers focused on enhancing performance by developing complex interaction modules. However, there has been a recent shift toward refining loss functions and negative sampling strategies. This shift has led to an increased interest in contrastive learning, which pulls similar pairs closer while pushing dissimilar ones apart. Contrastive learning involves key practices such as heavy data augmentation, large batch sizes, and hard-negative sampling, but these also bring challenges like high memory demands and under-utilization of some negative samples. The proposed Multi-Margin Loss (MML) addresses these challenges by introducing multiple margins and varying weights for negative samples. This allows MML to efficiently utilize not only the hardest negatives but also other non-trivial negatives, offering a simpler yet effective loss function that outperforms more complex methods, especially when resources are limited. Experiments on two well-known datasets demonstrated that MML achieved up to a 20% performance improvement compared to a baseline contrastive loss function when fewer number of negative samples are used.","sentences":["Recommender systems guide users through vast amounts of information by suggesting items based on their predicted preferences.","Collaborative filtering-based deep learning techniques have regained popularity due to their straightforward nature, relying only on user-item interactions.","Typically, these systems consist of three main components: an interaction module, a loss function, and a negative sampling strategy.","Initially, researchers focused on enhancing performance by developing complex interaction modules.","However, there has been a recent shift toward refining loss functions and negative sampling strategies.","This shift has led to an increased interest in contrastive learning, which pulls similar pairs closer while pushing dissimilar ones apart.","Contrastive learning involves key practices such as heavy data augmentation, large batch sizes, and hard-negative sampling, but these also bring challenges like high memory demands and under-utilization of some negative samples.","The proposed Multi-Margin Loss (MML) addresses these challenges by introducing multiple margins and varying weights for negative samples.","This allows MML to efficiently utilize not only the hardest negatives but also other non-trivial negatives, offering a simpler yet effective loss function that outperforms more complex methods, especially when resources are limited.","Experiments on two well-known datasets demonstrated that MML achieved up to a 20% performance improvement compared to a baseline contrastive loss function when fewer number of negative samples are used."],"url":"http://arxiv.org/abs/2405.04614v1","category":"cs.LG"}
{"created":"2024-05-07 18:50:57","title":"Numerical Fuzz: A Type System for Rounding Error Analysis","abstract":"Algorithms operating on real numbers are implemented as floating-point computations in practice, but floating-point operations introduce roundoff errors that can degrade the accuracy of the result. We propose $\\Lambda_{num}$, a functional programming language with a type system that can express quantitative bounds on roundoff error. Our type system combines a sensitivity analysis, enforced through a linear typing discipline, with a novel graded monad to track the accumulation of roundoff errors. We prove that our type system is sound by relating the denotational semantics of our language to the exact and floating-point operational semantics. To demonstrate our system, we instantiate $\\Lambda_{num}$ with error metrics proposed in the numerical analysis literature and we show how to incorporate rounding operations that faithfully model aspects of the IEEE 754 floating-point standard. To show that $\\Lambda_{num}$ can be a useful tool for automated error analysis, we develop a prototype implementation for $\\Lambda_{num}$ that infers error bounds that are competitive with existing tools, while often running significantly faster. Finally, we consider semantic extensions of our graded monad to bound error under more complex rounding behaviors, such as non-deterministic and randomized rounding.","sentences":["Algorithms operating on real numbers are implemented as floating-point computations in practice, but floating-point operations introduce roundoff errors that can degrade the accuracy of the result.","We propose $\\Lambda_{num}$, a functional programming language with a type system that can express quantitative bounds on roundoff error.","Our type system combines a sensitivity analysis, enforced through a linear typing discipline, with a novel graded monad to track the accumulation of roundoff errors.","We prove that our type system is sound by relating the denotational semantics of our language to the exact and floating-point operational semantics.","To demonstrate our system, we instantiate $\\Lambda_{num}$ with error metrics proposed in the numerical analysis literature and we show how to incorporate rounding operations that faithfully model aspects of the IEEE 754 floating-point standard.","To show that $\\Lambda_{num}$ can be a useful tool for automated error analysis, we develop a prototype implementation for $\\Lambda_{num}$ that infers error bounds that are competitive with existing tools, while often running significantly faster.","Finally, we consider semantic extensions of our graded monad to bound error under more complex rounding behaviors, such as non-deterministic and randomized rounding."],"url":"http://arxiv.org/abs/2405.04612v1","category":"cs.PL"}
{"created":"2024-05-07 18:49:34","title":"Exploring Explainable AI Techniques for Improved Interpretability in Lung and Colon Cancer Classification","abstract":"Lung and colon cancer are serious worldwide health challenges that require early and precise identification to reduce mortality risks. However, diagnosis, which is mostly dependent on histopathologists' competence, presents difficulties and hazards when expertise is insufficient. While diagnostic methods like imaging and blood markers contribute to early detection, histopathology remains the gold standard, although time-consuming and vulnerable to inter-observer mistakes. Limited access to high-end technology further limits patients' ability to receive immediate medical care and diagnosis. Recent advances in deep learning have generated interest in its application to medical imaging analysis, specifically the use of histopathological images to diagnose lung and colon cancer. The goal of this investigation is to use and adapt existing pre-trained CNN-based models, such as Xception, DenseNet201, ResNet101, InceptionV3, DenseNet121, DenseNet169, ResNet152, and InceptionResNetV2, to enhance classification through better augmentation strategies. The results show tremendous progress, with all eight models reaching impressive accuracy ranging from 97% to 99%. Furthermore, attention visualization techniques such as GradCAM, GradCAM++, ScoreCAM, Faster Score-CAM, and LayerCAM, as well as Vanilla Saliency and SmoothGrad, are used to provide insights into the models' classification decisions, thereby improving interpretability and understanding of malignant and benign image classification.","sentences":["Lung and colon cancer are serious worldwide health challenges that require early and precise identification to reduce mortality risks.","However, diagnosis, which is mostly dependent on histopathologists' competence, presents difficulties and hazards when expertise is insufficient.","While diagnostic methods like imaging and blood markers contribute to early detection, histopathology remains the gold standard, although time-consuming and vulnerable to inter-observer mistakes.","Limited access to high-end technology further limits patients' ability to receive immediate medical care and diagnosis.","Recent advances in deep learning have generated interest in its application to medical imaging analysis, specifically the use of histopathological images to diagnose lung and colon cancer.","The goal of this investigation is to use and adapt existing pre-trained CNN-based models, such as Xception, DenseNet201, ResNet101, InceptionV3, DenseNet121, DenseNet169, ResNet152, and InceptionResNetV2, to enhance classification through better augmentation strategies.","The results show tremendous progress, with all eight models reaching impressive accuracy ranging from 97% to 99%.","Furthermore, attention visualization techniques such as GradCAM, GradCAM++, ScoreCAM, Faster Score-CAM, and LayerCAM, as well as Vanilla Saliency and SmoothGrad, are used to provide insights into the models' classification decisions, thereby improving interpretability and understanding of malignant and benign image classification."],"url":"http://arxiv.org/abs/2405.04610v1","category":"eess.IV"}
{"created":"2024-05-07 18:37:21","title":"Probabilistic Byzantine Fault Tolerance (Extended Version)","abstract":"Consensus is a fundamental building block for constructing reliable and fault-tolerant distributed services. Many Byzantine fault-tolerant consensus protocols designed for partially synchronous systems adopt a pessimistic approach when dealing with adversaries, ensuring safety in a deterministic way even under the worst-case scenarios that adversaries can create. Following this approach typically results in either an increase in the message complexity (e.g., PBFT) or an increase in the number of communication steps (e.g., HotStuff). In practice, however, adversaries are not as powerful as the ones assumed by these protocols. Furthermore, it might suffice to ensure safety and liveness properties with high probability. In order to accommodate more realistic and optimistic adversaries and improve the scalability of the BFT consensus, we propose ProBFT (Probabilistic Byzantine Fault Tolerance). ProBFT is a leader-based probabilistic consensus protocol with a message complexity of $O(n\\sqrt{n})$ and an optimal number of communication steps that tolerates Byzantine faults in permissioned partially synchronous systems. It is built on top of well-known primitives, such as probabilistic Byzantine quorums and verifiable random functions. ProBFT guarantees safety and liveness with high probabilities even with faulty leaders, as long as a supermajority of replicas is correct, and using only a fraction of messages employed in PBFT (e.g., $20\\%$). We provide a detailed description of ProBFT's protocol and its analysis.","sentences":["Consensus is a fundamental building block for constructing reliable and fault-tolerant distributed services.","Many Byzantine fault-tolerant consensus protocols designed for partially synchronous systems adopt a pessimistic approach when dealing with adversaries, ensuring safety in a deterministic way even under the worst-case scenarios that adversaries can create.","Following this approach typically results in either an increase in the message complexity (e.g., PBFT) or an increase in the number of communication steps (e.g., HotStuff).","In practice, however, adversaries are not as powerful as the ones assumed by these protocols.","Furthermore, it might suffice to ensure safety and liveness properties with high probability.","In order to accommodate more realistic and optimistic adversaries and improve the scalability of the BFT consensus, we propose ProBFT (Probabilistic Byzantine Fault Tolerance).","ProBFT is a leader-based probabilistic consensus protocol with a message complexity of $O(n\\sqrt{n})$ and an optimal number of communication steps that tolerates Byzantine faults in permissioned partially synchronous systems.","It is built on top of well-known primitives, such as probabilistic Byzantine quorums and verifiable random functions.","ProBFT guarantees safety and liveness with high probabilities even with faulty leaders, as long as a supermajority of replicas is correct, and using only a fraction of messages employed in PBFT (e.g., $20\\%$).","We provide a detailed description of ProBFT's protocol and its analysis."],"url":"http://arxiv.org/abs/2405.04606v1","category":"cs.DC"}
{"created":"2024-05-07 18:21:04","title":"Cue: A Fast and Flexible Photoionization Emulator for Modeling Nebular Emission Powered By Almost Any Ionizing Source","abstract":"The complex physics governing nebular emission in galaxies, particularly in the early universe, often defy simple low-dimensional models. This has proven to be a significant barrier in understanding the (often diverse) ionizing sources powering this emission. We present Cue, a highly flexible tool for interpreting nebular emission across a wide range of abundances and ionizing conditions of galaxies at different redshifts. Unlike typical nebular models used to interpret extragalactic nebular emission, our model does not require a specific ionizing spectrum as a source, instead approximating the ionizing spectrum with a 4-part piece-wise power-law. We train a neural net emulator based on the CLOUDY photoionization modeling code and make self-consistent nebular continuum and line emission predictions. Along with the flexible ionizing spectra, we allow freedom in [O/H], [N/O], [C/O], gas density, and total ionizing photon budget. This flexibility allows us to either marginalize over or directly measure the incident ionizing radiation, thereby directly interrogating the source of the ionizing photons in distant galaxies via their nebular emission. Our emulator demonstrates a high accuracy, with $\\sim$1% uncertainty in predicting the nebular continuum and $\\sim$5% uncertainty in the emission lines. Mock tests suggest Cue is well-calibrated and produces useful constraints on the ionizing spectra when $S/N (\\mathrm{H}_\\alpha) \\gtrsim 10$, and furthermore capable of distinguishing between the ionizing spectra predicted by single and binary stellar models. The compute efficiency of neural networks facilitates future applications of Cue for rapid modeling of the nebular emission in large samples and Monte Carlo sampling techniques.","sentences":["The complex physics governing nebular emission in galaxies, particularly in the early universe, often defy simple low-dimensional models.","This has proven to be a significant barrier in understanding the (often diverse) ionizing sources powering this emission.","We present Cue, a highly flexible tool for interpreting nebular emission across a wide range of abundances and ionizing conditions of galaxies at different redshifts.","Unlike typical nebular models used to interpret extragalactic nebular emission, our model does not require a specific ionizing spectrum as a source, instead approximating the ionizing spectrum with a 4-part piece-wise power-law.","We train a neural net emulator based on the CLOUDY photoionization modeling code and make self-consistent nebular continuum and line emission predictions.","Along with the flexible ionizing spectra, we allow freedom in [O/H], [N/O], [C/O], gas density, and total ionizing photon budget.","This flexibility allows us to either marginalize over or directly measure the incident ionizing radiation, thereby directly interrogating the source of the ionizing photons in distant galaxies via their nebular emission.","Our emulator demonstrates a high accuracy, with $\\sim$1% uncertainty in predicting the nebular continuum and $\\sim$5% uncertainty in the emission lines.","Mock tests suggest Cue is well-calibrated and produces useful constraints on the ionizing spectra when $S/N (\\mathrm{H}_\\alpha) \\gtrsim 10$, and furthermore capable of distinguishing between the ionizing spectra predicted by single and binary stellar models.","The compute efficiency of neural networks facilitates future applications of Cue for rapid modeling of the nebular emission in large samples and Monte Carlo sampling techniques."],"url":"http://arxiv.org/abs/2405.04598v1","category":"astro-ph.GA"}
{"created":"2024-05-07 18:14:05","title":"UltraDark.jl: A Julia package for simulation of cosmological scalar fields","abstract":"UltraDark.jl is a Julia package for the simulation of cosmological scalar fields. Scalar fields are proposed solutions to two of the fundamental questions in cosmology: the nature of dark matter and the universe's initial conditions. Modeling their dynamics requires solving the Gross-Pitaevskii-Poisson equations, which is analytically challenging. This makes simulations essential to understanding the dynamics of cosmological scalar fields. UltraDark.jl is an open, performant and user friendly option for solving these equations numerically.","sentences":["UltraDark.jl is a Julia package for the simulation of cosmological scalar fields.","Scalar fields are proposed solutions to two of the fundamental questions in cosmology: the nature of dark matter and the universe's initial conditions.","Modeling their dynamics requires solving the Gross-Pitaevskii-Poisson equations, which is analytically challenging.","This makes simulations essential to understanding the dynamics of cosmological scalar fields.","UltraDark.jl is an open, performant and user friendly option for solving these equations numerically."],"url":"http://arxiv.org/abs/2405.04593v1","category":"astro-ph.CO"}
{"created":"2024-05-07 18:06:40","title":"A Novel Wide-Area Multiobject Detection System with High-Probability Region Searching","abstract":"In recent years, wide-area visual surveillance systems have been widely applied in various industrial and transportation scenarios. These systems, however, face significant challenges when implementing multi-object detection due to conflicts arising from the need for high-resolution imaging, efficient object searching, and accurate localization. To address these challenges, this paper presents a hybrid system that incorporates a wide-angle camera, a high-speed search camera, and a galvano-mirror. In this system, the wide-angle camera offers panoramic images as prior information, which helps the search camera capture detailed images of the targeted objects. This integrated approach enhances the overall efficiency and effectiveness of wide-area visual detection systems. Specifically, in this study, we introduce a wide-angle camera-based method to generate a panoramic probability map (PPM) for estimating high-probability regions of target object presence. Then, we propose a probability searching module that uses the PPM-generated prior information to dynamically adjust the sampling range and refine target coordinates based on uncertainty variance computed by the object detector. Finally, the integration of PPM and the probability searching module yields an efficient hybrid vision system capable of achieving 120 fps multi-object search and detection. Extensive experiments are conducted to verify the system's effectiveness and robustness.","sentences":["In recent years, wide-area visual surveillance systems have been widely applied in various industrial and transportation scenarios.","These systems, however, face significant challenges when implementing multi-object detection due to conflicts arising from the need for high-resolution imaging, efficient object searching, and accurate localization.","To address these challenges, this paper presents a hybrid system that incorporates a wide-angle camera, a high-speed search camera, and a galvano-mirror.","In this system, the wide-angle camera offers panoramic images as prior information, which helps the search camera capture detailed images of the targeted objects.","This integrated approach enhances the overall efficiency and effectiveness of wide-area visual detection systems.","Specifically, in this study, we introduce a wide-angle camera-based method to generate a panoramic probability map (PPM) for estimating high-probability regions of target object presence.","Then, we propose a probability searching module that uses the PPM-generated prior information to dynamically adjust the sampling range and refine target coordinates based on uncertainty variance computed by the object detector.","Finally, the integration of PPM and the probability searching module yields an efficient hybrid vision system capable of achieving 120 fps multi-object search and detection.","Extensive experiments are conducted to verify the system's effectiveness and robustness."],"url":"http://arxiv.org/abs/2405.04589v1","category":"cs.CV"}
{"created":"2024-05-07 18:00:11","title":"First Constraints on the ISM Conditions of a Low Mass, Highly Obscured z=4.27 Main Sequence Galaxy","abstract":"We present the molecular gas content and ISM conditions of MACSJ0717 Az9, a strong gravitationally lensed $z=4.273$, $M_{*} \\simeq 2\\times10^9M_{\\odot}$ star-forming galaxy with an unusually high ($\\sim 80\\%$) obscured star formation fraction. We detect CO(4-3) in two independent lensed images, as well as [N II]205$\\mu$m, with ALMA. We derive a molecular gas mass of log$_{10}[M_{H_{2}} (M_{\\odot})] = 9.77$ making it moderately deficient in molecular gas compared to the lower redshift gas fraction scaling relation. Leveraging photodissociation region (PDR) models, we combine our CO(4-3) measurements with existing measurements of the [C II] 158$\\mu$m line and total infrared luminosity to model the PDR conditions. We find PDR conditions similar to local star-forming galaxies, with a mean hydrogen density log$_{10}$[$n_H$ $cm^{-3}$] = $4.80\\pm0.39$ and a mean radiation field strength log$_{10}$[G$_0$ Habing] = $2.83\\pm0.26$. Based on Band 3 continuum data, we derive an upper limit on the intrinsic dust mass of log$_{10}[M_{\\rm dust} (M_{\\odot})] < 7.73$, consistent with existing estimates. We use the 3D tilted-ring model fitting code 3D-Barolo to determine the kinematic properties of the CO(4-3) emitting gas. We find that it is rotationally dominated, with a $V/\\sigma=4.6 \\pm 1.7$, consistent with the kinematics of the [C II]. With PDR conditions remarkably similar to normal dusty star-forming galaxies at z ~ 0.2 and a stable molecular disk, our observations of Az9 suggest that the dust-obscured phase for a low-mass galaxy at z$\\sim$4 is relatively long. Thus, Az9 may be representative of a more widespread population that has been missed due to insufficiently deep existing millimeter surveys.","sentences":["We present the molecular gas content and ISM conditions of MACSJ0717 Az9, a strong gravitationally lensed $z=4.273$, $M_{*} \\simeq 2\\times10^9M_{\\odot}$ star-forming galaxy with an unusually high ($\\sim 80\\%$) obscured star formation fraction.","We detect CO(4-3) in two independent lensed images, as well as [N II]205$\\mu$m, with ALMA.","We derive a molecular gas mass of log$_{10}[M_{H_{2}} (M_{\\odot})]","= 9.77$ making it moderately deficient in molecular gas compared to the lower redshift gas fraction scaling relation.","Leveraging photodissociation region (PDR) models, we combine our CO(4-3) measurements with existing measurements of the [C II] 158$\\mu$m line and total infrared luminosity to model the PDR conditions.","We find PDR conditions similar to local star-forming galaxies, with a mean hydrogen density log$_{10}$[$n_H$ $cm^{-3}$] = $4.80\\pm0.39$ and a mean radiation field strength log$_{10}$[G$_0$ Habing] = $2.83\\pm0.26$. Based on Band 3 continuum data, we derive an upper limit on the intrinsic dust mass of log$_{10}[M_{\\rm dust} (M_{\\odot})]","< 7.73$, consistent with existing estimates.","We use the 3D tilted-ring model fitting code 3D-Barolo to determine the kinematic properties of the CO(4-3) emitting gas.","We find that it is rotationally dominated, with a $V/\\sigma=4.6 \\pm 1.7$, consistent with the kinematics of the [C II].","With PDR conditions remarkably similar to normal dusty star-forming galaxies at z ~ 0.2 and a stable molecular disk, our observations of Az9 suggest that the dust-obscured phase for a low-mass galaxy at z$\\sim$4 is relatively long.","Thus, Az9 may be representative of a more widespread population that has been missed due to insufficiently deep existing millimeter surveys."],"url":"http://arxiv.org/abs/2405.04582v1","category":"astro-ph.GA"}
{"created":"2024-05-07 18:00:06","title":"Spin-Hall effect in topological materials: Evaluating the proper spin current in systems with arbitrary degeneracies","abstract":"The spin-Hall effect underpins some of the most active topics in modern physics, including spin torques and the inverse spin-Hall effect, yet it lacks a proper theoretical description. This makes it difficult to differentiate the SHE from other mechanisms, as well as differentiate band structure and disorder contributions. Here, by exploiting recent analytical breakthroughs in the understanding of the intrinsic spin-Hall effect, we devise a density functional theory method for evaluating the conserved (proper) spin current in a generic system. Spin non-conservation makes the conventional spin current physically meaningless, while the conserved spin current has been challenging to evaluate since it involves the position operator between Bloch bands. The novel method we introduce here can handle band structures with arbitrary degeneracies and incorporates all matrix elements of the position operator, including the notoriously challenging diagonal elements, which are associated with Fermi surface, group velocity, and dipolar effects but often diverge if not treated correctly. We apply this method to the most important classes of spin-Hall materials: topological insulators, 2D quantum spin-Hall insulators, non-collinear antiferromagnets, and strongly spin-orbit coupled metals. We demonstrate that the torque dipole systematically suppresses contributions to the conventional spin current such that, the proper spin current is generally smaller in magnitude and often has a different sign. Remarkably, its energy-dependence is relatively flat and featureless, and its magnitude is comparable in all classes of materials studied. These findings will guide the experiment in characterizing charge-to-spin interconversion in spintronic and orbitronic devices. We also discuss briefly a potential generalisation of the method to calculate extrinsic spin currents generated by disorder scattering.","sentences":["The spin-Hall effect underpins some of the most active topics in modern physics, including spin torques and the inverse spin-Hall effect, yet it lacks a proper theoretical description.","This makes it difficult to differentiate the SHE from other mechanisms, as well as differentiate band structure and disorder contributions.","Here, by exploiting recent analytical breakthroughs in the understanding of the intrinsic spin-Hall effect, we devise a density functional theory method for evaluating the conserved (proper) spin current in a generic system.","Spin non-conservation makes the conventional spin current physically meaningless, while the conserved spin current has been challenging to evaluate since it involves the position operator between Bloch bands.","The novel method we introduce here can handle band structures with arbitrary degeneracies and incorporates all matrix elements of the position operator, including the notoriously challenging diagonal elements, which are associated with Fermi surface, group velocity, and dipolar effects but often diverge if not treated correctly.","We apply this method to the most important classes of spin-Hall materials: topological insulators, 2D quantum spin-Hall insulators, non-collinear antiferromagnets, and strongly spin-orbit coupled metals.","We demonstrate that the torque dipole systematically suppresses contributions to the conventional spin current such that, the proper spin current is generally smaller in magnitude and often has a different sign.","Remarkably, its energy-dependence is relatively flat and featureless, and its magnitude is comparable in all classes of materials studied.","These findings will guide the experiment in characterizing charge-to-spin interconversion in spintronic and orbitronic devices.","We also discuss briefly a potential generalisation of the method to calculate extrinsic spin currents generated by disorder scattering."],"url":"http://arxiv.org/abs/2405.04581v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 18:00:01","title":"Null states and time evolution in a toy model of black hole dynamics","abstract":"Spacetime wormholes can provide non-perturbative contributions to the gravitational path integral that make the actual number of states $e^S$ in a gravitational system much smaller than the number of states $e^{S_{\\mathrm{p}}}$ predicted by perturbative semiclassical effective field theory. The effects on the physics of the system are naturally profound in contexts in which the perturbative description actively involves $N = O(e^S)$ of the possible $e^{S_{\\mathrm{p}}}$ perturbative states; e.g., in late stages of black hole evaporation. Such contexts are typically associated with the existence of non-trivial quantum extremal surfaces. However, by forcing a simple topological gravity model to evolve in time, we find that such effects can also have large impact for $N\\ll e^S$ (in which case no quantum extremal surfaces can arise). In particular, even for small $N$, the insertion of generic operators into the path integral can cause the non-perturbative time evolution to differ dramatically from perturbative expectations. On the other hand, this discrepancy is small for the special case where the inserted operators are non-trivial only in a subspace of dimension $D \\ll e^S$. We thus study this latter case in detail. We also discuss potential implications for more realistic gravitational systems.","sentences":["Spacetime wormholes can provide non-perturbative contributions to the gravitational path integral that make the actual number of states $e^S$ in a gravitational system much smaller than the number of states $e^{S_{\\mathrm{p}}}$ predicted by perturbative semiclassical effective field theory.","The effects on the physics of the system are naturally profound in contexts in which the perturbative description actively involves $N = O(e^S)$ of the possible $e^{S_{\\mathrm{p}}}$ perturbative states; e.g., in late stages of black hole evaporation.","Such contexts are typically associated with the existence of non-trivial quantum extremal surfaces.","However, by forcing a simple topological gravity model to evolve in time, we find that such effects can also have large impact for $N\\ll e^S$ (in which case no quantum extremal surfaces can arise).","In particular, even for small $N$, the insertion of generic operators into the path integral can cause the non-perturbative time evolution to differ dramatically from perturbative expectations.","On the other hand, this discrepancy is small for the special case where the inserted operators are non-trivial only in a subspace of dimension $D \\ll e^S$.","We thus study this latter case in detail.","We also discuss potential implications for more realistic gravitational systems."],"url":"http://arxiv.org/abs/2405.04571v1","category":"hep-th"}
{"created":"2024-05-07 18:00:00","title":"Relic Neutrino Background from Cosmic-Ray Reservoirs","abstract":"We compute the flux of relic neutrino background (R$\\nu$B) up-scattered by ultra-high-energy (UHE) cosmic rays (CRs) in clusters that act as CR-reservoirs. The long trapping times of UHECRs make this flux larger than that of R$\\nu$B up-scattered by UHECRs on their way to Earth, which we also compute. We find that IceCube excludes R$\\nu$B weighted overdensities larger than $10^{10}$ in clusters, and that PUEO, RNO-G, GRAND and IceCube-Gen2 will test values down to $10^{8}$. Our treatment incorporates the momentum transfer dependence of the neutrino-nucleus cross section, deep inelastic scattering, a mixed UHECR composition, and flavour information on the up-scattered R$\\nu$B fluxes for both cases of neutrino mass spectrum with normal and inverted ordering, providing new handles to possibly disentangle the up-scattered R$\\nu$B from cosmogenic neutrinos.","sentences":["We compute the flux of relic neutrino background (R$\\nu$B) up-scattered by ultra-high-energy (UHE) cosmic rays (CRs) in clusters that act as CR-reservoirs.","The long trapping times of UHECRs make this flux larger than that of R$\\nu$B up-scattered by UHECRs on their way to Earth, which we also compute.","We find that IceCube excludes R$\\nu$B weighted overdensities larger than $10^{10}$ in clusters, and that PUEO, RNO-G, GRAND and IceCube-Gen2 will test values down to $10^{8}$. Our treatment incorporates the momentum transfer dependence of the neutrino-nucleus cross section, deep inelastic scattering, a mixed UHECR composition, and flavour information on the up-scattered R$\\nu$B fluxes for both cases of neutrino mass spectrum with normal and inverted ordering, providing new handles to possibly disentangle the up-scattered R$\\nu$B from cosmogenic neutrinos."],"url":"http://arxiv.org/abs/2405.04568v1","category":"hep-ph"}
{"created":"2024-05-07 17:57:31","title":"PoW Security-Latency under Random Delays and the Effect of Transaction Fees","abstract":"Safety guarantees and security-latency problem of Nakamoto consensus have been extensively studied in the last decade with a bounded delay model. Recent studies have shown that PoW protocol is secure under random delay models as well. In this paper, we analyze the security-latency problem, i.e., how secure a block is, after it becomes k-deep in the blockchain, under general random delay distributions. We provide tight and explicit bounds which only require determining the distribution of the number of Poisson arrivals during the random delay. We further consider potential effects of recent Bitcoin halving on the security-latency problem by extending our results.","sentences":["Safety guarantees and security-latency problem of Nakamoto consensus have been extensively studied in the last decade with a bounded delay model.","Recent studies have shown that PoW protocol is secure under random delay models as well.","In this paper, we analyze the security-latency problem, i.e., how secure a block is, after it becomes k-deep in the blockchain, under general random delay distributions.","We provide tight and explicit bounds which only require determining the distribution of the number of Poisson arrivals during the random delay.","We further consider potential effects of recent Bitcoin halving on the security-latency problem by extending our results."],"url":"http://arxiv.org/abs/2405.04526v1","category":"cs.CR"}
{"created":"2024-05-07 17:56:06","title":"The ramification tree and almost Dedekind domains of prescribed SP-rank","abstract":"Given a valuation $v$ with quotient field $K$ and a sequence $\\mathcal{K} :K_0\\subseteq K_1\\subseteq\\cdots$ of finite extensions of $K$, we construct a weighted tree $\\mathcal{T}(v,\\mathcal{K})$ encoding information about the ramification of $v$ in the extensions $K_i$; conversely, we show that a weighted tree $\\mathcal{T}$ can be expressed as $\\mathcal{T}(v,\\mathcal{K})$ under some mild hypothesis on $v$ or on $\\mathcal{T}$. We use this construction to construct, for every countable successor ordinal number $\\alpha$, an almost Dedekind domain $D$, integral over $V$ (the valuation domain of $v$) whose SP-rank is $\\alpha$. Subsequently, we extend this result to countable limit ordinal numbers by considering integral extensions of Dedekind domains with countably many maximal ideals.","sentences":["Given a valuation $v$ with quotient field $K$ and a sequence $\\mathcal{K} :K_0\\subseteq K_1\\subseteq\\cdots$ of finite extensions of $K$, we construct a weighted tree $\\mathcal{T}(v,\\mathcal{K})$ encoding information about the ramification of $v$ in the extensions $K_i$; conversely, we show that a weighted tree $\\mathcal{T}$ can be expressed as $\\mathcal{T}(v,\\mathcal{K})$ under some mild hypothesis on $v$ or on $\\mathcal{T}$. We use this construction to construct, for every countable successor ordinal number $\\alpha$, an almost Dedekind domain $D$, integral over $V$ (the valuation domain of $v$) whose SP-rank is $\\alpha$. Subsequently, we extend this result to countable limit ordinal numbers by considering integral extensions of Dedekind domains with countably many maximal ideals."],"url":"http://arxiv.org/abs/2405.04523v1","category":"math.AC"}
{"created":"2024-05-07 17:56:04","title":"Astrometric Redshifts of Supernovae","abstract":"Differential Chromatic Refraction (DCR) is caused by the wavelength dependence of our atmosphere's refractive index, which shifts the apparent positions of stars and galaxies and distorts their shapes depending on their spectral energy distributions (SEDs). While this effect is typically mitigated and corrected for in imaging observations, we investigate how DCR can instead be used to our advantage to infer the redshifts of supernovae from multi-band, time-series imaging data. We simulate Type Ia supernovae (SNe Ia) in the proposed Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) Deep Drilling Field (DDF), and evaluate astrometric redshifts. We find that the redshift accuracy improves dramatically with the statistical quality of the astrometric measurements as well as with the accuracy of the astrometric solution. For a conservative choice of a 5-mas systematic uncertainty floor, we find that our redshift estimation is accurate at $z < 0.6$. We then combine our astrometric redshifts with both host galaxy photometric redshifts and supernovae photometric (light-curve) redshifts and show that this considerably improves the overall redshift estimates. These astrometric redshifts will be valuable especially since Rubin will discover a vast number of supernovae for which we will not be able to obtain spectroscopic redshifts.","sentences":["Differential Chromatic Refraction (DCR) is caused by the wavelength dependence of our atmosphere's refractive index, which shifts the apparent positions of stars and galaxies and distorts their shapes depending on their spectral energy distributions (SEDs).","While this effect is typically mitigated and corrected for in imaging observations, we investigate how DCR can instead be used to our advantage to infer the redshifts of supernovae from multi-band, time-series imaging data.","We simulate Type Ia supernovae (SNe Ia) in the proposed Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST)","Deep Drilling Field (DDF), and evaluate astrometric redshifts.","We find that the redshift accuracy improves dramatically with the statistical quality of the astrometric measurements as well as with the accuracy of the astrometric solution.","For a conservative choice of a 5-mas systematic uncertainty floor, we find that our redshift estimation is accurate at $z < 0.6$.","We then combine our astrometric redshifts with both host galaxy photometric redshifts and supernovae photometric (light-curve) redshifts and show that this considerably improves the overall redshift estimates.","These astrometric redshifts will be valuable especially since Rubin will discover a vast number of supernovae for which we will not be able to obtain spectroscopic redshifts."],"url":"http://arxiv.org/abs/2405.04522v1","category":"astro-ph.CO"}
{"created":"2024-05-07 17:51:10","title":"Local Advice and Local Decompression","abstract":"Algorithms with advice have received ample attention in the distributed and online settings, and they have recently proven useful also in dynamic settings. In this work we study local computation with advice: the goal is to solve a graph problem $\\Pi$ with a distributed algorithm in $f(\\Delta)$ communication rounds, for some function $f$ that only depends on the maximum degree $\\Delta$ of the graph, and the key question is how many bits of advice per node are needed. Our main results are:   - Any locally checkable labeling problem can be solved in graphs with sub-exponential growth with only $1$ bit of advice per node. Moreover, we can make the set of nodes that carry advice bits arbitrarily sparse, that is, we can make arbitrarily small the ratio between nodes carrying a 1 and the nodes carrying a 0. - The assumption of sub-exponential growth is necessary: assuming the Exponential-Time Hypothesis, there are LCLs that cannot be solved in general with any constant number of bits per node. - In any graph we can find an almost-balanced orientation (indegrees and outdegrees differ by at most one) with $1$ bit of advice per node, and again we can make the advice arbitrarily sparse. - As a corollary, we can also compress an arbitrary subset of edges so that a node of degree $d$ stores only $d/2 + 2$ bits, and we can decompress it locally, in $f(\\Delta)$ rounds. - In any graph of maximum degree $\\Delta$, we can find a $\\Delta$-coloring (if it exists) with $1$ bit of advice per node, and again, we can make the advice arbitrarily sparse. - In any $3$-colorable graph, we can find a $3$-coloring with $1$ bit of advice per node. Here, it remains open whether we can make the advice arbitrarily sparse.   Our work shows that for many problems the key threshold is not whether we can achieve, say, $1$ bit of advice per node, but whether we can make the advice arbitrarily sparse.","sentences":["Algorithms with advice have received ample attention in the distributed and online settings, and they have recently proven useful also in dynamic settings.","In this work we study local computation with advice: the goal is to solve a graph problem $\\Pi$ with a distributed algorithm in $f(\\Delta)$ communication rounds, for some function $f$ that only depends on the maximum degree $\\Delta$ of the graph, and the key question is how many bits of advice per node are needed.","Our main results are:   - Any locally checkable labeling problem can be solved in graphs with sub-exponential growth with only $1$ bit of advice per node.","Moreover, we can make the set of nodes that carry advice bits arbitrarily sparse, that is, we can make arbitrarily small the ratio between nodes carrying a 1 and the nodes carrying a 0. -","The assumption of sub-exponential growth is necessary: assuming the Exponential-Time Hypothesis, there are LCLs that cannot be solved in general with any constant number of bits per node.","- In any graph we can find an almost-balanced orientation (indegrees and outdegrees differ by at most one) with $1$ bit of advice per node, and again we can make the advice arbitrarily sparse.","- As a corollary, we can also compress an arbitrary subset of edges so that a node of degree $d$ stores only $d/2 + 2$ bits, and we can decompress it locally, in $f(\\Delta)$ rounds.","-","In any graph of maximum degree $\\Delta$, we can find a $\\Delta$-coloring (if it exists) with $1$ bit of advice per node, and again, we can make the advice arbitrarily sparse.","- In any $3$-colorable graph, we can find a $3$-coloring with $1$ bit of advice per node.","Here, it remains open whether we can make the advice arbitrarily sparse.   ","Our work shows that for many problems the key threshold is not whether we can achieve, say, $1$ bit of advice per node, but whether we can make the advice arbitrarily sparse."],"url":"http://arxiv.org/abs/2405.04519v1","category":"cs.DC"}
{"created":"2024-05-07 17:24:20","title":"Diatomic Molecules in deSitter and Anti-deSitter Spaces","abstract":"The Schr\\\"odinger equation for diatomic molecules in deSitter and anti-deSitter spaces is studied using the extended uncertainty principle formulation. The equations are solved by the Nikiforov-Uvarov method for both the Kratzer potential and the pseudoharmonic oscillator. The energy eigenvalues of the system have been derived analytically, and the exact expressions of the eigenfunctions are provided in terms of Romanovski and Jacobi polynomials. The impact of the spatial deformation parameter on the bound states is also examined, with experimental results used to establish an upper limit for this parameter.","sentences":["The Schr\\\"odinger equation for diatomic molecules in deSitter and anti-deSitter spaces is studied using the extended uncertainty principle formulation.","The equations are solved by the Nikiforov-Uvarov method for both the Kratzer potential and the pseudoharmonic oscillator.","The energy eigenvalues of the system have been derived analytically, and the exact expressions of the eigenfunctions are provided in terms of Romanovski and Jacobi polynomials.","The impact of the spatial deformation parameter on the bound states is also examined, with experimental results used to establish an upper limit for this parameter."],"url":"http://arxiv.org/abs/2405.04502v1","category":"math-ph"}
{"created":"2024-05-07 17:18:05","title":"The Chemical Composition of Ryugu: Prospects as a Reference Material for Solar System Composition","abstract":"The Hayabusa 2 spacecraft sampled approximately 5.4 g of asteroid material from the Cb-type asteroid Ryugu. Initial analysis of the Ryugu materials revealed a mineralogical, chemical, and isotopic kinship to the CI chondrites. The pristine nature of Ryugu makes the returned samples ideal for constraining the composition of the Solar System. However, some elements (e.g., P, Ca, Mn, and rare earth elements) show large relative dispersions compared to the other elements in the returned materials studied so far, most likely due to the presence of aqueously formed secondary minerals (e.g., carbonates, phosphates) in Ryugu. Therefore, the estimation of the Solar System composition using currently available Ryugu data is challenging due to the so-called nugget effect of carbonates, phosphates, and possibly other accessory minerals. The nugget effect can be mitigated by analyzing a homogenized, relatively large amount of sample. We estimate that for approximately 0.1 g of Ryugu sample, the dispersion (2SD) of the bulk Mn/Cr and Rb/Sr ratios are +/-13% and +/-15%, respectively, while they will be improved to be better than +/-5% for approximately 1 g of homogenized Ryugu sample. To further constrain the Solar System composition and to evaluate if previous estimates based on CI chondrites stored in museums for decades to centuries are reliable, it is strongly recommended to determine the chemical and isotopic compositions of Ryugu using a homogenized sample prepared from relatively large (approx. 1 g) returned material. Determining Ryugu reference compositions will be used by multidisciplinary communities, including Earth and planetary sciences, astronomy, physics, and chemistry.","sentences":["The Hayabusa 2 spacecraft sampled approximately 5.4 g of asteroid material from the Cb-type asteroid Ryugu.","Initial analysis of the Ryugu materials revealed a mineralogical, chemical, and isotopic kinship to the CI chondrites.","The pristine nature of Ryugu makes the returned samples ideal for constraining the composition of the Solar System.","However, some elements (e.g., P, Ca, Mn, and rare earth elements) show large relative dispersions compared to the other elements in the returned materials studied so far, most likely due to the presence of aqueously formed secondary minerals (e.g., carbonates, phosphates) in Ryugu.","Therefore, the estimation of the Solar System composition using currently available Ryugu data is challenging due to the so-called nugget effect of carbonates, phosphates, and possibly other accessory minerals.","The nugget effect can be mitigated by analyzing a homogenized, relatively large amount of sample.","We estimate that for approximately 0.1 g of Ryugu sample, the dispersion (2SD) of the bulk Mn/Cr and Rb/Sr ratios are +/-13% and +/-15%, respectively, while they will be improved to be better than +/-5% for approximately 1 g of homogenized Ryugu sample.","To further constrain the Solar System composition and to evaluate if previous estimates based on CI chondrites stored in museums for decades to centuries are reliable, it is strongly recommended to determine the chemical and isotopic compositions of Ryugu using a homogenized sample prepared from relatively large (approx.","1 g) returned material.","Determining Ryugu reference compositions will be used by multidisciplinary communities, including Earth and planetary sciences, astronomy, physics, and chemistry."],"url":"http://arxiv.org/abs/2405.04500v2","category":"astro-ph.EP"}
{"created":"2024-05-07 17:15:58","title":"Benchmarking Optimizers for Qumode State Preparation with Variational Quantum Algorithms","abstract":"Quantum state preparation involves preparing a target state from an initial system, a process integral to applications such as quantum machine learning and solving systems of linear equations. Recently, there has been a growing interest in qumodes due to advancements in the field and their potential applications. However there is a notable gap in the literature specifically addressing this area. This paper aims to bridge this gap by providing performance benchmarks of various optimizers used in state preparation with Variational Quantum Algorithms. We conducted extensive testing across multiple scenarios, including different target states, both ideal and sampling simulations, and varying numbers of basis gate layers. Our evaluations offer insights into the complexity of learning each type of target state and demonstrate that some optimizers perform better than others in this context. Notably, the Powell optimizer was found to be exceptionally robust against sampling errors, making it a preferred choice in scenarios prone to such inaccuracies. Additionally, the Simultaneous Perturbation Stochastic Approximation optimizer was distinguished for its efficiency and ability to handle increased parameter dimensionality effectively.","sentences":["Quantum state preparation involves preparing a target state from an initial system, a process integral to applications such as quantum machine learning and solving systems of linear equations.","Recently, there has been a growing interest in qumodes due to advancements in the field and their potential applications.","However there is a notable gap in the literature specifically addressing this area.","This paper aims to bridge this gap by providing performance benchmarks of various optimizers used in state preparation with Variational Quantum Algorithms.","We conducted extensive testing across multiple scenarios, including different target states, both ideal and sampling simulations, and varying numbers of basis gate layers.","Our evaluations offer insights into the complexity of learning each type of target state and demonstrate that some optimizers perform better than others in this context.","Notably, the Powell optimizer was found to be exceptionally robust against sampling errors, making it a preferred choice in scenarios prone to such inaccuracies.","Additionally, the Simultaneous Perturbation Stochastic Approximation optimizer was distinguished for its efficiency and ability to handle increased parameter dimensionality effectively."],"url":"http://arxiv.org/abs/2405.04499v1","category":"quant-ph"}
{"created":"2024-05-07 17:06:59","title":"Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing","abstract":"Existing diffusion-based video editing methods have achieved impressive results in motion editing. Most of the existing methods focus on the motion alignment between the edited video and the reference video. However, these methods do not constrain the background and object content of the video to remain unchanged, which makes it possible for users to generate unexpected videos. In this paper, we propose a one-shot video motion editing method called Edit-Your-Motion that requires only a single text-video pair for training. Specifically, we design the Detailed Prompt-Guided Learning Strategy (DPL) to decouple spatio-temporal features in space-time diffusion models. DPL separates learning object content and motion into two training stages. In the first training stage, we focus on learning the spatial features (the features of object content) and breaking down the temporal relationships in the video frames by shuffling them. We further propose Recurrent-Causal Attention (RC-Attn) to learn the consistent content features of the object from unordered video frames. In the second training stage, we restore the temporal relationship in video frames to learn the temporal feature (the features of the background and object's motion). We also adopt the Noise Constraint Loss to smooth out inter-frame differences. Finally, in the inference stage, we inject the content features of the source object into the editing branch through a two-branch structure (editing branch and reconstruction branch). With Edit-Your-Motion, users can edit the motion of objects in the source video to generate more exciting and diverse videos. Comprehensive qualitative experiments, quantitative experiments and user preference studies demonstrate that Edit-Your-Motion performs better than other methods.","sentences":["Existing diffusion-based video editing methods have achieved impressive results in motion editing.","Most of the existing methods focus on the motion alignment between the edited video and the reference video.","However, these methods do not constrain the background and object content of the video to remain unchanged, which makes it possible for users to generate unexpected videos.","In this paper, we propose a one-shot video motion editing method called Edit-Your-Motion that requires only a single text-video pair for training.","Specifically, we design the Detailed Prompt-Guided Learning Strategy (DPL) to decouple spatio-temporal features in space-time diffusion models.","DPL separates learning object content and motion into two training stages.","In the first training stage, we focus on learning the spatial features (the features of object content) and breaking down the temporal relationships in the video frames by shuffling them.","We further propose Recurrent-Causal Attention (RC-Attn) to learn the consistent content features of the object from unordered video frames.","In the second training stage, we restore the temporal relationship in video frames to learn the temporal feature (the features of the background and object's motion).","We also adopt the Noise Constraint Loss to smooth out inter-frame differences.","Finally, in the inference stage, we inject the content features of the source object into the editing branch through a two-branch structure (editing branch and reconstruction branch).","With Edit-Your-Motion, users can edit the motion of objects in the source video to generate more exciting and diverse videos.","Comprehensive qualitative experiments, quantitative experiments and user preference studies demonstrate that Edit-Your-Motion performs better than other methods."],"url":"http://arxiv.org/abs/2405.04496v1","category":"cs.CV"}
{"created":"2024-05-07 16:49:01","title":"CloudDiff: Super-resolution ensemble retrieval of cloud properties for all day using the generative diffusion model","abstract":"Clouds play a crucial role in the Earth's water and energy cycles, underscoring the importance of high spatiotemporal resolution data on cloud phase and properties for accurate numerical modeling and weather prediction. Currently, Moderate Resolution Imaging Spectroradiometer (MODIS) provides cloud products with a spatial resolution of 1 km. However, these products suffer from a lengthy revisit cycle. This study develops a generative diffusion model (donated as CloudDiff) for super-resolution retrieval of high spatiotemporal cloud phase and properties, applicable both day and night. Leveraging 2 km spatial resolution Himawari-8 Advanced Himawari Imager (AHI) thermal infrared (TIR) radiances and viewing geometry as condition, alongside daytime MODIS products as targets, the model can generate cloud phase (CLP), cloud top height (CTH), cloud optical thickness (COT), and cloud effective radius (CER) at 1 km spatial resolution and 10-minute temporal resolution. The conditional diffusion model can generate sharper images and capture finer local features than deterministic super-resolution approaches. It draws multiple samples based on the underlying probability distribution, enabling retrieval uncertainty assessment. Evaluations show agreement between cloud phase and properties derived from the CloudDiff and MODIS cloud products. The ensemble mean is found to enhance retrieval accuracy and credibility, outperforming the deterministic model.","sentences":["Clouds play a crucial role in the Earth's water and energy cycles, underscoring the importance of high spatiotemporal resolution data on cloud phase and properties for accurate numerical modeling and weather prediction.","Currently, Moderate Resolution Imaging Spectroradiometer (MODIS) provides cloud products with a spatial resolution of 1 km.","However, these products suffer from a lengthy revisit cycle.","This study develops a generative diffusion model (donated as CloudDiff) for super-resolution retrieval of high spatiotemporal cloud phase and properties, applicable both day and night.","Leveraging 2 km spatial resolution Himawari-8 Advanced Himawari Imager (AHI) thermal infrared (TIR) radiances and viewing geometry as condition, alongside daytime MODIS products as targets, the model can generate cloud phase (CLP), cloud top height (CTH), cloud optical thickness (COT), and cloud effective radius (CER) at 1 km spatial resolution and 10-minute temporal resolution.","The conditional diffusion model can generate sharper images and capture finer local features than deterministic super-resolution approaches.","It draws multiple samples based on the underlying probability distribution, enabling retrieval uncertainty assessment.","Evaluations show agreement between cloud phase and properties derived from the CloudDiff and MODIS cloud products.","The ensemble mean is found to enhance retrieval accuracy and credibility, outperforming the deterministic model."],"url":"http://arxiv.org/abs/2405.04483v1","category":"physics.ao-ph"}
{"created":"2024-05-07 16:42:48","title":"Designing an Objective-Driven Test Method for the Comparative Performance Evaluation of Commercial DTI Solutions for Counter UAS systems","abstract":"Unmanned Aerial Systems (UASs) or drones become more and more commercially available and cheap. There has been much emphasis on developing and deploying Counter-UAS systems (UASs) with Detection Tracking and Identification (DTI) solutions. However, the capabilities of these systems are hard to benchmark. Performance claims of these systems are currently not supported by evidence. In addition, no standard test methodologies are available for these DTI systems and different test methodologies make comparison of these systems hard or impossible. We report on the definition, development and verification of an objective-driven test method and corresponding comparative performance evaluation for commercial DTI solutions for C-UASs. The developed methodology is based on end-user scenarios that are operationally relevant. The test methodology is based on a generic DTI system lay-out and is detailed towards detection, tracking and identification, taking into account contextual information and end-user input. The comparative performance evaluation is developed to enable the use of the methodology in a relevant environment, thereby taking into account any potential environmental aspect that might influence DTI system performance. Validation of the work in a relevant environment has been done in three operational trials. The operational trial results show that the method allows for performance evaluation at component level (i.e., detection, tracking or identification component) and at system level (combinations of these components and integrated DTI system of system solutions).","sentences":["Unmanned Aerial Systems (UASs) or drones become more and more commercially available and cheap.","There has been much emphasis on developing and deploying Counter-UAS systems (UASs) with Detection Tracking and Identification (DTI) solutions.","However, the capabilities of these systems are hard to benchmark.","Performance claims of these systems are currently not supported by evidence.","In addition, no standard test methodologies are available for these DTI systems and different test methodologies make comparison of these systems hard or impossible.","We report on the definition, development and verification of an objective-driven test method and corresponding comparative performance evaluation for commercial DTI solutions for C-UASs.","The developed methodology is based on end-user scenarios that are operationally relevant.","The test methodology is based on a generic DTI system lay-out and is detailed towards detection, tracking and identification, taking into account contextual information and end-user input.","The comparative performance evaluation is developed to enable the use of the methodology in a relevant environment, thereby taking into account any potential environmental aspect that might influence DTI system performance.","Validation of the work in a relevant environment has been done in three operational trials.","The operational trial results show that the method allows for performance evaluation at component level (i.e., detection, tracking or identification component) and at system level (combinations of these components and integrated DTI system of system solutions)."],"url":"http://arxiv.org/abs/2405.04477v1","category":"cs.SE"}
{"created":"2024-05-07 16:41:36","title":"Bayesian Copula Density Estimation Using Bernstein Yett-Uniform Priors","abstract":"Probability density estimation is a central task in statistics. Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions. We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution. The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas. Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied. The methodology is illustrated by means of simulated and real data.","sentences":["Probability density estimation is a central task in statistics.","Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution.","Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions.","We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution.","The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas.","Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied.","The methodology is illustrated by means of simulated and real data."],"url":"http://arxiv.org/abs/2405.04475v1","category":"stat.ME"}
{"created":"2024-05-07 16:36:28","title":"Neural Network Quantum States for the Interacting Hofstadter Model with Higher Local Occupations and Long-Range Interactions","abstract":"Due to their immense representative power, neural network quantum states (NQS) have gained significant interest in current research. In recent advances in the field of NQS, it has been demonstrated that this approach can compete with state-of-the-art numerical techniques, making NQS a compelling alternative, in particular for the simulation of large, two-dimensional quantum systems. In this study, we show that recurrent neural network (RNN) wave functions can be employed to study systems relevant to current research in quantum many-body physics. Specifically, we employ a 2D tensorized gated RNN to explore the bosonic Hofstadter model with a variable local Hilbert space cut-off and long-range interactions. At first, we benchmark the RNN-NQS for the Hofstadter-Bose-Hubbard (HBH) Hamiltonian on a square lattice. We find that this method is, despite the complexity of the wave function, capable of efficiently identifying and representing most ground state properties. Afterwards, we apply the method to an even more challenging model for current methods, namely the Hofstadter model with long-range interactions. This model describes Rydberg-dressed atoms on a lattice subject to a synthetic magnetic field. We study systems of size up to $12 \\times 12$ sites and identify three different regimes by tuning the interaction range and the filling fraction $\\nu$. In addition to phases known from the HBH model at short-ranged interaction, we observe bubble crystals and Wigner crystals for long-ranged interactions. Especially interesting is the evidence of a bubble crystal phase on a lattice, as this gives experiments a starting point for the search of clustered liquid phases, possibly hosting non-Abelian anyon excitations. In our work we show that NQS are an efficient and reliable simulation method for quantum systems, which are the subject of current research.","sentences":["Due to their immense representative power, neural network quantum states (NQS) have gained significant interest in current research.","In recent advances in the field of NQS, it has been demonstrated that this approach can compete with state-of-the-art numerical techniques, making NQS a compelling alternative, in particular for the simulation of large, two-dimensional quantum systems.","In this study, we show that recurrent neural network (RNN) wave functions can be employed to study systems relevant to current research in quantum many-body physics.","Specifically, we employ a 2D tensorized gated RNN to explore the bosonic Hofstadter model with a variable local Hilbert space cut-off and long-range interactions.","At first, we benchmark the RNN-NQS for the Hofstadter-Bose-Hubbard (HBH) Hamiltonian on a square lattice.","We find that this method is, despite the complexity of the wave function, capable of efficiently identifying and representing most ground state properties.","Afterwards, we apply the method to an even more challenging model for current methods, namely the Hofstadter model with long-range interactions.","This model describes Rydberg-dressed atoms on a lattice subject to a synthetic magnetic field.","We study systems of size up to $12 \\times 12$ sites and identify three different regimes by tuning the interaction range and the filling fraction $\\nu$. In addition to phases known from the HBH model at short-ranged interaction, we observe bubble crystals and Wigner crystals for long-ranged interactions.","Especially interesting is the evidence of a bubble crystal phase on a lattice, as this gives experiments a starting point for the search of clustered liquid phases, possibly hosting non-Abelian anyon excitations.","In our work we show that NQS are an efficient and reliable simulation method for quantum systems, which are the subject of current research."],"url":"http://arxiv.org/abs/2405.04472v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 16:31:18","title":"Turning the Ratchet: Dynamic Screening with Multiple Agents","abstract":"We study a dynamic contracting problem with multiple agents and a lack of commitment. A principal who can only commit to one-period contracts wants to screen efficient agents over time. Once an agent reveals his type, the principal becomes tempted to revise contract terms, causing a \"ratchet effect.\" Alterations of contracts are observable and, hence, whenever past promises are not honored future information revelation stops. We provide a necessary and sufficient condition under which the principal is able to foster information revelation. When players are sufficiently patient, the agents' private information is either never revealed or fully revealed in a sequential manner. Optimal contracts entail high-powered incentives after an agent's type is initially disclosed, and rewards for information revelation disappear in the long run.","sentences":["We study a dynamic contracting problem with multiple agents and a lack of commitment.","A principal who can only commit to one-period contracts wants to screen efficient agents over time.","Once an agent reveals his type, the principal becomes tempted to revise contract terms, causing a \"ratchet effect.\"","Alterations of contracts are observable and, hence, whenever past promises are not honored future information revelation stops.","We provide a necessary and sufficient condition under which the principal is able to foster information revelation.","When players are sufficiently patient, the agents' private information is either never revealed or fully revealed in a sequential manner.","Optimal contracts entail high-powered incentives after an agent's type is initially disclosed, and rewards for information revelation disappear in the long run."],"url":"http://arxiv.org/abs/2405.04468v1","category":"econ.TH"}
{"created":"2024-05-07 16:29:37","title":"Two-way Fixed Effects and Differences-in-Differences Estimators in Heterogeneous Adoption Designs","abstract":"We consider treatment-effect estimation under a parallel trends assumption, in heterogeneous adoption designs where no unit is treated at period one, and units receive a weakly positive dose at period two. First, we develop a test of the assumption that the treatment effect is mean independent of the treatment, under which the commonly-used two-way-fixed-effects estimator is consistent. When this test is rejected, we propose alternative, robust estimators. If there are stayers with a period-two treatment equal to 0, the robust estimator is a difference-in-differences (DID) estimator using stayers as the control group. If there are quasi-stayers with a period-two treatment arbitrarily close to zero, the robust estimator is a DID using units with a period-two treatment below a bandwidth as controls. Finally, without stayers or quasi-stayers, we propose non-parametric bounds, and an estimator relying on a parametric specification of treatment-effect heterogeneity. We use our results to revisit Pierce and Schott (2016) and Enikolopov et al. (2011).","sentences":["We consider treatment-effect estimation under a parallel trends assumption, in heterogeneous adoption designs where no unit is treated at period one, and units receive a weakly positive dose at period two.","First, we develop a test of the assumption that the treatment effect is mean independent of the treatment, under which the commonly-used two-way-fixed-effects estimator is consistent.","When this test is rejected, we propose alternative, robust estimators.","If there are stayers with a period-two treatment equal to 0, the robust estimator is a difference-in-differences (DID) estimator using stayers as the control group.","If there are quasi-stayers with a period-two treatment arbitrarily close to zero, the robust estimator is a DID using units with a period-two treatment below a bandwidth as controls.","Finally, without stayers or quasi-stayers, we propose non-parametric bounds, and an estimator relying on a parametric specification of treatment-effect heterogeneity.","We use our results to revisit Pierce and Schott (2016) and Enikolopov et al. (2011)."],"url":"http://arxiv.org/abs/2405.04465v1","category":"econ.EM"}
{"created":"2024-05-07 16:24:46","title":"Transition of dimuonium through foil","abstract":"This article presents a study of the passage of dimuonium through the foil of ordinary matter. First, we provide an overview of how dimuonium is planned to be produced for such a type of experiment and how it is expected to interact with the ordinary atoms -- predominantly electromagnetically via the screened coulomb potential of the atomic nuclei. Then, we describe the transport equations that represent the evolution of dimuonium states during the passage and their solution methods. Finally, for three different foils (Beryllium, Aluminium and Lead), we present the results of this study. To estimate impact of uncertainties in the potential of a target atom, we study 15 different approximations of the atomic potential and show that the corresponding atomic-potential-model-dependent error in the yields of the low lying states of dimuonium is quite small within the framework of the applied Born approximation. The convergence of the results after truncation of the infinite system of transport equations to the finite number of quantum states of dimuonium is also studied, and good convergence for the yields of low-lying states is demonstrated.","sentences":["This article presents a study of the passage of dimuonium through the foil of ordinary matter.","First, we provide an overview of how dimuonium is planned to be produced for such a type of experiment and how it is expected to interact with the ordinary atoms -- predominantly electromagnetically via the screened coulomb potential of the atomic nuclei.","Then, we describe the transport equations that represent the evolution of dimuonium states during the passage and their solution methods.","Finally, for three different foils (Beryllium, Aluminium and Lead), we present the results of this study.","To estimate impact of uncertainties in the potential of a target atom, we study 15 different approximations of the atomic potential and show that the corresponding atomic-potential-model-dependent error in the yields of the low lying states of dimuonium is quite small within the framework of the applied Born approximation.","The convergence of the results after truncation of the infinite system of transport equations to the finite number of quantum states of dimuonium is also studied, and good convergence for the yields of low-lying states is demonstrated."],"url":"http://arxiv.org/abs/2405.04460v1","category":"hep-ph"}
{"created":"2024-05-07 16:23:37","title":"Component Separation method for CMB using Convolutional Neural Networks","abstract":"The aim of this project is to recover the CMB anisotropies maps in temperature and polarized intensity by means of a deep convolutional neural network (CNN) which, after appropiate training, can remove the foregrounds from Planck and QUIJOTE data. The results are then compared with those obtained by COMMANDER, based on Bayesian parametric component separation. The CNN successfully recovered the CMB signal for both All Sky and Partial Sky maps showing frequency dependant results, being optimum for central frequencies where there is less contamination by foregrounds emissions such as galactic synchrotron and thermal dust emissions. Recovered maps in temperature are consistent with those obtained by Planck Collaboration, while polarized intensity has been recovered as a new observable. The polarized intensity maps recovered from QUIJOTE experiment are novel and of potential interest to the scientific community for the detection of primordial gravitational waves. The way forward will be to recover the maps at higher NSIDE and make them available to the scientific community.","sentences":["The aim of this project is to recover the CMB anisotropies maps in temperature and polarized intensity by means of a deep convolutional neural network (CNN) which, after appropiate training, can remove the foregrounds from Planck and QUIJOTE data.","The results are then compared with those obtained by COMMANDER, based on Bayesian parametric component separation.","The CNN successfully recovered the CMB signal for both All Sky and Partial Sky maps showing frequency dependant results, being optimum for central frequencies where there is less contamination by foregrounds emissions such as galactic synchrotron and thermal dust emissions.","Recovered maps in temperature are consistent with those obtained by Planck Collaboration, while polarized intensity has been recovered as a new observable.","The polarized intensity maps recovered from QUIJOTE experiment are novel and of potential interest to the scientific community for the detection of primordial gravitational waves.","The way forward will be to recover the maps at higher NSIDE and make them available to the scientific community."],"url":"http://arxiv.org/abs/2405.04564v1","category":"astro-ph.CO"}
{"created":"2024-05-07 16:13:45","title":"Exact solution of long-range stabilizer R\u00e9nyi entropy in the dual-unitary XXZ model","abstract":"Quantum systems can not be efficiently simulated classically due to the presence of entanglement and nonstabilizerness, also known as quantum magic. Here we study the generation of magic under evolution by a quantum circuit. To be able to provide exact solutions, we focus on the dual-unitary XXZ model and a measure of magic called stabilizer R\\'enyi entropy (SRE). Moreover, we focus also on long-range SRE, which cannot be removed by short-depth quantum circuits. To obtain exact solutions we use a ZX-calculus representation and graphical rules for the evaluation of the required expressions. We obtain exact results for SRE after short-time evolution in the thermodynamic limit and for long-range SRE for all times and all R\\'enyi parameters for a particular partition of the state. Since the numerical evaluation of these quantities is exponentially costly in the R\\'enyi parameter, we verify this numerically for low R\\'enyi parameters and accessible system sizes and provide numerical results for the long-range SRE in other bipartitions.","sentences":["Quantum systems can not be efficiently simulated classically due to the presence of entanglement and nonstabilizerness, also known as quantum magic.","Here we study the generation of magic under evolution by a quantum circuit.","To be able to provide exact solutions, we focus on the dual-unitary XXZ model and a measure of magic called stabilizer R\\'enyi entropy (SRE).","Moreover, we focus also on long-range SRE, which cannot be removed by short-depth quantum circuits.","To obtain exact solutions we use a ZX-calculus representation and graphical rules for the evaluation of the required expressions.","We obtain exact results for SRE after short-time evolution in the thermodynamic limit and for long-range SRE for all times and all R\\'enyi parameters for a particular partition of the state.","Since the numerical evaluation of these quantities is exponentially costly in the R\\'enyi parameter, we verify this numerically for low R\\'enyi parameters and accessible system sizes and provide numerical results for the long-range SRE in other bipartitions."],"url":"http://arxiv.org/abs/2405.04448v1","category":"quant-ph"}
{"created":"2024-05-07 16:03:55","title":"Generalized classical Yang-Baxter equation and regular decompositions","abstract":"The focus of the paper is on constructing new solutions of the generalized classical Yang-Baxter equation (GCYBE) that are not skew-symmetric. Using regular decompositions of finite-dimensional simple Lie algebras, we construct Lie algebra decompositions of $\\mathfrak{g}(\\!(x)\\!) \\times \\mathfrak{g}[x]/x^m \\mathfrak{g}[x]$. The latter decompositions are in bijection with the solutions to the GCYBE. Under appropriate regularity conditions, we obtain a partial classification of such solutions. The paper is concluded with the presentations of the Gaudin-type models associated to these solutions.","sentences":["The focus of the paper is on constructing new solutions of the generalized classical Yang-Baxter equation (GCYBE) that are not skew-symmetric.","Using regular decompositions of finite-dimensional simple Lie algebras, we construct Lie algebra decompositions of $\\mathfrak{g}(\\!(x)\\!)","\\times \\mathfrak{g}[x]/x^m \\mathfrak{g}[x]$. The latter decompositions are in bijection with the solutions to the GCYBE.","Under appropriate regularity conditions, we obtain a partial classification of such solutions.","The paper is concluded with the presentations of the Gaudin-type models associated to these solutions."],"url":"http://arxiv.org/abs/2405.04440v1","category":"math.RA"}
{"created":"2024-05-07 15:51:13","title":"Optimizing Information Freshness in IoT Systems with Update Rate Constraints: A Token-Based Approach","abstract":"In Internet of Things (IoT) status update systems, where information is sampled and subsequently transmitted from a source to a destination node, the imperative necessity lies in maintaining the timeliness of information and updating the system with optimal frequency. Optimizing information freshness in resource-limited status update systems often involves Constrained Markov Decision Process (CMDP) problems with update rate constraints. Solving CMDP problems, especially with multiple constraints, is a challenging task. To address this, we present a token-based approach that transforms CMDP into an unconstrained MDP, simplifying the solution process. We apply this approach to systems with one and two update rate constraints for optimizing Age of Incorrect Information (AoII) and Age of Information (AoI) metrics, respectively, and explore the analytical and numerical aspects. Additionally, we introduce an iterative triangle bisection method for solving the CMDP problems with two constraints, comparing its results with the token-based MDP approach. Our findings show that the token-based approach yields superior performance over baseline policies, converging to the optimal policy as the maximum number of tokens increases.","sentences":["In Internet of Things (IoT) status update systems, where information is sampled and subsequently transmitted from a source to a destination node, the imperative necessity lies in maintaining the timeliness of information and updating the system with optimal frequency.","Optimizing information freshness in resource-limited status update systems often involves Constrained Markov Decision Process (CMDP) problems with update rate constraints.","Solving CMDP problems, especially with multiple constraints, is a challenging task.","To address this, we present a token-based approach that transforms CMDP into an unconstrained MDP, simplifying the solution process.","We apply this approach to systems with one and two update rate constraints for optimizing Age of Incorrect Information (AoII) and Age of Information (AoI) metrics, respectively, and explore the analytical and numerical aspects.","Additionally, we introduce an iterative triangle bisection method for solving the CMDP problems with two constraints, comparing its results with the token-based MDP approach.","Our findings show that the token-based approach yields superior performance over baseline policies, converging to the optimal policy as the maximum number of tokens increases."],"url":"http://arxiv.org/abs/2405.04431v1","category":"cs.IT"}
{"created":"2024-05-07 15:49:21","title":"Josephson threshold detector in the phase diffusion regime","abstract":"We demonstrate that the performance of threshold detectors based on Al Josephson junctions can be significantly improved by exploiting the phase diffusion regime. When the escape dynamics of the detector switches to this regime, a decrease in both - dark count rate and the standard deviation of switching current is simultaneously observed. However, this effect is essential for (i) critical currents below 100 nA, and (ii) temperatures of the order of several hundreds millikelvin. Importantly that for such detectors optimal performance occurs at finite temperatures, making the microwave single photon detection feasible even in the sub-K range. Possible explanation of these findings is discussed.","sentences":["We demonstrate that the performance of threshold detectors based on Al Josephson junctions can be significantly improved by exploiting the phase diffusion regime.","When the escape dynamics of the detector switches to this regime, a decrease in both - dark count rate and the standard deviation of switching current is simultaneously observed.","However, this effect is essential for (i) critical currents below 100 nA, and (ii) temperatures of the order of several hundreds millikelvin.","Importantly that for such detectors optimal performance occurs at finite temperatures, making the microwave single photon detection feasible even in the sub-K range.","Possible explanation of these findings is discussed."],"url":"http://arxiv.org/abs/2405.04426v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 15:44:39","title":"Fully Automated Selfish Mining Analysis in Efficient Proof Systems Blockchains","abstract":"We study selfish mining attacks in longest-chain blockchains like Bitcoin, but where the proof of work is replaced with efficient proof systems -- like proofs of stake or proofs of space -- and consider the problem of computing an optimal selfish mining attack which maximizes expected relative revenue of the adversary, thus minimizing the chain quality. To this end, we propose a novel selfish mining attack that aims to maximize this objective and formally model the attack as a Markov decision process (MDP). We then present a formal analysis procedure which computes an $\\epsilon$-tight lower bound on the optimal expected relative revenue in the MDP and a strategy that achieves this $\\epsilon$-tight lower bound, where $\\epsilon>0$ may be any specified precision. Our analysis is fully automated and provides formal guarantees on the correctness. We evaluate our selfish mining attack and observe that it achieves superior expected relative revenue compared to two considered baselines.   In concurrent work [Sarenche FC'24] does an automated analysis on selfish mining in predictable longest-chain blockchains based on efficient proof systems. Predictable means the randomness for the challenges is fixed for many blocks (as used e.g., in Ouroboros), while we consider unpredictable (Bitcoin-like) chains where the challenge is derived from the previous block.","sentences":["We study selfish mining attacks in longest-chain blockchains like Bitcoin, but where the proof of work is replaced with efficient proof systems -- like proofs of stake or proofs of space -- and consider the problem of computing an optimal selfish mining attack which maximizes expected relative revenue of the adversary, thus minimizing the chain quality.","To this end, we propose a novel selfish mining attack that aims to maximize this objective and formally model the attack as a Markov decision process (MDP).","We then present a formal analysis procedure which computes an $\\epsilon$-tight lower bound on the optimal expected relative revenue in the MDP and a strategy that achieves this $\\epsilon$-tight lower bound, where $\\epsilon>0$ may be any specified precision.","Our analysis is fully automated and provides formal guarantees on the correctness.","We evaluate our selfish mining attack and observe that it achieves superior expected relative revenue compared to two considered baselines.   ","In concurrent work [Sarenche FC'24] does an automated analysis on selfish mining in predictable longest-chain blockchains based on efficient proof systems.","Predictable means the randomness for the challenges is fixed for many blocks (as used e.g., in Ouroboros), while we consider unpredictable (Bitcoin-like) chains where the challenge is derived from the previous block."],"url":"http://arxiv.org/abs/2405.04420v1","category":"cs.CR"}
{"created":"2024-05-07 15:43:21","title":"Mathematical Modeling of $^{18}$F-Fluoromisonidazole ($^{18}$F-FMISO) Radiopharmaceutical Transport in Vascularized Solid Tumors","abstract":"$^{18}$F-Fluoromisonidazole ($^{18}$F-FMISO) is a highly promising positron emission tomography radiopharmaceutical for identifying hypoxic regions in solid tumors. This research employs spatiotemporal multi-scale mathematical modeling to explore how different levels of angiogenesis influence the transport of radiopharmaceuticals within tumors. In this study, two tumor geometries with heterogeneous and uniform distributions of capillary networks were employed to incorporate varying degrees of microvascular density. The synthetic image of the heterogeneous and vascularized tumor was generated by simulating the angiogenesis process. The proposed multi-scale spatiotemporal model accounts for intricate physiological and biochemical factors within the tumor microenvironment, such as the transvascular transport of the radiopharmaceutical agent, its movement into the interstitial space by diffusion and convection mechanisms, and ultimately its uptake by tumor cells. Results showed that both quantitative and semi-quantitative metrics of $^{18}$F-FMISO uptake differ spatially and temporally at different stages during tumor growth. The presence of a high microvascular density in uniformly vascularized tumor increases cellular uptake, as it allows for more efficient release and rapid distribution of radiopharmaceutical molecules. This results in enhanced uptake compared to the heterogeneous vascularized tumor. In both heterogeneous and uniform distribution of microvessels in tumors, the diffusion transport mechanism has a more pronounced than convection. The findings of this study shed light on the transport phenomena behind $^{18}$F-FMISO radiopharmaceutical distribution and its delivery in the tumor microenvironment, aiding oncologists in their routine decision-making processes.","sentences":["$^{18}$F-Fluoromisonidazole ($^{18}$F-FMISO) is a highly promising positron emission tomography radiopharmaceutical for identifying hypoxic regions in solid tumors.","This research employs spatiotemporal multi-scale mathematical modeling to explore how different levels of angiogenesis influence the transport of radiopharmaceuticals within tumors.","In this study, two tumor geometries with heterogeneous and uniform distributions of capillary networks were employed to incorporate varying degrees of microvascular density.","The synthetic image of the heterogeneous and vascularized tumor was generated by simulating the angiogenesis process.","The proposed multi-scale spatiotemporal model accounts for intricate physiological and biochemical factors within the tumor microenvironment, such as the transvascular transport of the radiopharmaceutical agent, its movement into the interstitial space by diffusion and convection mechanisms, and ultimately its uptake by tumor cells.","Results showed that both quantitative and semi-quantitative metrics of $^{18}$F-FMISO uptake differ spatially and temporally at different stages during tumor growth.","The presence of a high microvascular density in uniformly vascularized tumor increases cellular uptake, as it allows for more efficient release and rapid distribution of radiopharmaceutical molecules.","This results in enhanced uptake compared to the heterogeneous vascularized tumor.","In both heterogeneous and uniform distribution of microvessels in tumors, the diffusion transport mechanism has a more pronounced than convection.","The findings of this study shed light on the transport phenomena behind $^{18}$F-FMISO radiopharmaceutical distribution and its delivery in the tumor microenvironment, aiding oncologists in their routine decision-making processes."],"url":"http://arxiv.org/abs/2405.04418v1","category":"physics.bio-ph"}
{"created":"2024-05-07 15:31:58","title":"Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation","abstract":"Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios. Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning. In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations. To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions. On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE. Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks.","sentences":["Uncertainty estimation (UE), as an effective means of quantifying predictive uncertainty, is crucial for safe and reliable decision-making, especially in high-risk scenarios.","Existing UE schemes usually assume that there are completely-labeled samples to support fully-supervised learning.","In practice, however, many UE tasks often have no sufficiently-labeled data to use, such as the Multiple Instance Learning (MIL) with only weak instance annotations.","To bridge this gap, this paper, for the first time, addresses the weakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline scheme, Multi-Instance Residual Evidential Learning (MIREL).","Particularly, at the fine-grained instance UE with only weak supervision, we derive a multi-instance residual operator through the Fundamental Theorem of Symmetric Functions.","On this operator derivation, we further propose MIREL to jointly model the high-order predictive distribution at bag and instance levels for MIUE.","Extensive experiments empirically demonstrate that our MIREL not only could often make existing MIL networks perform better in MIUE, but also could surpass representative UE methods by large margins, especially in instance-level UE tasks."],"url":"http://arxiv.org/abs/2405.04405v1","category":"cs.LG"}
{"created":"2024-05-07 15:29:48","title":"Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks","abstract":"Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs). While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention. In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach. By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking. We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM's safety guardrails. Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning.","sentences":["Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs).","While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention.","In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach.","By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking.","We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM's safety guardrails.","Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning."],"url":"http://arxiv.org/abs/2405.04403v1","category":"cs.CV"}
{"created":"2024-05-07 15:27:46","title":"Utility-driven Optimization of TTL Cache Hierarchies under Network Delays","abstract":"We optimize hierarchies of Time-to-Live (TTL) caches under random network delays. A TTL cache assigns individual eviction timers to cached objects that are usually refreshed upon a hit where upon a miss the object requires a random time to be fetched from a parent cache. Due to their object decoupling property, TTL caches are of particular interest since the optimization of a per-object utility enables service differentiation. However, state-of-the-art exact TTL cache optimization does not extend beyond single TTL caches, especially under network delays. In this paper, we leverage the object decoupling effect to formulate the non-linear utility maximization problem for TTL cache hierarchies in terms of the exact object hit probability under random network delays. We iteratively solve the utility maximization problem to find the optimal per-object TTLs. Further, we show that the exact model suffers from tractability issues for large hierarchies and propose a machine learning approach to estimate the optimal TTL values for large systems. Finally, we provide numerical and data center trace-based evaluations for both methods showing the significant offloading improvement due to TTL optimization considering the network delays.","sentences":["We optimize hierarchies of Time-to-Live (TTL) caches under random network delays.","A TTL cache assigns individual eviction timers to cached objects that are usually refreshed upon a hit where upon a miss the object requires a random time to be fetched from a parent cache.","Due to their object decoupling property, TTL caches are of particular interest since the optimization of a per-object utility enables service differentiation.","However, state-of-the-art exact TTL cache optimization does not extend beyond single TTL caches, especially under network delays.","In this paper, we leverage the object decoupling effect to formulate the non-linear utility maximization problem for TTL cache hierarchies in terms of the exact object hit probability under random network delays.","We iteratively solve the utility maximization problem to find the optimal per-object TTLs.","Further, we show that the exact model suffers from tractability issues for large hierarchies and propose a machine learning approach to estimate the optimal TTL values for large systems.","Finally, we provide numerical and data center trace-based evaluations for both methods showing the significant offloading improvement due to TTL optimization considering the network delays."],"url":"http://arxiv.org/abs/2405.04402v1","category":"cs.NI"}
{"created":"2024-05-07 15:25:39","title":"Solving ill-conditioned linear algebraic systems using methods that improve conditioning","abstract":"We consider the solution of systems of linear algebraic equations (SLAEs) with an ill-conditioned or degenerate exact matrix and an approximate right-hand side. An approach to solving such a problem is proposed and justified, which makes it possible to improve the conditionality of the SLAE matrix and, as a result, obtain an approximate solution that is stable to perturbations of the right hand side with higher accuracy than using other methods. The approach is implemented by an algorithm that uses so-called minimal pseudoinverse matrices. The results of numerical experiments are presented that confirm the theoretical provisions of the article.","sentences":["We consider the solution of systems of linear algebraic equations (SLAEs) with an ill-conditioned or degenerate exact matrix and an approximate right-hand side.","An approach to solving such a problem is proposed and justified, which makes it possible to improve the conditionality of the SLAE matrix and, as a result, obtain an approximate solution that is stable to perturbations of the right hand side with higher accuracy than using other methods.","The approach is implemented by an algorithm that uses so-called minimal pseudoinverse matrices.","The results of numerical experiments are presented that confirm the theoretical provisions of the article."],"url":"http://arxiv.org/abs/2405.04399v1","category":"math.NA"}
{"created":"2024-05-07 15:25:33","title":"A first-principles study of structural, elastic, electronic, and transport properties of Cs2Te","abstract":"The pursuit to operate photocathodes at high accelerating gradients to increase brightness of electron beams is gaining interests within the accelerator community. Cesium telluride (Cs2Te) is a widely used photocathode material and it is presumed to offer resilience to higher gradients because of its wider band gap compared to other semiconductors. Despite its advantages, crucial material properties of Cs2Te remain largely unknown both in theory and experiments. In this study, we employ first-principles calculations to provide detailed structural, elastic, electronic and transport properties of Cs2Te. It is found that Cs2Te has an intrinsic mobility of 20 cm2/Vs for electrons and 2.0 cm2/Vs for holes at room temperature. The low mobility is primarily limited by the strong polar optical phonon scattering. Cs2Te also exhibits ultralow lattice thermal conductivity of 0.2 W/(m*K) at room temperature. Based on the energy gain/loss balance under external field and electron-phonon scattering, we predict that Cs2Te has a dielectric breakdown field in the range from ~60 MV/m to ~132 MV/m at room temperature dependent on the doping level of Cs2Te. Our results are crucial to advance the understanding of applicability of Cs2Te photocathodes for high-gradient operation.","sentences":["The pursuit to operate photocathodes at high accelerating gradients to increase brightness of electron beams is gaining interests within the accelerator community.","Cesium telluride (Cs2Te) is a widely used photocathode material and it is presumed to offer resilience to higher gradients because of its wider band gap compared to other semiconductors.","Despite its advantages, crucial material properties of Cs2Te remain largely unknown both in theory and experiments.","In this study, we employ first-principles calculations to provide detailed structural, elastic, electronic and transport properties of Cs2Te.","It is found that Cs2Te has an intrinsic mobility of 20 cm2/Vs for electrons and 2.0 cm2/Vs for holes at room temperature.","The low mobility is primarily limited by the strong polar optical phonon scattering.","Cs2Te also exhibits ultralow lattice thermal conductivity of 0.2 W/(m*K) at room temperature.","Based on the energy gain/loss balance under external field and electron-phonon scattering, we predict that Cs2Te has a dielectric breakdown field in the range from ~60 MV/m to ~132 MV/m at room temperature dependent on the doping level of Cs2Te.","Our results are crucial to advance the understanding of applicability of Cs2Te photocathodes for high-gradient operation."],"url":"http://arxiv.org/abs/2405.04398v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 15:20:33","title":"Primordial monopoles, black holes and gravitational waves","abstract":"We show how topologically stable superheavy magnetic monopoles and primordial black holes can be generated at observable levels by the waterfall field in hybrid inflation models based on grand unified theories. In $SU(5) \\times U(1)_\\chi$ grand unification, the monopole mass is of order $4 \\times 10^{17}$ GeV, and it carries a single unit ($2 \\pi /e$) of Dirac magnetic charge as well as screened color magnetic charge. The monopole density is partially diluted to an observable value, and accompanied with the production of primordial black holes with mass of order $10^{17}$-$10^{19}$ g which may make up the entire dark matter in the universe. The tensor to scalar ratio $r$ is predicted to be of order $10^{-5}$ - $10^{-4}$ which should be testable in the next generation of CMB experiments such as CMB-S4 and LiteBIRD. The gravitational wave spectrum generated during the waterfall transition is also presented.","sentences":["We show how topologically stable superheavy magnetic monopoles and primordial black holes can be generated at observable levels by the waterfall field in hybrid inflation models based on grand unified theories.","In $SU(5) \\times U(1)_\\chi$ grand unification, the monopole mass is of order $4 \\times 10^{17}$ GeV, and it carries a single unit ($2 \\pi /e$) of Dirac magnetic charge as well as screened color magnetic charge.","The monopole density is partially diluted to an observable value, and accompanied with the production of primordial black holes with mass of order $10^{17}$-$10^{19}$ g which may make up the entire dark matter in the universe.","The tensor to scalar ratio $r$ is predicted to be of order $10^{-5}$ - $10^{-4}$ which should be testable in the next generation of CMB experiments such as CMB-S4 and LiteBIRD.","The gravitational wave spectrum generated during the waterfall transition is also presented."],"url":"http://arxiv.org/abs/2405.04397v1","category":"hep-ph"}
{"created":"2024-05-07 15:14:51","title":"Efficient Online Set-valued Classification with Bandit Feedback","abstract":"Conformal prediction is a distribution-free method that wraps a given machine learning model and returns a set of plausible labels that contain the true label with a prescribed coverage rate. In practice, the empirical coverage achieved highly relies on fully observed label information from data both in the training phase for model fitting and the calibration phase for quantile estimation. This dependency poses a challenge in the context of online learning with bandit feedback, where a learner only has access to the correctness of actions (i.e., pulled an arm) but not the full information of the true label. In particular, when the pulled arm is incorrect, the learner only knows that the pulled one is not the true class label, but does not know which label is true. Additionally, bandit feedback further results in a smaller labeled dataset for calibration, limited to instances with correct actions, thereby affecting the accuracy of quantile estimation. To address these limitations, we propose Bandit Class-specific Conformal Prediction (BCCP), offering coverage guarantees on a class-specific granularity. Using an unbiased estimation of an estimand involving the true label, BCCP trains the model and makes set-valued inferences through stochastic gradient descent. Our approach overcomes the challenges of sparsely labeled data in each iteration and generalizes the reliability and applicability of conformal prediction to online decision-making environments.","sentences":["Conformal prediction is a distribution-free method that wraps a given machine learning model and returns a set of plausible labels that contain the true label with a prescribed coverage rate.","In practice, the empirical coverage achieved highly relies on fully observed label information from data both in the training phase for model fitting and the calibration phase for quantile estimation.","This dependency poses a challenge in the context of online learning with bandit feedback, where a learner only has access to the correctness of actions (i.e., pulled an arm) but not the full information of the true label.","In particular, when the pulled arm is incorrect, the learner only knows that the pulled one is not the true class label, but does not know which label is true.","Additionally, bandit feedback further results in a smaller labeled dataset for calibration, limited to instances with correct actions, thereby affecting the accuracy of quantile estimation.","To address these limitations, we propose Bandit Class-specific Conformal Prediction (BCCP), offering coverage guarantees on a class-specific granularity.","Using an unbiased estimation of an estimand involving the true label, BCCP trains the model and makes set-valued inferences through stochastic gradient descent.","Our approach overcomes the challenges of sparsely labeled data in each iteration and generalizes the reliability and applicability of conformal prediction to online decision-making environments."],"url":"http://arxiv.org/abs/2405.04393v1","category":"stat.ML"}
{"created":"2024-05-08 17:59:58","title":"OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies","abstract":"Event-based semantic segmentation (ESS) is a fundamental yet challenging task for event camera sensing. The difficulties in interpreting and annotating event data limit its scalability. While domain adaptation from images to event data can help to mitigate this issue, there exist data representational differences that require additional effort to resolve. In this work, for the first time, we synergize information from image, text, and event-data domains and introduce OpenESS to enable scalable ESS in an open-world, annotation-efficient manner. We achieve this goal by transferring the semantically rich CLIP knowledge from image-text pairs to event streams. To pursue better cross-modality adaptation, we propose a frame-to-event contrastive distillation and a text-to-event semantic consistency regularization. Experimental results on popular ESS benchmarks showed our approach outperforms existing methods. Notably, we achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either event or frame labels.","sentences":["Event-based semantic segmentation (ESS) is a fundamental yet challenging task for event camera sensing.","The difficulties in interpreting and annotating event data limit its scalability.","While domain adaptation from images to event data can help to mitigate this issue, there exist data representational differences that require additional effort to resolve.","In this work, for the first time, we synergize information from image, text, and event-data domains and introduce OpenESS to enable scalable ESS in an open-world, annotation-efficient manner.","We achieve this goal by transferring the semantically rich CLIP knowledge from image-text pairs to event streams.","To pursue better cross-modality adaptation, we propose a frame-to-event contrastive distillation and a text-to-event semantic consistency regularization.","Experimental results on popular ESS benchmarks showed our approach outperforms existing methods.","Notably, we achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either event or frame labels."],"url":"http://arxiv.org/abs/2405.05259v1","category":"cs.CV"}
{"created":"2024-05-08 16:07:56","title":"Custom Gradient Estimators are Straight-Through Estimators in Disguise","abstract":"Quantization-aware training comes with a fundamental challenge: the derivative of quantization functions such as rounding are zero almost everywhere and nonexistent elsewhere. Various differentiable approximations of quantization functions have been proposed to address this issue. In this paper, we prove that when the learning rate is sufficiently small, a large class of weight gradient estimators is equivalent with the straight through estimator (STE). Specifically, after swapping in the STE and adjusting both the weight initialization and the learning rate in SGD, the model will train in almost exactly the same way as it did with the original gradient estimator. Moreover, we show that for adaptive learning rate algorithms like Adam, the same result can be seen without any modifications to the weight initialization and learning rate. We experimentally show that these results hold for both a small convolutional model trained on the MNIST dataset and for a ResNet50 model trained on ImageNet.","sentences":["Quantization-aware training comes with a fundamental challenge: the derivative of quantization functions such as rounding are zero almost everywhere and nonexistent elsewhere.","Various differentiable approximations of quantization functions have been proposed to address this issue.","In this paper, we prove that when the learning rate is sufficiently small, a large class of weight gradient estimators is equivalent with the straight through estimator (STE).","Specifically, after swapping in the STE and adjusting both the weight initialization and the learning rate in SGD, the model will train in almost exactly the same way as it did with the original gradient estimator.","Moreover, we show that for adaptive learning rate algorithms like Adam, the same result can be seen without any modifications to the weight initialization and learning rate.","We experimentally show that these results hold for both a small convolutional model trained on the MNIST dataset and for a ResNet50 model trained on ImageNet."],"url":"http://arxiv.org/abs/2405.05171v1","category":"cs.LG"}
{"created":"2024-05-08 15:13:33","title":"XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples","abstract":"Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, presents challenges due to the scarcity of available cross-lingual retrievers and annotated data. In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever with positive/negative English samples, which are constructed based on the predictions of the multilingual large language model for in-context learning. Then, the trained retriever is directly employed to retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on the massively multilingual text classification benchmark of SIB200 with 176 languages demonstrate that XAMPLER substantially improves the in-context learning performance across languages. Our code is available at https://github.com/cisnlp/XAMPLER.","sentences":["Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in-context learning of English.","However, adapting these methods to other languages, especially low-resource ones, presents challenges due to the scarcity of available cross-lingual retrievers and annotated data.","In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data.","XAMPLER first trains a retriever with positive/negative English samples, which are constructed based on the predictions of the multilingual large language model for in-context learning.","Then, the trained retriever is directly employed to retrieve English examples as few-shot examples for in-context learning of target languages.","Experiments on the massively multilingual text classification benchmark of SIB200 with 176 languages demonstrate that XAMPLER substantially improves the in-context learning performance across languages.","Our code is available at https://github.com/cisnlp/XAMPLER."],"url":"http://arxiv.org/abs/2405.05116v1","category":"cs.CL"}
{"created":"2024-05-08 14:35:39","title":"Longitudinal spin polarization in a thermal model with dissipative corrections","abstract":"In this work, we address the problem of longitudinal spin polarization of the $\\Lambda$ hyperons produced in relativistic heavy-ion collisions. We combine a relativistic kinetic-theory framework that includes spin degrees of freedom treated in a classical way with the freeze-out parametrization used in previous investigations. The use of the kinetic theory allows us to incorporate dissipative corrections (due to the thermal shear and gradients of thermal vorticity) into the Pauli-Lubanski vector that determines spin polarization and can be directly compared with the experimental data. As in earlier similar studies, it turns out that a successful description of data can only be achieved with additional assumptions -- in our case, they involve the use of projected thermal vorticity and a suitably adjusted time for spin relaxation ($\\tau_s$). From our analysis, we find that $\\tau_s \\sim 5$ fm/$c$, which is comparable with other estimates.","sentences":["In this work, we address the problem of longitudinal spin polarization of the $\\Lambda$ hyperons produced in relativistic heavy-ion collisions.","We combine a relativistic kinetic-theory framework that includes spin degrees of freedom treated in a classical way with the freeze-out parametrization used in previous investigations.","The use of the kinetic theory allows us to incorporate dissipative corrections (due to the thermal shear and gradients of thermal vorticity) into the Pauli-Lubanski vector that determines spin polarization and can be directly compared with the experimental data.","As in earlier similar studies, it turns out that a successful description of data can only be achieved with additional assumptions -- in our case, they involve the use of projected thermal vorticity and a suitably adjusted time for spin relaxation ($\\tau_s$).","From our analysis, we find that $\\tau_s \\sim 5$ fm/$c$, which is comparable with other estimates."],"url":"http://arxiv.org/abs/2405.05089v1","category":"hep-ph"}
{"created":"2024-05-08 12:42:50","title":"AI-based Dynamic Schedule Calculation in Time Sensitive Networks using GCN-TD3","abstract":"Offline scheduling in Time Sensitive Networking (TSN) utilizing the Time Aware Shaper (TAS) facilitates optimal deterministic latency and jitter-bounds calculation for Time- Triggered (TT) flows. However, the dynamic nature of traffic in industrial settings necessitates a strategy for adaptively scheduling flows without interrupting existing schedules. Our research identifies critical gaps in current dynamic scheduling methods for TSN and introduces the novel GCN-TD3 approach. This novel approach utilizes a Graph Convolutional Network (GCN) for representing the various relations within different components of TSN and employs the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to dynamically schedule any incoming flow. Additionally, an Integer Linear Programming (ILP) based offline scheduler is used both to initiate the simulation and serve as a fallback mechanism. This mechanism is triggered to recalculate the entire schedule when the predefined threshold of Gate Control List(GCL) length exceeds. Comparative analyses demonstrate that GCN-TD3 outperforms existing methods like Deep Double Q-Network (DDQN) and Deep Deterministic Policy Gradient (DDPG), exhibiting convergence within 4000 epochs with a 90\\% dynamic TT flow admission rate while maintaining deadlines and reducing jitter to as low as 2us. Finally, two modules were developed for the OMNeT++ simulator, facilitating dynamic simulation to evaluate the methodology.","sentences":["Offline scheduling in Time Sensitive Networking (TSN) utilizing the Time Aware Shaper (TAS) facilitates optimal deterministic latency and jitter-bounds calculation for Time- Triggered (TT) flows.","However, the dynamic nature of traffic in industrial settings necessitates a strategy for adaptively scheduling flows without interrupting existing schedules.","Our research identifies critical gaps in current dynamic scheduling methods for TSN and introduces the novel GCN-TD3 approach.","This novel approach utilizes a Graph Convolutional Network (GCN) for representing the various relations within different components of TSN and employs the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to dynamically schedule any incoming flow.","Additionally, an Integer Linear Programming (ILP) based offline scheduler is used both to initiate the simulation and serve as a fallback mechanism.","This mechanism is triggered to recalculate the entire schedule when the predefined threshold of Gate Control List(GCL) length exceeds.","Comparative analyses demonstrate that GCN-TD3 outperforms existing methods like Deep Double Q-Network (DDQN) and Deep Deterministic Policy Gradient (DDPG), exhibiting convergence within 4000 epochs with a 90\\% dynamic TT flow admission rate while maintaining deadlines and reducing jitter to as low as 2us.","Finally, two modules were developed for the OMNeT++ simulator, facilitating dynamic simulation to evaluate the methodology."],"url":"http://arxiv.org/abs/2405.05019v1","category":"cs.NI"}
{"created":"2024-05-08 12:20:10","title":"Analyzing design principles for competitive evolution strategies in constrained search spaces","abstract":"In the context of the 2018 IEEE Congress of Evolutionary Computation, the Matrix Adaptation Evolution Strategy for constrained optimization turned out to be notably successful in the competition on constrained single objective real-parameter optimization. Across all considered instances the so-called $\\epsilon$MAg-ES achieved the second rank. However, it can be considered to be the most successful participant in high dimensions. Unfortunately, the competition result does not provide any information about the modus operandi of a successful algorithm or its suitability for problems of a particular shape. To this end, the present paper is concerned with an extensive empirical analysis of the $\\epsilon$MAg-ES working principles that is expected to provide insights about the performance contribution of specific algorithmic components. To avoid rankings with respect to insignificant differences within the algorithm realizations, the paper additionally introduces significance testing into the ranking process.","sentences":["In the context of the 2018 IEEE Congress of Evolutionary Computation, the Matrix Adaptation Evolution Strategy for constrained optimization turned out to be notably successful in the competition on constrained single objective real-parameter optimization.","Across all considered instances the so-called $\\epsilon$MAg-ES achieved the second rank.","However, it can be considered to be the most successful participant in high dimensions.","Unfortunately, the competition result does not provide any information about the modus operandi of a successful algorithm or its suitability for problems of a particular shape.","To this end, the present paper is concerned with an extensive empirical analysis of the $\\epsilon$MAg-ES working principles that is expected to provide insights about the performance contribution of specific algorithmic components.","To avoid rankings with respect to insignificant differences within the algorithm realizations, the paper additionally introduces significance testing into the ranking process."],"url":"http://arxiv.org/abs/2405.05005v1","category":"cs.NE"}
{"created":"2024-05-08 12:19:08","title":"TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking","abstract":"There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion. However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data. To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity. In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes. The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF). Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively. Our code will be available at https://github.com/SSSpc333/TENet.","sentences":["There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion.","However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data.","To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity.","In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes.","The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF).","Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively.","Our code will be available at https://github.com/SSSpc333/TENet."],"url":"http://arxiv.org/abs/2405.05004v1","category":"cs.CV"}
{"created":"2024-05-08 09:38:11","title":"Guiding adaptive shrinkage by co-data to improve regression-based prediction and feature selection","abstract":"The high dimensional nature of genomics data complicates feature selection, in particular in low sample size studies - not uncommon in clinical prediction settings. It is widely recognized that complementary data on the features, `co-data', may improve results. Examples are prior feature groups or p-values from a related study. Such co-data are ubiquitous in genomics settings due to the availability of public repositories. Yet, the uptake of learning methods that structurally use such co-data is limited. We review guided adaptive shrinkage methods: a class of regression-based learners that use co-data to adapt the shrinkage parameters, crucial for the performance of those learners. We discuss technical aspects, but also the applicability in terms of types of co-data that can be handled. This class of methods is contrasted with several others. In particular, group-adaptive shrinkage is compared with the better-known sparse group-lasso by evaluating feature selection. Finally, we demonstrate the versatility of the guided shrinkage methodology by showing how to `do-it-yourself': we integrate implementations of a co-data learner and the spike-and-slab prior for the purpose of improving feature selection in genetics studies.","sentences":["The high dimensional nature of genomics data complicates feature selection, in particular in low sample size studies - not uncommon in clinical prediction settings.","It is widely recognized that complementary data on the features, `co-data', may improve results.","Examples are prior feature groups or p-values from a related study.","Such co-data are ubiquitous in genomics settings due to the availability of public repositories.","Yet, the uptake of learning methods that structurally use such co-data is limited.","We review guided adaptive shrinkage methods: a class of regression-based learners that use co-data to adapt the shrinkage parameters, crucial for the performance of those learners.","We discuss technical aspects, but also the applicability in terms of types of co-data that can be handled.","This class of methods is contrasted with several others.","In particular, group-adaptive shrinkage is compared with the better-known sparse group-lasso by evaluating feature selection.","Finally, we demonstrate the versatility of the guided shrinkage methodology by showing how to `do-it-yourself': we integrate implementations of a co-data learner and the spike-and-slab prior for the purpose of improving feature selection in genetics studies."],"url":"http://arxiv.org/abs/2405.04917v1","category":"stat.ME"}
{"created":"2024-05-08 01:37:50","title":"Transverse Cooper-Pair Rectifier","abstract":"Non-reciprocal devices like diodes are key components in modern electronics covering broad applications ranging from transistors to logic circuits thanks to the output rectified signal in the direction parallel to the input. In this work, we predict a transverse Cooper-pair rectifier in which a non-reciprocal current is driven perpendicular to the driving field, when inversion, time reversal, and mirror symmetries are broken simultaneously. The Blonder-Tinkham-Klapwijk formalism is developed to describe the transverse current-voltage relation in a normal-metal/superconductor tunneling junction with an effective built-in supercurrent which leads to an asymmetric and anisotropic Andreev reflection due to the lack of symmetry constraints. The asymmetry in the Andreev reflection is induced when inversion and time reversal symmetry are broken by the supercurrent component parallel to the junction while the anisotropy occurs when the mirror symmetry with respect to the normal of the junction interface is broken by the perpendicular supercurrent component to the junction. Compared to the conventional longitudinal one, the transverse rectifier supports fully polarized diode efficiency and colossal nonreciprocal conductance rectification, completely decoupling the path of the input excitation from the output rectified signal. This work provides a formalism for realizing transverse non-reciprocity in superconducting junctions, which is expected to be achieved after some modification to the current experimental setups and may pave the way for future low-dissipation and fast superconducting electronics.","sentences":["Non-reciprocal devices like diodes are key components in modern electronics covering broad applications ranging from transistors to logic circuits thanks to the output rectified signal in the direction parallel to the input.","In this work, we predict a transverse Cooper-pair rectifier in which a non-reciprocal current is driven perpendicular to the driving field, when inversion, time reversal, and mirror symmetries are broken simultaneously.","The Blonder-Tinkham-Klapwijk formalism is developed to describe the transverse current-voltage relation in a normal-metal/superconductor tunneling junction with an effective built-in supercurrent which leads to an asymmetric and anisotropic Andreev reflection due to the lack of symmetry constraints.","The asymmetry in the Andreev reflection is induced when inversion and time reversal symmetry are broken by the supercurrent component parallel to the junction while the anisotropy occurs when the mirror symmetry with respect to the normal of the junction interface is broken by the perpendicular supercurrent component to the junction.","Compared to the conventional longitudinal one, the transverse rectifier supports fully polarized diode efficiency and colossal nonreciprocal conductance rectification, completely decoupling the path of the input excitation from the output rectified signal.","This work provides a formalism for realizing transverse non-reciprocity in superconducting junctions, which is expected to be achieved after some modification to the current experimental setups and may pave the way for future low-dissipation and fast superconducting electronics."],"url":"http://arxiv.org/abs/2405.04751v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 23:44:09","title":"Remote Diffusion","abstract":"I explored adapting Stable Diffusion v1.5 for generating domain-specific satellite and aerial images in remote sensing. Recognizing the limitations of existing models like Midjourney and Stable Diffusion, trained primarily on natural RGB images and lacking context for remote sensing, I used the RSICD dataset to train a Stable Diffusion model with a loss of 0.2. I incorporated descriptive captions from the dataset for text-conditioning. Additionally, I created a synthetic dataset for a Land Use Land Classification (LULC) task, employing prompting techniques with RAG and ChatGPT and fine-tuning a specialized remote sensing LLM. However, I faced challenges with prompt quality and model performance. I trained a classification model (ResNet18) on the synthetic dataset achieving 49.48% test accuracy in TorchGeo to create a baseline. Quantitative evaluation through FID scores and qualitative feedback from domain experts assessed the realism and quality of the generated images and dataset. Despite extensive fine-tuning and dataset iterations, results indicated subpar image quality and realism, as indicated by high FID scores and domain-expert evaluation. These findings call attention to the potential of diffusion models in remote sensing while highlighting significant challenges related to insufficient pretraining data and computational resources.","sentences":["I explored adapting Stable Diffusion v1.5 for generating domain-specific satellite and aerial images in remote sensing.","Recognizing the limitations of existing models like Midjourney and Stable Diffusion, trained primarily on natural RGB images and lacking context for remote sensing, I used the RSICD dataset to train a Stable Diffusion model with a loss of 0.2.","I incorporated descriptive captions from the dataset for text-conditioning.","Additionally, I created a synthetic dataset for a Land Use Land Classification (LULC) task, employing prompting techniques with RAG and ChatGPT and fine-tuning a specialized remote sensing LLM.","However, I faced challenges with prompt quality and model performance.","I trained a classification model (ResNet18) on the synthetic dataset achieving 49.48% test accuracy in TorchGeo to create a baseline.","Quantitative evaluation through FID scores and qualitative feedback from domain experts assessed the realism and quality of the generated images and dataset.","Despite extensive fine-tuning and dataset iterations, results indicated subpar image quality and realism, as indicated by high FID scores and domain-expert evaluation.","These findings call attention to the potential of diffusion models in remote sensing while highlighting significant challenges related to insufficient pretraining data and computational resources."],"url":"http://arxiv.org/abs/2405.04717v1","category":"cs.CV"}
{"created":"2024-05-07 21:38:13","title":"Pipe Routing with Topology Control for UAV Networks","abstract":"Routing protocols help in transmitting the sensed data from UAVs monitoring the targets (called target UAVs) to the BS. However, the highly dynamic nature of an autonomous, decentralized UAV network leads to frequent route breaks or traffic disruptions. Traditional routing schemes cannot quickly adapt to dynamic UAV networks and/or incur large control overhead and delays. To establish stable, high-quality routes from target UAVs to the BS, we design a hybrid reactive routing scheme called pipe routing that is mobility, congestion, and energy-aware. The pipe routing scheme discovers routes on-demand and proactively switches to alternate high-quality routes within a limited region around the active routes (called the pipe) when needed, reducing the number of route breaks and increasing data throughput. We then design a novel topology control-based pipe routing scheme to maintain robust connectivity in the pipe region around the active routes, leading to improved route stability and increased throughput with minimal impact on the coverage performance of the UAV network.","sentences":["Routing protocols help in transmitting the sensed data from UAVs monitoring the targets (called target UAVs) to the BS.","However, the highly dynamic nature of an autonomous, decentralized UAV network leads to frequent route breaks or traffic disruptions.","Traditional routing schemes cannot quickly adapt to dynamic UAV networks and/or incur large control overhead and delays.","To establish stable, high-quality routes from target UAVs to the BS, we design a hybrid reactive routing scheme called pipe routing that is mobility, congestion, and energy-aware.","The pipe routing scheme discovers routes on-demand and proactively switches to alternate high-quality routes within a limited region around the active routes (called the pipe) when needed, reducing the number of route breaks and increasing data throughput.","We then design a novel topology control-based pipe routing scheme to maintain robust connectivity in the pipe region around the active routes, leading to improved route stability and increased throughput with minimal impact on the coverage performance of the UAV network."],"url":"http://arxiv.org/abs/2405.04678v1","category":"cs.NI"}
{"created":"2024-05-07 21:32:07","title":"Measures of maximal entropy for non-uniformly hyperbolic maps","abstract":"For $C^{1+}$ maps, possibly non-invertible and with singularities, we prove that each homoclinic class of an adapted hyperbolic measure carries at most one adapted hyperbolic measure of maximal entropy. We then apply this to study the finiteness/uniqueness of such measures in several different settings: finite horizon dispersing billiards, codimension one partially hyperbolic endomorphisms with \"large\" entropy, robustly non-uniformly hyperbolic volume-preserving endomorphisms as in Andersson-Carrasco-Saghin, and strongly transitive non-uniformly expanding maps.","sentences":["For $C^{1+}$ maps, possibly non-invertible and with singularities, we prove that each homoclinic class of an adapted hyperbolic measure carries at most one adapted hyperbolic measure of maximal entropy.","We then apply this to study the finiteness/uniqueness of such measures in several different settings: finite horizon dispersing billiards, codimension one partially hyperbolic endomorphisms with \"large\" entropy, robustly non-uniformly hyperbolic volume-preserving endomorphisms as in Andersson-Carrasco-Saghin, and strongly transitive non-uniformly expanding maps."],"url":"http://arxiv.org/abs/2405.04676v1","category":"math.DS"}
{"created":"2024-05-07 21:02:11","title":"Energy Bounds for Discontinuous Galerkin Spectral Element Approximations of Well-Posed Overset Grid Problems for Hyperbolic Systems","abstract":"We show that even though the Discontinuous Galerkin Spectral Element Method is stable for hyperbolic boundary-value problems, and the overset domain problem is well-posed in an appropriate norm, the energy of the approximation is bounded by data only for fixed polynomial order and time. In the absence of dissipation, coupling of the overlapping domains is destabilizing by allowing positive eigenvalues in the system to be integrated in time. This coupling can be stabilized in one space dimension by using the upwind numerical flux. To help provide additional dissipation, we introduce a novel penalty method that applies dissipation at arbitrary points within the overlap region and depends only on the difference between the solutions. We present numerical experiments in one space dimension to illustrate the implementation of the well-posed penalty formulation, and show spectral convergence of the approximations when dissipation is applied.","sentences":["We show that even though the Discontinuous Galerkin Spectral Element Method is stable for hyperbolic boundary-value problems, and the overset domain problem is well-posed in an appropriate norm, the energy of the approximation is bounded by data only for fixed polynomial order and time.","In the absence of dissipation, coupling of the overlapping domains is destabilizing by allowing positive eigenvalues in the system to be integrated in time.","This coupling can be stabilized in one space dimension by using the upwind numerical flux.","To help provide additional dissipation, we introduce a novel penalty method that applies dissipation at arbitrary points within the overlap region and depends only on the difference between the solutions.","We present numerical experiments in one space dimension to illustrate the implementation of the well-posed penalty formulation, and show spectral convergence of the approximations when dissipation is applied."],"url":"http://arxiv.org/abs/2405.04668v1","category":"math.NA"}
{"created":"2024-05-07 19:11:45","title":"Adaptive design of experiments methodology for noise resistance with unreplicated experiments","abstract":"A new gradient-based adaptive sampling method is proposed for design of experiments applications which balances space filling, local refinement, and error minimization objectives while reducing reliance on delicate tuning parameters. High order local maximum entropy approximants are used for metamodelling, which take advantage of boundary-corrected kernel density estimation to increase accuracy and robustness on highly clumped datasets, as well as conferring the resulting metamodel with some robustness against data noise in the common case of unreplicated experiments. Two-dimensional test cases are analyzed against full factorial and latin hypercube designs and compare favourably. The proposed method is then applied in a unique manner to the problem of adaptive spatial resolution in time-varying non-linear functions, opening up the possibility to adapt the method to solve partial differential equations.","sentences":["A new gradient-based adaptive sampling method is proposed for design of experiments applications which balances space filling, local refinement, and error minimization objectives while reducing reliance on delicate tuning parameters.","High order local maximum entropy approximants are used for metamodelling, which take advantage of boundary-corrected kernel density estimation to increase accuracy and robustness on highly clumped datasets, as well as conferring the resulting metamodel with some robustness against data noise in the common case of unreplicated experiments.","Two-dimensional test cases are analyzed against full factorial and latin hypercube designs and compare favourably.","The proposed method is then applied in a unique manner to the problem of adaptive spatial resolution in time-varying non-linear functions, opening up the possibility to adapt the method to solve partial differential equations."],"url":"http://arxiv.org/abs/2405.04624v1","category":"stat.ME"}
{"created":"2024-05-07 18:00:02","title":"A Dissipative Dark Cosmology: From Early Matter Dominance to Delayed Compact Objects","abstract":"We demonstrate a novel mechanism for producing dark compact objects and black holes through a dark sector, where all the dark matter can be dissipative. Heavy dark sector particles with masses above $10^4$ GeV can come to dominate the Universe and yield an early matter-dominated era before Big Bang Nucleosynthesis (BBN). Density perturbations in this epoch can grow and collapse into tiny dark matter halos, which cool via self interactions. The typical halo size is set by the Hubble length once perturbations begin growing, offering a straightforward prediction of the halo size and evolution depending on ones choice of dark matter model. Once these primordial halos have formed, a thermal phase transition can then shift the Universe back into radiation domination and standard cosmology. These halos can continue to collapse after BBN, resulting in the late-time formation of fragmented dark compact objects and sub-solar mass primordial black holes. We find that these compact objects can constitute a sizable fraction of all of dark matter. The resulting fragments can have masses between $10^{20}$ g to $10^{32}$ g, with radii ranging from $10^{-2}$ m to $10^5$ m, while the black holes can have masses between $10^{8}$ g to $10^{34}$ g. Furthermore, a unique feature of this model is the late-time formation of black holes which can evaporate away today. We compare where these objects lie with respect to current primordial black hole and MACHO constraints.","sentences":["We demonstrate a novel mechanism for producing dark compact objects and black holes through a dark sector, where all the dark matter can be dissipative.","Heavy dark sector particles with masses above $10^4$ GeV can come to dominate the Universe and yield an early matter-dominated era before Big Bang Nucleosynthesis (BBN).","Density perturbations in this epoch can grow and collapse into tiny dark matter halos, which cool via self interactions.","The typical halo size is set by the Hubble length once perturbations begin growing, offering a straightforward prediction of the halo size and evolution depending on ones choice of dark matter model.","Once these primordial halos have formed, a thermal phase transition can then shift the Universe back into radiation domination and standard cosmology.","These halos can continue to collapse after BBN, resulting in the late-time formation of fragmented dark compact objects and sub-solar mass primordial black holes.","We find that these compact objects can constitute a sizable fraction of all of dark matter.","The resulting fragments can have masses between $10^{20}$ g to $10^{32}$ g, with radii ranging from $10^{-2}$ m to $10^5$ m, while the black holes can have masses between $10^{8}$ g to $10^{34}$ g.","Furthermore, a unique feature of this model is the late-time formation of black holes which can evaporate away today.","We compare where these objects lie with respect to current primordial black hole and MACHO constraints."],"url":"http://arxiv.org/abs/2405.04575v1","category":"hep-ph"}
{"created":"2024-05-07 18:00:00","title":"Baryon-number -flavor separation in the topological expansion of QCD","abstract":"Gauge invariance of QCD dictates the presence of string junctions in the wave functions of baryons. In high-energy inclusive processes, these baryon junctions have been predicted to induce the separation of the flows of baryon number and flavor. In this paper we describe this phenomenon using the analog-gas model of multiparticle production proposed long time ago by Feynman and Wilson and adapted here to accommodate the topological expansion in QCD. In this framework, duality arguments suggest the existence of two degenerate junction-antijunction glueball Regge trajectories of opposite $\\cal{C}$-parity with intercept close to 1/2. The corresponding results for the energy and rapidity dependence of baryon stopping are in reasonably good agreement with recent experimental findings from STAR and ALICE experiments. We show that accounting for correlations between the fragmenting strings further improves agreement with the data, and outline additional experimental tests of our picture at the existing (RHIC, LHC, JLab) and future (EIC) facilities.","sentences":["Gauge invariance of QCD dictates the presence of string junctions in the wave functions of baryons.","In high-energy inclusive processes, these baryon junctions have been predicted to induce the separation of the flows of baryon number and flavor.","In this paper we describe this phenomenon using the analog-gas model of multiparticle production proposed long time ago by Feynman and Wilson and adapted here to accommodate the topological expansion in QCD.","In this framework, duality arguments suggest the existence of two degenerate junction-antijunction glueball Regge trajectories of opposite $\\cal{C}$-parity with intercept close to 1/2.","The corresponding results for the energy and rapidity dependence of baryon stopping are in reasonably good agreement with recent experimental findings from STAR and ALICE experiments.","We show that accounting for correlations between the fragmenting strings further improves agreement with the data, and outline additional experimental tests of our picture at the existing (RHIC, LHC, JLab) and future (EIC) facilities."],"url":"http://arxiv.org/abs/2405.04569v1","category":"hep-ph"}
{"created":"2024-05-07 17:53:12","title":"SCExAO/CHARIS Multi-Wavelength, High-Contrast Imaging of the BD+45$^\\circ$598 Debris Disk","abstract":"We present a multi-wavelength (1.16$\\mu$m-2.37$\\mu$m) view of the debris disk around BD+45$^\\circ$598, using the Subaru Coronagraphic Extreme Adaptive Optics system paired with the Coronagraphic High Angular Resolution Imaging Spectrograph. With an assumed age of 23 Myr, this source allows us to study the early evolution of debris disks and search for forming planets. We fit a scattered light model to our disk using a differential evolution algorithm, and constrain its geometry. We find the disk to have a peak density radius of $R_0 = 109.6$ au, an inclination of $i = 88.1^\\circ$, and position angle $PA = 111.0^\\circ$. While we do not detect a substellar companion in the disk, our calculated contrast limits indicate sensitivity to planets as small as $\\sim 10 M_{\\rm Jup}$ at a projected separation of 12 au of the star, and as small as $\\sim 4 M_{\\rm Jup}$ beyond 38 au. When measuring intensity as a function of wavelength, the disk color constrains the minimum dust grain size within a range of $\\sim0.13$ to 1.01$\\mu$m.","sentences":["We present a multi-wavelength (1.16$\\mu$m-2.37$\\mu$m) view of the debris disk around BD+45$^\\circ$598, using the Subaru Coronagraphic Extreme Adaptive Optics system paired with the Coronagraphic High Angular Resolution Imaging Spectrograph.","With an assumed age of 23 Myr, this source allows us to study the early evolution of debris disks and search for forming planets.","We fit a scattered light model to our disk using a differential evolution algorithm, and constrain its geometry.","We find the disk to have a peak density radius of $R_0 = 109.6$ au, an inclination of $i = 88.1^\\circ$, and position angle $PA = 111.0^\\circ$.","While we do not detect a substellar companion in the disk, our calculated contrast limits indicate sensitivity to planets as small as $\\sim 10 M_{\\rm Jup}$ at a projected separation of 12 au of the star, and as small as $\\sim 4 M_{\\rm Jup}$ beyond 38 au.","When measuring intensity as a function of wavelength, the disk color constrains the minimum dust grain size within a range of $\\sim0.13$ to 1.01$\\mu$m."],"url":"http://arxiv.org/abs/2405.04521v1","category":"astro-ph.EP"}
{"created":"2024-05-07 17:00:19","title":"Resource-Efficient and Self-Adaptive Quantum Search in a Quantum-Classical Hybrid System","abstract":"Over the past decade, the rapid advancement of deep learning and big data applications has been driven by vast datasets and high-performance computing systems. However, as we approach the physical limits of semiconductor fabrication in the post-Moore's Law era, questions arise about the future of these applications. In parallel, quantum computing has made significant progress with the potential to break limits. Major companies like IBM, Google, and Microsoft provide access to noisy intermediate-scale quantum (NISQ) computers. Despite the theoretical promise of Shor's and Grover's algorithms, practical implementation on current quantum devices faces challenges, such as demanding additional resources and a high number of controlled operations. To tackle these challenges and optimize the utilization of limited onboard qubits, we introduce ReSaQuS, a resource-efficient index-value searching system within a quantum-classical hybrid framework. Building on Grover's algorithm, ReSaQuS employs an automatically managed iterative search approach. This method analyzes problem size, filters fewer probable data points, and progressively reduces the dataset with decreasing qubit requirements. Implemented using Qiskit and evaluated through extensive experiments, ReSaQuS has demonstrated a substantial reduction, up to 86.36\\% in cumulative qubit consumption and 72.72\\% in active periods, reinforcing its potential in optimizing quantum computing application deployment.","sentences":["Over the past decade, the rapid advancement of deep learning and big data applications has been driven by vast datasets and high-performance computing systems.","However, as we approach the physical limits of semiconductor fabrication in the post-Moore's Law era, questions arise about the future of these applications.","In parallel, quantum computing has made significant progress with the potential to break limits.","Major companies like IBM, Google, and Microsoft provide access to noisy intermediate-scale quantum (NISQ) computers.","Despite the theoretical promise of Shor's and Grover's algorithms, practical implementation on current quantum devices faces challenges, such as demanding additional resources and a high number of controlled operations.","To tackle these challenges and optimize the utilization of limited onboard qubits, we introduce ReSaQuS, a resource-efficient index-value searching system within a quantum-classical hybrid framework.","Building on Grover's algorithm, ReSaQuS employs an automatically managed iterative search approach.","This method analyzes problem size, filters fewer probable data points, and progressively reduces the dataset with decreasing qubit requirements.","Implemented using Qiskit and evaluated through extensive experiments, ReSaQuS has demonstrated a substantial reduction, up to 86.36\\% in cumulative qubit consumption and 72.72\\% in active periods, reinforcing its potential in optimizing quantum computing application deployment."],"url":"http://arxiv.org/abs/2405.04490v1","category":"cs.DC"}
{"created":"2024-05-07 16:53:42","title":"Adapting WavLM for Speech Emotion Recognition","abstract":"Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.","sentences":["Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention.","While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent.","In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus.","More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances.","We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024."],"url":"http://arxiv.org/abs/2405.04485v1","category":"cs.LG"}
{"created":"2024-05-07 16:32:26","title":"Universal Spatial Audio Transcoder","abstract":"This paper addresses the challenges associated with both the conversion between different spatial audio formats and the decoding of a spatial audio format to a specific loudspeaker layout. Existing approaches often rely on layout remapping tools, which may not guarantee optimal conversion from a psychoacoustic perspective. To overcome these challenges, we present the Universal Spatial Audio Transcoder(USAT) method and its corresponding open source implementation. USAT generates an optimal decoder or transcoder for any input spatial audio format, adapting it to any output format or 2D/3D loudspeaker configuration. Drawing upon optimization techniques based on psychoacoustic principles, the algorithm maximizes the preservation of spatial information. We present examples of the decoding and transcoding of several audio formats, and show that USAT approach is advantageous compared to the most common methods in the field.","sentences":["This paper addresses the challenges associated with both the conversion between different spatial audio formats and the decoding of a spatial audio format to a specific loudspeaker layout.","Existing approaches often rely on layout remapping tools, which may not guarantee optimal conversion from a psychoacoustic perspective.","To overcome these challenges, we present the Universal Spatial Audio Transcoder(USAT) method and its corresponding open source implementation.","USAT generates an optimal decoder or transcoder for any input spatial audio format, adapting it to any output format or 2D/3D loudspeaker configuration.","Drawing upon optimization techniques based on psychoacoustic principles, the algorithm maximizes the preservation of spatial information.","We present examples of the decoding and transcoding of several audio formats, and show that USAT approach is advantageous compared to the most common methods in the field."],"url":"http://arxiv.org/abs/2405.04471v1","category":"cs.SD"}
{"created":"2024-05-07 16:30:02","title":"A fully differentiable GNN-based PDE Solver: With Applications to Poisson and Navier-Stokes Equations","abstract":"In this study, we present a novel computational framework that integrates the finite volume method with graph neural networks to address the challenges in Physics-Informed Neural Networks(PINNs). Our approach leverages the flexibility of graph neural networks to adapt to various types of two-dimensional unstructured grids, enhancing the model's applicability across different physical equations and boundary conditions. The core innovation lies in the development of an unsupervised training algorithm that utilizes GPU parallel computing to implement a fully differentiable finite volume method discretization process. This method includes differentiable integral and gradient reconstruction algorithms, enabling the model to directly solve partial-differential equations(PDEs) during training without the need for pre-computed data. Our results demonstrate the model's superior mesh generalization and its capability to handle multiple boundary conditions simultaneously, significantly boosting its generalization capabilities. The proposed method not only shows potential for extensive applications in CFD but also establishes a new paradigm for integrating traditional numerical methods with deep learning technologies, offering a robust platform for solving complex physical problems.","sentences":["In this study, we present a novel computational framework that integrates the finite volume method with graph neural networks to address the challenges in Physics-Informed Neural Networks(PINNs).","Our approach leverages the flexibility of graph neural networks to adapt to various types of two-dimensional unstructured grids, enhancing the model's applicability across different physical equations and boundary conditions.","The core innovation lies in the development of an unsupervised training algorithm that utilizes GPU parallel computing to implement a fully differentiable finite volume method discretization process.","This method includes differentiable integral and gradient reconstruction algorithms, enabling the model to directly solve partial-differential equations(PDEs) during training without the need for pre-computed data.","Our results demonstrate the model's superior mesh generalization and its capability to handle multiple boundary conditions simultaneously, significantly boosting its generalization capabilities.","The proposed method not only shows potential for extensive applications in CFD but also establishes a new paradigm for integrating traditional numerical methods with deep learning technologies, offering a robust platform for solving complex physical problems."],"url":"http://arxiv.org/abs/2405.04466v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 15:35:43","title":"DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks","abstract":"Document image restoration is a crucial aspect of Document AI systems, as the quality of document images significantly influences the overall performance. Prevailing methods address distinct restoration tasks independently, leading to intricate systems and the incapability to harness the potential synergies of multi-task learning. To overcome this challenge, we propose DocRes, a generalist model that unifies five document image restoration tasks including dewarping, deshadowing, appearance enhancement, deblurring, and binarization. To instruct DocRes to perform various restoration tasks, we propose a novel visual prompt approach called Dynamic Task-Specific Prompt (DTSPrompt). The DTSPrompt for different tasks comprises distinct prior features, which are additional characteristics extracted from the input image. Beyond its role as a cue for task-specific execution, DTSPrompt can also serve as supplementary information to enhance the model's performance. Moreover, DTSPrompt is more flexible than prior visual prompt approaches as it can be seamlessly applied and adapted to inputs with high and variable resolutions. Experimental results demonstrate that DocRes achieves competitive or superior performance compared to existing state-of-the-art task-specific models. This underscores the potential of DocRes across a broader spectrum of document image restoration tasks. The source code is publicly available at https://github.com/ZZZHANG-jx/DocRes","sentences":["Document image restoration is a crucial aspect of Document AI systems, as the quality of document images significantly influences the overall performance.","Prevailing methods address distinct restoration tasks independently, leading to intricate systems and the incapability to harness the potential synergies of multi-task learning.","To overcome this challenge, we propose DocRes, a generalist model that unifies five document image restoration tasks including dewarping, deshadowing, appearance enhancement, deblurring, and binarization.","To instruct DocRes to perform various restoration tasks, we propose a novel visual prompt approach called Dynamic Task-Specific Prompt (DTSPrompt).","The DTSPrompt for different tasks comprises distinct prior features, which are additional characteristics extracted from the input image.","Beyond its role as a cue for task-specific execution, DTSPrompt can also serve as supplementary information to enhance the model's performance.","Moreover, DTSPrompt is more flexible than prior visual prompt approaches as it can be seamlessly applied and adapted to inputs with high and variable resolutions.","Experimental results demonstrate that DocRes achieves competitive or superior performance compared to existing state-of-the-art task-specific models.","This underscores the potential of DocRes across a broader spectrum of document image restoration tasks.","The source code is publicly available at https://github.com/ZZZHANG-jx/DocRes"],"url":"http://arxiv.org/abs/2405.04408v1","category":"cs.CV"}
{"created":"2024-05-07 15:14:49","title":"BILTS: A novel bi-invariant local trajectory-shape descriptor for rigid-body motion","abstract":"Measuring the similarity between motions and established motion models is crucial for motion analysis, recognition, generation, and adaptation. To enhance similarity measurement across diverse contexts, invariant motion descriptors have been proposed. However, for rigid-body motion, few invariant descriptors exist that are bi-invariant, meaning invariant to both the body and world reference frames used to describe the motion. Moreover, their robustness to singularities is limited. This paper introduces a novel Bi-Invariant Local Trajectory-Shape descriptor (BILTS) and a corresponding dissimilarity measure. Mathematical relationships between BILTS and existing descriptors are derived, providing new insights into their properties. The paper also includes an algorithm to reproduce the motion from the BILTS descriptor, demonstrating its bidirectionality and usefulness for trajectory generation. Experimental validation using datasets of daily-life activities shows the higher robustness of the BILTS descriptor compared to the bi-invariant ISA descriptor. This higher robustness supports the further application of bi-invariant descriptors for motion recognition and generalization.","sentences":["Measuring the similarity between motions and established motion models is crucial for motion analysis, recognition, generation, and adaptation.","To enhance similarity measurement across diverse contexts, invariant motion descriptors have been proposed.","However, for rigid-body motion, few invariant descriptors exist that are bi-invariant, meaning invariant to both the body and world reference frames used to describe the motion.","Moreover, their robustness to singularities is limited.","This paper introduces a novel Bi-Invariant Local Trajectory-Shape descriptor (BILTS) and a corresponding dissimilarity measure.","Mathematical relationships between BILTS and existing descriptors are derived, providing new insights into their properties.","The paper also includes an algorithm to reproduce the motion from the BILTS descriptor, demonstrating its bidirectionality and usefulness for trajectory generation.","Experimental validation using datasets of daily-life activities shows the higher robustness of the BILTS descriptor compared to the bi-invariant ISA descriptor.","This higher robustness supports the further application of bi-invariant descriptors for motion recognition and generalization."],"url":"http://arxiv.org/abs/2405.04392v1","category":"cs.RO"}
{"created":"2024-05-07 15:07:47","title":"Topology and $\\mathcal{PT}$ Symmetry in a Non-Hermitian Su-Schrieffer-Heeger Chain with Periodic Hopping Modulation","abstract":"We study the effect of periodic hopping modulation on a Su-Schrieffer-Heeger (SSH) chain that exhibits non-Hermiticity in presence of an onsite staggered imaginary potential. This dissipative, non-Hermitian (NH) extension amply modifies the features of the topological trivial phase (TTP) and the topological nontrivial phase (TNP) of the SSH chain. Though a weak potential can respect the parity-time ($\\mathcal{PT}$) symmetry keeping the energy eigenvalues real, a strong potential breaks $\\mathcal{PT}$ conservation leading to imaginary end state and complex bulk state energies in the system. Furthermore for large commensurate periodicity of the hopping, in-gap states appear that take either purely real or purely imaginary eigenvalues depending on the strenth of both NH potential and hopping modulation. In particular, this paper is engaged with hopping periodicities of 2, 4 and 8 lattice spacings. The localization of end states and in-gap states at the boundaries are investigated for those hopping periodicities. Though we find that topology and $\\mathcal{PT}$ symmetry are not very directly connected, distinguishing distribution of $\\mathcal{PT}$ broken and unbroken phases are clearly observed within TNP and TTP in our systems.","sentences":["We study the effect of periodic hopping modulation on a Su-Schrieffer-Heeger (SSH) chain that exhibits non-Hermiticity in presence of an onsite staggered imaginary potential.","This dissipative, non-Hermitian (NH) extension amply modifies the features of the topological trivial phase (TTP) and the topological nontrivial phase (TNP) of the SSH chain.","Though a weak potential can respect the parity-time ($\\mathcal{PT}$) symmetry keeping the energy eigenvalues real, a strong potential breaks $\\mathcal{PT}$ conservation leading to imaginary end state and complex bulk state energies in the system.","Furthermore for large commensurate periodicity of the hopping, in-gap states appear that take either purely real or purely imaginary eigenvalues depending on the strenth of both NH potential and hopping modulation.","In particular, this paper is engaged with hopping periodicities of 2, 4 and 8 lattice spacings.","The localization of end states and in-gap states at the boundaries are investigated for those hopping periodicities.","Though we find that topology and $\\mathcal{PT}$ symmetry are not very directly connected, distinguishing distribution of $\\mathcal{PT}$ broken and unbroken phases are clearly observed within TNP and TTP in our systems."],"url":"http://arxiv.org/abs/2405.04562v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 15:00:11","title":"Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing","abstract":"Scene text images contain not only style information (font, background) but also content information (character, texture). Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance. We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need). Specifically, we synthesize a dataset of image pairs with identical style but different content. Based on the dataset, we decouple the two types of features by the supervision design. Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs. Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content. Such an operation effectively decouples the features based on their distinctive properties. To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images. Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing.","sentences":["Scene text images contain not only style information (font, background) but also content information (character, texture).","Different scene text tasks need different information, but previous representation learning methods use tightly coupled features for all tasks, resulting in sub-optimal performance.","We propose a Disentangled Representation Learning framework (DARLING) aimed at disentangling these two types of features for improved adaptability in better addressing various downstream tasks (choose what you really need).","Specifically, we synthesize a dataset of image pairs with identical style but different content.","Based on the dataset, we decouple the two types of features by the supervision design.","Clearly, we directly split the visual representation into style and content features, the content features are supervised by a text recognition loss, while an alignment loss aligns the style features in the image pairs.","Then, style features are employed in reconstructing the counterpart image via an image decoder with a prompt that indicates the counterpart's content.","Such an operation effectively decouples the features based on their distinctive properties.","To the best of our knowledge, this is the first time in the field of scene text that disentangles the inherent properties of the text images.","Our method achieves state-of-the-art performance in Scene Text Recognition, Removal, and Editing."],"url":"http://arxiv.org/abs/2405.04377v1","category":"cs.CV"}
{"created":"2024-05-07 14:58:12","title":"Towards Stability of Parameter-free Optimization","abstract":"Hyperparameter tuning, particularly the selection of an appropriate learning rate in adaptive gradient training methods, remains a challenge. To tackle this challenge, in this paper, we propose a novel parameter-free optimizer, AdamG (Adam with the golden step size), designed to automatically adapt to diverse optimization problems without manual tuning. The core technique underlying AdamG is our golden step size derived for the AdaGrad-Norm algorithm, which is expected to help AdaGrad-Norm preserve the tuning-free convergence and approximate the optimal step size in expectation w.r.t. various optimization scenarios. To better evaluate tuning-free performance, we propose a novel evaluation criterion, stability, to comprehensively assess the efficacy of parameter-free optimizers in addition to classical performance criteria. Empirical results demonstrate that compared with other parameter-free baselines, AdamG achieves superior performance, which is consistently on par with Adam using a manually tuned learning rate across various optimization tasks.","sentences":["Hyperparameter tuning, particularly the selection of an appropriate learning rate in adaptive gradient training methods, remains a challenge.","To tackle this challenge, in this paper, we propose a novel parameter-free optimizer, AdamG (Adam with the golden step size), designed to automatically adapt to diverse optimization problems without manual tuning.","The core technique underlying AdamG is our golden step size derived for the AdaGrad-Norm algorithm, which is expected to help AdaGrad-Norm preserve the tuning-free convergence and approximate the optimal step size in expectation w.r.t.","various optimization scenarios.","To better evaluate tuning-free performance, we propose a novel evaluation criterion, stability, to comprehensively assess the efficacy of parameter-free optimizers in addition to classical performance criteria.","Empirical results demonstrate that compared with other parameter-free baselines, AdamG achieves superior performance, which is consistently on par with Adam using a manually tuned learning rate across various optimization tasks."],"url":"http://arxiv.org/abs/2405.04376v1","category":"cs.LG"}
{"created":"2024-05-07 14:36:33","title":"A Personalizable Controller for the Walking Assistive omNi-Directional Exo-Robot (WANDER)","abstract":"Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance. However, existing walking aids are either inadequate to provide sufficient support to users' stability or too bulky and poorly maneuverable to be used outside hospital environments. In addition, they all lack adaptability to individual requirements. To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot. It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support. A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users' intentions, which are translated into command velocities by means of a variable admittance controller. An optimization technique based on users' preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics. Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort. The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users' comfort and mobility while ensuring reliable physical support.","sentences":["Preserving and encouraging mobility in the elderly and adults with chronic conditions is of paramount importance.","However, existing walking aids are either inadequate to provide sufficient support to users' stability or too bulky and poorly maneuverable to be used outside hospital environments.","In addition, they all lack adaptability to individual requirements.","To address these challenges, this paper introduces WANDER, a novel Walking Assistive omNi-Directional Exo-Robot.","It consists of an omnidirectional platform and a robust aluminum structure mounted on top of it, which provides partial body weight support.","A comfortable and minimally restrictive coupling interface embedded with a force/torque sensor allows to detect users' intentions, which are translated into command velocities by means of a variable admittance controller.","An optimization technique based on users' preferences, i.e., Preference-Based Optimization (PBO) guides the choice of the admittance parameters (i.e., virtual mass and damping) to better fit subject-specific needs and characteristics.","Experiments with twelve healthy subjects exhibited a significant decrease in energy consumption and jerk when using WANDER with PBO parameters as well as improved user performance and comfort.","The great interpersonal variability in the optimized parameters highlights the importance of personalized control settings when walking with an assistive device, aiming to enhance users' comfort and mobility while ensuring reliable physical support."],"url":"http://arxiv.org/abs/2405.04359v1","category":"cs.RO"}
{"created":"2024-05-07 13:35:58","title":"Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion Transformer","abstract":"Diffusion models have shown remarkable performance in image generation in recent years. However, due to a quadratic increase in memory during generating ultra-high-resolution images (e.g. 4096*4096), the resolution of generated images is often limited to 1024*1024. In this work. we propose a unidirectional block attention mechanism that can adaptively adjust the memory overhead during the inference process and handle global dependencies. Building on this module, we adopt the DiT structure for upsampling and develop an infinite super-resolution model capable of upsampling images of various shapes and resolutions. Comprehensive experiments show that our model achieves SOTA performance in generating ultra-high-resolution images in both machine and human evaluation. Compared to commonly used UNet structures, our model can save more than 5x memory when generating 4096*4096 images. The project URL is https://github.com/THUDM/Inf-DiT.","sentences":["Diffusion models have shown remarkable performance in image generation in recent years.","However, due to a quadratic increase in memory during generating ultra-high-resolution images (e.g. 4096*4096), the resolution of generated images is often limited to 1024*1024.","In this work.","we propose a unidirectional block attention mechanism that can adaptively adjust the memory overhead during the inference process and handle global dependencies.","Building on this module, we adopt the DiT structure for upsampling and develop an infinite super-resolution model capable of upsampling images of various shapes and resolutions.","Comprehensive experiments show that our model achieves SOTA performance in generating ultra-high-resolution images in both machine and human evaluation.","Compared to commonly used UNet structures, our model can save more than 5x memory when generating 4096*4096 images.","The project URL is https://github.com/THUDM/Inf-DiT."],"url":"http://arxiv.org/abs/2405.04312v2","category":"cs.CV"}
{"created":"2024-05-07 13:33:52","title":"Time-asymptotics of a heated string","abstract":"In the present paper, we study a model of a thermoelastic string that is initially heated. We classify all the possible asymptotic states when time tends to infinity of such a model. Actually, we show that whatever the initial data is, a heated string must converge to a flat, steady string with uniformly distributed heat. The latter distribution is calculated from the energy conservation. In order to obtain the result, we need to take a few steps. In the first two steps, time-independent bounds from above and from below (by a positive constant) of the temperature are obtained. This is done via the Moser-like iteration. The lower bound is obtained via the Moser iteration on the negative part of the logarithm of temperature. In the third step, we obtain a time-independent higher-order estimate, which yields compactness of a sequence of the values of the solution when time tends to infinity. Here, an estimate involving the Fisher information of temperature, together with a recent functional inequality from \\cite{CFHS} and an $L^2(L^2)$ estimate of the gradient of entropy, enable us to arrive at a tricky Gr\\\"{o}nwall type inequality. Finally, in the last steps, we define the dynamical system on a proper functional phase space and study its $\\omega$-limit set. To this end, we use, in particular, the quantitative version of the second principle of thermodynamics. Also, the entropy dissipation term and the bound of the entropy from below are useful when identifying the structure of the $\\omega$-limit set.","sentences":["In the present paper, we study a model of a thermoelastic string that is initially heated.","We classify all the possible asymptotic states when time tends to infinity of such a model.","Actually, we show that whatever the initial data is, a heated string must converge to a flat, steady string with uniformly distributed heat.","The latter distribution is calculated from the energy conservation.","In order to obtain the result, we need to take a few steps.","In the first two steps, time-independent bounds from above and from below (by a positive constant) of the temperature are obtained.","This is done via the Moser-like iteration.","The lower bound is obtained via the Moser iteration on the negative part of the logarithm of temperature.","In the third step, we obtain a time-independent higher-order estimate, which yields compactness of a sequence of the values of the solution when time tends to infinity.","Here, an estimate involving the Fisher information of temperature, together with a recent functional inequality from \\cite{CFHS} and an $L^2(L^2)$ estimate of the gradient of entropy, enable us to arrive at a tricky Gr\\\"{o}nwall type inequality.","Finally, in the last steps, we define the dynamical system on a proper functional phase space and study its $\\omega$-limit set.","To this end, we use, in particular, the quantitative version of the second principle of thermodynamics.","Also, the entropy dissipation term and the bound of the entropy from below are useful when identifying the structure of the $\\omega$-limit set."],"url":"http://arxiv.org/abs/2405.04310v1","category":"math.AP"}
{"created":"2024-05-07 13:33:50","title":"Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling","abstract":"Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively studied and great progress has been made, there are still key challenges that hinder their broad real-world applications: 1) the inherent motion/rotation ambiguity requires either explicit camera motion recovery with extra constraint or complex Procrustean Alignment; 2) existing low-rank modeling of the global shape can over-penalize drastic deformations in the 3D shape sequence. This paper proposes to resolve the above issues from a spatial-temporal modeling perspective. First, we propose a novel Temporally-smooth Procrustean Alignment module that estimates 3D deforming shapes and adjusts the camera motion by aligning the 3D shape sequence consecutively. Our new alignment module remedies the requirement of complex reference 3D shape during alignment, which is more conductive to non-isotropic deformation modeling. Second, we propose a spatial-weighted approach to enforce the low-rank constraint adaptively at different locations to accommodate drastic spatially-variant deformation reconstruction better. Our modeling outperform existing low-rank based methods, and extensive experiments across different datasets validate the effectiveness of our method.","sentences":["Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively studied and great progress has been made, there are still key challenges that hinder their broad real-world applications: 1) the inherent motion/rotation ambiguity requires either explicit camera motion recovery with extra constraint or complex Procrustean Alignment; 2) existing low-rank modeling of the global shape can over-penalize drastic deformations in the 3D shape sequence.","This paper proposes to resolve the above issues from a spatial-temporal modeling perspective.","First, we propose a novel Temporally-smooth Procrustean Alignment module that estimates 3D deforming shapes and adjusts the camera motion by aligning the 3D shape sequence consecutively.","Our new alignment module remedies the requirement of complex reference 3D shape during alignment, which is more conductive to non-isotropic deformation modeling.","Second, we propose a spatial-weighted approach to enforce the low-rank constraint adaptively at different locations to accommodate drastic spatially-variant deformation reconstruction better.","Our modeling outperform existing low-rank based methods, and extensive experiments across different datasets validate the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.04309v1","category":"cs.CV"}
{"created":"2024-05-07 12:44:14","title":"Alfv\u00e9n waves at low Magnetic Reynolds number","abstract":"This paper seeks whether Alfv\\'en waves (AW) can be produced in laboratory-scale liquid metal experiments, \\emph{i.e.) at low-magnetic Reynolds Number ($R\\!m$). AW are incompressible waves propagating along magnetic fields typically found geo and astrophysical systems. Until now, only faint linear waves have been experimentally produced in liquid metals because of the large magnetic dissipation they undergo when $R\\!m\\ll1$. Yet, controlling laboratory AW could emulate such far remote processes as anomalous heating in the solar corona, oscillations of the Earth inner core or turbulence in the solar wind. To answer this question, we force AW with an AC electric current in a liquid metal channel in a transverse magnetic field. We derive a wave-bearing extension of the usual low$-R\\!m$ MHD approximation to identify two linear regimes: The purely diffusive regime exists when $N_\\omega$, the ratio of the oscillation period to the timescale of diffusive two-dimensionalisation by the Lorentz force, is small. The propagative regime is governed by the ratio of the forcing period to the AW propagation timescale which, we call the Jameson number $J\\!a$ after Jameson (1964), JFM. In this regime, AW are dissipative and dispersive as they propagate more slowly where velocity gradients are higher. Both regimes are recovered in the FLOWCUBE experiment, in excellent agreement with the model up to $J\\!a \\lesssim 0.85$ but near the $J\\!a=1$ resonance, high amplitude waves become clearly nonlinear. Hence, in electrically driving AW, we were able to produce some of the propagative, diffusive and nonlinear processes of astro and geophysical AW.","sentences":["This paper seeks whether Alfv\\'en waves (AW) can be produced in laboratory-scale liquid metal experiments, \\emph{i.e.)","at low-magnetic Reynolds Number ($R\\!m$).","AW are incompressible waves propagating along magnetic fields typically found geo and astrophysical systems.","Until now, only faint linear waves have been experimentally produced in liquid metals because of the large magnetic dissipation they undergo when $R\\!m\\ll1$. Yet, controlling laboratory AW could emulate such far remote processes as anomalous heating in the solar corona, oscillations of the Earth inner core or turbulence in the solar wind.","To answer this question, we force AW with an AC electric current in a liquid metal channel in a transverse magnetic field.","We derive a wave-bearing extension of the usual low$-R\\!m$ MHD approximation to identify two linear regimes: The purely diffusive regime exists when $N_\\omega$, the ratio of the oscillation period to the timescale of diffusive two-dimensionalisation by the Lorentz force, is small.","The propagative regime is governed by the ratio of the forcing period to the AW propagation timescale which, we call the Jameson number $J\\!a$ after Jameson (1964), JFM.","In this regime, AW are dissipative and dispersive as they propagate more slowly where velocity gradients are higher.","Both regimes are recovered in the FLOWCUBE experiment, in excellent agreement with the model up to $J\\!a \\lesssim 0.85$ but near the $J\\!a=1$ resonance, high amplitude waves become clearly nonlinear.","Hence, in electrically driving AW, we were able to produce some of the propagative, diffusive and nonlinear processes of astro and geophysical AW."],"url":"http://arxiv.org/abs/2405.04276v2","category":"physics.flu-dyn"}
{"created":"2024-05-07 12:43:10","title":"Grey-box Recursive Parameter Identification of a Nonlinear Dynamic Model for Mineral Flotation","abstract":"This study presents a grey-box recursive identification technique to estimate key parameters in a mineral flotation process across two scenarios. The method is applied to a nonlinear physics-based dynamic model validated at a laboratory scale, allowing real-time updates of two model parameters, n and C, in response to changing conditions. The proposed approach effectively adapts to process variability and allows for continuous adjustments based on operational fluctuations, resulting in a significantly improved estimation of concentrate grade - one key performance indicator. In Scenario 1, parameters n and C achieved fit metrics of 97.99 and 96.86, respectively, with concentrate grade estimations improving from 75.1 to 98.69 using recursive identification. In Scenario 2, the fit metrics for n and C were 96.27 and 95.48, respectively, with the concentrate grade estimations increasing from 96.27 to 99.45 with recursive identification. The results demonstrate the effectiveness of the proposed grey-box recursive identification method in accurately estimating parameters and predicting concentrate grade in a mineral flotation process.","sentences":["This study presents a grey-box recursive identification technique to estimate key parameters in a mineral flotation process across two scenarios.","The method is applied to a nonlinear physics-based dynamic model validated at a laboratory scale, allowing real-time updates of two model parameters, n and C, in response to changing conditions.","The proposed approach effectively adapts to process variability and allows for continuous adjustments based on operational fluctuations, resulting in a significantly improved estimation of concentrate grade - one key performance indicator.","In Scenario 1, parameters n and C achieved fit metrics of 97.99 and 96.86, respectively, with concentrate grade estimations improving from 75.1 to 98.69 using recursive identification.","In Scenario 2, the fit metrics for n and C were 96.27 and 95.48, respectively, with the concentrate grade estimations increasing from 96.27 to 99.45 with recursive identification.","The results demonstrate the effectiveness of the proposed grey-box recursive identification method in accurately estimating parameters and predicting concentrate grade in a mineral flotation process."],"url":"http://arxiv.org/abs/2405.04275v1","category":"eess.SY"}
{"created":"2024-05-07 12:42:23","title":"Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression","abstract":"Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents. Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding. To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters. This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution. Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration. Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time. We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark.","sentences":["Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents.","Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding.","To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU).","Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters.","This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution.","Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration.","Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time.","We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark."],"url":"http://arxiv.org/abs/2405.04274v1","category":"eess.IV"}
{"created":"2024-05-07 12:14:39","title":"Fermat Number Transform Based Chromatic Dispersion Compensation and Adaptive Equalization Algorithm","abstract":"By introducing the Fermat number transform into chromatic dispersion compensation and adaptive equalization, the computational complexity has been reduced by 68% compared with the con?ventional implementation. Experimental results validate its transmission performance with only 0.8 dB receiver sensitivity penalty in a 75 km-40 GBaud-PDM-16QAM system.","sentences":["By introducing the Fermat number transform into chromatic dispersion compensation and adaptive equalization, the computational complexity has been reduced by 68% compared with the con?ventional implementation.","Experimental results validate its transmission performance with only 0.8 dB receiver sensitivity penalty in a 75 km-40 GBaud-PDM-16QAM system."],"url":"http://arxiv.org/abs/2405.04253v1","category":"eess.SP"}
{"created":"2024-05-07 12:11:15","title":"A General Model for Detecting Learner Engagement: Implementation and Evaluation","abstract":"Considering learner engagement has a mutual benefit for both learners and instructors. Instructors can help learners increase their attention, involvement, motivation, and interest. On the other hand, instructors can improve their instructional performance by evaluating the cumulative results of all learners and upgrading their training programs. This paper proposes a general, lightweight model for selecting and processing features to detect learners' engagement levels while preserving the sequential temporal relationship over time. During training and testing, we analyzed the videos from the publicly available DAiSEE dataset to capture the dynamic essence of learner engagement. We have also proposed an adaptation policy to find new labels that utilize the affective states of this dataset related to education, thereby improving the models' judgment. The suggested model achieves an accuracy of 68.57\\% in a specific implementation and outperforms the studied state-of-the-art models detecting learners' engagement levels.","sentences":["Considering learner engagement has a mutual benefit for both learners and instructors.","Instructors can help learners increase their attention, involvement, motivation, and interest.","On the other hand, instructors can improve their instructional performance by evaluating the cumulative results of all learners and upgrading their training programs.","This paper proposes a general, lightweight model for selecting and processing features to detect learners' engagement levels while preserving the sequential temporal relationship over time.","During training and testing, we analyzed the videos from the publicly available DAiSEE dataset to capture the dynamic essence of learner engagement.","We have also proposed an adaptation policy to find new labels that utilize the affective states of this dataset related to education, thereby improving the models' judgment.","The suggested model achieves an accuracy of 68.57\\% in a specific implementation and outperforms the studied state-of-the-art models detecting learners' engagement levels."],"url":"http://arxiv.org/abs/2405.04251v1","category":"cs.CV"}
{"created":"2024-05-07 11:54:22","title":"LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe Diffusion-based Planning","abstract":"Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people. In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time. We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic ($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on $\\text{LTL}_f$ and guides the sampling steps using its gradient field. This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability. Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences.","sentences":["Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people.","In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time.","We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic ($\\text{LTL}_f$).","LTLDoG leverages a satisfaction value function on $\\text{LTL}_f$ and guides the sampling steps using its gradient field.","This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability.","Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences."],"url":"http://arxiv.org/abs/2405.04235v1","category":"cs.RO"}
{"created":"2024-05-07 11:46:36","title":"NEST: Neural Estimation by Sequential Testing","abstract":"Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements. With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures. If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space. Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward. In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection. We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions. We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration.","sentences":["Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements.","With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures.","If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space.","Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward.","In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection.","We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions.","We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration."],"url":"http://arxiv.org/abs/2405.04226v1","category":"stat.ME"}
{"created":"2024-05-07 11:20:04","title":"Darboux's Theorem, Lie series and the standardization of the Salerno and Ablowitz-Ladik models","abstract":"In the framework of nonlinear Hamiltonian lattices, we revisit the proof of Moser-Darboux's Theorem, in order to present a general scheme for its constructive applicability to Hamiltonian models with non-standard symplectic structures. We take as a guiding example the Salerno and Ablowitz-Ladik (AL) models: we justify the form of a well-known change of coordinates which is adapted to the Gauge symmetry, by showing that it comes out in a natural way within the general strategy outlined in the proof. Moreover, the full or truncated Lie-series technique in the extended phase-space is used to transform the Salerno model, at leading orders in the Darboux coordinates: thus the dNLS Hamiltonian turns out to be a normal form of the Salerno and AL models; as a byproduct we also get estimates of the dynamics of these models by means of dNLS one. We also stress that, once it is cast into the perturbative approach, the method allows to deal with the cases where the explicit trasformation is not known, or even worse it is not writable in terms of elementary functions.","sentences":["In the framework of nonlinear Hamiltonian lattices, we revisit the proof of Moser-Darboux's Theorem, in order to present a general scheme for its constructive applicability to Hamiltonian models with non-standard symplectic structures.","We take as a guiding example the Salerno and Ablowitz-Ladik (AL) models: we justify the form of a well-known change of coordinates which is adapted to the Gauge symmetry, by showing that it comes out in a natural way within the general strategy outlined in the proof.","Moreover, the full or truncated Lie-series technique in the extended phase-space is used to transform the Salerno model, at leading orders in the Darboux coordinates: thus the dNLS Hamiltonian turns out to be a normal form of the Salerno and AL models; as a byproduct we also get estimates of the dynamics of these models by means of dNLS one.","We also stress that, once it is cast into the perturbative approach, the method allows to deal with the cases where the explicit trasformation is not known, or even worse it is not writable in terms of elementary functions."],"url":"http://arxiv.org/abs/2405.04205v1","category":"math-ph"}
{"created":"2024-05-07 11:05:47","title":"Quantum software experiments: A reporting and laboratory package structure guidelines","abstract":"Background. In the realm of software engineering, there are widely accepted guidelines for reporting and creating laboratory packages. Unfortunately, the landscape differs considerably in the emerging field of quantum computing. To the best of our knowledge, no standardized guidelines exist for describing experiments or outlining the necessary structures for quantum software laboratory packages. Aims. This paper endeavors to enhance the replicability and verifiability of quantum software experiments. Method. This objective is pursued through the proposition of guidelines for reporting and the delineation of a structure for laboratory packages tailored to quantum computing experiments. Specifically, we advocate for an extension and adaption of established guidelines in experimental software engineering, integrating novel elements to address the specific requirements of quantum software engineering. Results. In validating the utility and effectiveness of the proposed guidelines, we conducted a review encompassing 11 works (5 focusing on reporting guidelines and 6 on laboratory packages). In particular, this review highlighted the absence of standardized guidelines and structure of laboratory packages for quantum software experiments. Conclusions. Our assessment revealed gaps in information and opportunities for enhancement within the evaluated papers and laboratory packages. Our proposal contributes to the advancement of quantum software engineering research, taking a fundamental step toward fostering rigorous and reliable scientific research in this emerging paradigm.","sentences":["Background.","In the realm of software engineering, there are widely accepted guidelines for reporting and creating laboratory packages.","Unfortunately, the landscape differs considerably in the emerging field of quantum computing.","To the best of our knowledge, no standardized guidelines exist for describing experiments or outlining the necessary structures for quantum software laboratory packages.","Aims.","This paper endeavors to enhance the replicability and verifiability of quantum software experiments.","Method.","This objective is pursued through the proposition of guidelines for reporting and the delineation of a structure for laboratory packages tailored to quantum computing experiments.","Specifically, we advocate for an extension and adaption of established guidelines in experimental software engineering, integrating novel elements to address the specific requirements of quantum software engineering.","Results.","In validating the utility and effectiveness of the proposed guidelines, we conducted a review encompassing 11 works (5 focusing on reporting guidelines and 6 on laboratory packages).","In particular, this review highlighted the absence of standardized guidelines and structure of laboratory packages for quantum software experiments.","Conclusions.","Our assessment revealed gaps in information and opportunities for enhancement within the evaluated papers and laboratory packages.","Our proposal contributes to the advancement of quantum software engineering research, taking a fundamental step toward fostering rigorous and reliable scientific research in this emerging paradigm."],"url":"http://arxiv.org/abs/2405.04192v1","category":"cs.SE"}
{"created":"2024-05-07 10:09:41","title":"D-TrAttUnet: Toward Hybrid CNN-Transformer Architecture for Generic and Subtle Segmentation in Medical Images","abstract":"Over the past two decades, machine analysis of medical imaging has advanced rapidly, opening up significant potential for several important medical applications. As complicated diseases increase and the number of cases rises, the role of machine-based imaging analysis has become indispensable. It serves as both a tool and an assistant to medical experts, providing valuable insights and guidance. A particularly challenging task in this area is lesion segmentation, a task that is challenging even for experienced radiologists. The complexity of this task highlights the urgent need for robust machine learning approaches to support medical staff. In response, we present our novel solution: the D-TrAttUnet architecture. This framework is based on the observation that different diseases often target specific organs. Our architecture includes an encoder-decoder structure with a composite Transformer-CNN encoder and dual decoders. The encoder includes two paths: the Transformer path and the Encoders Fusion Module path. The Dual-Decoder configuration uses two identical decoders, each with attention gates. This allows the model to simultaneously segment lesions and organs and integrate their segmentation losses.   To validate our approach, we performed evaluations on the Covid-19 and Bone Metastasis segmentation tasks. We also investigated the adaptability of the model by testing it without the second decoder in the segmentation of glands and nuclei. The results confirmed the superiority of our approach, especially in Covid-19 infections and the segmentation of bone metastases. In addition, the hybrid encoder showed exceptional performance in the segmentation of glands and nuclei, solidifying its role in modern medical image analysis.","sentences":["Over the past two decades, machine analysis of medical imaging has advanced rapidly, opening up significant potential for several important medical applications.","As complicated diseases increase and the number of cases rises, the role of machine-based imaging analysis has become indispensable.","It serves as both a tool and an assistant to medical experts, providing valuable insights and guidance.","A particularly challenging task in this area is lesion segmentation, a task that is challenging even for experienced radiologists.","The complexity of this task highlights the urgent need for robust machine learning approaches to support medical staff.","In response, we present our novel solution: the D-TrAttUnet architecture.","This framework is based on the observation that different diseases often target specific organs.","Our architecture includes an encoder-decoder structure with a composite Transformer-CNN encoder and dual decoders.","The encoder includes two paths: the Transformer path and the Encoders Fusion Module path.","The Dual-Decoder configuration uses two identical decoders, each with attention gates.","This allows the model to simultaneously segment lesions and organs and integrate their segmentation losses.   ","To validate our approach, we performed evaluations on the Covid-19 and Bone Metastasis segmentation tasks.","We also investigated the adaptability of the model by testing it without the second decoder in the segmentation of glands and nuclei.","The results confirmed the superiority of our approach, especially in Covid-19 infections and the segmentation of bone metastases.","In addition, the hybrid encoder showed exceptional performance in the segmentation of glands and nuclei, solidifying its role in modern medical image analysis."],"url":"http://arxiv.org/abs/2405.04169v1","category":"eess.IV"}
{"created":"2024-05-07 10:07:33","title":"Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised Domain Adaptation for Blind Image Quality Assessment","abstract":"The annotation of blind image quality assessment (BIQA) is labor-intensive and time-consuming, especially for authentic images. Training on synthetic data is expected to be beneficial, but synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that introducing more distortion types in the synthetic dataset may not improve or even be harmful to generalizing authentic image quality assessment. To solve this challenge, we propose distortion-guided unsupervised domain adaptation for BIQA (DGQA), a novel framework that leverages adaptive multi-domain selection via prior knowledge from distortion to match the data distribution between the source domains and the target domain, thereby reducing negative transfer from the outlier source domains. Extensive experiments on two cross-domain settings (synthetic distortion to authentic distortion and synthetic distortion to algorithmic distortion) have demonstrated the effectiveness of our proposed DGQA. Besides, DGQA is orthogonal to existing model-based BIQA methods, and can be used in combination with such models to improve performance with less training data.","sentences":["The annotation of blind image quality assessment (BIQA) is labor-intensive and time-consuming, especially for authentic images.","Training on synthetic data is expected to be beneficial, but synthetically trained models often suffer from poor generalization in real domains due to domain gaps.","In this work, we make a key observation that introducing more distortion types in the synthetic dataset may not improve or even be harmful to generalizing authentic image quality assessment.","To solve this challenge, we propose distortion-guided unsupervised domain adaptation for BIQA (DGQA), a novel framework that leverages adaptive multi-domain selection via prior knowledge from distortion to match the data distribution between the source domains and the target domain, thereby reducing negative transfer from the outlier source domains.","Extensive experiments on two cross-domain settings (synthetic distortion to authentic distortion and synthetic distortion to algorithmic distortion) have demonstrated the effectiveness of our proposed DGQA.","Besides, DGQA is orthogonal to existing model-based BIQA methods, and can be used in combination with such models to improve performance with less training data."],"url":"http://arxiv.org/abs/2405.04167v1","category":"cs.CV"}
{"created":"2024-05-07 10:00:38","title":"Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation","abstract":"Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos. We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations. We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin.","sentences":["Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages.","However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language.","We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation.","The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos.","We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations.","We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin."],"url":"http://arxiv.org/abs/2405.04164v1","category":"cs.CV"}
{"created":"2024-05-07 09:39:52","title":"Subdifferentially polynomially bounded functions and Gaussian smoothing-based zeroth-order optimization","abstract":"We introduce the class of subdifferentially polynomially bounded (SPB) functions, which is a rich class of locally Lipschitz functions that encompasses all Lipschitz functions, all gradient- or Hessian-Lipschitz functions, and even some non-smooth locally Lipschitz functions. We show that SPB functions are compatible with Gaussian smoothing (GS), in the sense that the GS of any SPB function is well-defined and satisfies a descent lemma akin to gradient-Lipschitz functions, with the Lipschitz constant replaced by a polynomial function. Leveraging this descent lemma, we propose GS-based zeroth-order optimization algorithms with an adaptive stepsize strategy for constrained minimization of SPB functions, and analyze their iteration complexity. An important instrument in our analysis, which could be of independent interest, is the quantification of Goldstein stationarity via the GS gradient.","sentences":["We introduce the class of subdifferentially polynomially bounded (SPB) functions, which is a rich class of locally Lipschitz functions that encompasses all Lipschitz functions, all gradient- or Hessian-Lipschitz functions, and even some non-smooth locally Lipschitz functions.","We show that SPB functions are compatible with Gaussian smoothing (GS), in the sense that the GS of any SPB function is well-defined and satisfies a descent lemma akin to gradient-Lipschitz functions, with the Lipschitz constant replaced by a polynomial function.","Leveraging this descent lemma, we propose GS-based zeroth-order optimization algorithms with an adaptive stepsize strategy for constrained minimization of SPB functions, and analyze their iteration complexity.","An important instrument in our analysis, which could be of independent interest, is the quantification of Goldstein stationarity via the GS gradient."],"url":"http://arxiv.org/abs/2405.04150v1","category":"math.OC"}
{"created":"2024-05-07 09:00:09","title":"Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method","abstract":"The generative model has made significant advancements in the creation of realistic videos, which causes security issues. However, this emerging risk has not been adequately addressed due to the absence of a benchmark dataset for AI-generated videos. In this paper, we first construct a video dataset using advanced diffusion-based video generation algorithms with various semantic contents. Besides, typical video lossy operations over network transmission are adopted to generate degraded samples. Then, by analyzing local and global temporal defects of current AI-generated videos, a novel detection framework by adaptively learning local motion information and global appearance variation is constructed to expose fake videos. Finally, experiments are conducted to evaluate the generalization and robustness of different spatial and temporal domain detection methods, where the results can serve as the baseline and demonstrate the research challenge for future studies.","sentences":["The generative model has made significant advancements in the creation of realistic videos, which causes security issues.","However, this emerging risk has not been adequately addressed due to the absence of a benchmark dataset for AI-generated videos.","In this paper, we first construct a video dataset using advanced diffusion-based video generation algorithms with various semantic contents.","Besides, typical video lossy operations over network transmission are adopted to generate degraded samples.","Then, by analyzing local and global temporal defects of current AI-generated videos, a novel detection framework by adaptively learning local motion information and global appearance variation is constructed to expose fake videos.","Finally, experiments are conducted to evaluate the generalization and robustness of different spatial and temporal domain detection methods, where the results can serve as the baseline and demonstrate the research challenge for future studies."],"url":"http://arxiv.org/abs/2405.04133v1","category":"cs.CV"}
{"created":"2024-05-07 08:50:25","title":"Refining Joint Text and Source Code Embeddings for Retrieval Task with Parameter-Efficient Fine-Tuning","abstract":"The latest developments in Natural Language Processing (NLP) have demonstrated remarkable progress in a code-text retrieval problem. As the Transformer-based models used in this task continue to increase in size, the computational costs and time required for end-to-end fine-tuning become substantial. This poses a significant challenge for adapting and utilizing these models when computational resources are limited. Motivated by these concerns, we propose a fine-tuning framework that leverages Parameter-Efficient Fine-Tuning (PEFT) techniques. Moreover, we adopt contrastive learning objectives to improve the quality of bimodal representations learned by transformer models. Additionally, for PEFT methods we provide extensive benchmarking, the lack of which has been highlighted as a crucial problem in the literature. Based on the thorough experimentation with the CodeT5+ model conducted on two datasets, we demonstrate that the proposed fine-tuning framework has the potential to improve code-text retrieval performance by tuning only 0.4% parameters at most.","sentences":["The latest developments in Natural Language Processing (NLP) have demonstrated remarkable progress in a code-text retrieval problem.","As the Transformer-based models used in this task continue to increase in size, the computational costs and time required for end-to-end fine-tuning become substantial.","This poses a significant challenge for adapting and utilizing these models when computational resources are limited.","Motivated by these concerns, we propose a fine-tuning framework that leverages Parameter-Efficient Fine-Tuning (PEFT) techniques.","Moreover, we adopt contrastive learning objectives to improve the quality of bimodal representations learned by transformer models.","Additionally, for PEFT methods we provide extensive benchmarking, the lack of which has been highlighted as a crucial problem in the literature.","Based on the thorough experimentation with the CodeT5+ model conducted on two datasets, we demonstrate that the proposed fine-tuning framework has the potential to improve code-text retrieval performance by tuning only 0.4% parameters at most."],"url":"http://arxiv.org/abs/2405.04126v1","category":"cs.LG"}
{"created":"2024-05-07 08:44:29","title":"Ranking-based Client Selection with Imitation Learning for Efficient Federated Learning","abstract":"Federated Learning (FL) enables multiple devices to collaboratively train a shared model while ensuring data privacy. The selection of participating devices in each training round critically affects both the model performance and training efficiency, especially given the vast heterogeneity in training capabilities and data distribution across devices. To address these challenges, we introduce a novel device selection solution called FedRank, which is an end-to-end, ranking-based approach that is pre-trained by imitation learning against state-of-the-art analytical approaches. It not only considers data and system heterogeneity at runtime but also adaptively and efficiently chooses the most suitable clients for model training. Specifically, FedRank views client selection in FL as a ranking problem and employs a pairwise training strategy for the smart selection process. Additionally, an imitation learning-based approach is designed to counteract the cold-start issues often seen in state-of-the-art learning-based approaches. Experimental results reveal that \\model~ boosts model accuracy by 5.2\\% to 56.9\\%, accelerates the training convergence up to $2.01 \\times$ and saves the energy consumption up to $40.1\\%$.","sentences":["Federated Learning (FL) enables multiple devices to collaboratively train a shared model while ensuring data privacy.","The selection of participating devices in each training round critically affects both the model performance and training efficiency, especially given the vast heterogeneity in training capabilities and data distribution across devices.","To address these challenges, we introduce a novel device selection solution called FedRank, which is an end-to-end, ranking-based approach that is pre-trained by imitation learning against state-of-the-art analytical approaches.","It not only considers data and system heterogeneity at runtime but also adaptively and efficiently chooses the most suitable clients for model training.","Specifically, FedRank views client selection in FL as a ranking problem and employs a pairwise training strategy for the smart selection process.","Additionally, an imitation learning-based approach is designed to counteract the cold-start issues often seen in state-of-the-art learning-based approaches.","Experimental results reveal that \\model~ boosts model accuracy by 5.2\\% to 56.9\\%, accelerates the training convergence up to $2.01 \\times$ and saves the energy consumption up to $40.1\\%$."],"url":"http://arxiv.org/abs/2405.04122v1","category":"cs.LG"}
{"created":"2024-05-07 08:28:51","title":"Adaptive Least Mean pth Power Graph Neural Networks","abstract":"In the presence of impulsive noise, and missing observations, accurate online prediction of time-varying graph signals poses a crucial challenge in numerous application domains. We propose the Adaptive Least Mean $p^{th}$ Power Graph Neural Networks (LMP-GNN), a universal framework combining adaptive filter and graph neural network for online graph signal estimation. LMP-GNN retains the advantage of adaptive filtering in handling noise and missing observations as well as the online update capability. The incorporated graph neural network within the LMP-GNN can train and update filter parameters online instead of predefined filter parameters in previous methods, outputting more accurate prediction results. The adaptive update scheme of the LMP-GNN follows the solution of a $l_p$-norm optimization, rooting to the minimum dispersion criterion, and yields robust estimation results for time-varying graph signals under impulsive noise. A special case of LMP-GNN named the Sign-GNN is also provided and analyzed, Experiment results on two real-world datasets of temperature graph and traffic graph under four different noise distributions prove the effectiveness and robustness of our proposed LMP-GNN.","sentences":["In the presence of impulsive noise, and missing observations, accurate online prediction of time-varying graph signals poses a crucial challenge in numerous application domains.","We propose the Adaptive Least Mean $p^{th}$ Power Graph Neural Networks (LMP-GNN), a universal framework combining adaptive filter and graph neural network for online graph signal estimation.","LMP-GNN retains the advantage of adaptive filtering in handling noise and missing observations as well as the online update capability.","The incorporated graph neural network within the LMP-GNN can train and update filter parameters online instead of predefined filter parameters in previous methods, outputting more accurate prediction results.","The adaptive update scheme of the LMP-GNN follows the solution of a $l_p$-norm optimization, rooting to the minimum dispersion criterion, and yields robust estimation results for time-varying graph signals under impulsive noise.","A special case of LMP-GNN named the Sign-GNN is also provided and analyzed, Experiment results on two real-world datasets of temperature graph and traffic graph under four different noise distributions prove the effectiveness and robustness of our proposed LMP-GNN."],"url":"http://arxiv.org/abs/2405.04111v1","category":"cs.LG"}
{"created":"2024-05-07 08:25:12","title":"The Malware as a Service ecosystem","abstract":"The goal of this chapter is to illuminate the operational frameworks, key actors, and significant cybersecurity implications of the Malware as a Service (MaaS) ecosystem. Highlighting the transformation of malware proliferation into a service-oriented model, the chapter discusses how MaaS democratises access to sophisticated cyberattack capabilities, enabling even those with minimal technical knowledge to execute catastrophic cyberattacks. The discussion extends to the roles within the MaaS ecosystem, including malware developers, affiliates, initial access brokers, and the essential infrastructure providers that support these nefarious activities. The study emphasises the profound challenges MaaS poses to traditional cybersecurity defences, rendered ineffective against the constantly evolving and highly adaptable threats generated by MaaS platforms. With the increase in malware sophistication, there is a parallel call for a paradigm shift in defensive strategies, advocating for dynamic analysis, behavioural detection, and the integration of AI and machine learning techniques. By exploring the intricacies of the MaaS ecosystem, including the economic motivations driving its growth and the blurred lines between legitimate service models and cyber crime, the chapter presents a comprehensive overview intended to foster a deeper understanding among researchers and cybersecurity professionals. The ultimate goal is to aid in developing more effective strategies for combating the spread of commoditised malware threats and safeguarding against the increasing accessibility and scalability of cyberattacks facilitated by the MaaS model.","sentences":["The goal of this chapter is to illuminate the operational frameworks, key actors, and significant cybersecurity implications of the Malware as a Service (MaaS) ecosystem.","Highlighting the transformation of malware proliferation into a service-oriented model, the chapter discusses how MaaS democratises access to sophisticated cyberattack capabilities, enabling even those with minimal technical knowledge to execute catastrophic cyberattacks.","The discussion extends to the roles within the MaaS ecosystem, including malware developers, affiliates, initial access brokers, and the essential infrastructure providers that support these nefarious activities.","The study emphasises the profound challenges MaaS poses to traditional cybersecurity defences, rendered ineffective against the constantly evolving and highly adaptable threats generated by MaaS platforms.","With the increase in malware sophistication, there is a parallel call for a paradigm shift in defensive strategies, advocating for dynamic analysis, behavioural detection, and the integration of AI and machine learning techniques.","By exploring the intricacies of the MaaS ecosystem, including the economic motivations driving its growth and the blurred lines between legitimate service models and cyber crime, the chapter presents a comprehensive overview intended to foster a deeper understanding among researchers and cybersecurity professionals.","The ultimate goal is to aid in developing more effective strategies for combating the spread of commoditised malware threats and safeguarding against the increasing accessibility and scalability of cyberattacks facilitated by the MaaS model."],"url":"http://arxiv.org/abs/2405.04109v1","category":"cs.CR"}
{"created":"2024-05-07 08:24:22","title":"Adaptive Graph Normalized Sign Algorithm","abstract":"Efficient and robust prediction of graph signals is challenging when the signals are under impulsive noise and have missing data. Exploiting graph signal processing (GSP) and leveraging the simplicity of the classical adaptive sign algorithm, we propose an adaptive algorithm on graphs named the Graph Normalized Sign (GNS). GNS approximated a normalization term into the update, therefore achieving faster convergence and lower error compared to previous adaptive GSP algorithms. In the task of the online prediction of multivariate temperature data under impulsive noise, GNS outputs fast and robust predictions.","sentences":["Efficient and robust prediction of graph signals is challenging when the signals are under impulsive noise and have missing data.","Exploiting graph signal processing (GSP) and leveraging the simplicity of the classical adaptive sign algorithm, we propose an adaptive algorithm on graphs named the Graph Normalized Sign (GNS).","GNS approximated a normalization term into the update, therefore achieving faster convergence and lower error compared to previous adaptive GSP algorithms.","In the task of the online prediction of multivariate temperature data under impulsive noise, GNS outputs fast and robust predictions."],"url":"http://arxiv.org/abs/2405.04107v1","category":"eess.SP"}
{"created":"2024-05-07 08:12:39","title":"Energy-based theory of autoresonance in chains of coupled damped-driven generic oscillators","abstract":"An energy-based theory of autoresonance in driven dissipative chains of coupled generic oscillators is discussed on the basis of a variational principle concerning the energy functional. The theory is applied to chains of delayed Duffing-Ueda oscillators and the equations that together govern the autoresonance forces and solutions are derived and solved analytically for generic values of parameters and initial conditions, including the case of quenched time-delay disorder. Remarkably, the presence of retarded potentials with time-delayed feedback drastically modify the autoresonance scenario preventing the growth of the energy oscillation over specific regions of the parameter space. Additionally, effective harmonic forces with a slowly varying frequency are derived from the exact autoresonant excitations and the effectiveness of the theory is demonstrated at suppressing the chaos induced by homogeneous periodic excitations in such oscillator chains. Numerical experiments confirmed all the theoretical predictions.","sentences":["An energy-based theory of autoresonance in driven dissipative chains of coupled generic oscillators is discussed on the basis of a variational principle concerning the energy functional.","The theory is applied to chains of delayed Duffing-Ueda oscillators and the equations that together govern the autoresonance forces and solutions are derived and solved analytically for generic values of parameters and initial conditions, including the case of quenched time-delay disorder.","Remarkably, the presence of retarded potentials with time-delayed feedback drastically modify the autoresonance scenario preventing the growth of the energy oscillation over specific regions of the parameter space.","Additionally, effective harmonic forces with a slowly varying frequency are derived from the exact autoresonant excitations and the effectiveness of the theory is demonstrated at suppressing the chaos induced by homogeneous periodic excitations in such oscillator chains.","Numerical experiments confirmed all the theoretical predictions."],"url":"http://arxiv.org/abs/2405.04556v1","category":"nlin.CD"}
{"created":"2024-05-07 08:12:38","title":"Effect of realistic oscillator phase noise on the performance of cell-free networks","abstract":"To keep supporting 6G requirements, the radio access infrastructure will increasingly densify. Cell-free (CF) networks offer extreme flexibility by coherently serving users with multiple Access points (APs). This paradigm requires precise and stable phase synchronization. In this article, we adapt the standardized 5G NR setup (subcarrier spacing, OFDM symbol duration and allocation) to investigate the effect of Phase Noise (PN) on the simulated performance of scalable CF networks. In contrast to the prior literature relying on the simplified model of a free-running oscillator with the Wiener process, we deploy a realistic hardware-inspired phase noise model reproducing the Local Oscillator (LO) phase drift. Our results demonstrate that even affordable LOs offer sufficient stability to ensure negligible loss of uplink Spectral Efficiency (SE) on the time scale of the standardized 5G Transmission Time Interval of 1 ms. This study substantiates the feasibility of CF networks based on 5G standards.","sentences":["To keep supporting 6G requirements, the radio access infrastructure will increasingly densify.","Cell-free (CF) networks offer extreme flexibility by coherently serving users with multiple Access points (APs).","This paradigm requires precise and stable phase synchronization.","In this article, we adapt the standardized 5G NR setup (subcarrier spacing, OFDM symbol duration and allocation) to investigate the effect of Phase Noise (PN) on the simulated performance of scalable CF networks.","In contrast to the prior literature relying on the simplified model of a free-running oscillator with the Wiener process, we deploy a realistic hardware-inspired phase noise model reproducing the Local Oscillator (LO) phase drift.","Our results demonstrate that even affordable LOs offer sufficient stability to ensure negligible loss of uplink Spectral Efficiency (SE) on the time scale of the standardized 5G Transmission Time Interval of 1 ms.","This study substantiates the feasibility of CF networks based on 5G standards."],"url":"http://arxiv.org/abs/2405.04099v1","category":"cs.NI"}
{"created":"2024-05-07 07:57:15","title":"Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes","abstract":"The emergence of contemporary deepfakes has attracted significant attention in machine learning research, as artificial intelligence (AI) generated synthetic media increases the incidence of misinterpretation and is difficult to distinguish from genuine content. Currently, machine learning techniques have been extensively studied for automatically detecting deepfakes. However, human perception has been less explored. Malicious deepfakes could ultimately cause public and social problems. Can we humans correctly perceive the authenticity of the content of the videos we watch? The answer is obviously uncertain; therefore, this paper aims to evaluate the human ability to discern deepfake videos through a subjective study. We present our findings by comparing human observers to five state-ofthe-art audiovisual deepfake detection models. To this end, we used gamification concepts to provide 110 participants (55 native English speakers and 55 non-native English speakers) with a webbased platform where they could access a series of 40 videos (20 real and 20 fake) to determine their authenticity. Each participant performed the experiment twice with the same 40 videos in different random orders. The videos are manually selected from the FakeAVCeleb dataset. We found that all AI models performed better than humans when evaluated on the same 40 videos. The study also reveals that while deception is not impossible, humans tend to overestimate their detection capabilities. Our experimental results may help benchmark human versus machine performance, advance forensics analysis, and enable adaptive countermeasures.","sentences":["The emergence of contemporary deepfakes has attracted significant attention in machine learning research, as artificial intelligence (AI) generated synthetic media increases the incidence of misinterpretation and is difficult to distinguish from genuine content.","Currently, machine learning techniques have been extensively studied for automatically detecting deepfakes.","However, human perception has been less explored.","Malicious deepfakes could ultimately cause public and social problems.","Can we humans correctly perceive the authenticity of the content of the videos we watch?","The answer is obviously uncertain; therefore, this paper aims to evaluate the human ability to discern deepfake videos through a subjective study.","We present our findings by comparing human observers to five state-ofthe-art audiovisual deepfake detection models.","To this end, we used gamification concepts to provide 110 participants (55 native English speakers and 55 non-native English speakers) with a webbased platform where they could access a series of 40 videos (20 real and 20 fake) to determine their authenticity.","Each participant performed the experiment twice with the same 40 videos in different random orders.","The videos are manually selected from the FakeAVCeleb dataset.","We found that all AI models performed better than humans when evaluated on the same 40 videos.","The study also reveals that while deception is not impossible, humans tend to overestimate their detection capabilities.","Our experimental results may help benchmark human versus machine performance, advance forensics analysis, and enable adaptive countermeasures."],"url":"http://arxiv.org/abs/2405.04097v1","category":"cs.CV"}
{"created":"2024-05-07 07:56:30","title":"Speaker Characterization by means of Attention Pooling","abstract":"State-of-the-art Deep Learning systems for speaker verification are commonly based on speaker embedding extractors. These architectures are usually composed of a feature extractor front-end together with a pooling layer to encode variable-length utterances into fixed-length speaker vectors. The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker recognition, placed between a CNN-based front-end and a set of fully connected layers. This has shown to be an excellent approach to efficiently select the most relevant features captured by the front-end from the speech signal. In this paper we show excellent experimental results by adapting this architecture to other different speaker characterization tasks, such as emotion recognition, sex classification and COVID-19 detection.","sentences":["State-of-the-art Deep Learning systems for speaker verification are commonly based on speaker embedding extractors.","These architectures are usually composed of a feature extractor front-end together with a pooling layer to encode variable-length utterances into fixed-length speaker vectors.","The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker recognition, placed between a CNN-based front-end and a set of fully connected layers.","This has shown to be an excellent approach to efficiently select the most relevant features captured by the front-end from the speech signal.","In this paper we show excellent experimental results by adapting this architecture to other different speaker characterization tasks, such as emotion recognition, sex classification and COVID-19 detection."],"url":"http://arxiv.org/abs/2405.04096v1","category":"eess.AS"}
{"created":"2024-05-07 07:55:45","title":"Going Proactive and Explanatory Against Malware Concept Drift","abstract":"Deep learning-based malware classifiers face significant challenges due to concept drift. The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels. Previous research has primarily focused on detecting drift samples, relying on expert-led analysis and labeling for model retraining. However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs.   To address these limitations, we introduce DREAM, a novel system designed to surpass the capabilities of existing drift detectors and to establish an explanatory drift adaptation process. DREAM enhances drift detection through model sensitivity and data autonomy. The detector, trained in a semi-supervised approach, proactively captures malware behavior concepts through classifier feedback. During testing, it utilizes samples generated by the detector itself, eliminating reliance on extensive training data. For drift adaptation, DREAM enlarges human intervention, enabling revisions of malware labels and concept explanations embedded within the detector's latent space. To ensure a comprehensive response to concept drift, it facilitates a coordinated update process for both the classifier and the detector. Our evaluation shows that DREAM can effectively improve the drift detection accuracy and reduce the expert analysis effort in adaptation across different malware datasets and classifiers.","sentences":["Deep learning-based malware classifiers face significant challenges due to concept drift.","The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels.","Previous research has primarily focused on detecting drift samples, relying on expert-led analysis and labeling for model retraining.","However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs.   ","To address these limitations, we introduce DREAM, a novel system designed to surpass the capabilities of existing drift detectors and to establish an explanatory drift adaptation process.","DREAM enhances drift detection through model sensitivity and data autonomy.","The detector, trained in a semi-supervised approach, proactively captures malware behavior concepts through classifier feedback.","During testing, it utilizes samples generated by the detector itself, eliminating reliance on extensive training data.","For drift adaptation, DREAM enlarges human intervention, enabling revisions of malware labels and concept explanations embedded within the detector's latent space.","To ensure a comprehensive response to concept drift, it facilitates a coordinated update process for both the classifier and the detector.","Our evaluation shows that DREAM can effectively improve the drift detection accuracy and reduce the expert analysis effort in adaptation across different malware datasets and classifiers."],"url":"http://arxiv.org/abs/2405.04095v1","category":"cs.CR"}
{"created":"2024-05-07 07:20:15","title":"A simple theory for training response of deep neural networks","abstract":"Deep neural networks give us a powerful method to model the training dataset's relationship between input and output. We can regard that as a complex adaptive system consisting of many artificial neurons that work as an adaptive memory as a whole. The network's behavior is training dynamics with a feedback loop from the evaluation of the loss function. We already know the training response can be constant or shows power law-like aging in some ideal situations. However, we still have gaps between those findings and other complex phenomena, like network fragility. To fill the gap, we introduce a very simple network and analyze it. We show the training response consists of some different factors based on training stages, activation functions, or training methods. In addition, we show feature space reduction as an effect of stochastic training dynamics, which can result in network fragility. Finally, we discuss some complex phenomena of deep networks.","sentences":["Deep neural networks give us a powerful method to model the training dataset's relationship between input and output.","We can regard that as a complex adaptive system consisting of many artificial neurons that work as an adaptive memory as a whole.","The network's behavior is training dynamics with a feedback loop from the evaluation of the loss function.","We already know the training response can be constant or shows power law-like aging in some ideal situations.","However, we still have gaps between those findings and other complex phenomena, like network fragility.","To fill the gap, we introduce a very simple network and analyze it.","We show the training response consists of some different factors based on training stages, activation functions, or training methods.","In addition, we show feature space reduction as an effect of stochastic training dynamics, which can result in network fragility.","Finally, we discuss some complex phenomena of deep networks."],"url":"http://arxiv.org/abs/2405.04074v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 07:07:44","title":"Generalized Cauchy-Schwarz Divergence and Its Deep Learning Applications","abstract":"Divergence measures play a central role in machine learning and become increasingly essential in deep learning. However, valid and computationally efficient divergence measures for multiple (more than two) distributions are scarcely investigated. This becomes particularly crucial in areas where the simultaneous management of multiple distributions is both unavoidable and essential. Examples include clustering, multi-source domain adaptation or generalization, and multi-view learning, among others. Although calculating the mean of pairwise distances between any two distributions serves as a common way to quantify the total divergence among multiple distributions, it is crucial to acknowledge that this approach is not straightforward and requires significant computational resources. In this study, we introduce a new divergence measure for multiple distributions named the generalized Cauchy-Schwarz divergence (GCSD), which is inspired by the classic Cauchy-Schwarz divergence. Additionally, we provide a closed-form sample estimator based on kernel density estimation, making it convenient and straightforward to use in various machine-learning applications. Finally, we apply the proposed GCSD to two challenging machine learning tasks, namely deep learning-based clustering and the problem of multi-source domain adaptation. The experimental results showcase the impressive performance of GCSD in both tasks, highlighting its potential application in machine-learning areas that involve quantifying multiple distributions.","sentences":["Divergence measures play a central role in machine learning and become increasingly essential in deep learning.","However, valid and computationally efficient divergence measures for multiple (more than two) distributions are scarcely investigated.","This becomes particularly crucial in areas where the simultaneous management of multiple distributions is both unavoidable and essential.","Examples include clustering, multi-source domain adaptation or generalization, and multi-view learning, among others.","Although calculating the mean of pairwise distances between any two distributions serves as a common way to quantify the total divergence among multiple distributions, it is crucial to acknowledge that this approach is not straightforward and requires significant computational resources.","In this study, we introduce a new divergence measure for multiple distributions named the generalized Cauchy-Schwarz divergence (GCSD), which is inspired by the classic Cauchy-Schwarz divergence.","Additionally, we provide a closed-form sample estimator based on kernel density estimation, making it convenient and straightforward to use in various machine-learning applications.","Finally, we apply the proposed GCSD to two challenging machine learning tasks, namely deep learning-based clustering and the problem of multi-source domain adaptation.","The experimental results showcase the impressive performance of GCSD in both tasks, highlighting its potential application in machine-learning areas that involve quantifying multiple distributions."],"url":"http://arxiv.org/abs/2405.04061v1","category":"cs.LG"}
{"created":"2024-05-07 06:47:12","title":"Learning Linear Block Error Correction Codes","abstract":"Error correction codes are a crucial part of the physical communication layer, ensuring the reliable transfer of data over noisy channels. The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths. While neural decoders have recently demonstrated their advantage over classical decoding techniques, the neural design of the codes remains a challenge. In this work, we propose for the first time a unified encoder-decoder training of binary linear block codes. To this end, we adapt the coding setting to support efficient and differentiable training of the code for end-to-end optimization over the order two Galois field. We also propose a novel Transformer model in which the self-attention masking is performed in a differentiable fashion for the efficient backpropagation of the code gradient. Our results show that (i) the proposed decoder outperforms existing neural decoding on conventional codes, (ii) the suggested framework generates codes that outperform the {analogous} conventional codes, and (iii) the codes we developed not only excel with our decoder but also show enhanced performance with traditional decoding techniques.","sentences":["Error correction codes are a crucial part of the physical communication layer, ensuring the reliable transfer of data over noisy channels.","The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths.","While neural decoders have recently demonstrated their advantage over classical decoding techniques, the neural design of the codes remains a challenge.","In this work, we propose for the first time a unified encoder-decoder training of binary linear block codes.","To this end, we adapt the coding setting to support efficient and differentiable training of the code for end-to-end optimization over the order two Galois field.","We also propose a novel Transformer model in which the self-attention masking is performed in a differentiable fashion for the efficient backpropagation of the code gradient.","Our results show that (i) the proposed decoder outperforms existing neural decoding on conventional codes, (ii) the suggested framework generates codes that outperform the {analogous} conventional codes, and (iii) the codes we developed not only excel with our decoder but also show enhanced performance with traditional decoding techniques."],"url":"http://arxiv.org/abs/2405.04050v1","category":"cs.IT"}
{"created":"2024-05-07 06:42:30","title":"Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks","abstract":"As spiking neural networks (SNNs) gain traction in deploying neuromorphic computing solutions, protecting their intellectual property (IP) has become crucial. Without adequate safeguards, proprietary SNN architectures are at risk of theft, replication, or misuse, which could lead to significant financial losses for the owners. While IP protection techniques have been extensively explored for artificial neural networks (ANNs), their applicability and effectiveness for the unique characteristics of SNNs remain largely unexplored. In this work, we pioneer an investigation into adapting two prominent watermarking approaches, namely, fingerprint-based and backdoor-based mechanisms to secure proprietary SNN architectures. We conduct thorough experiments to evaluate the impact on fidelity, resilience against overwrite threats, and resistance to compression attacks when applying these watermarking techniques to SNNs, drawing comparisons with their ANN counterparts. This study lays the groundwork for developing neuromorphic-aware IP protection strategies tailored to the distinctive dynamics of SNNs.","sentences":["As spiking neural networks (SNNs) gain traction in deploying neuromorphic computing solutions, protecting their intellectual property (IP) has become crucial.","Without adequate safeguards, proprietary SNN architectures are at risk of theft, replication, or misuse, which could lead to significant financial losses for the owners.","While IP protection techniques have been extensively explored for artificial neural networks (ANNs), their applicability and effectiveness for the unique characteristics of SNNs remain largely unexplored.","In this work, we pioneer an investigation into adapting two prominent watermarking approaches, namely, fingerprint-based and backdoor-based mechanisms to secure proprietary SNN architectures.","We conduct thorough experiments to evaluate the impact on fidelity, resilience against overwrite threats, and resistance to compression attacks when applying these watermarking techniques to SNNs, drawing comparisons with their ANN counterparts.","This study lays the groundwork for developing neuromorphic-aware IP protection strategies tailored to the distinctive dynamics of SNNs."],"url":"http://arxiv.org/abs/2405.04049v1","category":"cs.CR"}
{"created":"2024-05-07 06:33:20","title":"Uniform-in-time estimates for mean-field type SDEs and applications","abstract":"Via constructing an asymptotic coupling by reflection, in this paper we establish uniform-in-time estimates on probability distances for mean-field type SDEs, where the drift terms under consideration are dissipative merely in the long distance. As applications, we (i) explore the long time probability distance estimate between an SDE and its delay version; (ii) investigate the issue on uniform-in-time propagation of chaos for McKean-Vlasov SDEs, where the drifts might be singular with respect to the spatial variables and need not to be of convolution type; (iii) tackle the discretization error bounds in an infinite-time horizon for stochastic algorithms (e.g. backward/tamed/adaptive Euler-Maruyama schemes as three typical candidates) associated with McKean-Vlasov SDEs.","sentences":["Via constructing an asymptotic coupling by reflection, in this paper we establish uniform-in-time estimates on probability distances for mean-field type SDEs, where the drift terms under consideration are dissipative merely in the long distance.","As applications, we (i) explore the long time probability distance estimate between an SDE and its delay version; (ii) investigate the issue on uniform-in-time propagation of chaos for McKean-Vlasov SDEs, where the drifts might be singular with respect to the spatial variables and need not to be of convolution type; (iii) tackle the discretization error bounds in an infinite-time horizon for stochastic algorithms (e.g. backward/tamed/adaptive Euler-Maruyama schemes as three typical candidates) associated with McKean-Vlasov SDEs."],"url":"http://arxiv.org/abs/2405.04047v1","category":"math.PR"}
{"created":"2024-05-07 05:32:26","title":"Generalized Langevin dynamics for single beads in linear elastic network","abstract":"We derive generalized Langevin equations (GLEs) for single beads in linear elastic networks. In particular, the derivations of the GLEs are conducted without employing normal modes, resulting in two distinct representations in terms of resistance and mobility kernels. The fluctuation-dissipation relations are also confirmed for both GLEs. Subsequently, we demonstrate that these two representations are interconnected via Laplace transforms. Furthermore, another GLE is derived by utilizing a projection operator method, and it is shown that the equation obtained through the projection scheme is consistent with the GLE with the resistance kernel. Finally, as the simplest example, the present framework is applied to the Rouse model, and the GLEs with the resistance and mobility kernels are explicitly derived for arbitrary positions of the tagged bead in the Rouse chain.","sentences":["We derive generalized Langevin equations (GLEs) for single beads in linear elastic networks.","In particular, the derivations of the GLEs are conducted without employing normal modes, resulting in two distinct representations in terms of resistance and mobility kernels.","The fluctuation-dissipation relations are also confirmed for both GLEs.","Subsequently, we demonstrate that these two representations are interconnected via Laplace transforms.","Furthermore, another GLE is derived by utilizing a projection operator method, and it is shown that the equation obtained through the projection scheme is consistent with the GLE with the resistance kernel.","Finally, as the simplest example, the present framework is applied to the Rouse model, and the GLEs with the resistance and mobility kernels are explicitly derived for arbitrary positions of the tagged bead in the Rouse chain."],"url":"http://arxiv.org/abs/2405.04019v1","category":"cond-mat.soft"}
{"created":"2024-05-07 05:14:31","title":"On the uniqueness of the mild solution of the critical quasi-geostrophic equation","abstract":"We demonstrate that the uniqueness of the mild solution of the two-dimensional quasi-geostrophic equation with the critical dissipation holds in the scaling critical homogeneous Besov space $\\dot{B}^0_{\\infty,1}$. We consider a solustion of integral equation, and our result does not need regularity assumption.","sentences":["We demonstrate that the uniqueness of the mild solution of the two-dimensional quasi-geostrophic equation with the critical dissipation holds in the scaling critical homogeneous Besov space $\\dot{B}^0_{\\infty,1}$. We consider a solustion of integral equation, and our result does not need regularity assumption."],"url":"http://arxiv.org/abs/2405.04014v1","category":"math.AP"}
{"created":"2024-05-07 04:57:25","title":"Structured Click Control in Transformer-based Interactive Segmentation","abstract":"Click-point-based interactive segmentation has received widespread attention due to its efficiency. However, it's hard for existing algorithms to obtain precise and robust responses after multiple clicks. In this case, the segmentation results tend to have little change or are even worse than before. To improve the robustness of the response, we propose a structured click intent model based on graph neural networks, which adaptively obtains graph nodes via the global similarity of user-clicked Transformer tokens. Then the graph nodes will be aggregated to obtain structured interaction features. Finally, the dual cross-attention will be used to inject structured interaction features into vision Transformer features, thereby enhancing the control of clicks over segmentation results. Extensive experiments demonstrated the proposed algorithm can serve as a general structure in improving Transformer-based interactive segmenta?tion performance. The code and data will be released at https://github.com/hahamyt/scc.","sentences":["Click-point-based interactive segmentation has received widespread attention due to its efficiency.","However, it's hard for existing algorithms to obtain precise and robust responses after multiple clicks.","In this case, the segmentation results tend to have little change or are even worse than before.","To improve the robustness of the response, we propose a structured click intent model based on graph neural networks, which adaptively obtains graph nodes via the global similarity of user-clicked Transformer tokens.","Then the graph nodes will be aggregated to obtain structured interaction features.","Finally, the dual cross-attention will be used to inject structured interaction features into vision Transformer features, thereby enhancing the control of clicks over segmentation results.","Extensive experiments demonstrated the proposed algorithm can serve as a general structure in improving Transformer-based interactive segmenta?tion performance.","The code and data will be released at https://github.com/hahamyt/scc."],"url":"http://arxiv.org/abs/2405.04009v1","category":"cs.CV"}
{"created":"2024-05-07 04:00:30","title":"Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application","abstract":"Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and long-tail user recommendations. Leveraging the capabilities of Large Language Models (LLMs) pretrained on massive text corpus presents a promising avenue for enhancing recommender systems by integrating open-world domain knowledge. In this paper, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge. We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge. To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application. Through offline experiments on the large-scale industrial dataset and online experiments on A/B tests, we demonstrate the efficacy of our approach.","sentences":["Contemporary recommender systems predominantly rely on collaborative filtering techniques, employing ID-embedding to capture latent associations among users and items.","However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance in cold-start scenarios and long-tail user recommendations.","Leveraging the capabilities of Large Language Models (LLMs) pretrained on massive text corpus presents a promising avenue for enhancing recommender systems by integrating open-world domain knowledge.","In this paper, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge.","We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge.","To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application.","Through offline experiments on the large-scale industrial dataset and online experiments on A/B tests, we demonstrate the efficacy of our approach."],"url":"http://arxiv.org/abs/2405.03988v1","category":"cs.IR"}
{"created":"2024-05-07 02:54:10","title":"Memristive switching in the surface of a charge-density-wave topological semimetal","abstract":"Owing to the outstanding properties provided by nontrivial band topology, topological phases of matter are considered as a promising platform towards low-dissipation electronics, efficient spin-charge conversion, and topological quantum computation. Achieving ferroelectricity in topological materials enables the non-volatile control of the quantum states, which could greatly facilitate topological electronic research. However, ferroelectricity is generally incompatible with systems featuring metallicity due to the screening effect of free carriers. In this study, we report the observation of memristive switching based on the ferroelectric surface state of a topological semimetal (TaSe4)2I. We find that the surface state of (TaSe4)2I presents out-of-plane ferroelectric polarization due to surface reconstruction. With the combination of ferroelectric surface and charge-density-wave-gapped bulk states, an electric switchable barrier height can be achieved in (TaSe4)2I-metal contact. By employing a multi-terminal grounding design, we manage to construct a prototype ferroelectric memristor based on (TaSe4)2I with on/off ratio up to 10^3, endurance over 10^3 cycles, and good retention characteristics. The origin of the ferroelectric surface state is further investigated by first-principles calculations, which reveals an interplay between ferroelectricity and band topology. The emergence of ferroelectricity in (TaSe4)2I not only demonstrates it as a rare but essential case of ferroelectric topological materials, but also opens new routes towards the implementation of topological materials in functional electronic devices.","sentences":["Owing to the outstanding properties provided by nontrivial band topology, topological phases of matter are considered as a promising platform towards low-dissipation electronics, efficient spin-charge conversion, and topological quantum computation.","Achieving ferroelectricity in topological materials enables the non-volatile control of the quantum states, which could greatly facilitate topological electronic research.","However, ferroelectricity is generally incompatible with systems featuring metallicity due to the screening effect of free carriers.","In this study, we report the observation of memristive switching based on the ferroelectric surface state of a topological semimetal (TaSe4)2I. We find that the surface state of (TaSe4)2I presents out-of-plane ferroelectric polarization due to surface reconstruction.","With the combination of ferroelectric surface and charge-density-wave-gapped bulk states, an electric switchable barrier height can be achieved in (TaSe4)2I-metal contact.","By employing a multi-terminal grounding design, we manage to construct a prototype ferroelectric memristor based on (TaSe4)2I with on/off ratio up to 10^3, endurance over 10^3 cycles, and good retention characteristics.","The origin of the ferroelectric surface state is further investigated by first-principles calculations, which reveals an interplay between ferroelectricity and band topology.","The emergence of ferroelectricity in (TaSe4)2I not only demonstrates it as a rare but essential case of ferroelectric topological materials, but also opens new routes towards the implementation of topological materials in functional electronic devices."],"url":"http://arxiv.org/abs/2405.03966v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 02:42:17","title":"Adaptive Speech Emotion Representation Learning Based On Dynamic Graph","abstract":"Graph representation learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings. However, for sequential data such as speech signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data. This may reduce the efficiency of graph representation learning for sequential data. For this reason, we propose an adaptive graph representation learning method based on dynamically evolved graphs, which are consecutively constructed on a series of subsequences segmented by a sliding window. In doing this, it is better to capture local and global context information within a long sequence. Moreover, we introduce a weighted approach to update the node representation rather than the conventional average one, where the weights are calculated by a novel matrix computation based on the degree of neighboring nodes. Finally, we construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to optimize the graph structure. To verify the effectiveness of the proposed method, we conducted experiments for speech emotion recognition on the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed method outperforms the latest (non-)graph-based models.","sentences":["Graph representation learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings.","However, for sequential data such as speech signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data.","This may reduce the efficiency of graph representation learning for sequential data.","For this reason, we propose an adaptive graph representation learning method based on dynamically evolved graphs, which are consecutively constructed on a series of subsequences segmented by a sliding window.","In doing this, it is better to capture local and global context information within a long sequence.","Moreover, we introduce a weighted approach to update the node representation rather than the conventional average one, where the weights are calculated by a novel matrix computation based on the degree of neighboring nodes.","Finally, we construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to optimize the graph structure.","To verify the effectiveness of the proposed method, we conducted experiments for speech emotion recognition on the IEMOCAP and RAVDESS datasets.","Experimental results show that the proposed method outperforms the latest (non-)graph-based models."],"url":"http://arxiv.org/abs/2405.03956v1","category":"cs.SD"}
{"created":"2024-05-06 22:44:32","title":"Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows","abstract":"Despite notable successes of Reinforcement Learning (RL), the prevalent use of an online learning paradigm prevents its widespread adoption, especially in hazardous or costly scenarios. Offline RL has emerged as an alternative solution, learning from pre-collected static datasets. However, this offline learning introduces a new challenge known as distributional shift, degrading the performance when the policy is evaluated on scenarios that are Out-Of-Distribution (OOD) from the training dataset. Most existing offline RL resolves this issue by regularizing policy learning within the information supported by the given dataset. However, such regularization overlooks the potential for high-reward regions that may exist beyond the dataset. This motivates exploring novel offline learning techniques that can make improvements beyond the data support without compromising policy performance, potentially by learning causation (cause-and-effect) instead of correlation from the dataset. In this paper, we propose the MOOD-CRL (Model-based Offline OOD-Adapting Causal RL) algorithm, which aims to address the challenge of extrapolation for offline policy training through causal inference instead of policy-regularizing methods. Specifically, Causal Normalizing Flow (CNF) is developed to learn the transition and reward functions for data generation and augmentation in offline policy evaluation and training. Based on the data-invariant, physics-based qualitative causal graph and the observational data, we develop a novel learning scheme for CNF to learn the quantitative structural causal model. As a result, CNF gains predictive and counterfactual reasoning capabilities for sequential decision-making tasks, revealing a high potential for OOD adaptation. Our CNF-based offline RL approach is validated through empirical evaluations, outperforming model-free and model-based methods by a significant margin.","sentences":["Despite notable successes of Reinforcement Learning (RL), the prevalent use of an online learning paradigm prevents its widespread adoption, especially in hazardous or costly scenarios.","Offline RL has emerged as an alternative solution, learning from pre-collected static datasets.","However, this offline learning introduces a new challenge known as distributional shift, degrading the performance when the policy is evaluated on scenarios that are Out-Of-Distribution (OOD) from the training dataset.","Most existing offline RL resolves this issue by regularizing policy learning within the information supported by the given dataset.","However, such regularization overlooks the potential for high-reward regions that may exist beyond the dataset.","This motivates exploring novel offline learning techniques that can make improvements beyond the data support without compromising policy performance, potentially by learning causation (cause-and-effect) instead of correlation from the dataset.","In this paper, we propose the MOOD-CRL (Model-based Offline OOD-Adapting Causal RL) algorithm, which aims to address the challenge of extrapolation for offline policy training through causal inference instead of policy-regularizing methods.","Specifically, Causal Normalizing Flow (CNF) is developed to learn the transition and reward functions for data generation and augmentation in offline policy evaluation and training.","Based on the data-invariant, physics-based qualitative causal graph and the observational data, we develop a novel learning scheme for CNF to learn the quantitative structural causal model.","As a result, CNF gains predictive and counterfactual reasoning capabilities for sequential decision-making tasks, revealing a high potential for OOD adaptation.","Our CNF-based offline RL approach is validated through empirical evaluations, outperforming model-free and model-based methods by a significant margin."],"url":"http://arxiv.org/abs/2405.03892v1","category":"cs.LG"}
{"created":"2024-05-06 21:20:48","title":"Resource Optimization in UAV-assisted IoT Networks: The Role of Generative AI","abstract":"We investigate how generative Artificial Intelligence (AI) can be used to optimize resources in Unmanned Aerial Vehicle (UAV)-assisted Internet of Things (IoT) networks. In particular, generative AI models for real-time decision-making have been used in public safety scenarios. This work describes how generative AI models can improve resource management within UAV-assisted networks. Furthermore, this work presents generative AI in UAV-assisted networks to demonstrate its practical applications and highlight its broader capabilities. We demonstrate a real-life case study for public safety, demonstrating how generative AI can enhance real-time decision-making and improve training datasets. By leveraging generative AI in UAV- assisted networks, we can design more intelligent, adaptive, and efficient ecosystems to meet the evolving demands of wireless networks and diverse applications. Finally, we discuss challenges and future research directions associated with generative AI for resource optimization in UAV-assisted networks.","sentences":["We investigate how generative Artificial Intelligence (AI) can be used to optimize resources in Unmanned Aerial Vehicle (UAV)-assisted Internet of Things (IoT) networks.","In particular, generative AI models for real-time decision-making have been used in public safety scenarios.","This work describes how generative AI models can improve resource management within UAV-assisted networks.","Furthermore, this work presents generative AI in UAV-assisted networks to demonstrate its practical applications and highlight its broader capabilities.","We demonstrate a real-life case study for public safety, demonstrating how generative AI can enhance real-time decision-making and improve training datasets.","By leveraging generative AI in UAV- assisted networks, we can design more intelligent, adaptive, and efficient ecosystems to meet the evolving demands of wireless networks and diverse applications.","Finally, we discuss challenges and future research directions associated with generative AI for resource optimization in UAV-assisted networks."],"url":"http://arxiv.org/abs/2405.03863v1","category":"eess.SY"}
{"created":"2024-05-06 20:50:17","title":"Self-Improving Customer Review Response Generation Based on LLMs","abstract":"Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings. Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews. Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews. To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs). Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs. Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains. Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline. Further validation through manual examination of the generated responses underscores the efficacy our proposed system.","sentences":["Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings.","Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews.","Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews.","To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).","Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs.","Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains.","Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline.","Further validation through manual examination of the generated responses underscores the efficacy our proposed system."],"url":"http://arxiv.org/abs/2405.03845v1","category":"cs.CL"}
{"created":"2024-05-06 20:22:56","title":"Cloud Storage Integrity at Scale: A Case for Dynamic Hash Trees","abstract":"Merkle hash trees are the state-of-the-art method to protect the integrity of storage systems. However, using a hash tree can severely degrade performance, and prior works optimizing them have yet to yield a concrete understanding of the scalability of certain designs in the context of large-scale cloud storage systems. In this paper, we take a first-principles approach to analyzing hash tree performance for storage by introducing a definition of an optimal hash tree and a principled methodology for evaluating hash tree designs. We show that state-of-the-art designs are not scalable; they incur up to 40.1X slowdowns over an insecure baseline and deliver <50% of optimal performance across various experiments. We then exploit the characteristics of optimal hash trees to design Dynamic Hash Trees (DHTs), hash trees that can adapt to workload patterns on-the-fly, delivering >95% of optimal read and write performance and up to 4.2X speedups over the state-of-the art. Our novel methodology and DHT design provides a new foundation in the search for integrity mechanisms that can operate efficiently at scale.","sentences":["Merkle hash trees are the state-of-the-art method to protect the integrity of storage systems.","However, using a hash tree can severely degrade performance, and prior works optimizing them have yet to yield a concrete understanding of the scalability of certain designs in the context of large-scale cloud storage systems.","In this paper, we take a first-principles approach to analyzing hash tree performance for storage by introducing a definition of an optimal hash tree and a principled methodology for evaluating hash tree designs.","We show that state-of-the-art designs are not scalable; they incur up to 40.1X slowdowns over an insecure baseline and deliver <50% of optimal performance across various experiments.","We then exploit the characteristics of optimal hash trees to design Dynamic Hash Trees (DHTs), hash trees that can adapt to workload patterns on-the-fly, delivering >95% of optimal read and write performance and up to 4.2X speedups over the state-of-the art.","Our novel methodology and DHT design provides a new foundation in the search for integrity mechanisms that can operate efficiently at scale."],"url":"http://arxiv.org/abs/2405.03830v1","category":"cs.CR"}
{"created":"2024-05-06 18:39:00","title":"Exploring Self-Interacting Dark Matter Halos with Diverse Baryonic Distributions: A Parametric Approach","abstract":"Galaxies residing in dark matter halos exert significant gravitational effects that alter halo structure and dynamics. The complexity of these interactions escalates with the diversity of galactic structures and the variability in dark matter halo profiles under self-interacting dark matter (SIDM) models. This work extends the parametric model for dark matter-only halos presented in arXiv:2305.16176 to incorporate baryons. We adapt this model to consistently represent the SIDM halo density profile over time, highlighting the role of a gravothermal phase in characterizing the state of an SIDM halo. Given this phase, the density profile in SIDM is determined by a fictitious progenitor -- consisting of an NFW halo influenced by a baryonic potential -- that has evolved to its present state. In the temporal dimension, the model incorporates a form factor that rescales the evolution time in the dark matter-only case, thereby enabling the introduction of a universal phase. In the radial dimension, the halo density profile is parametrized to reflect the influences of baryons. We calibrate the model through N-body simulations with baryon potentials to fit various stellar-to-halo mass ratios and size-mass relationships. Our parametric approach is numerically efficient, enabling the exploration of SIDM effects across a diverse set of halos, as exemplified by a case study using an illustrative sample that spans five orders of magnitude in the mass range. We also demonstrate that the effects of evolution history and the specific SIDM model can be separated from the current states of galaxies and halos, leaving the task of identifying consistent SIDM models to dedicated post-processing analyses.","sentences":["Galaxies residing in dark matter halos exert significant gravitational effects that alter halo structure and dynamics.","The complexity of these interactions escalates with the diversity of galactic structures and the variability in dark matter halo profiles under self-interacting dark matter (SIDM) models.","This work extends the parametric model for dark matter-only halos presented in arXiv:2305.16176 to incorporate baryons.","We adapt this model to consistently represent the SIDM halo density profile over time, highlighting the role of a gravothermal phase in characterizing the state of an SIDM halo.","Given this phase, the density profile in SIDM is determined by a fictitious progenitor -- consisting of an NFW halo influenced by a baryonic potential -- that has evolved to its present state.","In the temporal dimension, the model incorporates a form factor that rescales the evolution time in the dark matter-only case, thereby enabling the introduction of a universal phase.","In the radial dimension, the halo density profile is parametrized to reflect the influences of baryons.","We calibrate the model through N-body simulations with baryon potentials to fit various stellar-to-halo mass ratios and size-mass relationships.","Our parametric approach is numerically efficient, enabling the exploration of SIDM effects across a diverse set of halos, as exemplified by a case study using an illustrative sample that spans five orders of magnitude in the mass range.","We also demonstrate that the effects of evolution history and the specific SIDM model can be separated from the current states of galaxies and halos, leaving the task of identifying consistent SIDM models to dedicated post-processing analyses."],"url":"http://arxiv.org/abs/2405.03787v1","category":"astro-ph.CO"}
{"created":"2024-05-06 18:19:22","title":"An Autoregressive Model for Time Series of Random Objects","abstract":"Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr\\'echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence. Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.","sentences":["Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability.","However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free.","To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space.","The parameters of interest in this model are the Fr\\'echet mean and a concentration parameter, both of which we prove can be consistently estimated from data.","Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence.","Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis.","Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations."],"url":"http://arxiv.org/abs/2405.03778v1","category":"stat.ME"}
{"created":"2024-05-06 18:13:37","title":"Novel Tour Construction Heuristic for Pick-Up and Delivery Routing Problems","abstract":"In logistic applications that require the pickup and delivery of items, route optimization problems can be modeled as precedence constrained traveling salesperson problems. The combinatorial nature of this problem restricts the application of exact algorithms to small instances, and heuristics are largely preferred for tractability. However, due to precedence constraints that restrict the order in which locations can be visited, heuristics outside of the nearest neighbor algorithm have been neglected in literature. While the convex hull cheapest insertion heuristic is known to produce good solutions in the absence of precedence constraints, i.e., when locations can be visited in any order, it has not been adapted for pick-up and delivery considerations. This paper presents an adapted convex hull cheapest insertion heuristic that accounts for precedence constraints and compares its solutions with the nearest neighbor heuristic using the TSPLIB benchmark data set. The proposed algorithm is particularly suited to cases where pickups are located in the periphery and deliveries are centrally located, outperforming the Nearest Neighbor algorithm in every examined instance.","sentences":["In logistic applications that require the pickup and delivery of items, route optimization problems can be modeled as precedence constrained traveling salesperson problems.","The combinatorial nature of this problem restricts the application of exact algorithms to small instances, and heuristics are largely preferred for tractability.","However, due to precedence constraints that restrict the order in which locations can be visited, heuristics outside of the nearest neighbor algorithm have been neglected in literature.","While the convex hull cheapest insertion heuristic is known to produce good solutions in the absence of precedence constraints, i.e., when locations can be visited in any order, it has not been adapted for pick-up and delivery considerations.","This paper presents an adapted convex hull cheapest insertion heuristic that accounts for precedence constraints and compares its solutions with the nearest neighbor heuristic using the TSPLIB benchmark data set.","The proposed algorithm is particularly suited to cases where pickups are located in the periphery and deliveries are centrally located, outperforming the Nearest Neighbor algorithm in every examined instance."],"url":"http://arxiv.org/abs/2405.03774v1","category":"math.CO"}
{"created":"2024-05-06 18:09:48","title":"Foundation Models for Video Understanding: A Survey","abstract":"Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}","sentences":["Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks.","Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data.","This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories.","Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks.","We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework.","By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.","Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks.","Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks.","We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}"],"url":"http://arxiv.org/abs/2405.03770v1","category":"cs.CV"}
{"created":"2024-05-06 18:00:00","title":"White dwarf eccentricity fluctuation and dissipation by AGB convection","abstract":"Millisecond pulsars with white dwarf companions have typical eccentricities $e\\sim 10^{-6}-10^{-3}$. The eccentricities of helium white dwarfs are explained well by applying the fluctuation-dissipation theorem to convective eddies in their red giant progenitors. We extend this theory to more massive carbon-oxygen (CO) white dwarfs with asymptotic giant branch (AGB) progenitors. Due to the radiation pressure in AGB stars, the dominant factor in determining the remnant white dwarf's eccentricity is the critical residual hydrogen envelope mass $m_{\\rm env}$ required to inflate the star to giant proportions. Using a suite of MESA stellar evolution simulations with $\\Delta m_{\\rm c}=10^{-3}\\,{\\rm M}_\\odot$ core-mass intervals, we resolved the AGB thermal pulses and found that the critical $m_{\\rm env}\\propto m_{\\rm c}^{-6}$. This steep dependence causes the $e(m_{\\rm c})$ relation to turn over, such that $e\\sim 3\\times 10^{-3}$ almost independently of the remnant CO white dwarf's mass $m_{\\rm c}$. Nearly all of the measured eccentricities lie below this robust theoretical limit, indicating that the eccentricity is damped during the common-envelope inspiral that follows the unstable Roche-lobe overflow of the AGB star. Specifically, we focused on white dwarfs with median masses $m_{\\rm c}>0.6\\,{\\rm M}_\\odot$. These massive white dwarfs begin their inspiral with practically identical orbital periods and eccentricities, eliminating any dependence on the initial conditions. For this sub-sample, we find an empirical relation $e\\propto P^{3/2}$ between the final period and eccentricity that is much tighter than previous studies - motivating theoretical work on the eccentricity evolution during the common envelope phase.","sentences":["Millisecond pulsars with white dwarf companions have typical eccentricities $e\\sim 10^{-6}-10^{-3}$.","The eccentricities of helium white dwarfs are explained well by applying the fluctuation-dissipation theorem to convective eddies in their red giant progenitors.","We extend this theory to more massive carbon-oxygen (CO) white dwarfs with asymptotic giant branch (AGB) progenitors.","Due to the radiation pressure in AGB stars, the dominant factor in determining the remnant white dwarf's eccentricity is the critical residual hydrogen envelope mass $m_{\\rm env}$ required to inflate the star to giant proportions.","Using a suite of MESA stellar evolution simulations with $\\Delta m_{\\rm c}=10^{-3}\\,{\\rm M}_\\odot$ core-mass intervals, we resolved the AGB thermal pulses and found that the critical $m_{\\rm env}\\propto m_{\\rm c}^{-6}$. This steep dependence causes the $e(m_{\\rm c})$ relation to turn over, such that $e\\sim 3\\times 10^{-3}$ almost independently of the remnant CO white dwarf's mass $m_{\\rm c}$. Nearly all of the measured eccentricities lie below this robust theoretical limit, indicating that the eccentricity is damped during the common-envelope inspiral that follows the unstable Roche-lobe overflow of the AGB star.","Specifically, we focused on white dwarfs with median masses $m_{\\rm c}>0.6\\,{\\rm M}_\\odot$.","These massive white dwarfs begin their inspiral with practically identical orbital periods and eccentricities, eliminating any dependence on the initial conditions.","For this sub-sample, we find an empirical relation $e\\propto P^{3/2}$ between the final period and eccentricity that is much tighter than previous studies - motivating theoretical work on the eccentricity evolution during the common envelope phase."],"url":"http://arxiv.org/abs/2405.03745v1","category":"astro-ph.SR"}
{"created":"2024-05-06 17:57:27","title":"Language-Image Models with 3D Understanding","abstract":"Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering. Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective. Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information. (2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats. (3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists. Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively. Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning. Our project is available at https://janghyuncho.github.io/Cube-LLM.","sentences":["Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks.","We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space.","To that end, we first develop a large-scale pre-training dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering.","Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D.","We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective.","Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information.","(2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats.","(3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists.","Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively.","Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning.","Our project is available at https://janghyuncho.github.io/Cube-LLM."],"url":"http://arxiv.org/abs/2405.03685v1","category":"cs.CV"}
{"created":"2024-05-06 17:50:35","title":"Topological Quantum Batteries","abstract":"We propose an innovative design for topological quantum batteries that involves coupling two atoms to a one-dimensional lattice with topological features. Employing the resolvent method, we analytically explore the thermodynamic performances of quantum batteries (QBs). First, we demonstrate that only coherent bound states significantly contribute to the stored energy of QBs. We observe near-perfect energy transfer from the quantum charger to the quantum battery (QB) in the topologically nontrivial phase. Conversely, in the topologically trivial phase, we reveal that under the Markov limit, the charging process of the QB is almost completely prohibited due to the emergence of degenerate zero-energy bound states. Moreover, we discover that the maximum energy storage exhibits singular behavior at the phase boundaries. Second, we find that direct coupling between the QB and quantum charger renders the ergotropy immune to sublattice dissipation, facilitated by the presence of a dark state and vacancy-like dressed bound state. Further, we show that as dissipation intensifies along with the emergence of the quantum Zeno effect, the charging power of QBs is transiently enhanced. Our findings provide insightful guidelines for practically enhancing the performance of QBs through structured reservoir engineering.","sentences":["We propose an innovative design for topological quantum batteries that involves coupling two atoms to a one-dimensional lattice with topological features.","Employing the resolvent method, we analytically explore the thermodynamic performances of quantum batteries (QBs).","First, we demonstrate that only coherent bound states significantly contribute to the stored energy of QBs.","We observe near-perfect energy transfer from the quantum charger to the quantum battery (QB) in the topologically nontrivial phase.","Conversely, in the topologically trivial phase, we reveal that under the Markov limit, the charging process of the QB is almost completely prohibited due to the emergence of degenerate zero-energy bound states.","Moreover, we discover that the maximum energy storage exhibits singular behavior at the phase boundaries.","Second, we find that direct coupling between the QB and quantum charger renders the ergotropy immune to sublattice dissipation, facilitated by the presence of a dark state and vacancy-like dressed bound state.","Further, we show that as dissipation intensifies along with the emergence of the quantum Zeno effect, the charging power of QBs is transiently enhanced.","Our findings provide insightful guidelines for practically enhancing the performance of QBs through structured reservoir engineering."],"url":"http://arxiv.org/abs/2405.03675v1","category":"quant-ph"}
{"created":"2024-05-06 17:49:31","title":"MemoryMamba: Memory-Augmented State Space Model for Defect Recognition","abstract":"As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows. Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings. These models especially struggle in scenarios involving limited or imbalanced defect data. In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models. MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training. Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection. In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities. The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios.","sentences":["As automation advances in manufacturing, the demand for precise and sophisticated defect detection technologies grows.","Existing vision models for defect recognition methods are insufficient for handling the complexities and variations of defects in contemporary manufacturing settings.","These models especially struggle in scenarios involving limited or imbalanced defect data.","In this work, we introduce MemoryMamba, a novel memory-augmented state space model (SSM), designed to overcome the limitations of existing defect recognition models.","MemoryMamba integrates the state space model with the memory augmentation mechanism, enabling the system to maintain and retrieve essential defect-specific information in training.","Its architecture is designed to capture dependencies and intricate defect characteristics, which are crucial for effective defect detection.","In the experiments, MemoryMamba was evaluated across four industrial datasets with diverse defect types and complexities.","The model consistently outperformed other methods, demonstrating its capability to adapt to various defect recognition scenarios."],"url":"http://arxiv.org/abs/2405.03673v1","category":"cs.CV"}
{"created":"2024-05-06 17:38:20","title":"Competitive strategies to use \"warm start\" algorithms with predictions","abstract":"We consider the problem of learning and using predictions for warm start algorithms with predictions. In this setting, an algorithm is given an instance of a problem, and a prediction of the solution. The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance. Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function. In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost. We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move. We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories. This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$. Thus the guarantee holds for all $k$ simultaneously.","sentences":["We consider the problem of learning and using predictions for warm start algorithms with predictions.","In this setting, an algorithm is given an instance of a problem, and a prediction of the solution.","The runtime of the algorithm is bounded by the distance from the predicted solution to the true solution of the instance.","Previous work has shown that when instances are drawn iid from some distribution, it is possible to learn an approximately optimal fixed prediction (Dinitz et al, NeurIPS 2021), and in the adversarial online case, it is possible to compete with the best fixed prediction in hindsight (Khodak et al, NeurIPS 2022).   ","In this work we give competitive guarantees against stronger benchmarks that consider a set of $k$ predictions $\\mathbf{P}$. That is, the \"optimal offline cost\" to solve an instance with respect to $\\mathbf{P}$ is the distance from the true solution to the closest member of $\\mathbf{P}$. This is analogous to the $k$-medians objective function.","In the distributional setting, we show a simple strategy that incurs cost that is at most an $O(k)$ factor worse than the optimal offline cost.","We then show a way to leverage learnable coarse information, in the form of partitions of the instance space into groups of \"similar\" instances, that allows us to potentially avoid this $O(k)$ factor.   ","Finally, we consider an online version of the problem, where we compete against offline strategies that are allowed to maintain a moving set of $k$ predictions or \"trajectories,\" and are charged for how much the predictions move.","We give an algorithm that does at most $O(k^4 \\ln^2 k)$ times as much work as any offline strategy of $k$ trajectories.","This algorithm is deterministic (robust to an adaptive adversary), and oblivious to the setting of $k$.","Thus the guarantee holds for all $k$ simultaneously."],"url":"http://arxiv.org/abs/2405.03661v1","category":"cs.DS"}
{"created":"2024-05-06 17:14:34","title":"Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders","abstract":"Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization. DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity. We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries. Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation. At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items. Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches. Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines.","sentences":["Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than embedding-based models (dual-encoders) at estimating query-item relevance.","Existing approaches perform k-NN search with CE by approximating the CE similarity with a vector embedding space fit either with dual-encoders (DE) or CUR matrix factorization.","DE-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE.","While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale.","In this paper, we address these shortcomings with our proposed sparse-matrix factorization based method that efficiently computes latent query and item embeddings to approximate CE scores and performs k-NN search with the approximate CE similarity.","We compute item embeddings offline by factorizing a sparse matrix containing query-item CE scores for a set of train queries.","Our method produces a high-quality approximation while requiring only a fraction of CE calls as compared to CUR-based methods, and allows for leveraging DE to initialize the embedding space while avoiding compute- and resource-intensive finetuning of DE via distillation.","At test time, the item embeddings remain fixed and retrieval occurs over rounds, alternating between a) estimating the test query embedding by minimizing error in approximating CE scores of items retrieved thus far, and b) using the updated test query embedding for retrieving more items.","Our k-NN search method improves recall by up to 5% (k=1) and 54% (k=100) over DE-based approaches.","Additionally, our indexing approach achieves a speedup of up to 100x over CUR-based and 5x over DE distillation methods, while matching or improving k-NN search recall over baselines."],"url":"http://arxiv.org/abs/2405.03651v1","category":"cs.IR"}
{"created":"2024-05-06 16:17:33","title":"Trackable Island-model Genetic Algorithms at Wafer Scale","abstract":"Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation. However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts. Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform. We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware. Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million. This pace enables quadrillions of evaluations a day. We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions. In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled. Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable. Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community.","sentences":["Emerging ML/AI hardware accelerators, like the 850,000 processor Cerebras Wafer-Scale Engine (WSE), hold great promise to scale up the capabilities of evolutionary computation.","However, challenges remain in maintaining visibility into underlying evolutionary processes while efficiently utilizing these platforms' large processor counts.","Here, we focus on the problem of extracting phylogenetic information from digital evolution on the WSE platform.","We present a tracking-enabled asynchronous island-based genetic algorithm (GA) framework for WSE hardware.","Emulated and on-hardware GA benchmarks with a simple tracking-enabled agent model clock upwards of 1 million generations a minute for population sizes reaching 16 million.","This pace enables quadrillions of evaluations a day.","We validate phylogenetic reconstructions from these trials and demonstrate their suitability for inference of underlying evolutionary conditions.","In particular, we demonstrate extraction of clear phylometric signals that differentiate wafer-scale runs with adaptive dynamics enabled versus disabled.","Together, these benchmark and validation trials reflect strong potential for highly scalable evolutionary computation that is both efficient and observable.","Kernel code implementing the island-model GA supports drop-in customization to support any fixed-length genome content and fitness criteria, allowing it to be leveraged to advance research interests across the community."],"url":"http://arxiv.org/abs/2405.03605v1","category":"cs.NE"}
{"created":"2024-05-06 16:13:52","title":"Flexible terahertz metasurface absorbers empowered by bound states in the continuum","abstract":"Terahertz absorbers are crucial to the cutting-edge techniques in the next-generation wireless communications, imaging, sensing, and radar stealth, as they fundamentally determine the performance of detectors and cloaking capabilities. It has long been a pressing task to find absorbers with customizable performance that can adapt to various environments with low cost and great flexibility. Here, we demonstrate perfect absorption empowered by bound states in the continuum (BICs) allowing for the tailoring of absorption coefficient, bandwidth, and field of view. The one-port absorbers are interpreted using temporal coupled-mode theory highlighting the dominant role of BICs in the far-field radiation properties. Through a thorough investigation of BICs from the perspective of lattice symmetry, we unravel the radiation features of three BIC modes using both multipolar and topological analysis. The versatile radiation capabilities of BICs provide ample freedom to meet specific requirements of absorbers, including tunable bandwidth, stable performance in a large field of view, and multi-band absorption using a thin and flexible film without extreme geometric demands. Our findings offer a systematic approach to developing optoelectronic devices and demonstrate the significant potential of BICs for optical and photonic applications which will stimulate further studies on terahertz photonics and metasurfaces.","sentences":["Terahertz absorbers are crucial to the cutting-edge techniques in the next-generation wireless communications, imaging, sensing, and radar stealth, as they fundamentally determine the performance of detectors and cloaking capabilities.","It has long been a pressing task to find absorbers with customizable performance that can adapt to various environments with low cost and great flexibility.","Here, we demonstrate perfect absorption empowered by bound states in the continuum (BICs) allowing for the tailoring of absorption coefficient, bandwidth, and field of view.","The one-port absorbers are interpreted using temporal coupled-mode theory highlighting the dominant role of BICs in the far-field radiation properties.","Through a thorough investigation of BICs from the perspective of lattice symmetry, we unravel the radiation features of three BIC modes using both multipolar and topological analysis.","The versatile radiation capabilities of BICs provide ample freedom to meet specific requirements of absorbers, including tunable bandwidth, stable performance in a large field of view, and multi-band absorption using a thin and flexible film without extreme geometric demands.","Our findings offer a systematic approach to developing optoelectronic devices and demonstrate the significant potential of BICs for optical and photonic applications which will stimulate further studies on terahertz photonics and metasurfaces."],"url":"http://arxiv.org/abs/2405.03600v1","category":"physics.optics"}
{"created":"2024-05-06 15:59:46","title":"Effective Quadratic Error Bounds for Floating-Point Algorithms Computing the Hypotenuse Function","abstract":"We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers. The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off. Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions. We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms. Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases. An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area. This is particularly important when using low precision formats, which are increasingly common in modern processors. Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging.","sentences":["We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers.","The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off.","Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions.","We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms.","Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases.","An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area.","This is particularly important when using low precision formats, which are increasingly common in modern processors.","Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging."],"url":"http://arxiv.org/abs/2405.03588v1","category":"math.NA"}
{"created":"2024-05-06 15:59:29","title":"Dissipative gradient nonlinearities prevent $\u03b4$-formations in local and nonlocal attraction-repulsion chemotaxis models","abstract":"We study some attraction repulsion chemotaxis models, characterized by nonlinearities laws for the diffusion of the cell density, and for the chemosensitivities and the production rates of the chemoattractant and the chemorepellent. Additionally, a source also involving some expression of the gradient of the species is incorporated.","sentences":["We study some attraction repulsion chemotaxis models, characterized by nonlinearities laws for the diffusion of the cell density, and for the chemosensitivities and the production rates of the chemoattractant and the chemorepellent.","Additionally, a source also involving some expression of the gradient of the species is incorporated."],"url":"http://arxiv.org/abs/2405.03586v1","category":"math.AP"}
{"created":"2024-05-06 15:55:18","title":"A GPU-Accelerated Interior Point Method for Radiation Therapy Optimization","abstract":"Optimization plays a central role in modern radiation therapy, where it is used to determine optimal treatment machine parameters in order to deliver precise doses adapted to each patient case. In general, solving the optimization problems that arise can present a computational bottleneck in the treatment planning process, as they can be large in terms of both variables and constraints. In this paper, we develop a GPU accelerated optimization solver for radiation therapy applications, based on an interior point method (IPM) utilizing iterative linear algebra to find search directions. The use of iterative linear algebra makes the solver suitable for porting to GPUs, as the core computational kernels become standard matrix-vector or vector-vector operations. Our solver is implemented in C++20 and uses CUDA for GPU acceleration.   The problems we solve are from the commercial treatment planning system RayStation, developed by RaySearch Laboratories (Stockholm, Sweden), which is used clinically in hundreds of cancer clinics around the world. RayStation solves (in general) nonlinear optimization problems using a sequential quadratic programming (SQP) method, where the main computation lies in solving quadratic programming (QP) sub-problems in each iteration. GPU acceleration for the solution of such QP sub-problems is the focus of the interior point method of this work. We benchmark our solver against the existing QP-solver in RayStation and show that our GPU accelerated IPM can accelerate the aggregated time-to-solution for all QP sub-problems in one SQP solve by 1.4 and 4.4 times, respectively, for two real patient cases.","sentences":["Optimization plays a central role in modern radiation therapy, where it is used to determine optimal treatment machine parameters in order to deliver precise doses adapted to each patient case.","In general, solving the optimization problems that arise can present a computational bottleneck in the treatment planning process, as they can be large in terms of both variables and constraints.","In this paper, we develop a GPU accelerated optimization solver for radiation therapy applications, based on an interior point method (IPM) utilizing iterative linear algebra to find search directions.","The use of iterative linear algebra makes the solver suitable for porting to GPUs, as the core computational kernels become standard matrix-vector or vector-vector operations.","Our solver is implemented in C++20 and uses CUDA for GPU acceleration.   ","The problems we solve are from the commercial treatment planning system RayStation, developed by RaySearch Laboratories (Stockholm, Sweden), which is used clinically in hundreds of cancer clinics around the world.","RayStation solves (in general) nonlinear optimization problems using a sequential quadratic programming (SQP) method, where the main computation lies in solving quadratic programming (QP) sub-problems in each iteration.","GPU acceleration for the solution of such QP sub-problems is the focus of the interior point method of this work.","We benchmark our solver against the existing QP-solver in RayStation and show that our GPU accelerated IPM can accelerate the aggregated time-to-solution for all QP sub-problems in one SQP solve by 1.4 and 4.4 times, respectively, for two real patient cases."],"url":"http://arxiv.org/abs/2405.03584v1","category":"math.OC"}
{"created":"2024-05-06 14:55:37","title":"Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation","abstract":"Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics. Traditional approaches of accumulating data and periodically retraining models are outpaced. We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data. These locally adapted models are then aggregated at a central server via federated learning. To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns. We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation. Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge.","sentences":["Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics.","Traditional approaches of accumulating data and periodically retraining models are outpaced.","We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data.","These locally adapted models are then aggregated at a central server via federated learning.","To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns.","We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation.","Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge."],"url":"http://arxiv.org/abs/2405.03537v1","category":"cs.LG"}
{"created":"2024-05-06 14:53:32","title":"Asymptotic-preserving hybridizable discontinuous Galerkin method for the Westervelt quasilinear wave equation","abstract":"We discuss the asymptotic-preserving properties of a hybridizable discontinuous Galerkin method for the Westervelt model of ultrasound waves. More precisely, we show that the proposed method is robust with respect to small values of the sound diffusivity damping parameter~$\\delta$ by deriving low- and high-order energy stability estimates, and \\emph{a priori} error bounds that are independent of~$\\delta$. Such bounds are then used to show that, when~$\\delta \\rightarrow 0^+$, the method remains stable and the discrete acoustic velocity potential~$\\psi_h^{(\\delta)}$ converges to~$\\psi_h^{(0)}$, where the latter is the singular vanishing dissipation limit. Moreover, we prove optimal convergence for the approximation of the acoustic particle velocity variable~$\\bv = \\nabla \\psi$. The established theoretical results are illustrated with some numerical experiments.","sentences":["We discuss the asymptotic-preserving properties of a hybridizable discontinuous Galerkin method for the Westervelt model of ultrasound waves.","More precisely, we show that the proposed method is robust with respect to small values of the sound diffusivity damping parameter~$\\delta$ by deriving low- and high-order energy stability estimates, and \\emph{a priori} error bounds that are independent of~$\\delta$. Such bounds are then used to show that, when~$\\delta \\rightarrow 0^+$, the method remains stable and the discrete acoustic velocity potential~$\\psi_h^{(\\delta)}$ converges to~$\\psi_h^{(0)}$, where the latter is the singular vanishing dissipation limit.","Moreover, we prove optimal convergence for the approximation of the acoustic particle velocity variable~$\\bv","= \\nabla \\psi$.","The established theoretical results are illustrated with some numerical experiments."],"url":"http://arxiv.org/abs/2405.03535v1","category":"math.NA"}
{"created":"2024-05-06 14:41:46","title":"On anomalous dissipation induced by transport noise","abstract":"In this paper, we show that suitable transport noises produce anomalous dissipation of energy of solutions to the 2D Navier-Stokes equations and diffusion equations in all dimensions. The key ingredients are Meyers' type estimates for SPDEs with transport noise which are combined with recent scaling limits for such SPDEs. The former allow us to obtain, for the first time for such type of scaling limits, convergence in a space of positive smoothness uniformly in time. Compared to known results, one of the main novelties is that anomalous dissipation might take place even in presence of a transport noise of arbitrarily small intensity. Further, we discuss physical interpretations.","sentences":["In this paper, we show that suitable transport noises produce anomalous dissipation of energy of solutions to the 2D Navier-Stokes equations and diffusion equations in all dimensions.","The key ingredients are Meyers' type estimates for SPDEs with transport noise which are combined with recent scaling limits for such SPDEs.","The former allow us to obtain, for the first time for such type of scaling limits, convergence in a space of positive smoothness uniformly in time.","Compared to known results, one of the main novelties is that anomalous dissipation might take place even in presence of a transport noise of arbitrarily small intensity.","Further, we discuss physical interpretations."],"url":"http://arxiv.org/abs/2405.03525v1","category":"math.AP"}
{"created":"2024-05-06 14:20:49","title":"Emergence of condensation patterns in kinetic equations for opinion dynamics","abstract":"In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity. To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density. In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting. From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution. Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size. In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects.","sentences":["In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity.","To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density.","In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting.","From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution.","Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size.","In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects."],"url":"http://arxiv.org/abs/2405.03507v1","category":"nlin.AO"}
{"created":"2024-05-06 14:06:12","title":"Differentially Private Synthetic Data with Private Density Estimation","abstract":"The need to analyze sensitive data, such as medical records or financial data, has created a critical research challenge in recent years. In this paper, we adopt the framework of differential privacy, and explore mechanisms for generating an entire dataset which accurately captures characteristics of the original data. We build upon the work of Boedihardjo et al, which laid the foundations for a new optimization-based algorithm for generating private synthetic data. Importantly, we adapt their algorithm by replacing a uniform sampling step with a private distribution estimator; this allows us to obtain better computational guarantees for discrete distributions, and develop a novel algorithm suitable for continuous distributions. We also explore applications of our work to several statistical tasks.","sentences":["The need to analyze sensitive data, such as medical records or financial data, has created a critical research challenge in recent years.","In this paper, we adopt the framework of differential privacy, and explore mechanisms for generating an entire dataset which accurately captures characteristics of the original data.","We build upon the work of Boedihardjo et al, which laid the foundations for a new optimization-based algorithm for generating private synthetic data.","Importantly, we adapt their algorithm by replacing a uniform sampling step with a private distribution estimator; this allows us to obtain better computational guarantees for discrete distributions, and develop a novel algorithm suitable for continuous distributions.","We also explore applications of our work to several statistical tasks."],"url":"http://arxiv.org/abs/2405.04554v1","category":"cs.CR"}
{"created":"2024-05-06 13:55:39","title":"Whispy: Adapting STT Whisper Models to Real-Time Environments","abstract":"Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection. However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications. In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models. As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost. We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output. Experimental results show how Whispy excels in robustness, promptness, and accuracy.","sentences":["Large general-purpose transformer models have recently become the mainstay in the realm of speech analysis.","In particular, Whisper achieves state-of-the-art results in relevant tasks such as speech recognition, translation, language identification, and voice activity detection.","However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications.","In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models.","As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost.","We evaluate the performance of our system on a large repository of publicly available speech datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output.","Experimental results show how Whispy excels in robustness, promptness, and accuracy."],"url":"http://arxiv.org/abs/2405.03484v1","category":"cs.SD"}
{"created":"2024-05-06 13:45:44","title":"Motion Planning under Uncertainty: Integrating Learning-Based Multi-Modal Predictors into Branch Model Predictive Control","abstract":"In complex traffic environments, autonomous vehicles face multi-modal uncertainty about other agents' future behavior. To address this, recent advancements in learningbased motion predictors output multi-modal predictions. We present our novel framework that leverages Branch Model Predictive Control(BMPC) to account for these predictions. The framework includes an online scenario-selection process guided by topology and collision risk criteria. This efficiently selects a minimal set of predictions, rendering the BMPC realtime capable. Additionally, we introduce an adaptive decision postponing strategy that delays the planner's commitment to a single scenario until the uncertainty is resolved. Our comprehensive evaluations in traffic intersection and random highway merging scenarios demonstrate enhanced comfort and safety through our method.","sentences":["In complex traffic environments, autonomous vehicles face multi-modal uncertainty about other agents' future behavior.","To address this, recent advancements in learningbased motion predictors output multi-modal predictions.","We present our novel framework that leverages Branch Model Predictive Control(BMPC) to account for these predictions.","The framework includes an online scenario-selection process guided by topology and collision risk criteria.","This efficiently selects a minimal set of predictions, rendering the BMPC realtime capable.","Additionally, we introduce an adaptive decision postponing strategy that delays the planner's commitment to a single scenario until the uncertainty is resolved.","Our comprehensive evaluations in traffic intersection and random highway merging scenarios demonstrate enhanced comfort and safety through our method."],"url":"http://arxiv.org/abs/2405.03470v1","category":"cs.RO"}
{"created":"2024-05-06 13:23:57","title":"Large Language Models (LLMs) as Agents for Augmented Democracy","abstract":"We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants. At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants. At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs. We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population. These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy.","sentences":["We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections.","We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants.","At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants.","At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs.","We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population.","These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy."],"url":"http://arxiv.org/abs/2405.03452v2","category":"cs.CY"}
{"created":"2024-05-06 12:58:21","title":"Annealed adaptive importance sampling method in PINNs for solving high dimensional partial differential equations","abstract":"Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs). However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima. To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density. Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs. Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets. Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs. The adaptive sampling framework introduced here can be integrated into various PINN frameworks.","sentences":["Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs).","However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima.","To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density.","Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs.","Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets.","Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs.","The adaptive sampling framework introduced here can be integrated into various PINN frameworks."],"url":"http://arxiv.org/abs/2405.03433v1","category":"math.NA"}
{"created":"2024-05-06 12:47:16","title":"Geometry-aware framework for deep energy method: an application to structural mechanics with hyperelastic materials","abstract":"Physics-Informed Neural Networks (PINNs) have gained considerable interest in diverse engineering domains thanks to their capacity to integrate physical laws into deep learning models. Recently, geometry-aware PINN-based approaches that employ the strong form of underlying physical system equations have been developed with the aim of integrating geometric information into PINNs. Despite ongoing research, the assessment of PINNs in problems with various geometries remains an active area of investigation. In this work, we introduce a novel physics-informed framework named the Geometry-Aware Deep Energy Method (GADEM) for solving structural mechanics problems on different geometries. As the weak form of the physical system equation (or the energy-based approach) has demonstrated clear advantages compared to the strong form for solving solid mechanics problems, GADEM employs the weak form and aims to infer the solution on multiple shapes of geometries. Integrating a geometry-aware framework into an energy-based method results in an effective physics-informed deep learning model in terms of accuracy and computational cost. Different ways to represent the geometric information and to encode the geometric latent vectors are investigated in this work. We introduce a loss function of GADEM which is minimized based on the potential energy of all considered geometries. An adaptive learning method is also employed for the sampling of collocation points to enhance the performance of GADEM. We present some applications of GADEM to solve solid mechanics problems, including a loading simulation of a toy tire involving contact mechanics and large deformation hyperelasticity. The numerical results of this work demonstrate the remarkable capability of GADEM to infer the solution on various and new shapes of geometries using only one trained model.","sentences":["Physics-Informed Neural Networks (PINNs) have gained considerable interest in diverse engineering domains thanks to their capacity to integrate physical laws into deep learning models.","Recently, geometry-aware PINN-based approaches that employ the strong form of underlying physical system equations have been developed with the aim of integrating geometric information into PINNs.","Despite ongoing research, the assessment of PINNs in problems with various geometries remains an active area of investigation.","In this work, we introduce a novel physics-informed framework named the Geometry-Aware Deep Energy Method (GADEM) for solving structural mechanics problems on different geometries.","As the weak form of the physical system equation (or the energy-based approach) has demonstrated clear advantages compared to the strong form for solving solid mechanics problems, GADEM employs the weak form and aims to infer the solution on multiple shapes of geometries.","Integrating a geometry-aware framework into an energy-based method results in an effective physics-informed deep learning model in terms of accuracy and computational cost.","Different ways to represent the geometric information and to encode the geometric latent vectors are investigated in this work.","We introduce a loss function of GADEM which is minimized based on the potential energy of all considered geometries.","An adaptive learning method is also employed for the sampling of collocation points to enhance the performance of GADEM.","We present some applications of GADEM to solve solid mechanics problems, including a loading simulation of a toy tire involving contact mechanics and large deformation hyperelasticity.","The numerical results of this work demonstrate the remarkable capability of GADEM to infer the solution on various and new shapes of geometries using only one trained model."],"url":"http://arxiv.org/abs/2405.03427v1","category":"cs.LG"}
{"created":"2024-05-06 12:44:37","title":"Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models","abstract":"Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and poor calibration, particularly when fine-tuned on small datasets. To address these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs. Through extensive testing across several Natural Language Processing (NLP) benchmarks, we demonstrate that our straightforward and computationally efficient approach improves model generalization and calibration. We further show that our method exhibits greater robustness against distribution shift, as reflected in its performance on out-of-distribution tasks.","sentences":["Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and poor calibration, particularly when fine-tuned on small datasets.","To address these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA) with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate Bayesian inference in LLMs.","Through extensive testing across several Natural Language Processing (NLP) benchmarks, we demonstrate that our straightforward and computationally efficient approach improves model generalization and calibration.","We further show that our method exhibits greater robustness against distribution shift, as reflected in its performance on out-of-distribution tasks."],"url":"http://arxiv.org/abs/2405.03425v1","category":"cs.CL"}
{"created":"2024-05-06 12:40:15","title":"Implantable Adaptive Cells: differentiable architecture search to improve the performance of any trained U-shaped network","abstract":"This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using Neural Architecture Search (NAS) methods, specifically Differentiable Architecture Search (DARTS). We present the concept of Implantable Adaptive Cell (IAC), small but powerful modules identified through Partially-Connected DARTS, designed to be injected into the skip connections of an existing and already trained U-shaped model. Our strategy allows for the seamless integration of the IAC into the pre-existing architecture, thereby enhancing its performance without necessitating a complete retraining from scratch. The empirical studies, focusing on medical image segmentation tasks, demonstrate the efficacy of this method. The integration of specialized IAC cells into various configurations of the U-Net model increases segmentation accuracy by almost 2\\% points on average for the validation dataset and over 3\\% points for the training dataset. The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains.","sentences":["This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using Neural Architecture Search (NAS) methods, specifically Differentiable Architecture Search (DARTS).","We present the concept of Implantable Adaptive Cell (IAC), small but powerful modules identified through Partially-Connected DARTS, designed to be injected into the skip connections of an existing and already trained U-shaped model.","Our strategy allows for the seamless integration of the IAC into the pre-existing architecture, thereby enhancing its performance without necessitating a complete retraining from scratch.","The empirical studies, focusing on medical image segmentation tasks, demonstrate the efficacy of this method.","The integration of specialized IAC cells into various configurations of the U-Net model increases segmentation accuracy by almost 2\\% points on average for the validation dataset and over 3\\% points for the training dataset.","The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains."],"url":"http://arxiv.org/abs/2405.03420v1","category":"cs.CV"}
{"created":"2024-05-06 12:28:42","title":"Adaptive Accelerated Composite Minimization","abstract":"The choice of the stepsize in first-order convex optimization is typically based on the smoothness constant and plays a crucial role in the performance of algorithms. Recently, there has been a resurgent interest in introducing adaptive stepsizes that do not explicitly depend on smooth constant. In this paper, we propose a novel adaptive stepsize rule based on function evaluations (i.e., zero-order information) that enjoys provable convergence guarantees for both accelerated and non-accelerated gradient descent. We further discuss the similarities and differences between the proposed stepsize regimes and the existing stepsize rules (including Polyak and Armijo). Numerically, we benchmark the performance of our proposed algorithms with the state-of-the-art literature in three different classes of smooth minimization (logistic regression, quadratic programming, log-sum-exponential, and approximate semidefinite programming), composite minimization ($\\ell_1$ constrained and regularized problems), and non-convex minimization (cubic problem).","sentences":["The choice of the stepsize in first-order convex optimization is typically based on the smoothness constant and plays a crucial role in the performance of algorithms.","Recently, there has been a resurgent interest in introducing adaptive stepsizes that do not explicitly depend on smooth constant.","In this paper, we propose a novel adaptive stepsize rule based on function evaluations (i.e., zero-order information) that enjoys provable convergence guarantees for both accelerated and non-accelerated gradient descent.","We further discuss the similarities and differences between the proposed stepsize regimes and the existing stepsize rules (including Polyak and Armijo).","Numerically, we benchmark the performance of our proposed algorithms with the state-of-the-art literature in three different classes of smooth minimization (logistic regression, quadratic programming, log-sum-exponential, and approximate semidefinite programming), composite minimization ($\\ell_1$ constrained and regularized problems), and non-convex minimization (cubic problem)."],"url":"http://arxiv.org/abs/2405.03414v1","category":"math.OC"}
{"created":"2024-05-06 12:24:49","title":"SL-SLAM: A robust visual-inertial SLAM based deep feature extraction and matching","abstract":"This paper explores how deep learning techniques can improve visual-based SLAM performance in challenging environments. By combining deep feature extraction and deep matching methods, we introduce a versatile hybrid visual SLAM system designed to enhance adaptability in challenging scenarios, such as low-light conditions, dynamic lighting, weak-texture areas, and severe jitter. Our system supports multiple modes, including monocular, stereo, monocular-inertial, and stereo-inertial configurations. We also perform analysis how to combine visual SLAM with deep learning methods to enlighten other researches. Through extensive experiments on both public datasets and self-sampled data, we demonstrate the superiority of the SL-SLAM system over traditional approaches. The experimental results show that SL-SLAM outperforms state-of-the-art SLAM algorithms in terms of localization accuracy and tracking robustness. For the benefit of community, we make public the source code at https://github.com/zzzzxxxx111/SLslam.","sentences":["This paper explores how deep learning techniques can improve visual-based SLAM performance in challenging environments.","By combining deep feature extraction and deep matching methods, we introduce a versatile hybrid visual SLAM system designed to enhance adaptability in challenging scenarios, such as low-light conditions, dynamic lighting, weak-texture areas, and severe jitter.","Our system supports multiple modes, including monocular, stereo, monocular-inertial, and stereo-inertial configurations.","We also perform analysis how to combine visual SLAM with deep learning methods to enlighten other researches.","Through extensive experiments on both public datasets and self-sampled data, we demonstrate the superiority of the SL-SLAM system over traditional approaches.","The experimental results show that SL-SLAM outperforms state-of-the-art SLAM algorithms in terms of localization accuracy and tracking robustness.","For the benefit of community, we make public the source code at https://github.com/zzzzxxxx111/SLslam."],"url":"http://arxiv.org/abs/2405.03413v1","category":"cs.RO"}
{"created":"2024-05-06 11:27:27","title":"Knowledge-aware Text-Image Retrieval for Remote Sensing Images","abstract":"Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images. To address this challenge, we propose a Knowledge-aware Text-Image Retrieval (KTIR) method for remote sensing images. By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching. Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications. Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.","sentences":["Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide.","By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only.","For this reason, as a matching-based task, cross-modal text-image retrieval often suffers from information asymmetry between texts and images.","To address this challenge, we propose a Knowledge-aware Text-Image Retrieval (KTIR) method for remote sensing images.","By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between texts and images for better matching.","Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications.","Experimental results on three commonly used remote sensing text-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods."],"url":"http://arxiv.org/abs/2405.03373v1","category":"cs.CV"}
{"created":"2024-05-06 11:25:59","title":"Snake Learning: A Communication- and Computation-Efficient Distributed Learning Framework for 6G","abstract":"In the evolution towards 6G, integrating Artificial Intelligence (AI) with advanced network infrastructure emerges as a pivotal strategy for enhancing network intelligence and resource utilization. Existing distributed learning frameworks like Federated Learning and Split Learning often struggle with significant challenges in dynamic network environments including high synchronization demands, costly communication overheads, severe computing resource consumption, and data heterogeneity across network nodes. These obstacles hinder the applications of ubiquitous computing capabilities of 6G networks, especially in light of the trend of escalating model parameters and training data volumes. To address these challenges effectively, this paper introduces \"Snake Learning\", a cost-effective distributed learning framework. Specifically, Snake Learning respects the heterogeneity of inter-node computing capability and local data distribution in 6G networks, and sequentially trains the designated part of model layers on individual nodes. This layer-by-layer serpentine update mechanism contributes to significantly reducing the requirements for storage, memory and communication during the model training phase, and demonstrates superior adaptability and efficiency for both Computer Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across homogeneous and heterogeneous data distributions.","sentences":["In the evolution towards 6G, integrating Artificial Intelligence (AI) with advanced network infrastructure emerges as a pivotal strategy for enhancing network intelligence and resource utilization.","Existing distributed learning frameworks like Federated Learning and Split Learning often struggle with significant challenges in dynamic network environments including high synchronization demands, costly communication overheads, severe computing resource consumption, and data heterogeneity across network nodes.","These obstacles hinder the applications of ubiquitous computing capabilities of 6G networks, especially in light of the trend of escalating model parameters and training data volumes.","To address these challenges effectively, this paper introduces \"Snake Learning\", a cost-effective distributed learning framework.","Specifically, Snake Learning respects the heterogeneity of inter-node computing capability and local data distribution in 6G networks, and sequentially trains the designated part of model layers on individual nodes.","This layer-by-layer serpentine update mechanism contributes to significantly reducing the requirements for storage, memory and communication during the model training phase, and demonstrates superior adaptability and efficiency for both Computer Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across homogeneous and heterogeneous data distributions."],"url":"http://arxiv.org/abs/2405.03372v1","category":"cs.NI"}
{"created":"2024-05-06 11:04:32","title":"VACO: a Multi-perspective Development of a Therapeutic and Motivational Virtual Robotic Agent for Concentration for children with ADHD","abstract":"In this work, we present (i) a novel approach how artificial intelligence can support in the therapy for better concentration of children with Attention Deficit Hyperactivity Disorder (ADHD) through motivational attention training with a virtual robotic agent and (ii) a development process in which different stakeholders are included with their perspectives. Therefore, we present three participative approaches to include the perspectives of different stakeholders. An online survey (Study I) was conducted with parents in Germany with the aim of ascertaining whether they would use software to promote their children's attention, what influences their attitude towards using it, and what requirements it would have to meet. About half of the parents would be willing to use software to promote attention. To develop the software as close to practice as possible, one of the developers took part in an intensive training for ADHD with the aim of testing which of the elements are technically feasible. Afterward, a first prototype was presented to clinicians (Study II) to make further adjustments. A first feasibility test (Study III) was conducted with the end users to check if the system works and if children and adolescents can use it. Attentional performance software offers multiple opportunities in the treatment of ADHD if the system is adapted to the needs of the practitioner and end user. This development process requires a lot of time and close interdisciplinary collaboration.","sentences":["In this work, we present (i) a novel approach how artificial intelligence can support in the therapy for better concentration of children with Attention Deficit Hyperactivity Disorder (ADHD) through motivational attention training with a virtual robotic agent and (ii) a development process in which different stakeholders are included with their perspectives.","Therefore, we present three participative approaches to include the perspectives of different stakeholders.","An online survey (Study I) was conducted with parents in Germany with the aim of ascertaining whether they would use software to promote their children's attention, what influences their attitude towards using it, and what requirements it would have to meet.","About half of the parents would be willing to use software to promote attention.","To develop the software as close to practice as possible, one of the developers took part in an intensive training for ADHD with the aim of testing which of the elements are technically feasible.","Afterward, a first prototype was presented to clinicians (Study II) to make further adjustments.","A first feasibility test (Study III) was conducted with the end users to check if the system works and if children and adolescents can use it.","Attentional performance software offers multiple opportunities in the treatment of ADHD if the system is adapted to the needs of the practitioner and end user.","This development process requires a lot of time and close interdisciplinary collaboration."],"url":"http://arxiv.org/abs/2405.03354v1","category":"cs.HC"}
{"created":"2024-05-06 11:02:26","title":"Salient Object Detection From Arbitrary Modalities","abstract":"Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications. However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs. Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs. Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD). The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed. The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them. While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images. Accordingly, a preliminary solution to the above challenges, \\i.e. a modality switch network (MSN), is proposed in this paper. In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching. Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure. Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD. Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection.","sentences":["Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications.","However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs.","Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs.","Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD).","The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed.","The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them.","While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images.","Accordingly, a preliminary solution to the above challenges, \\i.e.","a modality switch network (MSN), is proposed in this paper.","In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching.","Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure.","Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD.","Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection."],"url":"http://arxiv.org/abs/2405.03352v1","category":"cs.CV"}
{"created":"2024-05-06 11:02:02","title":"Modality Prompts for Arbitrary Modality Salient Object Detection","abstract":"This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images. A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy. Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality. In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts. Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features. Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion. For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively. Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation.","sentences":["This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images.","A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy.","Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality.","In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts.","Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features.","Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion.","For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively.","Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation."],"url":"http://arxiv.org/abs/2405.03351v1","category":"cs.CV"}
{"created":"2024-05-06 10:53:13","title":"Accelerated MR Cholangiopancreatography with Deep Learning-based Reconstruction","abstract":"This study accelerates MR cholangiopancreatography (MRCP) acquisitions using deep learning-based (DL) reconstruction at 3T and 0.55T. Thirty healthy volunteers underwent conventional two-fold MRCP scans at field strengths of 3T or 0.55T. We trained a variational network (VN) using retrospectively six-fold undersampled data obtained at 3T. We then evaluated our method against standard techniques such as parallel imaging (PI) and compressed sensing (CS), focusing on peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics. Furthermore, considering acquiring fully-sampled MRCP is impractical, we added a self-supervised DL reconstruction (SSDU) to the evaluating group. We also tested our method in a prospective accelerated scenario to reflect real-world clinical applications and evaluated its adaptability to MRCP at 0.55T. Our method demonstrated a remarkable reduction of average acquisition time from 599/542 to 255/180 seconds for MRCP at 3T/0.55T. In both retrospective and prospective undersampling scenarios, the PSNR and SSIM of VN were higher than those of PI, CS, and SSDU. At the same time, VN preserved the image quality of undersampled data, i.e., sharpness and the visibility of hepatobiliary ducts. In addition, VN also produced high quality reconstructions at 0.55T resulting in the highest PSNR and SSIM. In summary, VN trained for highly accelerated MRCP allows to reduce the acquisition time by a factor of 2.4/3.0 at 3T/0.55T while maintaining the image quality of the conventional acquisition.","sentences":["This study accelerates MR cholangiopancreatography (MRCP) acquisitions using deep learning-based (DL) reconstruction at 3T and","0.55T.","Thirty healthy volunteers underwent conventional two-fold MRCP scans at field strengths of 3T or 0.55T.","We trained a variational network (VN) using retrospectively six-fold undersampled data obtained at 3T. We then evaluated our method against standard techniques such as parallel imaging (PI) and compressed sensing (CS), focusing on peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as metrics.","Furthermore, considering acquiring fully-sampled MRCP is impractical, we added a self-supervised DL reconstruction (SSDU) to the evaluating group.","We also tested our method in a prospective accelerated scenario to reflect real-world clinical applications and evaluated its adaptability to MRCP at 0.55T.","Our method demonstrated a remarkable reduction of average acquisition time from 599/542 to 255/180 seconds for MRCP at 3T/0.55T. In both retrospective and prospective undersampling scenarios, the PSNR and SSIM of VN were higher than those of PI, CS, and SSDU.","At the same time, VN preserved the image quality of undersampled data, i.e., sharpness and the visibility of hepatobiliary ducts.","In addition, VN also produced high quality reconstructions at 0.55T resulting in the highest PSNR and SSIM.","In summary, VN trained for highly accelerated MRCP allows to reduce the acquisition time by a factor of 2.4/3.0 at 3T/0.55T while maintaining the image quality of the conventional acquisition."],"url":"http://arxiv.org/abs/2405.03732v1","category":"eess.IV"}
{"created":"2024-05-06 10:50:17","title":"An efficient hierarchical Bayesian method for the Kuopio tomography challenge 2023","abstract":"The aim of Electrical Impedance Tomography (EIT) is to determine the electrical conductivity distribution inside a domain by applying currents and measuring voltages on its boundary. Mathematically, the EIT reconstruction task can be formulated as a non-linear inverse problem. The Bayesian inverse problems framework has been applied expensively to solutions of the EIT inverse problem, in particular in the cases when the unknown conductivity is believed to be blocky. Recently, the Sparsity Promoting Iterative Alternating Sequential (PS-IAS) algorithm, originally proposed for the solution of linear inverse problems, has been adapted for the non linear case of EIT reconstruction in a computationally efficient manner. Here we introduce a hybrid version of the SP-IAS algorithms for the nonlinear EIT inverse problem, providing a detailed description of the implementation details, with a specific focus on parameters selection. The method is applied to the 2023 Kuopio Tomography Challenge dataset, with a comprehensive report of the running times for the different cases and parameter selections.","sentences":["The aim of Electrical Impedance Tomography (EIT) is to determine the electrical conductivity distribution inside a domain by applying currents and measuring voltages on its boundary.","Mathematically, the EIT reconstruction task can be formulated as a non-linear inverse problem.","The Bayesian inverse problems framework has been applied expensively to solutions of the EIT inverse problem, in particular in the cases when the unknown conductivity is believed to be blocky.","Recently, the Sparsity Promoting Iterative Alternating Sequential (PS-IAS) algorithm, originally proposed for the solution of linear inverse problems, has been adapted for the non linear case of EIT reconstruction in a computationally efficient manner.","Here we introduce a hybrid version of the SP-IAS algorithms for the nonlinear EIT inverse problem, providing a detailed description of the implementation details, with a specific focus on parameters selection.","The method is applied to the 2023 Kuopio Tomography Challenge dataset, with a comprehensive report of the running times for the different cases and parameter selections."],"url":"http://arxiv.org/abs/2405.03343v1","category":"math.NA"}
{"created":"2024-05-06 10:49:51","title":"Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning","abstract":"Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.","sentences":["Causal effect estimation under networked interference is an important but challenging problem.","Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process.","To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks.","Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness.","Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss.","Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model.","Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators."],"url":"http://arxiv.org/abs/2405.03342v1","category":"cs.LG"}
{"created":"2024-05-06 10:40:34","title":"Functional Equivalence with NARS","abstract":"This study explores the concept of functional equivalence within the framework of the Non-Axiomatic Reasoning System (NARS), specifically through OpenNARS for Applications (ONA). Functional equivalence allows organisms to categorize and respond to varied stimuli based on their utility rather than perceptual similarity, thus enhancing cognitive efficiency and adaptability. In this study, ONA was modified to allow the derivation of functional equivalence. This paper provides practical examples of the capability of ONA to apply learned knowledge across different functional situations, demonstrating its utility in complex problem-solving and decision-making. An extended example is included, where training of ONA aimed to learn basic human-like language abilities, using a systematic procedure in relating spoken words, objects and written words. The research carried out as part of this study extends the understanding of functional equivalence in AGI systems, and argues for its necessity for level of flexibility in learning and adapting necessary for human-level AGI.","sentences":["This study explores the concept of functional equivalence within the framework of the Non-Axiomatic Reasoning System (NARS), specifically through OpenNARS for Applications (ONA).","Functional equivalence allows organisms to categorize and respond to varied stimuli based on their utility rather than perceptual similarity, thus enhancing cognitive efficiency and adaptability.","In this study, ONA was modified to allow the derivation of functional equivalence.","This paper provides practical examples of the capability of ONA to apply learned knowledge across different functional situations, demonstrating its utility in complex problem-solving and decision-making.","An extended example is included, where training of ONA aimed to learn basic human-like language abilities, using a systematic procedure in relating spoken words, objects and written words.","The research carried out as part of this study extends the understanding of functional equivalence in AGI systems, and argues for its necessity for level of flexibility in learning and adapting necessary for human-level AGI."],"url":"http://arxiv.org/abs/2405.03340v1","category":"cs.AI"}
{"created":"2024-05-06 10:05:46","title":"Clustering of Disease Trajectories with Explainable Machine Learning: A Case Study on Postoperative Delirium Phenotypes","abstract":"The identification of phenotypes within complex diseases or syndromes is a fundamental component of precision medicine, which aims to adapt healthcare to individual patient characteristics. Postoperative delirium (POD) is a complex neuropsychiatric condition with significant heterogeneity in its clinical manifestations and underlying pathophysiology. We hypothesize that POD comprises several distinct phenotypes, which cannot be directly observed in clinical practice. Identifying these phenotypes could enhance our understanding of POD pathogenesis and facilitate the development of targeted prevention and treatment strategies. In this paper, we propose an approach that combines supervised machine learning for personalized POD risk prediction with unsupervised clustering techniques to uncover potential POD phenotypes. We first demonstrate our approach using synthetic data, where we simulate patient cohorts with predefined phenotypes based on distinct sets of informative features. We aim to mimic any clinical disease with our synthetic data generation method. By training a predictive model and applying SHAP, we show that clustering patients in the SHAP feature importance space successfully recovers the true underlying phenotypes, outperforming clustering in the raw feature space. We then present a case study using real-world data from a cohort of elderly surgical patients. The results showcase the utility of our approach in uncovering clinically relevant subtypes of complex disorders like POD, paving the way for more precise and personalized treatment strategies.","sentences":["The identification of phenotypes within complex diseases or syndromes is a fundamental component of precision medicine, which aims to adapt healthcare to individual patient characteristics.","Postoperative delirium (POD) is a complex neuropsychiatric condition with significant heterogeneity in its clinical manifestations and underlying pathophysiology.","We hypothesize that POD comprises several distinct phenotypes, which cannot be directly observed in clinical practice.","Identifying these phenotypes could enhance our understanding of POD pathogenesis and facilitate the development of targeted prevention and treatment strategies.","In this paper, we propose an approach that combines supervised machine learning for personalized POD risk prediction with unsupervised clustering techniques to uncover potential POD phenotypes.","We first demonstrate our approach using synthetic data, where we simulate patient cohorts with predefined phenotypes based on distinct sets of informative features.","We aim to mimic any clinical disease with our synthetic data generation method.","By training a predictive model and applying SHAP, we show that clustering patients in the SHAP feature importance space successfully recovers the true underlying phenotypes, outperforming clustering in the raw feature space.","We then present a case study using real-world data from a cohort of elderly surgical patients.","The results showcase the utility of our approach in uncovering clinically relevant subtypes of complex disorders like POD, paving the way for more precise and personalized treatment strategies."],"url":"http://arxiv.org/abs/2405.03327v1","category":"cs.LG"}
{"created":"2024-05-06 09:50:04","title":"Enhancing DETRs Variants through Improved Content Query and Similar Query Aggregation","abstract":"The design of the query is crucial for the performance of DETR and its variants. Each query consists of two components: a content part and a positional one. Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance. In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation. The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling. This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects. However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones. To overcome this, we propose a query aggregation strategy to cooperate with SACQ. It merges similar predicted candidates from different queries, easing the optimization. Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP.","sentences":["The design of the query is crucial for the performance of DETR and its variants.","Each query consists of two components: a content part and a positional one.","Traditionally, the content query is initialized with a zero or learnable embedding, lacking essential content information and resulting in sub-optimal performance.","In this paper, we introduce a novel plug-and-play module, Self-Adaptive Content Query (SACQ), to address this limitation.","The SACQ module utilizes features from the transformer encoder to generate content queries via self-attention pooling.","This allows candidate queries to adapt to the input image, resulting in a more comprehensive content prior and better focus on target objects.","However, this improved concentration poses a challenge for the training process that utilizes the Hungarian matching, which selects only a single candidate and suppresses other similar ones.","To overcome this, we propose a query aggregation strategy to cooperate with SACQ.","It merges similar predicted candidates from different queries, easing the optimization.","Our extensive experiments on the COCO dataset demonstrate the effectiveness of our proposed approaches across six different DETR's variants with multiple configurations, achieving an average improvement of over 1.0 AP."],"url":"http://arxiv.org/abs/2405.03318v1","category":"cs.CV"}
{"created":"2024-05-06 09:14:58","title":"Deep Learning and genetic algorithms for cosmological Bayesian inference speed-up","abstract":"In this paper, we present a novel approach to accelerate the Bayesian inference process, focusing specifically on the nested sampling algorithms. Bayesian inference plays a crucial role in cosmological parameter estimation, providing a robust framework for extracting theoretical insights from observational data. However, its computational demands can be substantial, primarily due to the need for numerous likelihood function evaluations. Our proposed method utilizes the power of deep learning, employing feedforward neural networks to approximate the likelihood function dynamically during the Bayesian inference process. Unlike traditional approaches, our method trains neural networks on-the-fly using the current set of live points as training data, without the need for pre-training. This flexibility enables adaptation to various theoretical models and datasets. We perform simple hyperparameter optimization using genetic algorithms to suggest initial neural network architectures for learning each likelihood function. Once sufficient accuracy is achieved, the neural network replaces the original likelihood function. The implementation integrates with nested sampling algorithms and has been thoroughly evaluated using both simple cosmological dark energy models and diverse observational datasets. Additionally, we explore the potential of genetic algorithms for generating initial live points within nested sampling inference, opening up new avenues for enhancing the efficiency and effectiveness of Bayesian inference methods.","sentences":["In this paper, we present a novel approach to accelerate the Bayesian inference process, focusing specifically on the nested sampling algorithms.","Bayesian inference plays a crucial role in cosmological parameter estimation, providing a robust framework for extracting theoretical insights from observational data.","However, its computational demands can be substantial, primarily due to the need for numerous likelihood function evaluations.","Our proposed method utilizes the power of deep learning, employing feedforward neural networks to approximate the likelihood function dynamically during the Bayesian inference process.","Unlike traditional approaches, our method trains neural networks on-the-fly using the current set of live points as training data, without the need for pre-training.","This flexibility enables adaptation to various theoretical models and datasets.","We perform simple hyperparameter optimization using genetic algorithms to suggest initial neural network architectures for learning each likelihood function.","Once sufficient accuracy is achieved, the neural network replaces the original likelihood function.","The implementation integrates with nested sampling algorithms and has been thoroughly evaluated using both simple cosmological dark energy models and diverse observational datasets.","Additionally, we explore the potential of genetic algorithms for generating initial live points within nested sampling inference, opening up new avenues for enhancing the efficiency and effectiveness of Bayesian inference methods."],"url":"http://arxiv.org/abs/2405.03293v1","category":"astro-ph.IM"}
{"created":"2024-05-06 09:02:54","title":"A continuous approach for computing the pseudospectra of linear operators","abstract":"We propose a continuous approach for computing the pseudospectra of linear operators following a 'solve-then-discretize' strategy. Instead of taking a finite section approach or using a finite-dimensional matrix to approximate the operator of interest, the new method employs an operator analogue of the Lanczos process to work directly with operators and functions. The method is shown to be free of spectral pollution and spectral invisibility, fully adaptive, nearly optimal in accuracy, and well-conditioned. The advantages of the method are demonstrated by extensive numerical examples and comparison with the traditional method.","sentences":["We propose a continuous approach for computing the pseudospectra of linear operators following a 'solve-then-discretize' strategy.","Instead of taking a finite section approach or using a finite-dimensional matrix to approximate the operator of interest, the new method employs an operator analogue of the Lanczos process to work directly with operators and functions.","The method is shown to be free of spectral pollution and spectral invisibility, fully adaptive, nearly optimal in accuracy, and well-conditioned.","The advantages of the method are demonstrated by extensive numerical examples and comparison with the traditional method."],"url":"http://arxiv.org/abs/2405.03285v1","category":"math.NA"}
{"created":"2024-05-06 08:49:37","title":"Distributed Adaptive Spatial Filtering with Inexact Local Solvers","abstract":"The Distributed Adaptive Signal Fusion (DASF) framework is a meta-algorithm for computing data-driven spatial filters in a distributed sensing platform with limited bandwidth and computational resources, such as a wireless sensor network. The convergence and optimality of the DASF algorithm has been extensively studied under the assumption that an exact, but possibly impractical solver for the local optimization problem at each updating node is available. In this work, we provide convergence and optimality results for the DASF framework when used with an inexact, finite-time solver such as (proximal) gradient descent or Newton's method. We provide sufficient conditions that the solver should satisfy in order to guarantee convergence of the resulting algorithm, and a lower bound for the convergence rate. We also provide numerical simulations to validate these theoretical results.","sentences":["The Distributed Adaptive Signal Fusion (DASF) framework is a meta-algorithm for computing data-driven spatial filters in a distributed sensing platform with limited bandwidth and computational resources, such as a wireless sensor network.","The convergence and optimality of the DASF algorithm has been extensively studied under the assumption that an exact, but possibly impractical solver for the local optimization problem at each updating node is available.","In this work, we provide convergence and optimality results for the DASF framework when used with an inexact, finite-time solver such as (proximal) gradient descent or Newton's method.","We provide sufficient conditions that the solver should satisfy in order to guarantee convergence of the resulting algorithm, and a lower bound for the convergence rate.","We also provide numerical simulations to validate these theoretical results."],"url":"http://arxiv.org/abs/2405.03277v1","category":"eess.SP"}
{"created":"2024-05-06 08:44:17","title":"Evaluation of Drivers' Interaction Ability at Social Scenarios: A Process-Based Framework","abstract":"Assessing drivers' interaction capabilities is crucial for understanding human driving behavior and enhancing the interactive abilities of autonomous vehicles. In scenarios involving strong interaction, existing metrics focused on interaction outcomes struggle to capture the evolutionary process of drivers' interactive behaviors, making it challenging for autonomous vehicles to dynamically assess and respond to other agents during interactions. To address this issue, we propose a framework for assessing drivers' interaction capabilities, oriented towards the interactive process itself, which includes three components: Interaction Risk Perception, Interaction Process Modeling, and Interaction Ability Scoring. We quantify interaction risks through motion state estimation and risk field theory, followed by introducing a dynamic action assessment benchmark based on a game-theoretical rational agent model, and designing a capability scoring metric based on morphological similarity distance. By calculating real-time differences between a driver's actions and the assessment benchmark, the driver's interaction capabilities are scored dynamically. We validated our framework at unsignalized intersections as a typical scenario. Validation analysis on driver behavior datasets from China and the USA shows that our framework effectively distinguishes and evaluates conservative and aggressive driving states during interactions, demonstrating good adaptability and effectiveness in various regional settings.","sentences":["Assessing drivers' interaction capabilities is crucial for understanding human driving behavior and enhancing the interactive abilities of autonomous vehicles.","In scenarios involving strong interaction, existing metrics focused on interaction outcomes struggle to capture the evolutionary process of drivers' interactive behaviors, making it challenging for autonomous vehicles to dynamically assess and respond to other agents during interactions.","To address this issue, we propose a framework for assessing drivers' interaction capabilities, oriented towards the interactive process itself, which includes three components: Interaction Risk Perception, Interaction Process Modeling, and Interaction Ability Scoring.","We quantify interaction risks through motion state estimation and risk field theory, followed by introducing a dynamic action assessment benchmark based on a game-theoretical rational agent model, and designing a capability scoring metric based on morphological similarity distance.","By calculating real-time differences between a driver's actions and the assessment benchmark, the driver's interaction capabilities are scored dynamically.","We validated our framework at unsignalized intersections as a typical scenario.","Validation analysis on driver behavior datasets from China and the USA shows that our framework effectively distinguishes and evaluates conservative and aggressive driving states during interactions, demonstrating good adaptability and effectiveness in various regional settings."],"url":"http://arxiv.org/abs/2405.03273v1","category":"cs.RO"}
{"created":"2024-05-06 08:35:09","title":"Generalized thermodynamic relations for perfect spin hydrodynamics","abstract":"Generalized thermodynamic relations are introduced into the framework of a relativistic perfect spin hydrodynamics. They allow for consistent treatment of spin degrees of freedom, including the use of spin tensors whose structure follows from microscopic calculations. The obtained results are important for establishing consistency between different formulations of spin hydrodynamics and form the basis for introducing dissipative corrections.","sentences":["Generalized thermodynamic relations are introduced into the framework of a relativistic perfect spin hydrodynamics.","They allow for consistent treatment of spin degrees of freedom, including the use of spin tensors whose structure follows from microscopic calculations.","The obtained results are important for establishing consistency between different formulations of spin hydrodynamics and form the basis for introducing dissipative corrections."],"url":"http://arxiv.org/abs/2405.03263v1","category":"hep-ph"}
{"created":"2024-05-08 17:54:43","title":"Radiative corrections to the dynamics of a tracer particle coupled to a Bose scalar field","abstract":"We consider a tracer particle coupled to a Bose scalar field and study the regime where the field's propagation speed approaches infinity. For initial states devoid of field excitations, we introduce an effective approximation of the time-evolved wave function and prove its validity in Hilbert space norm. In this approximation, the field remains in the vacuum state while the tracer particle propagates with a modified dispersion relation. Physically, the new dispersion relation can be understood as the effect of radiative corrections due to interactions with virtual bosons. Mathematically, it is defined as the solution of a self-consistent equation, whose form depends on the relevant time scale.","sentences":["We consider a tracer particle coupled to a Bose scalar field and study the regime where the field's propagation speed approaches infinity.","For initial states devoid of field excitations, we introduce an effective approximation of the time-evolved wave function and prove its validity in Hilbert space norm.","In this approximation, the field remains in the vacuum state while the tracer particle propagates with a modified dispersion relation.","Physically, the new dispersion relation can be understood as the effect of radiative corrections due to interactions with virtual bosons.","Mathematically, it is defined as the solution of a self-consistent equation, whose form depends on the relevant time scale."],"url":"http://arxiv.org/abs/2405.05251v1","category":"math-ph"}
{"created":"2024-05-08 17:54:18","title":"DanceCam: atmospheric turbulence mitigation in wide-field astronomical images with short-exposure video streams","abstract":"We introduce a novel technique to mitigate the adverse effects of atmospheric turbulence on astronomical imaging. Utilizing a video-to-image neural network trained on simulated data, our method processes a sliding sequence of short-exposure ($\\sim$0.2s) stellar field images to reconstruct an image devoid of both turbulence and noise. We demonstrate the method with simulated and observed stellar fields, and show that the brief exposure sequence allows the network to accurately associate speckles to their originating stars and effectively disentangle light from adjacent sources across a range of seeing conditions, all while preserving flux to a lower signal-to-noise ratio than an average stack. This approach results in a marked improvement in angular resolution without compromising the astrometric stability of the final image.","sentences":["We introduce a novel technique to mitigate the adverse effects of atmospheric turbulence on astronomical imaging.","Utilizing a video-to-image neural network trained on simulated data, our method processes a sliding sequence of short-exposure ($\\sim$0.2s) stellar field images to reconstruct an image devoid of both turbulence and noise.","We demonstrate the method with simulated and observed stellar fields, and show that the brief exposure sequence allows the network to accurately associate speckles to their originating stars and effectively disentangle light from adjacent sources across a range of seeing conditions, all while preserving flux to a lower signal-to-noise ratio than an average stack.","This approach results in a marked improvement in angular resolution without compromising the astrometric stability of the final image."],"url":"http://arxiv.org/abs/2405.05250v1","category":"astro-ph.IM"}
{"created":"2024-05-08 17:15:06","title":"The Harnack inequality fails for nonlocal kinetic equations","abstract":"We prove that the Harnack inequality fails for nonlocal kinetic equations. Such equations arise as linearized models for the Boltzmann equation without cutoff and are of hypoelliptic type. We provide a counterexample for the simplest equation in this theory, the fractional Kolmogorov equation. Our result reflects a purely nonlocal phenomenon since the Harnack inequality holds true for local kinetic equations like the Kolmogorov equation.","sentences":["We prove that the Harnack inequality fails for nonlocal kinetic equations.","Such equations arise as linearized models for the Boltzmann equation without cutoff and are of hypoelliptic type.","We provide a counterexample for the simplest equation in this theory, the fractional Kolmogorov equation.","Our result reflects a purely nonlocal phenomenon since the Harnack inequality holds true for local kinetic equations like the Kolmogorov equation."],"url":"http://arxiv.org/abs/2405.05223v1","category":"math.AP"}
{"created":"2024-05-08 16:23:06","title":"ContEvol formalism: possibly a new twist on computational physics","abstract":"We present the ContEvol (continuous evolution) formalism, a family of implicit numerical methods which only need to solve linear equations and are almost symplectic. Combining values and derivatives of functions, ContEvol outputs allow users to recover full history and render full distributions. Using classic harmonic oscillator as a prototype case, we show that ContEvol methods lead to lower-order errors than two commonly used Runge--Kutta methods. Applying first-order ContEvol to simple celestial mechanics problems, we demonstrate that deviation from equation(s) of motion of ContEvol tracks is still $\\mathcal{O}(h^5)$ ($h$ is the step length) by our definition. Numerical experiments with an eccentric elliptical orbit indicate that first-order ContEvol is a viable alternative to classic Runge--Kutta or the symplectic leapfrog integrator. Solving stationary Schr\\\"odinger equation in quantum mechanics, we manifest ability of ContEvol to handle boundary value or eigenvalue problems. Important directions for future work, including mathematical foundation, higher dimensions, and technical improvements, are discussed at the end of this article.","sentences":["We present the ContEvol (continuous evolution) formalism, a family of implicit numerical methods which only need to solve linear equations and are almost symplectic.","Combining values and derivatives of functions, ContEvol outputs allow users to recover full history and render full distributions.","Using classic harmonic oscillator as a prototype case, we show that ContEvol methods lead to lower-order errors than two commonly used Runge--Kutta methods.","Applying first-order ContEvol to simple celestial mechanics problems, we demonstrate that deviation from equation(s) of motion of ContEvol tracks is still $\\mathcal{O}(h^5)$ ($h$ is the step length) by our definition.","Numerical experiments with an eccentric elliptical orbit indicate that first-order ContEvol is a viable alternative to classic Runge--Kutta or the symplectic leapfrog integrator.","Solving stationary Schr\\\"odinger equation in quantum mechanics, we manifest ability of ContEvol to handle boundary value or eigenvalue problems.","Important directions for future work, including mathematical foundation, higher dimensions, and technical improvements, are discussed at the end of this article."],"url":"http://arxiv.org/abs/2405.05188v1","category":"astro-ph.IM"}
{"created":"2024-05-08 16:22:47","title":"A score-based particle method for homogeneous Landau equation","abstract":"We propose a novel score-based particle method for solving the Landau equation in plasmas, that seamlessly integrates learning with structure-preserving particle methods [arXiv:1910.03080]. Building upon the Lagrangian viewpoint of the Landau equation, a central challenge stems from the nonlinear dependence of the velocity field on the density. Our primary innovation lies in recognizing that this nonlinearity is in the form of the score function, which can be approximated dynamically via techniques from score-matching. The resulting method inherits the conservation properties of the deterministic particle method while sidestepping the necessity for kernel density estimation in [arXiv:1910.03080]. This streamlines computation and enhances scalability with dimensionality. Furthermore, we provide a theoretical estimate by demonstrating that the KL divergence between our approximation and the true solution can be effectively controlled by the score-matching loss. Additionally, by adopting the flow map viewpoint, we derive an update formula for exact density computation. Extensive examples have been provided to show the efficiency of the method, including a physically relevant case of Coulomb interaction.","sentences":["We propose a novel score-based particle method for solving the Landau equation in plasmas, that seamlessly integrates learning with structure-preserving particle methods [arXiv:1910.03080].","Building upon the Lagrangian viewpoint of the Landau equation, a central challenge stems from the nonlinear dependence of the velocity field on the density.","Our primary innovation lies in recognizing that this nonlinearity is in the form of the score function, which can be approximated dynamically via techniques from score-matching.","The resulting method inherits the conservation properties of the deterministic particle method while sidestepping the necessity for kernel density estimation in [arXiv:1910.03080].","This streamlines computation and enhances scalability with dimensionality.","Furthermore, we provide a theoretical estimate by demonstrating that the KL divergence between our approximation and the true solution can be effectively controlled by the score-matching loss.","Additionally, by adopting the flow map viewpoint, we derive an update formula for exact density computation.","Extensive examples have been provided to show the efficiency of the method, including a physically relevant case of Coulomb interaction."],"url":"http://arxiv.org/abs/2405.05187v1","category":"math.NA"}
{"created":"2024-05-08 15:51:32","title":"Analysis of the SQP Method for Hyperbolic PDE-Constrained Optimization in Acoustic Full Waveform Inversion","abstract":"In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered. The model arises from the acoustic full waveform inversion in the time domain. The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure. This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques. We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations. Our final theoretical result is the R-superlinear convergence of the SQP method.","sentences":["In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered.","The model arises from the acoustic full waveform inversion in the time domain.","The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure.","This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques.","We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations.","Our final theoretical result is the R-superlinear convergence of the SQP method."],"url":"http://arxiv.org/abs/2405.05158v1","category":"math.NA"}
{"created":"2024-05-08 15:09:51","title":"Anharmonic phonons with Gaussian processes","abstract":"We provide a method for calculating anharmonic lattice dynamics, by building a surrogate model based on Gaussian Processes (GPs). Due to the underlying Gaussian form of a GP, the model is infinitely differentiable. This allows us to train the model trained directly on forces (the derivative of PESs) reducing the evaluations required for a given accuracy. We can extend this differentiation to directly calculate second and third order force-constants using automatic differentiation (AD). For the five model materials we study, we find that the force-constants are in close agreement with a standard finite-displacement approach. Our method appears to be linear scaling in the number of atoms at predicting both second and third-order (anharmonic) force-constants.","sentences":["We provide a method for calculating anharmonic lattice dynamics, by building a surrogate model based on Gaussian Processes (GPs).","Due to the underlying Gaussian form of a GP, the model is infinitely differentiable.","This allows us to train the model trained directly on forces (the derivative of PESs) reducing the evaluations required for a given accuracy.","We can extend this differentiation to directly calculate second and third order force-constants using automatic differentiation (AD).","For the five model materials we study, we find that the force-constants are in close agreement with a standard finite-displacement approach.","Our method appears to be linear scaling in the number of atoms at predicting both second and third-order (anharmonic) force-constants."],"url":"http://arxiv.org/abs/2405.05113v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 14:01:48","title":"Quasi-Banach Schatten-von Neumann properties in Weyl-H\u00f6rmander calculus","abstract":"We study structural properties of Wiener-Lebesgue spaces with respect to a slowly varying metrics and certain Lebesgue parameters. For $p\\in (0,1]$, we deduce Schatten-$p$ properties for pseudo-differential operators whose symbols, together with their derivatives, obey suitable Wiener-Lebesgue-boundedness conditions. Especially, we perform such investigations for the Weyl-H\\\"ormander calculus. Finally, we apply our results to global-type SG and Shubin pseudo-differential operators.","sentences":["We study structural properties of Wiener-Lebesgue spaces with respect to a slowly varying metrics and certain Lebesgue parameters.","For $p\\in (0,1]$, we deduce Schatten-$p$ properties for pseudo-differential operators whose symbols, together with their derivatives, obey suitable Wiener-Lebesgue-boundedness conditions.","Especially, we perform such investigations for the Weyl-H\\\"ormander calculus.","Finally, we apply our results to global-type SG and Shubin pseudo-differential operators."],"url":"http://arxiv.org/abs/2405.05065v1","category":"math.FA"}
{"created":"2024-05-08 13:00:56","title":"Mitigating Bias Using Model-Agnostic Data Attribution","abstract":"Mitigating bias in machine learning models is a critical endeavor for ensuring fairness and equity. In this paper, we propose a novel approach to address bias by leveraging pixel image attributions to identify and regularize regions of images containing significant information about bias attributes. Our method utilizes a model-agnostic approach to extract pixel attributions by employing a convolutional neural network (CNN) classifier trained on small image patches. By training the classifier to predict a property of the entire image using only a single patch, we achieve region-based attributions that provide insights into the distribution of important information across the image. We propose utilizing these attributions to introduce targeted noise into datasets with confounding attributes that bias the data, thereby constraining neural networks from learning these biases and emphasizing the primary attributes. Our approach demonstrates its efficacy in enabling the training of unbiased classifiers on heavily biased datasets.","sentences":["Mitigating bias in machine learning models is a critical endeavor for ensuring fairness and equity.","In this paper, we propose a novel approach to address bias by leveraging pixel image attributions to identify and regularize regions of images containing significant information about bias attributes.","Our method utilizes a model-agnostic approach to extract pixel attributions by employing a convolutional neural network (CNN) classifier trained on small image patches.","By training the classifier to predict a property of the entire image using only a single patch, we achieve region-based attributions that provide insights into the distribution of important information across the image.","We propose utilizing these attributions to introduce targeted noise into datasets with confounding attributes that bias the data, thereby constraining neural networks from learning these biases and emphasizing the primary attributes.","Our approach demonstrates its efficacy in enabling the training of unbiased classifiers on heavily biased datasets."],"url":"http://arxiv.org/abs/2405.05031v1","category":"cs.CV"}
{"created":"2024-05-08 12:54:52","title":"On some intrinsic differentiability properties for Absolutely continuous functions between Carnot groups and the Area formula","abstract":"We discuss Q-absolutely continuous functions between Carnot groups, following Maly's definition for maps of several variables. Such maps enjoy nice regularity properties, like continuity, Pansu differentiability a.e., weak differentiability and an Area formula. Furthermore, we extend Stein's result concerning the sharp condition for continuity and differentiability a.e. of a Sobolev map in terms of the integrability of the weak gradient: more precisely, we prove that a Sobolev map between Carnot groups with horizontal gradient of its sections uniformly bounded in L(Q,1) admits a representative which is Q-absolutely continuous.","sentences":["We discuss Q-absolutely continuous functions between Carnot groups, following Maly's definition for maps of several variables.","Such maps enjoy nice regularity properties, like continuity, Pansu differentiability a.e., weak differentiability and an Area formula.","Furthermore, we extend Stein's result concerning the sharp condition for continuity and differentiability a.e. of a Sobolev map in terms of the integrability of the weak gradient: more precisely, we prove that a Sobolev map between Carnot groups with horizontal gradient of its sections uniformly bounded in L(Q,1) admits a representative which is Q-absolutely continuous."],"url":"http://arxiv.org/abs/2405.05024v1","category":"math.FA"}
{"created":"2024-05-08 12:25:21","title":"${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields","abstract":"Neural fields (NeRF) have emerged as a promising approach for representing continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs poses a significant challenge for scene decomposition. To address this challenge, we present a single model, Multi-Modal Decomposition NeRF (${M^2D}$NeRF), that is capable of both text-based and visual patch-based edits. Specifically, we use multi-modal feature distillation to integrate teacher features from pretrained visual and language models into 3D semantic feature volumes, thereby facilitating consistent 3D editing. To enforce consistency between the visual and language features in our 3D feature volumes, we introduce a multi-modal similarity constraint. We also introduce a patch-based joint contrastive loss that helps to encourage object-regions to coalesce in the 3D feature space, resulting in more precise boundaries. Experiments on various real-world scenes show superior performance in 3D scene decomposition tasks compared to prior NeRF-based methods.","sentences":["Neural fields (NeRF) have emerged as a promising approach for representing continuous 3D scenes.","Nevertheless, the lack of semantic encoding in NeRFs poses a significant challenge for scene decomposition.","To address this challenge, we present a single model, Multi-Modal Decomposition NeRF (${M^2D}$NeRF), that is capable of both text-based and visual patch-based edits.","Specifically, we use multi-modal feature distillation to integrate teacher features from pretrained visual and language models into 3D semantic feature volumes, thereby facilitating consistent 3D editing.","To enforce consistency between the visual and language features in our 3D feature volumes, we introduce a multi-modal similarity constraint.","We also introduce a patch-based joint contrastive loss that helps to encourage object-regions to coalesce in the 3D feature space, resulting in more precise boundaries.","Experiments on various real-world scenes show superior performance in 3D scene decomposition tasks compared to prior NeRF-based methods."],"url":"http://arxiv.org/abs/2405.05010v1","category":"cs.CV"}
{"created":"2024-05-08 12:14:34","title":"HMANet: Hybrid Multi-Axis Aggregation Network for Image Super-Resolution","abstract":"Transformer-based methods have demonstrated excellent performance on super-resolution visual tasks, surpassing conventional convolutional neural networks. However, existing work typically restricts self-attention computation to non-overlapping windows to save computational costs. This means that Transformer-based networks can only use input information from a limited spatial range. Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA) is proposed in this paper to exploit feature potential information better. HMA is constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid Attention Blocks(GAB). On the one side, RHTB combines channel attention and self-attention to enhance non-local feature fusion and produce more attractive visual results. Conversely, GAB is used in cross-domain information interaction to jointly model similar features and obtain a larger perceptual field. For the super-resolution task in the training phase, a novel pre-training method is designed to enhance the model representation capabilities further and validate the proposed model's effectiveness through many experiments. The experimental results show that HMA outperforms the state-of-the-art methods on the benchmark dataset. We provide code and models at https://github.com/korouuuuu/HMA.","sentences":["Transformer-based methods have demonstrated excellent performance on super-resolution visual tasks, surpassing conventional convolutional neural networks.","However, existing work typically restricts self-attention computation to non-overlapping windows to save computational costs.","This means that Transformer-based networks can only use input information from a limited spatial range.","Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA) is proposed in this paper to exploit feature potential information better.","HMA is constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid Attention Blocks(GAB).","On the one side, RHTB combines channel attention and self-attention to enhance non-local feature fusion and produce more attractive visual results.","Conversely, GAB is used in cross-domain information interaction to jointly model similar features and obtain a larger perceptual field.","For the super-resolution task in the training phase, a novel pre-training method is designed to enhance the model representation capabilities further and validate the proposed model's effectiveness through many experiments.","The experimental results show that HMA outperforms the state-of-the-art methods on the benchmark dataset.","We provide code and models at https://github.com/korouuuuu/HMA."],"url":"http://arxiv.org/abs/2405.05001v1","category":"cs.CV"}
{"created":"2024-05-08 10:55:47","title":"Computation of some dispersive equations through their iterated linearisation","abstract":"It is often the case that, while the numerical solution of the non-linear dispersive equation $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}(u(t),t)u(t)$ represents a formidable challenge, it is fairly easy and cheap to solve closely related linear equations of the form $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}_1(t)u(t)+\\widetilde{\\mathcal H}_2(t)u(t)$, where $\\mathcal{H}_1(t)+\\mathcal{H}_2(v,t)=\\mathcal{H}(v,t)$. In that case we advocate an iterative linearisation procedure that involves fixed-point iteration of the latter equation to solve the former. A typical case is when the original problem is a nonlinear Schr\\\"odinger or Gross--Pitaevskii equation, while the `easy' equation is linear Schr\\\"odinger with time-dependent potential.   We analyse in detail the iterative scheme and its practical implementation, prove that each iteration increases the order, derive upper bounds on the speed of convergence and discuss in the case of nonlinear Schr\\\"odinger equation with cubic potential the preservation of structural features of the underlying equation: the $\\mathrm{L}_2$ norm, momentum and Hamiltonian energy. A key ingredient in our approach is the use of the Magnus expansion in conjunction with Hermite quadratures, which allows effective solutions of the linearised but non-autonomous equations in an iterative fashion. The resulting Magnus--Hermite methods can be combined with a wide range of numerical approximations to the matrix exponential. The paper concludes with a number of numerical experiments, demonstrating the power of the proposed approach.","sentences":["It is often the case that, while the numerical solution of the non-linear dispersive equation $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}(u(t),t)u(t)$ represents a formidable challenge, it is fairly easy and cheap to solve closely related linear equations of the form $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}_1(t)u(t)+\\widetilde{\\mathcal H}_2(t)u(t)$, where $\\mathcal{H}_1(t)+\\mathcal{H}_2(v,t)=\\mathcal{H}(v,t)$.","In that case we advocate an iterative linearisation procedure that involves fixed-point iteration of the latter equation to solve the former.","A typical case is when the original problem is a nonlinear Schr\\\"odinger or Gross--Pitaevskii equation, while the `easy' equation is linear Schr\\\"odinger with time-dependent potential.   ","We analyse in detail the iterative scheme and its practical implementation, prove that each iteration increases the order, derive upper bounds on the speed of convergence and discuss in the case of nonlinear Schr\\\"odinger equation with cubic potential the preservation of structural features of the underlying equation: the $\\mathrm{L}_2$ norm, momentum and Hamiltonian energy.","A key ingredient in our approach is the use of the Magnus expansion in conjunction with Hermite quadratures, which allows effective solutions of the linearised but non-autonomous equations in an iterative fashion.","The resulting Magnus--Hermite methods can be combined with a wide range of numerical approximations to the matrix exponential.","The paper concludes with a number of numerical experiments, demonstrating the power of the proposed approach."],"url":"http://arxiv.org/abs/2405.04958v1","category":"math.NA"}
{"created":"2024-05-08 10:51:08","title":"Modified version of open TASEP with dynamic defects","abstract":"We propose a modification to the study of site-wise dynamically disordered totally asymmetric simple exclusion process (TASEP). Motivated by the process of gene transcription, a study in ref. [39] introduced an extension of TASEP, where the defects (or obstacles) bind/un-bind dynamically to the sites of the lattice and the hopping of the particles on lattice faces a hindrance if the arrival site is occupied by an obstacle. In addition, the particle is only allowed to enter the lattice provided the first site is defect-free. In our study, we propose that the particle movement at the entry of the lattice must face an equal hindrance that is provided by the obstacles to the rest of the particles on the lattice. For open boundaries, the continuum mean-field equations are derived and solved numerically to obtain steady-state phase diagrams and density profiles. The presence of obstacles produces a shift in the phase boundaries obtained but the same three phases as obtained for the standard TASEP. Contrary to the model introduced in ref. \\cite{waclaw2019totally}, the idea to introduce the modification at the entrance shows that the limiting case $p_d \\rightarrow 1$ converges to the standard TASEP, where $p_d$ refers to the affected hopping rate due to presence of obstacle. The mean-field solutions are validated using extensive Monte Carlo simulations.","sentences":["We propose a modification to the study of site-wise dynamically disordered totally asymmetric simple exclusion process (TASEP).","Motivated by the process of gene transcription, a study in ref.","[39] introduced an extension of TASEP, where the defects (or obstacles) bind/un-bind dynamically to the sites of the lattice and the hopping of the particles on lattice faces a hindrance if the arrival site is occupied by an obstacle.","In addition, the particle is only allowed to enter the lattice provided the first site is defect-free.","In our study, we propose that the particle movement at the entry of the lattice must face an equal hindrance that is provided by the obstacles to the rest of the particles on the lattice.","For open boundaries, the continuum mean-field equations are derived and solved numerically to obtain steady-state phase diagrams and density profiles.","The presence of obstacles produces a shift in the phase boundaries obtained but the same three phases as obtained for the standard TASEP.","Contrary to the model introduced in ref.","\\cite{waclaw2019totally}, the idea to introduce the modification at the entrance shows that the limiting case $p_d \\rightarrow 1$ converges to the standard TASEP, where $p_d$ refers to the affected hopping rate due to presence of obstacle.","The mean-field solutions are validated using extensive Monte Carlo simulations."],"url":"http://arxiv.org/abs/2405.04956v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 09:55:40","title":"Minimal time of the pointwise controllability for degenerate singular operators and related numerical results via B-splines","abstract":"The goal of this paper is to analyze the pointwise controllability properties of a one-dimensional degenerate/singular equation. We prove the conditions that characterize approximate and null controllability. Besides, a numerical simulation based on B-splines will be provided, in which the state $u$ and the control function $h$ are represented in terms of B-spline basis functions. The numerical results obtained match the theoretical ones.","sentences":["The goal of this paper is to analyze the pointwise controllability properties of a one-dimensional degenerate/singular equation.","We prove the conditions that characterize approximate and null controllability.","Besides, a numerical simulation based on B-splines will be provided, in which the state $u$ and the control function $h$ are represented in terms of B-spline basis functions.","The numerical results obtained match the theoretical ones."],"url":"http://arxiv.org/abs/2405.04930v1","category":"math.OC"}
{"created":"2024-05-08 08:32:04","title":"Starshaped compact hypersurfaces in warped product manifolds II: a class of Hessian type equations","abstract":"We prove several results concerning one particular class of Hessian type equations, which has attracted much attention in recent years.","sentences":["We prove several results concerning one particular class of Hessian type equations, which has attracted much attention in recent years."],"url":"http://arxiv.org/abs/2405.04882v1","category":"math.DG"}
{"created":"2024-05-08 07:39:18","title":"Information Geometric Framework For Point Cloud Data","abstract":"In this paper, we introduce a novel method for comparing 3D point clouds, a critical task in various machine learning applications. By interpreting point clouds as samples from underlying probability density functions, the statistical manifold structure is given to the space of point clouds. This manifold structure will help us to use the information geometric tools to analyze the point clouds. Our method uses the Gaussian Mixture Model (GMM) to find the probability density functions and the Modified Symmetric KL divergence to measure how similar the corresponding probability density functions are. This method of comparing the point clouds takes care of the geometry of the objects represented by the point clouds. To demonstrate the effectiveness of our approach, we take up five distinct case studies:(i) comparison of basic geometric shapes, (ii) comparison of 3D human body shapes within the MP FAUST dataset, (iii) comparison of animal shapes, (iv) comparison of human and animal datasets and (v) comparison of audio signals.","sentences":["In this paper, we introduce a novel method for comparing 3D point clouds, a critical task in various machine learning applications.","By interpreting point clouds as samples from underlying probability density functions, the statistical manifold structure is given to the space of point clouds.","This manifold structure will help us to use the information geometric tools to analyze the point clouds.","Our method uses the Gaussian Mixture Model (GMM) to find the probability density functions and the Modified Symmetric KL divergence to measure how similar the corresponding probability density functions are.","This method of comparing the point clouds takes care of the geometry of the objects represented by the point clouds.","To demonstrate the effectiveness of our approach, we take up five distinct case studies:(i) comparison of basic geometric shapes, (ii) comparison of 3D human body shapes within the MP FAUST dataset, (iii) comparison of animal shapes, (iv) comparison of human and animal datasets and (v) comparison of audio signals."],"url":"http://arxiv.org/abs/2405.04864v1","category":"math.DG"}
{"created":"2024-05-08 07:37:40","title":"Markov numbers and rational $\\mathbb{C}^*$-surfaces","abstract":"The Markov triples, that means the positive integer solutions of the equation $x^2+y^2+z^2=3xyz$, form the vertex set of the Markov tree. Each Markov triple defines a weighted projective plane, giving a geometric interpretation of the vertex. We exhibit a class of rational, projective $\\mathbb{C}^*$-surfaces representing the edges of the Markov tree in the sense that they admit coverings onto the adjacent weighted projective planes. As an application, we obtain an explicit description of all normal degenerations of the projective plane admitting a $\\mathbb{C}^*$-action.","sentences":["The Markov triples, that means the positive integer solutions of the equation $x^2+y^2+z^2=3xyz$, form the vertex set of the Markov tree.","Each Markov triple defines a weighted projective plane, giving a geometric interpretation of the vertex.","We exhibit a class of rational, projective $\\mathbb{C}^*$-surfaces representing the edges of the Markov tree in the sense that they admit coverings onto the adjacent weighted projective planes.","As an application, we obtain an explicit description of all normal degenerations of the projective plane admitting a $\\mathbb{C}^*$-action."],"url":"http://arxiv.org/abs/2405.04862v1","category":"math.AG"}
{"created":"2024-05-08 07:24:01","title":"Coronal modulation in the ultra-fast rotator LO Peg","abstract":"We present coronal imaging of the ultra-fast rotator, LO Peg, using the X-ray observations from XMM-Newton. The X-ray light curves show one strong flare at the end of observation, as reported in an earlier study. On removal of flaring events, the quiescent state light curve shows rotational modulations, which are modelled using a maximum likelihood model. The results obtained from modelling show the corona of LO Peg is not uniform. Active regions are concentrated around two longitudes, where one active region appears to be compact. The large coronal area that covers almost 60 degrees along longitude from the poles to the equator does not consist of active regions.","sentences":["We present coronal imaging of the ultra-fast rotator, LO Peg, using the X-ray observations from XMM-Newton.","The X-ray light curves show one strong flare at the end of observation, as reported in an earlier study.","On removal of flaring events, the quiescent state light curve shows rotational modulations, which are modelled using a maximum likelihood model.","The results obtained from modelling show the corona of LO Peg is not uniform.","Active regions are concentrated around two longitudes, where one active region appears to be compact.","The large coronal area that covers almost 60 degrees along longitude from the poles to the equator does not consist of active regions."],"url":"http://arxiv.org/abs/2405.04857v1","category":"astro-ph.SR"}
{"created":"2024-05-08 07:12:47","title":"Effect of dark matter interaction on hybrid star in the light of the recent astrophysical observations","abstract":"We have explored the effect of dark matter interaction on hybrid star (HS) in the light of recent astrophysical observational constraints. The presence of dark matter is assumed to be there in both the hadron as well as the quark sector. The dark matter particle interacts with both hadron and quark matter through the exchange of a scalar as well as a vector meson. The equation of state (EOS) of the hadron part is computed using the NL3 version of the relativistic mean field(RMF) model, whereas the quark part is taken care of using the well-known MIT Bag model with the vector interaction. We investigate the effect of the dark matter density and the mass of the dark matter particle on various observables like mass, radius, tidal deformability of the dark matter admixed hybrid star(DMAHS). In this study, we have noted an intriguing aspect that is the speed of sound in the DMAHS is insensitive to both the mass as well as the density of dark matter. We also observe a striking similarity in the variation of transition mass and its corresponding radius, as well as the maximum mass of neutron stars, with dark matter density and mass. We employ observational constraints from neutron stars to narrow down the allowed range of the parameters of dark matter.","sentences":["We have explored the effect of dark matter interaction on hybrid star (HS) in the light of recent astrophysical observational constraints.","The presence of dark matter is assumed to be there in both the hadron as well as the quark sector.","The dark matter particle interacts with both hadron and quark matter through the exchange of a scalar as well as a vector meson.","The equation of state (EOS) of the hadron part is computed using the NL3 version of the relativistic mean field(RMF) model, whereas the quark part is taken care of using the well-known MIT Bag model with the vector interaction.","We investigate the effect of the dark matter density and the mass of the dark matter particle on various observables like mass, radius, tidal deformability of the dark matter admixed hybrid star(DMAHS).","In this study, we have noted an intriguing aspect that is the speed of sound in the DMAHS is insensitive to both the mass as well as the density of dark matter.","We also observe a striking similarity in the variation of transition mass and its corresponding radius, as well as the maximum mass of neutron stars, with dark matter density and mass.","We employ observational constraints from neutron stars to narrow down the allowed range of the parameters of dark matter."],"url":"http://arxiv.org/abs/2405.04856v1","category":"nucl-th"}
{"created":"2024-05-08 06:55:03","title":"Uniform degeneration of hyperbolic surfaces with boundary along harmonic map rays","abstract":"Unique harmonic maps between surfaces give a parametrization of the Teichm\\\"{u}ller space by holomorphic quadratic differentials on a Riemann surface. In this paper, we investigate the degeneration of hyperbolic surfaces corresponding to a ray of meromorphic quadratic differentials on a punctured Riemann surface in this parametrization, where the meromorphic quadratic differentials have a pole of order $\\geq 2$ at each puncture. We show that the rescaled distance functions of the universal covers of hyperbolic surfaces uniformly converge, on a certain non-compact region containing a fundamental domain, to the intersection number with the vertical measured foliation given by the meromorphic quadratic differential determining the direction of the ray. This implies the family of hyperbolic surfaces converges to the dual $\\mathbb{R}$-tree to the vertical measured foliation in the sense of Gromov-Hausdorff. As an application, we describe the limit in the function space on the set of isotopy classes of properly embedded arcs and simple closed curves on the surface.","sentences":["Unique harmonic maps between surfaces give a parametrization of the Teichm\\\"{u}ller space by holomorphic quadratic differentials on a Riemann surface.","In this paper, we investigate the degeneration of hyperbolic surfaces corresponding to a ray of meromorphic quadratic differentials on a punctured Riemann surface in this parametrization, where the meromorphic quadratic differentials have a pole of order $\\geq 2$ at each puncture.","We show that the rescaled distance functions of the universal covers of hyperbolic surfaces uniformly converge, on a certain non-compact region containing a fundamental domain, to the intersection number with the vertical measured foliation given by the meromorphic quadratic differential determining the direction of the ray.","This implies the family of hyperbolic surfaces converges to the dual $\\mathbb{R}$-tree to the vertical measured foliation in the sense of Gromov-Hausdorff.","As an application, we describe the limit in the function space on the set of isotopy classes of properly embedded arcs and simple closed curves on the surface."],"url":"http://arxiv.org/abs/2405.04851v1","category":"math.GT"}
{"created":"2024-05-08 06:07:08","title":"Hierarchical Characterization of Thermoelectric Performance in Copper-Based Chalcogenide CsCu$_3$S$_2$: Unveiling the role of Anharmonic Lattice Dynamics","abstract":"Fundamental understanding of anharmonic lattice dynamics and heat conductance physics in crystalline compounds is critical for the development of thermoelectric energy conversion devices. Herein, we thoroughly investigate the microscopic mechanisms of thermal transport in CsCu$_3$S$_2$ by coupling the self-consistent phonon (SCP) theory with the linearized Wigner transport equation (LWTE). We explicitly consider both phonon energy shifts and broadening arising from both cubic and quartic anharmonicities, as well as diagonal/non-diagonal terms of heat flux operators in thermal conductivity. Our findings show that the strong anharmonicity of CsCu$_3$S$_2$ primarily arises from the presence of $p$-$d$ anti-bonding hybridization between Cu and S atoms, coupled with the random oscillations of Cs atoms. Notably, the competition between phonon hardening described by the loop diagram and softening induced by the bubble diagram significantly influences particle-like propagation, predominantly reflected in group velocity and energy-conservation rule. Additionally, the electrical transport properties are determined by employing the precise momentum relaxation-time approximation (MRTA). At high temperatures, the thermoelectric performance of $p$-type CsCu$_3$S$_2$ reaches its optimum theoretical value of 0.94 along the in-plane direction based on advanced phonon renormalization theory. In striking contrast, the harmonic approximation theory significantly overestimates the thermoelectric efficiency at the same temperatures, rendering it an impractical expectation. Conversely, the first-order renormalization approach leads to a serious underestimation of the thermoelectric properties due to the over-correction of phonon energy. Our study not only reveals the pivotal role of anharmonic lattice dynamics in accurately assessing thermoelectric properties but also underscores the potential thermoelectric applications for novel copper-based chalcogenides.","sentences":["Fundamental understanding of anharmonic lattice dynamics and heat conductance physics in crystalline compounds is critical for the development of thermoelectric energy conversion devices.","Herein, we thoroughly investigate the microscopic mechanisms of thermal transport in CsCu$_3$S$_2$ by coupling the self-consistent phonon (SCP) theory with the linearized Wigner transport equation (LWTE).","We explicitly consider both phonon energy shifts and broadening arising from both cubic and quartic anharmonicities, as well as diagonal/non-diagonal terms of heat flux operators in thermal conductivity.","Our findings show that the strong anharmonicity of CsCu$_3$S$_2$ primarily arises from the presence of $p$-$d$ anti-bonding hybridization between Cu and S atoms, coupled with the random oscillations of Cs atoms.","Notably, the competition between phonon hardening described by the loop diagram and softening induced by the bubble diagram significantly influences particle-like propagation, predominantly reflected in group velocity and energy-conservation rule.","Additionally, the electrical transport properties are determined by employing the precise momentum relaxation-time approximation (MRTA).","At high temperatures, the thermoelectric performance of $p$-type CsCu$_3$S$_2$ reaches its optimum theoretical value of 0.94 along the in-plane direction based on advanced phonon renormalization theory.","In striking contrast, the harmonic approximation theory significantly overestimates the thermoelectric efficiency at the same temperatures, rendering it an impractical expectation.","Conversely, the first-order renormalization approach leads to a serious underestimation of the thermoelectric properties due to the over-correction of phonon energy.","Our study not only reveals the pivotal role of anharmonic lattice dynamics in accurately assessing thermoelectric properties but also underscores the potential thermoelectric applications for novel copper-based chalcogenides."],"url":"http://arxiv.org/abs/2405.04832v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 05:41:04","title":"Extrinsic Bonnet-Myers Theorem and almost rigidity","abstract":"We establish the extrinsic Bonnet-Myers Theorem for compact Riemannian manifolds with positive Ricci curvature. And we show the almost rigidity for compact hypersurfaces, which have positive sectional curvature and almost maximal extrinsic diameter in Euclidean space.","sentences":["We establish the extrinsic Bonnet-Myers Theorem for compact Riemannian manifolds with positive Ricci curvature.","And we show the almost rigidity for compact hypersurfaces, which have positive sectional curvature and almost maximal extrinsic diameter in Euclidean space."],"url":"http://arxiv.org/abs/2405.04822v1","category":"math.DG"}
{"created":"2024-05-08 04:21:03","title":"DeepDamageNet: A two-step deep-learning model for multi-disaster building damage segmentation and classification using satellite imagery","abstract":"Satellite imagery has played an increasingly important role in post-disaster building damage assessment. Unfortunately, current methods still rely on manual visual interpretation, which is often time-consuming and can cause very low accuracy. To address the limitations of manual interpretation, there has been a significant increase in efforts to automate the process. We present a solution that performs the two most important tasks in building damage assessment, segmentation and classification, through deep-learning models. We show our results submitted as part of the xView2 Challenge, a competition to design better models for identifying buildings and their damage level after exposure to multiple kinds of natural disasters. Our best model couples a building identification semantic segmentation convolutional neural network (CNN) to a building damage classification CNN, with a combined F1 score of 0.66, surpassing the xView2 challenge baseline F1 score of 0.28. We find that though our model was able to identify buildings with relatively high accuracy, building damage classification across various disaster types is a difficult task due to the visual similarity between different damage levels and different damage distribution between disaster types, highlighting the fact that it may be important to have a probabilistic prior estimate regarding disaster damage in order to obtain accurate predictions.","sentences":["Satellite imagery has played an increasingly important role in post-disaster building damage assessment.","Unfortunately, current methods still rely on manual visual interpretation, which is often time-consuming and can cause very low accuracy.","To address the limitations of manual interpretation, there has been a significant increase in efforts to automate the process.","We present a solution that performs the two most important tasks in building damage assessment, segmentation and classification, through deep-learning models.","We show our results submitted as part of the xView2 Challenge, a competition to design better models for identifying buildings and their damage level after exposure to multiple kinds of natural disasters.","Our best model couples a building identification semantic segmentation convolutional neural network (CNN) to a building damage classification CNN, with a combined F1 score of 0.66, surpassing the xView2 challenge baseline F1 score of 0.28.","We find that though our model was able to identify buildings with relatively high accuracy, building damage classification across various disaster types is a difficult task due to the visual similarity between different damage levels and different damage distribution between disaster types, highlighting the fact that it may be important to have a probabilistic prior estimate regarding disaster damage in order to obtain accurate predictions."],"url":"http://arxiv.org/abs/2405.04800v1","category":"cs.CV"}
{"created":"2024-05-08 03:58:28","title":"Orders for which there exist exactly six or seven groups","abstract":"Much progress has been made on the problem of calculating $g(n)$ for various classes of integers $n$, where $g$ is the group-counting function. We approach the inverse problem of solving the equations $g(n) = 6$ and $g(n) = 7$ in $n$. The determination of $n$ for which $g(n) = k$ has been carried out by G. A. Miller for $1 \\le k \\le 5$.","sentences":["Much progress has been made on the problem of calculating $g(n)$ for various classes of integers $n$, where $g$ is the group-counting function.","We approach the inverse problem of solving the equations $g(n) = 6$ and $g(n)","= 7$ in $n$. The determination of $n$ for which $g(n) = k$ has been carried out by G. A. Miller for $1 \\le k \\le 5$."],"url":"http://arxiv.org/abs/2405.04794v1","category":"math.GR"}
{"created":"2024-05-07 23:51:23","title":"Convergence Rate of the Hypersonic Similarity for Two-Dimensional Steady Potential Flows with Large Data","abstract":"We establish the optimal convergence rate of the hypersonic similarity for two-dimensional steady potential flows with {\\it large data} past over a straight wedge in the $BV\\cap L^1$ framework, provided that the total variation of the large data multiplied by $\\gamma-1+\\frac{a_{\\infty}^2}{M_\\infty^2}$ is uniformly bounded with respect to the adiabatic exponent $\\gamma>1$, the Mach number $M_\\infty$ of the incoming steady flow, and the hypersonic similarity parameter $a_\\infty$. Our main approach in this paper is first to establish the Standard Riemann Semigroup of the initial-boundary value problem for the isothermal hypersonic small disturbance equations with large data and then to compare the Riemann solutions between two systems with boundary locally case by case. Based on them, we derive the global $L^1$--estimate between the two solutions by employing the Standard Riemann Semigroup and the local $L^1$--estimates. We further construct an example to show that the convergence rate is optimal.","sentences":["We establish the optimal convergence rate of the hypersonic similarity for two-dimensional steady potential flows with {\\it large data} past over a straight wedge in the $BV\\cap L^1$ framework, provided that the total variation of the large data multiplied by $\\gamma-1+\\frac{a_{\\infty}^2}{M_\\infty^2}$ is uniformly bounded with respect to the adiabatic exponent $\\gamma>1$, the Mach number $M_\\infty$ of the incoming steady flow, and the hypersonic similarity parameter $a_\\infty$. Our main approach in this paper is first to establish the Standard Riemann Semigroup of the initial-boundary value problem for the isothermal hypersonic small disturbance equations with large data and then to compare the Riemann solutions between two systems with boundary locally case by case.","Based on them, we derive the global $L^1$--estimate between the two solutions by employing the Standard Riemann Semigroup and the local $L^1$--estimates.","We further construct an example to show that the convergence rate is optimal."],"url":"http://arxiv.org/abs/2405.04720v1","category":"math.AP"}
{"created":"2024-05-07 23:32:32","title":"Enhancing Knowledge Retrieval with Topic Modeling for Knowledge-Grounded Dialogue","abstract":"Knowledge retrieval is one of the major challenges in building a knowledge-grounded dialogue system. A common method is to use a neural retriever with a distributed approximate nearest-neighbor database to quickly find the relevant knowledge sentences. In this work, we propose an approach that utilizes topic modeling on the knowledge base to further improve retrieval accuracy and as a result, improve response generation. Additionally, we experiment with a large language model, ChatGPT, to take advantage of the improved retrieval performance to further improve the generation results. Experimental results on two datasets show that our approach can increase retrieval and generation performance. The results also indicate that ChatGPT is a better response generator for knowledge-grounded dialogue when relevant knowledge is provided.","sentences":["Knowledge retrieval is one of the major challenges in building a knowledge-grounded dialogue system.","A common method is to use a neural retriever with a distributed approximate nearest-neighbor database to quickly find the relevant knowledge sentences.","In this work, we propose an approach that utilizes topic modeling on the knowledge base to further improve retrieval accuracy and as a result, improve response generation.","Additionally, we experiment with a large language model, ChatGPT, to take advantage of the improved retrieval performance to further improve the generation results.","Experimental results on two datasets show that our approach can increase retrieval and generation performance.","The results also indicate that ChatGPT is a better response generator for knowledge-grounded dialogue when relevant knowledge is provided."],"url":"http://arxiv.org/abs/2405.04713v1","category":"cs.IR"}
{"created":"2024-05-07 23:24:38","title":"Tube Formulae for Generalized von Koch Fractals through Scaling Functional Equations","abstract":"In this work, we provide a treatment of scaling functional equations in a general setting involving fractals arising from sufficiently nice self-similar systems in order to analyze the tube functions, tube zeta functions, and complex dimensions of relative fractal drums. Namely, we express the volume of a tubular neighborhood in terms of scaled copies of itself and a remainder term and then solve this expression by means of the tube zeta functions.   We then apply our methods to analyze these generalized von Koch fractals, which are a class of fractals that allow for different regular polygons and scaling ratios to be used in the construction of the standard von Koch curve and snowflake. In particular, we describe the volume of an inner tubular neighborhoods and the possible complex dimensions of such fractal snowflakes.","sentences":["In this work, we provide a treatment of scaling functional equations in a general setting involving fractals arising from sufficiently nice self-similar systems in order to analyze the tube functions, tube zeta functions, and complex dimensions of relative fractal drums.","Namely, we express the volume of a tubular neighborhood in terms of scaled copies of itself and a remainder term and then solve this expression by means of the tube zeta functions.   ","We then apply our methods to analyze these generalized von Koch fractals, which are a class of fractals that allow for different regular polygons and scaling ratios to be used in the construction of the standard von Koch curve and snowflake.","In particular, we describe the volume of an inner tubular neighborhoods and the possible complex dimensions of such fractal snowflakes."],"url":"http://arxiv.org/abs/2405.04712v1","category":"math-ph"}
{"created":"2024-05-07 22:20:41","title":"The Existential Theory of the Reals with Summation Operators","abstract":"To characterize the computational complexity of satisfiability problems for probabilistic and causal reasoning within the Pearl's Causal Hierarchy, arXiv:2305.09508 [cs.AI] introduce a new natural class, named succ-$\\exists$R. This class can be viewed as a succinct variant of the well-studied class $\\exists$R based on the Existential Theory of the Reals (ETR). Analogously to $\\exists$R, succ-$\\exists$R is an intermediate class between NEXP and EXPSPACE, the exponential versions of NP and PSPACE. The main contributions of this work are threefold. Firstly, we characterize the class succ-$\\exists$R in terms of nondeterministic real RAM machines and develop structural complexity theoretic results for real RAMs, including translation and hierarchy theorems. Notably, we demonstrate the separation of $\\exists$R and succ-$\\exists$R. Secondly, we examine the complexity of model checking and satisfiability of fragments of existential second-order logic and probabilistic independence logic. We show succ-$\\exists$R- completeness of several of these problems, for which the best-known complexity lower and upper bounds were previously NEXP-hardness and EXPSPACE, respectively. Thirdly, while succ-$\\exists$R is characterized in terms of ordinary (non-succinct) ETR instances enriched by exponential sums and a mechanism to index exponentially many variables, in this paper, we prove that when only exponential sums are added, the corresponding class $\\exists$R^{\\Sigma} is contained in PSPACE. We conjecture that this inclusion is strict, as this class is equivalent to adding a VNP-oracle to a polynomial time nondeterministic real RAM. Conversely, the addition of exponential products to ETR, yields PSPACE. Additionally, we study the satisfiability problem for probabilistic reasoning, with the additional requirement of a small model and prove that this problem is complete for $\\exists$R^{\\Sigma}.","sentences":["To characterize the computational complexity of satisfiability problems for probabilistic and causal reasoning within the Pearl's Causal Hierarchy, arXiv:2305.09508","[cs.AI] introduce a new natural class, named succ-$\\exists$R. This class can be viewed as a succinct variant of the well-studied class $\\exists$R based on the Existential Theory of the Reals (ETR).","Analogously to $\\exists$R, succ-$\\exists$R is an intermediate class between NEXP and EXPSPACE, the exponential versions of NP and PSPACE.","The main contributions of this work are threefold.","Firstly, we characterize the class succ-$\\exists$R in terms of nondeterministic real RAM machines and develop structural complexity theoretic results for real RAMs, including translation and hierarchy theorems.","Notably, we demonstrate the separation of $\\exists$R and succ-$\\exists$R.","Secondly, we examine the complexity of model checking and satisfiability of fragments of existential second-order logic and probabilistic independence logic.","We show succ-$\\exists$R- completeness of several of these problems, for which the best-known complexity lower and upper bounds were previously NEXP-hardness and EXPSPACE, respectively.","Thirdly, while succ-$\\exists$R is characterized in terms of ordinary (non-succinct) ETR instances enriched by exponential sums and a mechanism to index exponentially many variables, in this paper, we prove that when only exponential sums are added, the corresponding class $\\exists$R^{\\Sigma} is contained in PSPACE.","We conjecture that this inclusion is strict, as this class is equivalent to adding a VNP-oracle to a polynomial time nondeterministic real RAM.","Conversely, the addition of exponential products to ETR, yields PSPACE.","Additionally, we study the satisfiability problem for probabilistic reasoning, with the additional requirement of a small model and prove that this problem is complete for $\\exists$R^{\\Sigma}."],"url":"http://arxiv.org/abs/2405.04697v1","category":"cs.CC"}
{"created":"2024-05-07 21:49:48","title":"Onset of scaling violation in pion and kaon elastic electromagnetic form factors","abstract":"Using a symmetry-preserving truncation of the quantum field equations describing hadron properties, parameter-free predictions are delivered for pion and kaon elastic electromagnetic form factors, $F_{P=\\pi,K}$, thereby unifying them with kindred results for nucleon elastic electromagnetic form factors. Regarding positive-charge states, the analysis stresses that the presence of scaling violations in QCD entails that $Q^2 F_P(Q^2)$ should exhibit a single maximum on $Q^2>0$. Locating such a maximum is both necessary and sufficient to establish the existence of scaling violations. The study predicts that, for charged $\\pi$, $K$ mesons, the $Q^2 F_P(Q^2)$ maximum lies in the neighbourhood $Q^2 \\simeq 5\\,$GeV$^2$. Foreseeable experiments will test these predictions and, providing their $Q^2$ reach meets expectations, potentially also provide details on the momentum dependence of meson form factor scaling violation.","sentences":["Using a symmetry-preserving truncation of the quantum field equations describing hadron properties, parameter-free predictions are delivered for pion and kaon elastic electromagnetic form factors, $F_{P=\\pi,K}$, thereby unifying them with kindred results for nucleon elastic electromagnetic form factors.","Regarding positive-charge states, the analysis stresses that the presence of scaling violations in QCD entails that $Q^2 F_P(Q^2)$ should exhibit a single maximum on $Q^2>0$. Locating such a maximum is both necessary and sufficient to establish the existence of scaling violations.","The study predicts that, for charged $\\pi$, $K$ mesons, the $Q^2 F_P(Q^2)$ maximum lies in the neighbourhood $Q^2 \\simeq 5\\,$GeV$^2$. Foreseeable experiments will test these predictions and, providing their $Q^2$ reach meets expectations, potentially also provide details on the momentum dependence of meson form factor scaling violation."],"url":"http://arxiv.org/abs/2405.04681v1","category":"hep-ph"}
{"created":"2024-05-07 21:13:53","title":"Singularity Structures of Linear Inviscid Damping in a Channel","abstract":"This paper studies singularity structures of the linear inviscid damping of two-dimensional Euler equations in a finite periodic channel. We introduce a recursive definition of singularity structures which characterize the singularities of the spectrum density function from different sources: the free part and the boundary part of the Green function. As an application, we demonstrate that the stream function exhibits smoothness away from the channel's boundary, yet it presents singularities in close proximity to the boundary. The singularities arise due to the interaction of boundary and interior singularities of the spectrum density function. We also show that the behavior of the initial data and background flow have an impact on the regularity of different components of the stream function.","sentences":["This paper studies singularity structures of the linear inviscid damping of two-dimensional Euler equations in a finite periodic channel.","We introduce a recursive definition of singularity structures which characterize the singularities of the spectrum density function from different sources: the free part and the boundary part of the Green function.","As an application, we demonstrate that the stream function exhibits smoothness away from the channel's boundary, yet it presents singularities in close proximity to the boundary.","The singularities arise due to the interaction of boundary and interior singularities of the spectrum density function.","We also show that the behavior of the initial data and background flow have an impact on the regularity of different components of the stream function."],"url":"http://arxiv.org/abs/2405.04673v1","category":"math.AP"}
{"created":"2024-05-07 21:05:50","title":"Interpretable Tensor Fusion","abstract":"Conventional machine learning methods are predominantly designed to predict outcomes based on a single data type. However, practical applications may encompass data of diverse types, such as text, images, and audio. We introduce interpretable tensor fusion (InTense), a multimodal learning method for training neural networks to simultaneously learn multimodal data representations and their interpretable fusion. InTense can separately capture both linear combinations and multiplicative interactions of diverse data types, thereby disentangling higher-order interactions from the individual effects of each modality. InTense provides interpretability out of the box by assigning relevance scores to modalities and their associations. The approach is theoretically grounded and yields meaningful relevance scores on multiple synthetic and real-world datasets. Experiments on six real-world datasets show that InTense outperforms existing state-of-the-art multimodal interpretable approaches in terms of accuracy and interpretability.","sentences":["Conventional machine learning methods are predominantly designed to predict outcomes based on a single data type.","However, practical applications may encompass data of diverse types, such as text, images, and audio.","We introduce interpretable tensor fusion (InTense), a multimodal learning method for training neural networks to simultaneously learn multimodal data representations and their interpretable fusion.","InTense can separately capture both linear combinations and multiplicative interactions of diverse data types, thereby disentangling higher-order interactions from the individual effects of each modality.","InTense provides interpretability out of the box by assigning relevance scores to modalities and their associations.","The approach is theoretically grounded and yields meaningful relevance scores on multiple synthetic and real-world datasets.","Experiments on six real-world datasets show that InTense outperforms existing state-of-the-art multimodal interpretable approaches in terms of accuracy and interpretability."],"url":"http://arxiv.org/abs/2405.04671v1","category":"cs.LG"}
{"created":"2024-05-07 20:44:48","title":"Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar","abstract":"Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle. While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored. Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques. Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors. We introduce Radar Fields - a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy. The proposed method does not rely on volume rendering. Instead, we learn fields in Fourier frequency space, supervised with raw radar data. We validate the effectiveness of the method across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and in harsh weather scenarios, where mm-wavelength sensing is especially favorable.","sentences":["Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle.","While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored.","Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques.","Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors.","We introduce Radar Fields - a neural scene reconstruction method designed for active radar imagers.","Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy.","The proposed method does not rely on volume rendering.","Instead, we learn fields in Fourier frequency space, supervised with raw radar data.","We validate the effectiveness of the method across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and in harsh weather scenarios, where mm-wavelength sensing is especially favorable."],"url":"http://arxiv.org/abs/2405.04662v1","category":"cs.CV"}
{"created":"2024-05-07 19:37:22","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic Segmentation of Diverse Landscapes","abstract":"Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a new tool to monitor territory and support public policies. Processing ALS data at scale requires efficient point classification methods that perform well over highly diverse territories. To evaluate them, researchers need large annotated Lidar datasets, however, current Lidar benchmark datasets have restricted scope and often cover a single urban area. To bridge this data gap, we present the FRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an ultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with high-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is built upon France's nationwide open Lidar data. It achieves spatial and semantic diversity via a sampling scheme that explicitly concentrates rare classes and challenging landscapes from five French regions. It should support the development of 3D deep learning approaches for large-scale land monitoring. We describe the nature of the source data, the sampling workflow, the content of the resulting dataset, and provide an initial evaluation of segmentation performance using a performant 3D neural architecture.","sentences":["Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a new tool to monitor territory and support public policies.","Processing ALS data at scale requires efficient point classification methods that perform well over highly diverse territories.","To evaluate them, researchers need large annotated Lidar datasets, however, current Lidar benchmark datasets have restricted scope and often cover a single urban area.","To bridge this data gap, we present the FRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an ultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with high-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is built upon France's nationwide open Lidar data.","It achieves spatial and semantic diversity via a sampling scheme that explicitly concentrates rare classes and challenging landscapes from five French regions.","It should support the development of 3D deep learning approaches for large-scale land monitoring.","We describe the nature of the source data, the sampling workflow, the content of the resulting dataset, and provide an initial evaluation of segmentation performance using a performant 3D neural architecture."],"url":"http://arxiv.org/abs/2405.04634v1","category":"cs.CV"}
{"created":"2024-05-07 18:49:53","title":"Plasma-Plasma Third Order Phase Transition from type IIB Supergravity","abstract":"We show that the planar, charged black hole in AdS, dual to the strongly coupled Quark-Gluon Plasma thermal state of large $N$, $SU(N)$, $\\mathcal{N}=4$ super Yang-Mills at finite chemical potential undergoes a third-order phase transition in the grand canonical ensemble to a hairy black hole of type IIB supergravity. The hairy phase is another strongly coupled fluid with a conformal equation of state and can be interpreted as another kind of Quark-Gluon plasma. This new Quark-Gluon plasma has less entropy and, therefore, seems to characterize some form of smooth hadronization. The locus of the transition in terms of the \"Baryon\" chemical potential, $\\mu$, and the temperature, $T$, is $\\mu=2\\pi T$.","sentences":["We show that the planar, charged black hole in AdS, dual to the strongly coupled Quark-Gluon Plasma thermal state of large $N$, $SU(N)$, $\\mathcal{N}=4$ super Yang-Mills at finite chemical potential undergoes a third-order phase transition in the grand canonical ensemble to a hairy black hole of type IIB supergravity.","The hairy phase is another strongly coupled fluid with a conformal equation of state and can be interpreted as another kind of Quark-Gluon plasma.","This new Quark-Gluon plasma has less entropy and, therefore, seems to characterize some form of smooth hadronization.","The locus of the transition in terms of the \"Baryon\" chemical potential, $\\mu$, and the temperature, $T$, is $\\mu=2\\pi T$."],"url":"http://arxiv.org/abs/2405.04611v1","category":"hep-th"}
{"created":"2024-05-07 18:29:37","title":"Neural network based approach for solving problems in plane wave duct acoustics","abstract":"Neural networks have emerged as a tool for solving differential equations in many branches of engineering and science. But their progress in frequency domain acoustics is limited by the vanishing gradient problem that occurs at higher frequencies. This paper discusses a formulation that can address this issue. The problem of solving the governing differential equation along with the boundary conditions is posed as an unconstrained optimization problem. The acoustic field is approximated to the output of a neural network which is constructed in such a way that it always satisfies the boundary conditions. The applicability of the formulation is demonstrated on popular problems in plane wave acoustic theory. The predicted solution from the neural network formulation is compared with those obtained from the analytical solution. A good agreement is observed between the two solutions. The method of transfer learning to calculate the particle velocity from the existing acoustic pressure field is demonstrated with and without mean flow effects. The sensitivity of the training process to the choice of the activation function and the number of collocation points is studied.","sentences":["Neural networks have emerged as a tool for solving differential equations in many branches of engineering and science.","But their progress in frequency domain acoustics is limited by the vanishing gradient problem that occurs at higher frequencies.","This paper discusses a formulation that can address this issue.","The problem of solving the governing differential equation along with the boundary conditions is posed as an unconstrained optimization problem.","The acoustic field is approximated to the output of a neural network which is constructed in such a way that it always satisfies the boundary conditions.","The applicability of the formulation is demonstrated on popular problems in plane wave acoustic theory.","The predicted solution from the neural network formulation is compared with those obtained from the analytical solution.","A good agreement is observed between the two solutions.","The method of transfer learning to calculate the particle velocity from the existing acoustic pressure field is demonstrated with and without mean flow effects.","The sensitivity of the training process to the choice of the activation function and the number of collocation points is studied."],"url":"http://arxiv.org/abs/2405.04603v1","category":"cs.CE"}
{"created":"2024-05-07 18:21:09","title":"Complex Scaling Method applied to the study of the Swanson Hamiltonian in the broken PT-symmetry phase","abstract":"In this work, we study the non-PT symmetry phase of the Swanson Hamiltonian in the framework of the Complex Scaling Method. By constructing a bi-orthogonality relation, we apply the formalism of the response function to analyse the time evolution of different initial wave packages. The Wigner Functions and mean value of operators are evaluated as a function of time. We analyse in detail the time evolution in the neighbourhood of Exceptional Points. We derive a continuity equation for the system. We compare the results obtained using the Complex Scaling Method to the ones obtained by working in a Rigged Hilbert Space.","sentences":["In this work, we study the non-PT symmetry phase of the Swanson Hamiltonian in the framework of the Complex Scaling Method.","By constructing a bi-orthogonality relation, we apply the formalism of the response function to analyse the time evolution of different initial wave packages.","The Wigner Functions and mean value of operators are evaluated as a function of time.","We analyse in detail the time evolution in the neighbourhood of Exceptional Points.","We derive a continuity equation for the system.","We compare the results obtained using the Complex Scaling Method to the ones obtained by working in a Rigged Hilbert Space."],"url":"http://arxiv.org/abs/2405.04599v1","category":"quant-ph"}
{"created":"2024-05-07 18:17:50","title":"Cross-Platform Autonomous Control of Minimal Kitaev Chains","abstract":"Contemporary quantum devices are reaching new limits in size and complexity, allowing for the experimental exploration of emergent quantum modes. However, this increased complexity introduces significant challenges in device tuning and control. Here, we demonstrate autonomous tuning of emergent Majorana zero modes in a minimal realization of a Kitaev chain. We achieve this task using cross-platform transfer learning. First, we train a tuning model on a theory model. Next, we retrain it using a Kitaev chain realization in a two-dimensional electron gas. Finally, we apply this model to tune a Kitaev chain realized in quantum dots coupled through a semiconductor-superconductor section in a one-dimensional nanowire. Utilizing a convolutional neural network, we predict the tunneling and Cooper pair splitting rates from differential conductance measurements, employing these predictions to adjust the electrochemical potential to a Majorana sweet spot. The algorithm successfully converges to the immediate vicinity of a sweet spot (within 1.5 mV in 67.6% of attempts and within 4.5 mV in 80.9% of cases), typically finding a sweet spot in 45 minutes or less. This advancement is a stepping stone towards autonomous tuning of emergent modes in interacting systems, and towards foundational tuning machine learning models that can be deployed across a range of experimental platforms.","sentences":["Contemporary quantum devices are reaching new limits in size and complexity, allowing for the experimental exploration of emergent quantum modes.","However, this increased complexity introduces significant challenges in device tuning and control.","Here, we demonstrate autonomous tuning of emergent Majorana zero modes in a minimal realization of a Kitaev chain.","We achieve this task using cross-platform transfer learning.","First, we train a tuning model on a theory model.","Next, we retrain it using a Kitaev chain realization in a two-dimensional electron gas.","Finally, we apply this model to tune a Kitaev chain realized in quantum dots coupled through a semiconductor-superconductor section in a one-dimensional nanowire.","Utilizing a convolutional neural network, we predict the tunneling and Cooper pair splitting rates from differential conductance measurements, employing these predictions to adjust the electrochemical potential to a Majorana sweet spot.","The algorithm successfully converges to the immediate vicinity of a sweet spot (within 1.5 mV in 67.6% of attempts and within 4.5 mV in 80.9% of cases), typically finding a sweet spot in 45 minutes or less.","This advancement is a stepping stone towards autonomous tuning of emergent modes in interacting systems, and towards foundational tuning machine learning models that can be deployed across a range of experimental platforms."],"url":"http://arxiv.org/abs/2405.04596v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 18:15:51","title":"An Advanced Features Extraction Module for Remote Sensing Image Super-Resolution","abstract":"In recent years, convolutional neural networks (CNNs) have achieved remarkable advancement in the field of remote sensing image super-resolution due to the complexity and variability of textures and structures in remote sensing images (RSIs), which often repeat in the same images but differ across others. Current deep learning-based super-resolution models focus less on high-frequency features, which leads to suboptimal performance in capturing contours, textures, and spatial information. State-of-the-art CNN-based methods now focus on the feature extraction of RSIs using attention mechanisms. However, these methods are still incapable of effectively identifying and utilizing key content attention signals in RSIs. To solve this problem, we proposed an advanced feature extraction module called Channel and Spatial Attention Feature Extraction (CSA-FE) for effectively extracting the features by using the channel and spatial attention incorporated with the standard vision transformer (ViT). The proposed method trained over the UCMerced dataset on scales 2, 3, and 4. The experimental results show that our proposed method helps the model focus on the specific channels and spatial locations containing high-frequency information so that the model can focus on relevant features and suppress irrelevant ones, which enhances the quality of super-resolved images. Our model achieved superior performance compared to various existing models.","sentences":["In recent years, convolutional neural networks (CNNs) have achieved remarkable advancement in the field of remote sensing image super-resolution due to the complexity and variability of textures and structures in remote sensing images (RSIs), which often repeat in the same images but differ across others.","Current deep learning-based super-resolution models focus less on high-frequency features, which leads to suboptimal performance in capturing contours, textures, and spatial information.","State-of-the-art CNN-based methods now focus on the feature extraction of RSIs using attention mechanisms.","However, these methods are still incapable of effectively identifying and utilizing key content attention signals in RSIs.","To solve this problem, we proposed an advanced feature extraction module called Channel and Spatial Attention Feature Extraction (CSA-FE) for effectively extracting the features by using the channel and spatial attention incorporated with the standard vision transformer (ViT).","The proposed method trained over the UCMerced dataset on scales 2, 3, and 4.","The experimental results show that our proposed method helps the model focus on the specific channels and spatial locations containing high-frequency information so that the model can focus on relevant features and suppress irrelevant ones, which enhances the quality of super-resolved images.","Our model achieved superior performance compared to various existing models."],"url":"http://arxiv.org/abs/2405.04595v1","category":"eess.IV"}
{"created":"2024-05-07 18:09:47","title":"Language Modeling Using Tensor Trains","abstract":"We propose a novel tensor network language model based on the simplest tensor network (i.e., tensor trains), called `Tensor Train Language Model' (TTLM). TTLM represents sentences in an exponential space constructed by the tensor product of words, but computing the probabilities of sentences in a low-dimensional fashion. We demonstrate that the architectures of Second-order RNNs, Recurrent Arithmetic Circuits (RACs), and Multiplicative Integration RNNs are, essentially, special cases of TTLM. Experimental evaluations on real language modeling tasks show that the proposed variants of TTLM (i.e., TTLM-Large and TTLM-Tiny) outperform the vanilla Recurrent Neural Networks (RNNs) with low-scale of hidden units. (The code is available at https://github.com/shuishen112/tensortrainlm.)","sentences":["We propose a novel tensor network language model based on the simplest tensor network (i.e., tensor trains), called `Tensor Train Language Model' (TTLM).","TTLM represents sentences in an exponential space constructed by the tensor product of words, but computing the probabilities of sentences in a low-dimensional fashion.","We demonstrate that the architectures of Second-order RNNs, Recurrent Arithmetic Circuits (RACs), and Multiplicative Integration RNNs are, essentially, special cases of TTLM.","Experimental evaluations on real language modeling tasks show that the proposed variants of TTLM (i.e., TTLM-Large and TTLM-Tiny) outperform the vanilla Recurrent Neural Networks (RNNs) with low-scale of hidden units.","(The code is available at https://github.com/shuishen112/tensortrainlm.)"],"url":"http://arxiv.org/abs/2405.04590v1","category":"cs.CL"}
{"created":"2024-05-07 17:56:12","title":"Neural network based deep learning analysis of semiconductor quantum dot qubits for automated control","abstract":"Machine learning offers a largely unexplored avenue for improving noisy disordered devices in physics using automated algorithms. Through simulations that include disorder in physical devices, particularly quantum devices, there is potential to learn about disordered landscapes and subsequently tune devices based on those insights. In this work, we introduce a novel methodology that employs machine learning, specifically convolutional neural networks (CNNs), to discern the disorder landscape in the parameters of the disordered extended Hubbard model underlying the semiconductor quantum dot spin qubit architectures. This technique takes advantage of experimentally obtainable charge stability diagrams from neighboring quantum dot pairs, enabling the CNN to accurately identify disorder in each parameter of the extended Hubbard model. Remarkably, our CNN can process site-specific disorder in Hubbard parameters, including variations in hopping constants, on-site potentials (gate voltages), and both intra-site and inter-site Coulomb terms. This advancement facilitates the prediction of spatially dependent disorder across all parameters simultaneously with high accuracy ($R^2>0.994$) and fewer parameter constraints, marking a significant improvement over previous methods that were focused only on analyzing on-site potentials at low coupling. Furthermore, our approach allows for the tuning of five or more quantum dots at a time, effectively addressing the often-overlooked issue of crosstalk. Not only does our method streamline the tuning process, potentially enabling fully automated adjustments, but it also introduces a \"no trust\" verification method to rigorously validate the neural network's predictions. Ultimately, this work aims to lay the groundwork for generalizing our method to tackle a broad spectrum of physical problems.","sentences":["Machine learning offers a largely unexplored avenue for improving noisy disordered devices in physics using automated algorithms.","Through simulations that include disorder in physical devices, particularly quantum devices, there is potential to learn about disordered landscapes and subsequently tune devices based on those insights.","In this work, we introduce a novel methodology that employs machine learning, specifically convolutional neural networks (CNNs), to discern the disorder landscape in the parameters of the disordered extended Hubbard model underlying the semiconductor quantum dot spin qubit architectures.","This technique takes advantage of experimentally obtainable charge stability diagrams from neighboring quantum dot pairs, enabling the CNN to accurately identify disorder in each parameter of the extended Hubbard model.","Remarkably, our CNN can process site-specific disorder in Hubbard parameters, including variations in hopping constants, on-site potentials (gate voltages), and both intra-site and inter-site Coulomb terms.","This advancement facilitates the prediction of spatially dependent disorder across all parameters simultaneously with high accuracy ($R^2>0.994$) and fewer parameter constraints, marking a significant improvement over previous methods that were focused only on analyzing on-site potentials at low coupling.","Furthermore, our approach allows for the tuning of five or more quantum dots at a time, effectively addressing the often-overlooked issue of crosstalk.","Not only does our method streamline the tuning process, potentially enabling fully automated adjustments, but it also introduces a \"no trust\" verification method to rigorously validate the neural network's predictions.","Ultimately, this work aims to lay the groundwork for generalizing our method to tackle a broad spectrum of physical problems."],"url":"http://arxiv.org/abs/2405.04524v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 17:49:12","title":"Disorder free many-body localization transition in two quasiperiodically coupled Heisenberg spin chains","abstract":"Disorder free many-body localization (MBL) can occur in interacting systems that can dynamically generate their own disorder. We address the thermal-MBL phase transition of two isotropic Heisenberg spin chains that are quasi-periodically coupled to each other. The spin chains are incommensurate and are coupled through a short range exchange interaction of the $XXZ$ type that decays exponentially with the distance. Using exact diagonalization, matrix product states and density matrix renormalization group, we calculate the time evolution of the entanglement entropy at long times and extract the inverse participation ratio in the thermodynamic limit. We show that this system has a robust MBL phase. We establish the phase diagram with the onset of MBL as a function of the interchain exchange coupling and of the incommensuration between the spin chains. The Ising limit of the interchain interaction optimizes the stability of the MBL phase over a broad range of incommensurations above a given critical exchange coupling. Incorporation of interchain spin flips significantly enhances entanglement between the spin chains and produces delocalization, favoring a pre-thermal phase whose entanglement entropy grows logarithmically with time.","sentences":["Disorder free many-body localization (MBL) can occur in interacting systems that can dynamically generate their own disorder.","We address the thermal-MBL phase transition of two isotropic Heisenberg spin chains that are quasi-periodically coupled to each other.","The spin chains are incommensurate and are coupled through a short range exchange interaction of the $XXZ$ type that decays exponentially with the distance.","Using exact diagonalization, matrix product states and density matrix renormalization group, we calculate the time evolution of the entanglement entropy at long times and extract the inverse participation ratio in the thermodynamic limit.","We show that this system has a robust MBL phase.","We establish the phase diagram with the onset of MBL as a function of the interchain exchange coupling and of the incommensuration between the spin chains.","The Ising limit of the interchain interaction optimizes the stability of the MBL phase over a broad range of incommensurations above a given critical exchange coupling.","Incorporation of interchain spin flips significantly enhances entanglement between the spin chains and produces delocalization, favoring a pre-thermal phase whose entanglement entropy grows logarithmically with time."],"url":"http://arxiv.org/abs/2405.04516v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 17:47:57","title":"A Transformer with Stack Attention","abstract":"Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.","sentences":["Natural languages are believed to be (mildly) context-sensitive.","Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks.","In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism.","Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model.","We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages."],"url":"http://arxiv.org/abs/2405.04515v1","category":"cs.CL"}
{"created":"2024-05-07 17:47:48","title":"Local Sum Rules for 5D Braneworlds","abstract":"This study investigates the consistency of field localization in 5D braneworld scenarios. We derive tensorial consistency relations, Local Sum Rules, which impose constraints on braneworld models featuring quasi-localized bulk fields. We apply these Local Sum Rules to simple cases involving massless scalar, vector, and spinor fields. Our analysis reveals that, when accounting for the effects of the Einstein equations on the bulk field, the free vector and spinor modes cannot be localized on the brane without the introduction of special mechanisms.","sentences":["This study investigates the consistency of field localization in 5D braneworld scenarios.","We derive tensorial consistency relations, Local Sum Rules, which impose constraints on braneworld models featuring quasi-localized bulk fields.","We apply these Local Sum Rules to simple cases involving massless scalar, vector, and spinor fields.","Our analysis reveals that, when accounting for the effects of the Einstein equations on the bulk field, the free vector and spinor modes cannot be localized on the brane without the introduction of special mechanisms."],"url":"http://arxiv.org/abs/2405.04567v1","category":"hep-ph"}
{"created":"2024-05-07 17:02:44","title":"Geometric Structures for the $G_2'$-Hitchin Component","abstract":"We give an explicit geometric structures interpretation of the $G_2'$-Hitchin component $Hit(S, G_2') \\subset \\chi(\\pi_1S,G_2')$ of a closed oriented surface $S$ of genus $g \\geq 2$. In particular, we prove $Hit(S, G_2')$ is naturally homeomorphic to a moduli space $\\mathscr{M}$ of $(G,X)$-structures for $G = G_2'$ and $X = Ein^{2,3}$ on a fiber bundle $\\mathscr{C}$ over $S$ via the descended holonomy map. Explicitly, $\\mathscr{C}$ is the direct sum of fiber bundles $\\mathscr{C} = UTS \\oplus UTS \\oplus \\underline{\\mathbb{R}_+}$ with fiber $\\mathscr{C}_p = UT_p S \\times UT_p S \\times \\mathbb{R}_+$, where $UT S$ denotes the unit tangent bundle.   The geometric structure associated to a $G_2'$-Hitchin representation $\\rho$ is explicitly constructed from the unique associated $\\rho$-equivariant alternating almost-complex curve $\\hat{\\nu}: \\tilde{S} \\rightarrow \\hat{\\mathbb{S}}^{2,4}$; we critically use recent work of Collier-Toulisse on the moduli space of such curves. Our explicit geometric structures are examined in the $G_2'$-Fuchsian case and shown to be unrelated to the $(G_2', Ein^{2,3})$-structures of Guichard-Wienhard.","sentences":["We give an explicit geometric structures interpretation of the $G_2'$-Hitchin component $Hit(S, G_2')","\\subset \\chi(\\pi_1S,G_2')$ of a closed oriented surface $S$ of genus $g \\geq 2$.","In particular, we prove $Hit(S, G_2')$ is naturally homeomorphic to a moduli space $\\mathscr{M}$ of $(G,X)$-structures for $G = G_2'$ and $X = Ein^{2,3}$ on a fiber bundle $\\mathscr{C}$ over $S$ via the descended holonomy map.","Explicitly, $\\mathscr{C}$ is the direct sum of fiber bundles $\\mathscr{C} = UTS \\oplus UTS \\oplus \\underline{\\mathbb{R}_+}$ with fiber $\\mathscr{C}_p = UT_p S \\times UT_p S \\times \\mathbb{R}_+$, where $UT S$ denotes the unit tangent bundle.   ","The geometric structure associated to a $G_2'$-Hitchin representation $\\rho$ is explicitly constructed from the unique associated $\\rho$-equivariant alternating almost-complex curve $\\hat{\\nu}: \\tilde{S} \\rightarrow \\hat{\\mathbb{S}}^{2,4}$; we critically use recent work of Collier-Toulisse on the moduli space of such curves.","Our explicit geometric structures are examined in the $G_2'$-Fuchsian case and shown to be unrelated to the $(G_2', Ein^{2,3})$-structures of Guichard-Wienhard."],"url":"http://arxiv.org/abs/2405.04492v1","category":"math.DG"}
{"created":"2024-05-07 16:53:29","title":"OptPDE: Discovering Novel Integrable Systems via AI-Human Collaboration","abstract":"Integrable partial differential equation (PDE) systems are of great interest in natural science, but are exceedingly rare and difficult to discover. To solve this, we introduce OptPDE, a first-of-its-kind machine learning approach that Optimizes PDEs' coefficients to maximize their number of conserved quantities, $n_{\\rm CQ}$, and thus discover new integrable systems. We discover four families of integrable PDEs, one of which was previously known, and three of which have at least one conserved quantity but are new to the literature to the best of our knowledge. We investigate more deeply the properties of one of these novel PDE families, $u_t = (u_x+a^2u_{xxx})^3$. Our paper offers a promising schema of AI-human collaboration for integrable system discovery: machine learning generates interpretable hypotheses for possible integrable systems, which human scientists can verify and analyze, to truly close the discovery loop.","sentences":["Integrable partial differential equation (PDE) systems are of great interest in natural science, but are exceedingly rare and difficult to discover.","To solve this, we introduce OptPDE, a first-of-its-kind machine learning approach that Optimizes PDEs' coefficients to maximize their number of conserved quantities, $n_{\\rm CQ}$, and thus discover new integrable systems.","We discover four families of integrable PDEs, one of which was previously known, and three of which have at least one conserved quantity but are new to the literature to the best of our knowledge.","We investigate more deeply the properties of one of these novel PDE families, $u_t = (u_x+a^2u_{xxx})^3$. Our paper offers a promising schema of AI-human collaboration for integrable system discovery: machine learning generates interpretable hypotheses for possible integrable systems, which human scientists can verify and analyze, to truly close the discovery loop."],"url":"http://arxiv.org/abs/2405.04484v1","category":"cs.LG"}
{"created":"2024-05-07 16:47:06","title":"Harnack inequality for parabolic equations in double-divergence form with singular lower order coefficients","abstract":"This paper investigates the Harnack inequality for nonnegative solutions to second-order parabolic equations in double divergence form. We impose conditions where the principal coefficients satisfy the Dini mean oscillation condition in $x$, while the drift and zeroth-order coefficients belong to specific Morrey classes. Our analysis contributes to advancing the theoretical foundations of parabolic equations in double divergence form, including Fokker-Planck-Kolmogorov equations for probability densities.","sentences":["This paper investigates the Harnack inequality for nonnegative solutions to second-order parabolic equations in double divergence form.","We impose conditions where the principal coefficients satisfy the Dini mean oscillation condition in $x$, while the drift and zeroth-order coefficients belong to specific Morrey classes.","Our analysis contributes to advancing the theoretical foundations of parabolic equations in double divergence form, including Fokker-Planck-Kolmogorov equations for probability densities."],"url":"http://arxiv.org/abs/2405.04482v1","category":"math.AP"}
{"created":"2024-05-07 16:44:24","title":"Exploration of Novel Neuromorphic Methodologies for Materials Applications","abstract":"Many of today's most interesting questions involve understanding and interpreting complex relationships within graph-based structures. For instance, in materials science, predicting material properties often relies on analyzing the intricate network of atomic interactions. Graph neural networks (GNNs) have emerged as a popular approach for these tasks; however, they suffer from limitations such as inefficient hardware utilization and over-smoothing. Recent advancements in neuromorphic computing offer promising solutions to these challenges. In this work, we evaluate two such neuromorphic strategies known as reservoir computing and hyperdimensional computing. We compare the performance of both approaches for bandgap classification and regression using a subset of the Materials Project dataset. Our results indicate recent advances in hyperdimensional computing can be applied effectively to better represent molecular graphs.","sentences":["Many of today's most interesting questions involve understanding and interpreting complex relationships within graph-based structures.","For instance, in materials science, predicting material properties often relies on analyzing the intricate network of atomic interactions.","Graph neural networks (GNNs) have emerged as a popular approach for these tasks; however, they suffer from limitations such as inefficient hardware utilization and over-smoothing.","Recent advancements in neuromorphic computing offer promising solutions to these challenges.","In this work, we evaluate two such neuromorphic strategies known as reservoir computing and hyperdimensional computing.","We compare the performance of both approaches for bandgap classification and regression using a subset of the Materials Project dataset.","Our results indicate recent advances in hyperdimensional computing can be applied effectively to better represent molecular graphs."],"url":"http://arxiv.org/abs/2405.04478v1","category":"cs.ET"}
{"created":"2024-05-07 16:39:53","title":"Thermodynamics and geometrothermodynamics of regular black holes","abstract":"We assume the validity of the Bekenstein-Hawking entropy, as given in terms of the horizon area of the Bardeen regular black hole, and consider it as the fundamental thermodynamic equation. We derive and investigate the behavior of the main thermodynamic variables. Using the formalism of geometrothermodynamics, we derive the geometric properties of the corresponding equilibrium space and show that the curvature contains information about the stability properties and phase transition structure of the black hole.","sentences":["We assume the validity of the Bekenstein-Hawking entropy, as given in terms of the horizon area of the Bardeen regular black hole, and consider it as the fundamental thermodynamic equation.","We derive and investigate the behavior of the main thermodynamic variables.","Using the formalism of geometrothermodynamics, we derive the geometric properties of the corresponding equilibrium space and show that the curvature contains information about the stability properties and phase transition structure of the black hole."],"url":"http://arxiv.org/abs/2405.04474v1","category":"gr-qc"}
{"created":"2024-05-07 16:14:54","title":"A natural model for curved inflation","abstract":"Inflationary models with a non-zero background curvature are ill-defined in general relativity because scalar modes cannot be canonically quantized. Therefore, there is no consensus on the primordial power spectrum that should be considered at large scales in a curved Universe. In this letter, we propose a model of curved inflation where canonical quantization is possible for any curvature, and we unambiguously obtain the resulting primordial power spectra. The framework is a recently proposed modification of general relativity in which a non-dynamical topological term is added to the Einstein equation. The main strength of this model is that no additional degree of freedom compared to the standard model of cosmology is needed, giving a natural solution to the problem of constructing curved inflation, and at the same time providing an additional argument for this topological modification of general relativity.","sentences":["Inflationary models with a non-zero background curvature are ill-defined in general relativity because scalar modes cannot be canonically quantized.","Therefore, there is no consensus on the primordial power spectrum that should be considered at large scales in a curved Universe.","In this letter, we propose a model of curved inflation where canonical quantization is possible for any curvature, and we unambiguously obtain the resulting primordial power spectra.","The framework is a recently proposed modification of general relativity in which a non-dynamical topological term is added to the Einstein equation.","The main strength of this model is that no additional degree of freedom compared to the standard model of cosmology is needed, giving a natural solution to the problem of constructing curved inflation, and at the same time providing an additional argument for this topological modification of general relativity."],"url":"http://arxiv.org/abs/2405.04450v1","category":"gr-qc"}
{"created":"2024-05-07 16:14:28","title":"Derivation of kinetic and diffusion equations from a hard-sphere Rayleigh gas using collision trees and semigroups","abstract":"We will revisit the classical questions of understanding the statistics of various deterministic dynamics of $N$ hard spheres of diameter $\\varepsilon$ with random initial data in the Boltzmann-Grad scaling as $\\varepsilon$ tends to zero and $N$ tends to infinity. The convergence of the empiric particle dynamics to the Boltzmann-type dynamics is shown using semigroup methods to describe probability measures on collision trees associated to physical trajectories in the case of a Rayleigh gas. As an application we derive the diffusion equation by a further rescaling.","sentences":["We will revisit the classical questions of understanding the statistics of various deterministic dynamics of $N$ hard spheres of diameter $\\varepsilon$ with random initial data in the Boltzmann-Grad scaling as $\\varepsilon$ tends to zero and $N$ tends to infinity.","The convergence of the empiric particle dynamics to the Boltzmann-type dynamics is shown using semigroup methods to describe probability measures on collision trees associated to physical trajectories in the case of a Rayleigh gas.","As an application we derive the diffusion equation by a further rescaling."],"url":"http://arxiv.org/abs/2405.04449v1","category":"math.AP"}
{"created":"2024-05-07 16:07:44","title":"Immortal solutions of the K\u00e4hler-Ricci flow","abstract":"We survey some recent developments on solutions of the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds which exist for all positive times.","sentences":["We survey some recent developments on solutions of the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds which exist for all positive times."],"url":"http://arxiv.org/abs/2405.04444v1","category":"math.DG"}
{"created":"2024-05-07 15:54:46","title":"Learning local Dirichlet-to-Neumann maps of nonlinear elliptic PDEs with rough coefficients","abstract":"Partial differential equations (PDEs) involving high contrast and oscillating coefficients are common in scientific and industrial applications. Numerical approximation of these PDEs is a challenging task that can be addressed, for example, by multi-scale finite element analysis. For linear problems, multi-scale finite element method (MsFEM) is well established and some viable extensions to non-linear PDEs are known. However, some features of the method seem to be intrinsically based on linearity-based. In particular, traditional MsFEM rely on the reuse of computations. For example, the stiffness matrix can be calculated just once, while being used for several right-hand sides, or as part of a multi-level iterative algorithm. Roughly speaking, the offline phase of the method amounts to pre-assembling the local linear Dirichlet-to-Neumann (DtN) operators. We present some preliminary results concerning the combination of MsFEM with machine learning tools. The extension of MsFEM to nonlinear problems is achieved by means of learning local nonlinear DtN maps. The resulting learning-based multi-scale method is tested on a set of model nonlinear PDEs involving the $p-$Laplacian and degenerate nonlinear diffusion.","sentences":["Partial differential equations (PDEs) involving high contrast and oscillating coefficients are common in scientific and industrial applications.","Numerical approximation of these PDEs is a challenging task that can be addressed, for example, by multi-scale finite element analysis.","For linear problems, multi-scale finite element method (MsFEM) is well established and some viable extensions to non-linear PDEs are known.","However, some features of the method seem to be intrinsically based on linearity-based.","In particular, traditional MsFEM rely on the reuse of computations.","For example, the stiffness matrix can be calculated just once, while being used for several right-hand sides, or as part of a multi-level iterative algorithm.","Roughly speaking, the offline phase of the method amounts to pre-assembling the local linear Dirichlet-to-Neumann (DtN) operators.","We present some preliminary results concerning the combination of MsFEM with machine learning tools.","The extension of MsFEM to nonlinear problems is achieved by means of learning local nonlinear DtN maps.","The resulting learning-based multi-scale method is tested on a set of model nonlinear PDEs involving the $p-$Laplacian and degenerate nonlinear diffusion."],"url":"http://arxiv.org/abs/2405.04433v1","category":"math.NA"}
{"created":"2024-05-07 15:41:20","title":"DistGrid: Scalable Scene Reconstruction with Distributed Multi-resolution Hash Grid","abstract":"Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively.","sentences":["Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction.","However, there exist some challenges when reconstructing large-scale scenes.","MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases.","Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF.","Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes.","However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning.","Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid.","In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs.","The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction.","The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.04416v2","category":"cs.CV"}
{"created":"2024-05-07 15:15:12","title":"Geometric approaches to Lagrangian averaging","abstract":"Lagrangian averaging theories, most notably the Generalised Lagrangian Mean (GLM) theory of Andrews & McIntyre (1978), have been primarily developed in Euclidean space and Cartesian coordinates. We re-interpret these theories using a geometric, coordinate-free formulation. This gives central roles to the flow map, its decomposition into mean and perturbation maps, and the momentum 1-form dual to the velocity vector. In this interpretation, the Lagrangian mean of any tensorial quantity is obtained by averaging its pull back to the mean configuration. Crucially, the mean velocity is not a Lagrangian mean in this sense. It can be defined in a variety of ways, leading to alternative Lagrangian mean formulations that include GLM and Soward & Roberts' (2010) glm. These formulations share key features which the geometric approach uncovers. We derive governing equations both for the mean flow and for wave activities constraining the dynamics of the pertubations. The presentation focusses on the Boussinesq model for inviscid rotating stratified flows and reviews the necessary tools of differential geometry.","sentences":["Lagrangian averaging theories, most notably the Generalised Lagrangian Mean (GLM) theory of Andrews & McIntyre (1978), have been primarily developed in Euclidean space and Cartesian coordinates.","We re-interpret these theories using a geometric, coordinate-free formulation.","This gives central roles to the flow map, its decomposition into mean and perturbation maps, and the momentum 1-form dual to the velocity vector.","In this interpretation, the Lagrangian mean of any tensorial quantity is obtained by averaging its pull back to the mean configuration.","Crucially, the mean velocity is not a Lagrangian mean in this sense.","It can be defined in a variety of ways, leading to alternative Lagrangian mean formulations that include GLM and Soward & Roberts' (2010) glm.","These formulations share key features which the geometric approach uncovers.","We derive governing equations both for the mean flow and for wave activities constraining the dynamics of the pertubations.","The presentation focusses on the Boussinesq model for inviscid rotating stratified flows and reviews the necessary tools of differential geometry."],"url":"http://arxiv.org/abs/2405.04394v1","category":"physics.flu-dyn"}
{"created":"2024-05-07 15:13:18","title":"Parallelized Multi-Agent Bayesian Optimization in Lava","abstract":"In parallel with the continuously increasing parameter space dimensionality, search and optimization algorithms should support distributed parameter evaluations to reduce cumulative runtime. Intel's neuromorphic optimization library, Lava-Optimization, was introduced as an abstract optimization system compatible with neuromorphic systems developed in the broader Lava software framework. In this work, we introduce Lava Multi-Agent Optimization (LMAO) with native support for distributed parameter evaluations communicating with a central Bayesian optimization system. LMAO provides an abstract framework for deploying distributed optimization and search algorithms within the Lava software framework. Moreover, LMAO introduces support for random and grid search along with process connections across multiple levels of mathematical precision. We evaluate the algorithmic performance of LMAO with a traditional non-convex optimization problem, a fixed-precision transductive spiking graph neural network for citation graph classification, and a neuromorphic satellite scheduling problem. Our results highlight LMAO's efficient scaling to multiple processes, reducing cumulative runtime and minimizing the likelihood of converging to local optima.","sentences":["In parallel with the continuously increasing parameter space dimensionality, search and optimization algorithms should support distributed parameter evaluations to reduce cumulative runtime.","Intel's neuromorphic optimization library, Lava-Optimization, was introduced as an abstract optimization system compatible with neuromorphic systems developed in the broader Lava software framework.","In this work, we introduce Lava Multi-Agent Optimization (LMAO) with native support for distributed parameter evaluations communicating with a central Bayesian optimization system.","LMAO provides an abstract framework for deploying distributed optimization and search algorithms within the Lava software framework.","Moreover, LMAO introduces support for random and grid search along with process connections across multiple levels of mathematical precision.","We evaluate the algorithmic performance of LMAO with a traditional non-convex optimization problem, a fixed-precision transductive spiking graph neural network for citation graph classification, and a neuromorphic satellite scheduling problem.","Our results highlight LMAO's efficient scaling to multiple processes, reducing cumulative runtime and minimizing the likelihood of converging to local optima."],"url":"http://arxiv.org/abs/2405.04387v1","category":"cs.DC"}
{"created":"2024-05-07 15:06:53","title":"Geodesic connectivity and rooftop envelopes in the Cegrell classes","abstract":"This study examines geodesics and plurisubharmonic envelopes within the Cegrell classes on bounded hyperconvex domains in $\\mathbb{C}^n$. We establish that solutions possessing comparable singularities to the complex Monge-Amp\\`ere equation are identical, affirmatively addressing a longstanding open question raised by Cegrell. This achievement furnishes the most general form of the Bedford-Taylor comparison principle within the Cegrell classes. Building on this foundational result, we explore plurisubharmonic geodesics, broadening the criteria for geodesic connectivity among plurisubharmonic functions with connectable boundary values. Our investigation also delves into the notion of rooftop envelopes, revealing that the rooftop equality condition and the idempotency conjecture are valid under substantially weaker conditions than previously established, a finding made possible by our proven uniqueness result. The paper concludes by discussing the core open problems within the Cegrell classes related to the complex Monge-Amp\\`ere equation.","sentences":["This study examines geodesics and plurisubharmonic envelopes within the Cegrell classes on bounded hyperconvex domains in $\\mathbb{C}^n$. We establish that solutions possessing comparable singularities to the complex Monge-Amp\\`ere equation are identical, affirmatively addressing a longstanding open question raised by Cegrell.","This achievement furnishes the most general form of the Bedford-Taylor comparison principle within the Cegrell classes.","Building on this foundational result, we explore plurisubharmonic geodesics, broadening the criteria for geodesic connectivity among plurisubharmonic functions with connectable boundary values.","Our investigation also delves into the notion of rooftop envelopes, revealing that the rooftop equality condition and the idempotency conjecture are valid under substantially weaker conditions than previously established, a finding made possible by our proven uniqueness result.","The paper concludes by discussing the core open problems within the Cegrell classes related to the complex Monge-Amp\\`ere equation."],"url":"http://arxiv.org/abs/2405.04384v1","category":"math.CV"}
{"created":"2024-05-07 15:02:58","title":"Preserving Nonlinear Constraints in Variational Flow Filtering Data Assimilation","abstract":"Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model. The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space. Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints. This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution. The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis. To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE). Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics. The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations.","sentences":["Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model.","The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space.","Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints.","This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution.","The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis.","To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE).","Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics.","The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations."],"url":"http://arxiv.org/abs/2405.04380v1","category":"math.OC"}
{"created":"2024-05-07 14:31:26","title":"A transversality theorem for semi-algebraic sets with application to signal recovery from the second moment and cryo-EM","abstract":"Semi-algebraic priors are ubiquitous in signal processing and machine learning. Prevalent examples include a) linear models where the signal lies in a low-dimensional subspace; b) sparse models where the signal can be represented by only a few coefficients under a suitable basis; and c) a large family of neural network generative models. In this paper, we prove a transversality theorem for semi-algebraic sets in orthogonal or unitary representations of groups: with a suitable dimension bound, a generic translate of any semi-algebraic set is transverse to the orbits of the group action. This, in turn, implies that if a signal lies in a low-dimensional semi-algebraic set, then it can be recovered uniquely from measurements that separate orbits.   As an application, we consider the implications of the transversality theorem to the problem of recovering signals that are translated by random group actions from their second moment. As a special case, we discuss cryo-EM: a leading technology to constitute the spatial structure of biological molecules, which serves as our prime motivation. In particular, we derive explicit bounds for recovering a molecular structure from the second moment under a semi-algebraic prior and deduce information-theoretic implications. We also obtain information-theoretic bounds for three additional applications: factoring Gram matrices, multi-reference alignment, and phase retrieval. Finally, we deduce bounds for designing permutation invariant separators in machine learning.","sentences":["Semi-algebraic priors are ubiquitous in signal processing and machine learning.","Prevalent examples include a) linear models where the signal lies in a low-dimensional subspace; b) sparse models where the signal can be represented by only a few coefficients under a suitable basis; and c) a large family of neural network generative models.","In this paper, we prove a transversality theorem for semi-algebraic sets in orthogonal or unitary representations of groups: with a suitable dimension bound, a generic translate of any semi-algebraic set is transverse to the orbits of the group action.","This, in turn, implies that if a signal lies in a low-dimensional semi-algebraic set, then it can be recovered uniquely from measurements that separate orbits.   ","As an application, we consider the implications of the transversality theorem to the problem of recovering signals that are translated by random group actions from their second moment.","As a special case, we discuss cryo-EM: a leading technology to constitute the spatial structure of biological molecules, which serves as our prime motivation.","In particular, we derive explicit bounds for recovering a molecular structure from the second moment under a semi-algebraic prior and deduce information-theoretic implications.","We also obtain information-theoretic bounds for three additional applications: factoring Gram matrices, multi-reference alignment, and phase retrieval.","Finally, we deduce bounds for designing permutation invariant separators in machine learning."],"url":"http://arxiv.org/abs/2405.04354v1","category":"cs.IT"}
{"created":"2024-05-07 14:26:05","title":"Overdetermined elliptic problems in nontrivial exterior domains of the hyperbolic space","abstract":"We construct nontrivial unbounded domains $\\Omega$ in the hyperbolic space $\\mathbb{H}^N$, $N \\in \\{2,3,4\\}$, bifurcating from the complement of a ball, such that the overdetermined elliptic problem \\begin{equation} -\\Delta_{\\mathbb{H}^N} u+u-u^p=0\\,\\, \\text{in}\\,\\,\\Omega, \\,\\, u=0,\\,\\,\\partial_\\nu u=\\text{const}\\,\\,\\text{on}\\,\\,\\partial\\Omega\\nonumber \\end{equation} has a positive bounded solution in $C^{2,\\alpha}\\left(\\Omega\\right) \\cap H^1\\left(\\Omega\\right)$. We also give a condition under which this construction holds for larger dimensions $N$. This is linked to the Berestycki-Caffarelli-Nirenberg conjecture on overdetermined elliptic problems, and, as far as we know, is the first nontrivial example of solution to an overdetermined elliptic problem in the hyperbolic space.","sentences":["We construct nontrivial unbounded domains $\\Omega$ in the hyperbolic space $\\mathbb{H}^N$, $N \\in \\{2,3,4\\}$, bifurcating from the complement of a ball, such that the overdetermined elliptic problem \\begin{equation} -\\Delta_{\\mathbb{H}^N} u+u-u^p=0\\,\\, \\text{in}\\,\\,\\Omega, \\,\\, u=0,\\,\\,\\partial_\\nu u=\\text{const}\\,\\,\\text{on}\\,\\,\\partial\\Omega\\nonumber \\end{equation} has a positive bounded solution in $C^{2,\\alpha}\\left(\\Omega\\right) \\cap H^1\\left(\\Omega\\right)$.","We also give a condition under which this construction holds for larger dimensions $N$. This is linked to the Berestycki-Caffarelli-Nirenberg conjecture on overdetermined elliptic problems, and, as far as we know, is the first nontrivial example of solution to an overdetermined elliptic problem in the hyperbolic space."],"url":"http://arxiv.org/abs/2405.04348v1","category":"math.AP"}
{"created":"2024-05-07 14:23:28","title":"Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint","abstract":"Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space. More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.","sentences":["Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint.","In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux.","For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space.","More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints.","The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux.","Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy."],"url":"http://arxiv.org/abs/2405.04347v1","category":"math.NA"}
{"created":"2024-05-07 13:48:59","title":"Genetic Drift Regularization: on preventing Actor Injection from breaking Evolution Strategies","abstract":"Evolutionary Algorithms (EA) have been successfully used for the optimization of neural networks for policy search, but they still remain sample inefficient and underperforming in some cases compared to gradient-based reinforcement learning (RL). Various methods combine the two approaches, many of them training a RL algorithm on data from EA evaluations and injecting the RL actor into the EA population. However, when using Evolution Strategies (ES) as the EA, the RL actor can drift genetically far from the the ES distribution and injection can cause a collapse of the ES performance. Here, we highlight the phenomenon of genetic drift where the actor genome and the ES population distribution progressively drift apart, leading to injection having a negative impact on the ES. We introduce Genetic Drift Regularization (GDR), a simple regularization method in the actor training loss that prevents the actor genome from drifting away from the ES. We show that GDR can improve ES convergence on problems where RL learns well, but also helps RL training on other tasks, , fixes the injection issues better than previous controlled injection methods.","sentences":["Evolutionary Algorithms (EA) have been successfully used for the optimization of neural networks for policy search, but they still remain sample inefficient and underperforming in some cases compared to gradient-based reinforcement learning (RL).","Various methods combine the two approaches, many of them training a RL algorithm on data from EA evaluations and injecting the RL actor into the EA population.","However, when using Evolution Strategies (ES) as the EA, the RL actor can drift genetically far from the the ES distribution and injection can cause a collapse of the ES performance.","Here, we highlight the phenomenon of genetic drift where the actor genome and the ES population distribution progressively drift apart, leading to injection having a negative impact on the ES.","We introduce Genetic Drift Regularization (GDR), a simple regularization method in the actor training loss that prevents the actor genome from drifting away from the ES.","We show that GDR can improve ES convergence on problems where RL learns well, but also helps RL training on other tasks, , fixes the injection issues better than previous controlled injection methods."],"url":"http://arxiv.org/abs/2405.04322v1","category":"cs.NE"}
{"created":"2024-05-07 13:41:34","title":"Stress solution of static linear elasticity with mixed boundary conditions via adjoint linear operators","abstract":"We revisit stress problems in linear elasticity to provide a perspective from the geometrical and functionalanalytic points of view. For the static stress problem of linear elasticity with mixed boundary conditions we write the associated pair of unbounded adjoint operators. The stress solution is found as an intersection of affine translations of the fundamental subspaces of the adjoint operators. In particular, we treat the equilibrium equation in the operator form, involving spaces of traces on a part of the boundary, known as Lions-Magenes spaces. Our analysis of the pair of adjoint operators for the problem with mixed boundary conditions relies on the properties of the analogous pair of operators for the problem with the displacement boundary conditions, which we also include in the paper.","sentences":["We revisit stress problems in linear elasticity to provide a perspective from the geometrical and functionalanalytic points of view.","For the static stress problem of linear elasticity with mixed boundary conditions we write the associated pair of unbounded adjoint operators.","The stress solution is found as an intersection of affine translations of the fundamental subspaces of the adjoint operators.","In particular, we treat the equilibrium equation in the operator form, involving spaces of traces on a part of the boundary, known as Lions-Magenes spaces.","Our analysis of the pair of adjoint operators for the problem with mixed boundary conditions relies on the properties of the analogous pair of operators for the problem with the displacement boundary conditions, which we also include in the paper."],"url":"http://arxiv.org/abs/2405.04320v1","category":"math.AP"}
{"created":"2024-05-07 13:33:36","title":"Quality with Just Enough Diversity in Evolutionary Policy Search","abstract":"Evolution Strategies (ES) are effective gradient-free optimization methods that can be competitive with gradient-based approaches for policy search. ES only rely on the total episodic scores of solutions in their population, from which they estimate fitness gradients for their update with no access to true gradient information. However this makes them sensitive to deceptive fitness landscapes, and they tend to only explore one way to solve a problem. Quality-Diversity methods such as MAP-Elites introduced additional information with behavior descriptors (BD) to return a population of diverse solutions, which helps exploration but leads to a large part of the evaluation budget not being focused on finding the best performing solution. Here we show that behavior information can also be leveraged to find the best policy by identifying promising search areas which can then be efficiently explored with ES. We introduce the framework of Quality with Just Enough Diversity (JEDi) which learns the relationship between behavior and fitness to focus evaluations on solutions that matter. When trying to reach higher fitness values, JEDi outperforms both QD and ES methods on hard exploration tasks like mazes and on complex control problems with large policies.","sentences":["Evolution Strategies (ES) are effective gradient-free optimization methods that can be competitive with gradient-based approaches for policy search.","ES only rely on the total episodic scores of solutions in their population, from which they estimate fitness gradients for their update with no access to true gradient information.","However this makes them sensitive to deceptive fitness landscapes, and they tend to only explore one way to solve a problem.","Quality-Diversity methods such as MAP-Elites introduced additional information with behavior descriptors (BD) to return a population of diverse solutions, which helps exploration but leads to a large part of the evaluation budget not being focused on finding the best performing solution.","Here we show that behavior information can also be leveraged to find the best policy by identifying promising search areas which can then be efficiently explored with ES.","We introduce the framework of Quality with Just Enough Diversity (JEDi) which learns the relationship between behavior and fitness to focus evaluations on solutions that matter.","When trying to reach higher fitness values, JEDi outperforms both QD and ES methods on hard exploration tasks like mazes and on complex control problems with large policies."],"url":"http://arxiv.org/abs/2405.04308v1","category":"cs.NE"}
{"created":"2024-05-07 13:24:38","title":"On the existence and uniqueness of weak solutions to elliptic equations with a singular drift","abstract":"In this paper we study the Dirichlet problem for a scalar elliptic equation in a bounded Lipschitz domain $\\Omega \\subset \\mathbb R^3$ with a singular drift of the form $b_0= b-\\alpha \\frac {x'}{|x'|^2}$ where $x'=(x_1,x_2,0)$, $\\alpha \\in \\mathbb R$ is a parameter and $b$ is a divergence free vector field having essentially the same regularity as the potential part of the drift. Such drifts naturally arise in the theory of axially symmetric solutions to the Navier-Stokes equations. For $\\alpha <0$ the divergence of such drifts is positive which potentially can ruin the uniqueness of solutions. Nevertheless, for $\\alpha<0$ we prove existence and H\\\"older continuity of a unique weak solution which vanishes on the axis $\\Gamma:=\\{ ~x\\in \\mathbb R^3:~|x'|=0~\\}$.","sentences":["In this paper we study the Dirichlet problem for a scalar elliptic equation in a bounded Lipschitz domain $\\Omega \\subset \\mathbb R^3$ with a singular drift of the form $b_0= b-\\alpha \\frac {x'}{|x'|^2}$ where $x'=(x_1,x_2,0)$, $\\alpha \\in \\mathbb R$ is a parameter and $b$ is a divergence free vector field having essentially the same regularity as the potential part of the drift.","Such drifts naturally arise in the theory of axially symmetric solutions to the Navier-Stokes equations.","For $\\alpha <0$ the divergence of such drifts is positive which potentially can ruin the uniqueness of solutions.","Nevertheless, for $\\alpha<0$ we prove existence and H\\\"older continuity of a unique weak solution which vanishes on the axis $\\Gamma:=\\{ ~x\\in \\mathbb R^3:~|x'|=0~\\}$."],"url":"http://arxiv.org/abs/2405.04302v1","category":"math.AP"}
{"created":"2024-05-07 13:22:03","title":"Classification of solutions to the isotropic horospherical $p$-Minkowski problem in hyperbolic plane","abstract":"In \\cite{LX}, the first author and Xu introduced and studied the horospherical $p$-Minkowski problem in hyperbolic space $\\mathbb{H}^{n+1}$. In particular, they established the uniqueness result for solutions to this problem when the prescribed function is constant and $p\\ge -n$. This paper focuses on the isotropic horospherical $p$-Minkowski problem in hyperbolic plane $\\mathbb{H}^{2}$, which corresponds to the equation \\begin{equation}\\label{0}   \\varphi^{-p}\\left(\\varphi_{\\theta\\theta}-\\frac{\\varphi_{\\theta}^2}{2\\varphi}+\\frac{\\varphi-\\varphi^{-1}}{2}\\right)=\\gamma\\quad\\text{on}\\ \\mathbb{S}^1, \\end{equation} where $\\gamma$ is a positive constant. We provide a classification of solutions to the above equation for $p\\ge -7$, as well as a nonuniqueness result of solutions for $p<-7$. Furthermore, we extend this problem to the isotropic horospherical $q$-weighted $p$-Minkowski problem in hyperbolic plane and derive some uniqueness and nonuniqueness results.","sentences":["In \\cite{LX}, the first author and Xu introduced and studied the horospherical $p$-Minkowski problem in hyperbolic space $\\mathbb{H}^{n+1}$. In particular, they established the uniqueness result for solutions to this problem when the prescribed function is constant and $p\\ge -n$.","This paper focuses on the isotropic horospherical $p$-Minkowski problem in hyperbolic plane $\\mathbb{H}^{2}$, which corresponds to the equation \\begin{equation}\\label{0}   \\varphi^{-p}\\left(\\varphi_{\\theta\\theta}-\\frac{\\varphi_{\\theta}^2}{2\\varphi}+\\frac{\\varphi-\\varphi^{-1}}{2}\\right)=\\gamma\\quad\\text{on}\\ \\mathbb{S}^1, \\end{equation} where $\\gamma$ is a positive constant.","We provide a classification of solutions to the above equation for $p\\ge -7$, as well as a nonuniqueness result of solutions for $p<-7$. Furthermore, we extend this problem to the isotropic horospherical $q$-weighted $p$-Minkowski problem in hyperbolic plane and derive some uniqueness and nonuniqueness results."],"url":"http://arxiv.org/abs/2405.04301v1","category":"math.DG"}
{"created":"2024-05-07 12:46:45","title":"Uncertainty Quantification Metrics for Deep Regression","abstract":"When deploying deep neural networks on robots or other physical systems, the learned model should reliably quantify predictive uncertainty. A reliable uncertainty allows downstream modules to reason about the safety of its actions. In this work, we address metrics for evaluating such an uncertainty. Specifically, we focus on regression tasks, and investigate Area Under Sparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation, and Negative Log-Likelihood (NLL). Using synthetic regression datasets, we look into how those metrics behave under four typical types of uncertainty, their stability regarding the size of the test set, and reveal their strengths and weaknesses. Our results indicate that Calibration Error is the most stable and interpretable metric, but AUSE and NLL also have their respective use cases. We discourage the usage of Spearman's Rank Correlation for evaluating uncertainties and recommend replacing it with AUSE.","sentences":["When deploying deep neural networks on robots or other physical systems, the learned model should reliably quantify predictive uncertainty.","A reliable uncertainty allows downstream modules to reason about the safety of its actions.","In this work, we address metrics for evaluating such an uncertainty.","Specifically, we focus on regression tasks, and investigate Area Under Sparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation, and Negative Log-Likelihood (NLL).","Using synthetic regression datasets, we look into how those metrics behave under four typical types of uncertainty, their stability regarding the size of the test set, and reveal their strengths and weaknesses.","Our results indicate that Calibration Error is the most stable and interpretable metric, but AUSE and NLL also have their respective use cases.","We discourage the usage of Spearman's Rank Correlation for evaluating uncertainties and recommend replacing it with AUSE."],"url":"http://arxiv.org/abs/2405.04278v1","category":"cs.LG"}
{"created":"2024-05-07 12:33:12","title":"Quenches in the Sherrington-Kirkpatrick model","abstract":"The Sherrington-Kirkpatrick (SK) model is a prototype of a complex non-convex energy landscape. Dynamical processes evolving on such landscapes and locally aiming to reach minima are generally poorly understood. Here, we study quenches, i.e. dynamics that locally aim to decrease energy. We analyse the energy at convergence for two distinct algorithmic classes, single-spin flip and synchronous dynamics, focusing on greedy and reluctant strategies. We provide precise numerical analysis of the finite size effects and conclude that, perhaps counter-intuitively, the reluctant algorithm is compatible with converging to the ground state energy density, while the greedy strategy is not. Inspired by the single-spin reluctant and greedy algorithms, we investigate two synchronous time algorithms, the sync-greedy and sync-reluctant algorithms. These synchronous processes can be analysed using dynamical mean field theory (DMFT), and a new backtracking version of DMFT. Notably, this is the first time the backtracking DMFT is applied to study dynamical convergence properties in fully connected disordered models. The analysis suggests that the sync-greedy algorithm can also achieve energies compatible with the ground state, and that it undergoes a dynamical phase transition.","sentences":["The Sherrington-Kirkpatrick (SK) model is a prototype of a complex non-convex energy landscape.","Dynamical processes evolving on such landscapes and locally aiming to reach minima are generally poorly understood.","Here, we study quenches, i.e. dynamics that locally aim to decrease energy.","We analyse the energy at convergence for two distinct algorithmic classes, single-spin flip and synchronous dynamics, focusing on greedy and reluctant strategies.","We provide precise numerical analysis of the finite size effects and conclude that, perhaps counter-intuitively, the reluctant algorithm is compatible with converging to the ground state energy density, while the greedy strategy is not.","Inspired by the single-spin reluctant and greedy algorithms, we investigate two synchronous time algorithms, the sync-greedy and sync-reluctant algorithms.","These synchronous processes can be analysed using dynamical mean field theory (DMFT), and a new backtracking version of DMFT.","Notably, this is the first time the backtracking DMFT is applied to study dynamical convergence properties in fully connected disordered models.","The analysis suggests that the sync-greedy algorithm can also achieve energies compatible with the ground state, and that it undergoes a dynamical phase transition."],"url":"http://arxiv.org/abs/2405.04267v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-07 12:33:06","title":"Self-Stabilizing MIS Computation in the Beeping Model","abstract":"We consider self-stabilizing algorithms to compute a Maximal Independent Set (MIS) in the extremely weak beeping communication model. The model consists of an anonymous network with synchronous rounds. In each round, each vertex can optionally transmit a signal to all its neighbors (beep). After the transmission of a signal, each vertex can only differentiate between no signal received, or at least one signal received. We assume that vertices have some knowledge about the topology of the network.   We revisit the not self-stabilizing algorithm proposed by Jeavons, Scott, and Xu (2013), which computes an MIS in the beeping model. We enhance this algorithm to be self-stabilizing, and explore two different variants, which differ in the knowledge about the topology available to the vertices. In the first variant, every vertex knows an upper bound on the maximum degree $\\Delta$ of the graph. For this case, we prove that the proposed self-stabilizing version maintains the same run-time as the original algorithm, i.e. it stabilizes after $O(\\log n)$ rounds w.h.p. on any $n$-vertex graph. In the second variant, each vertex only knows an upper bound on its own degree. For this case, we prove that the algorithm stabilizes after $O(\\log n\\cdot \\log \\log n)$ rounds on any $n$-vertex graph, w.h.p.","sentences":["We consider self-stabilizing algorithms to compute a Maximal Independent Set (MIS) in the extremely weak beeping communication model.","The model consists of an anonymous network with synchronous rounds.","In each round, each vertex can optionally transmit a signal to all its neighbors (beep).","After the transmission of a signal, each vertex can only differentiate between no signal received, or at least one signal received.","We assume that vertices have some knowledge about the topology of the network.   ","We revisit the not self-stabilizing algorithm proposed by Jeavons, Scott, and Xu (2013), which computes an MIS in the beeping model.","We enhance this algorithm to be self-stabilizing, and explore two different variants, which differ in the knowledge about the topology available to the vertices.","In the first variant, every vertex knows an upper bound on the maximum degree $\\Delta$ of the graph.","For this case, we prove that the proposed self-stabilizing version maintains the same run-time as the original algorithm, i.e. it stabilizes after $O(\\log n)$ rounds w.h.p.","on any $n$-vertex graph.","In the second variant, each vertex only knows an upper bound on its own degree.","For this case, we prove that the algorithm stabilizes after $O(\\log n\\cdot \\log \\log n)$ rounds on any $n$-vertex graph, w.h.p."],"url":"http://arxiv.org/abs/2405.04266v1","category":"cs.DC"}
{"created":"2024-05-07 12:30:37","title":"CMB spectrum in unified EFT of dark energy: scalar-tensor and vector-tensor theories","abstract":"We study the cosmic microwave background (CMB) radiation in the unified description of the effective field theory (EFT) of dark energy that accommodates both scalar-tensor and vector-tensor theories. The boundaries of different classes of theories are universally parameterised by a new EFT parameter $\\alpha_V$ characterising the vectorial nature of dark energy and a set of consistency relations associated with the global/local shift symmetry. After implementing the equations of motion in a Boltzmann code, as a demonstration, we compute the CMB power spectrum based on the $w$CDM background with the EFT parameterisation of perturbations and a concrete Horndeski/generalised Proca theory. We show that the vectorial nature generically prevents modifications of gravity in the CMB spectrum. On the other hand, while the shift symmetry is less significant in the perturbation equations unless the background is close to the $\\Lambda$CDM, it requires that the effective equation of state of dark energy is in the phantom region $w_{\\rm DE}<-1$. The latter is particularly interesting in light of the latest result of the DESI+CMB combination as the observational verification of $w_{\\rm DE}>-1$ can rule out shift-symmetric theories including vector-tensor theories in one shot.","sentences":["We study the cosmic microwave background (CMB) radiation in the unified description of the effective field theory (EFT) of dark energy that accommodates both scalar-tensor and vector-tensor theories.","The boundaries of different classes of theories are universally parameterised by a new EFT parameter $\\alpha_V$ characterising the vectorial nature of dark energy and a set of consistency relations associated with the global/local shift symmetry.","After implementing the equations of motion in a Boltzmann code, as a demonstration, we compute the CMB power spectrum based on the $w$CDM background with the EFT parameterisation of perturbations and a concrete Horndeski/generalised Proca theory.","We show that the vectorial nature generically prevents modifications of gravity in the CMB spectrum.","On the other hand, while the shift symmetry is less significant in the perturbation equations unless the background is close to the $\\Lambda$CDM, it requires that the effective equation of state of dark energy is in the phantom region $w_{\\rm DE}<-1$.","The latter is particularly interesting in light of the latest result of the DESI+CMB combination as the observational verification of $w_{\\rm DE}>-1$ can rule out shift-symmetric theories including vector-tensor theories in one shot."],"url":"http://arxiv.org/abs/2405.04265v1","category":"astro-ph.CO"}
{"created":"2024-05-07 12:17:07","title":"Ruled Ricci surfaces and curves of constant torsion","abstract":"We show that all non-developable ruled surfaces endowed with Ricci metrics in the three-dimensional Euclidean space may be constructed using curves of constant torsion and its binormal. This allows us to give characterizations of the helicoid as the only surface of this kind that admits a parametrization with plane line of striction, and as the only with constant mean curvature.","sentences":["We show that all non-developable ruled surfaces endowed with Ricci metrics in the three-dimensional Euclidean space may be constructed using curves of constant torsion and its binormal.","This allows us to give characterizations of the helicoid as the only surface of this kind that admits a parametrization with plane line of striction, and as the only with constant mean curvature."],"url":"http://arxiv.org/abs/2405.04255v1","category":"math.DG"}
{"created":"2024-05-07 11:59:12","title":"Investigation of sample paths properties of sub-Gaussian type random fields, with application to stochasic heat equations","abstract":"The paper presents bounds for the distributions of suprema for a particular class of sub-Gaussian type random fields defined over spaces with anisotropic metrics. The results are applied to random fields related to stochastic heat equations with fractional noise: bounds for the tail distributions of suprema and estimates for the rate of growth are provided for such fields.","sentences":["The paper presents bounds for the distributions of suprema for a particular class of sub-Gaussian type random fields defined over spaces with anisotropic metrics.","The results are applied to random fields related to stochastic heat equations with fractional noise: bounds for the tail distributions of suprema and estimates for the rate of growth are provided for such fields."],"url":"http://arxiv.org/abs/2405.04242v1","category":"math.PR"}
{"created":"2024-05-07 11:52:14","title":"Probing the polarized photon content of the proton in $ep$ collisions at the EIC","abstract":"We study the single-inclusive production of prompt photons in electron proton collisions, $ep \\to \\gamma X $, for kinematics relevant at the Electron-Ion Collider (EIC). We perform a perturbative calculation of the differential cross section to next-to-leading order in QCD and to lowest order in QED. We consider unpolarized collisions as well as scattering of longitudinally polarized incident electrons and protons. We show that the cross sections are sensitive to the parton distribution functions of photons inside the proton, which we find to generate the dominant contributions in certain kinematical regions at the EIC. We also investigate the effects of photon isolation on the unpolarized and polarized cross sections.","sentences":["We study the single-inclusive production of prompt photons in electron proton collisions, $ep \\to \\gamma X $, for kinematics relevant at the Electron-Ion Collider (EIC).","We perform a perturbative calculation of the differential cross section to next-to-leading order in QCD and to lowest order in QED.","We consider unpolarized collisions as well as scattering of longitudinally polarized incident electrons and protons.","We show that the cross sections are sensitive to the parton distribution functions of photons inside the proton, which we find to generate the dominant contributions in certain kinematical regions at the EIC.","We also investigate the effects of photon isolation on the unpolarized and polarized cross sections."],"url":"http://arxiv.org/abs/2405.04232v1","category":"hep-ph"}
{"created":"2024-05-07 11:46:43","title":"The Penrose Inequality for Metrics with Singular Sets","abstract":"We study the Penrose inequality and its rigidity for metrics with singular sets. Our result could be viewed as a complement of Theorem 1.1 of Lu and Miao (J. Funct. Anal. 281, 2021) and Theorem 1.2 of Shi, Wang and Yu (Math. Z. 291, 2019), in which they assume the singular set is a hypersurface and assume an additional condition on the mean curvature. As a complement, this paper study the case of singular set of dimensional less than n-1, without any additional conditions.","sentences":["We study the Penrose inequality and its rigidity for metrics with singular sets.","Our result could be viewed as a complement of Theorem 1.1 of Lu and Miao (J. Funct.","Anal.","281, 2021) and Theorem 1.2 of Shi, Wang and Yu (Math. Z. 291, 2019), in which they assume the singular set is a hypersurface and assume an additional condition on the mean curvature.","As a complement, this paper study the case of singular set of dimensional less than n-1, without any additional conditions."],"url":"http://arxiv.org/abs/2405.04227v1","category":"math.DG"}
{"created":"2024-05-07 11:40:35","title":"ChemPlasKin: a general-purpose program for unified gas and plasma kinetics simulations","abstract":"This work introduces ChemPlasKin, a freely accessible solver optimized for zero-dimensional (0D) simulations of chemical kinetics of neutral gas in non-equilibrium plasma environments. By integrating the electron Boltzmann equation solver, CppBOLOS, with the open-source combustion library, Cantera, at the source code level, ChemPlasKin computes time-resolved evolution of species concentration and gas temperature in a unified gas-plasma kinetics framework. The model allows high fidelity predictions of both chemical thermal effects and plasma-induced heating, including fast gas heating and slower vibrational-translational relaxation processes. Additionally, a new heat loss model is developed for nanosecond pulsed discharges, specifically within pin-pin electrode configurations. With its versatility, ChemPlasKin is well-suited for a wide range of applications, from plasma-assisted combustion (PAC) to fuel reforming. In this paper, the reliability, accuracy and efficiency of ChemPlasKin are validated through a number of test problems, demonstrating its utility in advancing gas-plasma kinetic studies.","sentences":["This work introduces ChemPlasKin, a freely accessible solver optimized for zero-dimensional (0D) simulations of chemical kinetics of neutral gas in non-equilibrium plasma environments.","By integrating the electron Boltzmann equation solver, CppBOLOS, with the open-source combustion library, Cantera, at the source code level, ChemPlasKin computes time-resolved evolution of species concentration and gas temperature in a unified gas-plasma kinetics framework.","The model allows high fidelity predictions of both chemical thermal effects and plasma-induced heating, including fast gas heating and slower vibrational-translational relaxation processes.","Additionally, a new heat loss model is developed for nanosecond pulsed discharges, specifically within pin-pin electrode configurations.","With its versatility, ChemPlasKin is well-suited for a wide range of applications, from plasma-assisted combustion (PAC) to fuel reforming.","In this paper, the reliability, accuracy and efficiency of ChemPlasKin are validated through a number of test problems, demonstrating its utility in advancing gas-plasma kinetic studies."],"url":"http://arxiv.org/abs/2405.04224v1","category":"physics.plasm-ph"}
{"created":"2024-05-07 11:29:39","title":"DESI 2024: Reconstructing Dark Energy using Crossing Statistics with DESI DR1 BAO data","abstract":"We implement Crossing Statistics to reconstruct in a model-agnostic manner the expansion history of the universe and properties of dark energy, using DESI Data Release 1 (DR1) BAO data in combination with one of three different supernova compilations (PantheonPlus, Union3, and DES-SN5YR) and Planck CMB observations. Our results hint towards an evolving and emergent dark energy behaviour, with negligible presence of dark energy at $z\\gtrsim 1$, at varying significance depending on data sets combined. In all these reconstructions, the cosmological constant lies outside the 95\\% confidence intervals for some redshift ranges. This dark energy behaviour, reconstructed using Crossing Statistics, is in agreement with results from the conventional $w_0$--$w_a$ dark energy equation of state parametrization reported in the DESI Key cosmology paper. Our results add an extensive class of model-agnostic reconstructions with acceptable fits to the data, including models where cosmic acceleration slows down at low redshifts. We also report constraints on \\Hord\\ from our model-agnostic analysis, independent of the pre-recombination physics.","sentences":["We implement Crossing Statistics to reconstruct in a model-agnostic manner the expansion history of the universe and properties of dark energy, using DESI Data Release 1 (DR1) BAO data in combination with one of three different supernova compilations (PantheonPlus, Union3, and DES-SN5YR) and Planck CMB observations.","Our results hint towards an evolving and emergent dark energy behaviour, with negligible presence of dark energy at $z\\gtrsim 1$, at varying significance depending on data sets combined.","In all these reconstructions, the cosmological constant lies outside the 95\\% confidence intervals for some redshift ranges.","This dark energy behaviour, reconstructed using Crossing Statistics, is in agreement with results from the conventional $w_0$--$w_a$ dark energy equation of state parametrization reported in the DESI Key cosmology paper.","Our results add an extensive class of model-agnostic reconstructions with acceptable fits to the data, including models where cosmic acceleration slows down at low redshifts.","We also report constraints on \\Hord\\ from our model-agnostic analysis, independent of the pre-recombination physics."],"url":"http://arxiv.org/abs/2405.04216v1","category":"astro-ph.CO"}
{"created":"2024-05-07 11:21:41","title":"Collapsing immortal K\u00e4hler-Ricci flows","abstract":"We consider the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds with semiample canonical bundle and intermediate Kodaira dimension, and show that the flow collapses to a canonical metric on the base of the Iitaka fibration in the locally smooth topology and with bounded Ricci curvature away from the singular fibers. This follows from an asymptotic expansion for the evolving metrics, in the spirit of recent work of the first and third-named authors on collapsing Calabi-Yau metrics, and proves two conjectures of Song and Tian.","sentences":["We consider the K\\\"ahler-Ricci flow on compact K\\\"ahler manifolds with semiample canonical bundle and intermediate Kodaira dimension, and show that the flow collapses to a canonical metric on the base of the Iitaka fibration in the locally smooth topology and with bounded Ricci curvature away from the singular fibers.","This follows from an asymptotic expansion for the evolving metrics, in the spirit of recent work of the first and third-named authors on collapsing Calabi-Yau metrics, and proves two conjectures of Song and Tian."],"url":"http://arxiv.org/abs/2405.04208v1","category":"math.DG"}
{"created":"2024-05-07 11:18:58","title":"Control in the coefficients of an elliptic differential operator: topological derivatives and Pontryagin maximum principle","abstract":"We consider optimal control problems, where the control appears in the main part of the operator. We derive the Pontryagin maximum principle as a necessary optimality condition. The proof uses the concept of topological derivatives. In contrast to earlier works, we do not need continuity assumptions for the coefficient or gradients of solutions of partial differential equations. Following classical proofs, we consider perturbations of optimal controls by multiples of characteristic functions of sets, whose scaling factor is send to zero. For $2d$ problems, we can perform an optimization over the elliptic shapes of such sets leading to stronger optimality conditions involving a variational inequality of a new type.","sentences":["We consider optimal control problems, where the control appears in the main part of the operator.","We derive the Pontryagin maximum principle as a necessary optimality condition.","The proof uses the concept of topological derivatives.","In contrast to earlier works, we do not need continuity assumptions for the coefficient or gradients of solutions of partial differential equations.","Following classical proofs, we consider perturbations of optimal controls by multiples of characteristic functions of sets, whose scaling factor is send to zero.","For $2d$ problems, we can perform an optimization over the elliptic shapes of such sets leading to stronger optimality conditions involving a variational inequality of a new type."],"url":"http://arxiv.org/abs/2405.04204v1","category":"math.OC"}
{"created":"2024-05-07 11:15:26","title":"Fibonacci Neural Network Approach for Numerical Solutions of Fractional Order Differential Equations","abstract":"In this paper, the authors propose the utilization of Fibonacci Neural Networks (FNN) for solving arbitrary order differential equations. The FNN architecture comprises input, middle, and output layers, with various degrees of Fibonacci polynomials serving as activation functions in the middle layer. The trial solution of the differential equation is treated as the output of the FNN, which involves adjustable parameters (weights). These weights are iteratively updated during the training of the Fibonacci neural network using backpropagation. The efficacy of the proposed method is evaluated by solving five differential problems with known exact solutions, allowing for an assessment of its accuracy. Comparative analyses are conducted against previously established techniques, demonstrating superior accuracy and efficacy in solving the addressed problems.","sentences":["In this paper, the authors propose the utilization of Fibonacci Neural Networks (FNN) for solving arbitrary order differential equations.","The FNN architecture comprises input, middle, and output layers, with various degrees of Fibonacci polynomials serving as activation functions in the middle layer.","The trial solution of the differential equation is treated as the output of the FNN, which involves adjustable parameters (weights).","These weights are iteratively updated during the training of the Fibonacci neural network using backpropagation.","The efficacy of the proposed method is evaluated by solving five differential problems with known exact solutions, allowing for an assessment of its accuracy.","Comparative analyses are conducted against previously established techniques, demonstrating superior accuracy and efficacy in solving the addressed problems."],"url":"http://arxiv.org/abs/2405.04200v1","category":"math.NT"}
{"created":"2024-05-07 11:15:12","title":"Asymptotics of the partition function for beta-ensembles at high temperature","abstract":"We consider a model for a gas of $N$ confined particles interacting via a two-body logarithmic interaction, namely the real $\\beta$-ensembles. We are interested in the regime where the inverse temperature scales as $N\\beta=2P$ with $P$ a fixed positive parameter; this is called the high-temperature regime. The confining potential is of the form $x^2+\\phi$ with bounded smooth function $\\phi$. We establish for this model, the existence of a large-$N$ asymptotic expansion for the associated partition function. We also prove the existence of a large-$N$ asymptotic expansion of linear statistics for general confining potentials. Our method is based on the analysis of the loop equations. Finally, we establish a continuity result for the equilibrium density with respect to the potential dependence.","sentences":["We consider a model for a gas of $N$ confined particles interacting via a two-body logarithmic interaction, namely the real $\\beta$-ensembles.","We are interested in the regime where the inverse temperature scales as $N\\beta=2P$ with $P$ a fixed positive parameter; this is called the high-temperature regime.","The confining potential is of the form $x^2+\\phi$ with bounded smooth function $\\phi$. We establish for this model, the existence of a large-$N$ asymptotic expansion for the associated partition function.","We also prove the existence of a large-$N$ asymptotic expansion of linear statistics for general confining potentials.","Our method is based on the analysis of the loop equations.","Finally, we establish a continuity result for the equilibrium density with respect to the potential dependence."],"url":"http://arxiv.org/abs/2405.04199v1","category":"math.PR"}
{"created":"2024-05-07 10:53:20","title":"Effective and Robust Adversarial Training against Data and Label Corruptions","abstract":"Corruptions due to data perturbations and label noise are prevalent in the datasets from unreliable sources, which poses significant threats to model training. Despite existing efforts in developing robust models, current learning methods commonly overlook the possible co-existence of both corruptions, limiting the effectiveness and practicability of the model. In this paper, we develop an Effective and Robust Adversarial Training (ERAT) framework to simultaneously handle two types of corruption (i.e., data and label) without prior knowledge of their specifics. We propose a hybrid adversarial training surrounding multiple potential adversarial perturbations, alongside a semi-supervised learning based on class-rebalancing sample selection to enhance the resilience of the model for dual corruption. On the one hand, in the proposed adversarial training, the perturbation generation module learns multiple surrogate malicious data perturbations by taking a DNN model as the victim, while the model is trained to maintain semantic consistency between the original data and the hybrid perturbed data. It is expected to enable the model to cope with unpredictable perturbations in real-world data corruption. On the other hand, a class-rebalancing data selection strategy is designed to fairly differentiate clean labels from noisy labels. Semi-supervised learning is performed accordingly by discarding noisy labels. Extensive experiments demonstrate the superiority of the proposed ERAT framework.","sentences":["Corruptions due to data perturbations and label noise are prevalent in the datasets from unreliable sources, which poses significant threats to model training.","Despite existing efforts in developing robust models, current learning methods commonly overlook the possible co-existence of both corruptions, limiting the effectiveness and practicability of the model.","In this paper, we develop an Effective and Robust Adversarial Training (ERAT) framework to simultaneously handle two types of corruption (i.e., data and label) without prior knowledge of their specifics.","We propose a hybrid adversarial training surrounding multiple potential adversarial perturbations, alongside a semi-supervised learning based on class-rebalancing sample selection to enhance the resilience of the model for dual corruption.","On the one hand, in the proposed adversarial training, the perturbation generation module learns multiple surrogate malicious data perturbations by taking a DNN model as the victim, while the model is trained to maintain semantic consistency between the original data and the hybrid perturbed data.","It is expected to enable the model to cope with unpredictable perturbations in real-world data corruption.","On the other hand, a class-rebalancing data selection strategy is designed to fairly differentiate clean labels from noisy labels.","Semi-supervised learning is performed accordingly by discarding noisy labels.","Extensive experiments demonstrate the superiority of the proposed ERAT framework."],"url":"http://arxiv.org/abs/2405.04191v1","category":"cs.LG"}
{"created":"2024-05-07 10:21:23","title":"Topicwise Separable Sentence Retrieval for Medical Report Generation","abstract":"Automated radiology reporting holds immense clinical potential in alleviating the burdensome workload of radiologists and mitigating diagnostic bias. Recently, retrieval-based report generation methods have garnered increasing attention due to their inherent advantages in terms of the quality and consistency of generated reports. However, due to the long-tail distribution of the training data, these models tend to learn frequently occurring sentences and topics, overlooking the rare topics. Regrettably, in many cases, the descriptions of rare topics often indicate critical findings that should be mentioned in the report. To address this problem, we introduce a Topicwise Separable Sentence Retrieval (Teaser) for medical report generation. To ensure comprehensive learning of both common and rare topics, we categorize queries into common and rare types to learn differentiated topics, and then propose Topic Contrastive Loss to effectively align topics and queries in the latent space. Moreover, we integrate an Abstractor module following the extraction of visual features, which aids the topic decoder in gaining a deeper understanding of the visual observational intent. Experiments on the MIMIC-CXR and IU X-ray datasets demonstrate that Teaser surpasses state-of-the-art models, while also validating its capability to effectively represent rare topics and establish more dependable correspondences between queries and topics.","sentences":["Automated radiology reporting holds immense clinical potential in alleviating the burdensome workload of radiologists and mitigating diagnostic bias.","Recently, retrieval-based report generation methods have garnered increasing attention due to their inherent advantages in terms of the quality and consistency of generated reports.","However, due to the long-tail distribution of the training data, these models tend to learn frequently occurring sentences and topics, overlooking the rare topics.","Regrettably, in many cases, the descriptions of rare topics often indicate critical findings that should be mentioned in the report.","To address this problem, we introduce a Topicwise Separable Sentence Retrieval (Teaser) for medical report generation.","To ensure comprehensive learning of both common and rare topics, we categorize queries into common and rare types to learn differentiated topics, and then propose Topic Contrastive Loss to effectively align topics and queries in the latent space.","Moreover, we integrate an Abstractor module following the extraction of visual features, which aids the topic decoder in gaining a deeper understanding of the visual observational intent.","Experiments on the MIMIC-CXR and IU X-ray datasets demonstrate that Teaser surpasses state-of-the-art models, while also validating its capability to effectively represent rare topics and establish more dependable correspondences between queries and topics."],"url":"http://arxiv.org/abs/2405.04175v1","category":"cs.CV"}
{"created":"2024-05-07 10:14:33","title":"An efficient active-set method with applications to sparse approximations and risk minimization","abstract":"In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization. The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions. Further local linear (and potentially superlinear) convergence is shown under standard additional conditions. The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems. These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters. The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements. We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines. We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances.","sentences":["In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization.","The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions.","Further local linear (and potentially superlinear) convergence is shown under standard additional conditions.","The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems.","These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters.","The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements.","We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines.","We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances."],"url":"http://arxiv.org/abs/2405.04172v1","category":"math.OC"}
{"created":"2024-05-07 09:59:55","title":"Vaisman's theorem and local reducibility","abstract":"As proven in a celebrated theorem due to Vaisman, pure locally conformally K\\\"ahler metrics do not exist on compact K\\\"ahler manifolds. In a previous paper, we extended this result to the singular setting, more precisely to K\\\"ahler spaces which are locally irreducible. Without the additional assumption of local irreducibility, there are counterexamples for which Vaisman's theorem does not hold. In this article, we give a much broader sufficient condition under which Vaisman's theorem still holds for compact K\\\"ahler spaces which are locally reducible.","sentences":["As proven in a celebrated theorem due to Vaisman, pure locally conformally K\\\"ahler metrics do not exist on compact K\\\"ahler manifolds.","In a previous paper, we extended this result to the singular setting, more precisely to K\\\"ahler spaces which are locally irreducible.","Without the additional assumption of local irreducibility, there are counterexamples for which Vaisman's theorem does not hold.","In this article, we give a much broader sufficient condition under which Vaisman's theorem still holds for compact K\\\"ahler spaces which are locally reducible."],"url":"http://arxiv.org/abs/2405.04162v1","category":"math.DG"}
{"created":"2024-05-08 16:57:44","title":"Gamification in Software Engineering Education: a Tertiary Study","abstract":"As the significance of Software Engineering (SE) professionals continues to grow in the industry, the adoption of gamification techniques for training purposes has gained traction due to its potential to enhance class appeal through game-derived elements. This paper presents a tertiary study investigating the application of gamification in Software Engineering (SE) education. The study was conducted in response to recent systematic literature reviews and mappings on the topic. The findings reveal that the areas of SE most frequently gamified are Software Testing and Software Quality, with competition and cooperation being the most commonly utilized gamification elements. Additionally, the majority of studies focus on structural gamification, where game elements are employed to modify the learning environment without altering the content. The results demonstrate the potential of gamification to improve students' engagement and motivation throughout the SE learning process, while also impacting other aspects such as performance improvement, skill development, and fostering good SE practices. However, caution is advised as unplanned and incorrectly applied gamification measures may lead to significant declines in performance and motivation. (English Version of the paper in Portuguese available here: HTTP://doi.org/10.1145/3613372.3614193","sentences":["As the significance of Software Engineering (SE) professionals continues to grow in the industry, the adoption of gamification techniques for training purposes has gained traction due to its potential to enhance class appeal through game-derived elements.","This paper presents a tertiary study investigating the application of gamification in Software Engineering (SE) education.","The study was conducted in response to recent systematic literature reviews and mappings on the topic.","The findings reveal that the areas of SE most frequently gamified are Software Testing and Software Quality, with competition and cooperation being the most commonly utilized gamification elements.","Additionally, the majority of studies focus on structural gamification, where game elements are employed to modify the learning environment without altering the content.","The results demonstrate the potential of gamification to improve students' engagement and motivation throughout the SE learning process, while also impacting other aspects such as performance improvement, skill development, and fostering good SE practices.","However, caution is advised as unplanned and incorrectly applied gamification measures may lead to significant declines in performance and motivation.","(English Version of the paper in Portuguese available here: HTTP://doi.org/10.1145/3613372.3614193"],"url":"http://arxiv.org/abs/2405.05209v1","category":"cs.SE"}
{"created":"2024-05-08 16:43:50","title":"Anomaly Detection in Certificate Transparency Logs","abstract":"We propose an anomaly detection technique for X.509 certificates utilizing Isolation Forest. This method can be beneficial when compliance testing with X.509 linters proves unsatisfactory, and we seek to identify anomalies beyond standards compliance. The technique is validated on a sample of certificates from Certificate Transparency logs.","sentences":["We propose an anomaly detection technique for X.509 certificates utilizing Isolation Forest.","This method can be beneficial when compliance testing with X.509 linters proves unsatisfactory, and we seek to identify anomalies beyond standards compliance.","The technique is validated on a sample of certificates from Certificate Transparency logs."],"url":"http://arxiv.org/abs/2405.05206v1","category":"cs.CR"}
{"created":"2024-05-08 16:37:58","title":"Graded Relevance Scoring of Written Essays with Dense Retrieval","abstract":"Automated Essay Scoring automates the grading process of essays, providing a great advantage for improving the writing proficiency of students. While holistic essay scoring research is prevalent, a noticeable gap exists in scoring essays for specific quality traits. In this work, we focus on the relevance trait, which measures the ability of the student to stay on-topic throughout the entire essay. We propose a novel approach for graded relevance scoring of written essays that employs dense retrieval encoders. Dense representations of essays at different relevance levels then form clusters in the embeddings space, such that their centroids are potentially separate enough to effectively represent their relevance levels. We hence use the simple 1-Nearest-Neighbor classification over those centroids to determine the relevance level of an unseen essay. As an effective unsupervised dense encoder, we leverage Contriever, which is pre-trained with contrastive learning and demonstrated comparable performance to supervised dense retrieval models. We tested our approach on both task-specific (i.e., training and testing on same task) and cross-task (i.e., testing on unseen task) scenarios using the widely used ASAP++ dataset. Our method establishes a new state-of-the-art performance in the task-specific scenario, while its extension for the cross-task scenario exhibited a performance that is on par with the state-of-the-art model for that scenario. We also analyzed the performance of our approach in a more practical few-shot scenario, showing that it can significantly reduce the labeling cost while sacrificing only 10% of its effectiveness.","sentences":["Automated Essay Scoring automates the grading process of essays, providing a great advantage for improving the writing proficiency of students.","While holistic essay scoring research is prevalent, a noticeable gap exists in scoring essays for specific quality traits.","In this work, we focus on the relevance trait, which measures the ability of the student to stay on-topic throughout the entire essay.","We propose a novel approach for graded relevance scoring of written essays that employs dense retrieval encoders.","Dense representations of essays at different relevance levels then form clusters in the embeddings space, such that their centroids are potentially separate enough to effectively represent their relevance levels.","We hence use the simple 1-Nearest-Neighbor classification over those centroids to determine the relevance level of an unseen essay.","As an effective unsupervised dense encoder, we leverage Contriever, which is pre-trained with contrastive learning and demonstrated comparable performance to supervised dense retrieval models.","We tested our approach on both task-specific (i.e., training and testing on same task) and cross-task (i.e., testing on unseen task) scenarios using the widely used ASAP++ dataset.","Our method establishes a new state-of-the-art performance in the task-specific scenario, while its extension for the cross-task scenario exhibited a performance that is on par with the state-of-the-art model for that scenario.","We also analyzed the performance of our approach in a more practical few-shot scenario, showing that it can significantly reduce the labeling cost while sacrificing only 10% of its effectiveness."],"url":"http://arxiv.org/abs/2405.05200v1","category":"cs.IR"}
{"created":"2024-05-08 16:35:06","title":"SINBAD: Saliency-informed detection of breakage caused by ad blocking","abstract":"Privacy-enhancing blocking tools based on filter-list rules tend to break legitimate functionality. Filter-list maintainers could benefit from automated breakage detection tools that allow them to proactively fix problematic rules before deploying them to millions of users. We introduce SINBAD, an automated breakage detector that improves the accuracy over the state of the art by 20%, and is the first to detect dynamic breakage and breakage caused by style-oriented filter rules. The success of SINBAD is rooted in three innovations: (1) the use of user-reported breakage issues in forums that enable the creation of a high-quality dataset for training in which only breakage that users perceive as an issue is included; (2) the use of 'web saliency' to automatically identify user-relevant regions of a website on which to prioritize automated interactions aimed at triggering breakage; and (3) the analysis of webpages via subtrees which enables fine-grained identification of problematic filter rules.","sentences":["Privacy-enhancing blocking tools based on filter-list rules tend to break legitimate functionality.","Filter-list maintainers could benefit from automated breakage detection tools that allow them to proactively fix problematic rules before deploying them to millions of users.","We introduce SINBAD, an automated breakage detector that improves the accuracy over the state of the art by 20%, and is the first to detect dynamic breakage and breakage caused by style-oriented filter rules.","The success of SINBAD is rooted in three innovations: (1) the use of user-reported breakage issues in forums that enable the creation of a high-quality dataset for training in which only breakage that users perceive as an issue is included; (2) the use of 'web saliency' to automatically identify user-relevant regions of a website on which to prioritize automated interactions aimed at triggering breakage; and (3) the analysis of webpages via subtrees which enables fine-grained identification of problematic filter rules."],"url":"http://arxiv.org/abs/2405.05196v1","category":"cs.CR"}
{"created":"2024-05-08 16:12:45","title":"Air Gap: Protecting Privacy-Conscious Conversational Agents","abstract":"The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns. While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors. We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.   Grounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task. Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality. For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective.","sentences":["The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns.","While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors.","We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.   ","Grounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task.","Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality.","For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective."],"url":"http://arxiv.org/abs/2405.05175v1","category":"cs.CR"}
{"created":"2024-05-08 15:27:08","title":"Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal Violence Detection","abstract":"Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available. In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges. In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges. Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features. Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features. Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance. Code is available at https://github.com/shengyangsun/MSBT.","sentences":["Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available.","In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges.","In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges.","Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features.","Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features.","Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance.","Code is available at https://github.com/shengyangsun/MSBT."],"url":"http://arxiv.org/abs/2405.05130v1","category":"cs.CV"}
{"created":"2024-05-08 14:44:34","title":"Approximation properties relative to continuous scale space for hybrid discretizations of Gaussian derivative operators","abstract":"This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences. The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.   While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.   In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods.","sentences":["This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences.","The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.   ","While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.   ","In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods."],"url":"http://arxiv.org/abs/2405.05095v1","category":"math.NA"}
{"created":"2024-05-08 13:38:56","title":"Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources","abstract":"Background Advancements in Large Language Models (LLMs) hold transformative potential in healthcare, however, recent work has raised concern about the tendency of these models to produce outputs that display racial or gender biases. Although training data is a likely source of such biases, exploration of disease and demographic associations in text data at scale has been limited.   Methods We conducted a large-scale textual analysis using a dataset comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl. The study analyzed the context in which various diseases are discussed alongside markers of race and gender. Given that LLMs are pre-trained on similar datasets, this approach allowed us to examine the potential biases that LLMs may learn and internalize. We compared these findings with actual demographic disease prevalence as well as GPT-4 outputs in order to evaluate the extent of bias representation.   Results Our findings indicate that demographic terms are disproportionately associated with specific disease concepts in online texts. gender terms are prominently associated with disease concepts, while racial terms are much less frequently associated. We find widespread disparities in the associations of specific racial and gender terms with the 18 diseases analyzed. Most prominently, we see an overall significant overrepresentation of Black race mentions in comparison to population proportions.   Conclusions Our results highlight the need for critical examination and transparent reporting of biases in LLM pretraining datasets. Our study suggests the need to develop mitigation strategies to counteract the influence of biased training data in LLMs, particularly in sensitive domains such as healthcare.","sentences":["Background Advancements in Large Language Models (LLMs) hold transformative potential in healthcare, however, recent work has raised concern about the tendency of these models to produce outputs that display racial or gender biases.","Although training data is a likely source of such biases, exploration of disease and demographic associations in text data at scale has been limited.   ","Methods We conducted a large-scale textual analysis using a dataset comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl.","The study analyzed the context in which various diseases are discussed alongside markers of race and gender.","Given that LLMs are pre-trained on similar datasets, this approach allowed us to examine the potential biases that LLMs may learn and internalize.","We compared these findings with actual demographic disease prevalence as well as GPT-4 outputs in order to evaluate the extent of bias representation.   ","Results Our findings indicate that demographic terms are disproportionately associated with specific disease concepts in online texts.","gender terms are prominently associated with disease concepts, while racial terms are much less frequently associated.","We find widespread disparities in the associations of specific racial and gender terms with the 18 diseases analyzed.","Most prominently, we see an overall significant overrepresentation of Black race mentions in comparison to population proportions.   ","Conclusions Our results highlight the need for critical examination and transparent reporting of biases in LLM pretraining datasets.","Our study suggests the need to develop mitigation strategies to counteract the influence of biased training data in LLMs, particularly in sensitive domains such as healthcare."],"url":"http://arxiv.org/abs/2405.05049v1","category":"cs.CL"}
{"created":"2024-05-08 11:01:21","title":"P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models","abstract":"In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL). However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself. In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type. With such significant information, the LLM can achieve entity classification more precisely. To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering. Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection.","sentences":["In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL).","However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself.","In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type.","With such significant information, the LLM can achieve entity classification more precisely.","To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering.","Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection."],"url":"http://arxiv.org/abs/2405.04960v1","category":"cs.CL"}
{"created":"2024-05-08 10:04:52","title":"A Graph-Theoretical Approach to Ring Analysis: An Exploration of Dominant Metric Dimension in Compressed Zero Divisor Graphs and Its Interplay with Ring Structures","abstract":"The paper systematically classifies rings based on the dominant metric dimensions (Ddim) of their associated CZDG, establishing consequential bounds for the Ddim of these compressed zero-divisor graphs. The authors investigate the interplay between the ring-theoretic properties of a ring ( R ) and associated CZDG. An undirected graph consisting of vertex set ( Z(R_E)\\{[0]}\\ =\\ R_E\\{[0],[1]}), where ( R_E=\\{[x]:\\ x\\in R\\} ) and ([x]=\\{y\\in R:\\ \\text{ann}(x)=\\text{ann}(y)\\} ) is called a compressed zero-divisor graph, denoted by ( \\Gamma_E(R) ). An edge is formed between two vertices ([x]) and ([y]) of ( Z(R_E) ) if and only if ([x][y]=[xy]=[0]), that is, iff ( xy=0 ). For a ring ( R ), graph ( G ) is said to be realizable as ( \\Gamma_E(R) ) if ( G ) is isomorphic to ( \\Gamma_E(R) ). Moreover, an exploration into the Ddim of realizable graphs for rings is conducted, complemented by illustrative examples reinforcing the presented results. A recent discussion within the paper elucidates the nuanced relationship between Ddim, diameter, and girth within the domain of compressed zero-divisor graphs. This research offers a comprehensive and insightful analysis at the intersection of algebraic structures and graph theory, providing valuable contributions to the current mathematical discourse.","sentences":["The paper systematically classifies rings based on the dominant metric dimensions (Ddim) of their associated CZDG, establishing consequential bounds for the Ddim of these compressed zero-divisor graphs.","The authors investigate the interplay between the ring-theoretic properties of a ring ( R ) and associated CZDG.","An undirected graph consisting of vertex set ( Z(R_E)\\{[0]}\\ =\\ R_E\\{[0],[1]}), where ( R_E=\\{[x]:\\ x\\in R\\} ) and ([x]=\\{y\\in R:\\ \\text{ann}(x)=\\text{ann}(y)\\} ) is called a compressed zero-divisor graph, denoted by ( \\Gamma_E(R) ).","An edge is formed between two vertices ([x]) and ([y]) of ( Z(R_E) )","if and only if ([x][y]=[xy]=[0]), that is, iff ( xy=0 ).","For a ring ( R ), graph ( G ) is said to be realizable as ( \\Gamma_E(R) ) if ( G ) is isomorphic to ( \\Gamma_E(R) ).","Moreover, an exploration into the Ddim of realizable graphs for rings is conducted, complemented by illustrative examples reinforcing the presented results.","A recent discussion within the paper elucidates the nuanced relationship between Ddim, diameter, and girth within the domain of compressed zero-divisor graphs.","This research offers a comprehensive and insightful analysis at the intersection of algebraic structures and graph theory, providing valuable contributions to the current mathematical discourse."],"url":"http://arxiv.org/abs/2405.04934v1","category":"math.AC"}
{"created":"2024-05-08 09:35:26","title":"Weakly-supervised Semantic Segmentation via Dual-stream Contrastive Learning of Cross-image Contextual Information","abstract":"Weakly supervised semantic segmentation (WSSS) aims at learning a semantic segmentation model with only image-level tags. Despite intensive research on deep learning approaches over a decade, there is still a significant performance gap between WSSS and full semantic segmentation. Most current WSSS methods always focus on a limited single image (pixel-wise) information while ignoring the valuable inter-image (semantic-wise) information. From this perspective, a novel end-to-end WSSS framework called DSCNet is developed along with two innovations: i) pixel-wise group contrast and semantic-wise graph contrast are proposed and introduced into the WSSS framework; ii) a novel dual-stream contrastive learning (DSCL) mechanism is designed to jointly handle pixel-wise and semantic-wise context information for better WSSS performance. Specifically, the pixel-wise group contrast learning (PGCL) and semantic-wise graph contrast learning (SGCL) tasks form a more comprehensive solution. Extensive experiments on PASCAL VOC and MS COCO benchmarks verify the superiority of DSCNet over SOTA approaches and baseline models.","sentences":["Weakly supervised semantic segmentation (WSSS) aims at learning a semantic segmentation model with only image-level tags.","Despite intensive research on deep learning approaches over a decade, there is still a significant performance gap between WSSS and full semantic segmentation.","Most current WSSS methods always focus on a limited single image (pixel-wise) information while ignoring the valuable inter-image (semantic-wise) information.","From this perspective, a novel end-to-end WSSS framework called DSCNet is developed along with two innovations: i) pixel-wise group contrast and semantic-wise graph contrast are proposed and introduced into the WSSS framework; ii) a novel dual-stream contrastive learning (DSCL) mechanism is designed to jointly handle pixel-wise and semantic-wise context information for better WSSS performance.","Specifically, the pixel-wise group contrast learning (PGCL) and semantic-wise graph contrast learning (SGCL) tasks form a more comprehensive solution.","Extensive experiments on PASCAL VOC and MS COCO benchmarks verify the superiority of DSCNet over SOTA approaches and baseline models."],"url":"http://arxiv.org/abs/2405.04913v1","category":"cs.CV"}
{"created":"2024-05-08 02:42:27","title":"Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches","abstract":"To build a cross-modal latent space between 3D human motion and language, acquiring large-scale and high-quality human motion data is crucial. However, unlike the abundance of image data, the scarcity of motion data has limited the performance of existing motion-language models. To counter this, we introduce \"motion patches\", a new representation of motion sequences, and propose using Vision Transformers (ViT) as motion encoders via transfer learning, aiming to extract useful knowledge from the image domain and apply it to the motion domain. These motion patches, created by dividing and sorting skeleton joints based on body parts in motion sequences, are robust to varying skeleton structures, and can be regarded as color image patches in ViT. We find that transfer learning with pre-trained weights of ViT obtained through training with 2D image data can boost the performance of motion analysis, presenting a promising direction for addressing the issue of limited motion data. Our extensive experiments show that the proposed motion patches, used jointly with ViT, achieve state-of-the-art performance in the benchmarks of text-to-motion retrieval, and other novel challenging tasks, such as cross-skeleton recognition, zero-shot motion classification, and human interaction recognition, which are currently impeded by the lack of data.","sentences":["To build a cross-modal latent space between 3D human motion and language, acquiring large-scale and high-quality human motion data is crucial.","However, unlike the abundance of image data, the scarcity of motion data has limited the performance of existing motion-language models.","To counter this, we introduce \"motion patches\", a new representation of motion sequences, and propose using Vision Transformers (ViT) as motion encoders via transfer learning, aiming to extract useful knowledge from the image domain and apply it to the motion domain.","These motion patches, created by dividing and sorting skeleton joints based on body parts in motion sequences, are robust to varying skeleton structures, and can be regarded as color image patches in ViT.","We find that transfer learning with pre-trained weights of ViT obtained through training with 2D image data can boost the performance of motion analysis, presenting a promising direction for addressing the issue of limited motion data.","Our extensive experiments show that the proposed motion patches, used jointly with ViT, achieve state-of-the-art performance in the benchmarks of text-to-motion retrieval, and other novel challenging tasks, such as cross-skeleton recognition, zero-shot motion classification, and human interaction recognition, which are currently impeded by the lack of data."],"url":"http://arxiv.org/abs/2405.04771v1","category":"cs.CV"}
{"created":"2024-05-08 01:04:36","title":"All in One Framework for Multimodal Re-identification in the Wild","abstract":"In Re-identification (ReID), recent advancements yield noteworthy progress in both unimodal and cross-modal retrieval tasks. However, the challenge persists in developing a unified framework that could effectively handle varying multimodal data, including RGB, infrared, sketches, and textual information. Additionally, the emergence of large-scale models shows promising performance in various vision tasks but the foundation model in ReID is still blank. In response to these challenges, a novel multimodal learning paradigm for ReID is introduced, referred to as All-in-One (AIO), which harnesses a frozen pre-trained big model as an encoder, enabling effective multimodal retrieval without additional fine-tuning. The diverse multimodal data in AIO are seamlessly tokenized into a unified space, allowing the modality-shared frozen encoder to extract identity-consistent features comprehensively across all modalities. Furthermore, a meticulously crafted ensemble of cross-modality heads is designed to guide the learning trajectory. AIO is the \\textbf{first} framework to perform all-in-one ReID, encompassing four commonly used modalities. Experiments on cross-modal and multimodal ReID reveal that AIO not only adeptly handles various modal data but also excels in challenging contexts, showcasing exceptional performance in zero-shot and domain generalization scenarios.","sentences":["In Re-identification (ReID), recent advancements yield noteworthy progress in both unimodal and cross-modal retrieval tasks.","However, the challenge persists in developing a unified framework that could effectively handle varying multimodal data, including RGB, infrared, sketches, and textual information.","Additionally, the emergence of large-scale models shows promising performance in various vision tasks but the foundation model in ReID is still blank.","In response to these challenges, a novel multimodal learning paradigm for ReID is introduced, referred to as All-in-One (AIO), which harnesses a frozen pre-trained big model as an encoder, enabling effective multimodal retrieval without additional fine-tuning.","The diverse multimodal data in AIO are seamlessly tokenized into a unified space, allowing the modality-shared frozen encoder to extract identity-consistent features comprehensively across all modalities.","Furthermore, a meticulously crafted ensemble of cross-modality heads is designed to guide the learning trajectory.","AIO is the \\textbf{first} framework to perform all-in-one ReID, encompassing four commonly used modalities.","Experiments on cross-modal and multimodal ReID reveal that AIO not only adeptly handles various modal data but also excels in challenging contexts, showcasing exceptional performance in zero-shot and domain generalization scenarios."],"url":"http://arxiv.org/abs/2405.04741v1","category":"cs.CV"}
{"created":"2024-05-08 00:18:56","title":"Learning Phonotactics from Linguistic Informants","abstract":"We propose an interactive approach to language learning that utilizes linguistic acceptability judgments from an informant (a competent language user) to learn a grammar. Given a grammar formalism and a framework for synthesizing data, our model iteratively selects or synthesizes a data-point according to one of a range of information-theoretic policies, asks the informant for a binary judgment, and updates its own parameters in preparation for the next query. We demonstrate the effectiveness of our model in the domain of phonotactics, the rules governing what kinds of sound-sequences are acceptable in a language, and carry out two experiments, one with typologically-natural linguistic data and another with a range of procedurally-generated languages. We find that the information-theoretic policies that our model uses to select items to query the informant achieve sample efficiency comparable to, and sometimes greater than, fully supervised approaches.","sentences":["We propose an interactive approach to language learning that utilizes linguistic acceptability judgments from an informant (a competent language user) to learn a grammar.","Given a grammar formalism and a framework for synthesizing data, our model iteratively selects or synthesizes a data-point according to one of a range of information-theoretic policies, asks the informant for a binary judgment, and updates its own parameters in preparation for the next query.","We demonstrate the effectiveness of our model in the domain of phonotactics, the rules governing what kinds of sound-sequences are acceptable in a language, and carry out two experiments, one with typologically-natural linguistic data and another with a range of procedurally-generated languages.","We find that the information-theoretic policies that our model uses to select items to query the informant achieve sample efficiency comparable to, and sometimes greater than, fully supervised approaches."],"url":"http://arxiv.org/abs/2405.04726v1","category":"cs.CL"}
{"created":"2024-05-07 23:08:24","title":"Untangling Lariats: Subgradient Following of Variationally Penalized Objectives","abstract":"We describe a novel subgradient following apparatus for calculating the optimum of convex problems with variational penalties. In this setting, we receive a sequence $y_i,\\ldots,y_n$ and seek a smooth sequence $x_1,\\ldots,x_n$. The smooth sequence attains the minimum Bregman divergence to an input sequence with additive variational penalties in the general form of $\\sum_i g_i(x_{i+1}-x_i)$. We derive, as special cases of our apparatus, known algorithms for the fused lasso and isotonic regression. Our approach also facilitates new variational penalties such as non-smooth barrier functions. We next derive and analyze multivariate problems in which $\\mathbf{x}_i,\\mathbf{y}_i\\in\\mathbb{R}^d$ and variational penalties that depend on $\\|\\mathbf{x}_{i+1}-\\mathbf{x}_i\\|$. The norms we consider are $\\ell_2$ and $\\ell_\\infty$ which promote group sparsity. Last but not least, we derive a lattice-based subgradient following for variational penalties characterized through the output of arbitrary convolutional filters. This paradigm yields efficient solvers for problems in which sparse high-order discrete derivatives such as acceleration and jerk are desirable.","sentences":["We describe a novel subgradient following apparatus for calculating the optimum of convex problems with variational penalties.","In this setting, we receive a sequence $y_i,\\ldots,y_n$ and seek a smooth sequence $x_1,\\ldots,x_n$.","The smooth sequence attains the minimum Bregman divergence to an input sequence with additive variational penalties in the general form of $\\sum_i g_i(x_{i+1}-x_i)$. We derive, as special cases of our apparatus, known algorithms for the fused lasso and isotonic regression.","Our approach also facilitates new variational penalties such as non-smooth barrier functions.","We next derive and analyze multivariate problems in which $\\mathbf{x}_i,\\mathbf{y}_i\\in\\mathbb{R}^d$ and variational penalties that depend on $\\|\\mathbf{x}_{i+1}-\\mathbf{x}_i\\|$. The norms we consider are $\\ell_2$ and $\\ell_\\infty$ which promote group sparsity.","Last but not least, we derive a lattice-based subgradient following for variational penalties characterized through the output of arbitrary convolutional filters.","This paradigm yields efficient solvers for problems in which sparse high-order discrete derivatives such as acceleration and jerk are desirable."],"url":"http://arxiv.org/abs/2405.04710v1","category":"cs.LG"}
{"created":"2024-05-07 22:06:24","title":"Carbon Filter: Real-time Alert Triage Using Large Scale Clustering and Fast Search","abstract":"\"Alert fatigue\" is one of the biggest challenges faced by the Security Operations Center (SOC) today, with analysts spending more than half of their time reviewing false alerts. Endpoint detection products raise alerts by pattern matching on event telemetry against behavioral rules that describe potentially malicious behavior, but can suffer from high false positives that distract from actual attacks. While alert triage techniques based on data provenance may show promise, these techniques can take over a minute to inspect a single alert, while EDR customers may face tens of millions of alerts per day; the current reality is that these approaches aren't nearly scalable enough for production environments.   We present Carbon Filter, a statistical learning based system that dramatically reduces the number of alerts analysts need to manually review. Our approach is based on the observation that false alert triggers can be efficiently identified and separated from suspicious behaviors by examining the process initiation context (e.g., the command line) that launched the responsible process. Through the use of fast-search algorithms for training and inference, our approach scales to millions of alerts per day. Through batching queries to the model, we observe a theoretical maximum throughput of 20 million alerts per hour. Based on the analysis of tens of million alerts from customer deployments, our solution resulted in a 6-fold improvement in the Signal-to-Noise ratio without compromising on alert triage performance.","sentences":["\"Alert fatigue\" is one of the biggest challenges faced by the Security Operations Center (SOC) today, with analysts spending more than half of their time reviewing false alerts.","Endpoint detection products raise alerts by pattern matching on event telemetry against behavioral rules that describe potentially malicious behavior, but can suffer from high false positives that distract from actual attacks.","While alert triage techniques based on data provenance may show promise, these techniques can take over a minute to inspect a single alert, while EDR customers may face tens of millions of alerts per day; the current reality is that these approaches aren't nearly scalable enough for production environments.   ","We present Carbon Filter, a statistical learning based system that dramatically reduces the number of alerts analysts need to manually review.","Our approach is based on the observation that false alert triggers can be efficiently identified and separated from suspicious behaviors by examining the process initiation context (e.g., the command line) that launched the responsible process.","Through the use of fast-search algorithms for training and inference, our approach scales to millions of alerts per day.","Through batching queries to the model, we observe a theoretical maximum throughput of 20 million alerts per hour.","Based on the analysis of tens of million alerts from customer deployments, our solution resulted in a 6-fold improvement in the Signal-to-Noise ratio without compromising on alert triage performance."],"url":"http://arxiv.org/abs/2405.04691v1","category":"cs.CR"}
{"created":"2024-05-07 21:18:34","title":"TexControl: Sketch-Based Two-Stage Fashion Image Generation Using Diffusion Model","abstract":"Deep learning-based sketch-to-clothing image generation provides the initial designs and inspiration in the fashion design processes. However, clothing generation from freehand drawing is challenging due to the sparse and ambiguous information from the drawn sketches. The current generation models may have difficulty generating detailed texture information. In this work, we propose TexControl, a sketch-based fashion generation framework that uses a two-stage pipeline to generate the fashion image corresponding to the sketch input. First, we adopt ControlNet to generate the fashion image from sketch and keep the image outline stable. Then, we use an image-to-image method to optimize the detailed textures of the generated images and obtain the final results. The evaluation results show that TexControl can generate fashion images with high-quality texture as fine-grained image generation.","sentences":["Deep learning-based sketch-to-clothing image generation provides the initial designs and inspiration in the fashion design processes.","However, clothing generation from freehand drawing is challenging due to the sparse and ambiguous information from the drawn sketches.","The current generation models may have difficulty generating detailed texture information.","In this work, we propose TexControl, a sketch-based fashion generation framework that uses a two-stage pipeline to generate the fashion image corresponding to the sketch input.","First, we adopt ControlNet to generate the fashion image from sketch and keep the image outline stable.","Then, we use an image-to-image method to optimize the detailed textures of the generated images and obtain the final results.","The evaluation results show that TexControl can generate fashion images with high-quality texture as fine-grained image generation."],"url":"http://arxiv.org/abs/2405.04675v1","category":"cs.CV"}
{"created":"2024-05-07 19:14:57","title":"SingIt! Singer Voice Transformation","abstract":"In this paper, we propose a model which can generate a singing voice from normal speech utterance by harnessing zero-shot, many-to-many style transfer learning. Our goal is to give anyone the opportunity to sing any song in a timely manner. We present a system comprising several available blocks, as well as a modified auto-encoder, and show how this highly-complex challenge can be achieved by tailoring rather simple solutions together. We demonstrate the applicability of the proposed system using a group of 25 non-expert listeners. Samples of the data generated from our model are provided.","sentences":["In this paper, we propose a model which can generate a singing voice from normal speech utterance by harnessing zero-shot, many-to-many style transfer learning.","Our goal is to give anyone the opportunity to sing any song in a timely manner.","We present a system comprising several available blocks, as well as a modified auto-encoder, and show how this highly-complex challenge can be achieved by tailoring rather simple solutions together.","We demonstrate the applicability of the proposed system using a group of 25 non-expert listeners.","Samples of the data generated from our model are provided."],"url":"http://arxiv.org/abs/2405.04627v1","category":"eess.AS"}
{"created":"2024-05-07 18:45:37","title":"Learning Distributional Demonstration Spaces for Task-Specific Cross-Pose Estimation","abstract":"Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object. Previous work has shown success in learning relative placement tasks from just a small number of demonstrations when using relational reasoning networks with geometric inductive biases. However, such methods cannot flexibly represent multimodal tasks, like a mug hanging on any of n racks. We propose a method that incorporates additional properties that enable learning multimodal relative placement solutions, while retaining the provably translation-invariant and relational properties of prior work. We show that our method is able to learn precise relative placement tasks with only 10-20 multimodal demonstrations with no human annotations across a diverse set of objects within a category.","sentences":["Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object.","Previous work has shown success in learning relative placement tasks from just a small number of demonstrations when using relational reasoning networks with geometric inductive biases.","However, such methods cannot flexibly represent multimodal tasks, like a mug hanging on any of n racks.","We propose a method that incorporates additional properties that enable learning multimodal relative placement solutions, while retaining the provably translation-invariant and relational properties of prior work.","We show that our method is able to learn precise relative placement tasks with only 10-20 multimodal demonstrations with no human annotations across a diverse set of objects within a category."],"url":"http://arxiv.org/abs/2405.04609v1","category":"cs.RO"}
{"created":"2024-05-07 18:25:01","title":"Predictions for the abundance and clustering of H$\u03b1$ emitting galaxies","abstract":"We predict the surface density and clustering bias of H$\\alpha$ emitting galaxies for the Euclid and Nancy Grace Roman Space Telescope redshift surveys using a new calibration of the GALFORM galaxy formation model. We generate 3000 GALFORM models to train an ensemble of deep learning algorithms to create an emulator. We then use this emulator in a Markov Chain Monte Carlo (MCMC) parameter search of an eleven-dimensional parameter space, to find a best-fitting model to a calibration dataset that includes local luminosity function data, and, for the first time, higher redshift data, namely the number counts of H$\\alpha$ emitters. We discover tensions when exploring fits for the observational data when applying a heuristic weighting scheme in the MCMC framework. We find improved fits to the H$\\alpha$ number counts while maintaining appropriate predictions for the local universe luminosity function. For a flux limited Euclid-like survey to a depth of 2$\\times$10$^{-16}$ erg$^{-1}$ s$^{-1}$ cm$^{-2}$ for sources in the redshift range 0.9 < $z$ < 1.8, we estimate 2962-4331 H$\\alpha$ emission-line sources deg$^{-2}$. For a Nancy Grace Roman survey, with a flux limit of 1$\\times$10$^{-16}$ erg$^{-1}$ s$^{-1}$ cm$^{-2}$ and a redshift range 1.0 < $z$ < 2.0, we predict 6786-10322 H$\\alpha$ emission-line sources deg$^{-2}$.","sentences":["We predict the surface density and clustering bias of H$\\alpha$ emitting galaxies for the Euclid and Nancy Grace Roman Space Telescope redshift surveys using a new calibration of the GALFORM galaxy formation model.","We generate 3000 GALFORM models to train an ensemble of deep learning algorithms to create an emulator.","We then use this emulator in a Markov Chain Monte Carlo (MCMC) parameter search of an eleven-dimensional parameter space, to find a best-fitting model to a calibration dataset that includes local luminosity function data, and, for the first time, higher redshift data, namely the number counts of H$\\alpha$ emitters.","We discover tensions when exploring fits for the observational data when applying a heuristic weighting scheme in the MCMC framework.","We find improved fits to the H$\\alpha$ number counts while maintaining appropriate predictions for the local universe luminosity function.","For a flux limited Euclid-like survey to a depth of 2$\\times$10$^{-16}$ erg$^{-1}$ s$^{-1}$ cm$^{-2}$ for sources in the redshift range 0.9 <","$z$ < 1.8, we estimate 2962-4331 H$\\alpha$ emission-line sources deg$^{-2}$.","For a Nancy Grace Roman survey, with a flux limit of 1$\\times$10$^{-16}$ erg$^{-1}$ s$^{-1}$ cm$^{-2}$ and a redshift range 1.0 < $z$ < 2.0, we predict 6786-10322 H$\\alpha$ emission-line sources deg$^{-2}$."],"url":"http://arxiv.org/abs/2405.04601v1","category":"astro-ph.CO"}
{"created":"2024-05-07 18:10:54","title":"Integrating knowledge-guided symbolic regression and model-based design of experiments to automate process flow diagram development","abstract":"New products must be formulated rapidly to succeed in the global formulated product market; however, key product indicators (KPIs) can be complex, poorly understood functions of the chemical composition and processing history. Consequently, scale-up must currently undergo expensive trial-and-error campaigns. To accelerate process flow diagram (PFD) optimisation and knowledge discovery, this work proposed a novel digital framework to automatically quantify process mechanisms by integrating symbolic regression (SR) within model-based design of experiments (MBDoE). Each iteration, SR proposed a Pareto front of interpretable mechanistic expressions, and then MBDoE designed a new experiment to discriminate between them while balancing PFD optimisation. To investigate the framework's performance, a new process model capable of simulating general formulated product synthesis was constructed to generate in-silico data for different case studies. The framework could effectively discover ground-truth process mechanisms within a few iterations, indicating its great potential for use within the general chemical industry for digital manufacturing and product innovation.","sentences":["New products must be formulated rapidly to succeed in the global formulated product market; however, key product indicators (KPIs) can be complex, poorly understood functions of the chemical composition and processing history.","Consequently, scale-up must currently undergo expensive trial-and-error campaigns.","To accelerate process flow diagram (PFD) optimisation and knowledge discovery, this work proposed a novel digital framework to automatically quantify process mechanisms by integrating symbolic regression (SR) within model-based design of experiments (MBDoE).","Each iteration, SR proposed a Pareto front of interpretable mechanistic expressions, and then MBDoE designed a new experiment to discriminate between them while balancing PFD optimisation.","To investigate the framework's performance, a new process model capable of simulating general formulated product synthesis was constructed to generate in-silico data for different case studies.","The framework could effectively discover ground-truth process mechanisms within a few iterations, indicating its great potential for use within the general chemical industry for digital manufacturing and product innovation."],"url":"http://arxiv.org/abs/2405.04592v1","category":"cs.LG"}
{"created":"2024-05-07 17:59:31","title":"ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning","abstract":"Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including the estimation of 3D pose, shape, contact, human-object interaction, emotion, and more. Each of these methods works in isolation instead of synergistically. Here we address this problem and build a language-driven human understanding system -- ChatHuman, which combines and integrates the skills of many different methods. To do so, we finetune a Large Language Model (LLM) to select and use a wide variety of existing tools in response to user inputs. In doing so, ChatHuman is able to combine information from multiple tools to solve problems more accurately than the individual tools themselves and to leverage tool output to improve its ability to reason about humans. The novel features of ChatHuman include leveraging academic publications to guide the application of 3D human-related tools, employing a retrieval-augmented generation model to generate in-context-learning examples for handling new tools, and discriminating and integrating tool results to enhance 3D human understanding. Our experiments show that ChatHuman outperforms existing models in both tool selection accuracy and performance across multiple 3D human-related tasks. ChatHuman is a step towards consolidating diverse methods for human analysis into a single, powerful, system for 3D human reasoning.","sentences":["Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including the estimation of 3D pose, shape, contact, human-object interaction, emotion, and more.","Each of these methods works in isolation instead of synergistically.","Here we address this problem and build a language-driven human understanding system -- ChatHuman, which combines and integrates the skills of many different methods.","To do so, we finetune a Large Language Model (LLM) to select and use a wide variety of existing tools in response to user inputs.","In doing so, ChatHuman is able to combine information from multiple tools to solve problems more accurately than the individual tools themselves and to leverage tool output to improve its ability to reason about humans.","The novel features of ChatHuman include leveraging academic publications to guide the application of 3D human-related tools, employing a retrieval-augmented generation model to generate in-context-learning examples for handling new tools, and discriminating and integrating tool results to enhance 3D human understanding.","Our experiments show that ChatHuman outperforms existing models in both tool selection accuracy and performance across multiple 3D human-related tasks.","ChatHuman is a step towards consolidating diverse methods for human analysis into a single, powerful, system for 3D human reasoning."],"url":"http://arxiv.org/abs/2405.04533v1","category":"cs.CV"}
{"created":"2024-05-07 17:52:51","title":"NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts","abstract":"Large language models (LLMs) have manifested strong ability to generate codes for productive activities. However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding. To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains. Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction. Comparing with manual solutions, it achieves an efficiency increase of more than 4 times. Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval. On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB. The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench.","sentences":["Large language models (LLMs) have manifested strong ability to generate codes for productive activities.","However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding.","To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks.","NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains.","Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction.","Comparing with manual solutions, it achieves an efficiency increase of more than 4 times.","Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval.","On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB.","The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench."],"url":"http://arxiv.org/abs/2405.04520v1","category":"cs.CL"}
{"created":"2024-05-07 17:25:56","title":"Fast Decentralized Gradient Tracking for Federated Minimax Optimization with Local Updates","abstract":"Federated learning (FL) for minimax optimization has emerged as a powerful paradigm for training models across distributed nodes/clients while preserving data privacy and model robustness on data heterogeneity. In this work, we delve into the decentralized implementation of federated minimax optimization by proposing \\texttt{K-GT-Minimax}, a novel decentralized minimax optimization algorithm that combines local updates and gradient tracking techniques. Our analysis showcases the algorithm's communication efficiency and convergence rate for nonconvex-strongly-concave (NC-SC) minimax optimization, demonstrating a superior convergence rate compared to existing methods. \\texttt{K-GT-Minimax}'s ability to handle data heterogeneity and ensure robustness underscores its significance in advancing federated learning research and applications.","sentences":["Federated learning (FL) for minimax optimization has emerged as a powerful paradigm for training models across distributed nodes/clients while preserving data privacy and model robustness on data heterogeneity.","In this work, we delve into the decentralized implementation of federated minimax optimization by proposing \\texttt{K-GT-Minimax}, a novel decentralized minimax optimization algorithm that combines local updates and gradient tracking techniques.","Our analysis showcases the algorithm's communication efficiency and convergence rate for nonconvex-strongly-concave (NC-SC) minimax optimization, demonstrating a superior convergence rate compared to existing methods.","\\texttt{K-GT-Minimax}'s ability to handle data heterogeneity and ensure robustness underscores its significance in advancing federated learning research and applications."],"url":"http://arxiv.org/abs/2405.04566v1","category":"cs.LG"}
{"created":"2024-05-07 17:25:14","title":"Physics-data hybrid dynamic model of a multi-axis manipulator for sensorless dexterous manipulation and high-performance motion planning","abstract":"We report on the development of an implementable physics-data hybrid dynamic model for an articulated manipulator to plan and operate in various scenarios. Meanwhile, the physics-based and data-driven dynamic models are studied in this research to select the best model for planning. The physics-based model is constructed using the Lagrangian method, and the loss terms include inertia loss, viscous loss, and friction loss. As for the data-driven model, three methods are explored, including DNN, LSTM, and XGBoost. Our modeling results demonstrate that, after comprehensive hyperparameter optimization, the XGBoost architecture outperforms DNN and LSTM in accurately representing manipulator dynamics. The hybrid model with physics-based and data-driven terms has the best performance among all models based on the RMSE criteria, and it only needs about 24k of training data. In addition, we developed a virtual force sensor of a manipulator using the observed external torque derived from the dynamic model and designed a motion planner through the physics-data hybrid dynamic model. The external torque contributes to forces and torque on the end effector, facilitating interaction with the surroundings, while the internal torque governs manipulator motion dynamics and compensates for internal losses. By estimating external torque via the difference between measured joint torque and internal losses, we implement a sensorless control strategy which is demonstrated through a peg-in-hole task. Lastly, a learning-based motion planner based on the hybrid dynamic model assists in planning time-efficient trajectories for the manipulator. This comprehensive approach underscores the efficacy of integrating physics-based and data-driven models for advanced manipulator control and planning in industrial environments.","sentences":["We report on the development of an implementable physics-data hybrid dynamic model for an articulated manipulator to plan and operate in various scenarios.","Meanwhile, the physics-based and data-driven dynamic models are studied in this research to select the best model for planning.","The physics-based model is constructed using the Lagrangian method, and the loss terms include inertia loss, viscous loss, and friction loss.","As for the data-driven model, three methods are explored, including DNN, LSTM, and XGBoost.","Our modeling results demonstrate that, after comprehensive hyperparameter optimization, the XGBoost architecture outperforms DNN and LSTM in accurately representing manipulator dynamics.","The hybrid model with physics-based and data-driven terms has the best performance among all models based on the RMSE criteria, and it only needs about 24k of training data.","In addition, we developed a virtual force sensor of a manipulator using the observed external torque derived from the dynamic model and designed a motion planner through the physics-data hybrid dynamic model.","The external torque contributes to forces and torque on the end effector, facilitating interaction with the surroundings, while the internal torque governs manipulator motion dynamics and compensates for internal losses.","By estimating external torque via the difference between measured joint torque and internal losses, we implement a sensorless control strategy which is demonstrated through a peg-in-hole task.","Lastly, a learning-based motion planner based on the hybrid dynamic model assists in planning time-efficient trajectories for the manipulator.","This comprehensive approach underscores the efficacy of integrating physics-based and data-driven models for advanced manipulator control and planning in industrial environments."],"url":"http://arxiv.org/abs/2405.04503v1","category":"cs.RO"}
{"created":"2024-05-07 17:04:21","title":"Representation Learning of Daily Movement Data Using Text Encoders","abstract":"Time-series representation learning is a key area of research for remote healthcare monitoring applications. In this work, we focus on a dataset of recordings of in-home activity from people living with Dementia. We design a representation learning method based on converting activity to text strings that can be encoded using a language model fine-tuned to transform data from the same participants within a $30$-day window to similar embeddings in the vector space. This allows for clustering and vector searching over participants and days, and the identification of activity deviations to aid with personalised delivery of care.","sentences":["Time-series representation learning is a key area of research for remote healthcare monitoring applications.","In this work, we focus on a dataset of recordings of in-home activity from people living with Dementia.","We design a representation learning method based on converting activity to text strings that can be encoded using a language model fine-tuned to transform data from the same participants within a $30$-day window to similar embeddings in the vector space.","This allows for clustering and vector searching over participants and days, and the identification of activity deviations to aid with personalised delivery of care."],"url":"http://arxiv.org/abs/2405.04494v1","category":"cs.LG"}
{"created":"2024-05-07 16:56:21","title":"S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling","abstract":"As the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident. Renewable energies have emerged as a viable solution for users, with Photovoltaic energy being a favored choice for small installations due to its reliability and efficiency. Accurate mapping of PV installations is crucial for understanding the extension of its adoption and informing energy policy. To meet this need, we introduce S3Former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid. Solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, Ground Sampling Distance variations and lack of appropriate initialization weights for optimized training. To tackle these complexities, S3Former features a Masked Attention Mask Transformer incorporating a self-supervised learning pretrained backbone. Specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the Transformer architecture to enhance the localization of solar PV installations. We introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of S3Former. We evaluated S3Former using diverse datasets, demonstrate improvement state-of-the-art models.","sentences":["As the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident.","Renewable energies have emerged as a viable solution for users, with Photovoltaic energy being a favored choice for small installations due to its reliability and efficiency.","Accurate mapping of PV installations is crucial for understanding the extension of its adoption and informing energy policy.","To meet this need, we introduce S3Former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid.","Solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, Ground Sampling Distance variations and lack of appropriate initialization weights for optimized training.","To tackle these complexities, S3Former features a Masked Attention Mask Transformer incorporating a self-supervised learning pretrained backbone.","Specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the Transformer architecture to enhance the localization of solar PV installations.","We introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of S3Former.","We evaluated S3Former using diverse datasets, demonstrate improvement state-of-the-art models."],"url":"http://arxiv.org/abs/2405.04489v1","category":"cs.CV"}
{"created":"2024-05-07 16:53:30","title":"Free Will and Falling Cats","abstract":"If we consider a cat to be an isolated mechanical system governed by T-invariant mechanics, then its ability to land on its feet after being released from rest is incomprehensible. It is more appropriate to treat the cat as a creature that can change its shape in order to accomplish a purpose. Within that framework we can construct a useful and informative of the observed motion. One can learn from this example.","sentences":["If we consider a cat to be an isolated mechanical system governed by T-invariant mechanics, then its ability to land on its feet after being released from rest is incomprehensible.","It is more appropriate to treat the cat as a creature that can change its shape in order to accomplish a purpose.","Within that framework we can construct a useful and informative of the observed motion.","One can learn from this example."],"url":"http://arxiv.org/abs/2405.04565v1","category":"physics.class-ph"}
{"created":"2024-05-07 16:13:23","title":"Cosmology from one galaxy in voids?","abstract":"Understanding galaxy properties may be the key to unlocking some of the most intriguing mysteries of modern cosmology. Recent work relied on machine learning to extract cosmological constraints on $\\Omega_\\mathrm{m}$ using only $\\textit{one}$ galaxy. But if this is true, how should we select $\\textit{the}$ galaxy to use for cosmology inference? In this paper, we consider selecting a galaxy that lies in cosmic voids, the underdense regions of the cosmic web, and compare the constraints obtained with the ones obtained when randomly selecting a galaxy in the whole sample. We use the IllustrisTNG galaxy catalog from the CAMELS project and the VIDE void finder to identify galaxies inside voids. We show that void galaxies provide stronger constraints on $\\Omega_\\mathrm{m}$ compared to randomly selected galaxies. This result suggests that the distinctive characteristics of void galaxies may provide a cleaner and more effective environment for extracting cosmological information.","sentences":["Understanding galaxy properties may be the key to unlocking some of the most intriguing mysteries of modern cosmology.","Recent work relied on machine learning to extract cosmological constraints on $\\Omega_\\mathrm{m}$ using only $\\textit{one}$ galaxy.","But if this is true, how should we select $\\textit{the}$ galaxy to use for cosmology inference?","In this paper, we consider selecting a galaxy that lies in cosmic voids, the underdense regions of the cosmic web, and compare the constraints obtained with the ones obtained when randomly selecting a galaxy in the whole sample.","We use the IllustrisTNG galaxy catalog from the CAMELS project and the VIDE void finder to identify galaxies inside voids.","We show that void galaxies provide stronger constraints on $\\Omega_\\mathrm{m}$ compared to randomly selected galaxies.","This result suggests that the distinctive characteristics of void galaxies may provide a cleaner and more effective environment for extracting cosmological information."],"url":"http://arxiv.org/abs/2405.04447v1","category":"astro-ph.CO"}
{"created":"2024-05-07 16:00:32","title":"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention","abstract":"Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency.   In this paper, we propose vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.","sentences":["Efficient use of GPU memory is essential for high throughput LLM inference.","Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation.","Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache.","This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes.","However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory.","This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager.","Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency.   ","In this paper, we propose vAttention for dynamic KV-cache memory management.","In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation.","Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework.","We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels.","vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer."],"url":"http://arxiv.org/abs/2405.04437v1","category":"cs.LG"}
{"created":"2024-05-08 15:31:30","title":"Low-Distortion Clustering in Bounded Growth Graphs","abstract":"The well-known clustering algorithm of Miller, Peng, and Xu (SPAA 2013) is useful for many applications, including low-diameter decomposition and low-energy distributed algorithms. One nice property of their clustering, shown in previous work by Chang, Dani, Hayes, and Pettie (PODC 2020), is that distances in the cluster graph are rescaled versions of distances in the original graph, up to an $O(\\log n)$ distortion factor and rounding issues. Minimizing this distortion factor is important for efficiency in computing the clustering, as well as in other applications.   We prove that there exist graphs for which an $\\Omega((\\log n)^{1/3})$ distortion factor is necessary for any clustering. We also consider a class of nice graphs which we call uniformly bounded independence graphs. These include, for example, paths, lattice graphs, and \"dense\" unit disk graphs. For these graphs, we prove that clusterings of distortion $O(1)$ always exist, and moreover, we give new efficient distributed algorithms to construct them. This clustering is based on Voronoi cells centered at the vertices of a maximal independent set in a suitable power graph.   Applications include low-energy simulation of distributed algorithms in the LOCAL, CONGEST, and RADIO-CONGEST models and efficient approximate solutions to distributed combinatorial optimization problems. We also investigate related lower bounds.","sentences":["The well-known clustering algorithm of Miller, Peng, and Xu (SPAA 2013) is useful for many applications, including low-diameter decomposition and low-energy distributed algorithms.","One nice property of their clustering, shown in previous work by Chang, Dani, Hayes, and Pettie (PODC 2020), is that distances in the cluster graph are rescaled versions of distances in the original graph, up to an $O(\\log n)$ distortion factor and rounding issues.","Minimizing this distortion factor is important for efficiency in computing the clustering, as well as in other applications.   ","We prove that there exist graphs for which an $\\Omega((\\log n)^{1/3})$ distortion factor is necessary for any clustering.","We also consider a class of nice graphs which we call uniformly bounded independence graphs.","These include, for example, paths, lattice graphs, and \"dense\" unit disk graphs.","For these graphs, we prove that clusterings of distortion $O(1)$ always exist, and moreover, we give new efficient distributed algorithms to construct them.","This clustering is based on Voronoi cells centered at the vertices of a maximal independent set in a suitable power graph.   ","Applications include low-energy simulation of distributed algorithms in the LOCAL, CONGEST, and RADIO-CONGEST models and efficient approximate solutions to distributed combinatorial optimization problems.","We also investigate related lower bounds."],"url":"http://arxiv.org/abs/2405.05132v1","category":"cs.DC"}
{"created":"2024-05-08 14:52:35","title":"Energy stable gradient flow schemes for shape and topology optimization in Navier-Stokes flows","abstract":"We study topology optimization governed by the incompressible Navier-Stokes flows using a phase field model. Novel stabilized semi-implicit schemes for the gradient flows of Allen-Cahn and Cahn-Hilliard types are proposed for solving the resulting optimal control problem. Unconditional energy stability is shown for the gradient flow schemes in continuous and discrete spaces. Numerical experiments of computational fluid dynamics in 2d and 3d show the effectiveness and robustness of the optimization algorithms proposed.","sentences":["We study topology optimization governed by the incompressible Navier-Stokes flows using a phase field model.","Novel stabilized semi-implicit schemes for the gradient flows of Allen-Cahn and Cahn-Hilliard types are proposed for solving the resulting optimal control problem.","Unconditional energy stability is shown for the gradient flow schemes in continuous and discrete spaces.","Numerical experiments of computational fluid dynamics in 2d and 3d show the effectiveness and robustness of the optimization algorithms proposed."],"url":"http://arxiv.org/abs/2405.05098v1","category":"math.NA"}
{"created":"2024-05-08 14:49:01","title":"Rapid Co-design of Task-Specialized Whegged Robots for Ad-Hoc Needs","abstract":"In this work, we investigate the use of co-design methods to iterate upon robot designs in the field, performing time sensitive, ad-hoc tasks. Our method optimizes the morphology and wheg trajectory for a MiniRHex robot, producing 3D printable structures and leg trajectory parameters. Tested in four terrains, we show that robots optimized in simulation exhibit strong sim-to-real transfer and are nearly twice as efficient as the nominal platform when tested in hardware.","sentences":["In this work, we investigate the use of co-design methods to iterate upon robot designs in the field, performing time sensitive, ad-hoc tasks.","Our method optimizes the morphology and wheg trajectory for a MiniRHex robot, producing 3D printable structures and leg trajectory parameters.","Tested in four terrains, we show that robots optimized in simulation exhibit strong sim-to-real transfer and are nearly twice as efficient as the nominal platform when tested in hardware."],"url":"http://arxiv.org/abs/2405.05096v1","category":"cs.RO"}
{"created":"2024-05-08 13:08:39","title":"Locally-Measured R\u00e9nyi Divergences","abstract":"We propose an extension of the classical R\\'enyi divergences to quantum states through an optimization over probability distributions induced by restricted sets of measurements. In particular, we define the notion of locally-measured R\\'enyi divergences, where the set of allowed measurements originates from variants of locality constraints between (distant) parties $A$ and $B$. We then derive variational bounds on the locally-measured R\\'enyi divergences and systematically discuss when these bounds become exact characterizations. As an application, we evaluate the locally-measured R\\'enyi divergences on variants of highly symmetric data-hiding states, showcasing the reduced distinguishing power of locality-constrained measurements. For $n$-fold tensor powers, we further employ our variational formulae to derive corresponding additivity results, which gives the locally-measured R\\'enyi divergences operational meaning as optimal rate exponents in asymptotic locally-measured hypothesis testing.","sentences":["We propose an extension of the classical R\\'enyi divergences to quantum states through an optimization over probability distributions induced by restricted sets of measurements.","In particular, we define the notion of locally-measured R\\'enyi divergences, where the set of allowed measurements originates from variants of locality constraints between (distant) parties $A$ and $B$. We then derive variational bounds on the locally-measured R\\'enyi divergences and systematically discuss when these bounds become exact characterizations.","As an application, we evaluate the locally-measured R\\'enyi divergences on variants of highly symmetric data-hiding states, showcasing the reduced distinguishing power of locality-constrained measurements.","For $n$-fold tensor powers, we further employ our variational formulae to derive corresponding additivity results, which gives the locally-measured R\\'enyi divergences operational meaning as optimal rate exponents in asymptotic locally-measured hypothesis testing."],"url":"http://arxiv.org/abs/2405.05037v1","category":"quant-ph"}
{"created":"2024-05-08 11:51:30","title":"The Riemannian geometry of Sinkhorn divergences","abstract":"We propose a new metric between probability measures on a compact metric space that mirrors the Riemannian manifold-like structure of quadratic optimal transport but includes entropic regularization. Its metric tensor is given by the Hessian of the Sinkhorn divergence, a debiased variant of entropic optimal transport. We precisely identify the tangent space it induces, which turns out to be related to a Reproducing Kernel Hilbert Space (RKHS). As usual in Riemannian geometry, the distance is built by looking for shortest paths. We prove that our distance is geodesic, metrizes the weak-star topology, and is equivalent to a RKHS norm. Still it retains the geometric flavor of optimal transport: as a paradigmatic example, translations are geodesics for the quadratic cost on $\\mathbb{R}^d$. We also show two negative results on the Sinkhorn divergence that may be of independent interest: that it is not jointly convex, and that its square root is not a distance because it fails to satisfy the triangle inequality.","sentences":["We propose a new metric between probability measures on a compact metric space that mirrors the Riemannian manifold-like structure of quadratic optimal transport but includes entropic regularization.","Its metric tensor is given by the Hessian of the Sinkhorn divergence, a debiased variant of entropic optimal transport.","We precisely identify the tangent space it induces, which turns out to be related to a Reproducing Kernel Hilbert Space (RKHS).","As usual in Riemannian geometry, the distance is built by looking for shortest paths.","We prove that our distance is geodesic, metrizes the weak-star topology, and is equivalent to a RKHS norm.","Still it retains the geometric flavor of optimal transport: as a paradigmatic example, translations are geodesics for the quadratic cost on $\\mathbb{R}^d$. We also show two negative results on the Sinkhorn divergence that may be of independent interest: that it is not jointly convex, and that its square root is not a distance because it fails to satisfy the triangle inequality."],"url":"http://arxiv.org/abs/2405.04987v1","category":"math.OC"}
{"created":"2024-05-08 10:46:22","title":"Evolving R2 to R2+: Optimal, Delayed Line-of-sight Vector-based Path Planning","abstract":"A vector-based any-angle path planner, R2, is evolved in to R2+ in this paper. By delaying line-of-sight, R2 and R2+ search times are largely unaffected by the distance between the start and goal points, but are exponential in the worst case with respect to the number of collisions during searches. To improve search times, additional discarding conditions in the overlap rule are introduced in R2+. In addition, R2+ resolves interminable chases in R2 by replacing ad hoc points with limited occupied-sector traces from target nodes, and simplifies R2 by employing new abstract structures and ensuring target progression during a trace. R2+ preserves the speed of R2 when paths are expected to detour around few obstacles, and searches significantly faster than R2 in maps with many disjoint obstacles.","sentences":["A vector-based any-angle path planner, R2, is evolved in to R2+ in this paper.","By delaying line-of-sight, R2 and R2+ search times are largely unaffected by the distance between the start and goal points, but are exponential in the worst case with respect to the number of collisions during searches.","To improve search times, additional discarding conditions in the overlap rule are introduced in R2+.","In addition, R2+ resolves interminable chases in R2 by replacing ad hoc points with limited occupied-sector traces from target nodes, and simplifies R2 by employing new abstract structures and ensuring target progression during a trace.","R2+ preserves the speed of R2 when paths are expected to detour around few obstacles, and searches significantly faster than R2 in maps with many disjoint obstacles."],"url":"http://arxiv.org/abs/2405.04952v1","category":"cs.RO"}
{"created":"2024-05-08 05:42:26","title":"Quantum-Edge Cloud Computing: A Future Paradigm for IoT Applications","abstract":"The Internet of Things (IoT) is expanding rapidly, which has created a need for sophisticated computational frameworks that can handle the data and security requirements inherent in modern IoT applications. However, traditional cloud computing frameworks have struggled with latency, scalability, and security vulnerabilities. Quantum-Edge Cloud Computing (QECC) is a new paradigm that effectively addresses these challenges by combining the computational power of quantum computing, the low-latency benefits of edge computing, and the scalable resources of cloud computing. This study has been conducted based on a published literature review, performance improvements, and metrics data from Bangladesh on smart city infrastructure, healthcare monitoring, and the industrial IoT sector. We have discussed the integration of quantum cryptography to enhance data integrity, the role of edge computing in reducing response times, and how cloud computing's resource abundance can support large IoT networks. We examine case studies, such as the use of quantum sensors in self-driving vehicles, to illustrate the real-world impact of QECC. Furthermore, the paper identifies future research directions, including developing quantum-resistant encryption and optimizing quantum algorithms for edge computing. The convergence of these technologies in QECC promises to overcome the existing limitations of IoT frameworks and set a new standard for the future of IoT applications.","sentences":["The Internet of Things (IoT) is expanding rapidly, which has created a need for sophisticated computational frameworks that can handle the data and security requirements inherent in modern IoT applications.","However, traditional cloud computing frameworks have struggled with latency, scalability, and security vulnerabilities.","Quantum-Edge Cloud Computing (QECC) is a new paradigm that effectively addresses these challenges by combining the computational power of quantum computing, the low-latency benefits of edge computing, and the scalable resources of cloud computing.","This study has been conducted based on a published literature review, performance improvements, and metrics data from Bangladesh on smart city infrastructure, healthcare monitoring, and the industrial IoT sector.","We have discussed the integration of quantum cryptography to enhance data integrity, the role of edge computing in reducing response times, and how cloud computing's resource abundance can support large IoT networks.","We examine case studies, such as the use of quantum sensors in self-driving vehicles, to illustrate the real-world impact of QECC.","Furthermore, the paper identifies future research directions, including developing quantum-resistant encryption and optimizing quantum algorithms for edge computing.","The convergence of these technologies in QECC promises to overcome the existing limitations of IoT frameworks and set a new standard for the future of IoT applications."],"url":"http://arxiv.org/abs/2405.04824v1","category":"cs.CR"}
{"created":"2024-05-08 02:15:23","title":"High sensitivity measurement of ULF, VLF and LF fields with Rydberg-atom sensor","abstract":"Fields with frequencies below megahertz are challenging for Rydberg-atom-based measurements, due to the low-frequency electric field screening effect that is caused by the alkali-metal atoms adsorbed on the inner surface of the container. In this paper, we investigate on electric fields measurements in the ULF, VLF and LF bands in a Cs vapor cell with built-in parallel electrodes. With optimization of the applied DC field, we achieve high-sensitive detection of the electric field at frequencies of 1kHz, 10kHz and 100kHz based on Rydberg-atom sensor, with the minimum electric field strength down to 18.0{\\mu}V/cm, 6.9{\\mu}V/cm and 3.0{\\mu}V/cm, respectively. The corresponding sensitivity is 5.7 {\\mu}V/cm/{\\sqrt{Hz}}, 2.2{\\mu}V/cm/{\\sqrt{Hz}} and 0.95{\\mu}V/cm/{\\sqrt{Hz}} for ULF, VLF and LF fields, which is better than 1-cm dipole antenna. Besides, the linear dynamic range of Rydberg-atom sensor is over 50 dB. This work presents the potential to enable more applications that utilize atomic sensing technology in ULF, VLF and LF fields.","sentences":["Fields with frequencies below megahertz are challenging for Rydberg-atom-based measurements, due to the low-frequency electric field screening effect that is caused by the alkali-metal atoms adsorbed on the inner surface of the container.","In this paper, we investigate on electric fields measurements in the ULF, VLF and LF bands in a Cs vapor cell with built-in parallel electrodes.","With optimization of the applied DC field, we achieve high-sensitive detection of the electric field at frequencies of 1kHz, 10kHz and 100kHz based on Rydberg-atom sensor, with the minimum electric field strength down to 18.0{\\mu}V/cm, 6.9{\\mu}V/cm and 3.0{\\mu}V/cm, respectively.","The corresponding sensitivity is 5.7 {\\mu}V/cm/{\\sqrt{Hz}}, 2.2{\\mu}V/cm/{\\sqrt{Hz}} and 0.95{\\mu}V/cm/{\\sqrt{Hz}} for ULF, VLF and LF fields, which is better than 1-cm dipole antenna.","Besides, the linear dynamic range of Rydberg-atom sensor is over 50 dB. This work presents the potential to enable more applications that utilize atomic sensing technology in ULF, VLF and LF fields."],"url":"http://arxiv.org/abs/2405.04761v1","category":"physics.atom-ph"}
{"created":"2024-05-07 22:03:45","title":"Signature of T$_\\textrm{c}$ above 111 K in Li-doped (Bi,Pb)-2223 superconductors: synergistic nature of hole concentration, coherence length and Josephson interlayer coupling","abstract":"Understanding the bottleneck to drive higher critical transition temperature $T_\\textrm{c}$ plays a pivotal role in the underlying study of superconductors. We systematically investigate the effect of Li$^+$ substitution for Cu$^{2+}$ cations on the $T_\\textrm{c}$, hole concentration, coherence length and interlayer coupling, and microstructure in Li-doped Bi$_{1.6}$Pb$_{0.4}$Sr$_2$Ca$_2$Cu$_3$O$_{10 + \\delta}$ or (Bi,Pb)-2223 compound. Remarkably, we demonstrate by utilizing a long-time sintering accompanied by a multiple recurrent intermediate stages of calcining and pressing within our renovated solid-state reaction method, the optimal Li-doped (Bi,Pb)-2223 samples achieve the well-enhanced $T_\\textrm{c}$ of 111--113.8 K compared with the standard value of 110 K. We evince the superconducting mechanism that the substitution of Li$^{+}$ for Cu$^{2+}$ ions on the CuO$_2$ layers causes augmenting the hole concentrations and promotes the correlation between the overdoped outer and the underdoped inner CuO$_2$ planes, and thus effects improve $T_\\textrm{c}$. Following a universal quadratic relation between $T_\\textrm{c}$ and hole concentration, a new higher optimal hole concentration is provided. Additionally, by analyzing the Aslamazov-Larkin and Lawrence-Doniach theories on the reliable excess conductivity data near the critical temperature, we observe the strong effect of Li-doping on the system. The coherence length steadily increases versus the Li-doped content, while the Josephson interlayer coupling strength between the CuO$_2$ layers almost remains a constant for the whole series of Li-doping. Our findings establish an insightful roadmap to improve the critical temperature and intrinsic superconducting properties in the Bi-2223 compounds through the doping process.","sentences":["Understanding the bottleneck to drive higher critical transition temperature $T_\\textrm{c}$ plays a pivotal role in the underlying study of superconductors.","We systematically investigate the effect of Li$^+$ substitution for Cu$^{2+}$ cations on the $T_\\textrm{c}$, hole concentration, coherence length and interlayer coupling, and microstructure in Li-doped Bi$_{1.6}$Pb$_{0.4}$Sr$_2$Ca$_2$Cu$_3$O$_{10 + \\delta}$ or (Bi,Pb)-2223 compound.","Remarkably, we demonstrate by utilizing a long-time sintering accompanied by a multiple recurrent intermediate stages of calcining and pressing within our renovated solid-state reaction method, the optimal Li-doped (Bi,Pb)-2223 samples achieve the well-enhanced $T_\\textrm{c}$ of 111--113.8 K compared with the standard value of 110 K. We evince the superconducting mechanism that the substitution of Li$^{+}$ for Cu$^{2+}$ ions on the CuO$_2$ layers causes augmenting the hole concentrations and promotes the correlation between the overdoped outer and the underdoped inner CuO$_2$ planes, and thus effects improve $T_\\textrm{c}$. Following a universal quadratic relation between $T_\\textrm{c}$ and hole concentration, a new higher optimal hole concentration is provided.","Additionally, by analyzing the Aslamazov-Larkin and Lawrence-Doniach theories on the reliable excess conductivity data near the critical temperature, we observe the strong effect of Li-doping on the system.","The coherence length steadily increases versus the Li-doped content, while the Josephson interlayer coupling strength between the CuO$_2$ layers almost remains a constant for the whole series of Li-doping.","Our findings establish an insightful roadmap to improve the critical temperature and intrinsic superconducting properties in the Bi-2223 compounds through the doping process."],"url":"http://arxiv.org/abs/2405.04689v1","category":"cond-mat.supr-con"}
{"created":"2024-05-07 22:02:12","title":"On existence of solutions to non-convex minimization problems","abstract":"We provide new sufficient conditions for the finiteness of the optimal value and existence of solutions to a general problem of minimizing a proper closed function over a nonempty closed set. The conditions require an asymptotically bounded decay of a function, a relaxation of p-supercoercivity, and a certain relation for the asymptotic cone of the constraint set and the asymptotic function of the objective function. Our analysis combines these conditions with a regularization technique. We refine the notion of retractive directions of a set, extend its definition to functions, and establish some basic relations for such directions for both sets and functions. Using these tools, we provide existence of solutions results that generalize many of the results in the literature for both non-convex and convex problems.","sentences":["We provide new sufficient conditions for the finiteness of the optimal value and existence of solutions to a general problem of minimizing a proper closed function over a nonempty closed set.","The conditions require an asymptotically bounded decay of a function, a relaxation of p-supercoercivity, and a certain relation for the asymptotic cone of the constraint set and the asymptotic function of the objective function.","Our analysis combines these conditions with a regularization technique.","We refine the notion of retractive directions of a set, extend its definition to functions, and establish some basic relations for such directions for both sets and functions.","Using these tools, we provide existence of solutions results that generalize many of the results in the literature for both non-convex and convex problems."],"url":"http://arxiv.org/abs/2405.04688v1","category":"math.OC"}
{"created":"2024-05-07 19:04:22","title":"Underground Freight Transportation for Package Delivery in Urban Environments","abstract":"The use of underground freight transportation (UFT) is gaining attention because of its ability to quickly move freight to locations in urban areas while reducing road traffic and the need for delivery drivers. Since packages are transported through the tunnels by electric motors, the use of tunnels is also environmentally friendly. Unlike other UFT projects, we examine the use of tunnels to transport individual orders, motivated by the last mile delivery of goods from e-commerce providers. The use of UFT for last mile delivery requires more complex network planning than for direct lines that have previously been considered for networks connecting large cities. We introduce a new network design problem based on this delivery model and transform the problem into a fixed charge multicommodity flow problem with additional constraints. We show that this problem, the nd-UFT, is NP-hard, and provide an exact solution method for solving large-scale instances. Our solution approach exploits the combinatorial sub-structures of the problem in a cutting planes fashion, significantly reducing the time to find optimal solutions on most instances compared to a MIP. We provide computational results for real urban environments to build a set of insights into the structure of such networks and evaluate the benefits of such systems. We see that a budget of only 45 miles of tunnel can remove 42% of packages off the roads in Chicago and 32% in New York City. We estimate the fixed and operational costs for implementing UFT systems and break them down into a per package cost. Our estimates indicate over a 40% savings from using a UFT over traditional delivery models. This indicates that UFT systems for last mile delivery are a promising area for future research.","sentences":["The use of underground freight transportation (UFT) is gaining attention because of its ability to quickly move freight to locations in urban areas while reducing road traffic and the need for delivery drivers.","Since packages are transported through the tunnels by electric motors, the use of tunnels is also environmentally friendly.","Unlike other UFT projects, we examine the use of tunnels to transport individual orders, motivated by the last mile delivery of goods from e-commerce providers.","The use of UFT for last mile delivery requires more complex network planning than for direct lines that have previously been considered for networks connecting large cities.","We introduce a new network design problem based on this delivery model and transform the problem into a fixed charge multicommodity flow problem with additional constraints.","We show that this problem, the nd-UFT, is NP-hard, and provide an exact solution method for solving large-scale instances.","Our solution approach exploits the combinatorial sub-structures of the problem in a cutting planes fashion, significantly reducing the time to find optimal solutions on most instances compared to a MIP.","We provide computational results for real urban environments to build a set of insights into the structure of such networks and evaluate the benefits of such systems.","We see that a budget of only 45 miles of tunnel can remove 42% of packages off the roads in Chicago and 32% in New York City.","We estimate the fixed and operational costs for implementing UFT systems and break them down into a per package cost.","Our estimates indicate over a 40% savings from using a UFT over traditional delivery models.","This indicates that UFT systems for last mile delivery are a promising area for future research."],"url":"http://arxiv.org/abs/2405.04618v1","category":"math.OC"}
{"created":"2024-05-07 18:43:14","title":"Arrival Times Versus Detection Times","abstract":"How to compute the probability distribution of a detection time, i.e., of the time which a detector registers as the arrival time of a quantum particle, is a long-debated problem. In this regard, Bohmian mechanics provides in a straightforward way the distribution of the time at which the particle actually does arrive at a given surface in 3-space in the absence of detectors. However, as we discuss here, since the presence of detectors can change the evolution of the wave function and thus the particle trajectories, it cannot be taken for granted that the arrival time of the Bohmian trajectories in the absence of detectors agrees with the one in the presence of detectors, and even less with the detection time. In particular, we explain why certain distributions that Das and D\\\"urr [arXiv:1802.07141] presented as the distribution of the detection time in a case with spin, based on assuming that all three times mentioned coincide, is actually not what Bohmian mechanics predicts.","sentences":["How to compute the probability distribution of a detection time, i.e., of the time which a detector registers as the arrival time of a quantum particle, is a long-debated problem.","In this regard, Bohmian mechanics provides in a straightforward way the distribution of the time at which the particle actually does arrive at a given surface in 3-space in the absence of detectors.","However, as we discuss here, since the presence of detectors can change the evolution of the wave function and thus the particle trajectories, it cannot be taken for granted that the arrival time of the Bohmian trajectories in the absence of detectors agrees with the one in the presence of detectors, and even less with the detection time.","In particular, we explain why certain distributions that Das and D\\\"urr [arXiv:1802.07141] presented as the distribution of the detection time in a case with spin, based on assuming that all three times mentioned coincide, is actually not what Bohmian mechanics predicts."],"url":"http://arxiv.org/abs/2405.04607v1","category":"quant-ph"}
{"created":"2024-05-07 18:22:28","title":"Contextual API Completion for Unseen Repositories Using LLMs","abstract":"Large language models have made substantial progress in addressing diverse code-related tasks. However, their adoption is hindered by inconsistencies in generating output due to the lack of real-world, domain-specific information, such as for intra-repository API calls for unseen software projects. We introduce a novel technique to mitigate hallucinations by leveraging global and local contextual information within a code repository for API completion tasks. Our approach is tailored to refine code completion tasks, with a focus on optimizing local API completions. We examine relevant import statements during API completion to derive insights into local APIs, drawing from their method signatures. For API token completion, we analyze the inline variables and correlate them with the appropriate imported modules, thereby allowing our approach to rank the most contextually relevant suggestions from the available local APIs. Further, for conversational API completion, we gather APIs that are most relevant to the developer query with a retrieval-based search across the project. We employ our tool, LANCE, within the framework of our proposed benchmark, APIEval, encompassing two different programming languages. Our evaluation yields an average accuracy of 82.6% for API token completion and 76.9% for conversational API completion tasks. On average, LANCE surpasses Copilot by 143% and 142% for API token completion and conversational API completion, respectively. The implications of our findings are substantial for developers, suggesting that our lightweight context analysis can be applied to multilingual environments without language-specific training or fine-tuning, allowing for efficient implementation with minimal examples and effort.","sentences":["Large language models have made substantial progress in addressing diverse code-related tasks.","However, their adoption is hindered by inconsistencies in generating output due to the lack of real-world, domain-specific information, such as for intra-repository API calls for unseen software projects.","We introduce a novel technique to mitigate hallucinations by leveraging global and local contextual information within a code repository for API completion tasks.","Our approach is tailored to refine code completion tasks, with a focus on optimizing local API completions.","We examine relevant import statements during API completion to derive insights into local APIs, drawing from their method signatures.","For API token completion, we analyze the inline variables and correlate them with the appropriate imported modules, thereby allowing our approach to rank the most contextually relevant suggestions from the available local APIs.","Further, for conversational API completion, we gather APIs that are most relevant to the developer query with a retrieval-based search across the project.","We employ our tool, LANCE, within the framework of our proposed benchmark, APIEval, encompassing two different programming languages.","Our evaluation yields an average accuracy of 82.6% for API token completion and 76.9% for conversational API completion tasks.","On average, LANCE surpasses Copilot by 143% and 142% for API token completion and conversational API completion, respectively.","The implications of our findings are substantial for developers, suggesting that our lightweight context analysis can be applied to multilingual environments without language-specific training or fine-tuning, allowing for efficient implementation with minimal examples and effort."],"url":"http://arxiv.org/abs/2405.04600v1","category":"cs.SE"}
{"created":"2024-05-07 17:57:15","title":"Comparing Ways of Obtaining Candidate Orderings from Approval Ballots","abstract":"To understand and summarize approval preferences and other binary evaluation data, it is useful to order the items on an axis which explains the data. In a political election using approval voting, this could be an ideological left-right axis such that each voter approves adjacent candidates, an analogue of single-peakedness. In a perfect axis, every approval set would be an interval, which is usually not possible, and so we need to choose an axis that gets closest to this ideal. The literature has developed algorithms for optimizing several objective functions (e.g., minimize the number of added approvals needed to get a perfect axis), but provides little help with choosing among different objectives. In this paper, we take a social choice approach and compare 5 different axis selection rules axiomatically, by studying the properties they satisfy. We establish some impossibility theorems, and characterize (within the class of scoring rules) the rule that chooses the axes that maximize the number of votes that form intervals, using the axioms of ballot monotonicity and resistance to cloning. Finally, we study the behavior of the rules on data from French election surveys, on the votes of justices of the US Supreme Court, and on synthetic data.","sentences":["To understand and summarize approval preferences and other binary evaluation data, it is useful to order the items on an axis which explains the data.","In a political election using approval voting, this could be an ideological left-right axis such that each voter approves adjacent candidates, an analogue of single-peakedness.","In a perfect axis, every approval set would be an interval, which is usually not possible, and so we need to choose an axis that gets closest to this ideal.","The literature has developed algorithms for optimizing several objective functions (e.g., minimize the number of added approvals needed to get a perfect axis), but provides little help with choosing among different objectives.","In this paper, we take a social choice approach and compare 5 different axis selection rules axiomatically, by studying the properties they satisfy.","We establish some impossibility theorems, and characterize (within the class of scoring rules) the rule that chooses the axes that maximize the number of votes that form intervals, using the axioms of ballot monotonicity and resistance to cloning.","Finally, we study the behavior of the rules on data from French election surveys, on the votes of justices of the US Supreme Court, and on synthetic data."],"url":"http://arxiv.org/abs/2405.04525v1","category":"cs.GT"}
{"created":"2024-05-07 17:45:53","title":"Scalable Circuit Cutting and Scheduling in a Resource-constrained and Distributed Quantum System","abstract":"Despite quantum computing's rapid development, current systems remain limited in practical applications due to their limited qubit count and quality. Various technologies, such as superconducting, trapped ions, and neutral atom quantum computing technologies are progressing towards a fault tolerant era, however they all face a diverse set of challenges in scalability and control. Recent efforts have focused on multi-node quantum systems that connect multiple smaller quantum devices to execute larger circuits. Future demonstrations hope to use quantum channels to couple systems, however current demonstrations can leverage classical communication with circuit cutting techniques. This involves cutting large circuits into smaller subcircuits and reconstructing them post-execution. However, existing cutting methods are hindered by lengthy search times as the number of qubits and gates increases. Additionally, they often fail to effectively utilize the resources of various worker configurations in a multi-node system. To address these challenges, we introduce FitCut, a novel approach that transforms quantum circuits into weighted graphs and utilizes a community-based, bottom-up approach to cut circuits according to resource constraints, e.g., qubit counts, on each worker. FitCut also includes a scheduling algorithm that optimizes resource utilization across workers. Implemented with Qiskit and evaluated extensively, FitCut significantly outperforms the Qiskit Circuit Knitting Toolbox, reducing time costs by factors ranging from 3 to 2000 and improving resource utilization rates by up to 3.88 times on the worker side, achieving a system-wide improvement of 2.86 times.","sentences":["Despite quantum computing's rapid development, current systems remain limited in practical applications due to their limited qubit count and quality.","Various technologies, such as superconducting, trapped ions, and neutral atom quantum computing technologies are progressing towards a fault tolerant era, however they all face a diverse set of challenges in scalability and control.","Recent efforts have focused on multi-node quantum systems that connect multiple smaller quantum devices to execute larger circuits.","Future demonstrations hope to use quantum channels to couple systems, however current demonstrations can leverage classical communication with circuit cutting techniques.","This involves cutting large circuits into smaller subcircuits and reconstructing them post-execution.","However, existing cutting methods are hindered by lengthy search times as the number of qubits and gates increases.","Additionally, they often fail to effectively utilize the resources of various worker configurations in a multi-node system.","To address these challenges, we introduce FitCut, a novel approach that transforms quantum circuits into weighted graphs and utilizes a community-based, bottom-up approach to cut circuits according to resource constraints, e.g., qubit counts, on each worker.","FitCut also includes a scheduling algorithm that optimizes resource utilization across workers.","Implemented with Qiskit and evaluated extensively, FitCut significantly outperforms the Qiskit Circuit Knitting Toolbox, reducing time costs by factors ranging from 3 to 2000 and improving resource utilization rates by up to 3.88 times on the worker side, achieving a system-wide improvement of 2.86 times."],"url":"http://arxiv.org/abs/2405.04514v1","category":"quant-ph"}
{"created":"2024-05-07 17:35:14","title":"$\\mathrm{D}$ and $\\mathrm{D_s}$ decay constants in $N_{\\rm f}=2+1$ QCD with Wilson fermions","abstract":"We present results for the leptonic decay constants of the D and D$_{\\rm s}$ mesons from $N_{\\rm f}=2+1$ lattice QCD. We employ a set of 49 high statistics gauge ensembles generated by the Coordinated Lattice Simulations (CLS) effort utilising non-perturbatively improved Wilson fermions and the tree-level Symanzik improved gauge action at six values of the lattice spacing in the range $a = 0.098\\,$fm down to $a = 0.039\\,$fm, with pion masses varying from around $420\\,$MeV down to below the physical point. The ensembles lie on three trajectories in the quark mass plane, two trajectories intersecting close to the physical quark mass point and the third one approaching the SU(3) chiral limit, enabling tight control of the light and strange quark mass dependence. We obtain $f_{\\mathrm{D_s}}=246.8(1.3)\\,$MeV, $f_\\mathrm{D}=208.4(1.5)\\,$MeV and $f_{\\mathrm{D_s}}/f_\\mathrm{D}=1.1842(36)$, where the precision of our results is mostly limited by the determination of the scale.","sentences":["We present results for the leptonic decay constants of the D and D$_{\\rm s}$ mesons from $N_{\\rm f}=2+1$ lattice QCD.","We employ a set of 49 high statistics gauge ensembles generated by the Coordinated Lattice Simulations (CLS) effort utilising non-perturbatively improved Wilson fermions and the tree-level Symanzik improved gauge action at six values of the lattice spacing in the range $a = 0.098\\,$fm down to $a = 0.039\\,$fm, with pion masses varying from around $420\\,$MeV down to below the physical point.","The ensembles lie on three trajectories in the quark mass plane, two trajectories intersecting close to the physical quark mass point and the third one approaching the SU(3) chiral limit, enabling tight control of the light and strange quark mass dependence.","We obtain $f_{\\mathrm{D_s}}=246.8(1.3)\\,$MeV, $f_\\mathrm{D}=208.4(1.5)\\,$MeV and $f_{\\mathrm{D_s}}/f_\\mathrm{D}=1.1842(36)$, where the precision of our results is mostly limited by the determination of the scale."],"url":"http://arxiv.org/abs/2405.04506v1","category":"hep-lat"}
{"created":"2024-05-07 17:14:42","title":"Generative Planning with Fast Collision Checks for High Speed Navigation","abstract":"Reasoning about large numbers of diverse plans to achieve high speed navigation in cluttered environments remains a challenge for robotic systems even in the case of perfect perceptual information. Often, this is tackled by methods that iteratively optimize around a prior seeded trajectory and consequently restrict to local optima. We present a novel planning method using normalizing flows (NFs) to encode expert-styled motion primitives. We also present an accelerated collision checking framework that enables rejecting samples from the prior distribution before running them through the NF model for rapid sampling of collision-free trajectories. The choice of an NF as the generator permits a flexible way to encode diverse multi-modal behavior distributions while maintaining a smooth relation to the input space which allows approximating collision checks on NF inputs rather than outputs. We show comparable performance to model predictive path integral control in random cluttered environments and improved exit rates in a cul-de-sac environment. We conclude by discussing our plans for future work to improve both safety and performance of our controller.","sentences":["Reasoning about large numbers of diverse plans to achieve high speed navigation in cluttered environments remains a challenge for robotic systems even in the case of perfect perceptual information.","Often, this is tackled by methods that iteratively optimize around a prior seeded trajectory and consequently restrict to local optima.","We present a novel planning method using normalizing flows (NFs) to encode expert-styled motion primitives.","We also present an accelerated collision checking framework that enables rejecting samples from the prior distribution before running them through the NF model for rapid sampling of collision-free trajectories.","The choice of an NF as the generator permits a flexible way to encode diverse multi-modal behavior distributions while maintaining a smooth relation to the input space which allows approximating collision checks on NF inputs rather than outputs.","We show comparable performance to model predictive path integral control in random cluttered environments and improved exit rates in a cul-de-sac environment.","We conclude by discussing our plans for future work to improve both safety and performance of our controller."],"url":"http://arxiv.org/abs/2405.04498v1","category":"cs.RO"}
{"created":"2024-05-07 16:54:16","title":"Quantum Rabin oblivious transfer using two pure states","abstract":"Oblivious transfer between two untrusting parties is an important primitive in cryptography. There are different variants of oblivious transfer. In Rabin oblivious transfer, the sender Alice holds a bit, and the receiver Bob either obtains the bit, or obtains no information with probability $p_?$. Alice should not know whether or not Bob obtained the bit. We examine a quantum Rabin oblivious transfer protocol that uses two pure states. Investigating different cheating scenarios for the sender and for the receiver, we determine optimal cheating probabilities in each case. Comparing the quantum Rabin oblivious transfer protocol to classical Rabin oblivious transfer protocols, we show that the quantum protocol outperforms classical protocols which do not use a third party, for some values of $p_?$.","sentences":["Oblivious transfer between two untrusting parties is an important primitive in cryptography.","There are different variants of oblivious transfer.","In Rabin oblivious transfer, the sender Alice holds a bit, and the receiver Bob either obtains the bit, or obtains no information with probability $p_?$. Alice should not know whether or not Bob obtained the bit.","We examine a quantum Rabin oblivious transfer protocol that uses two pure states.","Investigating different cheating scenarios for the sender and for the receiver, we determine optimal cheating probabilities in each case.","Comparing the quantum Rabin oblivious transfer protocol to classical Rabin oblivious transfer protocols, we show that the quantum protocol outperforms classical protocols which do not use a third party, for some values of $p_?$."],"url":"http://arxiv.org/abs/2405.04486v1","category":"quant-ph"}
{"created":"2024-05-07 16:38:02","title":"Nonlinear Landau damping and wave operators in sharp Gevrey spaces","abstract":"We prove nonlinear Landau damping in optimal weighted Gevrey-3 spaces for solutions of the confined Vlasov-Poisson system on $\\T^d\\times\\R^d$ which are small perturbations of homogeneous Penrose-stable equilibria.   We also prove the existence of nonlinear scattering operators associated to the confined Vlasov-Poisson evolution, as well as suitable injectivity properties and Lipschitz estimates (also in weighted Gevrey-3 spaces) on these operators.   Our results give definitive answers to two well-known open problems in the field, both of them stated in the recent review of Bedrossian [4, Section 6].","sentences":["We prove nonlinear Landau damping in optimal weighted Gevrey-3 spaces for solutions of the confined Vlasov-Poisson system on $\\T^d\\times\\R^d$ which are small perturbations of homogeneous Penrose-stable equilibria.   ","We also prove the existence of nonlinear scattering operators associated to the confined Vlasov-Poisson evolution, as well as suitable injectivity properties and Lipschitz estimates (also in weighted Gevrey-3 spaces) on these operators.   ","Our results give definitive answers to two well-known open problems in the field, both of them stated in the recent review of Bedrossian [4, Section 6]."],"url":"http://arxiv.org/abs/2405.04473v1","category":"math.AP"}
{"created":"2024-05-07 16:26:24","title":"A Constructive Winning Maker Strategy in the Maker-Breaker $C_4$-Game","abstract":"Maker-Breaker subgraph games are among the most famous combinatorial games. For given $n,q \\in \\mathbb{N}$ and a subgraph $C$ of the complete graph $K_n$, the two players, called Maker and Breaker, alternately claim edges of $K_n$. In each round of the game Maker claims one edge and Breaker is allowed to claim up to $q$ edges. If Maker is able to claim all edges of a copy of $C$, he wins the game. Otherwise Breaker wins. In this work we introduce the first constructive strategy for Maker for the $C_4$-Maker-Breaker game and show that he can win the game if $q < 0.16 n^{2/3}$. According to the theorem of Bednarska and Luczak (2000) $n^{2/3}$ is asymptotically optimal for this game, but the constant given there for a random Maker strategy is magnitudes apart from our constant 0.16.","sentences":["Maker-Breaker subgraph games are among the most famous combinatorial games.","For given $n,q \\in \\mathbb{N}$ and a subgraph $C$ of the complete graph $K_n$, the two players, called Maker and Breaker, alternately claim edges of $K_n$. In each round of the game Maker claims one edge and Breaker is allowed to claim up to $q$ edges.","If Maker is able to claim all edges of a copy of $C$, he wins the game.","Otherwise Breaker wins.","In this work we introduce the first constructive strategy for Maker for the $C_4$-Maker-Breaker game and show that he can win the game if $q < 0.16 n^{2/3}$.","According to the theorem of Bednarska and Luczak (2000) $n^{2/3}$ is asymptotically optimal for this game, but the constant given there for a random Maker strategy is magnitudes apart from our constant 0.16."],"url":"http://arxiv.org/abs/2405.04462v1","category":"math.CO"}
{"created":"2024-05-07 15:39:50","title":"Enhancement of terahertz emission during single-color filamentation by chirping laser pulse","abstract":"An experimental study of laser pulse duration influence on the terahertz emission during single-color filamentation is carried out. It is shown that for each terahertz frequency there is an optimal laser pulse duration providing maximal generation at constant pulse energy. It is demonstrated that longer pulses are required for stronger low-frequency terahertz emission, thus despite considerable laser peak power decreasing the terahertz radiation yield can be increased by more than 3 times.","sentences":["An experimental study of laser pulse duration influence on the terahertz emission during single-color filamentation is carried out.","It is shown that for each terahertz frequency there is an optimal laser pulse duration providing maximal generation at constant pulse energy.","It is demonstrated that longer pulses are required for stronger low-frequency terahertz emission, thus despite considerable laser peak power decreasing the terahertz radiation yield can be increased by more than 3 times."],"url":"http://arxiv.org/abs/2405.04413v1","category":"physics.optics"}
{"created":"2024-05-07 15:36:30","title":"On Bias and Its Reduction via Standardization in Source Localization Problems","abstract":"In source localization problems, the aim is to locate the sources within a domain that causes given measurements on the boundary. In this type of problem, biasing of the solution is one of the main causes of mislocalization. A technique called standardization was developed to reduce biasing. However, the lack of a mathematical background for this method can cause difficulties in its application and confusion regarding the reliability of solutions. Here, we give a rigorous and generalized background for the technique using the Bayesian framework to shed light on the technique's abilities and limitations. In addition, we take a look at the noise robustness of the method that is widely reported in numerical studies. The paper starts by giving a gentle introduction to the problem and its bias and works its way toward standardization.","sentences":["In source localization problems, the aim is to locate the sources within a domain that causes given measurements on the boundary.","In this type of problem, biasing of the solution is one of the main causes of mislocalization.","A technique called standardization was developed to reduce biasing.","However, the lack of a mathematical background for this method can cause difficulties in its application and confusion regarding the reliability of solutions.","Here, we give a rigorous and generalized background for the technique using the Bayesian framework to shed light on the technique's abilities and limitations.","In addition, we take a look at the noise robustness of the method that is widely reported in numerical studies.","The paper starts by giving a gentle introduction to the problem and its bias and works its way toward standardization."],"url":"http://arxiv.org/abs/2405.04409v1","category":"math.OC"}
{"created":"2024-05-07 14:51:05","title":"Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos","abstract":"Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously predict hand trajectories and object affordances on human egocentric videos. They are regarded as the representation of future hand-object interactions, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction. The experimental results show that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our proposed new evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D will be released at https://github.com/IRMVLab/Diff-IP2D.","sentences":["Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality.","To achieve this, some recent works have been proposed to simultaneously predict hand trajectories and object affordances on human egocentric videos.","They are regarded as the representation of future hand-object interactions, indicating potential human motion and motivation.","However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis.","Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions.","To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner.","We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones.","Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction.","The experimental results show that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our proposed new evaluation protocol.","This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction.","The code of Diff-IP2D will be released at https://github.com/IRMVLab/Diff-IP2D."],"url":"http://arxiv.org/abs/2405.04370v1","category":"cs.CV"}
{"created":"2024-05-07 14:48:42","title":"Quantum Circuit for Imputation of Missing Data","abstract":"The imputation of missing data is a common procedure in data analysis that consists in predicting missing values of incomplete data points. In this work we analyse a variational quantum circuit for the imputation of missing data. We construct variational quantum circuits with gates complexity $O(N)$ and $O(N^2)$ that return the last missing bit of a binary string for a specific distribution. We train and test the performance of the algorithms on a series of datasets finding good convergence of the results. Finally, we test the circuit for generalization to unseen data. For simple systems, we are able to describe the circuit analytically, making possible to skip the tedious and unresolved problem of training the circuit with repetitive measurements. We find beforehand the optimal values of the parameters and we make use of them to construct an optimal circuit suited to the generation of truly random data.","sentences":["The imputation of missing data is a common procedure in data analysis that consists in predicting missing values of incomplete data points.","In this work we analyse a variational quantum circuit for the imputation of missing data.","We construct variational quantum circuits with gates complexity $O(N)$ and $O(N^2)$ that return the last missing bit of a binary string for a specific distribution.","We train and test the performance of the algorithms on a series of datasets finding good convergence of the results.","Finally, we test the circuit for generalization to unseen data.","For simple systems, we are able to describe the circuit analytically, making possible to skip the tedious and unresolved problem of training the circuit with repetitive measurements.","We find beforehand the optimal values of the parameters and we make use of them to construct an optimal circuit suited to the generation of truly random data."],"url":"http://arxiv.org/abs/2405.04367v1","category":"quant-ph"}
{"created":"2024-05-07 14:44:41","title":"Some Notes on the Sample Complexity of Approximate Channel Simulation","abstract":"Channel simulation algorithms can efficiently encode random samples from a prescribed target distribution $Q$ and find applications in machine learning-based lossy data compression. However, algorithms that encode exact samples usually have random runtime, limiting their applicability when a consistent encoding time is desirable. Thus, this paper considers approximate schemes with a fixed runtime instead. First, we strengthen a result of Agustsson and Theis and show that there is a class of pairs of target distribution $Q$ and coding distribution $P$, for which the runtime of any approximate scheme scales at least super-polynomially in $D_\\infty[Q \\Vert P]$. We then show, by contrast, that if we have access to an unnormalised Radon-Nikodym derivative $r \\propto dQ/dP$ and knowledge of $D_{KL}[Q \\Vert P]$, we can exploit global-bound, depth-limited A* coding to ensure $\\mathrm{TV}[Q \\Vert P] \\leq \\epsilon$ and maintain optimal coding performance with a sample complexity of only $\\exp_2\\big((D_{KL}[Q \\Vert P] + o(1)) \\big/ \\epsilon\\big)$.","sentences":["Channel simulation algorithms can efficiently encode random samples from a prescribed target distribution $Q$ and find applications in machine learning-based lossy data compression.","However, algorithms that encode exact samples usually have random runtime, limiting their applicability when a consistent encoding time is desirable.","Thus, this paper considers approximate schemes with a fixed runtime instead.","First, we strengthen a result of Agustsson and Theis and show that there is a class of pairs of target distribution $Q$ and coding distribution $P$, for which the runtime of any approximate scheme scales at least super-polynomially in $D_\\infty[Q \\Vert P]$.","We then show, by contrast, that if we have access to an unnormalised Radon-Nikodym derivative $r \\propto dQ/dP$ and knowledge of $D_{KL}[Q \\Vert P]$, we can exploit global-bound, depth-limited A* coding to ensure $\\mathrm{TV}[Q \\Vert P] \\leq \\epsilon$ and maintain optimal coding performance with a sample complexity of only $\\exp_2\\big((D_{KL}[Q \\Vert P] + o(1))","\\big/ \\epsilon\\big)$."],"url":"http://arxiv.org/abs/2405.04363v1","category":"cs.IT"}
{"created":"2024-05-07 14:28:04","title":"Decision-Dependent Uncertainty-Aware Distribution System Planning Under Wildfire Risk","abstract":"The interaction between power systems and wildfires can be dangerous and costly. Damaged structures, load shedding, and high operational costs are potential consequences when the grid is unprepared. In fact, the operation of distribution grids can be liable for the outbreak of wildfires when extreme weather conditions arise. Within this context, investment planning should consider the impact of operational actions on the uncertainty related to wildfires that can directly affect line failure likelihood. Neglecting this can compromise the cost-benefit evaluation in planning system investments for wildfire risk. In this paper, we propose a decision-dependent uncertainty (DDU) aware methodology that provides the optimal portfolio of investments for distribution systems while considering that high power-flow levels through line segments in high-threat areas can ignite wildfires and, therefore, increase the probability of line failures. The methodology identifies the best combination of system upgrades (installation of new lines, hardening existing lines, and placement of switching devices) to provide the necessary leeway to operate the distribution system under wildfire-prone conditions. Our case study demonstrates that by modeling the DDU relationship between power flow prescriptions and line failures, investment decisions are more accurate and better prepare the grid infrastructure to deal with wildfire risk.","sentences":["The interaction between power systems and wildfires can be dangerous and costly.","Damaged structures, load shedding, and high operational costs are potential consequences when the grid is unprepared.","In fact, the operation of distribution grids can be liable for the outbreak of wildfires when extreme weather conditions arise.","Within this context, investment planning should consider the impact of operational actions on the uncertainty related to wildfires that can directly affect line failure likelihood.","Neglecting this can compromise the cost-benefit evaluation in planning system investments for wildfire risk.","In this paper, we propose a decision-dependent uncertainty (DDU) aware methodology that provides the optimal portfolio of investments for distribution systems while considering that high power-flow levels through line segments in high-threat areas can ignite wildfires and, therefore, increase the probability of line failures.","The methodology identifies the best combination of system upgrades (installation of new lines, hardening existing lines, and placement of switching devices) to provide the necessary leeway to operate the distribution system under wildfire-prone conditions.","Our case study demonstrates that by modeling the DDU relationship between power flow prescriptions and line failures, investment decisions are more accurate and better prepare the grid infrastructure to deal with wildfire risk."],"url":"http://arxiv.org/abs/2405.04350v1","category":"math.OC"}
{"created":"2024-05-07 14:00:51","title":"Packings of Smoothed Polygons","abstract":"This book uses optimal control theory to prove that the most unpackable centrally symmetric convex disk in the plane is a smoothed polygon. A smoothed polygon is a polygon whose corners have been rounded in a special way by arcs of hyperbolas. To be highly unpackable means that even densest packing of that disk has low density.   Motivated by Minkowski's geometry of numbers, researchers began to search for the most unpackable centrally symmetric convex disk (in brief, the most unpackable disk) starting in the early 1920s. In 1934, Reinhardt conjectured that the most unpackable disk is a smoothed octagon. Working independently of Reinhardt, Mahler attempted without success in 1947 to prove that the most unpackable disk must be a smoothed polygon. This book proves what Mahler set out to prove: Mahler's First conjecture on smoothed polygons. His second conjecture is identical to the Reinhardt conjecture, which remains open.   This book explores the many remarkable structures of this packing problem, formulated as a problem in optimal control theory on a Lie group, with connections to hyperbolic geometry and Hamiltonian mechanics. Bang-bang Pontryagin extremals to the optimal control problem are smoothed polygons. Extreme difficulties arise in the proof because of chattering behavior in the optimal control problem, corresponding to possible smoothed polygons with infinitely many sides that need to be ruled out. To analyze and eliminate the possibility of chattering solutions, the book introduces a discrete dynamical system (the Poincare first recurrence map) and gives a full description of its fixed points, stable and unstable manifolds, and basin of attraction on a blowup centered at a singular set. Some proofs in this book are computer-assisted using a computer algebra system.","sentences":["This book uses optimal control theory to prove that the most unpackable centrally symmetric convex disk in the plane is a smoothed polygon.","A smoothed polygon is a polygon whose corners have been rounded in a special way by arcs of hyperbolas.","To be highly unpackable means that even densest packing of that disk has low density.   ","Motivated by Minkowski's geometry of numbers, researchers began to search for the most unpackable centrally symmetric convex disk (in brief, the most unpackable disk) starting in the early 1920s.","In 1934, Reinhardt conjectured that the most unpackable disk is a smoothed octagon.","Working independently of Reinhardt, Mahler attempted without success in 1947 to prove that the most unpackable disk must be a smoothed polygon.","This book proves what Mahler set out to prove: Mahler's First conjecture on smoothed polygons.","His second conjecture is identical to the Reinhardt conjecture, which remains open.   ","This book explores the many remarkable structures of this packing problem, formulated as a problem in optimal control theory on a Lie group, with connections to hyperbolic geometry and Hamiltonian mechanics.","Bang-bang Pontryagin extremals to the optimal control problem are smoothed polygons.","Extreme difficulties arise in the proof because of chattering behavior in the optimal control problem, corresponding to possible smoothed polygons with infinitely many sides that need to be ruled out.","To analyze and eliminate the possibility of chattering solutions, the book introduces a discrete dynamical system (the Poincare first recurrence map) and gives a full description of its fixed points, stable and unstable manifolds, and basin of attraction on a blowup centered at a singular set.","Some proofs in this book are computer-assisted using a computer algebra system."],"url":"http://arxiv.org/abs/2405.04331v1","category":"math.OC"}
{"created":"2024-05-07 13:47:35","title":"Molecular Identification via Molecular Fingerprint extraction from Atomic Force Microscopy images","abstract":"Non--Contact Atomic Force Microscopy with CO--functionalized metal tips (referred to as HR-AFM) provides access to the internal structure of individual molecules adsorbed on a surface with totally unprecedented resolution. Previous works have shown that deep learning (DL) models can retrieve the chemical and structural information encoded in a 3D stack of constant-height HR--AFM images, leading to molecular identification. In this work, we overcome their limitations by using a well-established description of the molecular structure in terms of topological fingerprints, the 1024--bit Extended Connectivity Chemical Fingerprints of radius 2 (ECFP4), that were developed for substructure and similarity searching. ECFPs provide local structural information of the molecule, each bit correlating with a particular substructure within the molecule. Our DL model is able to extract this optimized structural descriptor from the 3D HR--AFM stacks and use it, through virtual screening, to identify molecules from their predicted ECFP4 with a retrieval accuracy on theoretical images of 95.4\\%. Furthermore, this approach, unlike previous DL models, assigns a confidence score, the Tanimoto similarity, to each of the candidate molecules, thus providing information on the reliability of the identification.   By construction, the number of times a certain substructure is present in the molecule is lost during the hashing process, necessary to make them useful for machine learning applications. We show that it is possible to complement the fingerprint-based virtual screening with global information provided by another DL model that predicts from the same HR--AFM stacks the chemical formula, boosting the identification accuracy up to a 97.6\\%. Finally, we perform a limited test with experimental images, obtaining promising results towards the application of this pipeline under real conditions","sentences":["Non--Contact Atomic Force Microscopy with CO--functionalized metal tips (referred to as HR-AFM) provides access to the internal structure of individual molecules adsorbed on a surface with totally unprecedented resolution.","Previous works have shown that deep learning (DL) models can retrieve the chemical and structural information encoded in a 3D stack of constant-height HR--AFM images, leading to molecular identification.","In this work, we overcome their limitations by using a well-established description of the molecular structure in terms of topological fingerprints, the 1024--bit Extended Connectivity Chemical Fingerprints of radius 2 (ECFP4), that were developed for substructure and similarity searching.","ECFPs provide local structural information of the molecule, each bit correlating with a particular substructure within the molecule.","Our DL model is able to extract this optimized structural descriptor from the 3D HR--AFM stacks and use it, through virtual screening, to identify molecules from their predicted ECFP4 with a retrieval accuracy on theoretical images of 95.4\\%.","Furthermore, this approach, unlike previous DL models, assigns a confidence score, the Tanimoto similarity, to each of the candidate molecules, thus providing information on the reliability of the identification.   ","By construction, the number of times a certain substructure is present in the molecule is lost during the hashing process, necessary to make them useful for machine learning applications.","We show that it is possible to complement the fingerprint-based virtual screening with global information provided by another DL model that predicts from the same HR--AFM stacks the chemical formula, boosting the identification accuracy up to a 97.6\\%.","Finally, we perform a limited test with experimental images, obtaining promising results towards the application of this pipeline under real conditions"],"url":"http://arxiv.org/abs/2405.04321v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 13:36:33","title":"Deck of Cards method for Hierarchical, Robust and Stochastic Ordinal Regression","abstract":"In this paper, we consider the recently introduced application of the Deck of Cards Method (DCM) to ordinal regression proposing an extension to Robust Ordinal Regression and Stochastic Multiattribute Acceptability Analysis. In Multiple Criteria Decision Aiding context, the proposed methodology permits to assign a value to each alternative evaluated on a set of criteria hierarchically structured. The Decision Maker can provide precise or imprecise information at different levels of the hierarchy of criteria using the classical DCM framework. This information is therefore used to infer a value function compatible with it. The compatible value function can be a simple weighted sum, a piecewise linear value function, a general monotonic value function, or a Choquet integral. To provide robust recommendations to the Decision Maker, we consider the Robust Ordinal Regression and the Stochastic Multicriteria Acceptability Analysis because, even if in different ways, both of them take into account the whole set of models compatible with the preference information provided by the Decision Maker. The applicability of the proposed methodology is shown by a didactic example in which Italian regions are evaluated on criteria representing Circular Economy, Innovation Driven Development and Smart Specialization Strategies.","sentences":["In this paper, we consider the recently introduced application of the Deck of Cards Method (DCM) to ordinal regression proposing an extension to Robust Ordinal Regression and Stochastic Multiattribute Acceptability Analysis.","In Multiple Criteria Decision Aiding context, the proposed methodology permits to assign a value to each alternative evaluated on a set of criteria hierarchically structured.","The Decision Maker can provide precise or imprecise information at different levels of the hierarchy of criteria using the classical DCM framework.","This information is therefore used to infer a value function compatible with it.","The compatible value function can be a simple weighted sum, a piecewise linear value function, a general monotonic value function, or a Choquet integral.","To provide robust recommendations to the Decision Maker, we consider the Robust Ordinal Regression and the Stochastic Multicriteria Acceptability Analysis because, even if in different ways, both of them take into account the whole set of models compatible with the preference information provided by the Decision Maker.","The applicability of the proposed methodology is shown by a didactic example in which Italian regions are evaluated on criteria representing Circular Economy, Innovation Driven Development and Smart Specialization Strategies."],"url":"http://arxiv.org/abs/2405.04313v1","category":"math.OC"}
{"created":"2024-05-07 13:27:52","title":"Accelerating Speculative Decoding using Dynamic Speculation Length","abstract":"Speculative decoding is a promising method for reducing the inference latency of large language models. The effectiveness of the method depends on the speculation length (SL) - the number of tokens generated by the draft model at each iteration. The vast majority of speculative decoding approaches use the same SL for all iterations. In this work, we show that this practice is suboptimal. We introduce DISCO, a DynamIc SpeCulation length Optimization method that uses a classifier to dynamically adjust the SL at each iteration, while provably preserving the decoding quality. Experiments with four benchmarks demonstrate average speedup gains of 10.3% relative to our best baselines.","sentences":["Speculative decoding is a promising method for reducing the inference latency of large language models.","The effectiveness of the method depends on the speculation length (SL) - the number of tokens generated by the draft model at each iteration.","The vast majority of speculative decoding approaches use the same SL for all iterations.","In this work, we show that this practice is suboptimal.","We introduce DISCO, a DynamIc SpeCulation length Optimization method that uses a classifier to dynamically adjust the SL at each iteration, while provably preserving the decoding quality.","Experiments with four benchmarks demonstrate average speedup gains of 10.3% relative to our best baselines."],"url":"http://arxiv.org/abs/2405.04304v1","category":"cs.CL"}
{"created":"2024-05-07 13:26:09","title":"Progressive Quantum Algorithm for Quantum Alternating Operator Ansatz","abstract":"Recently, Hadfield has proposed a novel Quantum Alternating Operator Ansatz (QAOA+) to tackle Constrained Combinatorial Optimization Problems (CCOPs), and it has wide applications. However, the large requirement of multi-qubit controlled gates in QAOA+ limits its applications in solving larger-scale CCOPs. To mitigate the resources overhead of QAOA+, we introduce an approach termed Progressive Quantum Algorithm (PQA). In this paper, the concept and performance of PQA are introduced focusing on the Maximal Independent Set (MIS) problem. PQA aims to yield the solution of the target graph $G$ with fewer resources by solving the MIS problem on a desired derived subgraph that has the same MIS solution as $G$ but has a much smaller graph size. To construct such a desired subgraph, PQA gradually and regularly expands the graph size starting from a well-designed initial subgraph. After each expansion, PQA solves the MIS problem on the current subgraph using QAOA+ and estimates whether the current graph has the same MIS solution as the target graph. PQA repeats the graph expansion and solving process until reaching the stop condition. In our simulations, the performance of PQA is benchmarked on Erd\\H{o}s-R\\'enyi (ER) and regular graphs. The simulation results suggest that PQA showcases higher average approximation ratio (AAR) and significant quantum resource savings compared with directly solves the original problem using QAOA+ (DS-QAOA+) at the same level depth $p$. Remarkably, the AAR obtained by PQA is $12.9305\\%$ ($4.8645\\%$) higher than DS-QAOA+ on ER (regular) graphs, and the average number of multi-qubit gates (qubits) consumed by PQA is 1/3 (1/2) of that of DS-QAOA+. The remarkable efficiency of PQA makes it possible to solve larger-scale CCOPs on the current quantum devices.","sentences":["Recently, Hadfield has proposed a novel Quantum Alternating Operator Ansatz (QAOA+) to tackle Constrained Combinatorial Optimization Problems (CCOPs), and it has wide applications.","However, the large requirement of multi-qubit controlled gates in QAOA+ limits its applications in solving larger-scale CCOPs.","To mitigate the resources overhead of QAOA+, we introduce an approach termed Progressive Quantum Algorithm (PQA).","In this paper, the concept and performance of PQA are introduced focusing on the Maximal Independent Set (MIS) problem.","PQA aims to yield the solution of the target graph $G$ with fewer resources by solving the MIS problem on a desired derived subgraph that has the same MIS solution as $G$ but has a much smaller graph size.","To construct such a desired subgraph, PQA gradually and regularly expands the graph size starting from a well-designed initial subgraph.","After each expansion, PQA solves the MIS problem on the current subgraph using QAOA+ and estimates whether the current graph has the same MIS solution as the target graph.","PQA repeats the graph expansion and solving process until reaching the stop condition.","In our simulations, the performance of PQA is benchmarked on Erd\\H{o}s-R\\'enyi (ER) and regular graphs.","The simulation results suggest that PQA showcases higher average approximation ratio (AAR) and significant quantum resource savings compared with directly solves the original problem using QAOA+ (DS-QAOA+) at the same level depth $p$. Remarkably, the AAR obtained by PQA is $12.9305\\%$ ($4.8645\\%$) higher than DS-QAOA+ on ER (regular) graphs, and the average number of multi-qubit gates (qubits) consumed by PQA is 1/3 (1/2) of that of DS-QAOA+.","The remarkable efficiency of PQA makes it possible to solve larger-scale CCOPs on the current quantum devices."],"url":"http://arxiv.org/abs/2405.04303v1","category":"quant-ph"}
{"created":"2024-05-07 12:51:44","title":"PDCCH Scheduling via Maximum Independent Set","abstract":"In 5G, the Physical Downlink Control CHannel (PDCCH) carries crucial information enabling the User Equipment (UE) to connect in UL and DL. UEs are unaware of the frequency location at which PDCCH is encoded, hence they need to perform blind decoding over a limited set of possible candidates. We address the problem faced by the gNodeB of selecting PDCCH candidates for each UE to optimize data transmission. We formulate it as a Maximum Weighted Independent Set (MWIS) problem, that is known to be an NP-hard problem and cannot even be approximated. A solution method called Weight-to-Degree Ratio (WDR) Greedy emerges as a strong contender for practical implementations due to its favorable performance-to-complexity trade-off and theoretical performance guarantees.","sentences":["In 5G, the Physical Downlink Control CHannel (PDCCH) carries crucial information enabling the User Equipment (UE) to connect in UL and DL.","UEs are unaware of the frequency location at which PDCCH is encoded, hence they need to perform blind decoding over a limited set of possible candidates.","We address the problem faced by the gNodeB of selecting PDCCH candidates for each UE to optimize data transmission.","We formulate it as a Maximum Weighted Independent Set (MWIS) problem, that is known to be an NP-hard problem and cannot even be approximated.","A solution method called Weight-to-Degree Ratio (WDR) Greedy emerges as a strong contender for practical implementations due to its favorable performance-to-complexity trade-off and theoretical performance guarantees."],"url":"http://arxiv.org/abs/2405.04283v1","category":"cs.IT"}
{"created":"2024-05-07 12:48:37","title":"Modal Folding: Discovering Smooth Folding Patterns for Sheet Materials using Strain-Space Modes","abstract":"Folding can transform mundane objects such as napkins into stunning works of art. However, finding new folding transformations for sheet materials is a challenging problem that requires expertise and real-world experimentation. In this paper, we present Modal Folding -- an automated approach for discovering energetically optimal folding transformations, i.e., large deformations that require little mechanical work. For small deformations, minimizing internal energy for fixed displacement magnitudes leads to the well-known elastic eigenmodes. While linear modes provide promising directions for bending, they cannot capture the rotational motion required for folding. To overcome this limitation, we introduce strain-space modes -- nonlinear analogues of elastic eigenmodes that operate on per-element curvatures instead of vertices. Using strain-space modes to determine target curvatures for bending elements, we can generate complex nonlinear folding motions by simply minimizing the sheet's internal energy. Our modal folding approach offers a systematic and automated way to create complex designs. We demonstrate the effectiveness of our method with simulation results for a range of shapes and materials, and validate our designs with physical prototypes.","sentences":["Folding can transform mundane objects such as napkins into stunning works of art.","However, finding new folding transformations for sheet materials is a challenging problem that requires expertise and real-world experimentation.","In this paper, we present Modal Folding -- an automated approach for discovering energetically optimal folding transformations, i.e., large deformations that require little mechanical work.","For small deformations, minimizing internal energy for fixed displacement magnitudes leads to the well-known elastic eigenmodes.","While linear modes provide promising directions for bending, they cannot capture the rotational motion required for folding.","To overcome this limitation, we introduce strain-space modes -- nonlinear analogues of elastic eigenmodes that operate on per-element curvatures instead of vertices.","Using strain-space modes to determine target curvatures for bending elements, we can generate complex nonlinear folding motions by simply minimizing the sheet's internal energy.","Our modal folding approach offers a systematic and automated way to create complex designs.","We demonstrate the effectiveness of our method with simulation results for a range of shapes and materials, and validate our designs with physical prototypes."],"url":"http://arxiv.org/abs/2405.04280v1","category":"cs.GR"}
{"created":"2024-05-07 12:41:31","title":"BUDDy: Single-Channel Blind Unsupervised Dereverberation with Diffusion Models","abstract":"In this paper, we present an unsupervised single-channel method for joint blind dereverberation and room impulse response estimation, based on posterior sampling with diffusion models. We parameterize the reverberation operator using a filter with exponential decay for each frequency subband, and iteratively estimate the corresponding parameters as the speech utterance gets refined along the reverse diffusion trajectory. A measurement consistency criterion enforces the fidelity of the generated speech with the reverberant measurement, while an unconditional diffusion model implements a strong prior for clean speech generation. Without any knowledge of the room impulse response nor any coupled reverberant-anechoic data, we can successfully perform dereverberation in various acoustic scenarios. Our method significantly outperforms previous blind unsupervised baselines, and we demonstrate its increased robustness to unseen acoustic conditions in comparison to blind supervised methods. Audio samples and code are available online.","sentences":["In this paper, we present an unsupervised single-channel method for joint blind dereverberation and room impulse response estimation, based on posterior sampling with diffusion models.","We parameterize the reverberation operator using a filter with exponential decay for each frequency subband, and iteratively estimate the corresponding parameters as the speech utterance gets refined along the reverse diffusion trajectory.","A measurement consistency criterion enforces the fidelity of the generated speech with the reverberant measurement, while an unconditional diffusion model implements a strong prior for clean speech generation.","Without any knowledge of the room impulse response nor any coupled reverberant-anechoic data, we can successfully perform dereverberation in various acoustic scenarios.","Our method significantly outperforms previous blind unsupervised baselines, and we demonstrate its increased robustness to unseen acoustic conditions in comparison to blind supervised methods.","Audio samples and code are available online."],"url":"http://arxiv.org/abs/2405.04272v1","category":"eess.AS"}
{"created":"2024-05-07 12:17:58","title":"Insights from Basilisk: Are Open-Source EDA Tools Ready for a Multi-Million-Gate, Linux-Booting RV64 SoC Design?","abstract":"Designing complex, multi-million-gate application-specific integrated circuits requires robust and mature electronic design automation (EDA) tools. We describe our efforts in enhancing the open-source Yosys+Openroad EDA flow to implement Basilisk, a fully open-source, Linux-booting RV64GC system-on-chip (SoC) design. We analyze the quality-of-results impact of our enhancements to synthesis tools, interfaces between EDA tools, logic optimization scripts, and a newly open-sourced library of optimized arithmetic macro-operators. We also introduce a streamlined physical design flow with an improved power grid and cell placement integration. Our Basilisk SoC design was taped out in IHP's open 130 nm technology. It achieves an operating frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA flow, while also reducing logic area by 1.6x. Furthermore, tool runtime was reduced by 2.5x, and peak RAM usage decreased by 2.9x. Through collaboration with EDA tool developers and domain experts, Basilisk establishes solid \"proof of existence\" for a fully open-source EDA flow used in designing a competitive multi-million-gate digital SoC.","sentences":["Designing complex, multi-million-gate application-specific integrated circuits requires robust and mature electronic design automation (EDA) tools.","We describe our efforts in enhancing the open-source Yosys+Openroad EDA flow to implement Basilisk, a fully open-source, Linux-booting RV64GC system-on-chip (SoC) design.","We analyze the quality-of-results impact of our enhancements to synthesis tools, interfaces between EDA tools, logic optimization scripts, and a newly open-sourced library of optimized arithmetic macro-operators.","We also introduce a streamlined physical design flow with an improved power grid and cell placement integration.","Our Basilisk SoC design was taped out in IHP's open 130 nm technology.","It achieves an operating frequency of 77 MHz (51 logic levels) under typical conditions, a 2.3x improvement compared to the baseline open-source EDA flow, while also reducing logic area by 1.6x.","Furthermore, tool runtime was reduced by 2.5x, and peak RAM usage decreased by 2.9x.","Through collaboration with EDA tool developers and domain experts, Basilisk establishes solid \"proof of existence\" for a fully open-source EDA flow used in designing a competitive multi-million-gate digital SoC."],"url":"http://arxiv.org/abs/2405.04257v2","category":"cs.AR"}
{"created":"2024-05-07 12:09:22","title":"Weighted Least-Squares PARSIM","abstract":"Subspace identification methods (SIMs) have proven very powerful for estimating linear state-space models. To overcome the deficiencies of classical SIMs, a significant number of algorithms has appeared over the last two decades, where most of them involve a common intermediate step, that is to estimate the range space of the extended observability matrix. In this contribution, an optimized version of the parallel and parsimonious SIM (PARSIM), PARSIM\\textsubscript{opt}, is proposed by using weighted least-squares. It not only inherits all the benefits of PARSIM but also attains the best linear unbiased estimator for the above intermediate step. Furthermore, inspired by SIMs based on the predictor form, consistent estimates of the optimal weighting matrix for weighted least-squares are derived. Essential similarities, differences and simulated comparisons of some key SIMs related to our method are also presented.","sentences":["Subspace identification methods (SIMs) have proven very powerful for estimating linear state-space models.","To overcome the deficiencies of classical SIMs, a significant number of algorithms has appeared over the last two decades, where most of them involve a common intermediate step, that is to estimate the range space of the extended observability matrix.","In this contribution, an optimized version of the parallel and parsimonious SIM (PARSIM), PARSIM\\textsubscript{opt}, is proposed by using weighted least-squares.","It not only inherits all the benefits of PARSIM but also attains the best linear unbiased estimator for the above intermediate step.","Furthermore, inspired by SIMs based on the predictor form, consistent estimates of the optimal weighting matrix for weighted least-squares are derived.","Essential similarities, differences and simulated comparisons of some key SIMs related to our method are also presented."],"url":"http://arxiv.org/abs/2405.04250v1","category":"eess.SY"}
{"created":"2024-05-07 11:32:37","title":"Deep Reinforcement Learning for Multi-User RF Charging with Non-linear Energy Harvesters","abstract":"Radio frequency (RF) wireless power transfer (WPT) is a promising technology for sustainable support of massive Internet of Things (IoT). However, RF-WPT systems are characterized by low efficiency due to channel attenuation, which can be mitigated by precoders that adjust the transmission directivity. This work considers a multi-antenna RF-WPT system with multiple non-linear energy harvesting (EH) nodes with energy demands changing over discrete time slots. This leads to the charging scheduling problem, which involves choosing the precoders at each slot to minimize the total energy consumption and meet the EH requirements. We model the problem as a Markov decision process and propose a solution relying on a low-complexity beamforming and deep deterministic policy gradient (DDPG). The results show that the proposed beamforming achieves near-optimal performance with low computational complexity, and the DDPG-based approach converges with the number of episodes and reduces the system's power consumption, while the outage probability and the power consumption increase with the number of devices.","sentences":["Radio frequency (RF) wireless power transfer (WPT) is a promising technology for sustainable support of massive Internet of Things (IoT).","However, RF-WPT systems are characterized by low efficiency due to channel attenuation, which can be mitigated by precoders that adjust the transmission directivity.","This work considers a multi-antenna RF-WPT system with multiple non-linear energy harvesting (EH) nodes with energy demands changing over discrete time slots.","This leads to the charging scheduling problem, which involves choosing the precoders at each slot to minimize the total energy consumption and meet the EH requirements.","We model the problem as a Markov decision process and propose a solution relying on a low-complexity beamforming and deep deterministic policy gradient (DDPG).","The results show that the proposed beamforming achieves near-optimal performance with low computational complexity, and the DDPG-based approach converges with the number of episodes and reduces the system's power consumption, while the outage probability and the power consumption increase with the number of devices."],"url":"http://arxiv.org/abs/2405.04218v1","category":"eess.SP"}
{"created":"2024-05-07 11:17:01","title":"Exact calculation of the probabilities of rare events in cluster-cluster aggregation","abstract":"We develop an action formalism to calculate probabilities of rare events in cluster-cluster aggregation for arbitrary collision kernels and establish a pathwise large deviation principle with total mass being the rate. As an application, the rate function for the number of surviving particles as well as the optimal evolution trajectory are calculated exactly for the constant, sum and product kernels. For the product kernel, we argue that the second derivative of the rate function has a discontinuity. The theoretical results agree with simulations tailored to the calculation of rare events.","sentences":["We develop an action formalism to calculate probabilities of rare events in cluster-cluster aggregation for arbitrary collision kernels and establish a pathwise large deviation principle with total mass being the rate.","As an application, the rate function for the number of surviving particles as well as the optimal evolution trajectory are calculated exactly for the constant, sum and product kernels.","For the product kernel, we argue that the second derivative of the rate function has a discontinuity.","The theoretical results agree with simulations tailored to the calculation of rare events."],"url":"http://arxiv.org/abs/2405.04201v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 10:48:37","title":"Behavioral Manifolds: Representing the Landscape of Grasp Affordances in the Relative Pose Space","abstract":"The use of machine learning to investigate grasp affordances has received extensive attention over the past several decades. The existing literature provides a robust basis to build upon, though a number of aspects may be improved. Results commonly work in terms of grasp configuration, with little consideration for the manner in which the grasp may be (re-)produced from a reachability and trajectory planning perspective. In addition, the majority of existing learning approaches focus of producing a single viable grasp, offering little transparency on how the result was reached, or insights on its robustness. We propose a different perspective on grasp affordance learning, explicitly accounting for grasp synthesis; that is, the manner in which manipulator kinematics are used to allow materialization of grasps. The approach allows to explicitly map the grasp policy space in terms of generated grasp types and associated grasp quality. Results of numerical simulations illustrate merit of the method and highlight the manner in which it may promote a greater degree of explainability for otherwise intransparent reinforcement processes.","sentences":["The use of machine learning to investigate grasp affordances has received extensive attention over the past several decades.","The existing literature provides a robust basis to build upon, though a number of aspects may be improved.","Results commonly work in terms of grasp configuration, with little consideration for the manner in which the grasp may be (re-)produced from a reachability and trajectory planning perspective.","In addition, the majority of existing learning approaches focus of producing a single viable grasp, offering little transparency on how the result was reached, or insights on its robustness.","We propose a different perspective on grasp affordance learning, explicitly accounting for grasp synthesis; that is, the manner in which manipulator kinematics are used to allow materialization of grasps.","The approach allows to explicitly map the grasp policy space in terms of generated grasp types and associated grasp quality.","Results of numerical simulations illustrate merit of the method and highlight the manner in which it may promote a greater degree of explainability for otherwise intransparent reinforcement processes."],"url":"http://arxiv.org/abs/2405.04188v1","category":"cs.RO"}
{"created":"2024-05-07 10:46:20","title":"Detectability of eccentric binary black holes with PyCBC and cWB pipelines during the third observing run of LIGO-Virgo-KAGRA","abstract":"Detecting binary black hole (BBH) mergers with quantifiable orbital eccentricity would confirm the existence of a dynamical formation channel for these binaries. The current state-of-the-art gravitational wave searches of LIGO-Virgo-KAGRA strain data focus more on quasicircular mergers due to increased dimensionality and lack of efficient eccentric waveform models. In this work, we compare the sensitivities of two search pipelines, the matched filter-based \\texttt{PyCBC} and the unmodelled coherent Wave Burst (\\texttt{cWB}) algorithms towards the spinning eccentric BBH mergers, using a multipolar nonprecessing-spin eccentric signal model, \\texttt{SEOBNRv4EHM}. Our findings show that neglecting eccentricity leads to missed opportunities for detecting eccentric BBH mergers, with \\texttt{PyCBC} exhibiting a $10-20\\, \\%$ sensitivity loss for eccentricities exceeding $0.2$ defined at $10$ Hz. In contrast, \\texttt{cWB} is resilient, with a $10\\, \\%$ sensitivity increase for heavier ($\\mathcal{M} \\ge 30 \\, \\text{M}_{\\odot}$) eccentric BBH mergers, but is significantly less sensitive than \\texttt{PyCBC} for lighter BBH mergers. Our fitting factor study confirmed that neglecting eccentricity biases the estimation of chirp mass, mass ratio, and effective spin parameter, skewing our understanding of astrophysical BBH populations, fundamental physics, and precision cosmology. Our results demonstrate that the current search pipelines are not sufficiently sensitive to eccentric BBH mergers, necessitating the development of a dedicated matched-filter search for these binaries. Whereas, burst searches should be optimized to detect lower chirp mass BBH mergers as eccentricity does not affect their search sensitivity significantly.","sentences":["Detecting binary black hole (BBH) mergers with quantifiable orbital eccentricity would confirm the existence of a dynamical formation channel for these binaries.","The current state-of-the-art gravitational wave searches of LIGO-Virgo-KAGRA strain data focus more on quasicircular mergers due to increased dimensionality and lack of efficient eccentric waveform models.","In this work, we compare the sensitivities of two search pipelines, the matched filter-based \\texttt{PyCBC} and the unmodelled coherent Wave Burst (\\texttt{cWB}) algorithms towards the spinning eccentric BBH mergers, using a multipolar nonprecessing-spin eccentric signal model, \\texttt{SEOBNRv4EHM}.","Our findings show that neglecting eccentricity leads to missed opportunities for detecting eccentric BBH mergers, with \\texttt{PyCBC} exhibiting a $10-20\\, \\%$ sensitivity loss for eccentricities exceeding $0.2$ defined at $10$ Hz.","In contrast, \\texttt{cWB} is resilient, with a $10\\, \\%$ sensitivity increase for heavier ($\\mathcal{M} \\ge 30 \\, \\text{M}_{\\odot}$) eccentric BBH mergers, but is significantly less sensitive than \\texttt{PyCBC} for lighter BBH mergers.","Our fitting factor study confirmed that neglecting eccentricity biases the estimation of chirp mass, mass ratio, and effective spin parameter, skewing our understanding of astrophysical BBH populations, fundamental physics, and precision cosmology.","Our results demonstrate that the current search pipelines are not sufficiently sensitive to eccentric BBH mergers, necessitating the development of a dedicated matched-filter search for these binaries.","Whereas, burst searches should be optimized to detect lower chirp mass BBH mergers as eccentricity does not affect their search sensitivity significantly."],"url":"http://arxiv.org/abs/2405.04186v1","category":"gr-qc"}
{"created":"2024-05-07 10:03:19","title":"LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection","abstract":"Nowadays, Information spreads at an unprecedented pace in social media and discerning truth from misinformation and fake news has become an acute societal challenge. Machine learning (ML) models have been employed to identify fake news but are far from perfect with challenging problems like limited accuracy, interpretability, and generalizability. In this paper, we enhance ML-based solutions with linguistics input and we propose LingML, linguistic-informed ML, for fake news detection. We conducted an experimental study with a popular dataset on fake news during the pandemic. The experiment results show that our proposed solution is highly effective. There are fewer than two errors out of every ten attempts with only linguistic input used in ML and the knowledge is highly explainable. When linguistics input is integrated with advanced large-scale ML models for natural language processing, our solution outperforms existing ones with 1.8% average error rate. LingML creates a new path with linguistics to push the frontier of effective and efficient fake news detection. It also sheds light on real-world multi-disciplinary applications requiring both ML and domain expertise to achieve optimal performance.","sentences":["Nowadays, Information spreads at an unprecedented pace in social media and discerning truth from misinformation and fake news has become an acute societal challenge.","Machine learning (ML) models have been employed to identify fake news but are far from perfect with challenging problems like limited accuracy, interpretability, and generalizability.","In this paper, we enhance ML-based solutions with linguistics input and we propose LingML, linguistic-informed ML, for fake news detection.","We conducted an experimental study with a popular dataset on fake news during the pandemic.","The experiment results show that our proposed solution is highly effective.","There are fewer than two errors out of every ten attempts with only linguistic input used in ML and the knowledge is highly explainable.","When linguistics input is integrated with advanced large-scale ML models for natural language processing, our solution outperforms existing ones with 1.8% average error rate.","LingML creates a new path with linguistics to push the frontier of effective and efficient fake news detection.","It also sheds light on real-world multi-disciplinary applications requiring both ML and domain expertise to achieve optimal performance."],"url":"http://arxiv.org/abs/2405.04165v1","category":"cs.CL"}
{"created":"2024-05-07 09:54:09","title":"Economic Complexity in Mono-Partite Networks","abstract":"Initially designed to predict and explain the economic trajectories of countries, cities, and regions, economic complexity has been found applicable in diverse contexts such as ecology and chess openings. The success of economic complexity stems from its capacity to assess hidden capabilities within a system indirectly. The existing algorithms for economic complexity operate only when the underlying interaction topology conforms to a bipartite graph. A single link disrupting the bipartite structure renders these algorithms inapplicable, even if the weight of that link is tiny compared to others. This paper presents a novel extension of economic complexity to encompass any graph, overcoming the constraints of bipartite structures. Additionally, it introduces fitness centrality and orthofitness centrality as new centrality measures in graphs. Fitness Centrality emerges as a promising metric for assessing node vulnerability, akin to node betweenness centrality. Furthermore, we unveil the cost functions that drive the minimization procedures underlying the economic complexity index and fitness centrality algorithms. This extension broadens the scope of economic complexity analysis, enabling its application in diverse network structures beyond bipartite graphs.","sentences":["Initially designed to predict and explain the economic trajectories of countries, cities, and regions, economic complexity has been found applicable in diverse contexts such as ecology and chess openings.","The success of economic complexity stems from its capacity to assess hidden capabilities within a system indirectly.","The existing algorithms for economic complexity operate only when the underlying interaction topology conforms to a bipartite graph.","A single link disrupting the bipartite structure renders these algorithms inapplicable, even if the weight of that link is tiny compared to others.","This paper presents a novel extension of economic complexity to encompass any graph, overcoming the constraints of bipartite structures.","Additionally, it introduces fitness centrality and orthofitness centrality as new centrality measures in graphs.","Fitness Centrality emerges as a promising metric for assessing node vulnerability, akin to node betweenness centrality.","Furthermore, we unveil the cost functions that drive the minimization procedures underlying the economic complexity index and fitness centrality algorithms.","This extension broadens the scope of economic complexity analysis, enabling its application in diverse network structures beyond bipartite graphs."],"url":"http://arxiv.org/abs/2405.04158v1","category":"physics.soc-ph"}
{"created":"2024-05-07 09:19:53","title":"Lossy Compression with Data, Perception, and Classification Constraints","abstract":"Balancing diverse task objectives under limited rate is crucial for developing robust multi-task deep learning (DL) models and improving performance across various domains. In this paper, we consider the lossy compression problem with human-centric and task-oriented metrics, such as perceptual quality and classification accuracy. We investigate two ternary relationships, namely, the rate-distortion-classification (RDC) and rate-perception-classification (RPC). For both RDC and RPC functions, we derive the closed-form expressions of the optimal rate for both binary and Gaussian sources. Notably, both RDC and RPC relationships exhibit distinct characteristics compared to the previous RDP tradeoff proposed by Blau et al. Then, we conduct experiments by implementing a DL-based image compression framework, incorporating rate, distortion, perception, and classification constraints. The experimental results verify the theoretical characteristics of RDC and RPC tradeoffs, providing information-theoretical insights into the design of loss functions to balance diverse task objectives in deep learning.","sentences":["Balancing diverse task objectives under limited rate is crucial for developing robust multi-task deep learning (DL) models and improving performance across various domains.","In this paper, we consider the lossy compression problem with human-centric and task-oriented metrics, such as perceptual quality and classification accuracy.","We investigate two ternary relationships, namely, the rate-distortion-classification (RDC) and rate-perception-classification (RPC).","For both RDC and RPC functions, we derive the closed-form expressions of the optimal rate for both binary and Gaussian sources.","Notably, both RDC and RPC relationships exhibit distinct characteristics compared to the previous RDP tradeoff proposed by Blau et al.","Then, we conduct experiments by implementing a DL-based image compression framework, incorporating rate, distortion, perception, and classification constraints.","The experimental results verify the theoretical characteristics of RDC and RPC tradeoffs, providing information-theoretical insights into the design of loss functions to balance diverse task objectives in deep learning."],"url":"http://arxiv.org/abs/2405.04144v1","category":"cs.IT"}
{"created":"2024-05-07 08:49:01","title":"Optimizing Prosumer Policies in Periodic Double Auctions Inspired by Equilibrium Analysis (Extended Version)","abstract":"We consider a periodic double auction (PDA) wherein the main participants are wholesale suppliers and brokers representing retailers. The suppliers are represented by a composite supply curve and the brokers are represented by individual bids. Additionally, the brokers can participate in small-scale selling by placing individual asks; hence, they act as prosumers. Specifically, in a PDA, the prosumers who are net buyers have multiple opportunities to buy or sell multiple units of a commodity with the aim of minimizing the cost of buying across multiple rounds of the PDA. Formulating optimal bidding strategies for such a PDA setting involves planning across current and future rounds while considering the bidding strategies of other agents. In this work, we propose Markov perfect Nash equilibrium (MPNE) policies for a setup where multiple prosumers with knowledge of the composite supply curve compete to procure commodities. Thereafter, the MPNE policies are used to develop an algorithm called MPNE-BBS for the case wherein the prosumers need to re-construct an approximate composite supply curve using past auction information. The efficacy of the proposed algorithm is demonstrated on the PowerTAC wholesale market simulator against several baselines and state-of-the-art bidding policies.","sentences":["We consider a periodic double auction (PDA) wherein the main participants are wholesale suppliers and brokers representing retailers.","The suppliers are represented by a composite supply curve and the brokers are represented by individual bids.","Additionally, the brokers can participate in small-scale selling by placing individual asks; hence, they act as prosumers.","Specifically, in a PDA, the prosumers who are net buyers have multiple opportunities to buy or sell multiple units of a commodity with the aim of minimizing the cost of buying across multiple rounds of the PDA.","Formulating optimal bidding strategies for such a PDA setting involves planning across current and future rounds while considering the bidding strategies of other agents.","In this work, we propose Markov perfect Nash equilibrium (MPNE) policies for a setup where multiple prosumers with knowledge of the composite supply curve compete to procure commodities.","Thereafter, the MPNE policies are used to develop an algorithm called MPNE-BBS for the case wherein the prosumers need to re-construct an approximate composite supply curve using past auction information.","The efficacy of the proposed algorithm is demonstrated on the PowerTAC wholesale market simulator against several baselines and state-of-the-art bidding policies."],"url":"http://arxiv.org/abs/2405.04125v2","category":"eess.SY"}
{"created":"2024-05-07 08:43:25","title":"Movable Antennas-Enabled Two-User Multicasting: Do We Really Need Alternating Optimization for Minimum Rate Maximization?","abstract":"Movable antenna (MA) technology, which can reconfigure wireless channels by flexibly moving antenna positions in a specified region, has great potential for improving communication performance. In this paper, we consider a new setup of MAs-enabled multicasting, where we adopt a simple setting in which a linear MA array-enabled source (${\\rm{S}}$) transmits a common message to two single-antenna users ${\\rm{U}}_1$ and ${\\rm{U}}_2$. We aim to maximize the minimum rate among these two users, by jointly optimizing the transmit beamforming and antenna positions at ${\\rm{S}}$. Instead of utilizing the widely-used alternating optimization (AO) approach, we reveal, with rigorous proof, that the above two variables can be optimized separately: i) the optimal antenna positions can be firstly determined via the successive convex approximation technique, based on the rule of maximizing the correlation between ${\\rm{S}}$-${\\rm{U}}_1$ and ${\\rm{S}}$-${\\rm{U}}_2$ channels; ii) afterwards, the optimal closed-form transmit beamforming can be derived via simple arguments. Compared to AO, this new approach yields the same performance but reduces the computational complexities significantly. Moreover, it can provide insightful conclusions which are not possible with AO.","sentences":["Movable antenna (MA) technology, which can reconfigure wireless channels by flexibly moving antenna positions in a specified region, has great potential for improving communication performance.","In this paper, we consider a new setup of MAs-enabled multicasting, where we adopt a simple setting in which a linear MA array-enabled source (${\\rm{S}}$) transmits a common message to two single-antenna users ${\\rm{U}}_1$ and ${\\rm{U}}_2$. We aim to maximize the minimum rate among these two users, by jointly optimizing the transmit beamforming and antenna positions at ${\\rm{S}}$. Instead of utilizing the widely-used alternating optimization (AO) approach, we reveal, with rigorous proof, that the above two variables can be optimized separately: i) the optimal antenna positions can be firstly determined via the successive convex approximation technique, based on the rule of maximizing the correlation between ${\\rm{S}}$-${\\rm{U}}_1$ and ${\\rm{S}}$-${\\rm{U}}_2$ channels; ii) afterwards, the optimal closed-form transmit beamforming can be derived via simple arguments.","Compared to AO, this new approach yields the same performance but reduces the computational complexities significantly.","Moreover, it can provide insightful conclusions which are not possible with AO."],"url":"http://arxiv.org/abs/2405.04120v1","category":"cs.IT"}
{"created":"2024-05-07 08:16:13","title":"COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval","abstract":"In this paper, we investigate an open research task of cross-modal retrieval between 3D shapes and textual descriptions. Previous approaches mainly rely on point cloud encoders for feature extraction, which may ignore key inherent features of 3D shapes, including depth, spatial hierarchy, geometric continuity, etc. To address this issue, we propose COM3D, making the first attempt to exploit the cross-view correspondence and cross-modal mining to enhance the retrieval performance. Notably, we augment the 3D features through a scene representation transformer, to generate cross-view correspondence features of 3D shapes, which enrich the inherent features and enhance their compatibility with text matching. Furthermore, we propose to optimize the cross-modal matching process based on the semi-hard negative example mining method, in an attempt to improve the learning efficiency. Extensive quantitative and qualitative experiments demonstrate the superiority of our proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset.","sentences":["In this paper, we investigate an open research task of cross-modal retrieval between 3D shapes and textual descriptions.","Previous approaches mainly rely on point cloud encoders for feature extraction, which may ignore key inherent features of 3D shapes, including depth, spatial hierarchy, geometric continuity, etc.","To address this issue, we propose COM3D, making the first attempt to exploit the cross-view correspondence and cross-modal mining to enhance the retrieval performance.","Notably, we augment the 3D features through a scene representation transformer, to generate cross-view correspondence features of 3D shapes, which enrich the inherent features and enhance their compatibility with text matching.","Furthermore, we propose to optimize the cross-modal matching process based on the semi-hard negative example mining method, in an attempt to improve the learning efficiency.","Extensive quantitative and qualitative experiments demonstrate the superiority of our proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset."],"url":"http://arxiv.org/abs/2405.04103v1","category":"cs.CV"}
{"created":"2024-05-07 07:39:15","title":"Optimizing Language Model's Reasoning Abilities with Weak Supervision","abstract":"While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts. However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow. To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision. In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions. Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions. Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations. However, current reasoning benchmarks typically only include golden-reference answers or rationales. Therefore, we present \\textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities. Our experiments underscore the significance of \\textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors. Our dataset and code will be published soon on \\texttt{Anonymity Link}.","sentences":["While Large Language Models (LLMs) have demonstrated proficiency in handling complex queries, much of the past work has depended on extensively annotated datasets by human experts.","However, this reliance on fully-supervised annotations poses scalability challenges, particularly as models and data requirements grow.","To mitigate this, we explore the potential of enhancing LLMs' reasoning abilities with minimal human supervision.","In this work, we introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of the model using a small collection of annotated questions.","Then it iteratively improves LLMs by learning from the differences in responses from the SFT and unfinetuned models on unlabeled questions.","Our approach provides an efficient approach without relying heavily on extensive human-annotated explanations.","However, current reasoning benchmarks typically only include golden-reference answers or rationales.","Therefore, we present \\textsc{PuzzleBen}, a weakly supervised benchmark that comprises 25,147 complex questions, answers, and human-generated rationales across various domains, such as brainteasers, puzzles, riddles, parajumbles, and critical reasoning tasks.","A unique aspect of our dataset is the inclusion of 10,000 unannotated questions, enabling us to explore utilizing fewer supersized data to boost LLMs' inference capabilities.","Our experiments underscore the significance of \\textsc{PuzzleBen}, as well as the effectiveness of our methodology as a promising direction in future endeavors.","Our dataset and code will be published soon on \\texttt{Anonymity Link}."],"url":"http://arxiv.org/abs/2405.04086v1","category":"cs.CL"}
{"created":"2024-05-07 07:27:28","title":"Logic-Skill Programming: An Optimization-based Approach to Sequential Skill Planning","abstract":"Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks. Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal. To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks. We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions. To solve such programs, we leverage the use of Tensor Train to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence. Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art Reinforcement Learning methods. Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives. The results demonstrate its capability to identify the optimal solution over the full logic and geometric path. The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world.","sentences":["Recent advances in robot skill learning have unlocked the potential to construct task-agnostic skill libraries, facilitating the seamless sequencing of multiple simple manipulation primitives (aka. skills) to tackle significantly more complex tasks.","Nevertheless, determining the optimal sequence for independently learned skills remains an open problem, particularly when the objective is given solely in terms of the final geometric configuration rather than a symbolic goal.","To address this challenge, we propose Logic-Skill Programming (LSP), an optimization-based approach that sequences independently learned skills to solve long-horizon tasks.","We formulate a first-order extension of a mathematical program to optimize the overall cumulative reward of all skills within a plan, abstracted by the sum of value functions.","To solve such programs, we leverage the use of Tensor Train to construct the value function space, and rely on alternations between symbolic search and skill value optimization to find the appropriate skill skeleton and optimal subgoal sequence.","Experimental results indicate that the obtained value functions provide a superior approximation of cumulative rewards compared to state-of-the-art Reinforcement Learning methods.","Furthermore, we validate LSP in three manipulation domains, encompassing both prehensile and non-prehensile primitives.","The results demonstrate its capability to identify the optimal solution over the full logic and geometric path.","The real-robot experiments showcase the effectiveness of our approach to cope with contact uncertainty and external disturbances in the real world."],"url":"http://arxiv.org/abs/2405.04082v1","category":"cs.RO"}
{"created":"2024-05-07 06:29:52","title":"DMOFC: Discrimination Metric-Optimized Feature Compression","abstract":"Feature compression, as an important branch of video coding for machines (VCM), has attracted significant attention and exploration. However, the existing methods mainly focus on intra-feature similarity, such as the Mean Squared Error (MSE) between the reconstructed and original features, while neglecting the importance of inter-feature relationships. In this paper, we analyze the inter-feature relationships, focusing on feature discriminability in machine vision and underscoring its significance in feature compression. To maintain the feature discriminability of reconstructed features, we introduce a discrimination metric for feature compression. The discrimination metric is designed to ensure that the distance between features of the same category is smaller than the distance between features of different categories. Furthermore, we explore the relationship between the discrimination metric and the discriminability of the original features. Experimental results confirm the effectiveness of the proposed discrimination metric and reveal there exists a trade-off between the discrimination metric and the discriminability of the original features.","sentences":["Feature compression, as an important branch of video coding for machines (VCM), has attracted significant attention and exploration.","However, the existing methods mainly focus on intra-feature similarity, such as the Mean Squared Error (MSE) between the reconstructed and original features, while neglecting the importance of inter-feature relationships.","In this paper, we analyze the inter-feature relationships, focusing on feature discriminability in machine vision and underscoring its significance in feature compression.","To maintain the feature discriminability of reconstructed features, we introduce a discrimination metric for feature compression.","The discrimination metric is designed to ensure that the distance between features of the same category is smaller than the distance between features of different categories.","Furthermore, we explore the relationship between the discrimination metric and the discriminability of the original features.","Experimental results confirm the effectiveness of the proposed discrimination metric and reveal there exists a trade-off between the discrimination metric and the discriminability of the original features."],"url":"http://arxiv.org/abs/2405.04044v1","category":"cs.CV"}
{"created":"2024-05-07 06:25:49","title":"Feature Map Convergence Evaluation for Functional Module","abstract":"Autonomous driving perception models are typically composed of multiple functional modules that interact through complex relationships to accomplish environment understanding. However, perception models are predominantly optimized as a black box through end-to-end training, lacking independent evaluation of functional modules, which poses difficulties for interpretability and optimization. Pioneering in the issue, we propose an evaluation method based on feature map analysis to gauge the convergence of model, thereby assessing functional modules' training maturity. We construct a quantitative metric named as the Feature Map Convergence Score (FMCS) and develop Feature Map Convergence Evaluation Network (FMCE-Net) to measure and predict the convergence degree of models respectively. FMCE-Net achieves remarkable predictive accuracy for FMCS across multiple image classification experiments, validating the efficacy and robustness of the introduced approach. To the best of our knowledge, this is the first independent evaluation method for functional modules, offering a new paradigm for the training assessment towards perception models.","sentences":["Autonomous driving perception models are typically composed of multiple functional modules that interact through complex relationships to accomplish environment understanding.","However, perception models are predominantly optimized as a black box through end-to-end training, lacking independent evaluation of functional modules, which poses difficulties for interpretability and optimization.","Pioneering in the issue, we propose an evaluation method based on feature map analysis to gauge the convergence of model, thereby assessing functional modules' training maturity.","We construct a quantitative metric named as the Feature Map Convergence Score (FMCS) and develop Feature Map Convergence Evaluation Network (FMCE-Net) to measure and predict the convergence degree of models respectively.","FMCE-Net achieves remarkable predictive accuracy for FMCS across multiple image classification experiments, validating the efficacy and robustness of the introduced approach.","To the best of our knowledge, this is the first independent evaluation method for functional modules, offering a new paradigm for the training assessment towards perception models."],"url":"http://arxiv.org/abs/2405.04041v1","category":"cs.AI"}
{"created":"2024-05-07 06:09:37","title":"Differentially Private Post-Processing for Fair Regression","abstract":"This paper describes a differentially private post-processing algorithm for learning fair regressors satisfying statistical parity, addressing privacy concerns of machine learning models trained on sensitive data, as well as fairness concerns of their potential to propagate historical biases. Our algorithm can be applied to post-process any given regressor to improve fairness by remapping its outputs. It consists of three steps: first, the output distributions are estimated privately via histogram density estimation and the Laplace mechanism, then their Wasserstein barycenter is computed, and the optimal transports to the barycenter are used for post-processing to satisfy fairness. We analyze the sample complexity of our algorithm and provide fairness guarantee, revealing a trade-off between the statistical bias and variance induced from the choice of the number of bins in the histogram, in which using less bins always favors fairness at the expense of error.","sentences":["This paper describes a differentially private post-processing algorithm for learning fair regressors satisfying statistical parity, addressing privacy concerns of machine learning models trained on sensitive data, as well as fairness concerns of their potential to propagate historical biases.","Our algorithm can be applied to post-process any given regressor to improve fairness by remapping its outputs.","It consists of three steps: first, the output distributions are estimated privately via histogram density estimation and the Laplace mechanism, then their Wasserstein barycenter is computed, and the optimal transports to the barycenter are used for post-processing to satisfy fairness.","We analyze the sample complexity of our algorithm and provide fairness guarantee, revealing a trade-off between the statistical bias and variance induced from the choice of the number of bins in the histogram, in which using less bins always favors fairness at the expense of error."],"url":"http://arxiv.org/abs/2405.04034v1","category":"cs.LG"}
{"created":"2024-05-07 05:59:10","title":"Federated Control in Markov Decision Processes","abstract":"We study problems of federated control in Markov Decision Processes. To solve an MDP with large state space, multiple learning agents are introduced to collaboratively learn its optimal policy without communication of locally collected experience. In our settings, these agents have limited capabilities, which means they are restricted within different regions of the overall state space during the training process. In face of the difference among restricted regions, we firstly introduce concepts of leakage probabilities to understand how such heterogeneity affects the learning process, and then propose a novel communication protocol that we call Federated-Q protocol (FedQ), which periodically aggregates agents' knowledge of their restricted regions and accordingly modifies their learning problems for further training. In terms of theoretical analysis, we justify the correctness of FedQ as a communication protocol, then give a general result on sample complexity of derived algorithms FedQ-X with the RL oracle , and finally conduct a thorough study on the sample complexity of FedQ-SynQ. Specifically, FedQ-X has been shown to enjoy linear speedup in terms of sample complexity when workload is uniformly distributed among agents. Moreover, we carry out experiments in various environments to justify the efficiency of our methods.","sentences":["We study problems of federated control in Markov Decision Processes.","To solve an MDP with large state space, multiple learning agents are introduced to collaboratively learn its optimal policy without communication of locally collected experience.","In our settings, these agents have limited capabilities, which means they are restricted within different regions of the overall state space during the training process.","In face of the difference among restricted regions, we firstly introduce concepts of leakage probabilities to understand how such heterogeneity affects the learning process, and then propose a novel communication protocol that we call Federated-Q protocol (FedQ), which periodically aggregates agents' knowledge of their restricted regions and accordingly modifies their learning problems for further training.","In terms of theoretical analysis, we justify the correctness of FedQ as a communication protocol, then give a general result on sample complexity of derived algorithms FedQ-X with the RL oracle , and finally conduct a thorough study on the sample complexity of FedQ-SynQ. Specifically, FedQ-X has been shown to enjoy linear speedup in terms of sample complexity when workload is uniformly distributed among agents.","Moreover, we carry out experiments in various environments to justify the efficiency of our methods."],"url":"http://arxiv.org/abs/2405.04026v1","category":"stat.ML"}
{"created":"2024-05-07 05:58:44","title":"Optimal Group Fair Classifiers from Linear Post-Processing","abstract":"We propose a post-processing algorithm for fair classification that mitigates model bias under a unified family of group fairness criteria covering statistical parity, equal opportunity, and equalized odds, applicable to multi-class problems and both attribute-aware and attribute-blind settings. It achieves fairness by re-calibrating the output score of the given base model with a \"fairness cost\" -- a linear combination of the (predicted) group memberships. Our algorithm is based on a representation result showing that the optimal fair classifier can be expressed as a linear post-processing of the loss function and the group predictor, derived via using these as sufficient statistics to reformulate the fair classification problem as a linear program. The parameters of the post-processor are estimated by solving the empirical LP. Experiments on benchmark datasets show the efficiency and effectiveness of our algorithm at reducing disparity compared to existing algorithms, including in-processing, especially on larger problems.","sentences":["We propose a post-processing algorithm for fair classification that mitigates model bias under a unified family of group fairness criteria covering statistical parity, equal opportunity, and equalized odds, applicable to multi-class problems and both attribute-aware and attribute-blind settings.","It achieves fairness by re-calibrating the output score of the given base model with a \"fairness cost\" -- a linear combination of the (predicted) group memberships.","Our algorithm is based on a representation result showing that the optimal fair classifier can be expressed as a linear post-processing of the loss function and the group predictor, derived via using these as sufficient statistics to reformulate the fair classification problem as a linear program.","The parameters of the post-processor are estimated by solving the empirical LP.","Experiments on benchmark datasets show the efficiency and effectiveness of our algorithm at reducing disparity compared to existing algorithms, including in-processing, especially on larger problems."],"url":"http://arxiv.org/abs/2405.04025v1","category":"cs.LG"}
{"created":"2024-05-07 05:29:55","title":"An Improved Finite-time Analysis of Temporal Difference Learning with Deep Neural Networks","abstract":"Temporal difference (TD) learning algorithms with neural network function parameterization have well-established empirical success in many practical large-scale reinforcement learning tasks. However, theoretical understanding of these algorithms remains challenging due to the nonlinearity of the action-value approximation. In this paper, we develop an improved non-asymptotic analysis of the neural TD method with a general $L$-layer neural network. New proof techniques are developed and an improved new $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ sample complexity is derived. To our best knowledge, this is the first finite-time analysis of neural TD that achieves an $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ complexity under the Markovian sampling, as opposed to the best known $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ complexity in the existing literature.","sentences":["Temporal difference (TD) learning algorithms with neural network function parameterization have well-established empirical success in many practical large-scale reinforcement learning tasks.","However, theoretical understanding of these algorithms remains challenging due to the nonlinearity of the action-value approximation.","In this paper, we develop an improved non-asymptotic analysis of the neural TD method with a general $L$-layer neural network.","New proof techniques are developed and an improved new $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ sample complexity is derived.","To our best knowledge, this is the first finite-time analysis of neural TD that achieves an $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ complexity under the Markovian sampling, as opposed to the best known $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ complexity in the existing literature."],"url":"http://arxiv.org/abs/2405.04017v1","category":"cs.LG"}
{"created":"2024-05-07 05:12:35","title":"Latency and Energy Minimization in NOMA-Assisted MEC Network: A Federated Deep Reinforcement Learning Approach","abstract":"Multi-access edge computing (MEC) is seen as a vital component of forthcoming 6G wireless networks, aiming to support emerging applications that demand high service reliability and low latency. However, ensuring the ultra-reliable and low-latency performance of MEC networks poses a significant challenge due to uncertainties associated with wireless links, constraints imposed by communication and computing resources, and the dynamic nature of network traffic. Enabling ultra-reliable and low-latency MEC mandates efficient load balancing jointly with resource allocation. In this paper, we investigate the joint optimization problem of offloading decisions, computation and communication resource allocation to minimize the expected weighted sum of delivery latency and energy consumption in a non-orthogonal multiple access (NOMA)-assisted MEC network. Given the formulated problem is a mixed-integer non-linear programming (MINLP), a new multi-agent federated deep reinforcement learning (FDRL) solution based on double deep Q-network (DDQN) is developed to efficiently optimize the offloading strategies across the MEC network while accelerating the learning process of the Internet-of-Thing (IoT) devices. Simulation results show that the proposed FDRL scheme can effectively reduce the weighted sum of delivery latency and energy consumption of IoT devices in the MEC network and outperform the baseline approaches.","sentences":["Multi-access edge computing (MEC) is seen as a vital component of forthcoming 6G wireless networks, aiming to support emerging applications that demand high service reliability and low latency.","However, ensuring the ultra-reliable and low-latency performance of MEC networks poses a significant challenge due to uncertainties associated with wireless links, constraints imposed by communication and computing resources, and the dynamic nature of network traffic.","Enabling ultra-reliable and low-latency MEC mandates efficient load balancing jointly with resource allocation.","In this paper, we investigate the joint optimization problem of offloading decisions, computation and communication resource allocation to minimize the expected weighted sum of delivery latency and energy consumption in a non-orthogonal multiple access (NOMA)-assisted MEC network.","Given the formulated problem is a mixed-integer non-linear programming (MINLP), a new multi-agent federated deep reinforcement learning (FDRL) solution based on double deep Q-network (DDQN) is developed to efficiently optimize the offloading strategies across the MEC network while accelerating the learning process of the Internet-of-Thing (IoT) devices.","Simulation results show that the proposed FDRL scheme can effectively reduce the weighted sum of delivery latency and energy consumption of IoT devices in the MEC network and outperform the baseline approaches."],"url":"http://arxiv.org/abs/2405.04012v1","category":"eess.SY"}
{"created":"2024-05-07 05:06:45","title":"Adjoint Sensitivity Analysis on Multi-Scale Bioprocess Stochastic Reaction Network","abstract":"Motivated by the pressing challenges in the digital twin development for biomanufacturing process, we introduce an adjoint sensitivity analysis (SA) approach to expedite the learning of mechanistic model parameters. In this paper, we consider enzymatic stochastic reaction networks representing a multi-scale bioprocess mechanistic model that allows us to integrate disparate data from diverse production processes and leverage the information from existing macro-kinetic and genome-scale models. To support forward prediction and backward reasoning, we develop a convergent adjoint SA algorithm studying how the perturbations of model parameters and inputs (e.g., initial state) propagate through enzymatic reaction networks and impact on output trajectory predictions. This SA can provide a sample efficient and interpretable way to assess the sensitivities between inputs and outputs accounting for their causal dependencies. Our empirical study underscores the resilience of these sensitivities and illuminates a deeper comprehension of the regulatory mechanisms behind bioprocess through sensitivities.","sentences":["Motivated by the pressing challenges in the digital twin development for biomanufacturing process, we introduce an adjoint sensitivity analysis (SA) approach to expedite the learning of mechanistic model parameters.","In this paper, we consider enzymatic stochastic reaction networks representing a multi-scale bioprocess mechanistic model that allows us to integrate disparate data from diverse production processes and leverage the information from existing macro-kinetic and genome-scale models.","To support forward prediction and backward reasoning, we develop a convergent adjoint SA algorithm studying how the perturbations of model parameters and inputs (e.g., initial state) propagate through enzymatic reaction networks and impact on output trajectory predictions.","This SA can provide a sample efficient and interpretable way to assess the sensitivities between inputs and outputs accounting for their causal dependencies.","Our empirical study underscores the resilience of these sensitivities and illuminates a deeper comprehension of the regulatory mechanisms behind bioprocess through sensitivities."],"url":"http://arxiv.org/abs/2405.04011v1","category":"q-bio.MN"}
{"created":"2024-05-07 04:19:15","title":"Revisiting Kinetic Monte Carlo Algorithms for Time-dependent Processes: from open-loop control to feedback control","abstract":"Simulating stochastic systems with feedback control is challenging due to the complex interplay between the system's dynamics and the feedback-dependent control protocols. We present a single-step-trajectory probability analysis to time-dependent stochastic systems. Based on this analysis, we revisit several time-dependent kinetic Monte Carlo (KMC) algorithms designed for systems under open-loop-control protocols. Our analysis provides an unified alternative proof to these algorithms, summarized into a pedagogical tutorial. Moreover, with the trajectory probability analysis, we present a novel feedback-controlled KMC algorithm that accurately captures the dynamics systems controlled by external signal based on measurements of the system's state. Our method correctly captures the system dynamics and avoids the artificial Zeno effect that arises from incorrectly applying the direct Gillespie algorithm to feedback-controlled systems. This work provides a unified perspective on existing open-loop-control KMC algorithms and also offers a powerful and accurate tool for simulating stochastic systems with feedback control.","sentences":["Simulating stochastic systems with feedback control is challenging due to the complex interplay between the system's dynamics and the feedback-dependent control protocols.","We present a single-step-trajectory probability analysis to time-dependent stochastic systems.","Based on this analysis, we revisit several time-dependent kinetic Monte Carlo (KMC) algorithms designed for systems under open-loop-control protocols.","Our analysis provides an unified alternative proof to these algorithms, summarized into a pedagogical tutorial.","Moreover, with the trajectory probability analysis, we present a novel feedback-controlled KMC algorithm that accurately captures the dynamics systems controlled by external signal based on measurements of the system's state.","Our method correctly captures the system dynamics and avoids the artificial Zeno effect that arises from incorrectly applying the direct Gillespie algorithm to feedback-controlled systems.","This work provides a unified perspective on existing open-loop-control KMC algorithms and also offers a powerful and accurate tool for simulating stochastic systems with feedback control."],"url":"http://arxiv.org/abs/2405.03997v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-07 04:11:03","title":"Research on financial fraud algorithm based on federal learning and big data technology","abstract":"With the deepening of the digitization degree of financial business, financial fraud presents more complex and hidden characteristics, which poses a severe challenge to the risk prevention and control ability of financial institutions. At the same time, the vigorous development of big data technology provides massive potential information resources, and federated learning, as an emerging distributed machine learning paradigm, can realize multi-party data collaborative modeling under the premise of protecting data privacy. This paper firstly elaborates the basic principle, advantages and unique value of federated learning in solving data silos and protecting user privacy. Aiming at the needs of financial fraud detection, this paper discusses the design of federal learning architecture suitable for this scenario, including selecting suitable model type (such as neural network), setting reasonable data partitioning and updating rules. The central theme of the dissertation revolves around the exploration and execution of an algorithm for detecting financial fraud, which is grounded in federated learning methodologies. With a federated learning framework, each participant trains the model locally and exchanges only model parameters rather than raw data, enabling iterative optimization of the global model while protecting data privacy. To ascertain the efficacy and superiority of the suggested algorithm, a meticulous experimental investigation is both devised and executed. A real-world financial fraud dataset is selected to compare the fraud detection performance using traditional centralized learning and federated learning. The findings from the experiments reveal that the federated learning-based financial fraud algorithm achieves a substantial reduction in the likelihood of data privacy breaches without compromising on high detection accuracies.","sentences":["With the deepening of the digitization degree of financial business, financial fraud presents more complex and hidden characteristics, which poses a severe challenge to the risk prevention and control ability of financial institutions.","At the same time, the vigorous development of big data technology provides massive potential information resources, and federated learning, as an emerging distributed machine learning paradigm, can realize multi-party data collaborative modeling under the premise of protecting data privacy.","This paper firstly elaborates the basic principle, advantages and unique value of federated learning in solving data silos and protecting user privacy.","Aiming at the needs of financial fraud detection, this paper discusses the design of federal learning architecture suitable for this scenario, including selecting suitable model type (such as neural network), setting reasonable data partitioning and updating rules.","The central theme of the dissertation revolves around the exploration and execution of an algorithm for detecting financial fraud, which is grounded in federated learning methodologies.","With a federated learning framework, each participant trains the model locally and exchanges only model parameters rather than raw data, enabling iterative optimization of the global model while protecting data privacy.","To ascertain the efficacy and superiority of the suggested algorithm, a meticulous experimental investigation is both devised and executed.","A real-world financial fraud dataset is selected to compare the fraud detection performance using traditional centralized learning and federated learning.","The findings from the experiments reveal that the federated learning-based financial fraud algorithm achieves a substantial reduction in the likelihood of data privacy breaches without compromising on high detection accuracies."],"url":"http://arxiv.org/abs/2405.03992v1","category":"cs.CE"}
{"created":"2024-05-07 03:55:57","title":"Navigating Chemical Space with Latent Flows","abstract":"Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery. In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings. Codes and demos are publicly available on GitHub at https://github.com/garywei944/ChemFlow.","sentences":["Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules.","However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery.","In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows.","We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity.","Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors.","We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings.","Codes and demos are publicly available on GitHub at https://github.com/garywei944/ChemFlow."],"url":"http://arxiv.org/abs/2405.03987v2","category":"cs.LG"}
{"created":"2024-05-07 03:05:37","title":"Contextualization with SPLADE for High Recall Retrieval","abstract":"High Recall Retrieval (HRR), such as eDiscovery and medical systematic review, is a search problem that optimizes the cost of retrieving most relevant documents in a given collection. Iterative approaches, such as iterative relevance feedback and uncertainty sampling, are shown to be effective under various operational scenarios. Despite neural models demonstrating success in other text-related tasks, linear models such as logistic regression, in general, are still more effective and efficient in HRR since the model is trained and retrieves documents from the same fixed collection. In this work, we leverage SPLADE, an efficient retrieval model that transforms documents into contextualized sparse vectors, for HRR. Our approach combines the best of both worlds, leveraging both the contextualization from pretrained language models and the efficiency of linear models. It reduces 10% and 18% of the review cost in two HRR evaluation collections under a one-phase review workflow with a target recall of 80%. The experiment is implemented with TARexp and is available at https://github.com/eugene-yang/LSR-for-TAR.","sentences":["High Recall Retrieval (HRR), such as eDiscovery and medical systematic review, is a search problem that optimizes the cost of retrieving most relevant documents in a given collection.","Iterative approaches, such as iterative relevance feedback and uncertainty sampling, are shown to be effective under various operational scenarios.","Despite neural models demonstrating success in other text-related tasks, linear models such as logistic regression, in general, are still more effective and efficient in HRR since the model is trained and retrieves documents from the same fixed collection.","In this work, we leverage SPLADE, an efficient retrieval model that transforms documents into contextualized sparse vectors, for HRR.","Our approach combines the best of both worlds, leveraging both the contextualization from pretrained language models and the efficiency of linear models.","It reduces 10% and 18% of the review cost in two HRR evaluation collections under a one-phase review workflow with a target recall of 80%.","The experiment is implemented with TARexp and is available at https://github.com/eugene-yang/LSR-for-TAR."],"url":"http://arxiv.org/abs/2405.03972v1","category":"cs.IR"}
{"created":"2024-05-07 02:54:31","title":"SwiftRL: Towards Efficient Reinforcement Learning on Real Processing-In-Memory Systems","abstract":"Reinforcement Learning (RL) trains agents to learn optimal behavior by maximizing reward signals from experience datasets. However, RL training often faces memory limitations, leading to execution latencies and prolonged training times. To overcome this, SwiftRL explores Processing-In-Memory (PIM) architectures to accelerate RL workloads. We achieve near-linear performance scaling by implementing RL algorithms like Tabular Q-learning and SARSA on UPMEM PIM systems and optimizing for hardware. Our experiments on OpenAI GYM environments using UPMEM hardware demonstrate superior performance compared to CPU and GPU implementations.","sentences":["Reinforcement Learning (RL) trains agents to learn optimal behavior by maximizing reward signals from experience datasets.","However, RL training often faces memory limitations, leading to execution latencies and prolonged training times.","To overcome this, SwiftRL explores Processing-In-Memory (PIM) architectures to accelerate RL workloads.","We achieve near-linear performance scaling by implementing RL algorithms like Tabular Q-learning and SARSA on UPMEM PIM systems and optimizing for hardware.","Our experiments on OpenAI GYM environments using UPMEM hardware demonstrate superior performance compared to CPU and GPU implementations."],"url":"http://arxiv.org/abs/2405.03967v1","category":"cs.LG"}
{"created":"2024-05-07 02:49:59","title":"ERATTA: Extreme RAG for Table To Answers with Large Language Models","abstract":"Large language models (LLMs) with residual augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. However, the choice of use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval and custom prompting for question answering capabilities from data tables that are highly varying and large in size. Our system is tuned to extract information from Enterprise-level data products and furnish real time responses under 10 seconds. One prompt manages user-to-data authentication followed by three prompts to route, fetch data and generate a customizable prompt natural language responses. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.","sentences":["Large language models (LLMs) with residual augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past.","However, the choice of use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches.","In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user query routing, data retrieval and custom prompting for question answering capabilities from data tables that are highly varying and large in size.","Our system is tuned to extract information from Enterprise-level data products and furnish real time responses under 10 seconds.","One prompt manages user-to-data authentication followed by three prompts to route, fetch data and generate a customizable prompt natural language responses.","Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses.","Our proposed system and scoring metrics achieve >90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains.","Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs."],"url":"http://arxiv.org/abs/2405.03963v1","category":"cs.AI"}
{"created":"2024-05-07 02:49:21","title":"AdsorbDiff: Adsorbate Placement via Conditional Denoising Diffusion","abstract":"Determining the optimal configuration of adsorbates on a slab (adslab) is pivotal in the exploration of novel catalysts across diverse applications. Traditionally, the quest for the lowest energy adslab configuration involves placing the adsorbate onto the slab followed by an optimization process. Prior methodologies have relied on heuristics, problem-specific intuitions, or brute-force approaches to guide adsorbate placement. In this work, we propose a novel framework for adsorbate placement using denoising diffusion. The model is designed to predict the optimal adsorbate site and orientation corresponding to the lowest energy configuration. Further, we have an end-to-end evaluation framework where diffusion-predicted adslab configuration is optimized with a pretrained machine learning force field and finally evaluated with Density Functional Theory (DFT). Our findings demonstrate an acceleration of up to 5x or 3.5x improvement in accuracy compared to the previous best approach. Given the novelty of this framework and application, we provide insights into the impact of pre-training, model architectures, and conduct extensive experiments to underscore the significance of this approach.","sentences":["Determining the optimal configuration of adsorbates on a slab (adslab) is pivotal in the exploration of novel catalysts across diverse applications.","Traditionally, the quest for the lowest energy adslab configuration involves placing the adsorbate onto the slab followed by an optimization process.","Prior methodologies have relied on heuristics, problem-specific intuitions, or brute-force approaches to guide adsorbate placement.","In this work, we propose a novel framework for adsorbate placement using denoising diffusion.","The model is designed to predict the optimal adsorbate site and orientation corresponding to the lowest energy configuration.","Further, we have an end-to-end evaluation framework where diffusion-predicted adslab configuration is optimized with a pretrained machine learning force field and finally evaluated with Density Functional Theory (DFT).","Our findings demonstrate an acceleration of up to 5x or 3.5x improvement in accuracy compared to the previous best approach.","Given the novelty of this framework and application, we provide insights into the impact of pre-training, model architectures, and conduct extensive experiments to underscore the significance of this approach."],"url":"http://arxiv.org/abs/2405.03962v1","category":"cs.LG"}
{"created":"2024-05-07 02:17:34","title":"Entanglement swapping via lossy channels using photon-number-encoded states","abstract":"Entanglement shared between distant parties is a key resource in quantum networks. However, photon losses in quantum channels significantly reduce the success probability of entanglement sharing, which scales quadratically with the channel transmission. Quantum repeaters using entanglement swapping can mitigate this effect, but usually require high-performance photonic quantum memories to synchronize photonic qubits. In this work, we theoretically and experimentally investigate an entanglement swapping protocol using photon-number-encoded states that can effectively alleviate quantum channel losses without requiring photonic quantum memories. We demonstrate that the protocol exhibits a success probability scaling linearly with the channel transmission. Furthermore, we show that while unbalanced channel losses can degrade the shared entanglement, this effect can be compensated by optimally adjusting the initial entangled states. Our results highlight the potential of photon-number encoding for realizing robust entanglement distribution in lossy quantum networks.","sentences":["Entanglement shared between distant parties is a key resource in quantum networks.","However, photon losses in quantum channels significantly reduce the success probability of entanglement sharing, which scales quadratically with the channel transmission.","Quantum repeaters using entanglement swapping can mitigate this effect, but usually require high-performance photonic quantum memories to synchronize photonic qubits.","In this work, we theoretically and experimentally investigate an entanglement swapping protocol using photon-number-encoded states that can effectively alleviate quantum channel losses without requiring photonic quantum memories.","We demonstrate that the protocol exhibits a success probability scaling linearly with the channel transmission.","Furthermore, we show that while unbalanced channel losses can degrade the shared entanglement, this effect can be compensated by optimally adjusting the initial entangled states.","Our results highlight the potential of photon-number encoding for realizing robust entanglement distribution in lossy quantum networks."],"url":"http://arxiv.org/abs/2405.03951v1","category":"quant-ph"}
{"created":"2024-05-07 02:12:17","title":"The Fault in Our Recommendations: On the Perils of Optimizing the Measurable","abstract":"Recommendation systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end, data on engagement is collected and used. Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement. However, the engagement signals are often only a crude proxy for utility, as data on the latter is rarely collected or available. This paper explores the following question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on utility? If so, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring ``popular'' content, and a minority favoring ``niche'' content. The system initially lacks knowledge of individual user preferences but can learn them through observations of users' choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. As the platform becomes more forward-looking, our utility-aware policy achieves the best of both worlds: near-optimal utility and near-optimal engagement simultaneously. Our study elucidates an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.","sentences":["Recommendation systems are widespread, and through customized recommendations, promise to match users with options they will like.","To that end, data on engagement is collected and used.","Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement.","However, the engagement signals are often only a crude proxy for utility, as data on the latter is rarely collected or available.","This paper explores the following question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on utility?","If so, how can one improve utility which is seldom measured?","To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set.","Our model accounts for user heterogeneity, with the majority preferring ``popular'' content, and a minority favoring ``niche'' content.","The system initially lacks knowledge of individual user preferences but can learn them through observations of users' choices over time.","Our theoretical and numerical analysis demonstrate that optimizing for engagement can lead to significant utility losses.","Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content.","As the platform becomes more forward-looking, our utility-aware policy achieves the best of both worlds: near-optimal utility and near-optimal engagement simultaneously.","Our study elucidates an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in engagement.","By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement."],"url":"http://arxiv.org/abs/2405.03948v1","category":"cs.IR"}
{"created":"2024-05-07 01:17:30","title":"Shape optimization for high efficiency metasurfaces: theory and implementation","abstract":"Complex non-local behavior makes designing high efficiency and multifunctional metasurfaces a significant challenge. While using libraries of meta-atoms provide a simple and fast implementation methodology, pillar to pillar interaction often imposes performance limitations. On the other extreme, inverse design based on topology optimization leverages non-local coupling to achieve high efficiency, but leads to complex and difficult to fabricate structures. In this paper, we demonstrate numerically and experimentally a shape optimization method that enables high efficiency metasurfaces while providing direct control of the structure complexity. The proposed method provides a path towards manufacturability of inverse-designed high efficiency metasurfaces.","sentences":["Complex non-local behavior makes designing high efficiency and multifunctional metasurfaces a significant challenge.","While using libraries of meta-atoms provide a simple and fast implementation methodology, pillar to pillar interaction often imposes performance limitations.","On the other extreme, inverse design based on topology optimization leverages non-local coupling to achieve high efficiency, but leads to complex and difficult to fabricate structures.","In this paper, we demonstrate numerically and experimentally a shape optimization method that enables high efficiency metasurfaces while providing direct control of the structure complexity.","The proposed method provides a path towards manufacturability of inverse-designed high efficiency metasurfaces."],"url":"http://arxiv.org/abs/2405.03930v1","category":"physics.optics"}
{"created":"2024-05-07 01:08:32","title":"Generalized Nash equilibrium problems with quasi-linear constraints","abstract":"We study generalized Nash equilibrium problems (GNEPs) such that objectives are polynomial functions, and each player's constraints are linear in their own strategy. For such GNEPs, the KKT sets can be represented as unions of simpler sets by Carath\\'{e}odory's theorem. We give a convenient representation for KKT sets using partial Lagrange multiplier expressions. This produces a set of branch polynomial optimization problems, which can be efficiently solved by Moment-SOS relaxations. By doing this, we can compute all generalized Nash equilibria or detect their nonexistence. Numerical experiments are also provided to demonstrate the computational efficiency.","sentences":["We study generalized Nash equilibrium problems (GNEPs) such that objectives are polynomial functions, and each player's constraints are linear in their own strategy.","For such GNEPs, the KKT sets can be represented as unions of simpler sets by Carath\\'{e}odory's theorem.","We give a convenient representation for KKT sets using partial Lagrange multiplier expressions.","This produces a set of branch polynomial optimization problems, which can be efficiently solved by Moment-SOS relaxations.","By doing this, we can compute all generalized Nash equilibria or detect their nonexistence.","Numerical experiments are also provided to demonstrate the computational efficiency."],"url":"http://arxiv.org/abs/2405.03926v1","category":"math.OC"}
{"created":"2024-05-07 00:38:01","title":"A design method of an ultra wideband and easy-to-array Magic-T: A 6-14 GHz scaled model for a mm/submm camera","abstract":"We established a design method for a Magic-T with a single-layer dielectric/metal structure suitable for both wideband and multi-element applications for millimeter and submillimeter wave imaging observations. The design method was applied to a Magic-T with a coupled-line, stubs, and single-stage impedance transformers in a frequency-scaled model (6-14 GHz) that is relatively easy to demonstrate through manufacturing and evaluation. The major problem is that using the conventional perfect matching condition for a coupled-line alone produces an impractically large width coplanar coupled-line (CPCL) to satisfy the desired bandwidth ratio. In our study, by removing this constraint and optimizing impedances utilizing a circuit simulator with high computation speed, we found a solution with a $\\sim$ 180 $\\rm \\mu$m wide CPCL, which is approximately an order of magnitude smaller than the conventional analytical solution. Furthermore, considering the effect of transition discontinuities in the transmission lines, we optimized the line length and obtained a design solution with return loss < -20 dB, amplitude imbalance < 0.1 dB, and phase imbalance < 0.5$^\\circ$ from 6.1 GHz to 14.1 GHz.","sentences":["We established a design method for a Magic-T with a single-layer dielectric/metal structure suitable for both wideband and multi-element applications for millimeter and submillimeter wave imaging observations.","The design method was applied to a Magic-T with a coupled-line, stubs, and single-stage impedance transformers in a frequency-scaled model (6-14 GHz) that is relatively easy to demonstrate through manufacturing and evaluation.","The major problem is that using the conventional perfect matching condition for a coupled-line alone produces an impractically large width coplanar coupled-line (CPCL) to satisfy the desired bandwidth ratio.","In our study, by removing this constraint and optimizing impedances utilizing a circuit simulator with high computation speed, we found a solution with a $\\sim$ 180 $\\rm \\mu$m wide CPCL, which is approximately an order of magnitude smaller than the conventional analytical solution.","Furthermore, considering the effect of transition discontinuities in the transmission lines, we optimized the line length and obtained a design solution with return loss < -20 dB, amplitude imbalance < 0.1 dB, and phase imbalance < 0.5$^\\circ$ from 6.1 GHz to 14.1 GHz."],"url":"http://arxiv.org/abs/2405.03919v2","category":"astro-ph.IM"}
{"created":"2024-05-07 00:24:54","title":"Robust Optimization for Spot Scanning Proton Therapy based on Dose-Linear Energy Transfer (LET) Volume Constraints","abstract":"Purpose: Historically, spot scanning proton therapy (SSPT) treatment planning utilizes dose volume constraints and linear-energy-transfer (LET) volume constraints separately to balance tumor control and organs-at-risk (OARs) protection. We propose a novel dose-LET volume constraint (DLVC)-based robust optimization (DLVCRO) method for SSPT in treating prostate cancer to obtain a desirable joint dose and LET distribution to minimize adverse events (AEs).   Methods: DLVCRO treats DLVC as soft constraints controlling the joint distribution of dose and LET. Ten prostate cancer patients were included with rectum and bladder as OARs. DLVCRO was compared with the conventional robust optimization (RO) method using the worst-case analysis method. Besides the dose-volume histogram (DVH) indices, the analogous LETVH and extra-biological-dose (xBD)-volume histogram indices were also used. The Wilcoxon signed rank test was used to measure statistical significance.   Results: In nominal scenario, DLVCRO significantly improved dose, LET and xBD distributions to protect OARs (rectum: V70Gy: 3.07\\% vs. 2.90\\%, p = .0063, RO vs. DLVCRO; $\\text{LET}_{\\max}$ (keV/um): 11.53 vs. 9.44, p = .0101; $\\text{xBD}_{\\max}$ (Gy$\\cdot$keV/um): 420.55 vs. 398.79, p = .0086; bladder: V65Gy: 4.82\\% vs. 4.61\\%, p = .0032; $\\text{LET}_{\\max}$ 8.97 vs. 7.51, p = .0047; $\\text{xBD}_{\\max}$ 490.11 vs. 476.71, p = .0641). The physical dose distributions in targets are comparable (D2%: 98.57\\% vs. 98.39\\%; p = .0805; CTV D2% - D98%: 7.10\\% vs. 7.75\\%, p = .4624). In the worst-case scenario, DLVCRO robustly enhanced OAR while maintaining the similar plan robustness in target dose coverage and homogeneity.   Conclusion: DLVCRO upgrades 2D DVH-based to 3D DLVH-based treatment planning to adjust dose/LET distributions simultaneously and robustly. DLVCRO is potentially a powerful tool to improve patient outcomes in SSPT.","sentences":["Purpose: Historically, spot scanning proton therapy (SSPT) treatment planning utilizes dose volume constraints and linear-energy-transfer (LET) volume constraints separately to balance tumor control and organs-at-risk (OARs) protection.","We propose a novel dose-LET volume constraint (DLVC)-based robust optimization (DLVCRO) method for SSPT in treating prostate cancer to obtain a desirable joint dose and LET distribution to minimize adverse events (AEs).   ","Methods: DLVCRO treats DLVC as soft constraints controlling the joint distribution of dose and LET.","Ten prostate cancer patients were included with rectum and bladder as OARs.","DLVCRO was compared with the conventional robust optimization (RO) method using the worst-case analysis method.","Besides the dose-volume histogram (DVH) indices, the analogous LETVH and extra-biological-dose (xBD)-volume histogram indices were also used.","The Wilcoxon signed rank test was used to measure statistical significance.   ","Results:","In nominal scenario, DLVCRO significantly improved dose, LET and xBD distributions to protect OARs (rectum: V70Gy:","3.07\\% vs. 2.90\\%, p = .0063, RO vs. DLVCRO; $\\text{LET}_{\\max}$ (keV/um): 11.53 vs. 9.44, p = .0101; $\\text{xBD}_{\\max}$ (Gy$\\cdot$keV/um): 420.55 vs. 398.79, p = .0086; bladder: V65Gy: 4.82\\% vs. 4.61\\%, p = .0032; $\\text{LET}_{\\max}$ 8.97 vs. 7.51, p = .0047; $\\text{xBD}_{\\max}$ 490.11 vs. 476.71, p = .0641).","The physical dose distributions in targets are comparable (D2%: 98.57\\% vs. 98.39\\%; p = .0805; CTV D2% - D98%: 7.10\\% vs. 7.75\\%, p = .4624).","In the worst-case scenario, DLVCRO robustly enhanced OAR while maintaining the similar plan robustness in target dose coverage and homogeneity.   ","Conclusion: DLVCRO upgrades 2D DVH-based to 3D DLVH-based treatment planning to adjust dose/LET distributions simultaneously and robustly.","DLVCRO is potentially a powerful tool to improve patient outcomes in SSPT."],"url":"http://arxiv.org/abs/2405.03916v1","category":"physics.med-ph"}
{"created":"2024-05-07 00:22:13","title":"Digital Twin Calibration for Biological System-of-Systems: Cell Culture Manufacturing Process","abstract":"Biomanufacturing innovation relies on an efficient design of experiments (DoE) to optimize processes and product quality. Traditional DoE methods, ignoring the underlying bioprocessing mechanisms, often suffer from a lack of interpretability and sample efficiency. This limitation motivates us to create a new optimal learning approach that can guide a sequential DoEs for digital twin model calibration. In this study, we consider a multi-scale mechanistic model for cell culture process, also known as Biological Systems-of-Systems (Bio-SoS), as our digital twin. This model with modular design, composed of sub-models, allows us to integrate data across various production processes. To calibrate the Bio-SoS digital twin, we evaluate the mean squared error of model prediction and develop a computational approach to quantify the impact of parameter estimation error of individual sub-models on the prediction accuracy of digital twin, which can guide sample-efficient and interpretable DoEs.","sentences":["Biomanufacturing innovation relies on an efficient design of experiments (DoE) to optimize processes and product quality.","Traditional DoE methods, ignoring the underlying bioprocessing mechanisms, often suffer from a lack of interpretability and sample efficiency.","This limitation motivates us to create a new optimal learning approach that can guide a sequential DoEs for digital twin model calibration.","In this study, we consider a multi-scale mechanistic model for cell culture process, also known as Biological Systems-of-Systems (Bio-SoS), as our digital twin.","This model with modular design, composed of sub-models, allows us to integrate data across various production processes.","To calibrate the Bio-SoS digital twin, we evaluate the mean squared error of model prediction and develop a computational approach to quantify the impact of parameter estimation error of individual sub-models on the prediction accuracy of digital twin, which can guide sample-efficient and interpretable DoEs."],"url":"http://arxiv.org/abs/2405.03913v1","category":"q-bio.QM"}
{"created":"2024-05-06 23:52:20","title":"Deterministic Expander Routing: Faster and More Versatile","abstract":"We consider the expander routing problem formulated by Ghaffari, Kuhn, and Su (PODC 2017), where the goal is to route all the tokens to their destinations given that each vertex is the source and the destination of at most $\\deg(v)$ tokens. They developed $\\textit{randomized algorithms}$ that solve this problem in $\\text{poly}(\\phi^{-1}) \\cdot 2^{O(\\sqrt{\\log n \\log \\log n})}$ rounds in the $\\textsf{CONGEST}$ model, where $\\phi$ is the conductance of the graph. Later, Ghaffari and Li (DISC 2018) gave an improved algorithm. However, both algorithms are randomized, which means that all the resulting applications are also randomized. Recently, Chang and Saranurak (FOCS 2020) gave a deterministic algorithm that solves an expander routing instance in $2^{O(\\log^{2/3} n \\cdot \\log^{1/3} \\log n)}$ rounds. The deterministic algorithm is less efficient and does not allow preprocessing/query tradeoffs, which precludes the de-randomization of algorithms that require this feature, such as the $k$-clique enumeration algorithm in general graphs.   The main contribution of our work is a new deterministic expander routing algorithm that not only matches the randomized bound of [GKS 2017] but also allows preprocessing/query tradeoffs. Our algorithm solves a single instance of routing query in $2^{{O}(\\sqrt{\\log n \\cdot \\log \\log n})}$ rounds. Our algorithm achieves the following preprocessing and query tradeoffs: For $0 < \\epsilon < 1$, we can answer every routing query in $\\log^{O(1/\\epsilon)} n$ rounds at the cost of a $(n^{O(\\epsilon)} + \\log^{O(1/\\epsilon)} n)$-round preprocessing procedure. Combining this with the approach of Censor-Hillel, Leitersdorf, and Vulakh (PODC 2022), we obtain a near-optimal $\\tilde{O}(n^{1-2/k})$-round deterministic algorithm for $k$-clique enumeration in general graphs, improving the previous state-of-the-art $n^{1-2/k+o(1)}$.","sentences":["We consider the expander routing problem formulated by Ghaffari, Kuhn, and Su (PODC 2017), where the goal is to route all the tokens to their destinations given that each vertex is the source and the destination of at most $\\deg(v)$ tokens.","They developed $\\textit{randomized algorithms}$ that solve this problem in $\\text{poly}(\\phi^{-1})","\\cdot 2^{O(\\sqrt{\\log n \\log \\log n})}$ rounds in the $\\textsf{CONGEST}$ model, where $\\phi$ is the conductance of the graph.","Later, Ghaffari and Li (DISC 2018) gave an improved algorithm.","However, both algorithms are randomized, which means that all the resulting applications are also randomized.","Recently, Chang and Saranurak (FOCS 2020) gave a deterministic algorithm that solves an expander routing instance in $2^{O(\\log^{2/3} n \\cdot \\log^{1/3} \\log n)}$ rounds.","The deterministic algorithm is less efficient and does not allow preprocessing/query tradeoffs, which precludes the de-randomization of algorithms that require this feature, such as the $k$-clique enumeration algorithm in general graphs.   ","The main contribution of our work is a new deterministic expander routing algorithm that not only matches the randomized bound of [GKS 2017] but also allows preprocessing/query tradeoffs.","Our algorithm solves a single instance of routing query in $2^{{O}(\\sqrt{\\log n \\cdot \\log \\log n})}$ rounds.","Our algorithm achieves the following preprocessing and query tradeoffs: For $0 < \\epsilon < 1$, we can answer every routing query in $\\log^{O(1/\\epsilon)} n$ rounds at the cost of a $(n^{O(\\epsilon)} + \\log^{O(1/\\epsilon)} n)$-round preprocessing procedure.","Combining this with the approach of Censor-Hillel, Leitersdorf, and Vulakh (PODC 2022), we obtain a near-optimal $\\tilde{O}(n^{1-2/k})$-round deterministic algorithm for $k$-clique enumeration in general graphs, improving the previous state-of-the-art $n^{1-2/k+o(1)}$."],"url":"http://arxiv.org/abs/2405.03908v1","category":"cs.DC"}
{"created":"2024-05-06 23:36:03","title":"Transformer models classify random numbers","abstract":"Random numbers are incredibly important in a variety of fields, and the need for their validation remains important. A Quantum Random Number Generator (QRNG) can theoretically generate truly random numbers however this does not remove the need to thoroughly test their randomness. Generally, the task of validating random numbers has been delegated to different statistical tests such as the tests from the NIST Statistical Test Suite (STS) which are often slow and only perform one task at a time. Our work presents a deep learning model that utilizes the transformer architecture to encode some of the tests from the NIST STS in a single model that also runs much faster. This model performs multi-label classification on these tests and outputs the probability of passing each statistical test that it encodes. We perform a thorough hyper-parameter optimization to converge on the best possible model and as a result, achieve a high degree of accuracy with a sample f1 score of above 0.9.","sentences":["Random numbers are incredibly important in a variety of fields, and the need for their validation remains important.","A Quantum Random Number Generator (QRNG) can theoretically generate truly random numbers however this does not remove the need to thoroughly test their randomness.","Generally, the task of validating random numbers has been delegated to different statistical tests such as the tests from the NIST Statistical Test Suite (STS) which are often slow and only perform one task at a time.","Our work presents a deep learning model that utilizes the transformer architecture to encode some of the tests from the NIST STS in a single model that also runs much faster.","This model performs multi-label classification on these tests and outputs the probability of passing each statistical test that it encodes.","We perform a thorough hyper-parameter optimization to converge on the best possible model and as a result, achieve a high degree of accuracy with a sample f1 score of above 0.9."],"url":"http://arxiv.org/abs/2405.03904v1","category":"cs.LG"}
{"created":"2024-05-06 23:06:34","title":"Modeling and performance analysis of Implicit Electric Field Conjugation with two deformable mirrors applied to the Roman Coronagraph","abstract":"High-order wavefront sensing and control (HOWFSC) is key to create a dark hole region within the coronagraphic image plane where high contrasts are achieved. The Roman Coronagraph is expected to perform its HOWFSC with a ground-in-the-loop scheme due to the computational complexity of the Electric Field Conjugation (EFC) algorithm. This scheme provides the flexibility to alter the HOWFSC algorithm for given science objectives. The baseline HOWFSC scheme involves running EFC while observing a bright star such as {\\zeta} Puppis to create the initial dark hole followed by a slew to the science target. The new implicit EFC (iEFC) algorithm removes the optical diffraction model from the controller, making the final contrast independent of model accuracy. While previously demonstrated with a single DM, iEFC is extended to two deformable mirror systems in order to create annular dark holes. The algorithm is then applied to the Wide-Field-of-View Shaped Pupil Coronagraph (SPC-WFOV) mode designed for the Roman Space Telescope using end-to-end physical optics models. Initial monochromatic simulations demonstrate the efficacy of iEFC as well as the optimal choice of modes for the SPC-WFOV instrument. Further simulations with a 3.6% wavefront control bandpass and a broader 10% bandpass then demonstrate that iEFC can be used in broadband scenarios to achieve contrasts below 1E-8 with Roman. Finally, an EMCCD model is implemented to estimate calibration times and predict the controller's performance. Here, 1E-8 contrasts are achieved with a calibration time of about 6.8 hours assuming the reference star is {\\zeta} Puppis. The results here indicate that iEFC can be a valid HOWFSC method that can mitigate the risk of model errors associated with space-borne coronagraphs, but to maximize iEFC performance, lengthy calibration times will be required to mitigate the noise accumulated during calibration.","sentences":["High-order wavefront sensing and control (HOWFSC) is key to create a dark hole region within the coronagraphic image plane where high contrasts are achieved.","The Roman Coronagraph is expected to perform its HOWFSC with a ground-in-the-loop scheme due to the computational complexity of the Electric Field Conjugation (EFC) algorithm.","This scheme provides the flexibility to alter the HOWFSC algorithm for given science objectives.","The baseline HOWFSC scheme involves running EFC while observing a bright star such as {\\zeta} Puppis to create the initial dark hole followed by a slew to the science target.","The new implicit EFC (iEFC) algorithm removes the optical diffraction model from the controller, making the final contrast independent of model accuracy.","While previously demonstrated with a single DM, iEFC is extended to two deformable mirror systems in order to create annular dark holes.","The algorithm is then applied to the Wide-Field-of-View Shaped Pupil Coronagraph (SPC-WFOV) mode designed for the Roman Space Telescope using end-to-end physical optics models.","Initial monochromatic simulations demonstrate the efficacy of iEFC as well as the optimal choice of modes for the SPC-WFOV instrument.","Further simulations with a 3.6% wavefront control bandpass and a broader 10% bandpass then demonstrate that iEFC can be used in broadband scenarios to achieve contrasts below 1E-8 with Roman.","Finally, an EMCCD model is implemented to estimate calibration times and predict the controller's performance.","Here, 1E-8 contrasts are achieved with a calibration time of about 6.8 hours assuming the reference star is {\\zeta} Puppis.","The results here indicate that iEFC can be a valid HOWFSC method that can mitigate the risk of model errors associated with space-borne coronagraphs, but to maximize iEFC performance, lengthy calibration times will be required to mitigate the noise accumulated during calibration."],"url":"http://arxiv.org/abs/2405.03899v1","category":"astro-ph.IM"}
{"created":"2024-05-06 23:01:33","title":"Quantum sensing in the fractional Fourier domain","abstract":"Certain quantum sensing protocols rely on qubits that are initialized, coherently driven in the presence of a stimulus to be measured, then read out. Most widely employed pulse sequences used to drive sensing qubits act locally in either the time or frequency domain. We introduce a generalized set of sequences that effect a measurement in any fractional Fourier domain, i.e. along a linear trajectory of arbitrary angle through the time-frequency plane. Using an ensemble of nitrogen-vacancy centers we experimentally demonstrate advantages in sensing signals with time-varying spectra.","sentences":["Certain quantum sensing protocols rely on qubits that are initialized, coherently driven in the presence of a stimulus to be measured, then read out.","Most widely employed pulse sequences used to drive sensing qubits act locally in either the time or frequency domain.","We introduce a generalized set of sequences that effect a measurement in any fractional Fourier domain, i.e. along a linear trajectory of arbitrary angle through the time-frequency plane.","Using an ensemble of nitrogen-vacancy centers we experimentally demonstrate advantages in sensing signals with time-varying spectra."],"url":"http://arxiv.org/abs/2405.03896v1","category":"quant-ph"}
{"created":"2024-05-08 17:57:39","title":"You Only Cache Once: Decoder-Decoder Architectures for Language Models","abstract":"We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes. Code is available at https://aka.ms/YOCO.","sentences":["We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once.","It consists of two components, i.e., a cross-decoder stacked upon a self-decoder.","The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention.","The overall model behaves like a decoder-only Transformer, although YOCO only caches once.","The design substantially reduces GPU memory demands, yet retains global attention capability.","Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage.","Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens.","We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy.","The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes.","Code is available at https://aka.ms/YOCO."],"url":"http://arxiv.org/abs/2405.05254v1","category":"cs.CL"}
{"created":"2024-05-08 17:49:32","title":"Neutrinos and gamma rays from beta decays in an active galactic nucleus NGC 1068 jet","abstract":"We show that TeV neutrinos detected from the nearby active galaxy NGC 1068 can be explained by the beta decays of neutrons produced in photodisintegration of nuclei on ultraviolet photons in the jet. The photodisintergation of nuclei occurs at energies above a few PeV, which explains the 1-100 TeV energies of the observed neutrinos. The gamma-ray flux accompanying the beta decays is expected to be much lower than the neutrino flux, and it agrees well with the gamma-ray observations of NGC 1068. This scenario can be applicable to other jetted Seyfert galaxies such as NGC 4151. The flavor ratio studies could be a test of this beta decay jet scenario for gamma-ray deficit neutrino sources.","sentences":["We show that TeV neutrinos detected from the nearby active galaxy NGC 1068 can be explained by the beta decays of neutrons produced in photodisintegration of nuclei on ultraviolet photons in the jet.","The photodisintergation of nuclei occurs at energies above a few PeV, which explains the 1-100 TeV energies of the observed neutrinos.","The gamma-ray flux accompanying the beta decays is expected to be much lower than the neutrino flux, and it agrees well with the gamma-ray observations of NGC 1068.","This scenario can be applicable to other jetted Seyfert galaxies such as NGC 4151.","The flavor ratio studies could be a test of this beta decay jet scenario for gamma-ray deficit neutrino sources."],"url":"http://arxiv.org/abs/2405.05247v1","category":"astro-ph.HE"}
{"created":"2024-05-08 15:45:20","title":"On equivalence of the Mellin-Barnes and the Givental integral representations of the Whittaker functions","abstract":"We construct an integral transformation intertwining the Gelfand-Tsetlin and the (modified) Gauss-Givental realizations of principle series representations of gl(3). This provides a direct identification of the corresponding integral representations for the gl(3)-Whittaker function. The construction essentially uses integral identities due to Barnes and Gustafson thus providing a basis for their representation theory interpretation. The result of this paper might be useful for constructing the explicit analytic realization of the mirror symmetry map in the case of the flag manifold GL(3)/B.","sentences":["We construct an integral transformation intertwining the Gelfand-Tsetlin and the (modified) Gauss-Givental realizations of principle series representations of gl(3).","This provides a direct identification of the corresponding integral representations for the gl(3)-Whittaker function.","The construction essentially uses integral identities due to Barnes and Gustafson thus providing a basis for their representation theory interpretation.","The result of this paper might be useful for constructing the explicit analytic realization of the mirror symmetry map in the case of the flag manifold GL(3)/B."],"url":"http://arxiv.org/abs/2405.05152v1","category":"math.RT"}
{"created":"2024-05-08 15:43:22","title":"Simulating Spin Dynamics of Supersolid States in a Quantum Ising Magnet","abstract":"Motivated by the recent experimental study on a quantum Ising magnet $\\text{K}_2\\text{Co}(\\text{SeO}_3)_2$ where spectroscopic evidence of zero-field supersolidity is presented [arXiv: 2402.15869], we simulate the excitation spectrum of the corresponding microscopic $XXZ$ model for the compound, using the recently developed excitation ansatz of infinite projected entangled-pair states (iPEPS). We map out the ground state phase diagram and compute the dynamical spin structure factors across a range of magnetic field strengths, focusing especially on the two supersolid phases found near zero and saturation fields. Our simulated excitation spectra for the zero-field supersolid \"Y\" phase are in excellent agreement with the experimental data -- recovering the low-energy branches and integer quantized excited energy levels $\\omega_n=nJ_{zz}$. Furthermore, we demonstrate the nonlocal multi-spin-flip features for modes at $\\omega_2$, indicative of their multi-magnon nature. Additionally, we identify characteristics of the high-field supersolid \"V\" phase in the simulated spectra, to be compared with future experimental results.","sentences":["Motivated by the recent experimental study on a quantum Ising magnet $\\text{K}_2\\text{Co}(\\text{SeO}_3)_2$ where spectroscopic evidence of zero-field supersolidity is presented [arXiv: 2402.15869], we simulate the excitation spectrum of the corresponding microscopic $XXZ$ model for the compound, using the recently developed excitation ansatz of infinite projected entangled-pair states (iPEPS).","We map out the ground state phase diagram and compute the dynamical spin structure factors across a range of magnetic field strengths, focusing especially on the two supersolid phases found near zero and saturation fields.","Our simulated excitation spectra for the zero-field supersolid \"Y\" phase are in excellent agreement with the experimental data -- recovering the low-energy branches and integer quantized excited energy levels $\\omega_n=nJ_{zz}$.","Furthermore, we demonstrate the nonlocal multi-spin-flip features for modes at $\\omega_2$, indicative of their multi-magnon nature.","Additionally, we identify characteristics of the high-field supersolid \"V\" phase in the simulated spectra, to be compared with future experimental results."],"url":"http://arxiv.org/abs/2405.05151v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 15:34:51","title":"Direct measurement of bulk currents in the quantized Hall regime","abstract":"The integer quantized Hall effect reveals a state of scattering-free carrier transport and quantized resistance in a two-dimensional conductor exposed to a perpendicular magnetic field. The quantized resistance is observed for the Hall bar geometry, i.e., if the current carrying contacts are connected by sample edges. A widely accepted model is the Landauer-Buttiker picture, which assumes an incompressible, i.e., electrically insulating bulk state surrounded by one-dimensional edge channels giving rise to quantized resistance. This model is challenged by the screening theory, which takes into account electron-electron interaction and predicts for the quantized Hall plateaus current flow in incompressible strips, which gradually shifts from the sample edges into the bulk with increasing magnetic field. We present direct proof of the predicted scattering-free bulk transport by exploring a Hall bar augmented with an additional contact placed in its center away from the Hall bar edges. Our result supports the screening theory.","sentences":["The integer quantized Hall effect reveals a state of scattering-free carrier transport and quantized resistance in a two-dimensional conductor exposed to a perpendicular magnetic field.","The quantized resistance is observed for the Hall bar geometry, i.e., if the current carrying contacts are connected by sample edges.","A widely accepted model is the Landauer-Buttiker picture, which assumes an incompressible, i.e., electrically insulating bulk state surrounded by one-dimensional edge channels giving rise to quantized resistance.","This model is challenged by the screening theory, which takes into account electron-electron interaction and predicts for the quantized Hall plateaus current flow in incompressible strips, which gradually shifts from the sample edges into the bulk with increasing magnetic field.","We present direct proof of the predicted scattering-free bulk transport by exploring a Hall bar augmented with an additional contact placed in its center away from the Hall bar edges.","Our result supports the screening theory."],"url":"http://arxiv.org/abs/2405.05138v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 15:24:04","title":"Evolution of Spin in the Intermediate Polar CC Sculptoris","abstract":"We report on spin variations in the intermediate polar and cataclysmic variable CC Scl, as seen by the Transiting Exoplanet Survey Satellite (TESS). By studying both the spin period and its harmonic, we find that the spin has varied since it was first observed in 2011. We find the latest spin value for the source to be 389.473(6)s, equivalent to 0.00450779(7) days, 0.02s shorter than the first value measured. A linear fit to these and intermediate data give a rate of change of spin ~-4.26(2.66)e10^-11 and a characteristic timescale tau~2.90e10^5 years, in line with other known intermediate polars with varying spin. The spin profile of this source also matches theoretical spin profiles of high-inclination intermediate polars, and furthermore, appears to have changed in shape over a period of three years. Such `spin-up' in an intermediate polar is considered to be from mass accretion onto the white dwarf (the primary), and we note the presence of dwarf nova eruptions in this source as being a possible catalyst of the variations.","sentences":["We report on spin variations in the intermediate polar and cataclysmic variable CC Scl, as seen by the Transiting Exoplanet Survey Satellite (TESS).","By studying both the spin period and its harmonic, we find that the spin has varied since it was first observed in 2011.","We find the latest spin value for the source to be 389.473(6)s, equivalent to 0.00450779(7) days, 0.02s shorter than the first value measured.","A linear fit to these and intermediate data give a rate of change of spin ~-4.26(2.66)e10^-11 and a characteristic timescale tau~2.90e10^5 years, in line with other known intermediate polars with varying spin.","The spin profile of this source also matches theoretical spin profiles of high-inclination intermediate polars, and furthermore, appears to have changed in shape over a period of three years.","Such `spin-up' in an intermediate polar is considered to be from mass accretion onto the white dwarf (the primary), and we note the presence of dwarf nova eruptions in this source as being a possible catalyst of the variations."],"url":"http://arxiv.org/abs/2405.05127v1","category":"astro-ph.SR"}
{"created":"2024-05-08 15:18:48","title":"A goodness-of-fit diagnostic for count data derived from half-normal plots with a simulated envelope","abstract":"Traditional methods of model diagnostics may include a plethora of graphical techniques based on residual analysis, as well as formal tests (e.g. Shapiro-Wilk test for normality and Bartlett test for homogeneity of variance). In this paper we derive a new distance metric based on the half-normal plot with a simulation envelope, a graphical model evaluation method, and investigate its properties through simulation studies. The proposed metric can help to assess the fit of a given model, and also act as a model selection criterion by being comparable across models, whether based or not on a true likelihood. More specifically, it quantitatively encompasses the model evaluation principles and removes the subjective bias when closely related models are involved. We validate the technique by means of an extensive simulation study carried out using count data, and illustrate with two case studies in ecology and fisheries research.","sentences":["Traditional methods of model diagnostics may include a plethora of graphical techniques based on residual analysis, as well as formal tests (e.g. Shapiro-Wilk test for normality and Bartlett test for homogeneity of variance).","In this paper we derive a new distance metric based on the half-normal plot with a simulation envelope, a graphical model evaluation method, and investigate its properties through simulation studies.","The proposed metric can help to assess the fit of a given model, and also act as a model selection criterion by being comparable across models, whether based or not on a true likelihood.","More specifically, it quantitatively encompasses the model evaluation principles and removes the subjective bias when closely related models are involved.","We validate the technique by means of an extensive simulation study carried out using count data, and illustrate with two case studies in ecology and fisheries research."],"url":"http://arxiv.org/abs/2405.05121v1","category":"stat.ME"}
{"created":"2024-05-08 15:18:39","title":"GRB afterglows with energy injections in AGN accretion disks","abstract":"Active galactic nucleus (AGN) disks are widely considered potential hosts for various high-energy transients, including gamma-ray bursts (GRBs). The reactivation of GRB central engines can provide additional energy to shocks formed during the interaction of the initially ejected GRB jets with the circumburst material, commonly referred to as energy injections. In this paper, we study GRBs occurring in AGN disks within the context of energy injections. We adopt the standard external forward shock (EFS) model and consider both short- and long-duration GRB scenarios. Light curves for two types of radiation, namely the radiation from the heated disk material (RHDM) and GRB afterglows, are computed. We find that the energy injection facilitates the EFS to break out from the photosphere of the low-density AGN disk at relativistic velocity. Moreover, the energy injection almost does not affect the RHDM but significantly enhances the peak flux of the GRB afterglows.","sentences":["Active galactic nucleus (AGN) disks are widely considered potential hosts for various high-energy transients, including gamma-ray bursts (GRBs).","The reactivation of GRB central engines can provide additional energy to shocks formed during the interaction of the initially ejected GRB jets with the circumburst material, commonly referred to as energy injections.","In this paper, we study GRBs occurring in AGN disks within the context of energy injections.","We adopt the standard external forward shock (EFS) model and consider both short- and long-duration GRB scenarios.","Light curves for two types of radiation, namely the radiation from the heated disk material (RHDM) and GRB afterglows, are computed.","We find that the energy injection facilitates the EFS to break out from the photosphere of the low-density AGN disk at relativistic velocity.","Moreover, the energy injection almost does not affect the RHDM but significantly enhances the peak flux of the GRB afterglows."],"url":"http://arxiv.org/abs/2405.05120v1","category":"astro-ph.HE"}
{"created":"2024-05-08 15:00:14","title":"Dark photon constraints from CMB temperature anisotropies","abstract":"The resonant conversion, within the inter-galactic medium, of regular photons into dark photons amplifies the anisotropy observed in the CMB, thereby imposing stringent constraints on the existence of light dark photons. In this study, we investigate the impact of light dark photons, with masses in the range $3\\times 10^{-15} ~\\rm{eV} < m_{A'} < 3\\times 10^{-12}~\\rm{eV}$ on the power spectrum of temperature anisotropies within the cosmic microwave background (CMB) radiation utilizing the state-of-the-art large-volume FLAMINGO cosmological simulations. Our results show that using full Planck data, one can expect the existing constraints on the dark photon mixing parameter in this mass range to improve by an order of magnitude.","sentences":["The resonant conversion, within the inter-galactic medium, of regular photons into dark photons amplifies the anisotropy observed in the CMB, thereby imposing stringent constraints on the existence of light dark photons.","In this study, we investigate the impact of light dark photons, with masses in the range $3\\times 10^{-15} ~\\rm{eV} < m_{A'} < 3\\times 10^{-12}~\\rm{eV}$ on the power spectrum of temperature anisotropies within the cosmic microwave background (CMB) radiation utilizing the state-of-the-art large-volume FLAMINGO cosmological simulations.","Our results show that using full Planck data, one can expect the existing constraints on the dark photon mixing parameter in this mass range to improve by an order of magnitude."],"url":"http://arxiv.org/abs/2405.05104v1","category":"astro-ph.CO"}
{"created":"2024-05-08 14:55:11","title":"Fundamental Limits for Jammer-Resilient Communication in Finite-Resolution MIMO","abstract":"Spatial filtering based on multiple-input multiple-output (MIMO) processing is a powerful method for jammer mitigation. In principle, a MIMO receiver can null the interference of a single-antenna jammer at the cost of only one degree of freedom - if the number of receive antennas is large, communication performance is barely affected. In this paper, we show that the potential for MIMO jammer mitigation based on the digital outputs of finite-resolution analog-to-digital converters (ADCs) is fundamentally worse: Strong jammers will either cause the ADCs to saturate (when the ADCs' quantization range is small) or drown legitimate communication signals in quantization noise (when the ADCs' quantization range is large). We provide a fundamental bound on the mutual information between the quantized receive signal and the legitimate transmit signal. Our bound shows that, for any fixed ADC resolution, the mutual information tends to zero as the jammer power tends to infinity. Our bound also confirms the intuition that for every 6.02 dB increase in jamming power, the ADC resolution must be increased by 1 bit in order to prevent the mutual information from vanishing.","sentences":["Spatial filtering based on multiple-input multiple-output (MIMO) processing is a powerful method for jammer mitigation.","In principle, a MIMO receiver can null the interference of a single-antenna jammer at the cost of only one degree of freedom - if the number of receive antennas is large, communication performance is barely affected.","In this paper, we show that the potential for MIMO jammer mitigation based on the digital outputs of finite-resolution analog-to-digital converters (ADCs) is fundamentally worse: Strong jammers will either cause the ADCs to saturate (when the ADCs' quantization range is small) or drown legitimate communication signals in quantization noise (when the ADCs' quantization range is large).","We provide a fundamental bound on the mutual information between the quantized receive signal and the legitimate transmit signal.","Our bound shows that, for any fixed ADC resolution, the mutual information tends to zero as the jammer power tends to infinity.","Our bound also confirms the intuition that for every 6.02 dB increase in jamming power, the ADC resolution must be increased by 1 bit in order to prevent the mutual information from vanishing."],"url":"http://arxiv.org/abs/2405.05100v1","category":"cs.IT"}
{"created":"2024-05-08 13:58:27","title":"What role of gravity, turbulence and magnetic fields in high-mass star formation clouds?","abstract":"To explore the potential role of gravity, turbulence and magnetic fields in high-mass star formation in molecular clouds, this study revisits the velocity dispersion--size ($\\sigma$--$L$) and density--size ($\\rho$--$L$) scalings and the associated turbulent energy spectrum using an extensive data sample. The sample includes various hierarchical density structures in high-mass star formation clouds, across scales of 0.01 to 100 pc. We observe $\\sigma \\propto L^{0.26}$ and $\\rho \\propto L^{-1.54}$ scalings, converging toward a virial equilibrium state. A nearly flat virial parameter--mass ($\\alpha_{\\rm vir}-M$) distribution is seen across all density scales, with $\\alpha_{\\rm vir}$ values centered around unity, suggesting a global equilibrium maintained by the interplay between gravity and turbulence across multiple scales. Our turbulent energy spectrum ($E(k)$) analysis, based on the $\\sigma$--$L$ and $\\rho$--$L$ scalings, yields a characteristic $E(k) \\propto k^{-1.52}$. These findings indicate the potential significance of gravity, turbulence, and possibly magnetic fields all in regulating dynamics of molecular clouds and high-mass star formation therein.","sentences":["To explore the potential role of gravity, turbulence and magnetic fields in high-mass star formation in molecular clouds, this study revisits the velocity dispersion--size ($\\sigma$--$L$) and density--size ($\\rho$--$L$) scalings and the associated turbulent energy spectrum using an extensive data sample.","The sample includes various hierarchical density structures in high-mass star formation clouds, across scales of 0.01 to 100 pc.","We observe $\\sigma \\propto L^{0.26}$ and $\\rho \\propto L^{-1.54}$ scalings, converging toward a virial equilibrium state.","A nearly flat virial parameter--mass ($\\alpha_{\\rm vir}-M$) distribution is seen across all density scales, with $\\alpha_{\\rm vir}$ values centered around unity, suggesting a global equilibrium maintained by the interplay between gravity and turbulence across multiple scales.","Our turbulent energy spectrum ($E(k)$) analysis, based on the $\\sigma$--$L$ and $\\rho$--$L$ scalings, yields a characteristic $E(k)","\\propto k^{-1.52}$.","These findings indicate the potential significance of gravity, turbulence, and possibly magnetic fields all in regulating dynamics of molecular clouds and high-mass star formation therein."],"url":"http://arxiv.org/abs/2405.05063v1","category":"astro-ph.GA"}
{"created":"2024-05-08 13:48:50","title":"Large N limit of fuzzy geometries coupled to fermions","abstract":"In this paper we present an analysis of the large N limit of a family of quartic Dirac ensembles based on (0, 1) fuzzy geometries that are coupled to fermions. These Dirac ensembles are examples of single-matrix, multi-trace matrix ensembles. Additionally, they serve as examples of integer-valued $\\beta$-ensembles. Convergence of the spectral density in the large N limit for a large class of such matrix ensembles is proven, improving on existing results. The main results of this paper are the addition of the fermionic contribution in the matrix ensemble and the investigation of spectral estimators for finite dimensional spectral triples","sentences":["In this paper we present an analysis of the large N limit of a family of quartic Dirac ensembles based on (0, 1) fuzzy geometries that are coupled to fermions.","These Dirac ensembles are examples of single-matrix, multi-trace matrix ensembles.","Additionally, they serve as examples of integer-valued $\\beta$-ensembles.","Convergence of the spectral density in the large N limit for a large class of such matrix ensembles is proven, improving on existing results.","The main results of this paper are the addition of the fermionic contribution in the matrix ensemble and the investigation of spectral estimators for finite dimensional spectral triples"],"url":"http://arxiv.org/abs/2405.05056v1","category":"math-ph"}
{"created":"2024-05-08 13:37:10","title":"Underlying-event studies with strange hadrons in $pp$ collisions at $\\sqrt{s} = 13$ TeV with the ATLAS detector","abstract":"Properties of the underlying-event in $pp$ interactions are investigated primarily via the strange hadrons $K_{S}^{0}$, $\\Lambda$ and $\\bar\\Lambda$, as reconstructed using the ATLAS detector at the LHC in minimum-bias $pp$ collision data at $\\sqrt{s} = 13$ TeV. The hadrons are reconstructed via the identification of the displaced two-particle vertices corresponding to the decay modes $K_{S}^{0}\\rightarrow\\pi^+\\pi^-$, $\\Lambda\\rightarrow\\pi^-p$ and $\\bar\\Lambda\\rightarrow\\pi^+\\bar{p}$. These are used in the construction of underlying-event observables in azimuthal regions computed relative to the leading charged-particle jet in the event. None of the hadronisation and underlying-event physics models considered can describe the data over the full kinematic range considered. Events with a leading charged-particle jet in the range of $10 < p_T \\leq 40$ GeV are studied using the number of prompt charged particles in the transverse region. The ratio $N(\\Lambda\\rightarrow\\pi^\\mp p^\\pm)/N(K_{S}^{0}\\rightarrow\\pi^+\\pi^-)$ as a function of the number of such charged particles varies only slightly over this range. This disagrees with the expectations of some of the considered Monte Carlo models.","sentences":["Properties of the underlying-event in $pp$ interactions are investigated primarily via the strange hadrons $K_{S}^{0}$, $\\Lambda$ and $\\bar\\Lambda$, as reconstructed using the ATLAS detector at the LHC in minimum-bias $pp$ collision data at $\\sqrt{s} = 13$ TeV.","The hadrons are reconstructed via the identification of the displaced two-particle vertices corresponding to the decay modes $K_{S}^{0}\\rightarrow\\pi^+\\pi^-$, $\\Lambda\\rightarrow\\pi^-p$ and $\\bar\\Lambda\\rightarrow\\pi^+\\bar{p}$. These are used in the construction of underlying-event observables in azimuthal regions computed relative to the leading charged-particle jet in the event.","None of the hadronisation and underlying-event physics models considered can describe the data over the full kinematic range considered.","Events with a leading charged-particle jet in the range of $10 < p_T \\leq 40$ GeV are studied using the number of prompt charged particles in the transverse region.","The ratio $N(\\Lambda\\rightarrow\\pi^\\mp p^\\pm)/N(K_{S}^{0}\\rightarrow\\pi^+\\pi^-)$ as a function of the number of such charged particles varies only slightly over this range.","This disagrees with the expectations of some of the considered Monte Carlo models."],"url":"http://arxiv.org/abs/2405.05048v1","category":"hep-ex"}
{"created":"2024-05-08 13:27:22","title":"Fully Charmed Tetraquark States in $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ Color Structure via QCD Sum Rules","abstract":"Stimulated by the recent experimental results on the fully-charm tetraquark states, we systematically calculate the mass spectra of the fully-charm tetraquark states in $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ color configuration via QCD sum rules. By constructing nine $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ type currents with quantum numbers $J^{PC}=0^{-+},0^{--},1^{-+},1^{+-},1^{--}$ and $2^{++}$, we perform analytic calculation up to dimension six in the Operator Product Expansion (OPE). We find the fully-charm tetraquark states with $J^{PC}=1^{+-},2^{++}$ lie around 6.48 $\\sim$ 6.62 GeV while the fully-charm tetraquark states with $J^{PC}=0^{-+},0^{--},1^{--},1^{-+}$ are about 6.85$ \\sim $7.02 GeV. Notably, the mass predictions for the $c\\bar{c}c\\bar{c}$ tetraquarks, specifically those with $J^{PC}=2^{++}$, align with the broad structure identified by LHCb. Moreover, the masses of fully-charm tetraquarks with $J^{PC}=0^{-+}$ and $1^{-+}$ are anticipated to match closely with the mass of X(6900), considering the margin of error. Such findings hint at the presence of some $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ components within the di-$J/\\psi$ structures observed by LHCb. The predictions for tetraquark states with $J^{PC}=0^{--},1^{+-},1^{--}$ may be accessible in the future BelleII, Super-B, PANDA, and LHCb experiments.","sentences":["Stimulated by the recent experimental results on the fully-charm tetraquark states, we systematically calculate the mass spectra of the fully-charm tetraquark states in $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ color configuration via QCD sum rules.","By constructing nine $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ type currents with quantum numbers $J^{PC}=0^{-+},0^{--},1^{-+},1^{+-},1^{--}$ and $2^{++}$, we perform analytic calculation up to dimension six in the Operator Product Expansion (OPE).","We find the fully-charm tetraquark states with $J^{PC}=1^{+-},2^{++}$ lie around 6.48 $\\sim$ 6.62 GeV while the fully-charm tetraquark states with $J^{PC}=0^{-+},0^{--},1^{--},1^{-+}$ are about 6.85$ \\sim $7.02 GeV.","Notably, the mass predictions for the $c\\bar{c}c\\bar{c}$ tetraquarks, specifically those with $J^{PC}=2^{++}$, align with the broad structure identified by LHCb.","Moreover, the masses of fully-charm tetraquarks with $J^{PC}=0^{-+}$ and $1^{-+}$ are anticipated to match closely with the mass of X(6900), considering the margin of error.","Such findings hint at the presence of some $8_{[c\\bar{c}]}\\otimes8_{[c\\bar{c}]}$ components within the di-$J/\\psi$ structures observed by LHCb.","The predictions for tetraquark states with $J^{PC}=0^{--},1^{+-},1^{--}$ may be accessible in the future BelleII, Super-B, PANDA, and LHCb experiments."],"url":"http://arxiv.org/abs/2405.05042v1","category":"hep-ph"}
{"created":"2024-05-08 11:48:34","title":"Constraining the core radius and density jumps inside Earth using atmospheric neutrino oscillations","abstract":"Atmospheric neutrinos can act as a tool to probe the interior of Earth using weak interactions, and can provide information complementary to that obtained from gravitational and seismic measurements. While passing through Earth, multi-GeV neutrinos encounter Earth matter effects due to the coherent forward scattering with the ambient electrons, which alter the neutrino oscillation probabilities. These matter effects depend upon the density distribution of electrons inside Earth, and hence, can be used to determine the internal structure of Earth. In this work, we employ a five-layered model of Earth where the layer densities and radii are modified, keeping the mass and moment of inertia of Earth unchanged and respecting the hydrostatic equilibrium condition. We use the proposed INO-ICAL detector as an example of an atmospheric neutrino experiment that can distinguish between neutrinos and antineutrinos efficiently in the multi-GeV energy range. Our analysis demonstrates the role such an experiment can play in simultaneously constraining the density jumps inside Earth and the location of the core-mantle boundary.","sentences":["Atmospheric neutrinos can act as a tool to probe the interior of Earth using weak interactions, and can provide information complementary to that obtained from gravitational and seismic measurements.","While passing through Earth, multi-GeV neutrinos encounter Earth matter effects due to the coherent forward scattering with the ambient electrons, which alter the neutrino oscillation probabilities.","These matter effects depend upon the density distribution of electrons inside Earth, and hence, can be used to determine the internal structure of Earth.","In this work, we employ a five-layered model of Earth where the layer densities and radii are modified, keeping the mass and moment of inertia of Earth unchanged and respecting the hydrostatic equilibrium condition.","We use the proposed INO-ICAL detector as an example of an atmospheric neutrino experiment that can distinguish between neutrinos and antineutrinos efficiently in the multi-GeV energy range.","Our analysis demonstrates the role such an experiment can play in simultaneously constraining the density jumps inside Earth and the location of the core-mantle boundary."],"url":"http://arxiv.org/abs/2405.04986v1","category":"hep-ph"}
{"created":"2024-05-08 11:44:28","title":"Probing axion-like particles with RF cavities separated by thin barrier","abstract":"We address the Light-Shining-through thin Wall (LSthinW) laboratory setup to estimate the axion-like particle (ALP) sensitivity of a two radio frequency (RF) cavities immersed in static magnetic field. We show that the sufficiently thin wall between cavities can lead the improved sensitivity of the purely laboratory probes of ALP in the mass range $10^{-6}~\\mbox{eV} \\lesssim m_a \\lesssim 5 \\times 10^{-5}~\\mbox{eV}$.","sentences":["We address the Light-Shining-through thin Wall (LSthinW) laboratory setup to estimate the axion-like particle (ALP) sensitivity of a two radio frequency (RF) cavities immersed in static magnetic field.","We show that the sufficiently thin wall between cavities can lead the improved sensitivity of the purely laboratory probes of ALP in the mass range $10^{-6}~\\mbox{eV} \\lesssim m_a","\\lesssim 5 \\times 10^{-5}~\\mbox{eV}$."],"url":"http://arxiv.org/abs/2405.04983v1","category":"hep-ph"}
{"created":"2024-05-08 11:26:32","title":"SVARs with breaks: Identification and inference","abstract":"In this paper we propose a class of structural vector autoregressions (SVARs) characterized by structural breaks (SVAR-WB). Together with standard restrictions on the parameters and on functions of them, we also consider constraints across the different regimes. Such constraints can be either (a) in the form of stability restrictions, indicating that not all the parameters or impulse responses are subject to structural changes, or (b) in terms of inequalities regarding particular characteristics of the SVAR-WB across the regimes. We show that all these kinds of restrictions provide benefits in terms of identification. We derive conditions for point and set identification of the structural parameters of the SVAR-WB, mixing equality, sign, rank and stability restrictions, as well as constraints on forecast error variances (FEVs). As point identification, when achieved, holds locally but not globally, there will be a set of isolated structural parameters that are observationally equivalent in the parametric space. In this respect, both common frequentist and Bayesian approaches produce unreliable inference as the former focuses on just one of these observationally equivalent points, while for the latter on a non-vanishing sensitivity to the prior. To overcome these issues, we propose alternative approaches for estimation and inference that account for all admissible observationally equivalent structural parameters. Moreover, we develop a pure Bayesian and a robust Bayesian approach for doing inference in set-identified SVAR-WBs. Both the theory of identification and inference are illustrated through a set of examples and an empirical application on the transmission of US monetary policy over the great inflation and great moderation regimes.","sentences":["In this paper we propose a class of structural vector autoregressions (SVARs) characterized by structural breaks (SVAR-WB).","Together with standard restrictions on the parameters and on functions of them, we also consider constraints across the different regimes.","Such constraints can be either (a) in the form of stability restrictions, indicating that not all the parameters or impulse responses are subject to structural changes, or (b) in terms of inequalities regarding particular characteristics of the SVAR-WB across the regimes.","We show that all these kinds of restrictions provide benefits in terms of identification.","We derive conditions for point and set identification of the structural parameters of the SVAR-WB, mixing equality, sign, rank and stability restrictions, as well as constraints on forecast error variances (FEVs).","As point identification, when achieved, holds locally but not globally, there will be a set of isolated structural parameters that are observationally equivalent in the parametric space.","In this respect, both common frequentist and Bayesian approaches produce unreliable inference as the former focuses on just one of these observationally equivalent points, while for the latter on a non-vanishing sensitivity to the prior.","To overcome these issues, we propose alternative approaches for estimation and inference that account for all admissible observationally equivalent structural parameters.","Moreover, we develop a pure Bayesian and a robust Bayesian approach for doing inference in set-identified SVAR-WBs.","Both the theory of identification and inference are illustrated through a set of examples and an empirical application on the transmission of US monetary policy over the great inflation and great moderation regimes."],"url":"http://arxiv.org/abs/2405.04973v1","category":"econ.EM"}
{"created":"2024-05-08 10:57:39","title":"Boosted Imaginary Time Evolution of Matrix Product States","abstract":"In this work, we consider the imaginary time evolution of matrix product states. We present a novel quantum-inspired classical method that, when combined with time evolving block decimation (TEBD), is able to potentially speed-up the convergence to a ground state compared to TEBD alone. Our method, referred to as boosted imaginary time evolution, relies on the use of reflections to boost to lower energy states. Interleaving TEBD steps with boosts reduces the total number of TEBD steps and potentially the computational cost required to imaginary time evolve a matrix product state to a ground state. We give the mathematical details of the method followed by an algorithmic implementation and finally some results for a simple test case.","sentences":["In this work, we consider the imaginary time evolution of matrix product states.","We present a novel quantum-inspired classical method that, when combined with time evolving block decimation (TEBD), is able to potentially speed-up the convergence to a ground state compared to TEBD alone.","Our method, referred to as boosted imaginary time evolution, relies on the use of reflections to boost to lower energy states.","Interleaving TEBD steps with boosts reduces the total number of TEBD steps and potentially the computational cost required to imaginary time evolve a matrix product state to a ground state.","We give the mathematical details of the method followed by an algorithmic implementation and finally some results for a simple test case."],"url":"http://arxiv.org/abs/2405.04959v1","category":"quant-ph"}
{"created":"2024-05-08 10:05:29","title":"A high-resolution radio morphology and polarization of the kpc-scale X-ray jet of PKS 1127-145","abstract":"We report on new multi-frequency Very Large Array (VLA) radio observations and Chandra X-ray observations of a radio-loud quasar with a ~300 kpc long jet, PKS 1127-145, during a flaring event detected in $\\gamma$-rays by the Fermi Large Area Telescope in 2020 December. The high angular resolution of the new radio images allows us to disentangle for the first time the inner kpc-scale jet from the core contribution. The inner radio jet, up to 15 kpc from the core, is highly polarized (33 per cent) and the magnetic field is parallel to the jet axis. At about 18 arcsec from the core the jet slightly bends and we observe a re-brightening of the radio emission and a 90-degree rotation of the magnetic field, likely highlighting the presence of a shock that is compressing the magnetic field to a plane perpendicular to the jet axis and where efficient particle acceleration takes place. At the same position the X-ray emission fades, suggesting a deceleration of the bulk velocity of the jet after the bend. A change in velocity and collimation of the jet is supported by the widening of the jet profile and the detection of a limb-brightened structure connecting the bending region with the jet termination. The limb-brightened structure might indicate the co-existence of both longitudinal and transverse velocity gradients at the jet bending. There is no evidence for significant brightening of the kpc-scale jet in the radio or X-ray band during the $\\gamma$-ray flare. The X-ray flux, $F_{\\rm 2-10\\,keV} =   (6.24\\pm0.57)\\times10^{-12}$ ergs s$^{-1}$ cm$^{-2}$, measured by Chandra from the quasar core is consistent with the flux measured by the X-Ray Telescope on board the Neil Gehrels Swift Observatory after the high-energy flare. Our results indicate that the $\\gamma$-ray flaring region is located within the VLA source core.","sentences":["We report on new multi-frequency Very Large Array (VLA) radio observations and Chandra X-ray observations of a radio-loud quasar with a ~300 kpc long jet, PKS 1127-145, during a flaring event detected in $\\gamma$-rays by the Fermi Large Area Telescope in 2020 December.","The high angular resolution of the new radio images allows us to disentangle for the first time the inner kpc-scale jet from the core contribution.","The inner radio jet, up to 15 kpc from the core, is highly polarized (33 per cent) and the magnetic field is parallel to the jet axis.","At about 18 arcsec from the core the jet slightly bends and we observe a re-brightening of the radio emission and a 90-degree rotation of the magnetic field, likely highlighting the presence of a shock that is compressing the magnetic field to a plane perpendicular to the jet axis and where efficient particle acceleration takes place.","At the same position the X-ray emission fades, suggesting a deceleration of the bulk velocity of the jet after the bend.","A change in velocity and collimation of the jet is supported by the widening of the jet profile and the detection of a limb-brightened structure connecting the bending region with the jet termination.","The limb-brightened structure might indicate the co-existence of both longitudinal and transverse velocity gradients at the jet bending.","There is no evidence for significant brightening of the kpc-scale jet in the radio or X-ray band during the $\\gamma$-ray flare.","The X-ray flux, $F_{\\rm 2-10\\,keV} =   (6.24\\pm0.57)\\times10^{-12}$ ergs s$^{-1}$ cm$^{-2}$, measured by Chandra from the quasar core is consistent with the flux measured by the X-Ray Telescope on board the Neil Gehrels Swift Observatory after the high-energy flare.","Our results indicate that the $\\gamma$-ray flaring region is located within the VLA source core."],"url":"http://arxiv.org/abs/2405.04935v1","category":"astro-ph.HE"}
{"created":"2024-05-08 07:59:26","title":"MOSEL survey: Unwrapping the Epoch of Reionization through mimic galaxies at Cosmic Noon","abstract":"The nature of the first galaxies that reionized the universe during the Epoch of Reionization (EoR) remains unclear. Attempts to directly determine spectral properties of these early galaxies are affected by both limited photometric constraints across the spectrum and by the opacity of the intergalactic medium (IGM) to the Lyman Continuum (LyC) at high redshift. We approach this by analysing properties of analogous extreme emission line galaxies (EELGs, [OIII]+Hbeta EW $>400$ Angstrom) at $2.5<z<4$ from the ZFOURGE survey using the Multi-wavelength Analysis of Galaxy Physical Properties (MAGPHYS) SED fitting code. We compare these to galaxies at $z>5.5$ observed with the James Webb Space Telesope (JWST) with self-consistent spectral energy distribution fitting methodology. This work focuses on the comparison of their UV slopes ($\\beta_P$), ionizing photon production efficiencies $\\xi_{ion}$, star formation rates and dust properties to determine the effectiveness of this analogue selection technique. We report the median ionizing photon production efficiencies as log$_{10}(\\xi_{ion}/(Hz\\ {\\rm erg}^{-1}))=$$25.14^{+0.06}_{-0.04}$,$25.16^{+0.06}_{-0.05}$,$25.16^{+0.04}_{-0.05}$,$25.18^{+0.06}_{-0.07}$ for our ZFOURGE control, ZFOURGE EELG, JADES and CEERS samples respectively. ZFOURGE EELGs are 0.57 dex lower in stellar mass and have half the dust extinction, compared to their ZFOURGE control counterparts. They also have a similar specific star formation rates and $\\beta_P$ to the $z>5.5$ samples. We find that EELGs at low redshift ($2.5<z<4$) are analogous to EoR galaxies in their dust attenuation and specific star formation rates. Their extensive photometric coverage and the accessibility of their LyC region opens pathways to infer stellar population properties in the EoR.","sentences":["The nature of the first galaxies that reionized the universe during the Epoch of Reionization (EoR) remains unclear.","Attempts to directly determine spectral properties of these early galaxies are affected by both limited photometric constraints across the spectrum and by the opacity of the intergalactic medium (IGM) to the Lyman Continuum (LyC) at high redshift.","We approach this by analysing properties of analogous extreme emission line galaxies (EELGs, [OIII]+Hbeta EW $>400$ Angstrom) at $2.5<z<4$ from the ZFOURGE survey using the Multi-wavelength Analysis of Galaxy Physical Properties (MAGPHYS) SED fitting code.","We compare these to galaxies at $z>5.5$ observed with the James Webb Space Telesope (JWST) with self-consistent spectral energy distribution fitting methodology.","This work focuses on the comparison of their UV slopes ($\\beta_P$), ionizing photon production efficiencies $\\xi_{ion}$, star formation rates and dust properties to determine the effectiveness of this analogue selection technique.","We report the median ionizing photon production efficiencies as log$_{10}(\\xi_{ion}/(Hz\\ {\\rm erg}^{-1}))=$$25.14^{+0.06}_{-0.04}$,$25.16^{+0.06}_{-0.05}$,$25.16^{+0.04}_{-0.05}$,$25.18^{+0.06}_{-0.07}$ for our ZFOURGE control, ZFOURGE EELG, JADES and CEERS samples respectively.","ZFOURGE EELGs are 0.57 dex lower in stellar mass and have half the dust extinction, compared to their ZFOURGE control counterparts.","They also have a similar specific star formation rates and $\\beta_P$ to the $z>5.5$ samples.","We find that EELGs at low redshift ($2.5<z<4$) are analogous to EoR galaxies in their dust attenuation and specific star formation rates.","Their extensive photometric coverage and the accessibility of their LyC region opens pathways to infer stellar population properties in the EoR."],"url":"http://arxiv.org/abs/2405.04870v1","category":"astro-ph.GA"}
{"created":"2024-05-08 06:24:34","title":"Amplitude analysis of $ B^0 \\to K^0_S K^+ K^-$ decays in a quasi-two-body QCD factorization approach","abstract":"The $B^0 \\to K^0_S K^+ K^- $ decay amplitude is derived within a quasi-two-body QCD factorization framework in terms of kaon form factors and $B^0$ to two-kaon-transition functions. The final state kaon-kaon interactions in the $S$, $P$, and $D$ waves are taken into account. The unitarity constraints are satisfied for the two kaons in scalar states. It is shown that with few terms of the full decay amplitude one may reach a fair agreement with the total branching fraction and Dalitz-plot projections published in 2010 by the Belle Collaboration and in 2012 by the \\textit{BABAR} Collaboration. With~13 free parameters, our model fits the corresponding~422 data with a $\\chi^2$ of~583.6 which leads to a $\\chi^2$ per degree of freedom equal to 1.43. The dominant branching fraction arises from the $f_0(K^+K^-) K^0_S$ mode with~83.0~\\% of the total branching. The next important mode is dominated by $\\phi K^0_S$ plus small $\\omega K^0_S$ and $\\rho^0 K^0_S$ modes with~18.3~\\% of the total. Then follows the $a_0^\\pm K^\\mp$ mode with~6.2~\\%. Adding the other smaller modes, the total percentage sum is~107.7~\\% which indicates a small interference contribution. In most regions of the Dalitz plot, our model gives rather small $CP$ asymmetry, but in some parts its values can be large and positive or negative. Its predicted total value is equal to $-0.11$~\\%. The calculated time dependent {\\it CP}-asymmetry parameters agree, within errors, with those obtained by the \\textit{BABAR} analysis. Our model amplitude can be the basis for a parametrization in experimental Dalitz plot analyses of LHCb and Belle II Collaborations.","sentences":["The $B^0 \\to K^0_S K^+ K^- $ decay amplitude is derived within a quasi-two-body QCD factorization framework in terms of kaon form factors and $B^0$ to two-kaon-transition functions.","The final state kaon-kaon interactions in the $S$, $P$, and $D$ waves are taken into account.","The unitarity constraints are satisfied for the two kaons in scalar states.","It is shown that with few terms of the full decay amplitude one may reach a fair agreement with the total branching fraction and Dalitz-plot projections published in 2010 by the Belle Collaboration and in 2012 by the \\textit{BABAR} Collaboration.","With~13 free parameters, our model fits the corresponding~422 data with a $\\chi^2$ of~583.6 which leads to a $\\chi^2$ per degree of freedom equal to 1.43.","The dominant branching fraction arises from the $f_0(K^+K^-) K^0_S$ mode with~83.0~\\% of the total branching.","The next important mode is dominated by $\\phi K^0_S$ plus small $\\omega K^0_S$ and $\\rho^0 K^0_S$ modes with~18.3~\\% of the total.","Then follows the $a_0^\\pm K^\\mp$ mode with~6.2~\\%.","Adding the other smaller modes, the total percentage sum is~107.7~\\% which indicates a small interference contribution.","In most regions of the Dalitz plot, our model gives rather small $CP$ asymmetry, but in some parts its values can be large and positive or negative.","Its predicted total value is equal to $-0.11$~\\%.","The calculated time dependent {\\it CP}-asymmetry parameters agree, within errors, with those obtained by the \\textit{BABAR} analysis.","Our model amplitude can be the basis for a parametrization in experimental Dalitz plot analyses of LHCb and Belle II Collaborations."],"url":"http://arxiv.org/abs/2405.04838v1","category":"hep-ph"}
{"created":"2024-05-08 04:21:26","title":"The Northern Cross Fast Radio Burst project IV. Multiwavelength study of the actively repeating FRB 20220912A","abstract":"Fast radio bursts (FRBs) are energetic, millisecond-duration radio pulses observed at extragalactic distances and whose origin is still largely debated. A fraction of the FRB population have shown repeating bursts. It is still unclear whether these represent a distinct class of sources. We investigate the bursting behaviour of FRB 20220912A, one of the most active repeating FRBs known. In particular, we focus on its burst energy distribution, linked to the source energetics, and its emission spectrum, the latter directly related to the underlying emission mechanism. We monitored FRB 20220912A at $408$ MHz with the Northern Cross radio telescope and at $1.4$ GHz using the $32$-m Medicina Grueff radio telescope. Additionaly, we conducted $1.2$ GHz observations taken with the upgraded-Giant Meter Wave Radio Telescope searching for a persistent radio source coincident with FRB 20220912A, and we present the first upper limits obtained from a monitoring in X and $\\gamma$ rays conducted with Swift and AGILE satellites. We report 16 new bursts from FRB 20220912A at $408$ MHz during the period of time between October 16$^{\\rm th}$ 2022 and December 31$^{\\rm st}$ 2023. Their cumulative spectral energy distribution follows a power law with slope $\\alpha_E = -1.5 \\pm 0.3$ and we measure a repetition rate of $0.15 \\pm 0.04$ hr$^{-1}$ for bursts having fluence $\\mathcal{F} \\geq 20$ Jy ms. Furthermore, we report no detections at $1.4$ GHz during down to a fluence of $\\mathcal{F} \\geq 13$ Jy ms. These non-detections imply an upper limit of $\\beta < -2.3$, with $\\beta$ being the global spectral index of FRB 20220912A. This is inconsistent with positive $\\beta$ values found for the only two known cases in which an FRB has been detected in separate spectral bands. We find that FRB 20220912A has shown a decline of $4$ orders of magnitude in its bursting activity at $1.4$ GHz over a one year ... (abridged)","sentences":["Fast radio bursts (FRBs) are energetic, millisecond-duration radio pulses observed at extragalactic distances and whose origin is still largely debated.","A fraction of the FRB population have shown repeating bursts.","It is still unclear whether these represent a distinct class of sources.","We investigate the bursting behaviour of FRB 20220912A, one of the most active repeating FRBs known.","In particular, we focus on its burst energy distribution, linked to the source energetics, and its emission spectrum, the latter directly related to the underlying emission mechanism.","We monitored FRB 20220912A at $408$ MHz with the Northern Cross radio telescope and at $1.4$ GHz using the $32$-m Medicina Grueff radio telescope.","Additionaly, we conducted $1.2$ GHz observations taken with the upgraded-Giant Meter Wave Radio Telescope searching for a persistent radio source coincident with FRB 20220912A, and we present the first upper limits obtained from a monitoring in X and $\\gamma$ rays conducted with Swift and AGILE satellites.","We report 16 new bursts from FRB 20220912A at $408$ MHz during the period of time between October 16$^{\\rm th}$ 2022 and December 31$^{\\rm st}$ 2023.","Their cumulative spectral energy distribution follows a power law with slope $\\alpha_E = -1.5 \\pm 0.3$ and we measure a repetition rate of $0.15 \\pm 0.04$ hr$^{-1}$ for bursts having fluence $\\mathcal{F} \\geq 20$ Jy ms.","Furthermore, we report no detections at $1.4$ GHz during down to a fluence of $\\mathcal{F} \\geq 13$ Jy ms.","These non-detections imply an upper limit of $\\beta <","-2.3$, with $\\beta$ being the global spectral index of FRB 20220912A.","This is inconsistent with positive $\\beta$ values found for the only two known cases in which an FRB has been detected in separate spectral bands.","We find that FRB 20220912A has shown a decline of $4$ orders of magnitude in its bursting activity at $1.4$ GHz over a one year ... (abridged)"],"url":"http://arxiv.org/abs/2405.04802v1","category":"astro-ph.HE"}
{"created":"2024-05-08 03:30:12","title":"Inductive calculation of superconformal indices based on giant graviton expansion","abstract":"We investigate a simple-sum giant graviton expansion of the superconformal indices of ${\\cal N}=2$ superconformal field theories realized on D3-branes probing $7$-brane backgrounds with constant axio-dilation field. The expansion is of self-dual type, and imposes strong constraints on the indices. By using the constraints we determine first few terms in the superconformal indices for arbitrary rank $N$.","sentences":["We investigate a simple-sum giant graviton expansion of the superconformal indices of ${\\cal N}=2$ superconformal field theories realized on D3-branes probing $7$-brane backgrounds with constant axio-dilation field.","The expansion is of self-dual type, and imposes strong constraints on the indices.","By using the constraints we determine first few terms in the superconformal indices for arbitrary rank $N$."],"url":"http://arxiv.org/abs/2405.04786v1","category":"hep-th"}
{"created":"2024-05-08 02:25:35","title":"Multiwavelength observations of a breakout jet at an active region periphery","abstract":"We analysed Interface-Region Imaging Spectrograph (IRIS) and the Solar Dynamics Observatory/Atmospheric Imaging Assembly (SDO/AIA) observations of a small coronal jet that occurred at the solar west limb on 2014 August 29. The jet source region, a small bright point, was located at an active-region periphery and contains a fan-spine topology with a mini-filament. Our analysis has identified key features and timings that motivate the following interpretation of this event. As the stressed core flux rises, a current sheet forms beneath it; the ensuing reconnection forms a flux rope above a flare arcade. When the rising filament-carrying flux rope reaches the stressed null, it triggers a jet via explosive interchange (breakout) reconnection. During the flux-rope interaction with the external magnetic field, we observed brightening above the filament and within the dome, along with a growing flare arcade. EUV images reveal quasi-periodic ejections throughout the jet duration with a dominant period of 4 minutes, similar to coronal jetlets and larger jets. We conclude that these observations are consistent with the magnetic breakout model for coronal jets.","sentences":["We analysed Interface-Region Imaging Spectrograph (IRIS) and the Solar Dynamics Observatory/Atmospheric Imaging Assembly (SDO/AIA) observations of a small coronal jet that occurred at the solar west limb on 2014 August 29.","The jet source region, a small bright point, was located at an active-region periphery and contains a fan-spine topology with a mini-filament.","Our analysis has identified key features and timings that motivate the following interpretation of this event.","As the stressed core flux rises, a current sheet forms beneath it; the ensuing reconnection forms a flux rope above a flare arcade.","When the rising filament-carrying flux rope reaches the stressed null, it triggers a jet via explosive interchange (breakout) reconnection.","During the flux-rope interaction with the external magnetic field, we observed brightening above the filament and within the dome, along with a growing flare arcade.","EUV images reveal quasi-periodic ejections throughout the jet duration with a dominant period of 4 minutes, similar to coronal jetlets and larger jets.","We conclude that these observations are consistent with the magnetic breakout model for coronal jets."],"url":"http://arxiv.org/abs/2405.04766v1","category":"astro-ph.SR"}
{"created":"2024-05-08 01:30:47","title":"Bayesian Black Hole Photogrammetry","abstract":"We propose a simple, analytic dual-cone accretion model for horizon scale images of the cores of Low-Luminosity Active Galactic Nuclei (LLAGN), including those observed by the Event Horizon Telescope (EHT). Our underlying model is of synchrotron emission from an axisymmetric, magnetized plasma, which is constrained to flow within two oppositely oriented cones that are aligned with the black hole's spin axis. We show that this model can accurately reproduce images for a variety of time-averaged general relativistic magnetohydrodynamic (GRMHD) simulations, that it accurately recovers both the black hole and emission parameters from these simulations, and that it is sufficiently efficient to be used to measure these parameters in a Bayesian inference framework with radio interferometric data. We show that non-trivial topologies in the source image can result in non-trivial multi-modal solutions when applied to observations from a sparse array, such as the EHT 2017 observations of M87${}^*$. The presence of these degeneracies underscores the importance of employing Bayesian techniques that adequately sample the posterior space for the interpretation of EHT measurements. We fit our model to the EHT observations of M87${}^*$ and find a 95% Highest Posterior Density Interval (HPDI) for the mass-to-distance ratio of $\\theta_g\\in(2.84,3.75)\\,\\mu{\\rm as}$, and give an inclination of $\\theta_{\\rm o}\\in(11^\\circ,24^\\circ)$. These new measurements are consistent with mass measurements from the EHT and stellar dynamical estimates (e.g., Gebhardt et al. 2011; EHTC et al. 2019a,b; Liepold et al. 2023), and with the spin axis inclination inferred from properties of the M87${}^*$ jet (e.g., Walker et al. 2018).","sentences":["We propose a simple, analytic dual-cone accretion model for horizon scale images of the cores of Low-Luminosity Active Galactic Nuclei (LLAGN), including those observed by the Event Horizon Telescope (EHT).","Our underlying model is of synchrotron emission from an axisymmetric, magnetized plasma, which is constrained to flow within two oppositely oriented cones that are aligned with the black hole's spin axis.","We show that this model can accurately reproduce images for a variety of time-averaged general relativistic magnetohydrodynamic (GRMHD) simulations, that it accurately recovers both the black hole and emission parameters from these simulations, and that it is sufficiently efficient to be used to measure these parameters in a Bayesian inference framework with radio interferometric data.","We show that non-trivial topologies in the source image can result in non-trivial multi-modal solutions when applied to observations from a sparse array, such as the EHT 2017 observations of M87${}^*$. The presence of these degeneracies underscores the importance of employing Bayesian techniques that adequately sample the posterior space for the interpretation of EHT measurements.","We fit our model to the EHT observations of M87${}^*$ and find a 95% Highest Posterior Density Interval (HPDI) for the mass-to-distance ratio of $\\theta_g\\in(2.84,3.75)\\,\\mu{\\rm as}$, and give an inclination of $\\theta_{\\rm o}\\in(11^\\circ,24^\\circ)$. These new measurements are consistent with mass measurements from the EHT and stellar dynamical estimates (e.g., Gebhardt et al. 2011; EHTC et al. 2019a,b; Liepold et al. 2023), and with the spin axis inclination inferred from properties of the M87${}^*$ jet (e.g., Walker et al. 2018)."],"url":"http://arxiv.org/abs/2405.04749v1","category":"astro-ph.HE"}
{"created":"2024-05-08 00:59:09","title":"Probabilistic Forward Modeling of Galaxy Catalogs with Normalizing Flows","abstract":"Evaluating the accuracy and calibration of the redshift posteriors produced by photometric redshift (photo-z) estimators is vital for enabling precision cosmology and extragalactic astrophysics with modern wide-field photometric surveys. Evaluating photo-z posteriors on a per-galaxy basis is difficult, however, as real galaxies have a true redshift but not a true redshift posterior. We introduce PZFlow, a Python package for the probabilistic forward modeling of galaxy catalogs with normalizing flows. For catalogs simulated with PZFlow, there is a natural notion of \"true\" redshift posteriors that can be used for photo-z validation. We use PZFlow to simulate a photometric galaxy catalog where each galaxy has a redshift, noisy photometry, shape information, and a true redshift posterior. We also demonstrate the use of an ensemble of normalizing flows for photo-z estimation. We discuss how PZFlow will be used to validate the photo-z estimation pipeline of the Dark Energy Science Collaboration (DESC), and the wider applicability of PZFlow for statistical modeling of any tabular data.","sentences":["Evaluating the accuracy and calibration of the redshift posteriors produced by photometric redshift (photo-z) estimators is vital for enabling precision cosmology and extragalactic astrophysics with modern wide-field photometric surveys.","Evaluating photo-z posteriors on a per-galaxy basis is difficult, however, as real galaxies have a true redshift but not a true redshift posterior.","We introduce PZFlow, a Python package for the probabilistic forward modeling of galaxy catalogs with normalizing flows.","For catalogs simulated with PZFlow, there is a natural notion of \"true\" redshift posteriors that can be used for photo-z validation.","We use PZFlow to simulate a photometric galaxy catalog where each galaxy has a redshift, noisy photometry, shape information, and a true redshift posterior.","We also demonstrate the use of an ensemble of normalizing flows for photo-z estimation.","We discuss how PZFlow will be used to validate the photo-z estimation pipeline of the Dark Energy Science Collaboration (DESC), and the wider applicability of PZFlow for statistical modeling of any tabular data."],"url":"http://arxiv.org/abs/2405.04740v1","category":"astro-ph.IM"}
{"created":"2024-05-08 00:54:57","title":"Compactifications of Type II Supergravities in Superspace","abstract":"We propose a way to describe compactifications of Type II supergravities with fluxes directly from superspace. The superspace used is the one that arises naturally from the pure spinor superstring. We show how previous results of flux compactifications can be obtained from our method.","sentences":["We propose a way to describe compactifications of Type II supergravities with fluxes directly from superspace.","The superspace used is the one that arises naturally from the pure spinor superstring.","We show how previous results of flux compactifications can be obtained from our method."],"url":"http://arxiv.org/abs/2405.04736v1","category":"hep-th"}
{"created":"2024-05-07 23:54:13","title":"Reconstruction of surface electron spectrum and cyclotron motion in the CDW phase of Weyl semimetals","abstract":"Charge density wave (CDW) instability drastically affects the surface electron spectrum of a Weyl semimetal. We show that in the CDW phase, the Fermi arcs reconnect into either closed Fermi loops or Frieze patterns traversing the reconstructed surface mini-Brillouin zone. For the closed reconnection topology, the application of an out-of-plane magnetic field leads to a cyclotron motion of the surface electrons. We determine the cyclotron frequency as a function of the electron energy and the magnitude of the CDW gap $\\Delta$ for various orientations of the Fermi arcs. For weak coupling, the period of cyclotron motion is dominated by the time of traversal of the arc reconnection regions and is inversely proportional to $\\Delta$.","sentences":["Charge density wave (CDW) instability drastically affects the surface electron spectrum of a Weyl semimetal.","We show that in the CDW phase, the Fermi arcs reconnect into either closed Fermi loops or Frieze patterns traversing the reconstructed surface mini-Brillouin zone.","For the closed reconnection topology, the application of an out-of-plane magnetic field leads to a cyclotron motion of the surface electrons.","We determine the cyclotron frequency as a function of the electron energy and the magnitude of the CDW gap $\\Delta$ for various orientations of the Fermi arcs.","For weak coupling, the period of cyclotron motion is dominated by the time of traversal of the arc reconnection regions and is inversely proportional to $\\Delta$."],"url":"http://arxiv.org/abs/2405.04721v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-07 23:49:16","title":"First detection of CF$^{+}$ in the Large Magellanic Cloud","abstract":"CF$^{+}$ has been established as a valuable diagnostic tool for investigating photo-dissociation regions (PDRs) and fluorine abundances in the Milky Way. However, its role in extragalactic environments remains largely uncharted. Our objective is to explore the significance of CF$^{+}$ in the Large Magellanic Cloud (LMC) and assess its utility as a valuable probe for examining C$^{+}$ and fluorine abundances in external galaxies. We performed pointed CF$^{+}$ observations toward an active star-forming region, N113 in the LMC, using the Atacama Pathfinder EXperiment 12~m sub-millimeter telescope. We report the first discovery of CF$^{+}$ in the LMC through the successful detection of the CF$^{+}$ (2$\\to$1) and (3$\\to$2) lines. The excitation models indicate that CF$^{+}$ emission originates from dense PDRs characterized by an H$_{2}$ number density of $(0.5-7.9)\\times 10^{4}$~cm$^{-3}$ in N113. Our observations provide the first constraint on the fluorine abundance in molecular clouds in the LMC, disclosing a value of $\\lesssim 1.7\\times 10^{-9}$. This value is about an order of magnitude lower than those previously measured toward red giants in the LMC, indicative of fluorine deficiency in the molecular gas. The estimated column density ratio between C$^{+}$ and CF$^{+}$ appears to be lower than the anticipated equilibrium ratio derived from the fluorine abundance in red giants. Both phenomena can be explained by the deficiency of CF$^{+}$ caused by the freeze-out of its primary chemical precursor, HF, onto dust grains. The deficiency of CF$^{+}$ within molecular clouds suggests that the measurements presented in this work serve exclusively as conservative estimates, establishing lower bounds for both the fluorine abundance and C$^{+}$ column densities in external galaxies.","sentences":["CF$^{+}$ has been established as a valuable diagnostic tool for investigating photo-dissociation regions (PDRs) and fluorine abundances in the Milky Way.","However, its role in extragalactic environments remains largely uncharted.","Our objective is to explore the significance of CF$^{+}$ in the Large Magellanic Cloud (LMC) and assess its utility as a valuable probe for examining C$^{+}$ and fluorine abundances in external galaxies.","We performed pointed CF$^{+}$ observations toward an active star-forming region, N113 in the LMC, using the Atacama Pathfinder EXperiment 12~m sub-millimeter telescope.","We report the first discovery of CF$^{+}$ in the LMC through the successful detection of the CF$^{+}$ (2$\\to$1) and (3$\\to$2) lines.","The excitation models indicate that CF$^{+}$ emission originates from dense PDRs characterized by an H$_{2}$ number density of $(0.5-7.9)\\times 10^{4}$~cm$^{-3}$ in N113.","Our observations provide the first constraint on the fluorine abundance in molecular clouds in the LMC, disclosing a value of $\\lesssim 1.7\\times 10^{-9}$.","This value is about an order of magnitude lower than those previously measured toward red giants in the LMC, indicative of fluorine deficiency in the molecular gas.","The estimated column density ratio between C$^{+}$ and CF$^{+}$ appears to be lower than the anticipated equilibrium ratio derived from the fluorine abundance in red giants.","Both phenomena can be explained by the deficiency of CF$^{+}$ caused by the freeze-out of its primary chemical precursor, HF, onto dust grains.","The deficiency of CF$^{+}$ within molecular clouds suggests that the measurements presented in this work serve exclusively as conservative estimates, establishing lower bounds for both the fluorine abundance and C$^{+}$ column densities in external galaxies."],"url":"http://arxiv.org/abs/2405.04719v1","category":"astro-ph.GA"}
{"created":"2024-05-07 23:05:27","title":"Renormalization of the nonprojectable Horava theory","abstract":"We present the proof of renormalization of the Horava theory, in the nonprojectable version. We obtain a form of the quantum action that exhibits a manifest BRST-symmetry structure. Previous analysis have shown that the divergences produced by irregular loops cancel completely between them. The remaining divergences are local. The renormalization is achieved by using the approach developed by Barvinsky et al. with the background-field formalism.","sentences":["We present the proof of renormalization of the Horava theory, in the nonprojectable version.","We obtain a form of the quantum action that exhibits a manifest BRST-symmetry structure.","Previous analysis have shown that the divergences produced by irregular loops cancel completely between them.","The remaining divergences are local.","The renormalization is achieved by using the approach developed by Barvinsky et al.","with the background-field formalism."],"url":"http://arxiv.org/abs/2405.04708v1","category":"hep-th"}
{"created":"2024-05-07 22:55:55","title":"A wearable anti-gravity supplement to therapy does not improve arm function in chronic stroke: a randomized trial","abstract":"Background: Gravity confounds arm movement ability in post-stroke hemiparesis. Reducing its influence allows effective practice leading to recovery. Yet, there is a scarcity of wearable devices suitable for personalized use across diverse therapeutic activities in the clinic. Objective: In this study, we investigated the safety, feasibility, and efficacy of anti-gravity therapy using the ExoNET device in post-stroke participants. Methods: Twenty chronic stroke survivors underwent six, 45-minute occupational therapy sessions while wearing the ExoNET, randomized into either the treatment (ExoNET tuned to gravity-support) or control group (ExoNET tuned to slack condition). Clinical outcomes were evaluated by a blinded-rater at baseline, post, and six-week follow-up sessions. Kinetic, kinematic, and patient experience outcomes were also assessed. Results: Mixed-effect models showed a significant improvement in Box and Blocks scores in the post-intervention session for the treatment group (effect size: 2.1, p = .04). No significant effects were found between the treatment and control groups for ARAT scores and other clinical metrics. Direct kinetic effects revealed a significant reduction in muscle activity during free exploration with an effect size of (-7.12%, p< 005). There were no significant longitudinal kinetic or kinematic trends. Subject feedback suggested a generally positive perception of the anti-gravity therapy. Conclusions: Anti-gravity therapy with the ExoNET is a safe and feasible treatment for post-stroke rehabilitation. The device provided anti-gravity forces, did not encumber range of motion, and clinical metrics of anti-gravity therapy demonstrated improvements in gross manual dexterity. Further research is required to explore potential benefits in broader clinical metrics.","sentences":["Background: Gravity confounds arm movement ability in post-stroke hemiparesis.","Reducing its influence allows effective practice leading to recovery.","Yet, there is a scarcity of wearable devices suitable for personalized use across diverse therapeutic activities in the clinic.","Objective:","In this study, we investigated the safety, feasibility, and efficacy of anti-gravity therapy using the ExoNET device in post-stroke participants.","Methods: Twenty chronic stroke survivors underwent six, 45-minute occupational therapy sessions while wearing the ExoNET, randomized into either the treatment (ExoNET tuned to gravity-support) or control group (ExoNET tuned to slack condition).","Clinical outcomes were evaluated by a blinded-rater at baseline, post, and six-week follow-up sessions.","Kinetic, kinematic, and patient experience outcomes were also assessed.","Results: Mixed-effect models showed a significant improvement in Box and Blocks scores in the post-intervention session for the treatment group (effect size: 2.1, p = .04).","No significant effects were found between the treatment and control groups for ARAT scores and other clinical metrics.","Direct kinetic effects revealed a significant reduction in muscle activity during free exploration with an effect size of (-7.12%, p< 005).","There were no significant longitudinal kinetic or kinematic trends.","Subject feedback suggested a generally positive perception of the anti-gravity therapy.","Conclusions: Anti-gravity therapy with the ExoNET is a safe and feasible treatment for post-stroke rehabilitation.","The device provided anti-gravity forces, did not encumber range of motion, and clinical metrics of anti-gravity therapy demonstrated improvements in gross manual dexterity.","Further research is required to explore potential benefits in broader clinical metrics."],"url":"http://arxiv.org/abs/2405.04707v1","category":"cs.RO"}
{"created":"2024-05-07 22:04:53","title":"An efficient GPU-accelerated multi-source global fit pipeline for LISA data analysis","abstract":"The large-scale analysis task of deciphering gravitational wave signals in the LISA data stream will be difficult, requiring a large amount of computational resources and extensive development of computational methods. Its high dimensionality, multiple model types, and complicated noise profile require a global fit to all parameters and input models simultaneously. In this work, we detail our global fit algorithm, called \"Erebor,\" designed to accomplish this challenging task. It is capable of analysing current state-of-the-art datasets and then growing into the future as more pieces of the pipeline are completed and added. We describe our pipeline strategy, the algorithmic setup, and the results from our analysis of the LDC2A Sangria dataset, which contains Massive Black Hole Binaries, compact Galactic Binaries, and a parameterized noise spectrum whose parameters are unknown to the user. We recover posterior distributions for all 15 (6) of the injected MBHBs in the LDC2A training (hidden) dataset. We catalog $\\sim12000$ Galactic Binaries ($\\sim8000$ as high confidence detections) for both the training and hidden datasets. All of the sources and their posterior distributions are provided in publicly available catalogs.","sentences":["The large-scale analysis task of deciphering gravitational wave signals in the LISA data stream will be difficult, requiring a large amount of computational resources and extensive development of computational methods.","Its high dimensionality, multiple model types, and complicated noise profile require a global fit to all parameters and input models simultaneously.","In this work, we detail our global fit algorithm, called \"Erebor,\" designed to accomplish this challenging task.","It is capable of analysing current state-of-the-art datasets and then growing into the future as more pieces of the pipeline are completed and added.","We describe our pipeline strategy, the algorithmic setup, and the results from our analysis of the LDC2A Sangria dataset, which contains Massive Black Hole Binaries, compact Galactic Binaries, and a parameterized noise spectrum whose parameters are unknown to the user.","We recover posterior distributions for all 15 (6) of the injected MBHBs in the LDC2A training (hidden) dataset.","We catalog $\\sim12000$ Galactic Binaries ($\\sim8000$ as high confidence detections) for both the training and hidden datasets.","All of the sources and their posterior distributions are provided in publicly available catalogs."],"url":"http://arxiv.org/abs/2405.04690v1","category":"gr-qc"}
{"created":"2024-05-07 21:58:46","title":"Ultrafast dynamics of wavelength-sensitive magnons in unconventional compensated semiconducting antiferromagnet","abstract":"Antiferromagnet is a promising candidate for the next generation spintronic devices, benefiting from its ultrafast dynamics and spontaneous zero stray field. However, the understanding of their ultrafast spin behaviors is lacking due to the challenges of controlling/detecting the quenched net magnetization. Unconventional compensated semiconducting antiferromagnets present strong time-reversal symmetry breaking, spin splitting in the momentum space, and suitable bandgap for optical control/detection. Thus, it is a powerful platform to uncover the ultrafast dynamics of antiferromagnets. Here, we show an exotic wavelength-dependent spin dynamic in the unconventional compensated semiconducting antiferromagnet {\\alpha}-MnTe via time-resolved quadratic magneto-optical Kerr effect measurement, where the probing photon energy of the laser matches its bandgap. This direct excitation and detection of distinct magnon modes reveal varying spin behaviors and time characteristics in a broad temperature range. It originates from the spins triggered at different bands of electronic structures and is depicted in an energy transfer model among electrons, phonons, and magnons. Our study of exotic optical properties in this unconventional semiconducting antiferromagnet fulfills the missing information of spin evolution in the time domain and paves the way for its utilization in ultrafast spintronic devices.","sentences":["Antiferromagnet is a promising candidate for the next generation spintronic devices, benefiting from its ultrafast dynamics and spontaneous zero stray field.","However, the understanding of their ultrafast spin behaviors is lacking due to the challenges of controlling/detecting the quenched net magnetization.","Unconventional compensated semiconducting antiferromagnets present strong time-reversal symmetry breaking, spin splitting in the momentum space, and suitable bandgap for optical control/detection.","Thus, it is a powerful platform to uncover the ultrafast dynamics of antiferromagnets.","Here, we show an exotic wavelength-dependent spin dynamic in the unconventional compensated semiconducting antiferromagnet {\\alpha}-MnTe via time-resolved quadratic magneto-optical Kerr effect measurement, where the probing photon energy of the laser matches its bandgap.","This direct excitation and detection of distinct magnon modes reveal varying spin behaviors and time characteristics in a broad temperature range.","It originates from the spins triggered at different bands of electronic structures and is depicted in an energy transfer model among electrons, phonons, and magnons.","Our study of exotic optical properties in this unconventional semiconducting antiferromagnet fulfills the missing information of spin evolution in the time domain and paves the way for its utilization in ultrafast spintronic devices."],"url":"http://arxiv.org/abs/2405.04686v1","category":"physics.app-ph"}
{"created":"2024-05-07 21:54:24","title":"Clustering and spatial distribution of mitochondria in dendritic trees","abstract":"Neuronal dendrites form densely branched tree architectures through which mitochondria must be distributed to supply the cell's energetic needs. Dendritic mitochondria circulate through the tree, undergoing fusion and fission to form clusters of varying sizes. We present a mathematical model for the distribution of such actively-driven particles in a branched geometry. Our model demonstrates that `balanced' trees (wherein cross-sectional area is conserved across junctions and thicker branches support more bushy subtrees) enable symmetric yet distally enriched particle distributions and promote dispersion into smaller clusters. These results highlight the importance of tree architecture and radius-dependent fusion in governing the distribution of neuronal mitochondria.","sentences":["Neuronal dendrites form densely branched tree architectures through which mitochondria must be distributed to supply the cell's energetic needs.","Dendritic mitochondria circulate through the tree, undergoing fusion and fission to form clusters of varying sizes.","We present a mathematical model for the distribution of such actively-driven particles in a branched geometry.","Our model demonstrates that `balanced' trees (wherein cross-sectional area is conserved across junctions and thicker branches support more bushy subtrees) enable symmetric yet distally enriched particle distributions and promote dispersion into smaller clusters.","These results highlight the importance of tree architecture and radius-dependent fusion in governing the distribution of neuronal mitochondria."],"url":"http://arxiv.org/abs/2405.04684v1","category":"physics.bio-ph"}
{"created":"2024-05-07 20:31:32","title":"Basis set extrapolation from the vanishing counterpoise correction condition","abstract":"Basis set extrapolations are typically rationalized either from analytical arguments involving the partial-wave or principal expansions of the correlation energy in helium-like systems, or from fitting extrapolation parameters to reference energetics for a small(ish) training set. Seeking to avoid both, we explore a third alternative: extracting extrapolation parameters from the requirement that the BSSE (basis set superposition error) should vanish at the complete basis set limit. We find this to be a viable approach provided that the underlying basis sets are not too small and reasonably well balanced. For basis sets not augmented by diffuse functions, BSSE minimization and energy fitting yield quite similar parameters.","sentences":["Basis set extrapolations are typically rationalized either from analytical arguments involving the partial-wave or principal expansions of the correlation energy in helium-like systems, or from fitting extrapolation parameters to reference energetics for a small(ish) training set.","Seeking to avoid both, we explore a third alternative: extracting extrapolation parameters from the requirement that the BSSE (basis set superposition error) should vanish at the complete basis set limit.","We find this to be a viable approach provided that the underlying basis sets are not too small and reasonably well balanced.","For basis sets not augmented by diffuse functions, BSSE minimization and energy fitting yield quite similar parameters."],"url":"http://arxiv.org/abs/2405.04658v1","category":"physics.chem-ph"}
{"created":"2024-05-07 20:26:10","title":"Metabolism, information, and viability in a simulated physically-plausible protocell","abstract":"Critical experimental design issues connecting energy transduction and inheritable information within a protocell are explored and elucidated. The protocell design utilizes a photo-driven energy transducer (a ruthenium complex) to turn resource molecules into building blocks, in a manner that is modulated by a combinatorial DNA-based co-factor. This co-factor molecule serves as part of an electron relay for the energy transduction mechanism, where the charge-transport rates depend on the sequence that contains an oxo-guanine. The co-factor also acts as a store of inheritable information due to its ability to replicate non-enzymatically through template-directed ligation. Together, the energy transducer and the co-factor act as a metabolic catalyst that produces co-factor DNA building blocks as well as fatty acids (from picolinium ester and modified DNA oligomers), where the fatty acids self-assemble into vesicles on which exterior surface both the co-factor (DNA) and the energy transducer are anchored with hydrophobic tails. Here we use simulations to study how the co-factor sequence determines its fitness as reflected by charge transfer and replication rates. To estimate the impact on the protocell, we compare these rates with previously measured metabolic rates from a similar system where the charge transfer is directly between the ruthenium complex and the oxo-guanine (without DNA replication and charge transport). Replication and charge transport turn out to have different and often opposing sequence requirements. Functional information of the co-factor molecules is used to probe the feasibility of randomly picking co-factor sequences from a limited population of co-factors molecules, where a good co-factor can enhance both metabolic biomass production and its own replication rate.","sentences":["Critical experimental design issues connecting energy transduction and inheritable information within a protocell are explored and elucidated.","The protocell design utilizes a photo-driven energy transducer (a ruthenium complex) to turn resource molecules into building blocks, in a manner that is modulated by a combinatorial DNA-based co-factor.","This co-factor molecule serves as part of an electron relay for the energy transduction mechanism, where the charge-transport rates depend on the sequence that contains an oxo-guanine.","The co-factor also acts as a store of inheritable information due to its ability to replicate non-enzymatically through template-directed ligation.","Together, the energy transducer and the co-factor act as a metabolic catalyst that produces co-factor DNA building blocks as well as fatty acids (from picolinium ester and modified DNA oligomers), where the fatty acids self-assemble into vesicles on which exterior surface both the co-factor (DNA) and the energy transducer are anchored with hydrophobic tails.","Here we use simulations to study how the co-factor sequence determines its fitness as reflected by charge transfer and replication rates.","To estimate the impact on the protocell, we compare these rates with previously measured metabolic rates from a similar system where the charge transfer is directly between the ruthenium complex and the oxo-guanine (without DNA replication and charge transport).","Replication and charge transport turn out to have different and often opposing sequence requirements.","Functional information of the co-factor molecules is used to probe the feasibility of randomly picking co-factor sequences from a limited population of co-factors molecules, where a good co-factor can enhance both metabolic biomass production and its own replication rate."],"url":"http://arxiv.org/abs/2405.04654v1","category":"physics.bio-ph"}
{"created":"2024-05-07 20:21:17","title":"AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large Language Models","abstract":"LGBTQ+ community face disproportionate mental health challenges, including higher rates of depression, anxiety, and suicidal ideation. Research has shown that LGBTQ+ people have been using large language model-based chatbots, such as ChatGPT, for their mental health needs. Despite the potential for immediate support and anonymity these chatbots offer, concerns regarding their capacity to provide empathetic, accurate, and affirming responses remain. In response to these challenges, we propose a framework for evaluating the affirmativeness of LLMs based on principles of affirmative therapy, emphasizing the need for attitudes, knowledge, and actions that support and validate LGBTQ+ experiences. We propose a combination of qualitative and quantitative analyses, hoping to establish benchmarks for \"Affirmative AI,\" ensuring that LLM-based chatbots can provide safe, supportive, and effective mental health support to LGBTQ+ individuals. We benchmark LLM affirmativeness not as a mental health solution for LGBTQ+ individuals or to claim it resolves their mental health issues, as we highlight the need to consider complex discrimination in the LGBTQ+ community when designing technological aids. Our goal is to evaluate LLMs for LGBTQ+ mental health support since many in the community already use them, aiming to identify potential harms of using general-purpose LLMs in this context.","sentences":["LGBTQ+ community face disproportionate mental health challenges, including higher rates of depression, anxiety, and suicidal ideation.","Research has shown that LGBTQ+ people have been using large language model-based chatbots, such as ChatGPT, for their mental health needs.","Despite the potential for immediate support and anonymity these chatbots offer, concerns regarding their capacity to provide empathetic, accurate, and affirming responses remain.","In response to these challenges, we propose a framework for evaluating the affirmativeness of LLMs based on principles of affirmative therapy, emphasizing the need for attitudes, knowledge, and actions that support and validate LGBTQ+ experiences.","We propose a combination of qualitative and quantitative analyses, hoping to establish benchmarks for \"Affirmative AI,\" ensuring that LLM-based chatbots can provide safe, supportive, and effective mental health support to LGBTQ+ individuals.","We benchmark LLM affirmativeness not as a mental health solution for LGBTQ+ individuals or to claim it resolves their mental health issues, as we highlight the need to consider complex discrimination in the LGBTQ+ community when designing technological aids.","Our goal is to evaluate LLMs for LGBTQ+ mental health support since many in the community already use them, aiming to identify potential harms of using general-purpose LLMs in this context."],"url":"http://arxiv.org/abs/2405.04652v1","category":"cs.HC"}
{"created":"2024-05-07 20:09:37","title":"First direct observations of interplanetary shock impact angle effects on actual geomagnetically induced currents: The case of the Finnish natural gas pipeline system","abstract":"The impact of interplanetary (IP) shocks on the Earth's magnetosphere can greatly disturb the geomagnetic field and electric currents in the magnetosphere-ionosphere system. At high latitudes, the current systems most affected by the shocks are the auroral electrojet currents. These currents then generate ground geomagnetically induced currents (GICs) that couple with and are highly detrimental to ground artificial conductors including power transmission lines, oil/gas pipelines, railways, and submarine cables. Recent research has shown that the shock impact angle, the angle the shock normal vector performs with the Sun-Earth line, plays a major role in controlling the subsequent geomagnetic activity. More specifically, due to more symmetric magnetospheric compressions, nearly frontal shocks are usually more geoeffective than highly inclined shocks. In this study, we utilize a subset (332 events) of a shock list with more than 600 events to investigate, for the first time, shock impact angle effects on the subsequent GICs right after shock impact (compression effects) and several minutes after shock impact (substorm-like effects). We use GIC recordings from the Finnish natural gas pipeline performed near the M\\\"ants\\\"al\\\"a compression station in southern Finland. We find that GIC peaks (> 5 A) occurring after shock impacts are mostly caused by nearly frontal shocks and occur in the post-noon/dusk magnetic local time sector. These GIC peaks are presumably triggered by partial ring current intensifications in the dusk sector. On the other hand, more intense GIC peaks (> 20 A) generally occur several minutes after shock impacts and are located around the magnetic midnight terminator. These GIC peaks are most likely caused by intense energetic particle injections from the magnetotail which frequently occur during substorms.","sentences":["The impact of interplanetary (IP) shocks on the Earth's magnetosphere can greatly disturb the geomagnetic field and electric currents in the magnetosphere-ionosphere system.","At high latitudes, the current systems most affected by the shocks are the auroral electrojet currents.","These currents then generate ground geomagnetically induced currents (GICs) that couple with and are highly detrimental to ground artificial conductors including power transmission lines, oil/gas pipelines, railways, and submarine cables.","Recent research has shown that the shock impact angle, the angle the shock normal vector performs with the Sun-Earth line, plays a major role in controlling the subsequent geomagnetic activity.","More specifically, due to more symmetric magnetospheric compressions, nearly frontal shocks are usually more geoeffective than highly inclined shocks.","In this study, we utilize a subset (332 events) of a shock list with more than 600 events to investigate, for the first time, shock impact angle effects on the subsequent GICs right after shock impact (compression effects) and several minutes after shock impact (substorm-like effects).","We use GIC recordings from the Finnish natural gas pipeline performed near the M\\\"ants\\\"al\\\"a compression station in southern Finland.","We find that GIC peaks (> 5 A) occurring after shock impacts are mostly caused by nearly frontal shocks and occur in the post-noon/dusk magnetic local time sector.","These GIC peaks are presumably triggered by partial ring current intensifications in the dusk sector.","On the other hand, more intense GIC peaks (> 20 A) generally occur several minutes after shock impacts and are located around the magnetic midnight terminator.","These GIC peaks are most likely caused by intense energetic particle injections from the magnetotail which frequently occur during substorms."],"url":"http://arxiv.org/abs/2405.04647v1","category":"physics.space-ph"}
{"created":"2024-05-07 20:08:49","title":"Lipid-mediated hydrophobic gating in the BK potassium channel","abstract":"The large-conductance, calcium-activated potassium (BK) channel lacks the typical intracellular bundle-crossing gate present in most ion channels of the 6TM family. This observation, initially inferred from Ca$^{2+}$-free-pore accessibility experiments and recently corroborated by a CryoEM structure of the non-conductive state, raises a puzzling question: how can gating occur in absence of steric hindrance? To answer this question, we carried out molecular simulations and accurate free energy calculations to obtain a microscopic picture of the sequence of events that, starting from a Ca$^{2+}$-free state leads to ion conduction upon Ca$^{2+}$ binding. Our results highlight an unexpected role for annular lipids, which turn out to be an integral part of the gating machinery. Due to the presence of fenestrations, the \"closed\" Ca$^{2+}$-free pore can be occupied by the methyl groups from the lipid alkyl chains. This dynamic occupancy triggers and stabilizes the nucleation of a vapor bubble into the inner pore cavity, thus hindering ion conduction. By contrast, Ca$^{2+}$ binding results into a displacement of these lipids outside the inner cavity, lowering the hydrophobicity of this region and thus allowing for pore hydration and conduction. This lipid-mediated hydrophobic gating rationalizes several seemingly problematic experimental observations, including the state-dependent pore accessibility of blockers.","sentences":["The large-conductance, calcium-activated potassium (BK) channel lacks the typical intracellular bundle-crossing gate present in most ion channels of the 6TM family.","This observation, initially inferred from Ca$^{2+}$-free-pore accessibility experiments and recently corroborated by a CryoEM structure of the non-conductive state, raises a puzzling question: how can gating occur in absence of steric hindrance?","To answer this question, we carried out molecular simulations and accurate free energy calculations to obtain a microscopic picture of the sequence of events that, starting from a Ca$^{2+}$-free state leads to ion conduction upon Ca$^{2+}$ binding.","Our results highlight an unexpected role for annular lipids, which turn out to be an integral part of the gating machinery.","Due to the presence of fenestrations, the \"closed\" Ca$^{2+}$-free pore can be occupied by the methyl groups from the lipid alkyl chains.","This dynamic occupancy triggers and stabilizes the nucleation of a vapor bubble into the inner pore cavity, thus hindering ion conduction.","By contrast, Ca$^{2+}$ binding results into a displacement of these lipids outside the inner cavity, lowering the hydrophobicity of this region and thus allowing for pore hydration and conduction.","This lipid-mediated hydrophobic gating rationalizes several seemingly problematic experimental observations, including the state-dependent pore accessibility of blockers."],"url":"http://arxiv.org/abs/2405.04644v1","category":"q-bio.BM"}
{"created":"2024-05-07 20:06:35","title":"Spatially Resolved Electron Number Density Measurements Via Coherent Microwave Scattering","abstract":"This paper reports on the use of coherent microwave scattering (CMS) for spatially resolved electron number density measurements of elongated plasma structures induced at mid-IR femtosecond filamentation. The presented studies comprise one-dimensional mapping of laser filaments induced via 3.9 um, 127.3 fs laser pulses at output energies up to 15 mJ. The axial electron number density and laser intensity were measured to be invariant along the entire filament length for the tested laser pulse energies 5-15 mJ (2x$10^{15}$ cm$^{-3}$ and 30-40 TW/cm$^2$, respectively) which supports that intensity clamping conditions were achieved in the experiments. The proposed approach enables unprecedented, currently unavailable capabilities to conduct direct, absolute, and longitudinally resolved measurements of electron number density in laser filaments and to precisely characterize conditions associated with self-focusing and intensity clamping.","sentences":["This paper reports on the use of coherent microwave scattering (CMS) for spatially resolved electron number density measurements of elongated plasma structures induced at mid-IR femtosecond filamentation.","The presented studies comprise one-dimensional mapping of laser filaments induced via 3.9 um, 127.3 fs laser pulses at output energies up to 15 mJ. The axial electron number density and laser intensity were measured to be invariant along the entire filament length for the tested laser pulse energies 5-15 mJ (2x$10^{15}$ cm$^{-3}$ and 30-40 TW/cm$^2$, respectively) which supports that intensity clamping conditions were achieved in the experiments.","The proposed approach enables unprecedented, currently unavailable capabilities to conduct direct, absolute, and longitudinally resolved measurements of electron number density in laser filaments and to precisely characterize conditions associated with self-focusing and intensity clamping."],"url":"http://arxiv.org/abs/2405.04643v1","category":"physics.plasm-ph"}
{"created":"2024-05-07 20:05:25","title":"First Measurement of Correlated Charge Noise in Superconducting Qubits at an Underground Facility","abstract":"We measure space- and time-correlated charge jumps on a four-qubit device, operating 107 meters below the Earth's surface in a low-radiation, cryogenic facility designed for the characterization of low-threshold particle detectors. The rock overburden of this facility reduces the cosmic ray muon flux by over 99% compared to laboratories at sea level. Combined with 4$\\pi$ coverage of a movable lead shield, this facility enables quantifiable control over the flux of ionizing radiation on the qubit device. Long-time-series charge tomography measurements on these weakly charge-sensitive qubits capture discontinuous jumps in the induced charge on the qubit islands, corresponding to the interaction of ionizing radiation with the qubit substrate. The rate of these charge jumps scales with the flux of ionizing radiation on the qubit package, as characterized by a series of independent measurements on another energy-resolving detector operating simultaneously in the same cryostat with the qubits. Using lead shielding, we achieve a minimum charge jump rate of 0.19$^{+0.04}_{-0.03}$ mHz, almost an order of magnitude lower than that measured in surface tests, but a factor of roughly eight higher than expected based on reduction of ambient gammas alone. We operate four qubits for over 22 consecutive hours with zero correlated charge jumps at length scales above three millimeters.","sentences":["We measure space- and time-correlated charge jumps on a four-qubit device, operating 107 meters below the Earth's surface in a low-radiation, cryogenic facility designed for the characterization of low-threshold particle detectors.","The rock overburden of this facility reduces the cosmic ray muon flux by over 99% compared to laboratories at sea level.","Combined with 4$\\pi$ coverage of a movable lead shield, this facility enables quantifiable control over the flux of ionizing radiation on the qubit device.","Long-time-series charge tomography measurements on these weakly charge-sensitive qubits capture discontinuous jumps in the induced charge on the qubit islands, corresponding to the interaction of ionizing radiation with the qubit substrate.","The rate of these charge jumps scales with the flux of ionizing radiation on the qubit package, as characterized by a series of independent measurements on another energy-resolving detector operating simultaneously in the same cryostat with the qubits.","Using lead shielding, we achieve a minimum charge jump rate of 0.19$^{+0.04}_{-0.03}$ mHz, almost an order of magnitude lower than that measured in surface tests, but a factor of roughly eight higher than expected based on reduction of ambient gammas alone.","We operate four qubits for over 22 consecutive hours with zero correlated charge jumps at length scales above three millimeters."],"url":"http://arxiv.org/abs/2405.04642v1","category":"quant-ph"}
{"created":"2024-05-07 20:00:00","title":"Momentum non-conservation in a scalar quantum field theory with a planar $\u03b8$ interface","abstract":"Motivated by the recent interest aroused by non-dynamical axionic electrodynamics in the context of topological insulators and Weyl semimetals, we discuss a simple model of the magnetoelectric effect in terms of a $\\theta$-scalar field that interacts through a delta-like potential located at a planar interface. Thus, in the bulk regions the field is constructed by standard free waves with the absence of evanescent components. These waves have to be combined into linear superposition to account for the boundary conditions at the interface in order to yield the corresponding normal modes. Our aim is twofold: first we quantize the $\\theta$-scalar field using the normal modes in the canonical approach and then we look for applications emphasizing the effect of momentum non-conservation due to the presence of the interface. To this end we calculate the decay of a standard scalar particle into two $\\theta$-scalar particles showing the opening of new decay channels. As a second application we deal with the two body scattering of standard charged scalar particles mediated by a $\\theta$-scalar particle, focusing on the momentum non-conserving contribution of the scattering amplitude ${\\cal M}^{NC}$. We define a generalization of the usual cross section in order to quantify the emergence of these events. We also study the allowed kinematical region for momentum non-conservation as well as the position of the poles of the amplitude ${\\cal M}^{NC}$. Finally, the ratio of the magnitudes between ${\\cal M}^{NC}$ and the momentum conserving amplitude is discussed in the appropriate region of momentum space.","sentences":["Motivated by the recent interest aroused by non-dynamical axionic electrodynamics in the context of topological insulators and Weyl semimetals, we discuss a simple model of the magnetoelectric effect in terms of a $\\theta$-scalar field that interacts through a delta-like potential located at a planar interface.","Thus, in the bulk regions the field is constructed by standard free waves with the absence of evanescent components.","These waves have to be combined into linear superposition to account for the boundary conditions at the interface in order to yield the corresponding normal modes.","Our aim is twofold: first we quantize the $\\theta$-scalar field using the normal modes in the canonical approach and then we look for applications emphasizing the effect of momentum non-conservation due to the presence of the interface.","To this end we calculate the decay of a standard scalar particle into two $\\theta$-scalar particles showing the opening of new decay channels.","As a second application we deal with the two body scattering of standard charged scalar particles mediated by a $\\theta$-scalar particle, focusing on the momentum non-conserving contribution of the scattering amplitude ${\\cal M}^{NC}$. We define a generalization of the usual cross section in order to quantify the emergence of these events.","We also study the allowed kinematical region for momentum non-conservation as well as the position of the poles of the amplitude ${\\cal M}^{NC}$.","Finally, the ratio of the magnitudes between ${\\cal M}^{NC}$ and the momentum conserving amplitude is discussed in the appropriate region of momentum space."],"url":"http://arxiv.org/abs/2405.04640v1","category":"hep-th"}
{"created":"2024-05-07 19:54:24","title":"Impact of Dimensionality on the Magnetocaloric Effect in Two-dimensional Magnets","abstract":"Magnetocaloric materials, which exploit reversible temperature changes induced by magnetic field variations, are promising for advancing energy-efficient cooling technologies. The potential integration of two-dimensional materials into magnetocaloric systems represents an emerging opportunity to enhance the magnetocaloric cooling efficiency. In this study, we use atomistic spin dynamics simulations based on first-principles parameters to systematically evaluate how magnetocaloric properties transition from three-dimensional (3D) to two-dimensional (2D) ferromagnetic materials. We find that 2D features such as reduced Curie temperature, sharper magnetic transition, and higher magnetic susceptibility are beneficial for magnetocaloric applications, while the relatively higher lattice heat capacity in 2D can compromise achievable adiabatic temperature changes. We further propose GdSi$_2$ as a promising 2D magnetocaloric material near hydrogen liquefaction temperature. Our analysis offers valuable theoretical insights into the magnetocaloric effect in 2D ferromagnets and demonstrates that 2D ferromagnets hold promise for cooling and thermal management applications in compact and miniaturized nanodevices.","sentences":["Magnetocaloric materials, which exploit reversible temperature changes induced by magnetic field variations, are promising for advancing energy-efficient cooling technologies.","The potential integration of two-dimensional materials into magnetocaloric systems represents an emerging opportunity to enhance the magnetocaloric cooling efficiency.","In this study, we use atomistic spin dynamics simulations based on first-principles parameters to systematically evaluate how magnetocaloric properties transition from three-dimensional (3D) to two-dimensional (2D) ferromagnetic materials.","We find that 2D features such as reduced Curie temperature, sharper magnetic transition, and higher magnetic susceptibility are beneficial for magnetocaloric applications, while the relatively higher lattice heat capacity in 2D can compromise achievable adiabatic temperature changes.","We further propose GdSi$_2$ as a promising 2D magnetocaloric material near hydrogen liquefaction temperature.","Our analysis offers valuable theoretical insights into the magnetocaloric effect in 2D ferromagnets and demonstrates that 2D ferromagnets hold promise for cooling and thermal management applications in compact and miniaturized nanodevices."],"url":"http://arxiv.org/abs/2405.04639v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-07 19:48:45","title":"Validation of EUHFORIA cone and spheromak Coronal Mass Ejection Models","abstract":"Aims. We present the validation results for arrival times and geomagnetic impact of Coronal Mass Ejections (CMEs), using the cone and spheromak CME models implemented in EUropean Heliospheric FORecasting Information Asset (EUHFORIA). Validating numerical models is crucial in ensuring their accuracy and performance with respect to real data.   Methods. We compare CME plasma and magnetic field signatures, measured in situ by satellites at the L1 point, with the simulation output of EUHFORIA. The validation of this model was carried out by using two datasets in order to ensure a comprehensive evaluation. The first dataset focuses on 16 CMEs that arrived at the Earth, offering specific insights into the model's accuracy in predicting arrival time and geomagnetic impact. Meanwhile, the second dataset encompasses all CMEs observed over eight months within Solar Cycle 24, regardless of whether they arrived at Earth, covering periods of both solar minimum and maximum activity. This second dataset enables a more comprehensive evaluation of the model's predictive precision in term of CME arrivals and misses.   Results. Our results show that EUHFORIA provides good estimates in terms of arrival times, with root mean square errors (RMSE) values of 9 hours. Regarding the number of correctly predicted ICME arrivals and misses, we find a 75% probability of detection in a 12 hours time window and 100% probability of detection in a 24 hours time window. The geomagnetic impact forecasts, measured by the $K_p$ index, provide different degrees of accuracy, ranging from 31% to 69%. These results validate the use of cone and spheromak CMEs for real-time space weather forecasting.","sentences":["Aims.","We present the validation results for arrival times and geomagnetic impact of Coronal Mass Ejections (CMEs), using the cone and spheromak CME models implemented in EUropean Heliospheric FORecasting Information Asset (EUHFORIA).","Validating numerical models is crucial in ensuring their accuracy and performance with respect to real data.   ","Methods.","We compare CME plasma and magnetic field signatures, measured in situ by satellites at the L1 point, with the simulation output of EUHFORIA.","The validation of this model was carried out by using two datasets in order to ensure a comprehensive evaluation.","The first dataset focuses on 16 CMEs that arrived at the Earth, offering specific insights into the model's accuracy in predicting arrival time and geomagnetic impact.","Meanwhile, the second dataset encompasses all CMEs observed over eight months within Solar Cycle 24, regardless of whether they arrived at Earth, covering periods of both solar minimum and maximum activity.","This second dataset enables a more comprehensive evaluation of the model's predictive precision in term of CME arrivals and misses.   Results.","Our results show that EUHFORIA provides good estimates in terms of arrival times, with root mean square errors (RMSE) values of 9 hours.","Regarding the number of correctly predicted ICME arrivals and misses, we find a 75% probability of detection in a 12 hours time window and 100% probability of detection in a 24 hours time window.","The geomagnetic impact forecasts, measured by the $K_p$ index, provide different degrees of accuracy, ranging from 31% to 69%.","These results validate the use of cone and spheromak CMEs for real-time space weather forecasting."],"url":"http://arxiv.org/abs/2405.04637v1","category":"astro-ph.SR"}
{"created":"2024-05-07 19:38:06","title":"Mysterious anomalies in Earth's atmosphere and strongly interacting Dark Matter","abstract":"It has been recently argued [1-3] that there is a number of mysterious observations which are very hard to explain by conventional physics. The mysterious anomalies include (but not limited) to such unexpected correlations as temperature variation in stratosphere, the total electron content (TEC) of the Earths atmosphere, the earthquake activity from one hand, and positions of the planets from another hand. It has been hypothesized in [1-3] that the corresponding mysterious correlations is a result of the \"streaming invisible matter\" which suddenly become very strongly interacting material when enter the Earth's atmosphere. We propose that some of these (and many other) mysteries might be result of rare (but energetic) events when the so-called axion quark nuggets (AQN) hit the Earth. In different words, we identify \"streaming invisible matter\" conjectured by [1-3] with the AQNs. Therefore, we offer a specific microscopical mechanism which could shed some light on the mysterious correlations reported in [1-3]. One should emphasize that the AQN model was originally invented long ago to explain the observed similarity between the dark and the visible components in the Universe, i.e. $\\Omega_{\\rm DM}\\sim \\Omega_{\\rm visible}$; it was not invented to fit the observed anomalies which represent the topic of this work. We support this proposal by demonstrating that intensity and the spectral features of the AQN induced events are consistent with the corresponding characteristics of the puzzling observations as reported in [1-3].","sentences":["It has been recently argued [1-3] that there is a number of mysterious observations which are very hard to explain by conventional physics.","The mysterious anomalies include (but not limited) to such unexpected correlations as temperature variation in stratosphere, the total electron content (TEC) of the Earths atmosphere, the earthquake activity from one hand, and positions of the planets from another hand.","It has been hypothesized in [1-3] that the corresponding mysterious correlations is a result of the \"streaming invisible matter\" which suddenly become very strongly interacting material when enter the Earth's atmosphere.","We propose that some of these (and many other) mysteries might be result of rare (but energetic) events when the so-called axion quark nuggets (AQN) hit the Earth.","In different words, we identify \"streaming invisible matter\" conjectured by [1-3] with the AQNs.","Therefore, we offer a specific microscopical mechanism which could shed some light on the mysterious correlations reported in [1-3].","One should emphasize that the AQN model was originally invented long ago to explain the observed similarity between the dark and the visible components in the Universe, i.e. $\\Omega_{\\rm DM}\\sim \\Omega_{\\rm visible}$; it was not invented to fit the observed anomalies which represent the topic of this work.","We support this proposal by demonstrating that intensity and the spectral features of the AQN induced events are consistent with the corresponding characteristics of the puzzling observations as reported in [1-3]."],"url":"http://arxiv.org/abs/2405.04635v1","category":"hep-ph"}
