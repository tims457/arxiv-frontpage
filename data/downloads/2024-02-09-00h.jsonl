{"created":"2024-02-07 18:59:31","title":"Edu-ConvoKit: An Open-Source Library for Education Conversation Data","abstract":"We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository.","sentences":["We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education.","Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access.","We address these challenges with Edu-ConvoKit.","Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ).","Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- .","We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository."],"url":"http://arxiv.org/abs/2402.05111v1","category":"cs.CL"}
{"created":"2024-02-07 18:57:37","title":"Image captioning for Brazilian Portuguese using GRIT model","abstract":"This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.","sentences":["This work presents the early development of a model of image captioning for the Brazilian Portuguese language.","We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work.","GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions.","The GRIT method emerged as a proposal to be a more efficient way to generate image captioning.","In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language."],"url":"http://arxiv.org/abs/2402.05106v1","category":"cs.CV"}
{"created":"2024-02-07 18:56:20","title":"On gauge transformations in twistless torsional Newton--Cartan geometry","abstract":"We observe that in type I twistless torsional Newton--Cartan (TTNC) geometry, one can always find (at least locally) a gauge transformation that transforms a specific locally Galilei-invariant function -- that we dub the `locally Galilei-invariant potential' -- to zero, due to the corresponding equation for the gauge parameter taking the form of a Hamilton--Jacobi equation. In the case of type II TTNC geometry, the same gauge fixing may locally be performed by subleading spatial diffeomorphisms. We show (a) how this generalises a classical result in standard Newton--Cartan geometry, and (b) how it allows to parametrise the metric structure of a Galilei manifold as well as the gauge equivalence class of the Bargmann form of TTNC geometry in terms of just the space metric and a unit timelike vector field.","sentences":["We observe that in type I twistless torsional Newton--Cartan (TTNC) geometry, one can always find (at least locally) a gauge transformation that transforms a specific locally Galilei-invariant function -- that we dub the `locally Galilei-invariant potential' -- to zero, due to the corresponding equation for the gauge parameter taking the form of a Hamilton--Jacobi equation.","In the case of type II TTNC geometry, the same gauge fixing may locally be performed by subleading spatial diffeomorphisms.","We show (a) how this generalises a classical result in standard Newton--Cartan geometry, and (b) how it allows to parametrise the metric structure of a Galilei manifold as well as the gauge equivalence class of the Bargmann form of TTNC geometry in terms of just the space metric and a unit timelike vector field."],"url":"http://arxiv.org/abs/2402.05105v1","category":"gr-qc"}
{"created":"2024-02-07 18:55:41","title":"You Can REST Now: Automated Specification Inference and Black-Box Testing of RESTful APIs with Large Language Models","abstract":"RESTful APIs are popular web services, requiring documentation to ease their comprehension, reusability and testing practices. The OpenAPI Specification (OAS) is a widely adopted and machine-readable format used to document such APIs. However, manually documenting RESTful APIs is a time-consuming and error-prone task, resulting in unavailable, incomplete, or imprecise documentation. As RESTful API testing tools require an OpenAPI specification as input, insufficient or informal documentation hampers testing quality.   Recently, Large Language Models (LLMs) have demonstrated exceptional abilities to automate tasks based on their colossal training data. Accordingly, such capabilities could be utilized to assist the documentation and testing process of RESTful APIs.   In this paper, we present RESTSpecIT, the first automated RESTful API specification inference and black-box testing approach leveraging LLMs. The approach requires minimal user input compared to state-of-the-art RESTful API inference and testing tools; Given an API name and an LLM key, HTTP requests are generated and mutated with data returned by the LLM. By sending the requests to the API endpoint, HTTP responses can be analyzed for inference and testing purposes. RESTSpecIT utilizes an in-context prompt masking strategy, requiring no model fine-tuning. Our evaluation demonstrates that RESTSpecIT is capable of: (1) inferring specifications with 85.05% of GET routes and 81.05% of query parameters found on average, (2) discovering undocumented and valid routes and parameters, and (3) uncovering server errors in RESTful APIs. Inferred specifications can also be used as testing tool inputs.","sentences":["RESTful APIs are popular web services, requiring documentation to ease their comprehension, reusability and testing practices.","The OpenAPI Specification (OAS) is a widely adopted and machine-readable format used to document such APIs.","However, manually documenting RESTful APIs is a time-consuming and error-prone task, resulting in unavailable, incomplete, or imprecise documentation.","As RESTful API testing tools require an OpenAPI specification as input, insufficient or informal documentation hampers testing quality.   ","Recently, Large Language Models (LLMs) have demonstrated exceptional abilities to automate tasks based on their colossal training data.","Accordingly, such capabilities could be utilized to assist the documentation and testing process of RESTful APIs.   ","In this paper, we present RESTSpecIT, the first automated RESTful API specification inference and black-box testing approach leveraging LLMs.","The approach requires minimal user input compared to state-of-the-art RESTful API inference and testing tools; Given an API name and an LLM key, HTTP requests are generated and mutated with data returned by the LLM.","By sending the requests to the API endpoint, HTTP responses can be analyzed for inference and testing purposes.","RESTSpecIT utilizes an in-context prompt masking strategy, requiring no model fine-tuning.","Our evaluation demonstrates that RESTSpecIT is capable of: (1) inferring specifications with 85.05% of GET routes and 81.05% of query parameters found on average, (2) discovering undocumented and valid routes and parameters, and (3) uncovering server errors in RESTful APIs.","Inferred specifications can also be used as testing tool inputs."],"url":"http://arxiv.org/abs/2402.05102v1","category":"cs.SE"}
{"created":"2024-02-07 18:53:01","title":"Hydragen: High-Throughput LLM Inference with Shared Prefixes","abstract":"Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix length. Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.","sentences":["Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users.","LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt.","Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch.","In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes.","Hydragen computes attention over the shared prefix and unique suffixes separately.","This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications.","Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix length.","Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%.","Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%."],"url":"http://arxiv.org/abs/2402.05099v1","category":"cs.LG"}
{"created":"2024-02-07 18:51:49","title":"On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling","abstract":"We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.","sentences":["We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function.","We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks).","Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work.","We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions.","Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference."],"url":"http://arxiv.org/abs/2402.05098v1","category":"cs.LG"}
{"created":"2024-02-07 18:49:21","title":"Convergence of spatial branching processes to $\u03b1$-stable CSBPs: Genealogy of semi-pushed fronts","abstract":"We consider inhomogeneous branching diffusions on an infinite domain of $\\mathbb{R}^d$. The first aim of this article is to derive a general criterium under which the size process (number of particles) and the genealogy of the particle system become undistinguishable from the ones of an $\\alpha$-stable CSBP, with $\\alpha\\in(1,2)$. The branching diffusion is encoded as a random metric space capturing all the information about the positions and the genealogical structure of the population. Our convergence criterium is based on the convergence of the moments for random metric spaces, which in turn can be efficiently computed through many-to-few formulas. It requires an extension of the method of moments to general CSBPs (with or without finite second moment).   In a recent work, Tourniaire introduced a branching Brownian motion which can be thought of as a toy model for fluctuating pushed fronts. The size process was shown to converge to an $\\alpha$-stable CSBP and it was conjectured that a genealogical convergence should occur jointly. We prove this result as an application of our general methodology.","sentences":["We consider inhomogeneous branching diffusions on an infinite domain of $\\mathbb{R}^d$. The first aim of this article is to derive a general criterium under which the size process (number of particles) and the genealogy of the particle system become undistinguishable from the ones of an $\\alpha$-stable CSBP, with $\\alpha\\in(1,2)$. The branching diffusion is encoded as a random metric space capturing all the information about the positions and the genealogical structure of the population.","Our convergence criterium is based on the convergence of the moments for random metric spaces, which in turn can be efficiently computed through many-to-few formulas.","It requires an extension of the method of moments to general CSBPs (with or without finite second moment).   ","In a recent work, Tourniaire introduced a branching Brownian motion which can be thought of as a toy model for fluctuating pushed fronts.","The size process was shown to converge to an $\\alpha$-stable CSBP and it was conjectured that a genealogical convergence should occur jointly.","We prove this result as an application of our general methodology."],"url":"http://arxiv.org/abs/2402.05096v1","category":"math.PR"}
{"created":"2024-02-07 18:44:27","title":"Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation","abstract":"Deep Reinforcement Learning (DRL) has shown great potential in enabling robots to find certain objects (e.g., `find a fridge') in environments like homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL methods are predominantly trained and evaluated using environment simulators. Although DRL has shown impressive results, the simulators may be biased or limited. This creates a risk of shortcut learning, i.e., learning a policy tailored to specific visual details of training environments. We aim to deepen our understanding of shortcut learning in ObjectNav, its implications and propose a solution. We design an experiment for inserting a shortcut bias in the appearance of training environments. As a proof-of-concept, we associate room types to specific wall colors (e.g., bedrooms with green walls), and observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to environments where this is not the case (e.g., bedrooms with blue walls). We find that shortcut learning is the root cause: the agent learns to navigate to target objects, by simply searching for the associated wall color of the target object's room. To solve this, we propose Language-Based (L-B) augmentation. Our key insight is that we can leverage the multimodal feature space of a Vision-Language Model (VLM) to augment visual representations directly at the feature-level, requiring no changes to the simulator, and only an addition of one layer to the model. Where the SOTA ObjectNav method's success rate drops 69%, our proposal has only a drop of 23%.","sentences":["Deep Reinforcement Learning (DRL) has shown great potential in enabling robots to find certain objects (e.g., `find a fridge') in environments like homes or schools.","This task is known as Object-Goal Navigation (ObjectNav).","DRL methods are predominantly trained and evaluated using environment simulators.","Although DRL has shown impressive results, the simulators may be biased or limited.","This creates a risk of shortcut learning, i.e., learning a policy tailored to specific visual details of training environments.","We aim to deepen our understanding of shortcut learning in ObjectNav, its implications and propose a solution.","We design an experiment for inserting a shortcut bias in the appearance of training environments.","As a proof-of-concept, we associate room types to specific wall colors (e.g., bedrooms with green walls), and observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to environments where this is not the case (e.g., bedrooms with blue walls).","We find that shortcut learning is the root cause: the agent learns to navigate to target objects, by simply searching for the associated wall color of the target object's room.","To solve this, we propose Language-Based (L-B) augmentation.","Our key insight is that we can leverage the multimodal feature space of a Vision-Language Model (VLM) to augment visual representations directly at the feature-level, requiring no changes to the simulator, and only an addition of one layer to the model.","Where the SOTA ObjectNav method's success rate drops 69%, our proposal has only a drop of 23%."],"url":"http://arxiv.org/abs/2402.05090v1","category":"cs.RO"}
{"created":"2024-02-07 18:40:07","title":"Functional limit laws for the intensity measure of point processes and applications","abstract":"Motivated by applications to the study of depth functions for tree-indexed random variables generated by point processes, we describe functional limit theorems for the intensity measure of point processes. Specifically, we establish uniform laws of large numbers and uniform central limit theorems over a class of bounded measurable functions for estimates of the intensity measure. Using these results, we derive the uniform asymptotic properties of half-space depth and, as corollaries, obtain the asymptotic behavior of medians and other quantiles of the standardized intensity measure. Additionally, we obtain uniform concentration upper bound for the estimator of half-space depth. As a consequence of our results, we also derive uniform consistency and uniform asymptotic normality of Lotka-Nagaev and Harris-type estimators for the Laplace transform of the point processes in a branching random walk.","sentences":["Motivated by applications to the study of depth functions for tree-indexed random variables generated by point processes, we describe functional limit theorems for the intensity measure of point processes.","Specifically, we establish uniform laws of large numbers and uniform central limit theorems over a class of bounded measurable functions for estimates of the intensity measure.","Using these results, we derive the uniform asymptotic properties of half-space depth and, as corollaries, obtain the asymptotic behavior of medians and other quantiles of the standardized intensity measure.","Additionally, we obtain uniform concentration upper bound for the estimator of half-space depth.","As a consequence of our results, we also derive uniform consistency and uniform asymptotic normality of Lotka-Nagaev and Harris-type estimators for the Laplace transform of the point processes in a branching random walk."],"url":"http://arxiv.org/abs/2402.05087v1","category":"math.PR"}
{"created":"2024-02-07 18:40:06","title":"Hyperspectral acquisition with ScanImage at the single pixel level: Application to time domain coherent Raman imaging","abstract":"We present a comprehensive strategy and its practical implementation using the commercial ScanImage software platform to perform hyperspectral point scanning microscopy when a fast time dependent signal varies at each pixel level. In the proposed acquisition scheme the scan along the X axis is slowed down while the data acquisition is maintained at high pace to enable the rapid acquisition of the time dependent signal at each pixel level. The ScanImage generated raw 2D images have a very asymmetric aspect ratio between X and Y, the X axis encoding both for space and time acquisition. The results are X axis macro-pixel where the associated time depend signal is sampled therefore providing an hyperspectral information. We exemplified the proposed hyperspectral scheme in the context of time domain coherent Raman imaging where a pump pulse impulsively excites molecular vibrations that are subsequently probed by a time delayed probe pulse. In this case the time dependent signal is a fast acousto-optics delay line that can scan a delay of 4.5ps in 25$\\mu$s, at each pixel level. We this acquisition scheme we demonstrate ultra-fast hyperspectral vibrational imaging in the low frequency range [10$cm^{-1}$, 150 $cm^{-1}$] over a 500 $\\mu m$ field of view in 14ms (7 frames/s). The proposed acquisition scheme can be readily extended to other applications requiring to acquired a fast evolving signal at each pixel level.","sentences":["We present a comprehensive strategy and its practical implementation using the commercial ScanImage software platform to perform hyperspectral point scanning microscopy when a fast time dependent signal varies at each pixel level.","In the proposed acquisition scheme the scan along the X axis is slowed down while the data acquisition is maintained at high pace to enable the rapid acquisition of the time dependent signal at each pixel level.","The ScanImage generated raw 2D images have a very asymmetric aspect ratio between X and Y, the X axis encoding both for space and time acquisition.","The results are X axis macro-pixel where the associated time depend signal is sampled therefore providing an hyperspectral information.","We exemplified the proposed hyperspectral scheme in the context of time domain coherent Raman imaging where a pump pulse impulsively excites molecular vibrations that are subsequently probed by a time delayed probe pulse.","In this case the time dependent signal is a fast acousto-optics delay line that can scan a delay of 4.5ps in 25$\\mu$s, at each pixel level.","We this acquisition scheme we demonstrate ultra-fast hyperspectral vibrational imaging in the low frequency range [10$cm^{-1}$, 150 $cm^{-1}$] over a 500 $\\mu m$ field of view in 14ms (7 frames/s).","The proposed acquisition scheme can be readily extended to other applications requiring to acquired a fast evolving signal at each pixel level."],"url":"http://arxiv.org/abs/2402.05086v1","category":"physics.optics"}
{"created":"2024-02-07 18:33:28","title":"Designing three-way entangled and nonlocal two-way entangled single particle states via alternate quantum walks","abstract":"Entanglement with single-particle states is advantageous in quantum technology because of their ability to encode and process information more securely than their multi-particle analogs. Three-way and nonlocal two-way entangled single-particle states are desirable in this context. Herein, we generate three-way entanglement from an initially separable state involving three degrees of freedom (DoF) of a quantum particle, which evolves via a 2D alternate quantum walk employing a resource-saving single-qubit coin. We achieve maximum possible values for the three-way entanglement quantified by the $\\pi$-tangle between the 3 DoF. We also generate optimal two-way nonlocal entanglement, quantified by the negativity between the nonlocal position and the DoF of the particle. This prepared architecture using quantum walks can be experimentally realized with a photon.","sentences":["Entanglement with single-particle states is advantageous in quantum technology because of their ability to encode and process information more securely than their multi-particle analogs.","Three-way and nonlocal two-way entangled single-particle states are desirable in this context.","Herein, we generate three-way entanglement from an initially separable state involving three degrees of freedom (DoF) of a quantum particle, which evolves via a 2D alternate quantum walk employing a resource-saving single-qubit coin.","We achieve maximum possible values for the three-way entanglement quantified by the $\\pi$-tangle between the 3 DoF. We also generate optimal two-way nonlocal entanglement, quantified by the negativity between the nonlocal position and the DoF of the particle.","This prepared architecture using quantum walks can be experimentally realized with a photon."],"url":"http://arxiv.org/abs/2402.05080v1","category":"quant-ph"}
{"created":"2024-02-07 18:31:36","title":"Fluctuation-Induced First Order Transition to Collective Motion","abstract":"The nature of the transition to collective motion in assemblies of aligning self-propelled particles remains a long-standing matter of debate. In this article, we focus on dry active matter and show that weak fluctuations suffice to generically turn second-order mean-field transitions into a `discontinuous' coexistence scenario. Our theory shows how fluctuations induce a density-dependence of the polar-field mass, even when this effect is absent at mean-field level. In turn, this dependency on density triggers a feedback loop between ordering and advection that ultimately leads to an inhomogeneous transition to collective motion and the emergence of non-linear travelling `flocks'. Importantly, we show that such a fluctuation-induced first order transition is present in both metric models, in which particles align with neighbors within a finite distance, and in topological ones, in which alignment is not based on relative distances. We compute analytically the noise-induced renormalization of the polar-field mass using stochastic calculus, which we further back up by a one-loop field-theoretical analysis. Finally, we confirm our analytical predictions by numerical simulations of fluctuating hydrodynamics as well as of topological microscopic models with either $k$-nearest neighbors or Voronoi alignment.","sentences":["The nature of the transition to collective motion in assemblies of aligning self-propelled particles remains a long-standing matter of debate.","In this article, we focus on dry active matter and show that weak fluctuations suffice to generically turn second-order mean-field transitions into a `discontinuous' coexistence scenario.","Our theory shows how fluctuations induce a density-dependence of the polar-field mass, even when this effect is absent at mean-field level.","In turn, this dependency on density triggers a feedback loop between ordering and advection that ultimately leads to an inhomogeneous transition to collective motion and the emergence of non-linear travelling `flocks'.","Importantly, we show that such a fluctuation-induced first order transition is present in both metric models, in which particles align with neighbors within a finite distance, and in topological ones, in which alignment is not based on relative distances.","We compute analytically the noise-induced renormalization of the polar-field mass using stochastic calculus, which we further back up by a one-loop field-theoretical analysis.","Finally, we confirm our analytical predictions by numerical simulations of fluctuating hydrodynamics as well as of topological microscopic models with either $k$-nearest neighbors or Voronoi alignment."],"url":"http://arxiv.org/abs/2402.05078v1","category":"cond-mat.soft"}
{"created":"2024-02-07 18:27:49","title":"Minimal-error quantum state discrimination versus robustness of entanglement:More indistinguishability with less entanglement","abstract":"We relate the the distinguishability of quantum states with their robustness of the entanglement, where the robustness of any resource quantifies how tolerant it is to noise. In particular, we identify upper and lower bounds on the probability of discriminating the states, appearing in an arbitrary multiparty ensemble, in terms of their robustness of entanglement and the probability of discriminating states of the closest separable ensemble. These bounds hold true, irrespective of the dimension of the constituent systems the number of parties involved, the size of the ensemble, and whether the measurement strategies are local or global. Additional lower bounds on the same quantity is determined by considering two special cases of two-state multiparty ensembles, either having equal entanglement or at least one of them being separable. The case of equal entanglement reveals that it is always easier to discriminate the entangled states than the ones in the corresponding closest separable ensemble, a phenomenon which we refer as \"More indistinguishability with less entanglement\". Furthermore, we numerically explore how tight the bounds are by examining the global discrimination probability of states selected from Haar-uniformly generated ensembles of two two-qubit states. We find that for two-element ensembles of unequal entanglements, the minimum of the two entanglements must possess a threshold value for the ensemble to exhibit \"More indistinguishability with less entanglement\".","sentences":["We relate the the distinguishability of quantum states with their robustness of the entanglement, where the robustness of any resource quantifies how tolerant it is to noise.","In particular, we identify upper and lower bounds on the probability of discriminating the states, appearing in an arbitrary multiparty ensemble, in terms of their robustness of entanglement and the probability of discriminating states of the closest separable ensemble.","These bounds hold true, irrespective of the dimension of the constituent systems the number of parties involved, the size of the ensemble, and whether the measurement strategies are local or global.","Additional lower bounds on the same quantity is determined by considering two special cases of two-state multiparty ensembles, either having equal entanglement or at least one of them being separable.","The case of equal entanglement reveals that it is always easier to discriminate the entangled states than the ones in the corresponding closest separable ensemble, a phenomenon which we refer as \"More indistinguishability with less entanglement\".","Furthermore, we numerically explore how tight the bounds are by examining the global discrimination probability of states selected from Haar-uniformly generated ensembles of two two-qubit states.","We find that for two-element ensembles of unequal entanglements, the minimum of the two entanglements must possess a threshold value for the ensemble to exhibit \"More indistinguishability with less entanglement\"."],"url":"http://arxiv.org/abs/2402.05074v1","category":"quant-ph"}
{"created":"2024-02-07 18:27:29","title":"NITO: Neural Implicit Fields for Resolution-free Topology Optimization","abstract":"Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance. We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning. NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization. NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time. In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive simulation-based approaches. Crucially, NITO circumvents the domain and resolution limitations that restrict Convolutional Neural Network (CNN) models to a structured domain of fixed size -- limitations that hinder the widespread adoption of CNNs in engineering applications. This generalizability allows a single NITO model to train and generate solutions in countless domains, eliminating the need for numerous domain-specific CNNs and their extensive datasets. Despite its generalizability, NITO outperforms SOTA models even in specialized tasks, is an order of magnitude smaller, and is practically trainable at high resolutions that would be restrictive for CNNs. This combination of versatility, efficiency, and performance underlines NITO's potential to transform the landscape of engineering design optimization problems through implicit fields.","sentences":["Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance.","We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning.","NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization.","NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time.","In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive simulation-based approaches.","Crucially, NITO circumvents the domain and resolution limitations that restrict Convolutional Neural Network (CNN) models to a structured domain of fixed size -- limitations that hinder the widespread adoption of CNNs in engineering applications.","This generalizability allows a single NITO model to train and generate solutions in countless domains, eliminating the need for numerous domain-specific CNNs and their extensive datasets.","Despite its generalizability, NITO outperforms SOTA models even in specialized tasks, is an order of magnitude smaller, and is practically trainable at high resolutions that would be restrictive for CNNs.","This combination of versatility, efficiency, and performance underlines NITO's potential to transform the landscape of engineering design optimization problems through implicit fields."],"url":"http://arxiv.org/abs/2402.05073v1","category":"cs.LG"}
{"created":"2024-02-07 18:21:17","title":"A Roadmap to Pluralistic Alignment","abstract":"With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.","sentences":["With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives.","However, aligning models to serve pluralistic human values remains an open research question.","In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed.","We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution.","We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings.","We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment."],"url":"http://arxiv.org/abs/2402.05070v1","category":"cs.AI"}
{"created":"2024-02-07 18:19:58","title":"Arbitrary Scale Super-Resolution Assisted Lunar Crater Detection in Satellite Images","abstract":"Craters are one of the most studied planetary features used for different scientific analyses, such as estimation of surface age and surface processes. Satellite images utilized for crater detection often have low resolution (LR) due to hardware constraints and transmission time. Super-resolution (SR) is a practical and cost-effective solution; however, most SR approaches work on fixed integer scale factors, i.e., a single model can generate images of a specific resolution. In practical applications, SR on multiple scales provides various levels of detail, but training for each scale is resource-intensive. Therefore, this paper proposes a system for crater detection assisted with an arbitrary scale super-resolution approach (i.e., a single model can be used for multiple scale factors) for the lunar surface. Our work is composed of two subsystems. The first sub-system employs an arbitrary scale SR approach to generate super-resolved images of multiple resolutions. Subsequently, the second sub-system passes super-resolved images of multiple resolutions to a deep learning-based crater detection framework for identifying craters on the lunar surface. Employed arbitrary scale SR approach is based on a combination of convolution and transformer modules. For the crater detection sub-system, we utilize the Mask-RCNN framework. Using SR images of multiple resolutions, the proposed system detects 13.47% more craters from the ground truth than the craters detected using only the LR images. Further, in complex crater settings, specifically in overlapping and degraded craters, 11.84% and 15.01% more craters are detected as compared to the crater detection networks using only the LR images. The proposed system also leads to better localization performance, 3.19% IoU increment compared to the LR images","sentences":["Craters are one of the most studied planetary features used for different scientific analyses, such as estimation of surface age and surface processes.","Satellite images utilized for crater detection often have low resolution (LR) due to hardware constraints and transmission time.","Super-resolution (SR) is a practical and cost-effective solution; however, most SR approaches work on fixed integer scale factors, i.e., a single model can generate images of a specific resolution.","In practical applications, SR on multiple scales provides various levels of detail, but training for each scale is resource-intensive.","Therefore, this paper proposes a system for crater detection assisted with an arbitrary scale super-resolution approach (i.e., a single model can be used for multiple scale factors) for the lunar surface.","Our work is composed of two subsystems.","The first sub-system employs an arbitrary scale SR approach to generate super-resolved images of multiple resolutions.","Subsequently, the second sub-system passes super-resolved images of multiple resolutions to a deep learning-based crater detection framework for identifying craters on the lunar surface.","Employed arbitrary scale SR approach is based on a combination of convolution and transformer modules.","For the crater detection sub-system, we utilize the Mask-RCNN framework.","Using SR images of multiple resolutions, the proposed system detects 13.47% more craters from the ground truth than the craters detected using only the LR images.","Further, in complex crater settings, specifically in overlapping and degraded craters, 11.84% and 15.01% more craters are detected as compared to the crater detection networks using only the LR images.","The proposed system also leads to better localization performance, 3.19% IoU increment compared to the LR images"],"url":"http://arxiv.org/abs/2402.05068v1","category":"eess.IV"}
{"created":"2024-02-07 18:17:54","title":"Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning","abstract":"Operation of Autonomous Mobile Robots (AMRs) of all forms that include wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS denied environments without a-priori maps, exclusively using onboard sensors, is an unsolved problem that has potential to transform the economy, and vastly improve humanity's capabilities with improvements to agriculture, manufacturing, disaster response, military and space exploration. Conventional AMR automation approaches are modularized into perception, motion planning and control which is computationally inefficient, and requires explicit feature extraction and engineering, that inhibits generalization, and deployment at scale. Few works have focused on real-world end-to-end approaches that directly map sensor inputs to control outputs due to the large amount of well curated training data required for supervised Deep Learning (DL) which is time consuming and labor intensive to collect and label, and sample inefficiency and challenges to bridging the simulation to reality gap using Deep Reinforcement Learning (DRL). This paper presents a novel method to efficiently train DRL for robust end-to-end AMR exploration, in a constrained environment at physical limits in simulation, transferred zero-shot to the real-world. The representation learned in a compact parameter space with 2 fully connected layers with 64 nodes each is demonstrated to exhibit emergent behavior for out-of-distribution generalization to navigation in new environments that include unstructured terrain without maps, and dynamic obstacle avoidance. The learned policy outperforms conventional navigation algorithms while consuming a fraction of the computation resources, enabling execution on a range of AMR forms with varying embedded computer payloads.","sentences":["Operation of Autonomous Mobile Robots (AMRs) of all forms that include wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS denied environments without a-priori maps, exclusively using onboard sensors, is an unsolved problem that has potential to transform the economy, and vastly improve humanity's capabilities with improvements to agriculture, manufacturing, disaster response, military and space exploration.","Conventional AMR automation approaches are modularized into perception, motion planning and control which is computationally inefficient, and requires explicit feature extraction and engineering, that inhibits generalization, and deployment at scale.","Few works have focused on real-world end-to-end approaches that directly map sensor inputs to control outputs due to the large amount of well curated training data required for supervised Deep Learning (DL) which is time consuming and labor intensive to collect and label, and sample inefficiency and challenges to bridging the simulation to reality gap using Deep Reinforcement Learning (DRL).","This paper presents a novel method to efficiently train DRL for robust end-to-end AMR exploration, in a constrained environment at physical limits in simulation, transferred zero-shot to the real-world.","The representation learned in a compact parameter space with 2 fully connected layers with 64 nodes each is demonstrated to exhibit emergent behavior for out-of-distribution generalization to navigation in new environments that include unstructured terrain without maps, and dynamic obstacle avoidance.","The learned policy outperforms conventional navigation algorithms while consuming a fraction of the computation resources, enabling execution on a range of AMR forms with varying embedded computer payloads."],"url":"http://arxiv.org/abs/2402.05066v1","category":"cs.RO"}
{"created":"2024-02-07 18:16:05","title":"logitFD: an R package for functional principal component logit regression","abstract":"The functional logit regression model was proposed by Escabias et al. (2004) with the objective of modeling a scalar binary response variable from a functional predictor. The model estimation proposed in that case was performed in a subspace of L2(T) of squared integrable functions of finite dimension, generated by a finite set of basis functions. For that estimation it was assumed that the curves of the functional predictor and the functional parameter of the model belong to the same finite subspace. The estimation so obtained was affected by high multicollinearity problems and the solution given to these problems was based on different functional principal component analysis. The logitFD package introduced here provides a toolbox for the fit of these models by implementing the different proposed solutions and by generalizing the model proposed in 2004 to the case of several functional and non-functional predictors. The performance of the functions is illustrated by using data sets of functional data included in the fda.usc package from R-CRAN.","sentences":["The functional logit regression model was proposed by Escabias et al.","(2004) with the objective of modeling a scalar binary response variable from a functional predictor.","The model estimation proposed in that case was performed in a subspace of L2(T) of squared integrable functions of finite dimension, generated by a finite set of basis functions.","For that estimation it was assumed that the curves of the functional predictor and the functional parameter of the model belong to the same finite subspace.","The estimation so obtained was affected by high multicollinearity problems and the solution given to these problems was based on different functional principal component analysis.","The logitFD package introduced here provides a toolbox for the fit of these models by implementing the different proposed solutions and by generalizing the model proposed in 2004 to the case of several functional and non-functional predictors.","The performance of the functions is illustrated by using data sets of functional data included in the fda.usc package from R-CRAN."],"url":"http://arxiv.org/abs/2402.05065v1","category":"stat.ME"}
{"created":"2024-02-07 18:13:52","title":"Laser Beam Shaping Using a Photoinduced Azopolymer Droplet-Based Mask","abstract":"The dewetting of an azopolymer droplet, followed by the photostructuration of the evaporated droplet, is employed to create an amplitude mask. This straightforward process yields a large area featuring periodic micro- and nanostructures. The resulting pattern is utilized to generate a nondiffracting beam. Starting with a Gaussian beam illuminating the amplitude mask, the critical aspect is the production of a bright, ring-shaped beam with a high radius-to-width ratio and symmetric central laser spots, each with the same intensity. This alternative approach to shaping a laser beam is demonstrated as a rapid and cost-effective fabrication technique.","sentences":["The dewetting of an azopolymer droplet, followed by the photostructuration of the evaporated droplet, is employed to create an amplitude mask.","This straightforward process yields a large area featuring periodic micro- and nanostructures.","The resulting pattern is utilized to generate a nondiffracting beam.","Starting with a Gaussian beam illuminating the amplitude mask, the critical aspect is the production of a bright, ring-shaped beam with a high radius-to-width ratio and symmetric central laser spots, each with the same intensity.","This alternative approach to shaping a laser beam is demonstrated as a rapid and cost-effective fabrication technique."],"url":"http://arxiv.org/abs/2402.05062v1","category":"physics.optics"}
{"created":"2024-02-07 18:11:45","title":"On multicolor Tur\u00e1n numbers","abstract":"We address a problem which is a generalization of Tur\\'an-type problems recently introduced by Imolay, Karl, Nagy and V\\'ali. Let $F$ be a fixed graph and let $G$ be the union of $k$ edge-disjoint copies of $F$, namely $G = \\mathbin{\\dot{\\cup}}_{i=1}^{k} F_i$, where each $F_i$ is isomorphic to a fixed graph $F$ and $E(F_i)\\cap E(F_j)=\\emptyset$ for all $i \\neq j$. We call a subgraph $H\\subseteq G$ multicolored if $H$ and $F_i$ share at most one edge for all $i$. Define $\\text{ex}_F(H,n)$ to be the maximum value $k$ such that there exists $G = \\mathbin{\\dot{\\cup}}_{i=1}^{k} F_i$ on $n$ vertices without a multicolored copy of $H$. We show that $\\text{ex}_{C_5}(C_3,n) \\le n^2/25 + 3n/25+o(n)$ and that all extremal graphs are close to a blow-up of the 5-cycle. This bound is tight up to the linear error term.","sentences":["We address a problem which is a generalization of Tur\\'an-type problems recently introduced by Imolay, Karl, Nagy and V\\'ali.","Let $F$ be a fixed graph and let $G$ be the union of $k$ edge-disjoint copies of $F$, namely $G = \\mathbin{\\dot{\\cup}}_{i=1}^{k} F_i$, where each $F_i$ is isomorphic to a fixed graph $F$ and $E(F_i)\\cap E(F_j)=\\emptyset$ for all $i \\neq j$.","We call a subgraph $H\\subseteq G$ multicolored if $H$ and $F_i$ share at most one edge for all $i$. Define $\\text{ex}_F(H,n)$ to be the maximum value $k$ such that there exists $G = \\mathbin{\\dot{\\cup}}_{i=1}^{k} F_i$ on $n$ vertices without a multicolored copy of $H$. We show that $\\text{ex}_{C_5}(C_3,n) \\le n^2/25 +","3n/25+o(n)$ and that all extremal graphs are close to a blow-up of the 5-cycle.","This bound is tight up to the linear error term."],"url":"http://arxiv.org/abs/2402.05060v1","category":"math.CO"}
{"created":"2024-02-07 18:10:54","title":"Connecting Kani's Lemma and path-finding in the Bruhat-Tits tree to compute supersingular endomorphism rings","abstract":"We give a deterministic polynomial time algorithm to compute the endomorphism ring of a supersingular elliptic curve in characteristic p, provided that we are given two noncommuting endomorphisms and the factorization of the discriminant of the ring $\\mathcal{O}_0$ they generate. At each prime $q$ for which $\\mathcal{O}_0$ is not maximal, we compute the endomorphism ring locally by computing a q-maximal order containing it and, when $q \\neq p$, recovering a path to $\\text{End}(E) \\otimes \\mathbb{Z}_q$ in the Bruhat-Tits tree. We use techniques of higher-dimensional isogenies to navigate towards the local endomorphism ring. Our algorithm improves on a previous algorithm which requires a restricted input and runs in subexponential time under certain heuristics. Page and Wesolowski give a probabilistic polynomial time algorithm to compute the endomorphism ring on input of a single non-scalar endomorphism. Beyond using techniques of higher-dimensional isogenies to divide endomorphisms by a scalar, our methods are completely different.","sentences":["We give a deterministic polynomial time algorithm to compute the endomorphism ring of a supersingular elliptic curve in characteristic p, provided that we are given two noncommuting endomorphisms and the factorization of the discriminant of the ring $\\mathcal{O}_0$ they generate.","At each prime $q$ for which $\\mathcal{O}_0$ is not maximal, we compute the endomorphism ring locally by computing a q-maximal order containing it and, when $q \\neq p$, recovering a path to $\\text{End}(E)","\\otimes \\mathbb{Z}_q$ in the Bruhat-Tits tree.","We use techniques of higher-dimensional isogenies to navigate towards the local endomorphism ring.","Our algorithm improves on a previous algorithm which requires a restricted input and runs in subexponential time under certain heuristics.","Page and Wesolowski give a probabilistic polynomial time algorithm to compute the endomorphism ring on input of a single non-scalar endomorphism.","Beyond using techniques of higher-dimensional isogenies to divide endomorphisms by a scalar, our methods are completely different."],"url":"http://arxiv.org/abs/2402.05059v1","category":"math.NT"}
{"created":"2024-02-07 18:00:48","title":"Measuring Neutron Star Radius with second and third generation Gravitational Wave Detector Networks","abstract":"The next generation of ground-based interferometric gravitational wave detectors will observe mergers of black holes and neutron stars throughout cosmic time. A large number of the binary neutron star merger events will be observed with extreme high fidelity, and will provide stringent constraints on the equation of state of nuclear matter. In this paper, we investigate the systematic improvement in the measurability of the equation of state with increase in detector sensitivity by combining constraints obtained on the radius of a $1.4 \\, \\mathrm{M}_{\\odot}$ neutron star from a simulated source population. Since the measurability of the equation of state depends on its stiffness, we consider a range of realistic equations of state that span the current observational constraints. We show that a single 40km Cosmic Explorer detector can pin down the neutron star radius for a soft, medium and stiff equation of state to an accuracy of 10m within a decade, whereas the current generation of ground-based detectors like the Advanced LIGO-Virgo network would take $\\mathcal{O}(10^5)$ years to do so for a soft equation of state.","sentences":["The next generation of ground-based interferometric gravitational wave detectors will observe mergers of black holes and neutron stars throughout cosmic time.","A large number of the binary neutron star merger events will be observed with extreme high fidelity, and will provide stringent constraints on the equation of state of nuclear matter.","In this paper, we investigate the systematic improvement in the measurability of the equation of state with increase in detector sensitivity by combining constraints obtained on the radius of a $1.4 \\, \\mathrm{M}_{\\odot}$ neutron star from a simulated source population.","Since the measurability of the equation of state depends on its stiffness, we consider a range of realistic equations of state that span the current observational constraints.","We show that a single 40km Cosmic Explorer detector can pin down the neutron star radius for a soft, medium and stiff equation of state to an accuracy of 10m within a decade, whereas the current generation of ground-based detectors like the Advanced LIGO-Virgo network would take $\\mathcal{O}(10^5)$ years to do so for a soft equation of state."],"url":"http://arxiv.org/abs/2402.05056v1","category":"astro-ph.HE"}
{"created":"2024-02-07 17:57:03","title":"LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation","abstract":"3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.","sentences":["3D content creation has achieved significant progress in terms of both quality and speed.","Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training.","In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text prompts or single-view images.","Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering.","2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models.","Extensive experiments demonstrate the high fidelity and efficiency of our approach.","Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation."],"url":"http://arxiv.org/abs/2402.05054v1","category":"cs.CV"}
{"created":"2024-02-07 17:53:21","title":"Quantum circuit for multi-qubit Toffoli gate with optimal resource","abstract":"Resource consumption is an important issue in quantum information processing, particularly during the present NISQ era. In this paper, we investigate resource optimization of implementing multiple controlled operations, which are fundamental building blocks in the field of quantum computing and quantum simulation. We design new quantum circuits for the $n$-Toffoli gate and general multi-controlled unitary, which have only $O(\\log n)$-depth and $O(n)$-size, and only require $1$ ancillary qubit. To achieve these results, we explore the potential of ancillary qubits and discover a method to create new conditional clean qubits from existed ancillary qubits. These techniques can also be utilized to construct an efficient quantum circuit for incrementor, leading to an implementation of multi-qubit Toffoli gate with a depth of $O(\\log^2n)$ and size of $O(n)$ without any ancillary qubits. Furthermore, we explore the power of ancillary qubits from the perspective of resource theory. We demonstrate that without the assistance of ancillary qubit, any quantum circuit implementation of multi-qubit Toffoli gate must employ exponential precision gates. This finding indicates a significant disparity in computational power of quantum circuits between using and not using ancillary qubits. Additionally, we discuss the comparison of the power of ancillary qubits and extra energy levels in quantum circuit design.","sentences":["Resource consumption is an important issue in quantum information processing, particularly during the present NISQ era.","In this paper, we investigate resource optimization of implementing multiple controlled operations, which are fundamental building blocks in the field of quantum computing and quantum simulation.","We design new quantum circuits for the $n$-Toffoli gate and general multi-controlled unitary, which have only $O(\\log n)$-depth and $O(n)$-size, and only require $1$ ancillary qubit.","To achieve these results, we explore the potential of ancillary qubits and discover a method to create new conditional clean qubits from existed ancillary qubits.","These techniques can also be utilized to construct an efficient quantum circuit for incrementor, leading to an implementation of multi-qubit Toffoli gate with a depth of $O(\\log^2n)$ and size of $O(n)$ without any ancillary qubits.","Furthermore, we explore the power of ancillary qubits from the perspective of resource theory.","We demonstrate that without the assistance of ancillary qubit, any quantum circuit implementation of multi-qubit Toffoli gate must employ exponential precision gates.","This finding indicates a significant disparity in computational power of quantum circuits between using and not using ancillary qubits.","Additionally, we discuss the comparison of the power of ancillary qubits and extra energy levels in quantum circuit design."],"url":"http://arxiv.org/abs/2402.05053v1","category":"quant-ph"}
{"created":"2024-02-07 17:51:38","title":"Causal Representation Learning from Multiple Distributions: A General Setting","abstract":"In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects). For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\\mathcal{G}_Z$. This problem has recently been known as causal representation learning. This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes. We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions. We show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way. In some cases, each latent variable can even be recovered up to component-wise transformations. Experimental results verify our theoretical claims.","sentences":["In many problems, the measured variables (e.g., image pixels) are just mathematical functions of the hidden causal variables (e.g., the underlying concepts or objects).","For the purpose of making predictions in changing environments or making proper changes to the system, it is helpful to recover the hidden causal variables $Z_i$ and their causal relations represented by graph $\\mathcal{G}_Z$. This problem has recently been known as causal representation learning.","This paper is concerned with a general, completely nonparametric setting of causal representation learning from multiple distributions (arising from heterogeneous data or nonstationary time series), without assuming hard interventions behind distribution changes.","We aim to develop general solutions in this fundamental case; as a by product, this helps see the unique benefit offered by other assumptions such as parametric causal models or hard interventions.","We show that under the sparsity constraint on the recovered graph over the latent variables and suitable sufficient change conditions on the causal influences, interestingly, one can recover the moralized graph of the underlying directed acyclic graph, and the recovered latent variables and their relations are related to the underlying causal model in a specific, nontrivial way.","In some cases, each latent variable can even be recovered up to component-wise transformations.","Experimental results verify our theoretical claims."],"url":"http://arxiv.org/abs/2402.05052v1","category":"cs.LG"}
{"created":"2024-02-07 17:41:44","title":"Symplectic mechanics of relativistic spinning compact bodies II.: Canonical formalism in the Schwarzschild spacetime","abstract":"This work is the second part in a series aiming at exploiting tools from Hamiltonian mechanics to study the motion of an extended body in general relativity. In the first part of this work, we constructed a 10-dimensional, covariant Hamiltonian framework that encodes all the linear-in-spin corrections to the geodesic motion. This formulation, although non-canonical, revealed that, at this linear-in-spin order, the integrability of Schwarzschild and Kerr geodesics remain. Building on this formalism, in the present work, we translate this abstract integrability result into tangible applications for linear-in-spin dynamics of a compact object into a Schwarzschild background spacetime. In particular, we construct a canonical system of coordinates which exploits the spherical symmetry of the Schwarzschild spacetime. They are based on a relativistic generalization of the classical Andoyer variables of Newtonian rigid body motion. This canonical setup, then, allows us to derive ready-to-use formulae for action-angle coordinates and gauge-invariant Hamiltonian frequencies, which automatically include all linear-in-spin effects. No external parameters or ad hoc choices are necessary, and the framework can be used to find complete solutions by quadrature of generic, bound, linear-in-spin orbits, including orbital inclination, precession and eccentricity, as well as spin precession. We demonstrate the strength of the formalism in the simple setting of circular orbits with arbitrary spin and orbital precession, and validate them against known results in the literature.","sentences":["This work is the second part in a series aiming at exploiting tools from Hamiltonian mechanics to study the motion of an extended body in general relativity.","In the first part of this work, we constructed a 10-dimensional, covariant Hamiltonian framework that encodes all the linear-in-spin corrections to the geodesic motion.","This formulation, although non-canonical, revealed that, at this linear-in-spin order, the integrability of Schwarzschild and Kerr geodesics remain.","Building on this formalism, in the present work, we translate this abstract integrability result into tangible applications for linear-in-spin dynamics of a compact object into a Schwarzschild background spacetime.","In particular, we construct a canonical system of coordinates which exploits the spherical symmetry of the Schwarzschild spacetime.","They are based on a relativistic generalization of the classical Andoyer variables of Newtonian rigid body motion.","This canonical setup, then, allows us to derive ready-to-use formulae for action-angle coordinates and gauge-invariant Hamiltonian frequencies, which automatically include all linear-in-spin effects.","No external parameters or ad hoc choices are necessary, and the framework can be used to find complete solutions by quadrature of generic, bound, linear-in-spin orbits, including orbital inclination, precession and eccentricity, as well as spin precession.","We demonstrate the strength of the formalism in the simple setting of circular orbits with arbitrary spin and orbital precession, and validate them against known results in the literature."],"url":"http://arxiv.org/abs/2402.05049v1","category":"gr-qc"}
{"created":"2024-02-07 17:41:15","title":"How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation","abstract":"Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal. Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI. In some cases, even works from mathematics, statistics, and engineering would be affected. Worryingly, AI misdefinitions are observed from Western societies to the Global South. In this paper, we propose a framework to score how \\textit{validated as appropriately-defined for regulation} (VADER) an AI definition is. Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works. Regarding the latter, our score is based on a dataset of representative AI, non-AI ICT, and non-ICT examples. We demonstrate our contribution by reviewing the AI regulation proposals of key players, namely the United States, United Kingdom, European Union, and Brazil. Importantly, none of the proposals assessed achieve the appropriateness score, ranging from a revision need to a concrete risk to ICT systems and works from other fields.","sentences":["Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs.","Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal.","Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI.","In some cases, even works from mathematics, statistics, and engineering would be affected.","Worryingly, AI misdefinitions are observed from Western societies to the Global South.","In this paper, we propose a framework to score how \\textit{validated as appropriately-defined for regulation} (VADER) an AI definition is.","Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works.","Regarding the latter, our score is based on a dataset of representative AI, non-AI ICT, and non-ICT examples.","We demonstrate our contribution by reviewing the AI regulation proposals of key players, namely the United States, United Kingdom, European Union, and Brazil.","Importantly, none of the proposals assessed achieve the appropriateness score, ranging from a revision need to a concrete risk to ICT systems and works from other fields."],"url":"http://arxiv.org/abs/2402.05048v1","category":"cs.AI"}
{"created":"2024-02-07 17:33:54","title":"SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models","abstract":"In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under \\url{https://github.com/OpenSafetyLab/SALAD-BENCH}. Warning: this paper includes examples that may be offensive or harmful.","sentences":["In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount.","To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods.","Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.","SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice.","To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation.","Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility.","Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics.","Data and evaluator are released under \\url{https://github.com/OpenSafetyLab/SALAD-BENCH}.","Warning: this paper includes examples that may be offensive or harmful."],"url":"http://arxiv.org/abs/2402.05044v1","category":"cs.CL"}
{"created":"2024-02-07 17:30:49","title":"Laboratory study of magnetic reconnection in lunar-relevant mini-magnetospheres","abstract":"Mini-magnetospheres are small ion-scale structures that are well-suited to studying kinetic-scale physics of collisionless space plasmas. Such ion-scale magnetospheres can be found on local regions of the Moon, associated with the lunar crustal magnetic field. In this paper, we report on the laboratory experimental study of magnetic reconnection in laser-driven, lunar-like ion-scale magnetospheres on the Large Plasma Device (LAPD) at the University of California - Los Angeles. In the experiment, a high-repetition rate (1 Hz), nanosecond laser is used to drive a fast moving, collisionless plasma that expands into the field generated by a pulsed magnetic dipole embedded into a background plasma and magnetic field. The high-repetition rate enables the acquisition of time-resolved volumetric data of the magnetic and electric fields to characterize magnetic reconnection and calculate the reconnection rate. We notably observe the formation of Hall fields associated with reconnection. Particle-in-cell simulations reproducing the experimental results were performed to study the micro-physics of the interaction. By analyzing the generalized Ohm's law terms, we find that the electron-only reconnection is driven by kinetic effects, through the electron pressure anisotropy. These results are compared to recent satellite measurements that found evidence of magnetic reconnection near the lunar surface.","sentences":["Mini-magnetospheres are small ion-scale structures that are well-suited to studying kinetic-scale physics of collisionless space plasmas.","Such ion-scale magnetospheres can be found on local regions of the Moon, associated with the lunar crustal magnetic field.","In this paper, we report on the laboratory experimental study of magnetic reconnection in laser-driven, lunar-like ion-scale magnetospheres on the Large Plasma Device (LAPD) at the University of California - Los Angeles.","In the experiment, a high-repetition rate (1 Hz), nanosecond laser is used to drive a fast moving, collisionless plasma that expands into the field generated by a pulsed magnetic dipole embedded into a background plasma and magnetic field.","The high-repetition rate enables the acquisition of time-resolved volumetric data of the magnetic and electric fields to characterize magnetic reconnection and calculate the reconnection rate.","We notably observe the formation of Hall fields associated with reconnection.","Particle-in-cell simulations reproducing the experimental results were performed to study the micro-physics of the interaction.","By analyzing the generalized Ohm's law terms, we find that the electron-only reconnection is driven by kinetic effects, through the electron pressure anisotropy.","These results are compared to recent satellite measurements that found evidence of magnetic reconnection near the lunar surface."],"url":"http://arxiv.org/abs/2402.05043v1","category":"physics.plasm-ph"}
{"created":"2024-02-07 17:23:15","title":"PAC Learnability under Explanation-Preserving Graph Perturbations","abstract":"Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others. Graph neural networks (GNN) are neural models that operate over graphs, enabling the model to leverage the complex relationships and dependencies in graph-structured data. A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph. This work considers two methods for leveraging such perturbation invariances in the design and training of GNNs. First, explanation-assisted learning rules are considered. It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. Next, explanation-assisted data augmentation is considered, where the training set is enlarged by artificially producing new training samples via perturbation of the non-explanation edges in the original training set. It is shown that such data augmentation methods may improve performance if the augmented data is in-distribution, however, it may also lead to worse sample complexity compared to explanation-agnostic learning rules if the augmented data is out-of-distribution. Extensive empirical evaluations are provided to verify the theoretical analysis.","sentences":["Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others.","Graph neural networks (GNN) are neural models that operate over graphs, enabling the model to leverage the complex relationships and dependencies in graph-structured data.","A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label.","Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph.","This work considers two methods for leveraging such perturbation invariances in the design and training of GNNs.","First, explanation-assisted learning rules are considered.","It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning.","Next, explanation-assisted data augmentation is considered, where the training set is enlarged by artificially producing new training samples via perturbation of the non-explanation edges in the original training set.","It is shown that such data augmentation methods may improve performance if the augmented data is in-distribution, however, it may also lead to worse sample complexity compared to explanation-agnostic learning rules if the augmented data is out-of-distribution.","Extensive empirical evaluations are provided to verify the theoretical analysis."],"url":"http://arxiv.org/abs/2402.05039v1","category":"cs.LG"}
{"created":"2024-02-07 17:17:14","title":"Determining the nanoflare heating frequency of an X-ray Bright Point observed by MaGIXS","abstract":"Nanoflares are thought to be one of the prime candidates that can heat the solar corona to its multi-million kelvin temperature. Individual nanoflares are difficult to detect with the present generation instruments, however their presence can be inferred by comparing simulated nanoflare-heated plasma emissions with the observed emission. Using HYDRAD coronal loop simulations, we model the emission from an X-ray bright point (XBP) observed by the Marshall Grazing Incidence X-ray Spectrometer (MaGIXS), along with nearest-available observations from the Atmospheric Imaging Assembly (AIA) onboard Solar Dynamics Observatory (SDO) and X-Ray Telescope (XRT) onboard Hinode observatory. The length and magnetic field strength of the coronal loops are derived from the linear-force-free extrapolation of the observed photospheric magnetogram by Helioseismic and Magnetic Imager (HMI) onboard SDO. Each loop is assumed to be heated by random nanoflares, whose magnitude and frequency are determined by the loop length and magnetic field strength. The simulation results are then compared and matched against the measured intensity from AIA, XRT, and MaGIXS. Our model results indicate the observed emissions from the XBP under study could be well matched by a distribution of nanoflares with average delay times 1500 s to 3000 s, which suggest that the heating is dominated by high-frequency events. Further, we demonstrate the high sensitivity of MaGIXS and XRT to diagnose the heating frequency using this method, while AIA passbands are found to be the least sensitive.","sentences":["Nanoflares are thought to be one of the prime candidates that can heat the solar corona to its multi-million kelvin temperature.","Individual nanoflares are difficult to detect with the present generation instruments, however their presence can be inferred by comparing simulated nanoflare-heated plasma emissions with the observed emission.","Using HYDRAD coronal loop simulations, we model the emission from an X-ray bright point (XBP) observed by the Marshall Grazing Incidence X-ray Spectrometer (MaGIXS), along with nearest-available observations from the Atmospheric Imaging Assembly (AIA) onboard Solar Dynamics Observatory (SDO) and X-Ray Telescope (XRT) onboard Hinode observatory.","The length and magnetic field strength of the coronal loops are derived from the linear-force-free extrapolation of the observed photospheric magnetogram by Helioseismic and Magnetic Imager (HMI) onboard SDO.","Each loop is assumed to be heated by random nanoflares, whose magnitude and frequency are determined by the loop length and magnetic field strength.","The simulation results are then compared and matched against the measured intensity from AIA, XRT, and MaGIXS.","Our model results indicate the observed emissions from the XBP under study could be well matched by a distribution of nanoflares with average delay times 1500 s to 3000 s, which suggest that the heating is dominated by high-frequency events.","Further, we demonstrate the high sensitivity of MaGIXS and XRT to diagnose the heating frequency using this method, while AIA passbands are found to be the least sensitive."],"url":"http://arxiv.org/abs/2402.05036v1","category":"astro-ph.SR"}
{"created":"2024-02-07 17:08:27","title":"A Survey on Domain Generalization for Medical Image Analysis","abstract":"Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years. However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue. In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions. This paper presents the a comprehensive review of substantial developments in this area. First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings. Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints. Furthermore, we introduce the commonly used datasets. Finally, we summarize existing literature and present some potential research topics for the future. For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA","sentences":["Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years.","However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue.","In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions.","This paper presents the a comprehensive review of substantial developments in this area.","First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings.","Subsequently, we summarize the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints.","Furthermore, we introduce the commonly used datasets.","Finally, we summarize existing literature and present some potential research topics for the future.","For this survey, we also created a GitHub project by collecting the supporting resources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA"],"url":"http://arxiv.org/abs/2402.05035v1","category":"cs.CV"}
{"created":"2024-02-07 17:07:53","title":"How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models","abstract":"In this paper, we explore the idea of analysing the historical bias of contextual language models based on BERT by measuring their adequacy with respect to Early Modern (EME) and Modern (ME) English. In our preliminary experiments, we perform fill-in-the-blank tests with 60 masked sentences (20 EME-specific, 20 ME-specific and 20 generic) and three different models (i.e., BERT Base, MacBERTh, English HLM). We then rate the model predictions according to a 5-point bipolar scale between the two language varieties and derive a weighted score to measure the adequacy of each model to EME and ME varieties of English.","sentences":["In this paper, we explore the idea of analysing the historical bias of contextual language models based on BERT by measuring their adequacy with respect to Early Modern (EME) and Modern (ME) English.","In our preliminary experiments, we perform fill-in-the-blank tests with 60 masked sentences (20 EME-specific, 20 ME-specific and 20 generic) and three different models (i.e., BERT Base, MacBERTh, English HLM).","We then rate the model predictions according to a 5-point bipolar scale between the two language varieties and derive a weighted score to measure the adequacy of each model to EME and ME varieties of English."],"url":"http://arxiv.org/abs/2402.05034v1","category":"cs.CL"}
{"created":"2024-02-07 16:56:45","title":"Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment","abstract":"This study evaluates the health effects of long-term exposure to PM10 in Seoul. Building on the preliminary model Shin and Bithell (2019), an in-silico agent-based model (ABM) is used to simulate the travel patterns of individuals according to their origins and destinations. During the simulation, each person, with their inherent socio-economic attributes and allocated origin and destination location, is assumed to commute to and from the same places for 10 consecutive years. A nominal measure of their health is set to decrease whenever the concentration of PM10 exceeds the national standard. Sensitivity analysis on calibrated parameters reveals increased vulnerability among certain demographic groups, particularly those aged over 65 and under 15, with a significant health decline associated with road proximity. The study reveals a substantial health disparity after 7,000 simulation ticks (equivalent to 10 years), especially under scenarios of a 3% annual increase in pollution levels. Long-term exposure to PM10 has a significant impact on health vulnerabilities, despite initial resilience being minimal. The study emphasises the importance of future research that takes into account different pollution thresholds as well as more detailed models of population dynamics and pollution generation in order to better understand and mitigate the health effects of air pollution on diverse urban populations.","sentences":["This study evaluates the health effects of long-term exposure to PM10 in Seoul.","Building on the preliminary model Shin and Bithell (2019), an in-silico agent-based model (ABM) is used to simulate the travel patterns of individuals according to their origins and destinations.","During the simulation, each person, with their inherent socio-economic attributes and allocated origin and destination location, is assumed to commute to and from the same places for 10 consecutive years.","A nominal measure of their health is set to decrease whenever the concentration of PM10 exceeds the national standard.","Sensitivity analysis on calibrated parameters reveals increased vulnerability among certain demographic groups, particularly those aged over 65 and under 15, with a significant health decline associated with road proximity.","The study reveals a substantial health disparity after 7,000 simulation ticks (equivalent to 10 years), especially under scenarios of a 3% annual increase in pollution levels.","Long-term exposure to PM10 has a significant impact on health vulnerabilities, despite initial resilience being minimal.","The study emphasises the importance of future research that takes into account different pollution thresholds as well as more detailed models of population dynamics and pollution generation in order to better understand and mitigate the health effects of air pollution on diverse urban populations."],"url":"http://arxiv.org/abs/2402.05029v1","category":"cs.MA"}
{"created":"2024-02-07 16:53:09","title":"Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing","abstract":"Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph.","sentences":["Graph-based environments pose unique challenges to multi-agent reinforcement learning.","In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations.","The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead.","This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph.","We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors.","Agents receive the resulting learned graph observations based on their location in the graph.","Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice.","We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph."],"url":"http://arxiv.org/abs/2402.05027v1","category":"cs.MA"}
{"created":"2024-02-07 16:47:07","title":"Strong convexity-guided hyper-parameter optimization for flatter losses","abstract":"We propose a novel white-box approach to hyper-parameter optimization. Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness. Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss. By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion. Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime.","sentences":["We propose a novel white-box approach to hyper-parameter optimization.","Motivated by recent work establishing a relationship between flat minima and generalization, we first establish a relationship between the strong convexity of the loss and its flatness.","Based on this, we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss.","By using the structure of the underlying neural network, we derive closed-form equations to approximate the strong convexity parameter, and attempt to find hyper-parameters that minimize it in a randomized fashion.","Through experiments on 14 classification datasets, we show that our method achieves strong performance at a fraction of the runtime."],"url":"http://arxiv.org/abs/2402.05025v1","category":"cs.LG"}
{"created":"2024-02-07 16:46:01","title":"Does the Use of Unusual Combinations of Datasets Contribute to Greater Scientific Impact?","abstract":"Scientific datasets play a crucial role in contemporary data-driven research, as they allow for the progress of science by facilitating the discovery of new patterns and phenomena. This mounting demand for empirical research raises important questions on how strategic data utilization in research projects can stimulate scientific advancement. In this study, we examine the hypothesis inspired by the recombination theory, which suggests that innovative combinations of existing knowledge, including the use of unusual combinations of datasets, can lead to high-impact discoveries. We investigate the scientific outcomes of such atypical data combinations in more than 30,000 publications that leverage over 6,000 datasets curated within one of the largest social science databases, ICPSR. This study offers four important insights. First, combining datasets, particularly those infrequently paired, significantly contributes to both scientific and broader impacts (e.g., dissemination to the general public). Second, the combination of datasets with atypically combined topics has the opposite effect -- the use of such data is associated with fewer citations. Third, younger and less experienced research teams tend to use atypical combinations of datasets in research at a higher frequency than their older and more experienced counterparts. Lastly, despite the benefits of data combination, papers that amalgamate data remain infrequent. This finding suggests that the unconventional combination of datasets is an under-utilized but powerful strategy correlated with the scientific and broader impact of scientific discoveries.","sentences":["Scientific datasets play a crucial role in contemporary data-driven research, as they allow for the progress of science by facilitating the discovery of new patterns and phenomena.","This mounting demand for empirical research raises important questions on how strategic data utilization in research projects can stimulate scientific advancement.","In this study, we examine the hypothesis inspired by the recombination theory, which suggests that innovative combinations of existing knowledge, including the use of unusual combinations of datasets, can lead to high-impact discoveries.","We investigate the scientific outcomes of such atypical data combinations in more than 30,000 publications that leverage over 6,000 datasets curated within one of the largest social science databases, ICPSR.","This study offers four important insights.","First, combining datasets, particularly those infrequently paired, significantly contributes to both scientific and broader impacts (e.g., dissemination to the general public).","Second, the combination of datasets with atypically combined topics has the opposite effect -- the use of such data is associated with fewer citations.","Third, younger and less experienced research teams tend to use atypical combinations of datasets in research at a higher frequency than their older and more experienced counterparts.","Lastly, despite the benefits of data combination, papers that amalgamate data remain infrequent.","This finding suggests that the unconventional combination of datasets is an under-utilized but powerful strategy correlated with the scientific and broader impact of scientific discoveries."],"url":"http://arxiv.org/abs/2402.05024v1","category":"cs.DL"}
{"created":"2024-02-07 16:44:57","title":"On the Exact Linearization of Minimally Underactuated Configuration Flat Lagrangian Systems in Generalized State Representation","abstract":"In this paper, we examine the exact linearization of configuration flat Lagrangian control systems in generalized state representation with p degrees of freedom and p-1 control inputs by quasi-static feedback of its generalized state. We present a formal introduction to generalized Lagrangian control systems, which are obtained when configuration variables are considered as control inputs instead of forces or torques. The corresponding generalized state comprises those configuration variables not considered as control inputs and their respective velocities. This work presents all possible lengths of integrator chains achieved by an exact linearization with a quasi-static feedback law of the generalized state that allows for rest-to-rest transitions. We show that such feedback laws can be systematically derived without using Brunovsk\\'y states.","sentences":["In this paper, we examine the exact linearization of configuration flat Lagrangian control systems in generalized state representation with p degrees of freedom and p-1 control inputs by quasi-static feedback of its generalized state.","We present a formal introduction to generalized Lagrangian control systems, which are obtained when configuration variables are considered as control inputs instead of forces or torques.","The corresponding generalized state comprises those configuration variables not considered as control inputs and their respective velocities.","This work presents all possible lengths of integrator chains achieved by an exact linearization with a quasi-static feedback law of the generalized state that allows for rest-to-rest transitions.","We show that such feedback laws can be systematically derived without using Brunovsk\\'y states."],"url":"http://arxiv.org/abs/2402.05023v1","category":"math.DS"}
{"created":"2024-02-07 16:35:01","title":"An analog of multiplier sequences for the set of totally positive sequences","abstract":"A real sequence $(b_k)_{k=0}^\\infty$ is called totally positive if all minors of the infinite matrix $ \\left\\| b_{j-i} \\right\\|_{i, j =0}^\\infty$ are nonnegative (here $b_k=0$ for $k<0$). In this paper, we investigate the problem of description of the set of sequences $(a_k)_{k=0}^\\infty$ such that for every totally positive sequence $(b_k)_{k=0}^\\infty$ the sequence $(a_k b_k)_{k=0}^\\infty$ is also totally positive. We obtain the description of such sequences $(a_k)_{k=0}^\\infty$ in two cases: when the generating function of the sequence $\\sum_{k=0}^\\infty a_k z^k$ has at least one pole, and when the sequence $(a_k)_{k=0}^\\infty$ has not more than $4$ nonzero terms.","sentences":["A real sequence $(b_k)_{k=0}^\\infty$ is called totally positive if all minors of the infinite matrix $ \\left\\| b_{j-i} \\right\\|_{i, j =0}^\\infty$ are nonnegative (here $b_k=0$ for $k<0$).","In this paper, we investigate the problem of description of the set of sequences $(a_k)_{k=0}^\\infty$ such that for every totally positive sequence $(b_k)_{k=0}^\\infty$ the sequence $(a_k b_k)_{k=0}^\\infty$ is also totally positive.","We obtain the description of such sequences $(a_k)_{k=0}^\\infty$ in two cases: when the generating function of the sequence $\\sum_{k=0}^\\infty a_k","z^k$ has at least one pole, and when the sequence $(a_k)_{k=0}^\\infty$ has not more than $4$ nonzero terms."],"url":"http://arxiv.org/abs/2402.05017v1","category":"math.CV"}
{"created":"2024-02-07 16:34:10","title":"PhosNetVis: a web-based tool for kinase enrichment analysis and interactive 2D/3D network visualizations of phosphoproteomics data","abstract":"Protein phosphorylation is a vital process in cellular signaling that involves the reversible modification of a protein (substrate) residue by another protein (kinase). Advances in liquid chromatography-mass spectrometry have enabled the rapid generation of massive protein phosphorylation datasets across multiple conditions by many research groups. Researchers are then tasked with inferring kinases responsible for changes in phosphorylation sites of each substrate. Despite the recent explosion of tools to infer kinase-substrate interactions (KSIs) from such datasets, these are not optimized for the interactive exploration of the resulting large and complex KSI networks together with significant phosphorylation sites and states. There are also no dedicated tools that streamline kinase inferences together with interactive visualizations of the resulting networks. There is thus an unmet need for a tool that facilitates uster-intuitive analysis, interactive exploration, visualization, and communication of datasets from phosphoproteomics experiments. Here, we present PhosNetVis, a freely available web-based tool for researchers of all computational skill levels to easily infer, generate and interactively explore KSI networks in 2D or 3D by streamlining multiple phosphoproteomics data analysis steps within one single tool. PhostNetVis significantly lowers the barriers for researchers in rapidly generating high-quality visualizations to translate their rich phosphoproteomics datasets into biological and clinical insights.","sentences":["Protein phosphorylation is a vital process in cellular signaling that involves the reversible modification of a protein (substrate) residue by another protein (kinase).","Advances in liquid chromatography-mass spectrometry have enabled the rapid generation of massive protein phosphorylation datasets across multiple conditions by many research groups.","Researchers are then tasked with inferring kinases responsible for changes in phosphorylation sites of each substrate.","Despite the recent explosion of tools to infer kinase-substrate interactions (KSIs) from such datasets, these are not optimized for the interactive exploration of the resulting large and complex KSI networks together with significant phosphorylation sites and states.","There are also no dedicated tools that streamline kinase inferences together with interactive visualizations of the resulting networks.","There is thus an unmet need for a tool that facilitates uster-intuitive analysis, interactive exploration, visualization, and communication of datasets from phosphoproteomics experiments.","Here, we present PhosNetVis, a freely available web-based tool for researchers of all computational skill levels to easily infer, generate and interactively explore KSI networks in 2D or 3D by streamlining multiple phosphoproteomics data analysis steps within one single tool.","PhostNetVis significantly lowers the barriers for researchers in rapidly generating high-quality visualizations to translate their rich phosphoproteomics datasets into biological and clinical insights."],"url":"http://arxiv.org/abs/2402.05016v1","category":"q-bio.MN"}
{"created":"2024-02-07 16:32:29","title":"Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth","abstract":"Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a permutation). Finally, by exploiting a connection with approximate message passing algorithms, we show how to improve upon Gaussian performance for the compression of sparse data: adding a denoising function to a shallow architecture already reduces the loss provably, and a suitable multi-layer decoder leads to a further improvement. We validate our findings on image datasets, such as CIFAR-10 and MNIST.","sentences":["Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression.","However, basic theoretical questions remain unanswered even in a shallow two-layer setting.","In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution?","For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input.","Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity.","For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a permutation).","Finally, by exploiting a connection with approximate message passing algorithms, we show how to improve upon Gaussian performance for the compression of sparse data: adding a denoising function to a shallow architecture already reduces the loss provably, and a suitable multi-layer decoder leads to a further improvement.","We validate our findings on image datasets, such as CIFAR-10 and MNIST."],"url":"http://arxiv.org/abs/2402.05013v1","category":"cs.LG"}
{"created":"2024-02-07 16:32:13","title":"Information Theoretically Secure Encryption Key Generation over Wireless Networks by Exploiting Packet Errors","abstract":"This article presents a novel method for establishing an information theoretically secure encryption key over wireless channels. It exploits the fact that data transmission over wireless links is accompanied by packet error, while noise terms, and thereby the error events observed by two separate receivers are independent of each other. A number of data packets, with random data, are transmitted from a first legitimate node, say Alice, to a second legitimate node, say Bob. Bob identifies all packets that are received error-free in the first transmission attempt and sends their indices to Alice over a public channel. Then, both Alice and Bob mix the contents of identified packets, e.g., using a hash function, and thereby derive an identical encryption key. Since error events from Alice to Bob is independent of error events from Alice to Eve, the chances that Eve has successfully received all packets used in key generation error-free diminishes as the number of packet increases. In many wireless standards, the first stage in error detection and Automatic Repeat Request (ARQ) is deployed at the PHY/MAC (Physical Layer/Medium Access Control) layer. In such setups, the first re-transmission is manged by the PHY/MAC layer without informing higher layers. This makes it impossible to directly access the information related to packet errors through high-level programming interfaces available to an end-user. A method is presented for determining packets received error-free in first transmission attempts through high-level programming. Examples are presented in conjunction with an LTE cellular network.","sentences":["This article presents a novel method for establishing an information theoretically secure encryption key over wireless channels.","It exploits the fact that data transmission over wireless links is accompanied by packet error, while noise terms, and thereby the error events observed by two separate receivers are independent of each other.","A number of data packets, with random data, are transmitted from a first legitimate node, say Alice, to a second legitimate node, say Bob.","Bob identifies all packets that are received error-free in the first transmission attempt and sends their indices to Alice over a public channel.","Then, both Alice and Bob mix the contents of identified packets, e.g., using a hash function, and thereby derive an identical encryption key.","Since error events from Alice to Bob is independent of error events from Alice to Eve, the chances that Eve has successfully received all packets used in key generation error-free diminishes as the number of packet increases.","In many wireless standards, the first stage in error detection and Automatic Repeat Request (ARQ) is deployed at the PHY/MAC (Physical Layer/Medium Access Control) layer.","In such setups, the first re-transmission is manged by the PHY/MAC layer without informing higher layers.","This makes it impossible to directly access the information related to packet errors through high-level programming interfaces available to an end-user.","A method is presented for determining packets received error-free in first transmission attempts through high-level programming.","Examples are presented in conjunction with an LTE cellular network."],"url":"http://arxiv.org/abs/2402.05012v1","category":"cs.IT"}
{"created":"2024-02-07 16:28:36","title":"EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss","abstract":"We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.","sentences":["We present EfficientViT-SAM, a new family of accelerated segment anything models.","We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset.","Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance.","Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit."],"url":"http://arxiv.org/abs/2402.05008v1","category":"cs.CV"}
{"created":"2024-02-07 16:28:04","title":"Example-based Explanations for Random Forests using Machine Unlearning","abstract":"Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes machine unlearning to estimate the change in the tree structures of the random forest when parts of the underlying training data are removed, and then leverages the Apriori algorithm from frequent itemset mining to reduce the subset search space. We empirically evaluate our approach on three real-world datasets, and demonstrate that the explanations generated by FairDebugger are consistent with insights from prior studies on these datasets.","sentences":["Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation.","Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes.","Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior.","However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   ","We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier.","FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness.","Toward this goal, FairDebugger first utilizes machine unlearning to estimate the change in the tree structures of the random forest when parts of the underlying training data are removed, and then leverages the Apriori algorithm from frequent itemset mining to reduce the subset search space.","We empirically evaluate our approach on three real-world datasets, and demonstrate that the explanations generated by FairDebugger are consistent with insights from prior studies on these datasets."],"url":"http://arxiv.org/abs/2402.05007v1","category":"cs.LG"}
{"created":"2024-02-07 16:24:00","title":"Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in Signed Networks","abstract":"Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for irregularities. Our proposed region-based heuristic algorithm, tailored for this NP-hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100x speedup) in both traditional and generalized contexts.","sentences":["Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs.","Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance.","Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data.","Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for irregularities.","Our proposed region-based heuristic algorithm, tailored for this NP-hard problem, strikes a balance between low time complexity and high-quality outcomes.","Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100x speedup) in both traditional and generalized contexts."],"url":"http://arxiv.org/abs/2402.05006v1","category":"cs.SI"}
{"created":"2024-02-07 16:23:17","title":"Evidence and quantification of memory effects in competitive first passage events","abstract":"Splitting probabilities quantify the likelihood of a given outcome out of competitive events for general random processes. This key observable of random walk theory, historically introduced as the Gambler's ruin problem for a player in a casino, has a broad range of applications beyond mathematical finance in evolution genetics, physics and chemistry, such as allele fixation, polymer translocation, protein folding and more generally competitive reactions. The statistics of competitive events is well understood for memoryless (Markovian) processes. However, in complex systems such as polymer fluids, the motion of a particle should typically be described as a process with memory. Appart from scaling theories and perturbative approaches in one-dimension, the outcome of competitive events is much less characterized analytically for processes with memory. Here, we introduce an analytical approach that provides the splitting probabilities for general $d$-dimensional non-Markovian Gaussian processes. This analysis shows that splitting probabilities are critically controlled by the out of equilibrium statistics of reactive trajectories, observed after the first passage. This hallmark of non-Markovian dynamics and its quantitative impact on splitting probabilities are directly evidenced in a prototypical experimental reaction scheme in viscoelastic fluids. Altogether, these results reveal both experimentally and theoretically the importance of memory effects on competitive reactions.","sentences":["Splitting probabilities quantify the likelihood of a given outcome out of competitive events for general random processes.","This key observable of random walk theory, historically introduced as the Gambler's ruin problem for a player in a casino, has a broad range of applications beyond mathematical finance in evolution genetics, physics and chemistry, such as allele fixation, polymer translocation, protein folding and more generally competitive reactions.","The statistics of competitive events is well understood for memoryless (Markovian) processes.","However, in complex systems such as polymer fluids, the motion of a particle should typically be described as a process with memory.","Appart from scaling theories and perturbative approaches in one-dimension, the outcome of competitive events is much less characterized analytically for processes with memory.","Here, we introduce an analytical approach that provides the splitting probabilities for general $d$-dimensional non-Markovian Gaussian processes.","This analysis shows that splitting probabilities are critically controlled by the out of equilibrium statistics of reactive trajectories, observed after the first passage.","This hallmark of non-Markovian dynamics and its quantitative impact on splitting probabilities are directly evidenced in a prototypical experimental reaction scheme in viscoelastic fluids.","Altogether, these results reveal both experimentally and theoretically the importance of memory effects on competitive reactions."],"url":"http://arxiv.org/abs/2402.05005v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 16:22:02","title":"Near-Optimal Generalized Decoding of Polar-like Codes","abstract":"In this work, we present a framework that explores the tradeoff between the undetected error rate (UER) and block error rate (BLER) of polar-like codes. It relies on a novel approximation for what we call codebook probability, which assumes an auxiliary distribution mimicking the dynamics of decoding algorithms with successive cancellation (SC) decoding schedule. Simulation results demonstrates that, in the case of SC list decoding, the proposed framework outperforms the state-of-art approximations of Forney's generalized decoding rule for polar-like codes with dynamic frozen bits. In addition, the proposed generalized decoding outperforms the CRC-concatenated polar codes significantly in both BLER and UER. Finally, we briefly discuss two potential applications of the approximated codebook probability: coded pilot-free channel estimation and bitwise soft-output decoding.","sentences":["In this work, we present a framework that explores the tradeoff between the undetected error rate (UER) and block error rate (BLER) of polar-like codes.","It relies on a novel approximation for what we call codebook probability, which assumes an auxiliary distribution mimicking the dynamics of decoding algorithms with successive cancellation (SC) decoding schedule.","Simulation results demonstrates that, in the case of SC list decoding, the proposed framework outperforms the state-of-art approximations of Forney's generalized decoding rule for polar-like codes with dynamic frozen bits.","In addition, the proposed generalized decoding outperforms the CRC-concatenated polar codes significantly in both BLER and UER.","Finally, we briefly discuss two potential applications of the approximated codebook probability: coded pilot-free channel estimation and bitwise soft-output decoding."],"url":"http://arxiv.org/abs/2402.05004v1","category":"cs.IT"}
{"created":"2024-02-07 16:15:36","title":"Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design","abstract":"Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure.","sentences":["Combining discrete and continuous data is an important capability for generative models.","We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems.","Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains.","DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches.","We utilize our DFMs method to build a multimodal flow-based modeling framework.","We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence.","Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or structure."],"url":"http://arxiv.org/abs/2402.04997v1","category":"stat.ML"}
{"created":"2024-02-07 16:14:37","title":"Local interactions in active matter are reinforced by spatial structure","abstract":"The flocking of self-propelled particles in heterogeneous environments is relevant to both natural and artificial systems. The Vicsek model is a canonical choice to investigate such systems due to the minimal number of parameters required to define flocking. Prior research on the Vicsek model has investigated the effects of interaction rules, particle speed, and obstacle packing on the flocking behavior, but the effect of interaction radius remains an open question. Unlike obstacle-free domains, the locality of interactions not only affects how quickly the system can become polarized, but also how well the flocks can align or realign after colliding with obstacles. In this letter, we delve into this subtle relationship that exists in the scale of the perception of Vicsek particles in the presence of obstacles. We demonstrate that the presence of obstacles impacts group density, which provides the basis to identify distinct phases for collective behavior. This leads to the counter-intuitive result that obstacles, while generally confounding for macroscopic order, may enable global order even as noise in the system increases.","sentences":["The flocking of self-propelled particles in heterogeneous environments is relevant to both natural and artificial systems.","The Vicsek model is a canonical choice to investigate such systems due to the minimal number of parameters required to define flocking.","Prior research on the Vicsek model has investigated the effects of interaction rules, particle speed, and obstacle packing on the flocking behavior, but the effect of interaction radius remains an open question.","Unlike obstacle-free domains, the locality of interactions not only affects how quickly the system can become polarized, but also how well the flocks can align or realign after colliding with obstacles.","In this letter, we delve into this subtle relationship that exists in the scale of the perception of Vicsek particles in the presence of obstacles.","We demonstrate that the presence of obstacles impacts group density, which provides the basis to identify distinct phases for collective behavior.","This leads to the counter-intuitive result that obstacles, while generally confounding for macroscopic order, may enable global order even as noise in the system increases."],"url":"http://arxiv.org/abs/2402.04996v1","category":"physics.comp-ph"}
{"created":"2024-02-07 16:06:20","title":"PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses","abstract":"This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the PriorBoost algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that PriorBoost regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms.","sentences":["This work studies algorithms for learning from aggregate responses.","We focus on the construction of aggregation sets (called bags in the literature) for event-level loss functions.","We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering.","Further, we theoretically quantify the advantage of using curated bags over random bags.","We then propose the PriorBoost algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality.","We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that PriorBoost regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms."],"url":"http://arxiv.org/abs/2402.04987v1","category":"cs.LG"}
{"created":"2024-02-07 16:03:36","title":"Heat transport through an open coupled scalar field theory hosting stability-to-instability transition","abstract":"We investigate heat transport through a one-dimensional open coupled scalar field theory, depicted as a network of harmonic oscillators connected to thermal baths at the boundaries. The non-Hermitian dynamical matrix of the network undergoes a stability-to-instability transition at the exceptional points as the coupling strength between the scalar fields increases. The open network in the unstable regime, marked by the emergence of inverted oscillator modes, does not acquire a steady state, and the heat conduction is then unbounded for general bath couplings. In this work, we engineer a unique bath coupling where a single bath is connected to two fields at each edge with the same strength. This configuration leads to a finite steady-state heat conduction in the network, even in the unstable regime. We also study general bath couplings, e.g., connecting two fields to two separate baths at each boundary, which shows an exciting signature of approaching the unstable regime for massive fields. We derive analytical expressions for high-temperature classical heat current through the network for different bath couplings at the edges and compare them. Furthermore, we determine the temperature dependence of low-temperature quantum heat current in different cases. Our study will help to probe topological phases and phase transitions in various quadratic Hermitian bosonic models whose dynamical matrices resemble non-Hermitian Hamiltonians, hosting exciting topological phases.","sentences":["We investigate heat transport through a one-dimensional open coupled scalar field theory, depicted as a network of harmonic oscillators connected to thermal baths at the boundaries.","The non-Hermitian dynamical matrix of the network undergoes a stability-to-instability transition at the exceptional points as the coupling strength between the scalar fields increases.","The open network in the unstable regime, marked by the emergence of inverted oscillator modes, does not acquire a steady state, and the heat conduction is then unbounded for general bath couplings.","In this work, we engineer a unique bath coupling where a single bath is connected to two fields at each edge with the same strength.","This configuration leads to a finite steady-state heat conduction in the network, even in the unstable regime.","We also study general bath couplings, e.g., connecting two fields to two separate baths at each boundary, which shows an exciting signature of approaching the unstable regime for massive fields.","We derive analytical expressions for high-temperature classical heat current through the network for different bath couplings at the edges and compare them.","Furthermore, we determine the temperature dependence of low-temperature quantum heat current in different cases.","Our study will help to probe topological phases and phase transitions in various quadratic Hermitian bosonic models whose dynamical matrices resemble non-Hermitian Hamiltonians, hosting exciting topological phases."],"url":"http://arxiv.org/abs/2402.04986v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 16:01:22","title":"The cardinal characteristics of the ideal generated by the $F_\u03c3$ measure zero subsets of the reals","abstract":"Let $\\mathcal{E}$ be the ideal generated by the $F_\\sigma$ measure zero subsets of the reals. The purpose of this survey paper is to study the cardinal characteristics (the additivity, covering number, uniformity, and cofinality) of $\\mathcal{E}$.","sentences":["Let $\\mathcal{E}$ be the ideal generated by the $F_\\sigma$ measure zero subsets of the reals.","The purpose of this survey paper is to study the cardinal characteristics (the additivity, covering number, uniformity, and cofinality) of $\\mathcal{E}$."],"url":"http://arxiv.org/abs/2402.04984v1","category":"math.LO"}
{"created":"2024-02-07 15:59:53","title":"Broadband squeezed light field by magnetostriction in an opto-magnomechanical","abstract":"We present a novel mechanism for generating a wide bandwidth squeezed optical output field in an opto-magnomechanical system. In this system, the magnon (mechanical) mode in the yttrium-iron-garnet crystal is coupled to the microwave field (optical field) through magnetic dipole (radiation pressure) interaction. The magnetostrictive force induced by the yttrium-iron-garnet crystal causes a mechanical displacement and creates a quadrature squeezed magnon mode. Eventually, this quadrature squeezed mechanical mode is transferred to the output optical field through state-swap interaction. Our results demonstrate the optimal parameter range for obtaining a stable squeezed optical output field with a wide bandwidth. Moreover, the squeezed light field exhibits strong robustness to environmental temperature. The new scheme we propose has potential applications in quantum precision measurements, quantum wireless networks, quantum radar, etc.","sentences":["We present a novel mechanism for generating a wide bandwidth squeezed optical output field in an opto-magnomechanical system.","In this system, the magnon (mechanical) mode in the yttrium-iron-garnet crystal is coupled to the microwave field (optical field) through magnetic dipole (radiation pressure) interaction.","The magnetostrictive force induced by the yttrium-iron-garnet crystal causes a mechanical displacement and creates a quadrature squeezed magnon mode.","Eventually, this quadrature squeezed mechanical mode is transferred to the output optical field through state-swap interaction.","Our results demonstrate the optimal parameter range for obtaining a stable squeezed optical output field with a wide bandwidth.","Moreover, the squeezed light field exhibits strong robustness to environmental temperature.","The new scheme we propose has potential applications in quantum precision measurements, quantum wireless networks, quantum radar, etc."],"url":"http://arxiv.org/abs/2402.04983v1","category":"quant-ph"}
{"created":"2024-02-07 15:58:51","title":"Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for Energy Consumption Prediction","abstract":"This paper presents an approach integrating explainable artificial intelligence (XAI) techniques with adaptive learning to enhance energy consumption prediction models, with a focus on handling data distribution shifts. Leveraging SHAP clustering, our method provides interpretable explanations for model predictions and uses these insights to adaptively refine the model, balancing model complexity with predictive performance. We introduce a three-stage process: (1) obtaining SHAP values to explain model predictions, (2) clustering SHAP values to identify distinct patterns and outliers, and (3) refining the model based on the derived SHAP clustering characteristics. Our approach mitigates overfitting and ensures robustness in handling data distribution shifts. We evaluate our method on a comprehensive dataset comprising energy consumption records of buildings, as well as two additional datasets to assess the transferability of our approach to other domains, regression, and classification problems. Our experiments demonstrate the effectiveness of our approach in both task types, resulting in improved predictive performance and interpretable model explanations.","sentences":["This paper presents an approach integrating explainable artificial intelligence (XAI) techniques with adaptive learning to enhance energy consumption prediction models, with a focus on handling data distribution shifts.","Leveraging SHAP clustering, our method provides interpretable explanations for model predictions and uses these insights to adaptively refine the model, balancing model complexity with predictive performance.","We introduce a three-stage process: (1) obtaining SHAP values to explain model predictions, (2) clustering SHAP values to identify distinct patterns and outliers, and (3) refining the model based on the derived SHAP clustering characteristics.","Our approach mitigates overfitting and ensures robustness in handling data distribution shifts.","We evaluate our method on a comprehensive dataset comprising energy consumption records of buildings, as well as two additional datasets to assess the transferability of our approach to other domains, regression, and classification problems.","Our experiments demonstrate the effectiveness of our approach in both task types, resulting in improved predictive performance and interpretable model explanations."],"url":"http://arxiv.org/abs/2402.04982v1","category":"cs.LG"}
{"created":"2024-02-07 15:57:30","title":"Asymptotics of feature learning in two-layer networks after one gradient-step","abstract":"In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond perturbative finite width corrections of the conjugate and neural tangent kernels.","sentences":["In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step.","Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate.","We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime.","To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond perturbative finite width corrections of the conjugate and neural tangent kernels."],"url":"http://arxiv.org/abs/2402.04980v1","category":"stat.ML"}
{"created":"2024-02-07 15:57:28","title":"Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training","abstract":"Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications. The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene. We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices. Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability.","sentences":["Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications.","The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene.","We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices.","Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects.","Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability."],"url":"http://arxiv.org/abs/2402.04979v1","category":"cs.CV"}
{"created":"2024-02-07 15:56:17","title":"An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration","abstract":"While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.","sentences":["While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.","To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs.","This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning.","The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process.","Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results.","Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work.","Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues."],"url":"http://arxiv.org/abs/2402.04978v1","category":"cs.CL"}
{"created":"2024-02-07 15:55:51","title":"ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12","abstract":"As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children's autonomous Scratch learning: artist's block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist's block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children.","sentences":["As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12).","Through formative investigation with Scratch experts, we uncover three key obstacles to children's autonomous Scratch learning: artist's block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation.","To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children.","ChatScratch employs structured interactive storyboards and visual cues to overcome artist's block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance.","Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children."],"url":"http://arxiv.org/abs/2402.04975v1","category":"cs.HC"}
{"created":"2024-02-07 15:52:15","title":"Decay dynamics of a single spherical domain in near-critical phase-separated conditions","abstract":"Domain decay is at the heart of the so-called evaporation-condensation Ostwald-ripening regime of phase ordering kinetics, where the growth of large domains occurs at the expense of smaller ones, which are expected to `evaporate'. We experimentally investigate such decay dynamics at the level of a single spherical domain picked from one phase in coexistence and brought into the other phase by an opto-mechanical approach, in a near-critical phase-separated binary liquid mixture. We observe that the decay dynamics is generally not compatible with the theoretically expected surface-tension decay laws for conserved order parameters. Using a mean-field description, we quantitatively explain this apparent disagreement by the stratification of solute concentrations induced by gravity close to a critical point. Finally, we determine the conditions for which buoyancy becomes negligible compared to capillarity and perform dedicated experiments that retrieve the predicted surface-tension induced decay exponent. The surface-tension driven decay dynamics of conserved order parameter systems in the presence and the absence of gravity, is thus established at the level of a single domain.","sentences":["Domain decay is at the heart of the so-called evaporation-condensation Ostwald-ripening regime of phase ordering kinetics, where the growth of large domains occurs at the expense of smaller ones, which are expected to `evaporate'.","We experimentally investigate such decay dynamics at the level of a single spherical domain picked from one phase in coexistence and brought into the other phase by an opto-mechanical approach, in a near-critical phase-separated binary liquid mixture.","We observe that the decay dynamics is generally not compatible with the theoretically expected surface-tension decay laws for conserved order parameters.","Using a mean-field description, we quantitatively explain this apparent disagreement by the stratification of solute concentrations induced by gravity close to a critical point.","Finally, we determine the conditions for which buoyancy becomes negligible compared to capillarity and perform dedicated experiments that retrieve the predicted surface-tension induced decay exponent.","The surface-tension driven decay dynamics of conserved order parameter systems in the presence and the absence of gravity, is thus established at the level of a single domain."],"url":"http://arxiv.org/abs/2402.04973v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 15:51:39","title":"Distributed Fair Assignment and Rebalancing for Mobility-on-Demand Systems via an Auction-based Method","abstract":"In this paper, we consider fair assignment of complex requests for Mobility-On-Demand systems. We model the transportation requests as temporal logic formulas that must be satisfied by a fleet of vehicles. We require that the assignment of requests to vehicles is performed in a distributed manner based only on communication between vehicles while ensuring fair allocation. Our approach to the vehicle-request assignment problem is based on a distributed auction scheme with no centralized bidding that leverages utility history correction of bids to improve fairness. Complementarily, we propose a rebalancing scheme that employs rerouting vehicles to more rewarding areas to increase the potential future utility and ensure a fairer utility distribution. We adopt the max-min and deviation of utility as the two criteria for fairness. We demonstrate the methods in the mid-Manhattan map with a large number of requests generated in different probability settings. We show that we increase the fairness between vehicles based on the fairness criteria without degenerating the servicing quality.","sentences":["In this paper, we consider fair assignment of complex requests for Mobility-On-Demand systems.","We model the transportation requests as temporal logic formulas that must be satisfied by a fleet of vehicles.","We require that the assignment of requests to vehicles is performed in a distributed manner based only on communication between vehicles while ensuring fair allocation.","Our approach to the vehicle-request assignment problem is based on a distributed auction scheme with no centralized bidding that leverages utility history correction of bids to improve fairness.","Complementarily, we propose a rebalancing scheme that employs rerouting vehicles to more rewarding areas to increase the potential future utility and ensure a fairer utility distribution.","We adopt the max-min and deviation of utility as the two criteria for fairness.","We demonstrate the methods in the mid-Manhattan map with a large number of requests generated in different probability settings.","We show that we increase the fairness between vehicles based on the fairness criteria without degenerating the servicing quality."],"url":"http://arxiv.org/abs/2402.04972v1","category":"cs.FL"}
{"created":"2024-02-07 15:50:20","title":"Multi-Sender Persuasion -- A Computational Perspective","abstract":"We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems.","sentences":["We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions.","Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives.","The core solution concept here is the Nash equilibrium of senders' signaling policies.","Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard.","Given these intrinsic difficulties, we turn to finding local Nash equilibria.","We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities.","Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks.","Broadly, our theoretical and empirical contributions are of interest to a large class of economic problems."],"url":"http://arxiv.org/abs/2402.04971v1","category":"cs.AI"}
{"created":"2024-02-07 15:44:55","title":"Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?","abstract":"This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $\\Delta$F1 of 0.18.","sentences":["This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings.","We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset.","The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting.","Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02.","Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%).","Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $\\Delta$F1 of 0.18."],"url":"http://arxiv.org/abs/2402.04967v1","category":"cs.CL"}
{"created":"2024-02-07 15:44:04","title":"On the Cahn-Hilliard equation with kinetic rate dependent dynamic boundary conditions and non-smooth potentials: Well-posedness and asymptotic limits","abstract":"We consider a class of Cahn-Hilliard equation with kinetic rate dependent dynamic boundary conditions that describe possible short-range interactions between the binary mixture and the solid boundary. In the presence of surface diffusion on the boundary, the initial boundary value problem can be viewed as a transmission problem consisting of Cahn-Hilliard type equations both in the bulk and on the boundary. We first prove existence, uniqueness and continuous dependence of global weak solutions. In the construction of solutions, an explicit convergence rate in terms of the parameter for the Yosida approximation is established. Under some additional assumptions, we also obtain the existence and uniqueness of global strong solutions. Next, we study the asymptotic limit as the coefficient of the boundary diffusion goes to zero and show that the limit problem with a forward-backward dynamic boundary condition is well-posed in a suitable weak formulation. Besides, we investigate the asymptotic limit as the kinetic rate tends to zero and infinity, respectively. Our results are valid for a general class of bulk and boundary potentials with double-well structure, including the physically relevant logarithmic potential and the non-smooth double-obstacle potential.","sentences":["We consider a class of Cahn-Hilliard equation with kinetic rate dependent dynamic boundary conditions that describe possible short-range interactions between the binary mixture and the solid boundary.","In the presence of surface diffusion on the boundary, the initial boundary value problem can be viewed as a transmission problem consisting of Cahn-Hilliard type equations both in the bulk and on the boundary.","We first prove existence, uniqueness and continuous dependence of global weak solutions.","In the construction of solutions, an explicit convergence rate in terms of the parameter for the Yosida approximation is established.","Under some additional assumptions, we also obtain the existence and uniqueness of global strong solutions.","Next, we study the asymptotic limit as the coefficient of the boundary diffusion goes to zero and show that the limit problem with a forward-backward dynamic boundary condition is well-posed in a suitable weak formulation.","Besides, we investigate the asymptotic limit as the kinetic rate tends to zero and infinity, respectively.","Our results are valid for a general class of bulk and boundary potentials with double-well structure, including the physically relevant logarithmic potential and the non-smooth double-obstacle potential."],"url":"http://arxiv.org/abs/2402.04965v1","category":"math.AP"}
{"created":"2024-02-07 15:43:42","title":"Collective Departure Time Allocation in Large-scale Urban Networks: A Flexible Modeling Framework with Trip Length and Desired Arrival Time Distributions","abstract":"Urban traffic congestion remains a persistent issue for cities worldwide. Recent macroscopic models have adopted a mathematically well-defined relation between network flow and density to characterize traffic states over an urban region. Despite advances in these models, capturing the complex dynamics of urban traffic congestion requires considering the heterogeneous characteristics of trips. Classic macroscopic models, e.g., bottleneck and bathtub models and their extensions, have attempted to account for these characteristics, such as trip-length distribution and desired arrival times. However, they often make assumptions that fall short of reflecting real-world conditions. To address this, generalized bathtub models were recently proposed, introducing a new state variable to capture any distribution of remaining trip lengths. This study builds upon this work to formulate and solve the social optimum, a solution minimizing the sum of all users' generalized (i.e., social and monetary) costs for a departure time choice model. The proposed framework can accommodate any distribution for desired arrival time and trip length, making it more adaptable to the diverse array of trip characteristics in an urban setting. In addition, the existence of the solution is proven, and the proposed solution method calculates the social optimum analytically. The numerical results show that the method is computationally efficient. The proposed methodology is validated on the real test case of Lyon North City, benchmarking with deterministic and stochastic user equilibria.","sentences":["Urban traffic congestion remains a persistent issue for cities worldwide.","Recent macroscopic models have adopted a mathematically well-defined relation between network flow and density to characterize traffic states over an urban region.","Despite advances in these models, capturing the complex dynamics of urban traffic congestion requires considering the heterogeneous characteristics of trips.","Classic macroscopic models, e.g., bottleneck and bathtub models and their extensions, have attempted to account for these characteristics, such as trip-length distribution and desired arrival times.","However, they often make assumptions that fall short of reflecting real-world conditions.","To address this, generalized bathtub models were recently proposed, introducing a new state variable to capture any distribution of remaining trip lengths.","This study builds upon this work to formulate and solve the social optimum, a solution minimizing the sum of all users' generalized (i.e., social and monetary) costs for a departure time choice model.","The proposed framework can accommodate any distribution for desired arrival time and trip length, making it more adaptable to the diverse array of trip characteristics in an urban setting.","In addition, the existence of the solution is proven, and the proposed solution method calculates the social optimum analytically.","The numerical results show that the method is computationally efficient.","The proposed methodology is validated on the real test case of Lyon North City, benchmarking with deterministic and stochastic user equilibria."],"url":"http://arxiv.org/abs/2402.04963v1","category":"math.OC"}
{"created":"2024-02-07 15:42:50","title":"Hidden non-equilibrium pathways towards crystalline perfection","abstract":"A central paradigm of non-equilibrium physics concerns the dynamics of heterogeneity and disorder, impacting processes ranging from the behavior of glasses1 to the emergent functionality of active matter. Understanding these complex mesoscopic systems requires probing the microscopic trajectories associated with irreversible processes, the role of fluctuations and entropy growth, and the timescales on which non-equilibrium responses are ultimately maintained. Approaches that illuminate these processes in model systems may enable a more general understanding of other heterogeneous non-equilibrium phenomena, and potentially define ultimate speed and energy cost limits for information processing technologies. Here, we apply ultrafast single shot x-ray photon correlation spectroscopy (XPCS) to resolve the non-equilibrium, heterogeneous, and irreversible mesoscale dynamics during a light-induced phase transition. This approach defines a new way of capturing the nucleation of the induced phase, the formation of transient mesoscale defects at the boundaries of the nuclei, and the eventual disappearance of these defects, even in systems with complex polarization topologies. A non-equilibrium, sub-diffusive response spanning >10 orders of magnitude in timescales is observed with multistep behavior similar to the plateaus observed in supercooled liquids or glasses. We show how the observed time-dependent long-time correlations can be understood in terms of the stochastic dynamics of domain walls, encoded in effective waiting-time distributions with power-law tails. This work defines new possibilities for probing the non-equilibrium dynamics of disordered and heterogeneous media.","sentences":["A central paradigm of non-equilibrium physics concerns the dynamics of heterogeneity and disorder, impacting processes ranging from the behavior of glasses1 to the emergent functionality of active matter.","Understanding these complex mesoscopic systems requires probing the microscopic trajectories associated with irreversible processes, the role of fluctuations and entropy growth, and the timescales on which non-equilibrium responses are ultimately maintained.","Approaches that illuminate these processes in model systems may enable a more general understanding of other heterogeneous non-equilibrium phenomena, and potentially define ultimate speed and energy cost limits for information processing technologies.","Here, we apply ultrafast single shot x-ray photon correlation spectroscopy (XPCS) to resolve the non-equilibrium, heterogeneous, and irreversible mesoscale dynamics during a light-induced phase transition.","This approach defines a new way of capturing the nucleation of the induced phase, the formation of transient mesoscale defects at the boundaries of the nuclei, and the eventual disappearance of these defects, even in systems with complex polarization topologies.","A non-equilibrium, sub-diffusive response spanning >10 orders of magnitude in timescales is observed with multistep behavior similar to the plateaus observed in supercooled liquids or glasses.","We show how the observed time-dependent long-time correlations can be understood in terms of the stochastic dynamics of domain walls, encoded in effective waiting-time distributions with power-law tails.","This work defines new possibilities for probing the non-equilibrium dynamics of disordered and heterogeneous media."],"url":"http://arxiv.org/abs/2402.04962v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 15:41:01","title":"Channel-Selective Normalization for Label-Shift Robust Test-Time Adaptation","abstract":"Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution. For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets. To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference. Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks. It is implemented by recalculating batch normalization statistics on test batches. Prior work has focused on analysis with test data that has the same label distribution as the training data. However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure. This presents a risk in applying test time adaptation methods in deployment. We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts. Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes. We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts. We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters.","sentences":["Deep neural networks have useful applications in many different tasks, however their performance can be severely affected by changes in the data distribution.","For example, in the biomedical field, their performance can be affected by changes in the data (different machines, populations) between training and test datasets.","To ensure robustness and generalization to real-world scenarios, test-time adaptation has been recently studied as an approach to adjust models to a new data distribution during inference.","Test-time batch normalization is a simple and popular method that achieved compelling performance on domain shift benchmarks.","It is implemented by recalculating batch normalization statistics on test batches.","Prior work has focused on analysis with test data that has the same label distribution as the training data.","However, in many practical applications this technique is vulnerable to label distribution shifts, sometimes producing catastrophic failure.","This presents a risk in applying test time adaptation methods in deployment.","We propose to tackle this challenge by only selectively adapting channels in a deep network, minimizing drastic adaptation that is sensitive to label shifts.","Our selection scheme is based on two principles that we empirically motivate: (1) later layers of networks are more sensitive to label shift (2) individual features can be sensitive to specific classes.","We apply the proposed technique to three classification tasks, including CIFAR10-C, Imagenet-C, and diagnosis of fatty liver, where we explore both covariate and label distribution shifts.","We find that our method allows to bring the benefits of TTA while significantly reducing the risk of failure common in other methods, while being robust to choice in hyperparameters."],"url":"http://arxiv.org/abs/2402.04958v1","category":"cs.CV"}
{"created":"2024-02-07 15:40:22","title":"Reconfidencing LLMs from the Grouping Loss Perspective","abstract":"Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \\emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.","sentences":["Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone.","While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss.","In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA.","Experiments show that they tend to be overconfident.","Further, we show that they are more overconfident on some answers than others, \\emph{eg} depending on the nationality of the person in the query.","In uncertainty-quantification theory, this is grouping loss.","To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss.","The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses."],"url":"http://arxiv.org/abs/2402.04957v1","category":"cs.CL"}
{"created":"2024-02-07 15:39:07","title":"Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems","abstract":"Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks. Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner. However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial. As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and perceived performance than intent-based systems, suggesting that switching NLP techniques should be investigated further.","sentences":["Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks.","Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns.","However, this rigidness does not handle the diversity of natural language well.","Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner.","However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial.","As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability.","This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and perceived performance than intent-based systems, suggesting that switching NLP techniques should be investigated further."],"url":"http://arxiv.org/abs/2402.04955v1","category":"cs.HC"}
{"created":"2024-02-07 15:38:46","title":"Localization theorems for approximable triangulated categories","abstract":"Approximable triangulated categories, introduced and developed by Neeman, provide a reasonable framework to study localization sequences for triangulated categories. In the paper, we show that an arbitrary recollement of approximable triangulated categories, under mild conditions, induces short exact sequences of triangulated subcategories and Verdier quotient categories. In particular, a recollement of locally finite, noetherian and approximable triangulated categories induces a short exact sequence of bounded closures of compact objects in these categories. If the given recollement extends one step downwards, then we obtain a short exact sequence of the singularity categories of these triangulated categories, which generalizes the localization sequence for the singularity categories of finite-dimensional algebras recently established by Jin-Yang-Zhou.","sentences":["Approximable triangulated categories, introduced and developed by Neeman, provide a reasonable framework to study localization sequences for triangulated categories.","In the paper, we show that an arbitrary recollement of approximable triangulated categories, under mild conditions, induces short exact sequences of triangulated subcategories and Verdier quotient categories.","In particular, a recollement of locally finite, noetherian and approximable triangulated categories induces a short exact sequence of bounded closures of compact objects in these categories.","If the given recollement extends one step downwards, then we obtain a short exact sequence of the singularity categories of these triangulated categories, which generalizes the localization sequence for the singularity categories of finite-dimensional algebras recently established by Jin-Yang-Zhou."],"url":"http://arxiv.org/abs/2402.04954v1","category":"math.RT"}
{"created":"2024-02-07 15:36:53","title":"Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms","abstract":"Many state-of-the-art causal discovery methods aim to generate an output graph that encodes the graphical separation and connection statements of the causal graph that underlies the data-generating process. In this work, we argue that an evaluation of a causal discovery method against synthetic data should include an analysis of how well this explicit goal is achieved by measuring how closely the separations/connections of the method's output align with those of the ground truth. We show that established evaluation measures do not accurately capture the difference in separations/connections of two causal graphs, and we introduce three new measures of distance called s/c-distance, Markov distance and Faithfulness distance that address this shortcoming. We complement our theoretical analysis with toy examples, empirical experiments and pseudocode.","sentences":["Many state-of-the-art causal discovery methods aim to generate an output graph that encodes the graphical separation and connection statements of the causal graph that underlies the data-generating process.","In this work, we argue that an evaluation of a causal discovery method against synthetic data should include an analysis of how well this explicit goal is achieved by measuring how closely the separations/connections of the method's output align with those of the ground truth.","We show that established evaluation measures do not accurately capture the difference in separations/connections of two causal graphs, and we introduce three new measures of distance called s/c-distance, Markov distance and Faithfulness distance that address this shortcoming.","We complement our theoretical analysis with toy examples, empirical experiments and pseudocode."],"url":"http://arxiv.org/abs/2402.04952v1","category":"stat.ME"}
{"created":"2024-02-07 15:31:51","title":"Energy Dissipation to Tungsten Surfaces upon Eley-Rideal Recombination of N2 and H2","abstract":"Quasiclassical molecular dynamics simulations are performed to investigate energy dissipation to the (100) and (110) tungsten surfaces upon Eley-Rideal (ER) recombination of H2 and N2. Calculations are carried out within the single adsorbate limit under normal incidence. A generalized Langevin surface oscillator (GLO) scheme is used to simulate the coupling to phonons, whereas electron-hole (e-h) pair excitations are implemented using the local density friction approximation (LDFA). Phonon excitations are found to reduce the ER reactivity for N2 recombination, but do not affect H abstraction. In contrast, the effect of e-h pair excitations on the ER recombination cross section is small for N2, but can be important for H2. The analysis of the energy lost by the recombined species shows that most of the energy is dissipated into phonon excitations in the N2 recombination and into electronic excitations in the H2 recombination. In all cases, the energy dissipated into e-h pairs is taken away from the translational kinetic energy of the formed molecules, whereas dissipation to phonons, only significant for N2, also affects vibration. Interestingly, the electron mediated energy losses are found to be smaller in the case of N2 when surface motion is allowed.","sentences":["Quasiclassical molecular dynamics simulations are performed to investigate energy dissipation to the (100) and (110) tungsten surfaces upon Eley-Rideal (ER) recombination of H2 and N2.","Calculations are carried out within the single adsorbate limit under normal incidence.","A generalized Langevin surface oscillator (GLO) scheme is used to simulate the coupling to phonons, whereas electron-hole (e-h) pair excitations are implemented using the local density friction approximation (LDFA).","Phonon excitations are found to reduce the ER reactivity for N2 recombination, but do not affect H abstraction.","In contrast, the effect of e-h pair excitations on the ER recombination cross section is small for N2, but can be important for H2.","The analysis of the energy lost by the recombined species shows that most of the energy is dissipated into phonon excitations in the N2 recombination and into electronic excitations in the H2 recombination.","In all cases, the energy dissipated into e-h pairs is taken away from the translational kinetic energy of the formed molecules, whereas dissipation to phonons, only significant for N2, also affects vibration.","Interestingly, the electron mediated energy losses are found to be smaller in the case of N2 when surface motion is allowed."],"url":"http://arxiv.org/abs/2402.04949v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 15:30:53","title":"A geometric model for semilinear locally gentle algebras","abstract":"We consider certain generalizations of gentle algebras that we call semilinear locally gentle algebras. These rings are examples of semilinear clannish algebras as introduced by the second author and Crawley-Boevey. We generalise the notion of a nodal algebra from work of Burban and Drozd and prove that semilinear gentle algebras are nodal by adapting a theorem of Zembyk. We also provide a geometric realization of Zembyk's proof, which involves cutting the surface into simpler pieces in order to endow our locally gentle algebra with a semilinear structure. We then consider this surface glued back together, with the seams in place, and use it to give a geometric model for the finite-dimensional modules over the semilinear locally gentle algebra.","sentences":["We consider certain generalizations of gentle algebras that we call semilinear locally gentle algebras.","These rings are examples of semilinear clannish algebras as introduced by the second author and Crawley-Boevey.","We generalise the notion of a nodal algebra from work of Burban and Drozd and prove that semilinear gentle algebras are nodal by adapting a theorem of Zembyk.","We also provide a geometric realization of Zembyk's proof, which involves cutting the surface into simpler pieces in order to endow our locally gentle algebra with a semilinear structure.","We then consider this surface glued back together, with the seams in place, and use it to give a geometric model for the finite-dimensional modules over the semilinear locally gentle algebra."],"url":"http://arxiv.org/abs/2402.04947v1","category":"math.RT"}
{"created":"2024-02-07 15:16:21","title":"An approach to automated videogame beta testing","abstract":"Videogames developed in the 1970s and 1980s were modest programs created in a couple of months by a single person, who played the roles of designer, artist and programmer. Since then, videogames have evolved to become a multi-million dollar industry. Today, AAA game development involves hundreds of people working together over several years. Management and engineering requirements have changed at the same pace. Although many of the processes have been adapted over time, this is not quite true for quality assurance tasks, which are still done mainly manually by human beta testers due to the specific peculiarities of videogames. This paper presents an approach to automate this beta testing.","sentences":["Videogames developed in the 1970s and 1980s were modest programs created in a couple of months by a single person, who played the roles of designer, artist and programmer.","Since then, videogames have evolved to become a multi-million dollar industry.","Today, AAA game development involves hundreds of people working together over several years.","Management and engineering requirements have changed at the same pace.","Although many of the processes have been adapted over time, this is not quite true for quality assurance tasks, which are still done mainly manually by human beta testers due to the specific peculiarities of videogames.","This paper presents an approach to automate this beta testing."],"url":"http://arxiv.org/abs/2402.04938v1","category":"cs.AI"}
{"created":"2024-02-07 14:59:25","title":"Blue noise for diffusion models","abstract":"Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network. Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored. In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account. More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask. Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only. Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow. We perform both qualitative and quantitative evaluations on a variety of datasets using our method, achieving improvements on different tasks over existing deterministic diffusion models in terms of FID metric.","sentences":["Most of the existing diffusion models use Gaussian noise for training and sampling across all time steps, which may not optimally account for the frequency contents reconstructed by the denoising network.","Despite the diverse applications of correlated noise in computer graphics, its potential for improving the training process has been underexplored.","In this paper, we introduce a novel and general class of diffusion models taking correlated noise within and across images into account.","More specifically, we propose a time-varying noise model to incorporate correlated noise into the training process, as well as a method for fast generation of correlated noise mask.","Our model is built upon deterministic diffusion models and utilizes blue noise to help improve the generation quality compared to using Gaussian white (random) noise only.","Further, our framework allows introducing correlation across images within a single mini-batch to improve gradient flow.","We perform both qualitative and quantitative evaluations on a variety of datasets using our method, achieving improvements on different tasks over existing deterministic diffusion models in terms of FID metric."],"url":"http://arxiv.org/abs/2402.04930v1","category":"cs.CV"}
{"created":"2024-02-07 14:56:13","title":"Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation","abstract":"This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.","sentences":["This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA).","Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process.","Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model.","We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data.","We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA.","The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images."],"url":"http://arxiv.org/abs/2402.04929v1","category":"cs.CV"}
{"created":"2024-02-07 14:53:01","title":"Influences of Divalent Ions in Natural Seawater/River Water on Nanofluidic Osmotic Energy Generation","abstract":"Besides the dominant NaCl, natural seawater/river water contains trace multivalent ions, which can provide effective screening to surface charges. Here, in both negatively and positively charged nanopores, influences from divalent ions as counterions and coions have been investigated on the performance of osmotic energy conversion (OEC) under natural salt gradients. As counterions, trace Ca2+ ions can suppress the electric power and conversion efficiency significantly. The reduced OEC performance is due to the bivalence and low diffusion coefficient of Ca2 ions, instead of the uphill transport of divalent ions discovered in the previous work. Effectively screened charged surfaces by Ca2+ ions induce enhanced diffusion of Cl ions which simultaneously decreases the net ion penetration and ionic selectivity of the nanopore. While as coions, Ca2+ ions have weak effects on the OEC performance. The promotion from charged exterior surfaces on OEC processes for ultra-short nanopores is also studied, which effective region is ~200 nm in width beyond pore boundaries independent of the presence of Ca2+ ions. Our results shed light on the physical details of the nanofluidic OEC process under natural seawater/river water conditions, which can provide a useful guide for high-performance osmotic energy harvesting.","sentences":["Besides the dominant NaCl, natural seawater/river water contains trace multivalent ions, which can provide effective screening to surface charges.","Here, in both negatively and positively charged nanopores, influences from divalent ions as counterions and coions have been investigated on the performance of osmotic energy conversion (OEC) under natural salt gradients.","As counterions, trace Ca2+ ions can suppress the electric power and conversion efficiency significantly.","The reduced OEC performance is due to the bivalence and low diffusion coefficient of Ca2 ions, instead of the uphill transport of divalent ions discovered in the previous work.","Effectively screened charged surfaces by Ca2+ ions induce enhanced diffusion of Cl ions which simultaneously decreases the net ion penetration and ionic selectivity of the nanopore.","While as coions, Ca2+ ions have weak effects on the OEC performance.","The promotion from charged exterior surfaces on OEC processes for ultra-short nanopores is also studied, which effective region is ~200 nm in width beyond pore boundaries independent of the presence of Ca2+ ions.","Our results shed light on the physical details of the nanofluidic OEC process under natural seawater/river water conditions, which can provide a useful guide for high-performance osmotic energy harvesting."],"url":"http://arxiv.org/abs/2402.04928v1","category":"physics.chem-ph"}
{"created":"2024-02-07 14:52:17","title":"Generalized stochastic processes revisited","abstract":"The paper addresses the question whether a random functional, a map from a set $E$ into the space of real-valued measurable functions on a probability space, has a measurable version with values in ${\\mathbb R}^E$. Similarly, one may ask whether linear random functionals have versions in the algebraic dual. Most importantly, it can be asked which locally convex topological vector spaces $E$ have the ``regularity property'' that any linear random functional on $E$ has a version with values in the dual $E'$, an important issue in the theory of generalized stochastic processes. It has been shown by It\\^{o} and Nawata that this is the case when $E$ is nuclear. However, the question of uniqueness has only been partially answered. We build up a framework where these and related questions can be clarified in terms of spaces and mappings. We study classes of spaces $E$ (beyond nuclear spaces) with the said regularity property, prove a seemingly new uniqueness result and exhibit various examples and counterexamples.","sentences":["The paper addresses the question whether a random functional, a map from a set $E$ into the space of real-valued measurable functions on a probability space, has a measurable version with values in ${\\mathbb R}^E$.","Similarly, one may ask whether linear random functionals have versions in the algebraic dual.","Most importantly, it can be asked which locally convex topological vector spaces $E$ have the ``regularity property'' that any linear random functional on $E$ has a version with values in the dual $E'$, an important issue in the theory of generalized stochastic processes.","It has been shown by It\\^{o} and Nawata that this is the case when $E$ is nuclear.","However, the question of uniqueness has only been partially answered.","We build up a framework where these and related questions can be clarified in terms of spaces and mappings.","We study classes of spaces $E$ (beyond nuclear spaces) with the said regularity property, prove a seemingly new uniqueness result and exhibit various examples and counterexamples."],"url":"http://arxiv.org/abs/2402.04926v1","category":"math.FA"}
{"created":"2024-02-07 14:47:13","title":"Voronoi Candidates for Bayesian Optimization","abstract":"Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions. However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead. Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates. Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them. We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension. On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy.","sentences":["Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions.","However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead.","Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates.","Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them.","We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension.","On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy."],"url":"http://arxiv.org/abs/2402.04922v1","category":"stat.ML"}
{"created":"2024-02-07 14:44:42","title":"Prompting Implicit Discourse Relation Annotation","abstract":"Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.","sentences":["Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers.","Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches.","This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations.","In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks.","Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings."],"url":"http://arxiv.org/abs/2402.04918v1","category":"cs.CL"}
{"created":"2024-02-07 14:41:17","title":"Moco: A Learnable Meta Optimizer for Combinatorial Optimization","abstract":"Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum Independent Set (MIS) and show that it outperforms other approaches on MIS and is overall competitive on the TSP, especially outperforming related approaches, partially even if they use additional local search.","sentences":["Relevant combinatorial optimization problems (COPs) are often NP-hard.","While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data.","Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time.","Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state.","This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget.","This allows Moco to adapt to varying circumstances such as different computational budgets.","Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition.","We test Moco on the Traveling Salesman Problem (TSP) and Maximum Independent Set (MIS) and show that it outperforms other approaches on MIS and is overall competitive on the TSP, especially outperforming related approaches, partially even if they use additional local search."],"url":"http://arxiv.org/abs/2402.04915v1","category":"cs.LG"}
{"created":"2024-02-07 14:41:08","title":"Personalized Text Generation with Fine-Grained Linguistic Control","abstract":"As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.","sentences":["As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized.","However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment.","In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes.","We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes.","We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance.","We make our code, data, and pretrained models publicly available."],"url":"http://arxiv.org/abs/2402.04914v1","category":"cs.CL"}
{"created":"2024-02-07 14:39:26","title":"Fast Beam Training for Near-Field Communication Systems","abstract":"In millimeter-wave communications, large-scale antenna arrays are commonly employed to mitigate obstacle occlusion and path loss. However, these large-scale arrays generate pencil-shaped beams, which necessitate a higher number of training beams to cover the desired space. This results in the heavy beam training overhead. Furthermore, as the antenna aperture increases, users are more likely to be situated in the near-field region of the base station (BS) antenna array. This motivates our investigation into the beam training problem in the near-field region to achieve efficient beam alignment. To address the high complexity and low identification accuracy of existing beam training techniques, we propose an efficient hashing multi-arm beam (HMB) training scheme for the near-field scenario. Specifically, we first design a set of sparse bases based on the polar domain sparsity of the near-field channel and construct a near-field single-beam training codebook. Then, the hash functions are chosen to construct the near-field multi-arm beam training codebook. Each multi-arm beam training codeword is used in a time slot until the predefined codebook is traversed. Finally, the soft decision and voting methods are applied to distinguish the signal from different BS and obtain the correctly aligned beams. In addition, we provide the logically rigorous proof of computational complexity. Simulation results show that our proposed near-field HMB training method can achieve 96.4% identification accuracy of the exhaustive beam training method and greatly reduce the training overhead to the logarithmic level. Furthermore, we verify its applicability under the far-field scenario as well.","sentences":["In millimeter-wave communications, large-scale antenna arrays are commonly employed to mitigate obstacle occlusion and path loss.","However, these large-scale arrays generate pencil-shaped beams, which necessitate a higher number of training beams to cover the desired space.","This results in the heavy beam training overhead.","Furthermore, as the antenna aperture increases, users are more likely to be situated in the near-field region of the base station (BS) antenna array.","This motivates our investigation into the beam training problem in the near-field region to achieve efficient beam alignment.","To address the high complexity and low identification accuracy of existing beam training techniques, we propose an efficient hashing multi-arm beam (HMB) training scheme for the near-field scenario.","Specifically, we first design a set of sparse bases based on the polar domain sparsity of the near-field channel and construct a near-field single-beam training codebook.","Then, the hash functions are chosen to construct the near-field multi-arm beam training codebook.","Each multi-arm beam training codeword is used in a time slot until the predefined codebook is traversed.","Finally, the soft decision and voting methods are applied to distinguish the signal from different BS and obtain the correctly aligned beams.","In addition, we provide the logically rigorous proof of computational complexity.","Simulation results show that our proposed near-field HMB training method can achieve 96.4% identification accuracy of the exhaustive beam training method and greatly reduce the training overhead to the logarithmic level.","Furthermore, we verify its applicability under the far-field scenario as well."],"url":"http://arxiv.org/abs/2402.04913v1","category":"cs.IT"}
{"created":"2024-02-07 14:39:11","title":"Towards Biologically Plausible and Private Gene Expression Data Generation","abstract":"Generative models trained with Differential Privacy (DP) are becoming increasingly prominent in the creation of synthetic data for downstream applications. Existing literature, however, primarily focuses on basic benchmarking datasets and tends to report promising results only for elementary metrics and relatively simple data distributions. In this paper, we initiate a systematic analysis of how DP generative models perform in their natural application scenarios, specifically focusing on real-world gene expression data. We conduct a comprehensive analysis of five representative DP generation methods, examining them from various angles, such as downstream utility, statistical properties, and biological plausibility. Our extensive evaluation illuminates the unique characteristics of each DP generation method, offering critical insights into the strengths and weaknesses of each approach, and uncovering intriguing possibilities for future developments. Perhaps surprisingly, our analysis reveals that most methods are capable of achieving seemingly reasonable downstream utility, according to the standard evaluation metrics considered in existing literature. Nevertheless, we find that none of the DP methods are able to accurately capture the biological characteristics of the real dataset. This observation suggests a potential over-optimistic assessment of current methodologies in this field and underscores a pressing need for future enhancements in model design.","sentences":["Generative models trained with Differential Privacy (DP) are becoming increasingly prominent in the creation of synthetic data for downstream applications.","Existing literature, however, primarily focuses on basic benchmarking datasets and tends to report promising results only for elementary metrics and relatively simple data distributions.","In this paper, we initiate a systematic analysis of how DP generative models perform in their natural application scenarios, specifically focusing on real-world gene expression data.","We conduct a comprehensive analysis of five representative DP generation methods, examining them from various angles, such as downstream utility, statistical properties, and biological plausibility.","Our extensive evaluation illuminates the unique characteristics of each DP generation method, offering critical insights into the strengths and weaknesses of each approach, and uncovering intriguing possibilities for future developments.","Perhaps surprisingly, our analysis reveals that most methods are capable of achieving seemingly reasonable downstream utility, according to the standard evaluation metrics considered in existing literature.","Nevertheless, we find that none of the DP methods are able to accurately capture the biological characteristics of the real dataset.","This observation suggests a potential over-optimistic assessment of current methodologies in this field and underscores a pressing need for future enhancements in model design."],"url":"http://arxiv.org/abs/2402.04912v1","category":"cs.CR"}
{"created":"2024-02-07 14:38:51","title":"Entanglement Definitions for Tethered Robots: Exploration and Analysis","abstract":"In this article we consider the problem of tether entanglement for tethered robots. In many applications, such as maintenance of underwater structures, aerial inspection, and underground exploration, tethered robots are often used in place of standalone (i.e., untethered) ones. However, the presence of a tether also introduces the risk for it to get entangled with obstacles present in the environment or with itself. To avoid these situations, a non-entanglement constraint can be considered in the motion planning problem for tethered robots. This constraint can be expressed either as a set of specific tether configurations that must be avoided, or as a quantitative measure of a `level of entanglement' that can be minimized. However, the literature lacks a generally accepted definition of entanglement, with existing definitions being limited and partial. Namely, the existing entanglement definitions either require a taut tether to come into contact with an obstacle or with another tether, or they require for the tether to do a full loop around an obstacle. In practice, this means that the existing definitions do not effectively cover all instances of tether entanglement. Our goal in this article is to bridge this gap and provide new definitions of entanglement, which, together with the existing ones, can be effectively used to qualify the entanglement state of a tethered robot in diverse situations. The new definitions find application mainly in motion planning for tethered robot systems, where they can be used to obtain more safe and robust entanglement-free trajectories. The present article focuses exclusively on the presentation and analysis of the entanglement definitions. The application of the definitions to the motion planning problem is left for future work.","sentences":["In this article we consider the problem of tether entanglement for tethered robots.","In many applications, such as maintenance of underwater structures, aerial inspection, and underground exploration, tethered robots are often used in place of standalone (i.e., untethered) ones.","However, the presence of a tether also introduces the risk for it to get entangled with obstacles present in the environment or with itself.","To avoid these situations, a non-entanglement constraint can be considered in the motion planning problem for tethered robots.","This constraint can be expressed either as a set of specific tether configurations that must be avoided, or as a quantitative measure of a `level of entanglement' that can be minimized.","However, the literature lacks a generally accepted definition of entanglement, with existing definitions being limited and partial.","Namely, the existing entanglement definitions either require a taut tether to come into contact with an obstacle or with another tether, or they require for the tether to do a full loop around an obstacle.","In practice, this means that the existing definitions do not effectively cover all instances of tether entanglement.","Our goal in this article is to bridge this gap and provide new definitions of entanglement, which, together with the existing ones, can be effectively used to qualify the entanglement state of a tethered robot in diverse situations.","The new definitions find application mainly in motion planning for tethered robot systems, where they can be used to obtain more safe and robust entanglement-free trajectories.","The present article focuses exclusively on the presentation and analysis of the entanglement definitions.","The application of the definitions to the motion planning problem is left for future work."],"url":"http://arxiv.org/abs/2402.04909v1","category":"cs.RO"}
{"created":"2024-02-07 14:37:37","title":"On a Combinatorial Problem Arising in Machine Teaching","abstract":"We study a model of machine teaching where the teacher mapping is constructed from a size function on both concepts and examples. The main question in machine teaching is the minimum number of examples needed for any concept, the so-called teaching dimension. A recent paper [7] conjectured that the worst case for this model, as a function of the size of the concept class, occurs when the consistency matrix contains the binary representations of numbers from zero and up. In this paper we prove their conjecture. The result can be seen as a generalization of a theorem resolving the edge isoperimetry problem for hypercubes [12], and our proof is based on a lemma of [10].","sentences":["We study a model of machine teaching where the teacher mapping is constructed from a size function on both concepts and examples.","The main question in machine teaching is the minimum number of examples needed for any concept, the so-called teaching dimension.","A recent paper [7] conjectured that the worst case for this model, as a function of the size of the concept class, occurs when the consistency matrix contains the binary representations of numbers from zero and up.","In this paper we prove their conjecture.","The result can be seen as a generalization of a theorem resolving the edge isoperimetry problem for hypercubes [12], and our proof is based on a lemma of [10]."],"url":"http://arxiv.org/abs/2402.04907v1","category":"math.CO"}
{"created":"2024-02-07 14:35:19","title":"How much strangeness is needed for the axial-vector form factor of the nucleon","abstract":"We consider the axial-vector together with its induced pseudo-scalar form factor of the nucleon as computed from the chiral Lagrangian with nucleon and isobar degrees of freedom. The form factors are evaluated at the one-loop level, where particular emphasis is put on the use of on-shell masses in the loop expressions. Our results are presented in terms of a novel set of basis functions that generalize the Passarino--Veltman scheme to the case where power-counting violating structures are to be subtracted. The particularly important role of the isobar degrees of freedom is emphasized. We obtain a significant and simultaneous fit to the available Lattice QCD results based on flavour SU(2) ensembles for the baryon masses and form factors up to pion masses of about 500 MeV. Our fits includes sizeable finite volume effects that are implied by using in-box values for the hadron masses entering our one-loop expressions. We conclude that from flavour SU(2) ensembles it appears not possible to predict the empirical formfactor at the desired precision. Effects from strange quarks are expected to remedy the situation.","sentences":["We consider the axial-vector together with its induced pseudo-scalar form factor of the nucleon as computed from the chiral Lagrangian with nucleon and isobar degrees of freedom.","The form factors are evaluated at the one-loop level, where particular emphasis is put on the use of on-shell masses in the loop expressions.","Our results are presented in terms of a novel set of basis functions that generalize the Passarino--Veltman scheme to the case where power-counting violating structures are to be subtracted.","The particularly important role of the isobar degrees of freedom is emphasized.","We obtain a significant and simultaneous fit to the available Lattice QCD results based on flavour SU(2) ensembles for the baryon masses and form factors up to pion masses of about 500 MeV. Our fits includes sizeable finite volume effects that are implied by using in-box values for the hadron masses entering our one-loop expressions.","We conclude that from flavour SU(2) ensembles it appears not possible to predict the empirical formfactor at the desired precision.","Effects from strange quarks are expected to remedy the situation."],"url":"http://arxiv.org/abs/2402.04905v1","category":"hep-ph"}
{"created":"2024-02-07 14:35:05","title":"L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ","abstract":"Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size for LLMs, aiming to enhance generality. The simultaneous quantization-and-fine-tuning process of L4Q is applicable to high-precision models, yielding linearly quantized weights with superior accuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families using an instructional dataset, showcase L4Q's capabilities in language comprehension and few-shot in-context learning, achieving sub-4-bit precision while maintaining comparable training times to applying PEFT on a quantized model.","sentences":["Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs).","In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy.","Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques.","However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration.","Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance.","To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training.","L4Q leverages LoRA-wise learned quantization step size for LLMs, aiming to enhance generality.","The simultaneous quantization-and-fine-tuning process of L4Q is applicable to high-precision models, yielding linearly quantized weights with superior accuracy.","Our experiments, conducted on the LLaMA and LLaMA2 model families using an instructional dataset, showcase L4Q's capabilities in language comprehension and few-shot in-context learning, achieving sub-4-bit precision while maintaining comparable training times to applying PEFT on a quantized model."],"url":"http://arxiv.org/abs/2402.04902v1","category":"cs.LG"}
{"created":"2024-02-07 14:28:25","title":"Discrete-time staged progression epidemic models","abstract":"In the Staged Progression (SP) epidemic models, infected individuals are classified into a suitable number of states. The goal of these models is to describe as closely as possible the effect of differences in infectiousness exhibited by individuals going through the different stages. The main objective of this work is to study, from the methodological point of view, the behavior of solutions of the discrete time SP models without reinfection and with a general incidence function. Besides calculating $\\mathcal{R}_{0}$, we find bounds for the epidemic final size, characterize the asymptotic behavior of the infected classes, give results about the final monotonicity of the infected classes, and obtain results regarding the initial dynamics of the prevalence of the disease. Moreover, we incorporate into the model the probability distribution of the number of contacts in order to make the model amenable to study its effect in the dynamics of the disease.","sentences":["In the Staged Progression (SP) epidemic models, infected individuals are classified into a suitable number of states.","The goal of these models is to describe as closely as possible the effect of differences in infectiousness exhibited by individuals going through the different stages.","The main objective of this work is to study, from the methodological point of view, the behavior of solutions of the discrete time SP models without reinfection and with a general incidence function.","Besides calculating $\\mathcal{R}_{0}$, we find bounds for the epidemic final size, characterize the asymptotic behavior of the infected classes, give results about the final monotonicity of the infected classes, and obtain results regarding the initial dynamics of the prevalence of the disease.","Moreover, we incorporate into the model the probability distribution of the number of contacts in order to make the model amenable to study its effect in the dynamics of the disease."],"url":"http://arxiv.org/abs/2402.04899v1","category":"math.DS"}
{"created":"2024-02-07 14:28:04","title":"The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer","abstract":"In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.","sentences":["In this paper, we present a novel sequential team selection model in soccer.","Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data.","Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability.","We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season.","Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams."],"url":"http://arxiv.org/abs/2402.04898v1","category":"cs.AI"}
{"created":"2024-02-07 14:24:04","title":"A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration","abstract":"The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.","sentences":["The probabilistic formal verification (PFV) of AI systems is in its infancy.","So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   ","We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   ","Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   ","We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework."],"url":"http://arxiv.org/abs/2402.04892v1","category":"cs.AI"}
{"created":"2024-02-07 14:22:51","title":"Detecting Generated Native Ads in Conversational Search","abstract":"Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fine-tuned sentence transformers and state-of-the-art LLMs on the task of recognizing the ads. In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task.","sentences":["Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries.","It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results.","This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising.","It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models.","This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them.","For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fine-tuned sentence transformers and state-of-the-art LLMs on the task of recognizing the ads.","In our experiments sentence transformers achieve detection precision and recall values above 0.9, while the investigated LLMs struggle with the task."],"url":"http://arxiv.org/abs/2402.04889v1","category":"cs.IR"}
{"created":"2024-02-07 14:22:17","title":"Topological relations in water quality monitoring","abstract":"The Alqueva Multi-Purpose Project (EFMA) is a massive abduction and storage infrastructure system in the Alentejo, which has a water quality monitoring network with almost thousands of water quality stations distributed across three subsystems: Alqueva, Pedrog\\~ao, and Ardila. Identification of pollution sources in complex infrastructure systems, such as the EFMA, requires recognition of water flow direction and delimitation of areas being drained to specific sampling points. The transfer channels in the EFMA infrastructure artificially connect several water bodies that do not share drainage basins, which further complicates the interpretation of water quality data because the water does not flow exclusively downstream and is not restricted to specific basins.   The existing user-friendly GIS tools do not facilitate the exploration and visualisation of water quality data in spatial-temporal dimensions, such as defining temporal relationships between monitoring campaigns, nor do they allow the establishment of topological and hydrological relationships between different sampling points.   This thesis work proposes a framework capable of aggregating many types of information in a GIS environment, visualising large water quality-related datasets and, a graph data model to integrate and relate water quality between monitoring stations and land use. The graph model allows to exploit the relationship between water quality in a watercourse and reservoirs associated with infrastructures.   The graph data model and the developed framework demonstrated encouraging results and has proven to be preferred when compared to relational databases.","sentences":["The Alqueva Multi-Purpose Project (EFMA) is a massive abduction and storage infrastructure system in the Alentejo, which has a water quality monitoring network with almost thousands of water quality stations distributed across three subsystems: Alqueva, Pedrog\\~ao, and Ardila.","Identification of pollution sources in complex infrastructure systems, such as the EFMA, requires recognition of water flow direction and delimitation of areas being drained to specific sampling points.","The transfer channels in the EFMA infrastructure artificially connect several water bodies that do not share drainage basins, which further complicates the interpretation of water quality data because the water does not flow exclusively downstream and is not restricted to specific basins.   ","The existing user-friendly GIS tools do not facilitate the exploration and visualisation of water quality data in spatial-temporal dimensions, such as defining temporal relationships between monitoring campaigns, nor do they allow the establishment of topological and hydrological relationships between different sampling points.   ","This thesis work proposes a framework capable of aggregating many types of information in a GIS environment, visualising large water quality-related datasets and, a graph data model to integrate and relate water quality between monitoring stations and land use.","The graph model allows to exploit the relationship between water quality in a watercourse and reservoirs associated with infrastructures.   ","The graph data model and the developed framework demonstrated encouraging results and has proven to be preferred when compared to relational databases."],"url":"http://arxiv.org/abs/2402.04884v1","category":"cs.DB"}
{"created":"2024-02-07 14:21:26","title":"Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration","abstract":"Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization. Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements.","sentences":["Recent camera-based 3D object detection is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of object localization within the 3D space.","This paper aims to address such a fundamental problem of camera-based 3D object detection: How to effectively learn depth information for accurate feature lifting and object localization.","Different from previous methods which directly predict depth distributions by using a supervised estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms.","First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces.","Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D object localization perturbation along the depth axis.","In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection Transformer through a depth denoising mechanism in the training phase.","The entire model training is accomplished through an end-to-end manner.","We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP.","Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements."],"url":"http://arxiv.org/abs/2402.04883v1","category":"cs.CV"}
{"created":"2024-02-07 14:18:19","title":"STAR: Shape-focused Texture Agnostic Representations for Improved Object Detection and 6D Pose Estimation","abstract":"Recent advances in machine learning have greatly benefited object detection and 6D pose estimation for robotic grasping. However, textureless and metallic objects still pose a significant challenge due to fewer visual cues and the texture bias of CNNs. To address this issue, we propose a texture-agnostic approach that focuses on learning from CAD models and emphasizes object shape features. To achieve a focus on learning shape features, the textures are randomized during the rendering of the training data. By treating the texture as noise, the need for real-world object instances or their final appearance during training data generation is eliminated. The TLESS and ITODD datasets, specifically created for industrial settings in robotics and featuring textureless and metallic objects, were used for evaluation. Texture agnosticity also increases the robustness against image perturbations such as imaging noise, motion blur, and brightness changes, which are common in robotics applications. Code and datasets are publicly available at github.com/hoenigpeter/randomized_texturing.","sentences":["Recent advances in machine learning have greatly benefited object detection and 6D pose estimation for robotic grasping.","However, textureless and metallic objects still pose a significant challenge due to fewer visual cues and the texture bias of CNNs.","To address this issue, we propose a texture-agnostic approach that focuses on learning from CAD models and emphasizes object shape features.","To achieve a focus on learning shape features, the textures are randomized during the rendering of the training data.","By treating the texture as noise, the need for real-world object instances or their final appearance during training data generation is eliminated.","The TLESS and ITODD datasets, specifically created for industrial settings in robotics and featuring textureless and metallic objects, were used for evaluation.","Texture agnosticity also increases the robustness against image perturbations such as imaging noise, motion blur, and brightness changes, which are common in robotics applications.","Code and datasets are publicly available at github.com/hoenigpeter/randomized_texturing."],"url":"http://arxiv.org/abs/2402.04878v1","category":"cs.CV"}
{"created":"2024-02-07 14:16:28","title":"On Provable Length and Compositional Generalization","abstract":"Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.","sentences":["Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models.","In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets.","Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization."],"url":"http://arxiv.org/abs/2402.04875v1","category":"cs.LG"}
{"created":"2024-02-07 14:09:42","title":"Nonlinear Stability of Planar Shock Waves for the 3-D Boltzmann Equation","abstract":"This paper studies the stability and large-time behavior of the three-dimensional (3-D) Boltzmann equation near shock profiles. We prove the nonlinear stability of the composite wave consisting of two shock profiles under general perturbations without the assumption of integral zero of macroscopic quantities. To address the challenge caused by the compressibility of shock profiles, we apply the method of anti-derivative based on macro-micro decomposition. However, the system of anti-derivatives presents certain difficulties. Firstly, general perturbations may generate diffusion waves that evolve and interact with shock profiles, resulting in errors that are not controllable. We therefore introduce a set of coupled diffusion waves to cancel out these poor errors and perform careful estimates on wave interactions. Secondly, we perform diagonalized system estimates to fully exploit the compressibility of shock profiles and control terms that decay slowly. Thirdly, the presence of diffusion waves causes critical terms with decay $(1+t)^{-1}$, and we introduce a Poincar\\'e type of inequality to address these terms. Finally, estimates on anti-derivatives can only control terms along the propagation direction, while for transversal directions, we use the entropy-entropy flux pair as well as the Poincar\\'e inequality to control the lower order terms using diffusion terms. As a result, we obtain nonlinear stability through the energy method, which is the first stability result for the planar shock of the multi-dimensional Boltzmann equation to the best of our knowledge.","sentences":["This paper studies the stability and large-time behavior of the three-dimensional (3-D) Boltzmann equation near shock profiles.","We prove the nonlinear stability of the composite wave consisting of two shock profiles under general perturbations without the assumption of integral zero of macroscopic quantities.","To address the challenge caused by the compressibility of shock profiles, we apply the method of anti-derivative based on macro-micro decomposition.","However, the system of anti-derivatives presents certain difficulties.","Firstly, general perturbations may generate diffusion waves that evolve and interact with shock profiles, resulting in errors that are not controllable.","We therefore introduce a set of coupled diffusion waves to cancel out these poor errors and perform careful estimates on wave interactions.","Secondly, we perform diagonalized system estimates to fully exploit the compressibility of shock profiles and control terms that decay slowly.","Thirdly, the presence of diffusion waves causes critical terms with decay $(1+t)^{-1}$, and we introduce a Poincar\\'e type of inequality to address these terms.","Finally, estimates on anti-derivatives can only control terms along the propagation direction, while for transversal directions, we use the entropy-entropy flux pair as well as the Poincar\\'e inequality to control the lower order terms using diffusion terms.","As a result, we obtain nonlinear stability through the energy method, which is the first stability result for the planar shock of the multi-dimensional Boltzmann equation to the best of our knowledge."],"url":"http://arxiv.org/abs/2402.04871v1","category":"math.AP"}
{"created":"2024-02-07 14:09:34","title":"Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy","abstract":"As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that allow direct intervention in the state space, we design the root cause localization task in our simulated fault alarm environment and then empirically show the effectiveness and robustness of the proposed method against state-of-the-art baselines. Theoretical analysis shows that our performance improvement attributes to the virtuous cycle of causal-guided policy learning and causal structure learning, which aligns with our experimental results.","sentences":["As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space.","However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL.","In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy.","We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment.","To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation.","Due to the lack of public benchmarks that allow direct intervention in the state space, we design the root cause localization task in our simulated fault alarm environment and then empirically show the effectiveness and robustness of the proposed method against state-of-the-art baselines.","Theoretical analysis shows that our performance improvement attributes to the virtuous cycle of causal-guided policy learning and causal structure learning, which aligns with our experimental results."],"url":"http://arxiv.org/abs/2402.04869v1","category":"cs.LG"}
{"created":"2024-02-07 14:08:38","title":"Perceptually Equivalent Resolution in Handheld Devices for Streaming Bandwidth Saving","abstract":"We present the description, results, and analysis of the experiments conducted to find the equivalent resolution associated with handheld devices. That is, the resolution from which users stop perceiving quality improvements if better resolutions are presented to them in such devices. Thus, it is the maximum resolution that it is worth considering for generating and delivering video, as long as sequences are not too intensively compressed. Therefore, the detection of the equivalent resolutions allows for notable savings in bandwidth consumption. Subjective assessments have been carried out on fifty subjects using a set of video sequences of very different nature and four handheld devices with a broad range of screen dimensions. The results prove that the equivalent resolution in current handheld devices is 720p as higher resolutions are not valued by users.","sentences":["We present the description, results, and analysis of the experiments conducted to find the equivalent resolution associated with handheld devices.","That is, the resolution from which users stop perceiving quality improvements if better resolutions are presented to them in such devices.","Thus, it is the maximum resolution that it is worth considering for generating and delivering video, as long as sequences are not too intensively compressed.","Therefore, the detection of the equivalent resolutions allows for notable savings in bandwidth consumption.","Subjective assessments have been carried out on fifty subjects using a set of video sequences of very different nature and four handheld devices with a broad range of screen dimensions.","The results prove that the equivalent resolution in current handheld devices is 720p as higher resolutions are not valued by users."],"url":"http://arxiv.org/abs/2402.04868v1","category":"eess.IV"}
{"created":"2024-02-07 14:07:47","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback","abstract":"In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval.","sentences":["In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users.","Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries.","However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images.","In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results.","We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process.","Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach.","Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement.","Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval."],"url":"http://arxiv.org/abs/2402.04867v1","category":"cs.IR"}
{"created":"2024-02-07 13:59:47","title":"Equivariant Neural Network Force Fields for Magnetic Materials","abstract":"Neural network force fields have significantly advanced ab initio atomistic simulations across diverse fields. However, their application in the realm of magnetic materials is still in its early stage due to challenges posed by the subtle magnetic energy landscape and the difficulty of obtaining training data. Here we introduce a data-efficient neural network architecture to represent density functional theory total energy, atomic forces, and magnetic forces as functions of atomic and magnetic structures. Our approach incorporates the principle of equivariance under the three-dimensional Euclidean group into the neural network model. Through systematic experiments on various systems, including monolayer magnets, curved nanotube magnets, and moir\\'e-twisted bilayer magnets of $\\text{CrI}_{3}$, we showcase the method's high efficiency and accuracy, as well as exceptional generalization ability. The work creates opportunities for exploring magnetic phenomena in large-scale materials systems.","sentences":["Neural network force fields have significantly advanced ab initio atomistic simulations across diverse fields.","However, their application in the realm of magnetic materials is still in its early stage due to challenges posed by the subtle magnetic energy landscape and the difficulty of obtaining training data.","Here we introduce a data-efficient neural network architecture to represent density functional theory total energy, atomic forces, and magnetic forces as functions of atomic and magnetic structures.","Our approach incorporates the principle of equivariance under the three-dimensional Euclidean group into the neural network model.","Through systematic experiments on various systems, including monolayer magnets, curved nanotube magnets, and moir\\'e-twisted bilayer magnets of $\\text{CrI}_{3}$, we showcase the method's high efficiency and accuracy, as well as exceptional generalization ability.","The work creates opportunities for exploring magnetic phenomena in large-scale materials systems."],"url":"http://arxiv.org/abs/2402.04864v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 13:58:26","title":"Automated Smart Contract Summarization via LLMs","abstract":"Automatic code Summarization generation technology is widely used in the development and maintenance of smart contracts. In recent years, with the advent of Large Language Models (LLMs), Gemini has received a lot of attention as the first Large Multimodal models (LMMs) to support multimodal input. However, it is unclear how LMMs can generate contract code summarization from multimodal inputs. In this paper, we focus on evaluating Gemini on real-world smart contracts, comparing it to the MMTrans, and exploring how to combine multimodal prompts to generate a contract code summarization. We used several widely used metrics (BLEU, METEOR, and ROUGE-L) to measure the quality of the generated summarization. Our experiments show that METEOR and ROUGE-L metrics, Gemini-Pro-Vision achieves 21.17% and 21.05% scores for code comments generated by three-shot prompts. These scores are better than those generated by one-shot and five-shot prompts.","sentences":["Automatic code Summarization generation technology is widely used in the development and maintenance of smart contracts.","In recent years, with the advent of Large Language Models (LLMs), Gemini has received a lot of attention as the first Large Multimodal models (LMMs) to support multimodal input.","However, it is unclear how LMMs can generate contract code summarization from multimodal inputs.","In this paper, we focus on evaluating Gemini on real-world smart contracts, comparing it to the MMTrans, and exploring how to combine multimodal prompts to generate a contract code summarization.","We used several widely used metrics (BLEU, METEOR, and ROUGE-L) to measure the quality of the generated summarization.","Our experiments show that METEOR and ROUGE-L metrics, Gemini-Pro-Vision achieves 21.17% and 21.05% scores for code comments generated by three-shot prompts.","These scores are better than those generated by one-shot and five-shot prompts."],"url":"http://arxiv.org/abs/2402.04863v1","category":"cs.SE"}
{"created":"2024-02-07 13:55:27","title":"CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay","abstract":"Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines.","sentences":["Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability.","However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC).","In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt).","Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay.","By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis.","Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization.","CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset.","Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines."],"url":"http://arxiv.org/abs/2402.04858v1","category":"cs.AI"}
{"created":"2024-02-07 13:54:38","title":"Explaining Learned Reward Functions with Counterfactual Trajectories","abstract":"Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajectories and generalises to out-of-distribution examples. Although CTEs do not lead to a perfect understanding of the reward, our method, and more generally the adaptation of XAI methods, are presented as a fruitful approach for interpreting learned reward functions.","sentences":["Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions.","Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions.","We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive.","We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria.","Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs.","CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories.","Further, it learns to accurately judge differences in rewards between trajectories and generalises to out-of-distribution examples.","Although CTEs do not lead to a perfect understanding of the reward, our method, and more generally the adaptation of XAI methods, are presented as a fruitful approach for interpreting learned reward functions."],"url":"http://arxiv.org/abs/2402.04856v1","category":"cs.AI"}
{"created":"2024-02-07 13:54:15","title":"Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction","abstract":"Transformers have recently emerged as a significant force in the field of image deraining. Existing image deraining methods utilize extensive research on self-attention. Though showcasing impressive results, they tend to neglect critical frequency information, as self-attention is generally less adept at capturing high-frequency details. To overcome this shortcoming, we have developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that integrates information from both spatial and frequency domains through Spatial Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block (FFEBlock). We have further introduced an effective Adaptive Fusion Module (AFM) for the dual-path feature aggregation. Extensive experiments on six public deraining benchmarks and downstream vision tasks have demonstrated that our proposed method not only outperforms the existing state-of-the-art deraining method but also achieves visually pleasuring results with excellent robustness on downstream vision tasks.","sentences":["Transformers have recently emerged as a significant force in the field of image deraining.","Existing image deraining methods utilize extensive research on self-attention.","Though showcasing impressive results, they tend to neglect critical frequency information, as self-attention is generally less adept at capturing high-frequency details.","To overcome this shortcoming, we have developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that integrates information from both spatial and frequency domains through Spatial Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block (FFEBlock).","We have further introduced an effective Adaptive Fusion Module (AFM) for the dual-path feature aggregation.","Extensive experiments on six public deraining benchmarks and downstream vision tasks have demonstrated that our proposed method not only outperforms the existing state-of-the-art deraining method but also achieves visually pleasuring results with excellent robustness on downstream vision tasks."],"url":"http://arxiv.org/abs/2402.04855v1","category":"cs.CV"}
{"created":"2024-02-07 13:54:06","title":"Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey","abstract":"Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between \"Issue resolved\" and \"Issue finding\" that they hope to obtain. To address these issues, this study aims to support research insight surveys for beginner researchers by establishing a hierarchical tree-structured knowledge graph that reflects the inheritance insight of research topics and the relevance insight among the academic papers.","sentences":["Research surveys have always posed a challenge for beginner researchers who lack of research training.","These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time.","One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers.","However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly.","Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended.","They may lack of grasp important information about the insight connection between \"Issue resolved\" and \"Issue finding\" that they hope to obtain.","To address these issues, this study aims to support research insight surveys for beginner researchers by establishing a hierarchical tree-structured knowledge graph that reflects the inheritance insight of research topics and the relevance insight among the academic papers."],"url":"http://arxiv.org/abs/2402.04854v1","category":"cs.DL"}
{"created":"2024-02-07 13:48:51","title":"Grand zigzag knight's paths","abstract":"We study the enumeration of different classes of grand knight's paths in the plane. In particular, we focus on the subsets of zigzag knight's paths subject to constraints. These constraints include ending at ordinate 0, bounded by a horizontal line, confined within a tube, among other considerations. We present our results using generating functions or direct closed-form expressions. We derive asymptotic results, finding approximations for quantities such as the probability that a zigzag knight's path stays in some area of the plane, or for the average of the final height of such a path. Additionally, we exhibit some bijections between grand zigzag knight's paths and some pairs of compositions.","sentences":["We study the enumeration of different classes of grand knight's paths in the plane.","In particular, we focus on the subsets of zigzag knight's paths subject to constraints.","These constraints include ending at ordinate 0, bounded by a horizontal line, confined within a tube, among other considerations.","We present our results using generating functions or direct closed-form expressions.","We derive asymptotic results, finding approximations for quantities such as the probability that a zigzag knight's path stays in some area of the plane, or for the average of the final height of such a path.","Additionally, we exhibit some bijections between grand zigzag knight's paths and some pairs of compositions."],"url":"http://arxiv.org/abs/2402.04851v1","category":"math.CO"}
{"created":"2024-02-07 13:48:40","title":"Muon $g-2$ and Proton Lifetime in SUSY SU(5) GUTs with Split Superpartners","abstract":"We consider the interplay of the muon $g-2$ anomaly and the proton decay in the SUSY SU(5) GUTs with generation-independent scalar soft masses. In these scenarios, we introduce a number of $\\bf 5+{\\bar 5}$ messenger fields with doublet-triplet splitting in general gauge mediation to transmit SUSY breaking to the visible sector by gauge loops. As a result, squarks and sleptons receive generation-independent soft SUSY breaking masses, which are split already at the messenger scale. Taking into account the perturbative unification of gauge couplings as well as the bounds from electroweak precision and vacuum stability bounds, we showed the parameter space in general gauge mediation to explain the muon $g-2$ anomaly with smuon and sneutrino loops while evading the strong bounds on squarks and gluinos from the Large Hadron Collider. We also obtained the dominant Higgsino contributions to the proton decay mode, $p\\to K^+{\\bar\\nu}$, with general generation-independent sparticle masses for squarks and sleptons. Even for split scalar soft masses in our model, however, we found that the bounds from the proton decay are satisfied only if the effective Yukawa couplings of the colored Higgsinos are suppressed further by a factor of order $10^{-4}-10^{-3}$. We illustrated how such a suppression factor is realized in orbifold GUTs in the extra dimension where the colored Higgsinos in the bulk are not coupled to the matter fields localized at the orbifold fixed points at the leading order.","sentences":["We consider the interplay of the muon $g-2$ anomaly and the proton decay in the SUSY SU(5) GUTs with generation-independent scalar soft masses.","In these scenarios, we introduce a number of $\\bf 5+{\\bar 5}$ messenger fields with doublet-triplet splitting in general gauge mediation to transmit SUSY breaking to the visible sector by gauge loops.","As a result, squarks and sleptons receive generation-independent soft SUSY breaking masses, which are split already at the messenger scale.","Taking into account the perturbative unification of gauge couplings as well as the bounds from electroweak precision and vacuum stability bounds, we showed the parameter space in general gauge mediation to explain the muon $g-2$ anomaly with smuon and sneutrino loops while evading the strong bounds on squarks and gluinos from the Large Hadron Collider.","We also obtained the dominant Higgsino contributions to the proton decay mode, $p\\to K^+{\\bar\\nu}$, with general generation-independent sparticle masses for squarks and sleptons.","Even for split scalar soft masses in our model, however, we found that the bounds from the proton decay are satisfied only if the effective Yukawa couplings of the colored Higgsinos are suppressed further by a factor of order $10^{-4}-10^{-3}$. We illustrated how such a suppression factor is realized in orbifold GUTs in the extra dimension where the colored Higgsinos in the bulk are not coupled to the matter fields localized at the orbifold fixed points at the leading order."],"url":"http://arxiv.org/abs/2402.04850v1","category":"hep-ph"}
{"created":"2024-02-07 13:45:43","title":"Nonlinear behavior of area dependent interface type resistive switching devices","abstract":"Nonlinearity is a crucial characteristic for implementing hardware security primitives or neuromorphic computing systems. The main feature of all memristive devices is this nonlinear behavior observed in their current-voltage characteristics. To comprehend the nonlinear behavior, we have to understand the coexistence of resistive, capacitive, and inertia (virtual inductive) effects in these devices. These effects originate from physical and chemical processes in memristive devices. The physics-inspired compact model is employed to model and simulate interface-type RRAMs such as Au/BiFeO$_{3}$/Pt/Ti, Au/Nb$_{x}$O$_{y}$/Al$_{2}$O$_{3}$/Nb, while accounting for the modeling of capacitive and inertia effects. The proposed model's current-voltage characteristics align well with experimental data and accurately capture the non-zero crossing hysteresis generated by capacitive and inductive effects. The study examines the response of both devices to various frequencies, showing a shift in their nonlinear behavior as evidenced by a reduction in their hysteresis range. Fourier series analysis utilizing a sinusoidal input voltage of varying amplitudes and frequencies indicates harmonics or frequency components that considerably influence the functioning of RRAMs. Moreover, We propose and demonstrate using the frequency spectrum as a fingerprint for memristive devices.","sentences":["Nonlinearity is a crucial characteristic for implementing hardware security primitives or neuromorphic computing systems.","The main feature of all memristive devices is this nonlinear behavior observed in their current-voltage characteristics.","To comprehend the nonlinear behavior, we have to understand the coexistence of resistive, capacitive, and inertia (virtual inductive) effects in these devices.","These effects originate from physical and chemical processes in memristive devices.","The physics-inspired compact model is employed to model and simulate interface-type RRAMs such as Au/BiFeO$_{3}$/Pt/Ti, Au/Nb$_{x}$O$_{y}$/Al$_{2}$O$_{3}$/Nb, while accounting for the modeling of capacitive and inertia effects.","The proposed model's current-voltage characteristics align well with experimental data and accurately capture the non-zero crossing hysteresis generated by capacitive and inductive effects.","The study examines the response of both devices to various frequencies, showing a shift in their nonlinear behavior as evidenced by a reduction in their hysteresis range.","Fourier series analysis utilizing a sinusoidal input voltage of varying amplitudes and frequencies indicates harmonics or frequency components that considerably influence the functioning of RRAMs.","Moreover, We propose and demonstrate using the frequency spectrum as a fingerprint for memristive devices."],"url":"http://arxiv.org/abs/2402.04848v1","category":"cs.ET"}
{"created":"2024-02-07 13:44:47","title":"AlphaFold Meets Flow Matching for Generating Protein Ensembles","abstract":"The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow.","sentences":["The biological functions of proteins often depend on dynamic structural ensembles.","In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins.","We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow.","When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling.","When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins.","Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations.","Code is available at https://github.com/bjing2016/alphaflow."],"url":"http://arxiv.org/abs/2402.04845v1","category":"q-bio.BM"}
{"created":"2024-02-07 13:44:28","title":"Reconfigurable Intelligent Surface for Industrial Automation: mmWave Propagation Measurement, Simulation, and Control Algorithm Requirements","abstract":"Reconfigurable intelligent surfaces (RISs) enable reliable low-latency millimeter wave (mmWave) communication links in cases of a blocked line-of-sight (LoS) between the base station (BS) and the user equipment (UE), i.e. a RIS mounted on a wall or the ceiling provides a bypass for the radio communication link. We present an active RIS with 127 patch antenna elements arranged in a hexagonal grid for a center frequency of 23.8 GHz. Each RIS element uses an orthogonal polarization transformation to enable amplification using a field-effect transistor (FET). The source and drain voltages of each FET is controlled using two bits. We assume that the coordinates of the UE in an industrial control scenario are known to the RIS. We measure the received power on a 2D grid of 60 cm by 100 cm with the RIS working in reflective and active mode. The results show that the RIS can successfully focus the radio signal at the desired target points. The half-power beam width is characterized in axial and radial directions with respect to the RIS position, obtaining a practical RIS configuration update criterion for a mobile UE. These results clearly show that RISs are prominent solutions for enabling reliable wireless communication in indoor industrial scenarios.","sentences":["Reconfigurable intelligent surfaces (RISs) enable reliable low-latency millimeter wave (mmWave) communication links in cases of a blocked line-of-sight (LoS) between the base station (BS) and the user equipment (UE), i.e. a RIS mounted on a wall or the ceiling provides a bypass for the radio communication link.","We present an active RIS with 127 patch antenna elements arranged in a hexagonal grid for a center frequency of 23.8 GHz.","Each RIS element uses an orthogonal polarization transformation to enable amplification using a field-effect transistor (FET).","The source and drain voltages of each FET is controlled using two bits.","We assume that the coordinates of the UE in an industrial control scenario are known to the RIS.","We measure the received power on a 2D grid of 60 cm by 100 cm with the RIS working in reflective and active mode.","The results show that the RIS can successfully focus the radio signal at the desired target points.","The half-power beam width is characterized in axial and radial directions with respect to the RIS position, obtaining a practical RIS configuration update criterion for a mobile UE.","These results clearly show that RISs are prominent solutions for enabling reliable wireless communication in indoor industrial scenarios."],"url":"http://arxiv.org/abs/2402.04844v1","category":"eess.SY"}
{"created":"2024-02-07 13:41:53","title":"Data-efficient Large Vision Models through Sequential Autoregression","abstract":"Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding. These models are intended to not only comprehend but also seamlessly transit to out-of-domain tasks. However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens. In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset. We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase. Our empirical evaluations underscore the model's agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models. The code is available at https://github.com/ggjy/DeLVM.","sentences":["Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding.","These models are intended to not only comprehend but also seamlessly transit to out-of-domain tasks.","However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens.","In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset.","We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase.","Our empirical evaluations underscore the model's agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models.","The code is available at https://github.com/ggjy/DeLVM."],"url":"http://arxiv.org/abs/2402.04841v1","category":"cs.CV"}
{"created":"2024-02-07 13:39:38","title":"PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition","abstract":"In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.","sentences":["In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs).","The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length.","To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications.","PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency.","Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese.","Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets."],"url":"http://arxiv.org/abs/2402.04838v1","category":"cs.CL"}
{"created":"2024-02-07 13:37:47","title":"Distinguish dark matter theories with the cosmic web and next-generation surveys I: an alternative theory of gravity","abstract":"In the context of future large surveys like the Euclid mission, extracting the cosmic web from galaxies at higher redshifts with more statistical power will become feasible, particularly within the group-cluster mass regime. Therefore, it is imperative to enlarge the number of metrics that can used to constrain our cosmological models at these large scales. The number of cosmic filaments surrounding galaxies, groups and clusters, namely the connectivity, has recently emerged as a compelling probe of the large-scale structures, and has been investigated in various observational and numerical analyses. In this first paper, we examine dark matter-only cosmological simulations using the widely used DisPerSE filament finder code under two theories of gravity: the Poisson ($\\Lambda$CDM) and the Monge-Amp\\`ere models, in order to quantify how alternative models of gravity alter the properties of the cosmic skeleton. We specifically focused on this alternative gravity theory due to its propensity to enhance the formation of anisotropic structures such as filaments, but it also makes them more resistant to collapse, which consequently reduces the formation of halos. Indeed, our findings reveal that replacing the Poisson equation has a significant impact on the hierarchical formation scenario. This is evidenced by examining the redshift evolution of both the slope and the offset of the connectivity. Additionally, we demonstrated that current observations are generally in better agreement with our well-established gravity model. Finally, our study suggests that filament connectivity in the group-cluster regime could serve as a probe of our gravity model at cosmological scales. We also emphasize that our approach could be extended to alternative theories of dark matter, such as warm or fuzzy dark matter, given the extraordinary datasets provided by next-generation surveys.","sentences":["In the context of future large surveys like the Euclid mission, extracting the cosmic web from galaxies at higher redshifts with more statistical power will become feasible, particularly within the group-cluster mass regime.","Therefore, it is imperative to enlarge the number of metrics that can used to constrain our cosmological models at these large scales.","The number of cosmic filaments surrounding galaxies, groups and clusters, namely the connectivity, has recently emerged as a compelling probe of the large-scale structures, and has been investigated in various observational and numerical analyses.","In this first paper, we examine dark matter-only cosmological simulations using the widely used DisPerSE filament finder code under two theories of gravity: the Poisson ($\\Lambda$CDM) and the Monge-Amp\\`ere models, in order to quantify how alternative models of gravity alter the properties of the cosmic skeleton.","We specifically focused on this alternative gravity theory due to its propensity to enhance the formation of anisotropic structures such as filaments, but it also makes them more resistant to collapse, which consequently reduces the formation of halos.","Indeed, our findings reveal that replacing the Poisson equation has a significant impact on the hierarchical formation scenario.","This is evidenced by examining the redshift evolution of both the slope and the offset of the connectivity.","Additionally, we demonstrated that current observations are generally in better agreement with our well-established gravity model.","Finally, our study suggests that filament connectivity in the group-cluster regime could serve as a probe of our gravity model at cosmological scales.","We also emphasize that our approach could be extended to alternative theories of dark matter, such as warm or fuzzy dark matter, given the extraordinary datasets provided by next-generation surveys."],"url":"http://arxiv.org/abs/2402.04837v1","category":"astro-ph.CO"}
{"created":"2024-02-07 13:32:53","title":"On the Completeness of Invariant Geometric Deep Learning Models","abstract":"Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness of three well-established geometric models: DimeNet, GemNet and SphereNet. Our results fill the gap in the theoretical power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities. Experimentally, GeoNGNN exhibits good inductive bias in capturing local environments, and achieves competitive results w.r.t. complicated models relying on high-order invariant/equivariant representations while exhibiting significantly faster computational speed.","sentences":["Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features.","These models are characterized by their simplicity, good experimental results and computational efficiency.","However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models.","In this work, we concentrate on characterizing the theoretical expressiveness of invariant models.","We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs.","To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN.","Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness of three well-established geometric models: DimeNet, GemNet and SphereNet.","Our results fill the gap in the theoretical power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities.","Experimentally, GeoNGNN exhibits good inductive bias in capturing local environments, and achieves competitive results w.r.t.","complicated models relying on high-order invariant/equivariant representations while exhibiting significantly faster computational speed."],"url":"http://arxiv.org/abs/2402.04836v1","category":"cs.LG"}
{"created":"2024-02-07 13:32:11","title":"Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning","abstract":"There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no extra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses, thus ruling out any artificial improvement. In conclusion, our findings suggest that fine-tuning on the longest instructions should be the default baseline for any research on instruction fine-tuning.","sentences":["There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they?","LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer.","We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the OpenLLM benchmarks that test factual knowledge.","We demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k).","In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no extra preference data.","We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses, thus ruling out any artificial improvement.","In conclusion, our findings suggest that fine-tuning on the longest instructions should be the default baseline for any research on instruction fine-tuning."],"url":"http://arxiv.org/abs/2402.04833v1","category":"cs.CL"}
{"created":"2024-02-07 13:31:59","title":"Structured d-DNNF Is Not Closed Under Negation","abstract":"Both structured d-DNNF and SDD can be exponentially more succinct than OBDD. Moreover, SDD is essentially as tractable as OBDD. But this has left two important open questions. Firstly, does OBDD support more tractable transformations than structured d-DNNF? And secondly, is structured d-DNNF more succinct than SDD? In this paper, we answer both questions in the affirmative. For the first question we show that, unlike OBDD, structured d-DNNF does not support polytime negation, disjunction, or existential quantification operations. As a corollary, we deduce that there are functions with an equivalent polynomial-sized structured d-DNNF but with no such representation as an SDD, thus answering the second question. We also lift this second result to arithmetic circuits (AC) to show a succinctness gap between PSDD and the monotone AC analogue to structured d-DNNF.","sentences":["Both structured d-DNNF and SDD can be exponentially more succinct than OBDD.","Moreover, SDD is essentially as tractable as OBDD.","But this has left two important open questions.","Firstly, does OBDD support more tractable transformations than structured d-DNNF?","And secondly, is structured d-DNNF more succinct than SDD?","In this paper, we answer both questions in the affirmative.","For the first question we show that, unlike OBDD, structured d-DNNF does not support polytime negation, disjunction, or existential quantification operations.","As a corollary, we deduce that there are functions with an equivalent polynomial-sized structured d-DNNF","but with no such representation as an SDD, thus answering the second question.","We also lift this second result to arithmetic circuits (AC) to show a succinctness gap between PSDD and the monotone AC analogue to structured d-DNNF."],"url":"http://arxiv.org/abs/2402.04832v1","category":"cs.AI"}
{"created":"2024-02-07 13:26:36","title":"Novel Phase Detector Measurement Procedure Using Quasi-Synchronized RF Generator","abstract":"This paper presents a new procedure for phase detector measurements that allows the use of generators that share a 10 MHz reference oscillator but do not synchronize in phase, in other words, quasi-synchronized RF generators. The objectives are taking advantage of the benefits of using two generators but recovering lower-cost generators that have worse synchronization performance and opening the door to the possibility of using a very simple control element based in Arduino Uno and cheaper instruments. The new procedure is characterized by continuously alternating calibration and measurement sequences to make up for the phase drift of quasisynchronized generators and guarantee a maximum phase error specification (+-1 grade in this paper). Data acquisition has been divided in two stages: measurement of detector curves without phase reference (in-phase and phase-shifted) and measurement of reference data. All the data is later combined to obtain correctly referenced in-phase detector curves. The technique can be reproduced with other equivalent instrumentation. The novel procedure that allows compensation for errors (amplitude, phase shift, mismatching, etc.) is detailed, and its relation to the required measurement accuracy is amply discussed. The proposed technique is applied to characterize a phase detector based on in-phase and phase-shifted multiplication from 3 to 8 GHz with 1 GHz step. Measurements have a final maximum error of +-2 grade for both frequency and calibrated input power, according to the accuracy specifications of the VNA used to calibrate the signal distribution network, added to the +-1 grade specified in this new procedure.","sentences":["This paper presents a new procedure for phase detector measurements that allows the use of generators that share a 10 MHz reference oscillator but do not synchronize in phase, in other words, quasi-synchronized RF generators.","The objectives are taking advantage of the benefits of using two generators but recovering lower-cost generators that have worse synchronization performance and opening the door to the possibility of using a very simple control element based in Arduino Uno and cheaper instruments.","The new procedure is characterized by continuously alternating calibration and measurement sequences to make up for the phase drift of quasisynchronized generators and guarantee a maximum phase error specification (+-1 grade in this paper).","Data acquisition has been divided in two stages: measurement of detector curves without phase reference (in-phase and phase-shifted) and measurement of reference data.","All the data is later combined to obtain correctly referenced in-phase detector curves.","The technique can be reproduced with other equivalent instrumentation.","The novel procedure that allows compensation for errors (amplitude, phase shift, mismatching, etc.) is detailed, and its relation to the required measurement accuracy is amply discussed.","The proposed technique is applied to characterize a phase detector based on in-phase and phase-shifted multiplication from 3 to 8 GHz with 1 GHz step.","Measurements have a final maximum error of +-2 grade for both frequency and calibrated input power, according to the accuracy specifications of the VNA used to calibrate the signal distribution network, added to the +-1 grade specified in this new procedure."],"url":"http://arxiv.org/abs/2402.04831v1","category":"eess.SY"}
{"created":"2024-02-07 13:26:10","title":"Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming","abstract":"The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with modern machine learning techniques. Thus, we propose a novel orbital propagation paradigm, ML-dSGP4, where neural networks are integrated into the orbital propagator. Through stochastic gradient descent, this combined model's inputs, outputs, and parameters can be iteratively refined, surpassing SGP4's precision. Neural networks act as identity operators by default, adhering to SGP4's behavior. However, dSGP4's differentiability allows fine-tuning with ephemeris data, enhancing precision while maintaining computational speed. This empowers satellite operators and researchers to train the model using specific ephemeris or high-precision numerical propagation data, significantly advancing orbital prediction capabilities.","sentences":["The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably.","Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors.","This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch.","By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation.","Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times.","Furthermore, dSGP4's differentiability enables integration with modern machine learning techniques.","Thus, we propose a novel orbital propagation paradigm, ML-dSGP4, where neural networks are integrated into the orbital propagator.","Through stochastic gradient descent, this combined model's inputs, outputs, and parameters can be iteratively refined, surpassing SGP4's precision.","Neural networks act as identity operators by default, adhering to SGP4's behavior.","However, dSGP4's differentiability allows fine-tuning with ephemeris data, enhancing precision while maintaining computational speed.","This empowers satellite operators and researchers to train the model using specific ephemeris or high-precision numerical propagation data, significantly advancing orbital prediction capabilities."],"url":"http://arxiv.org/abs/2402.04830v1","category":"cs.LG"}
{"created":"2024-02-07 13:23:52","title":"The scaling limit of the volume of loop O(n) quadrangulations","abstract":"We study the volume of rigid loop-$O(n)$ quadrangulations with a boundary of length $2p$ in the critical non-generic regime. We prove that, as the half-perimeter $p$ goes to infinity, the volume scales in distribution to an explicit random variable. This limiting random variable is described in terms of the multiplicative cascades of Chen, Curien and Maillard arXiv:1702.06916, or alternatively (in the dilute case) as the law of the area of a unit-boundary $\\gamma$-quantum disc, as determined by Ang and Gwynne arXiv:1903.09120, for suitable $\\gamma$. Our arguments go through a classification of the map into several regions, where we rule out the contribution of bad regions to be left with a tractable portion of the map. One key observable for this classification is a Markov chain which explores the nested loops around a size-biased vertex pick in the map, making explicit the spinal structure of the discrete multiplicative cascade.","sentences":["We study the volume of rigid loop-$O(n)$ quadrangulations with a boundary of length $2p$ in the critical non-generic regime.","We prove that, as the half-perimeter $p$ goes to infinity, the volume scales in distribution to an explicit random variable.","This limiting random variable is described in terms of the multiplicative cascades of Chen, Curien and Maillard arXiv:1702.06916, or alternatively (in the dilute case) as the law of the area of a unit-boundary $\\gamma$-quantum disc, as determined by Ang and Gwynne arXiv:1903.09120, for suitable $\\gamma$. Our arguments go through a classification of the map into several regions, where we rule out the contribution of bad regions to be left with a tractable portion of the map.","One key observable for this classification is a Markov chain which explores the nested loops around a size-biased vertex pick in the map, making explicit the spinal structure of the discrete multiplicative cascade."],"url":"http://arxiv.org/abs/2402.04827v1","category":"math.PR"}
{"created":"2024-02-07 13:23:25","title":"Fast Timing-Conditioned Latent Audio Diffusion","abstract":"Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.","sentences":["Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding.","Further, most previous works do not tackle that music and sound effects naturally vary in their duration.","Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model.","Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder.","It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds.","Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU.","Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds."],"url":"http://arxiv.org/abs/2402.04825v1","category":"cs.SD"}
{"created":"2024-02-07 13:22:17","title":"Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game","abstract":"Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem \"due to the essentially unconstrained nature of what other agents may do\". In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategies that are less verbose (staying silent in some of the steps) and that with respect to that the Guide's strategies indeed adapt to the partner's level of confidence and autonomy.","sentences":["Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem \"due to the essentially unconstrained nature of what other agents may do\".","In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game.","In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors.","We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy.","We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort.","Our results indicate that this novel ingredient leads to communicative strategies that are less verbose (staying silent in some of the steps) and that with respect to that the Guide's strategies indeed adapt to the partner's level of confidence and autonomy."],"url":"http://arxiv.org/abs/2402.04824v1","category":"cs.CL"}
{"created":"2024-02-07 13:22:05","title":"How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data","abstract":"Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them. However, to generate realistic synthetic data, it is often not enough to have a good approximation of their distribution, as it also requires compliance with constraints that encode essential background knowledge on the problem at hand. In this paper, we address this limitation and show how DGMs for tabular data can be transformed into Constrained Deep Generative Models (C-DGMs), whose generated samples are guaranteed to be compliant with the given constraints. This is achieved by automatically parsing the constraints and transforming them into a Constraint Layer (CL) seamlessly integrated with the DGM. Our extensive experimental analysis with various DGMs and tasks reveals that standard DGMs often violate constraints, some exceeding $95\\%$ non-compliance, while their corresponding C-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at training time, C-DGMs are able to exploit the background knowledge expressed by the constraints to outperform their standard counterparts with up to $6.5\\%$ improvement in utility and detection. Further, we show how our CL does not necessarily need to be integrated at training time, as it can be also used as a guardrail at inference time, still producing some improvements in the overall performance of the models. Finally, we show that our CL does not hinder the sample generation time of the models.","sentences":["Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them.","However, to generate realistic synthetic data, it is often not enough to have a good approximation of their distribution, as it also requires compliance with constraints that encode essential background knowledge on the problem at hand.","In this paper, we address this limitation and show how DGMs for tabular data can be transformed into Constrained Deep Generative Models (C-DGMs), whose generated samples are guaranteed to be compliant with the given constraints.","This is achieved by automatically parsing the constraints and transforming them into a Constraint Layer (CL) seamlessly integrated with the DGM.","Our extensive experimental analysis with various DGMs and tasks reveals that standard DGMs often violate constraints, some exceeding $95\\%$ non-compliance, while their corresponding C-DGMs are never non-compliant.","Then, we quantitatively demonstrate that, at training time, C-DGMs are able to exploit the background knowledge expressed by the constraints to outperform their standard counterparts with up to $6.5\\%$ improvement in utility and detection.","Further, we show how our CL does not necessarily need to be integrated at training time, as it can be also used as a guardrail at inference time, still producing some improvements in the overall performance of the models.","Finally, we show that our CL does not hinder the sample generation time of the models."],"url":"http://arxiv.org/abs/2402.04823v1","category":"cs.LG"}
{"created":"2024-02-07 13:21:57","title":"Imaging a large coronal loop using type U solar radio burst interferometry","abstract":"Solar radio U-bursts are generated by electron beams traveling along closed magnetic loops in the solar corona. Low-frequency ($<$ 100 MHz) U-bursts serve as powerful diagnostic tools for studying large-sized coronal loops that extend into the middle corona. However, the positive frequency drift component (descending leg) of U-bursts has received less attention in previous studies, as the descending radio flux is weak. In this study, we utilized LOFAR interferometric solar imaging data from a U-burst that has a significant descending leg component, observed between 10 to 90 MHz on June 5th, 2020. By analyzing the radio source centroid positions, we determined the beam velocities and physical parameters of a large coronal magnetic loop that reached just about 1.3 $\\rm{R_{\\odot}}$ in altitude. At this altitude, we found the plasma temperature to be around 1.1 MK, the plasma pressure around 0.20 $\\rm{mdyn,cm^{-2}}$, and the minimum magnetic field strength around 0.07 G. The similarity in physical properties determined from the image suggests a symmetric loop. The average electron beam velocity on the ascending leg was found to be 0.21 c, while it was 0.14 c on the descending leg. This apparent deceleration is attributed to a decrease in the range of electron energies that resonate with Langmuir waves, likely due to the positive background plasma density gradient along the downward loop leg.","sentences":["Solar radio U-bursts are generated by electron beams traveling along closed magnetic loops in the solar corona.","Low-frequency ($<$ 100 MHz)","U-bursts serve as powerful diagnostic tools for studying large-sized coronal loops that extend into the middle corona.","However, the positive frequency drift component (descending leg) of U-bursts has received less attention in previous studies, as the descending radio flux is weak.","In this study, we utilized LOFAR interferometric solar imaging data from a U-burst that has a significant descending leg component, observed between 10 to 90 MHz on June 5th, 2020.","By analyzing the radio source centroid positions, we determined the beam velocities and physical parameters of a large coronal magnetic loop that reached just about 1.3 $\\rm{R_{\\odot}}$ in altitude.","At this altitude, we found the plasma temperature to be around 1.1 MK, the plasma pressure around 0.20","$\\rm{mdyn,cm^{-2}}$, and the minimum magnetic field strength around 0.07 G.","The similarity in physical properties determined from the image suggests a symmetric loop.","The average electron beam velocity on the ascending leg was found to be 0.21 c, while it was 0.14 c on the descending leg.","This apparent deceleration is attributed to a decrease in the range of electron energies that resonate with Langmuir waves, likely due to the positive background plasma density gradient along the downward loop leg."],"url":"http://arxiv.org/abs/2402.04822v1","category":"astro-ph.SR"}
{"created":"2024-02-07 13:19:28","title":"Detecting dark matter oscillations with gravitational waveforms","abstract":"We consider the phase shift in the gravitational wave signal induced by fast oscillations of scalar dark matter surrounding binary systems, which could be probed by the future experiments LISA and DECIGO. This effect depends on the local matter density and the mass of the dark matter particle. We compare it to the phase shift due to a standard dynamical friction term, which should generically be present. We find that the effect associated with the oscillations only dominates over the dynamical friction for dark matter masses below $10^{-21}$ eV, with masses below $10^{-23}$ eV implying cloud sizes that are too large to be realistic. Moreover, for masses of the order of $10^{-21}$ eV, LISA and DECIGO would only detect this effect for dark matter densities greater than that in the solar system by a factor $10^5$ or $10^4$ respectively. We conclude that this signal can be ignored for most dark matter scenarios unless very dense clouds of very light dark matter are created early in the Universe at a redshift $z\\sim 10^4$.","sentences":["We consider the phase shift in the gravitational wave signal induced by fast oscillations of scalar dark matter surrounding binary systems, which could be probed by the future experiments LISA and DECIGO.","This effect depends on the local matter density and the mass of the dark matter particle.","We compare it to the phase shift due to a standard dynamical friction term, which should generically be present.","We find that the effect associated with the oscillations only dominates over the dynamical friction for dark matter masses below $10^{-21}$ eV, with masses below $10^{-23}$ eV implying cloud sizes that are too large to be realistic.","Moreover, for masses of the order of $10^{-21}$ eV, LISA and DECIGO would only detect this effect for dark matter densities greater than that in the solar system by a factor $10^5$ or $10^4$ respectively.","We conclude that this signal can be ignored for most dark matter scenarios unless very dense clouds of very light dark matter are created early in the Universe at a redshift $z\\sim 10^4$."],"url":"http://arxiv.org/abs/2402.04819v1","category":"astro-ph.CO"}
{"created":"2024-02-07 13:11:37","title":"Perturbative application of next-to-leading order pionless EFT for $A\\le3$ nuclei in a finite volume","abstract":"Lattice quantum chromodynamics (LQCD) calculations with physical pion mass would revolutionize nuclear physics by enabling predictions based on the fundamental theory of the strong force. To bridge the gap between finite-volume LQCD results and free-space physical observables, two primary extrapolation methods have been employed so far. The traditional approach relies on the L\\\"{u}scher formula and its extensions, while a recent alternative employs effective field theories (EFTs) fitted directly to the finite volume data. In this study, we fit pionless EFT with perturbative inclusion of the next-to-leading order to finite-volume energies generated from a phenomenological $NN$ interaction. The theory is then used to extrapolate the finite-volume results into free space as well as to predict new few-body observables. As a benchmark, we also apply the L\\\"{u}scher formalism directly to the finite-volume data. Through a comprehensive analysis, we explore the characteristics of order-by-order predictions of the pionless EFT fitted within a finite volume, investigate the limitations of the different extrapolation techniques used, and derive recommended box sizes required for reliable predictions.","sentences":["Lattice quantum chromodynamics (LQCD) calculations with physical pion mass would revolutionize nuclear physics by enabling predictions based on the fundamental theory of the strong force.","To bridge the gap between finite-volume LQCD results and free-space physical observables, two primary extrapolation methods have been employed so far.","The traditional approach relies on the L\\\"{u}scher formula and its extensions, while a recent alternative employs effective field theories (EFTs) fitted directly to the finite volume data.","In this study, we fit pionless EFT with perturbative inclusion of the next-to-leading order to finite-volume energies generated from a phenomenological $NN$ interaction.","The theory is then used to extrapolate the finite-volume results into free space as well as to predict new few-body observables.","As a benchmark, we also apply the L\\\"{u}scher formalism directly to the finite-volume data.","Through a comprehensive analysis, we explore the characteristics of order-by-order predictions of the pionless EFT fitted within a finite volume, investigate the limitations of the different extrapolation techniques used, and derive recommended box sizes required for reliable predictions."],"url":"http://arxiv.org/abs/2402.04817v1","category":"nucl-th"}
{"created":"2024-02-07 13:08:01","title":"YBa$_{1-x}$Sr$_{x}$CuFeO$_{5}$ layered perovskites: exploring the magnetic order beyond the paramagnetic-collinear-spiral triple point","abstract":"Layered perovskites of general formula AA'CuFeO$_5$ are one of the few examples of cycloidal spiral magnets where the ordering temperatures $T_{spiral}$ can be tuned far beyond room temperature by introducing modest amounts of Cu/Fe chemical disorder in the crystal structure. This rare property makes these materials prominent candidates to host multiferroicity and magnetoelectric coupling at room temperature. Moreover, it has been proposed that the highest $T_{spiral}$ value that can be reached in this structural family ($\\sim$ 400 K) corresponds to a paramagnetic-collinear-spiral triple point with potential to show exotic physics. Since generating high amounts of Cu/Fe disorder is experimentally difficult, the phase diagram region beyond the triple point has been barely explored. To fill this gap we investigate here the YBa$_{1-x}$Sr$_{x}$CuFeO$_{5}$ solid solutions ($0 \\leq x \\leq 1$), where we replace Ba with Sr with the aim of enhancing the impact of the experimentally available Cu/Fe disorder. Using a combination of bulk magnetization, synchrotron X-ray and neutron powder diffraction we show that the spiral state is destabilized beyond a critical degree of Cu/Fe disorder, being replaced by a non-frustrated, fully antiferromagnetic state with propagation vector k$_{c2}$ = $(\\frac{1}{2}, \\frac{1}{2}, 0)$ and ordering temperature $T_{coll2}$ $\\geq$ $T_{spiral}$, which is progressively stabilized beyond the triple point. Interestingly, $T_{spiral}$ and $T_{coll2}$ increase with $x$ at the same rate. This suggests a common, disorder-driven origin, consistent with theoretical predictions.","sentences":["Layered perovskites of general formula AA'CuFeO$_5$ are one of the few examples of cycloidal spiral magnets where the ordering temperatures $T_{spiral}$ can be tuned far beyond room temperature by introducing modest amounts of Cu/Fe chemical disorder in the crystal structure.","This rare property makes these materials prominent candidates to host multiferroicity and magnetoelectric coupling at room temperature.","Moreover, it has been proposed that the highest $T_{spiral}$ value that can be reached in this structural family ($\\sim$ 400 K) corresponds to a paramagnetic-collinear-spiral triple point with potential to show exotic physics.","Since generating high amounts of Cu/Fe disorder is experimentally difficult, the phase diagram region beyond the triple point has been barely explored.","To fill this gap we investigate here the YBa$_{1-x}$Sr$_{x}$CuFeO$_{5}$ solid solutions ($0 \\leq x \\leq 1$), where we replace Ba with Sr with the aim of enhancing the impact of the experimentally available Cu/Fe disorder.","Using a combination of bulk magnetization, synchrotron X-ray and neutron powder diffraction we show that the spiral state is destabilized beyond a critical degree of Cu/Fe disorder, being replaced by a non-frustrated, fully antiferromagnetic state with propagation vector k$_{c2}$ = $(\\frac{1}{2}, \\frac{1}{2}, 0)$ and ordering temperature $T_{coll2}$ $\\geq$ $T_{spiral}$, which is progressively stabilized beyond the triple point.","Interestingly, $T_{spiral}$ and $T_{coll2}$ increase with $x$ at the same rate.","This suggests a common, disorder-driven origin, consistent with theoretical predictions."],"url":"http://arxiv.org/abs/2402.04816v1","category":"cond-mat.str-el"}
{"created":"2024-02-07 13:04:35","title":"BOWLL: A Deceptively Simple Open World Lifelong Learner","abstract":"The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning. However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets. A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime. Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend. To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline. Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning. Through extensive empirical evaluation, we highlight why our approach should serve as a future standard for models that are able to effectively maintain their knowledge, selectively focus on informative data, and accelerate future learning.","sentences":["The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning.","However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets.","A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime.","Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend.","To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline.","Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning.","Through extensive empirical evaluation, we highlight why our approach should serve as a future standard for models that are able to effectively maintain their knowledge, selectively focus on informative data, and accelerate future learning."],"url":"http://arxiv.org/abs/2402.04814v1","category":"cs.LG"}
{"created":"2024-02-07 13:04:25","title":"Reconstruction of the singularity-free $f(\\mathcal{R})$ gravity via Raychaudhuri equations","abstract":"We study the bounce cosmology to construct a singularity-free $f(\\mathcal{R})$ model using the reconstruction technique. The formulation of the $f(\\mathcal{R})$ model is based on the Raychaudhari equation, a key element employed in reconstructed models to eliminate singularities. We explore the feasibility of obtaining stable gravitational Lagrangians, adhering to the conditions $f_{\\mathcal{R}}>0$ and $f_{\\mathcal{R}\\mathcal{R}}>0$. Consequently, both models demonstrate stability, effectively avoiding the Dolgov-Kawasaki instability. Our assessment extends to testing the reconstructed model using energy conditions and the effective equation-of-state (EoS). Our findings indicate that the reconstructed super-bounce model facilitates the examination of a singularity-free accelerating universe for both phantom and non-phantom phases. However, in the case of the reconstructed oscillatory bounce model, two scenarios are considered with $\\omega=-1/3$ and $\\omega=-2/3$. While the model proves suitable for studying a singular-free accelerating universe in the $\\omega=-1/3$ case, it fails to demonstrate such behavior under energy conditions for the $\\omega=-2/3$ scenario. The reconstructed models accommodate early-time bouncing behavior and late-","sentences":["We study the bounce cosmology to construct a singularity-free $f(\\mathcal{R})$ model using the reconstruction technique.","The formulation of the $f(\\mathcal{R})$ model is based on the Raychaudhari equation, a key element employed in reconstructed models to eliminate singularities.","We explore the feasibility of obtaining stable gravitational Lagrangians, adhering to the conditions $f_{\\mathcal{R}}>0$ and $f_{\\mathcal{R}\\mathcal{R}}>0$. Consequently, both models demonstrate stability, effectively avoiding the Dolgov-Kawasaki instability.","Our assessment extends to testing the reconstructed model using energy conditions and the effective equation-of-state (EoS).","Our findings indicate that the reconstructed super-bounce model facilitates the examination of a singularity-free accelerating universe for both phantom and non-phantom phases.","However, in the case of the reconstructed oscillatory bounce model, two scenarios are considered with $\\omega=-1/3$ and $\\omega=-2/3$. While the model proves suitable for studying a singular-free accelerating universe in the $\\omega=-1/3$ case, it fails to demonstrate such behavior under energy conditions for the $\\omega=-2/3$ scenario.","The reconstructed models accommodate early-time bouncing behavior and late-"],"url":"http://arxiv.org/abs/2402.04813v1","category":"gr-qc"}
{"created":"2024-02-07 13:01:28","title":"Accurate Coverage Metrics for Compiler-Generated Debugging Information","abstract":"Many debugging tools rely on compiler-produced metadata to present a source-language view of program states, such as variable values and source line numbers. While this tends to work for unoptimised programs, current compilers often generate only partial debugging information in optimised programs. Current approaches for measuring the extent of coverage of local variables are based on crude assumptions (for example, assuming variables could cover their whole parent scope) and are not comparable from one compilation to another. In this work, we propose some new metrics, computable by our tools, which could serve as motivation for language implementations to improve debugging quality.","sentences":["Many debugging tools rely on compiler-produced metadata to present a source-language view of program states, such as variable values and source line numbers.","While this tends to work for unoptimised programs, current compilers often generate only partial debugging information in optimised programs.","Current approaches for measuring the extent of coverage of local variables are based on crude assumptions (for example, assuming variables could cover their whole parent scope) and are not comparable from one compilation to another.","In this work, we propose some new metrics, computable by our tools, which could serve as motivation for language implementations to improve debugging quality."],"url":"http://arxiv.org/abs/2402.04811v1","category":"cs.PL"}
{"created":"2024-02-07 12:55:19","title":"Nonlinear nonlocal potential theory at the gradient level","abstract":"The aim of this work is to establish numerous interrelated gradient estimates in the nonlinear nonlocal setting. First of all, we prove that weak solutions to a class of homogeneous nonlinear nonlocal equations of possibly arbitrarily low order have H\\\"{o}lder continuous gradients. Using these estimates in the homogeneous case, we then prove sharp higher differentiability as well as pointwise gradient potential estimates for nonlinear nonlocal equations of order larger than one in the presence of general measure data. Our pointwise estimates imply that the first-order regularity properties of such nonlinear nonlocal equations coincide with the sharp ones of the fractional Laplacian.","sentences":["The aim of this work is to establish numerous interrelated gradient estimates in the nonlinear nonlocal setting.","First of all, we prove that weak solutions to a class of homogeneous nonlinear nonlocal equations of possibly arbitrarily low order have H\\\"{o}lder continuous gradients.","Using these estimates in the homogeneous case, we then prove sharp higher differentiability as well as pointwise gradient potential estimates for nonlinear nonlocal equations of order larger than one in the presence of general measure data.","Our pointwise estimates imply that the first-order regularity properties of such nonlinear nonlocal equations coincide with the sharp ones of the fractional Laplacian."],"url":"http://arxiv.org/abs/2402.04809v1","category":"math.AP"}
{"created":"2024-02-07 12:43:10","title":"Non-linear population discrete models with two time scales: re-scaling of part of the slow process","abstract":"In this work we present a reduction result for discrete time systems with two time scales. In order to be valid, previous results in the field require some strong hypotheses that are difficult to check in practical applications. Roughly speaking, the iterates of a map as well as their differentials must converge uniformly on compact sets. Here, we eliminate the hypothesis of uniform convergence of the differentials at no significant cost in the conclusions of the result. This new result is then used to extend to nonlinear cases the reduction of some population discrete models involving processes acting at different time scales. In practical cases, some processes that occur at a fast time scale are often only measured at slow time intervals, notably mortality. For a general class of linear models that include such kind of processes, it has been shown that a more realistic approach requires the re-scaling of those processes to be considered at the fast time scale. We develop the same type of re-scaling in some nonlinear models and prove the corresponding reduction results. We also provide an application to a particular model of a structured population in a two-patch environment.","sentences":["In this work we present a reduction result for discrete time systems with two time scales.","In order to be valid, previous results in the field require some strong hypotheses that are difficult to check in practical applications.","Roughly speaking, the iterates of a map as well as their differentials must converge uniformly on compact sets.","Here, we eliminate the hypothesis of uniform convergence of the differentials at no significant cost in the conclusions of the result.","This new result is then used to extend to nonlinear cases the reduction of some population discrete models involving processes acting at different time scales.","In practical cases, some processes that occur at a fast time scale are often only measured at slow time intervals, notably mortality.","For a general class of linear models that include such kind of processes, it has been shown that a more realistic approach requires the re-scaling of those processes to be considered at the fast time scale.","We develop the same type of re-scaling in some nonlinear models and prove the corresponding reduction results.","We also provide an application to a particular model of a structured population in a two-patch environment."],"url":"http://arxiv.org/abs/2402.04803v1","category":"math.DS"}
{"created":"2024-02-07 12:42:16","title":"Extreme near-field heat transfer between silica surfaces","abstract":"Despite recent experiments exhibiting an impressive enhancement in radiative heat flux between parallel planar silica surfaces with gap sizes of about 10 nm, the exploration of sub-nanometric gap distances remains unexplored. In this work, by employing non-equilibrium molecular dynamics (NEMD) simulations, we study the heat transfer between two SiO2 plates in both their amorphous and crystalline forms. When the gap size is 2 nm, we find that the heat transfer coefficient experiences a substantial ~30-fold increase compared to the experimental value at the gap size of 10 nm confirming the dependence on the distance inversely quadratic as predicted by the fluctuational electrodynamics (FE) theory. Comparative analysis between NEMD and FE reveals a generally good agreement, particularly for amorphous silica. Spectral heat transfer analysis demonstrates the profound influence of gap size on heat transfer, with peaks corresponding to the resonances of dielectric function. Deviations from fluctuational electrodynamics theory at smaller gap sizes are interpreted in the context of acoustic phonon tunneling and the effects of a gradient of permittivity close to the surfaces.","sentences":["Despite recent experiments exhibiting an impressive enhancement in radiative heat flux between parallel planar silica surfaces with gap sizes of about 10 nm, the exploration of sub-nanometric gap distances remains unexplored.","In this work, by employing non-equilibrium molecular dynamics (NEMD) simulations, we study the heat transfer between two SiO2 plates in both their amorphous and crystalline forms.","When the gap size is 2 nm, we find that the heat transfer coefficient experiences a substantial ~30-fold increase compared to the experimental value at the gap size of 10 nm confirming the dependence on the distance inversely quadratic as predicted by the fluctuational electrodynamics (FE) theory.","Comparative analysis between NEMD and FE reveals a generally good agreement, particularly for amorphous silica.","Spectral heat transfer analysis demonstrates the profound influence of gap size on heat transfer, with peaks corresponding to the resonances of dielectric function.","Deviations from fluctuational electrodynamics theory at smaller gap sizes are interpreted in the context of acoustic phonon tunneling and the effects of a gradient of permittivity close to the surfaces."],"url":"http://arxiv.org/abs/2402.04801v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 12:39:47","title":"Strongly Polynomial Frame Scaling to High Precision","abstract":"The frame scaling problem is: given vectors $U := \\{u_{1}, ..., u_{n} \\} \\subseteq \\mathbb{R}^{d}$, marginals $c \\in \\mathbb{R}^{n}_{++}$, and precision $\\varepsilon > 0$, find left and right scalings $L \\in \\mathbb{R}^{d \\times d}, r \\in \\mathbb{R}^n$ such that $(v_1,\\dots,v_n) := (Lu_1 r_1,\\dots,Lu_nr_n)$ simultaneously satisfies $\\sum_{i=1}^n v_i v_i^{\\mathsf{T}} = I_d$ and $\\|v_{j}\\|_{2}^{2} = c_{j}, \\forall j \\in [n]$, up to error $\\varepsilon$. This problem has appeared in a variety of fields throughout linear algebra and computer science. In this work, we give a strongly polynomial algorithm for frame scaling with $\\log(1/\\varepsilon)$ convergence. This answers a question of Diakonikolas, Tzamos and Kane (STOC 2023), who gave the first strongly polynomial randomized algorithm with poly$(1/\\varepsilon)$ convergence for the special case $c = \\frac{d}{n} 1_{n}$. Our algorithm is deterministic, applies for general $c \\in \\mathbb{R}^{n}_{++}$, and requires $O(n^{3} \\log(n/\\varepsilon))$ iterations as compared to $O(n^{5} d^{11}/\\varepsilon^{5})$ iterations of DTK. By lifting the framework of Linial, Samorodnitsky and Wigderson (Combinatorica 2000) for matrix scaling to frames, we are able to simplify both the algorithm and analysis. Our main technical contribution is to generalize the potential analysis of LSW to the frame setting and compute an update step in strongly polynomial time that achieves geometric progress in each iteration. In fact, we can adapt our results to give an improved analysis of strongly polynomial matrix scaling, reducing the $O(n^{5} \\log(n/\\varepsilon))$ iteration bound of LSW to $O(n^{3} \\log(n/\\varepsilon))$. Additionally, we prove a novel bound on the size of approximate frame scaling solutions, involving the condition measure $\\bar{\\chi}$ studied in the linear programming literature, which may be of independent interest.","sentences":["The frame scaling problem is: given vectors $U := \\{u_{1}, ..., u_{n} \\} \\subseteq \\mathbb{R}^{d}$, marginals $c \\in \\mathbb{R}^{n}_{++}$, and precision $\\varepsilon > 0$, find left and right scalings $L \\in \\mathbb{R}^{d \\times d}, r \\in \\mathbb{R}^n$ such that $(v_1,\\dots,v_n) := (Lu_1 r_1,\\dots,Lu_nr_n)$ simultaneously satisfies $\\sum_{i=1}^n v_i v_i^{\\mathsf{T}} = I_d$ and $\\|v_{j}\\|_{2}^{2} = c_{j}, \\forall j \\in","[n]$, up to error $\\varepsilon$. This problem has appeared in a variety of fields throughout linear algebra and computer science.","In this work, we give a strongly polynomial algorithm for frame scaling with $\\log(1/\\varepsilon)$ convergence.","This answers a question of Diakonikolas, Tzamos and Kane (STOC 2023), who gave the first strongly polynomial randomized algorithm with poly$(1/\\varepsilon)$ convergence for the special case $c = \\frac{d}{n} 1_{n}$. Our algorithm is deterministic, applies for general $c \\in \\mathbb{R}^{n}_{++}$, and requires $O(n^{3} \\log(n/\\varepsilon))$ iterations as compared to $O(n^{5} d^{11}/\\varepsilon^{5})$ iterations of DTK.","By lifting the framework of Linial, Samorodnitsky and Wigderson (Combinatorica 2000) for matrix scaling to frames, we are able to simplify both the algorithm and analysis.","Our main technical contribution is to generalize the potential analysis of LSW to the frame setting and compute an update step in strongly polynomial time that achieves geometric progress in each iteration.","In fact, we can adapt our results to give an improved analysis of strongly polynomial matrix scaling, reducing the $O(n^{5} \\log(n/\\varepsilon))$ iteration bound of LSW to $O(n^{3} \\log(n/\\varepsilon))$. Additionally, we prove a novel bound on the size of approximate frame scaling solutions, involving the condition measure $\\bar{\\chi}$ studied in the linear programming literature, which may be of independent interest."],"url":"http://arxiv.org/abs/2402.04799v1","category":"cs.DS"}
{"created":"2024-02-07 12:38:47","title":"Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer","abstract":"Artificial neural networks (ANNs) can help camera-based remote photoplethysmography (rPPG) in measuring cardiac activity and physiological signals from facial videos, such as pulse wave, heart rate and respiration rate with better accuracy. However, most existing ANN-based methods require substantial computing resources, which poses challenges for effective deployment on mobile devices. Spiking neural networks (SNNs), on the other hand, hold immense potential for energy-efficient deep learning owing to their binary and event-driven architecture. To the best of our knowledge, we are the first to introduce SNNs into the realm of rPPG, proposing a hybrid neural network (HNN) model, the Spiking-PhysFormer, aimed at reducing power consumption. Specifically, the proposed Spiking-PhyFormer consists of an ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based predictor head. First, to simplify the transformer block while preserving its capacity to aggregate local and global spatio-temporal features, we design a parallel spike transformer block to replace sequential sub-blocks. Additionally, we propose a simplified spiking self-attention mechanism that omits the value parameter without compromising the model's performance. Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD demonstrate that the proposed model achieves a 12.4\\% reduction in power consumption compared to PhysFormer. Additionally, the power consumption of the transformer block is reduced by a factor of 12.2, while maintaining decent performance as PhysFormer and other ANN-based models.","sentences":["Artificial neural networks (ANNs) can help camera-based remote photoplethysmography (rPPG) in measuring cardiac activity and physiological signals from facial videos, such as pulse wave, heart rate and respiration rate with better accuracy.","However, most existing ANN-based methods require substantial computing resources, which poses challenges for effective deployment on mobile devices.","Spiking neural networks (SNNs), on the other hand, hold immense potential for energy-efficient deep learning owing to their binary and event-driven architecture.","To the best of our knowledge, we are the first to introduce SNNs into the realm of rPPG, proposing a hybrid neural network (HNN) model, the Spiking-PhysFormer, aimed at reducing power consumption.","Specifically, the proposed Spiking-PhyFormer consists of an ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based predictor head.","First, to simplify the transformer block while preserving its capacity to aggregate local and global spatio-temporal features, we design a parallel spike transformer block to replace sequential sub-blocks.","Additionally, we propose a simplified spiking self-attention mechanism that omits the value parameter without compromising the model's performance.","Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD demonstrate that the proposed model achieves a 12.4\\% reduction in power consumption compared to PhysFormer.","Additionally, the power consumption of the transformer block is reduced by a factor of 12.2, while maintaining decent performance as PhysFormer and other ANN-based models."],"url":"http://arxiv.org/abs/2402.04798v1","category":"cs.CV"}
{"created":"2024-02-07 12:38:12","title":"Offline Deep Model Predictive Control (MPC) for Visual Navigation","abstract":"In this paper, we propose a new visual navigation method based on a single RGB perspective camera. Using the Visual Teach & Repeat (VT&R) methodology, the robot acquires a visual trajectory consisting of multiple subgoal images in the teaching step. In the repeat step, we propose two network architectures, namely ViewNet and VelocityNet. The combination of the two networks allows the robot to follow the visual trajectory. ViewNet is trained to generate a future image based on the current view and the velocity command. The generated future image is combined with the subgoal image for training VelocityNet. We develop an offline Model Predictive Control (MPC) policy within VelocityNet with the dual goals of (1) reducing the difference between current and subgoal images and (2) ensuring smooth trajectories by mitigating velocity discontinuities. Offline training conserves computational resources, making it a more suitable option for scenarios with limited computational capabilities, such as embedded systems. We validate our experiments in a simulation environment, demonstrating that our model can effectively minimize the metric error between real and played trajectories.","sentences":["In this paper, we propose a new visual navigation method based on a single RGB perspective camera.","Using the Visual Teach & Repeat (VT&R) methodology, the robot acquires a visual trajectory consisting of multiple subgoal images in the teaching step.","In the repeat step, we propose two network architectures, namely ViewNet and VelocityNet.","The combination of the two networks allows the robot to follow the visual trajectory.","ViewNet is trained to generate a future image based on the current view and the velocity command.","The generated future image is combined with the subgoal image for training VelocityNet.","We develop an offline Model Predictive Control (MPC) policy within VelocityNet with the dual goals of (1) reducing the difference between current and subgoal images and (2) ensuring smooth trajectories by mitigating velocity discontinuities.","Offline training conserves computational resources, making it a more suitable option for scenarios with limited computational capabilities, such as embedded systems.","We validate our experiments in a simulation environment, demonstrating that our model can effectively minimize the metric error between real and played trajectories."],"url":"http://arxiv.org/abs/2402.04797v1","category":"cs.RO"}
{"created":"2024-02-07 12:36:54","title":"Mesh-based Gaussian Splatting for Real-time Large-scale Deformation","abstract":"Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene. Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion. Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views. However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology. To address this, we develop a novel GS-based method that enables interactive deformation. Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation. 3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians. Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation. Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh. Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation. Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average).","sentences":["Neural implicit representations, including Neural Distance Fields and Neural Radiance Fields, have demonstrated significant capabilities for reconstructing surfaces with complicated geometry and topology, and generating novel views of a scene.","Nevertheless, it is challenging for users to directly deform or manipulate these implicit representations with large deformations in the real-time fashion.","Gaussian Splatting(GS) has recently become a promising method with explicit geometry for representing static scenes and facilitating high-quality and real-time synthesis of novel views.","However,it cannot be easily deformed due to the use of discrete Gaussians and lack of explicit topology.","To address this, we develop a novel GS-based method that enables interactive deformation.","Our key idea is to design an innovative mesh-based GS representation, which is integrated into Gaussian learning and manipulation.","3D Gaussians are defined over an explicit mesh, and they are bound with each other: the rendering of 3D Gaussians guides the mesh face split for adaptive refinement, and the mesh face split directs the splitting of 3D Gaussians.","Moreover, the explicit mesh constraints help regularize the Gaussian distribution, suppressing poor-quality Gaussians(e.g. misaligned Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and avoiding artifacts during deformation.","Based on this representation, we further introduce a large-scale Gaussian deformation technique to enable deformable GS, which alters the parameters of 3D Gaussians according to the manipulation of the associated mesh.","Our method benefits from existing mesh deformation datasets for more realistic data-driven Gaussian deformation.","Extensive experiments show that our approach achieves high-quality reconstruction and effective deformation, while maintaining the promising rendering results at a high frame rate(65 FPS on average)."],"url":"http://arxiv.org/abs/2402.04796v1","category":"cs.GR"}
{"created":"2024-02-07 12:31:13","title":"Direct Language Model Alignment from Online AI Feedback","abstract":"Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.","sentences":["Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model.","However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline.","Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy.","In this study, we posit that online feedback is key and improves DAP methods.","Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback.","Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods.","We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator."],"url":"http://arxiv.org/abs/2402.04792v1","category":"cs.AI"}
{"created":"2024-02-07 12:28:55","title":"Making Multicurves Cross Minimally on Surfaces","abstract":"On an orientable surface $S$, consider a collection $\\Gamma$ of closed curves. The (geometric) intersection number $i_S(\\Gamma)$ is the minimum number of self-intersections that a collection $\\Gamma'$ can have, where $\\Gamma'$ results from a continuous deformation (homotopy) of $\\Gamma$. We provide algorithms that compute $i_S(\\Gamma)$ and such a $\\Gamma'$, assuming that $\\Gamma$ is given by a collection of closed walks of length $n$ in a graph $M$ cellularly embedded on $S$, in $O(n \\log n)$ time when $M$ and $S$ are fixed.   The state of the art is a paper of Despr\\'e and Lazarus [SoCG 2017, J. ACM 2019], who compute $i_S(\\Gamma)$ in $O(n^2)$ time, and $\\Gamma'$ in $O(n^4)$ time if $\\Gamma$ is a single closed curve. Our result is more general since we can put an arbitrary number of closed curves in minimal position. Also, our algorithms are quasi-linear in $n$ instead of quadratic and quartic, and our proofs are simpler and shorter.   We use techniques from two-dimensional topology and from the theory of hyperbolic surfaces. Most notably, we prove a new property of the reducing triangulations introduced by Colin de Verdi\\`ere, Despr\\'e, and Dubois [SODA 2024], reducing our problem to the case of surfaces with boundary. As a key subroutine, we rely on an algorithm of Fulek and T\\'oth [JCO 2020].","sentences":["On an orientable surface $S$, consider a collection $\\Gamma$ of closed curves.","The (geometric) intersection number $i_S(\\Gamma)$ is the minimum number of self-intersections that a collection $\\Gamma'$ can have, where $\\Gamma'$ results from a continuous deformation (homotopy) of $\\Gamma$. We provide algorithms that compute $i_S(\\Gamma)$ and such a $\\Gamma'$, assuming that $\\Gamma$ is given by a collection of closed walks of length $n$ in a graph $M$ cellularly embedded on $S$, in $O(n \\log n)$ time when $M$ and $S$ are fixed.   ","The state of the art is a paper of Despr\\'e and Lazarus","[SoCG 2017, J. ACM 2019], who compute $i_S(\\Gamma)$ in $O(n^2)$ time, and $\\Gamma'$ in $O(n^4)$ time if $\\Gamma$ is a single closed curve.","Our result is more general since we can put an arbitrary number of closed curves in minimal position.","Also, our algorithms are quasi-linear in $n$ instead of quadratic and quartic, and our proofs are simpler and shorter.   ","We use techniques from two-dimensional topology and from the theory of hyperbolic surfaces.","Most notably, we prove a new property of the reducing triangulations introduced by Colin de Verdi\\`ere, Despr\\'e, and","Dubois [SODA 2024], reducing our problem to the case of surfaces with boundary.","As a key subroutine, we rely on an algorithm of Fulek and T\\'oth [JCO 2020]."],"url":"http://arxiv.org/abs/2402.04789v1","category":"cs.CG"}
{"created":"2024-02-07 12:28:32","title":"MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark","abstract":"Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts regarding MLLMs as fully reliable evaluators. Code and dataset are available at https://github.com/Dongping-Chen/MLLM-as-a-Judge.","sentences":["Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence.","However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences.","Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking.","Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks.","Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts regarding MLLMs as fully reliable evaluators.","Code and dataset are available at https://github.com/Dongping-Chen/MLLM-as-a-Judge."],"url":"http://arxiv.org/abs/2402.04788v1","category":"cs.CL"}
{"created":"2024-02-07 12:26:12","title":"A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models","abstract":"The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further. To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM's and the Bayesian network's decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work.","sentences":["The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions.","However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.","To this end, we propose a hypothesis-driven statistical framework.","We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates.","Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations.","This allows us to judge how similar the LLM's and the Bayesian network's decision processes are.","We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks.","The resulting models do not exhibit a strong similarity to GPT-3.5.","We discuss the implications of this as well as the framework's potential to approximate LLM decisions better in future work."],"url":"http://arxiv.org/abs/2402.04787v1","category":"cs.CL"}
{"created":"2024-02-07 12:18:04","title":"Multiple bipolar fuzzy measures: an application to community detection problems for networks with additional information","abstract":"In this paper we introduce the concept of multiple bipolar fuzzy measures as a generalization of a bipolar fuzzy measure. We also propose a new definition of a group, which is based on the multidimensional bipolar fuzzy relations of its elements. Taking into account this information, we provide a novel procedure (based on the well-known Louvain algorithm) to deal with community detection problems. This new method considers the multidimensional bipolar information provided by multiple bipolar fuzzy measures, as well as the information provided by a graph. We also give some detailed computational tests, obtained from the application of this algorithm in several benchmark models.","sentences":["In this paper we introduce the concept of multiple bipolar fuzzy measures as a generalization of a bipolar fuzzy measure.","We also propose a new definition of a group, which is based on the multidimensional bipolar fuzzy relations of its elements.","Taking into account this information, we provide a novel procedure (based on the well-known Louvain algorithm) to deal with community detection problems.","This new method considers the multidimensional bipolar information provided by multiple bipolar fuzzy measures, as well as the information provided by a graph.","We also give some detailed computational tests, obtained from the application of this algorithm in several benchmark models."],"url":"http://arxiv.org/abs/2402.04786v1","category":"cs.SI"}
{"created":"2024-02-07 12:15:51","title":"Equidistribution of cusp points of Hecke triangle groups","abstract":"In the framework of infinite ergodic theory, we derive equidistribution results for suitable weighted sequences of cusp points of Hecke triangle groups encoded by group elements of constant word length with respect to a set of natural generators. This is a generalization of the corresponding results for the modular group, for which we rely on advanced results from infinite ergodic theory and transfer operator techniques developed for AFN-maps.","sentences":["In the framework of infinite ergodic theory, we derive equidistribution results for suitable weighted sequences of cusp points of Hecke triangle groups encoded by group elements of constant word length with respect to a set of natural generators.","This is a generalization of the corresponding results for the modular group, for which we rely on advanced results from infinite ergodic theory and transfer operator techniques developed for AFN-maps."],"url":"http://arxiv.org/abs/2402.04784v1","category":"math.DS"}
{"created":"2024-02-07 12:06:52","title":"Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks","abstract":"Recently, neural networks utilizing periodic activation functions have been proven to demonstrate superior performance in vision tasks compared to traditional ReLU-activated networks. However, there is still a limited understanding of the underlying reasons for this improved performance. In this paper, we aim to address this gap by providing a theoretical understanding of periodically activated networks through an analysis of their Neural Tangent Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the finite width setting, using a fairly general network architecture which requires only one wide layer that grows at least linearly with the number of data samples. Our findings indicate that periodically activated networks are \\textit{notably more well-behaved}, from the NTK perspective, than ReLU activated networks. Additionally, we give an application to the memorization capacity of such networks and verify our theoretical predictions empirically. Our study offers a deeper understanding of the properties of periodically activated neural networks and their potential in the field of deep learning.","sentences":["Recently, neural networks utilizing periodic activation functions have been proven to demonstrate superior performance in vision tasks compared to traditional ReLU-activated networks.","However, there is still a limited understanding of the underlying reasons for this improved performance.","In this paper, we aim to address this gap by providing a theoretical understanding of periodically activated networks through an analysis of their Neural Tangent Kernel (NTK).","We derive bounds on the minimum eigenvalue of their NTK in the finite width setting, using a fairly general network architecture which requires only one wide layer that grows at least linearly with the number of data samples.","Our findings indicate that periodically activated networks are \\textit{notably more well-behaved}, from the NTK perspective, than ReLU activated networks.","Additionally, we give an application to the memorization capacity of such networks and verify our theoretical predictions empirically.","Our study offers a deeper understanding of the properties of periodically activated neural networks and their potential in the field of deep learning."],"url":"http://arxiv.org/abs/2402.04783v1","category":"cs.LG"}
{"created":"2024-02-07 12:06:12","title":"Exact solutions for the probability density of various conditioned processes with an entrance boundary","abstract":"The probability density is a fundamental quantity for characterizing diffusion processes. However, it is seldom known except in a few renowned cases, including Brownian motion and the Ornstein-Uhlenbeck process and their bridges, geometric Brownian motion, Brownian excursion, or Bessel processes. In this paper, we utilize Girsanov's theorem, along with a variation of the method of images, to derive the exact expression of the probability density for diffusions that have one entrance boundary. Our analysis encompasses numerous families of conditioned diffusions, including the Taboo process and Brownian motion conditioned on its growth behavior, as well as the drifted Brownian meander and generalized Brownian excursion.","sentences":["The probability density is a fundamental quantity for characterizing diffusion processes.","However, it is seldom known except in a few renowned cases, including Brownian motion and the Ornstein-Uhlenbeck process and their bridges, geometric Brownian motion, Brownian excursion, or Bessel processes.","In this paper, we utilize Girsanov's theorem, along with a variation of the method of images, to derive the exact expression of the probability density for diffusions that have one entrance boundary.","Our analysis encompasses numerous families of conditioned diffusions, including the Taboo process and Brownian motion conditioned on its growth behavior, as well as the drifted Brownian meander and generalized Brownian excursion."],"url":"http://arxiv.org/abs/2402.04781v1","category":"math-ph"}
{"created":"2024-02-07 12:01:02","title":"StableMask: Refining Causal Masking in Decoder-only Transformer","abstract":"The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.","sentences":["The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling.","Despite its exceptional performance across various tasks, we have identified two limitations:","First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information.","This compels the model to assign disproportional excessive attention to specific tokens.","Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks.","In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask.","It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio.","StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods.","We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques."],"url":"http://arxiv.org/abs/2402.04779v1","category":"cs.CL"}
{"created":"2024-02-07 11:52:12","title":"Cyber risk and the cross-section of stock returns","abstract":"We extract firms' cyber risk with a machine learning algorithm measuring the proximity between their disclosures and a dedicated cyber corpus. Our approach outperforms dictionary methods, uses full disclosure and not devoted-only sections, and generates a cyber risk measure uncorrelated with other firms' characteristics. We find that a portfolio of US-listed stocks in the high cyber risk quantile generates an excess return of 18.72\\% p.a. Moreover, a long-short cyber risk portfolio has a significant and positive risk premium of 6.93\\% p.a., robust to all factors' benchmarks. Finally, using a Bayesian asset pricing method, we show that our cyber risk factor is the essential feature that allows any multi-factor model to price the cross-section of stock returns.","sentences":["We extract firms' cyber risk with a machine learning algorithm measuring the proximity between their disclosures and a dedicated cyber corpus.","Our approach outperforms dictionary methods, uses full disclosure and not devoted-only sections, and generates a cyber risk measure uncorrelated with other firms' characteristics.","We find that a portfolio of US-listed stocks in the high cyber risk quantile generates an excess return of 18.72\\% p.a.","Moreover, a long-short cyber risk portfolio has a significant and positive risk premium of 6.93\\% p.a., robust to all factors' benchmarks.","Finally, using a Bayesian asset pricing method, we show that our cyber risk factor is the essential feature that allows any multi-factor model to price the cross-section of stock returns."],"url":"http://arxiv.org/abs/2402.04775v1","category":"q-fin.PM"}
{"created":"2024-02-07 11:49:06","title":"Calculating the interaction index of a fuzzy measure: a polynomial approach based on sampling","abstract":"In this paper we address the problem of fuzzy measures index calculation. On the basis of fuzzy sets, Murofushi and Soneda proposed an interaction index to deal with the relations between two individuals. This index was later extended in a common frame-work by Grabisch. Both indices are fundamental in the literature of fuzzy measures. Nevertheless, the corresponding calculation still presents a highly complex problem for which no approximation solution has been proposed yet. Then, using a representation of the Shapley based on orders, here we suggest an alternative calculation of the interaction index, both for the simple case of pairs of individuals, and for the more complex situation in which any set could be considered. This alternative representation facilitates the handling of these indices. Moreover, we draw on this representation to define two polynomial methods based on sampling to estimate the interaction index, as well as a method to approximate the generalized version of it. We provide some computational results to test the goodness of the proposed algorithms.","sentences":["In this paper we address the problem of fuzzy measures index calculation.","On the basis of fuzzy sets, Murofushi and Soneda proposed an interaction index to deal with the relations between two individuals.","This index was later extended in a common frame-work by Grabisch.","Both indices are fundamental in the literature of fuzzy measures.","Nevertheless, the corresponding calculation still presents a highly complex problem for which no approximation solution has been proposed yet.","Then, using a representation of the Shapley based on orders, here we suggest an alternative calculation of the interaction index, both for the simple case of pairs of individuals, and for the more complex situation in which any set could be considered.","This alternative representation facilitates the handling of these indices.","Moreover, we draw on this representation to define two polynomial methods based on sampling to estimate the interaction index, as well as a method to approximate the generalized version of it.","We provide some computational results to test the goodness of the proposed algorithms."],"url":"http://arxiv.org/abs/2402.04774v1","category":"math.ST"}
{"created":"2024-02-07 11:44:18","title":"Prioritizing Investments in Cybersecurity: Empirical Evidence from an Event Study on the Determinants of Cyberattack Costs","abstract":"Along with the increasing frequency and severity of cyber incidents, understanding their economic implications is paramount. In this context, listed firms' reactions to cyber incidents are compelling to study since they (i) are a good proxy to estimate the costs borne by other organizations, (ii) have a critical position in the economy, and (iii) have their financial information publicly available. We extract listed firms' cyber incident dates and characteristics from newswire headlines. We use an event study over 2012--2022, using a three-day window around events and standard benchmarks. We find that the magnitude of abnormal returns around cyber incidents is on par with previous studies using newswire or alternative data to identify cyber incidents. Conversely, as we adjust the standard errors accounting for event-induced variance and residual cross-correlation, we find that the previously claimed significance of abnormal returns vanishes. Given these results, we run a horse race of specifications, in which we test for the marginal effects of type of cyber incidents, target firm sector, periods, and their interactions. Data breaches are the most detrimental incident type with an average loss of -1.3\\% or (USD -1.9 billion) over the last decade. The health sector is the most sensitive to cyber incidents, with an average loss of -5.21\\% (or USD -1.2 billion), and even more so when these are data breaches. Instead, we cannot show any time-varying effect of cyber incidents or a specific effect of the type of news as had previously been advocated.","sentences":["Along with the increasing frequency and severity of cyber incidents, understanding their economic implications is paramount.","In this context, listed firms' reactions to cyber incidents are compelling to study since they (i) are a good proxy to estimate the costs borne by other organizations, (ii) have a critical position in the economy, and (iii) have their financial information publicly available.","We extract listed firms' cyber incident dates and characteristics from newswire headlines.","We use an event study over 2012--2022, using a three-day window around events and standard benchmarks.","We find that the magnitude of abnormal returns around cyber incidents is on par with previous studies using newswire or alternative data to identify cyber incidents.","Conversely, as we adjust the standard errors accounting for event-induced variance and residual cross-correlation, we find that the previously claimed significance of abnormal returns vanishes.","Given these results, we run a horse race of specifications, in which we test for the marginal effects of type of cyber incidents, target firm sector, periods, and their interactions.","Data breaches are the most detrimental incident type with an average loss of -1.3\\% or (USD -1.9 billion) over the last decade.","The health sector is the most sensitive to cyber incidents, with an average loss of -5.21\\% (or USD -1.2 billion), and even more so when these are data breaches.","Instead, we cannot show any time-varying effect of cyber incidents or a specific effect of the type of news as had previously been advocated."],"url":"http://arxiv.org/abs/2402.04773v1","category":"q-fin.GN"}
{"created":"2024-02-07 11:38:22","title":"Hierarchical Motion Planning and Offline Robust Model Predictive Control for Autonomous Vehicles","abstract":"Driving vehicles in complex scenarios under harsh conditions is the biggest challenge for autonomous vehicles (AVs). To address this issue, we propose hierarchical motion planning and robust control strategy using the front-active steering system in complex scenarios with various slippery road adhesion coefficients while considering vehicle uncertain parameters. Behaviors of human vehicles (HVs) are considered and modeled in the form of a car-following model via the Intelligent Driver Model (IDM). Then, in the upper layer, the motion planner first generates an optimal trajectory by using the artificial potential field (APF) algorithm to formulate any surrounding objects, e.g., road marks, boundaries, and static/dynamic obstacles. To track the generated optimal trajectory, in the lower layer, an offline-constrained output feedback robust model predictive control (RMPC) is employed for the linear parameter varying (LPV) system by applying linear matrix inequality (LMI) optimization method that ensures the robustness against the model parameter uncertainties. Furthermore, by augmenting the system model, our proposed approach, called offline RMPC, achieves outstanding efficiency compared to three existing RMPC approaches, e.g., offset-offline RMPC, online RMPC, and offline RMPC without an augmented model (offline RMPC w/o AM), in both improving computing time and reducing input vibrations.","sentences":["Driving vehicles in complex scenarios under harsh conditions is the biggest challenge for autonomous vehicles (AVs).","To address this issue, we propose hierarchical motion planning and robust control strategy using the front-active steering system in complex scenarios with various slippery road adhesion coefficients while considering vehicle uncertain parameters.","Behaviors of human vehicles (HVs) are considered and modeled in the form of a car-following model via the Intelligent Driver Model (IDM).","Then, in the upper layer, the motion planner first generates an optimal trajectory by using the artificial potential field (APF) algorithm to formulate any surrounding objects, e.g., road marks, boundaries, and static/dynamic obstacles.","To track the generated optimal trajectory, in the lower layer, an offline-constrained output feedback robust model predictive control (RMPC) is employed for the linear parameter varying (LPV) system by applying linear matrix inequality (LMI) optimization method that ensures the robustness against the model parameter uncertainties.","Furthermore, by augmenting the system model, our proposed approach, called offline RMPC, achieves outstanding efficiency compared to three existing RMPC approaches, e.g., offset-offline RMPC, online RMPC, and offline RMPC without an augmented model (offline RMPC w/o AM), in both improving computing time and reducing input vibrations."],"url":"http://arxiv.org/abs/2402.04769v1","category":"cs.RO"}
{"created":"2024-02-07 11:37:14","title":"Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction","abstract":"Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands.","sentences":["Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics.","In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training.","We develop a transformer-based architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios.","Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene.","We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time.","Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands."],"url":"http://arxiv.org/abs/2402.04768v1","category":"cs.RO"}
{"created":"2024-02-07 11:36:11","title":"$\u039b$CDM Tensions: Localising Missing Physics through Consistency Checks","abstract":"$\\Lambda$CDM tensions are by definition model dependent; one sees anomalies through the prism of $\\Lambda$CDM. Thus, progress towards tension resolution necessitates checking the consistency of the $\\Lambda$CDM model to localise missing physics either in redshift or scale. Since the Universe is dynamical and redshift is a proxy for time, it is imperative to first perform consistency checks involving redshift, then consistency checks involving scale, as the next steps to settle the ``systematics versus new physics\" debate and foster informed model building. We present a review of the hierarchy of assumptions underlying the $\\Lambda$CDM cosmological model and comment on whether relaxing them can address the tensions. We focus on the lowest lying fruit of identifying missing physics through the identification of redshift dependent $\\Lambda$CDM model fitting parameters. We highlight recent progress made on ${S_8:= \\sigma_8 \\sqrt{\\Omega_{\\rm m}/0.3}}$ tension and elucidate how similar progress can be made on $H_0$ tension. Our discussions indicate that $H_0$ tension, equivalently a redshift dependent $H_0$, and a redshift dependent $S_8$ imply a problem with background $\\Lambda$CDM cosmology.","sentences":["$\\Lambda$CDM tensions are by definition model dependent; one sees anomalies through the prism of $\\Lambda$CDM.","Thus, progress towards tension resolution necessitates checking the consistency of the $\\Lambda$CDM model to localise missing physics either in redshift or scale.","Since the Universe is dynamical and redshift is a proxy for time, it is imperative to first perform consistency checks involving redshift, then consistency checks involving scale, as the next steps to settle the ``systematics versus new physics\" debate and foster informed model building.","We present a review of the hierarchy of assumptions underlying the $\\Lambda$CDM cosmological model and comment on whether relaxing them can address the tensions.","We focus on the lowest lying fruit of identifying missing physics through the identification of redshift dependent $\\Lambda$CDM model fitting parameters.","We highlight recent progress made on ${S_8:= \\sigma_8 \\sqrt{\\Omega_{\\rm m}/0.3}}$ tension and elucidate how similar progress can be made on $H_0$ tension.","Our discussions indicate that $H_0$ tension, equivalently a redshift dependent $H_0$, and a redshift dependent $S_8$ imply a problem with background $\\Lambda$CDM cosmology."],"url":"http://arxiv.org/abs/2402.04767v1","category":"astro-ph.CO"}
{"created":"2024-02-07 11:34:54","title":"Determination of Navier's slip parameter using data assimilation","abstract":"One of the crucial aspects of patient-specific blood flow simulations is to specify material parameters and boundary conditions. The choice of boundary conditions can have a substantial impact on the character of the flow. While no-slip is the most popular wall boundary condition, some amount of slip, which determines how much fluid is allowed to flow along the wall, might be beneficial for better agreement with flow patterns in medical images. However, even if one assumes the simple Navier's boundary conditions on the wall, in which the relationship between tangential components of the normal traction and the velocity is linear, the determination of the specific value of the slip parameter is often difficult.   In this work, we present and test an optimal control method to estimate Navier's slip parameter on the wall and the velocity profile at the inlet using artificially generated flow domain and flow data. The results show that it is possible to recover the flow patterns and Navier's slip parameter by using sufficiently accurate discretization even from data containing a substantial amount of noise.","sentences":["One of the crucial aspects of patient-specific blood flow simulations is to specify material parameters and boundary conditions.","The choice of boundary conditions can have a substantial impact on the character of the flow.","While no-slip is the most popular wall boundary condition, some amount of slip, which determines how much fluid is allowed to flow along the wall, might be beneficial for better agreement with flow patterns in medical images.","However, even if one assumes the simple Navier's boundary conditions on the wall, in which the relationship between tangential components of the normal traction and the velocity is linear, the determination of the specific value of the slip parameter is often difficult.   ","In this work, we present and test an optimal control method to estimate Navier's slip parameter on the wall and the velocity profile at the inlet using artificially generated flow domain and flow data.","The results show that it is possible to recover the flow patterns and Navier's slip parameter by using sufficiently accurate discretization even from data containing a substantial amount of noise."],"url":"http://arxiv.org/abs/2402.04766v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 11:27:45","title":"Code as Reward: Empowering Reinforcement Learning with VLMs","abstract":"Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.","sentences":["Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion.","In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents.","In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress.","However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent.","To address this challenge, we propose a framework named Code as Reward (VLM-CaR).","VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly.","We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards."],"url":"http://arxiv.org/abs/2402.04764v1","category":"cs.LG"}
{"created":"2024-02-07 11:26:53","title":"Emergence of specialized Collective Behaviors in Evolving Heterogeneous Swarms","abstract":"Natural groups of animals, such as swarms of social insects, exhibit astonishing degrees of task specialization, useful to address complex tasks and to survive. This is supported by phenotypic plasticity: individuals sharing the same genotype that is expressed differently for different classes of individuals, each specializing in one task. In this work, we evolve a swarm of simulated robots with phenotypic plasticity to study the emergence of specialized collective behavior during an emergent perception task. Phenotypic plasticity is realized in the form of heterogeneity of behavior by dividing the genotype into two components, with one different neural network controller associated to each component. The whole genotype, expressing the behavior of the whole group through the two components, is subject to evolution with a single fitness function. We analyse the obtained behaviors and use the insights provided by these results to design an online regulatory mechanism. Our experiments show three main findings: 1) The sub-groups evolve distinct emergent behaviors. 2) The effectiveness of the whole swarm depends on the interaction between the two sub-groups, leading to a more robust performance than with singular sub-group behavior. 3) The online regulatory mechanism enhances overall performance and scalability.","sentences":["Natural groups of animals, such as swarms of social insects, exhibit astonishing degrees of task specialization, useful to address complex tasks and to survive.","This is supported by phenotypic plasticity: individuals sharing the same genotype that is expressed differently for different classes of individuals, each specializing in one task.","In this work, we evolve a swarm of simulated robots with phenotypic plasticity to study the emergence of specialized collective behavior during an emergent perception task.","Phenotypic plasticity is realized in the form of heterogeneity of behavior by dividing the genotype into two components, with one different neural network controller associated to each component.","The whole genotype, expressing the behavior of the whole group through the two components, is subject to evolution with a single fitness function.","We analyse the obtained behaviors and use the insights provided by these results to design an online regulatory mechanism.","Our experiments show three main findings: 1) The sub-groups evolve distinct emergent behaviors.","2) The effectiveness of the whole swarm depends on the interaction between the two sub-groups, leading to a more robust performance than with singular sub-group behavior.","3)","The online regulatory mechanism enhances overall performance and scalability."],"url":"http://arxiv.org/abs/2402.04763v1","category":"cs.RO"}
{"created":"2024-02-07 11:21:44","title":"Floor of cosmogenic neutrino fluxes above $10^{17}~$eV","abstract":"The search for neutrinos with energies greater than $10^{17}~$eV is being actively pursued. Although normalization of the dominant neutrino flux is highly uncertain, a floor level is guaranteed by the interactions of extragalactic cosmic rays with Milky Way gas. We estimate that this floor level gives an energy flux of $E^2\\phi_\\nu\\simeq 10^{-13^{+0.5}_{-0.5}}~$GeV~cm$^{-2}$~sr$^{-1}$~s$^{-1}$ at $10^{18}~$eV, where uncertainties arise from the modeling of the gas distribution and the experimental determination of the mass composition of ultra-high-energy cosmic rays on Earth. Based on a minimal model of cosmic-ray production to explain the mass-discriminated energy spectra observed on Earth above $5{\\times}10^{18}$~eV, we also present generic estimates of the neutrino fluxes expected from extragalactic production that generally exceed the aforementioned guaranteed floor. The prospects for detecting neutrinos above $10^{18}$~eV remain however challenging, unless proton acceleration to the highest energies is at play in a sub-dominant population of cosmic-ray sources or new physical phenomena are at work.","sentences":["The search for neutrinos with energies greater than $10^{17}~$eV is being actively pursued.","Although normalization of the dominant neutrino flux is highly uncertain, a floor level is guaranteed by the interactions of extragalactic cosmic rays with Milky Way gas.","We estimate that this floor level gives an energy flux of $E^2\\phi_\\nu\\simeq 10^{-13^{+0.5}_{-0.5}}~$GeV~cm$^{-2}$~sr$^{-1}$~s$^{-1}$ at $10^{18}~$eV, where uncertainties arise from the modeling of the gas distribution and the experimental determination of the mass composition of ultra-high-energy cosmic rays on Earth.","Based on a minimal model of cosmic-ray production to explain the mass-discriminated energy spectra observed on Earth above $5{\\times}10^{18}$~eV, we also present generic estimates of the neutrino fluxes expected from extragalactic production that generally exceed the aforementioned guaranteed floor.","The prospects for detecting neutrinos above $10^{18}$~eV remain however challenging, unless proton acceleration to the highest energies is at play in a sub-dominant population of cosmic-ray sources or new physical phenomena are at work."],"url":"http://arxiv.org/abs/2402.04759v1","category":"astro-ph.HE"}
{"created":"2024-02-07 11:16:34","title":"Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation","abstract":"Semi-supervised segmentation methods have demonstrated promising results in natural scenarios, providing a solution to reduce dependency on manual annotation. However, these methods face significant challenges when directly applied to pathological images due to the subtle color differences between nuclei and tissues, as well as the significant morphological variations among nuclei. Consequently, the generated pseudo-labels often contain much noise, especially at the nuclei boundaries. To address the above problem, this paper proposes a boundary-aware contrastive learning network to denoise the boundary noise in a semi-supervised nuclei segmentation task. The model has two key designs: a low-resolution denoising (LRD) module and a cross-RoI contrastive learning (CRC) module. The LRD improves the smoothness of the nuclei boundary by pseudo-labels denoising, and the CRC enhances the discrimination between foreground and background by boundary feature contrastive learning. We conduct extensive experiments to demonstrate the superiority of our proposed method over existing semi-supervised instance segmentation methods.","sentences":["Semi-supervised segmentation methods have demonstrated promising results in natural scenarios, providing a solution to reduce dependency on manual annotation.","However, these methods face significant challenges when directly applied to pathological images due to the subtle color differences between nuclei and tissues, as well as the significant morphological variations among nuclei.","Consequently, the generated pseudo-labels often contain much noise, especially at the nuclei boundaries.","To address the above problem, this paper proposes a boundary-aware contrastive learning network to denoise the boundary noise in a semi-supervised nuclei segmentation task.","The model has two key designs: a low-resolution denoising (LRD) module and a cross-RoI contrastive learning (CRC) module.","The LRD improves the smoothness of the nuclei boundary by pseudo-labels denoising, and the CRC enhances the discrimination between foreground and background by boundary feature contrastive learning.","We conduct extensive experiments to demonstrate the superiority of our proposed method over existing semi-supervised instance segmentation methods."],"url":"http://arxiv.org/abs/2402.04756v1","category":"cs.CV"}
{"created":"2024-02-07 11:16:01","title":"Performance and first measurements of the MAGIC Stellar Intensity Interferometer","abstract":"In recent years, a new generation of optical intensity interferometers has emerged, leveraging the existing infrastructure of Imaging Atmospheric Cherenkov Telescopes (IACTs). The MAGIC telescopes host the MAGIC-SII system (Stellar Intensity Interferometer), implemented to investigate the feasibility and potential of this technique on IACTs. After the first successful measurements in 2019, the system was upgraded and now features a real-time, dead-time-free, 4-channel, GPU-based correlator. These hardware modifications allow seamless transitions between MAGIC's standard very-high-energy gamma-ray observations and optical interferometry measurements within seconds. We establish the feasibility and potential of employing IACTs as competitive optical Intensity Interferometers with minimal hardware adjustments. The measurement of a total of 22 stellar diameters are reported, 9 corresponding to reference stars with previous comparable measurements, and 13 with no prior measurements. A prospective implementation involving telescopes from the forthcoming Cherenkov Telescope Array Observatory's northern hemisphere array, such as the first prototype of its Large-Sized Telescopes, LST-1, is technically viable. This integration would significantly enhance the sensitivity of the current system and broaden the UV-plane coverage. This advancement would enable the system to achieve competitive sensitivity with the current generation of long-baseline optical interferometers over blue wavelengths.","sentences":["In recent years, a new generation of optical intensity interferometers has emerged, leveraging the existing infrastructure of Imaging Atmospheric Cherenkov Telescopes (IACTs).","The MAGIC telescopes host the MAGIC-SII system (Stellar Intensity Interferometer), implemented to investigate the feasibility and potential of this technique on IACTs.","After the first successful measurements in 2019, the system was upgraded and now features a real-time, dead-time-free, 4-channel, GPU-based correlator.","These hardware modifications allow seamless transitions between MAGIC's standard very-high-energy gamma-ray observations and optical interferometry measurements within seconds.","We establish the feasibility and potential of employing IACTs as competitive optical Intensity Interferometers with minimal hardware adjustments.","The measurement of a total of 22 stellar diameters are reported, 9 corresponding to reference stars with previous comparable measurements, and 13 with no prior measurements.","A prospective implementation involving telescopes from the forthcoming Cherenkov Telescope Array Observatory's northern hemisphere array, such as the first prototype of its Large-Sized Telescopes, LST-1, is technically viable.","This integration would significantly enhance the sensitivity of the current system and broaden the UV-plane coverage.","This advancement would enable the system to achieve competitive sensitivity with the current generation of long-baseline optical interferometers over blue wavelengths."],"url":"http://arxiv.org/abs/2402.04755v1","category":"astro-ph.IM"}
{"created":"2024-02-07 11:12:41","title":"Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints","abstract":"Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the $\\textbf{LA}$yout $\\textbf{C}$onstraint diffusion mod$\\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. For conditional generation, we introduce conditions via masked input. Extensive experiment results show that LACE produces high-quality layouts and outperforms existing state-of-the-art baselines.","sentences":["Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions.","Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models.","In this work, we propose the $\\textbf{LA}$yout $\\textbf{C}$onstraint diffusion mod$\\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design.","The model is based on continuous diffusion models.","Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training.","For conditional generation, we introduce conditions via masked input.","Extensive experiment results show that LACE produces high-quality layouts and outperforms existing state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.04754v1","category":"cs.CV"}
{"created":"2024-02-07 11:12:09","title":"Cortical Surface Diffusion Generative Models","abstract":"Cortical surface analysis has gained increased prominence, given its potential implications for neurological and developmental disorders. Traditional vision diffusion models, while effective in generating natural images, present limitations in capturing intricate development patterns in neuroimaging due to limited datasets. This is particularly true for generating cortical surfaces where individual variability in cortical morphology is high, leading to an urgent need for better methods to model brain development and diverse variability inherent across different individuals. In this work, we proposed a novel diffusion model for the generation of cortical surface metrics, using modified surface vision transformers as the principal architecture. We validate our method in the developing Human Connectome Project (dHCP), the results suggest our model demonstrates superior performance in capturing the intricate details of evolving cortical surfaces. Furthermore, our model can generate high-quality realistic samples of cortical surfaces conditioned on postmenstrual age(PMA) at scan.","sentences":["Cortical surface analysis has gained increased prominence, given its potential implications for neurological and developmental disorders.","Traditional vision diffusion models, while effective in generating natural images, present limitations in capturing intricate development patterns in neuroimaging due to limited datasets.","This is particularly true for generating cortical surfaces where individual variability in cortical morphology is high, leading to an urgent need for better methods to model brain development and diverse variability inherent across different individuals.","In this work, we proposed a novel diffusion model for the generation of cortical surface metrics, using modified surface vision transformers as the principal architecture.","We validate our method in the developing Human Connectome Project (dHCP), the results suggest our model demonstrates superior performance in capturing the intricate details of evolving cortical surfaces.","Furthermore, our model can generate high-quality realistic samples of cortical surfaces conditioned on postmenstrual age(PMA) at scan."],"url":"http://arxiv.org/abs/2402.04753v1","category":"eess.IV"}
{"created":"2024-02-07 11:10:24","title":"Homogenization of L\u00e9vy type operators with random, ergodic coefficients","abstract":"We study the problem of homogenization of a family of L\\'evy type processes $(X_t^{\\varepsilon,x;\\omega})_{t\\ge0}$, $\\varepsilon\\in(0,1]$, starting at $x\\in\\mathbb{R}^d$ and whose random Fourier symbols equal $q_\\varepsilon(x,\\xi;\\omega)=\\frac{1}{\\varepsilon^{\\alpha}}q\\Big(\\frac{x}{\\varepsilon},\\varepsilon\\xi;\\omega\\Big)$, where $$q(x,\\xi;\\omega)=-ib(x;\\omega)\\cdot\\xi+\\xi\\cdot a(x;\\omega)\\xi+\\int_{\\mathbb{R}^d}(1-e^{i y\\cdot\\xi}+i y \\cdot \\xi1_{\\{|y|\\le1\\}})n(x,dy;\\omega),$$ for $(x,\\xi,\\omega)\\in \\mathbb{R}^{2d}\\times\\Omega$. Here, $\\alpha\\in(0,2]$ and the L\\'evy triplet $(b(x;\\omega), a(x;\\omega), n(x,\\cdot;\\omega))_{x\\in \\mathbb{R}^d}$ is a stationary ergodic random field over some probability space $(\\Omega,\\mu)$. Our main assumptions are that: 1) for any $\\omega$, the operator $-q(\\cdot,D;\\omega)$, defined on $C^2_c(\\mathbb{R}^d)$, is closable in the space of continuous functions vanishing at infinity and its closure generates a Feller semigroup, 2) there exist constants $c_Q,C_Q>0$ independent of $(x,\\xi,\\omega)$, such that Re $q(x,\\xi;\\omega) \\ge c_Q|\\xi|^{\\alpha}$ for $\\xi\\in\\mathbb{R}^d$ and $|q(x,\\xi;\\omega)| \\le C_Q|\\xi|^{\\alpha}$ for $|\\xi|\\le 1$, and 3) $q_\\varepsilon(0,\\xi;\\omega)\\approx q_L(\\xi;\\omega)$, as $\\varepsilon\\to 0$, in $\\mu$-probability, where $q_L(\\xi;\\omega)$ is the Fourier symbol of some L\\'evy process for each $\\omega$. Under some technical assumptions, we show the weak convergence of the laws of $(X_t^{\\varepsilon,x;\\cdot})_{t\\ge0}$ in the Skorokhod space, as $\\varepsilon\\to0$, to a L\\'evy process whose Fourier symbol $\\bar q(\\xi)$ is given by $\\int_{\\Omega} q_L(\\xi)\\Phi_*d\\mu$, where $\\Phi_*$ is a strictly positive density w.r.t. $\\mu$. Our result has an analytic interpretation in terms of the convergence of the solutions of random integro-differential equations $\\partial_tu_\\varepsilon(t,x;\\omega)=-q_\\varepsilon(x,D;\\omega) u_\\varepsilon(t,x;\\omega)$.","sentences":["We study the problem of homogenization of a family of L\\'evy type processes $(X_t^{\\varepsilon,x;\\omega})_{t\\ge0}$, $\\varepsilon\\in(0,1]$, starting at $x\\in\\mathbb{R}^d$ and whose random Fourier symbols equal $q_\\varepsilon(x,\\xi;\\omega)=\\frac{1}{\\varepsilon^{\\alpha}}q\\Big(\\frac{x}{\\varepsilon},\\varepsilon\\xi;\\omega\\Big)$, where $$q(x,\\xi;\\omega)=-ib(x;\\omega)\\cdot\\xi+\\xi\\cdot a(x;\\omega)\\xi+\\int_{\\mathbb{R}^d}(1-e^{i y\\cdot\\xi}+i y \\cdot \\xi1_{\\{|y|\\le1\\}})n(x,dy;\\omega),$$ for $(x,\\xi,\\omega)\\in \\mathbb{R}^{2d}\\times\\Omega$. Here, $\\alpha\\in(0,2]$ and the L\\'evy triplet $(b(x;\\omega), a(x;\\omega), n(x,\\cdot;\\omega))_{x\\in \\mathbb{R}^d}$ is a stationary ergodic random field over some probability space $(\\Omega,\\mu)$. Our main assumptions are that: 1) for any $\\omega$, the operator $-q(\\cdot,D;\\omega)$, defined on $C^2_c(\\mathbb{R}^d)$, is closable in the space of continuous functions vanishing at infinity and its closure generates a Feller semigroup, 2) there exist constants $c_Q,C_Q>0$ independent of $(x,\\xi,\\omega)$, such that Re $q(x,\\xi;\\omega) \\ge c_Q|\\xi|^{\\alpha}$ for $\\xi\\in\\mathbb{R}^d$ and $|q(x,\\xi;\\omega)| \\le C_Q|\\xi|^{\\alpha}$ for $|\\xi|\\le 1$, and 3) $q_\\varepsilon(0,\\xi;\\omega)\\approx q_L(\\xi;\\omega)$, as $\\varepsilon\\to 0$, in $\\mu$-probability, where $q_L(\\xi;\\omega)$ is the Fourier symbol of some L\\'evy process for each $\\omega$. Under some technical assumptions, we show the weak convergence of the laws of $(X_t^{\\varepsilon,x;\\cdot})_{t\\ge0}$ in the Skorokhod space, as $\\varepsilon\\to0$, to a L\\'evy process whose Fourier symbol $\\bar q(\\xi)$ is given by $\\int_{\\Omega} q_L(\\xi)\\Phi_*d\\mu$, where $\\Phi_*$ is a strictly positive density w.r.t.","$\\mu$. Our result has an analytic interpretation in terms of the convergence of the solutions of random integro-differential equations $\\partial_tu_\\varepsilon(t,x;\\omega)=-q_\\varepsilon(x,D;\\omega) u_\\varepsilon(t,x;\\omega)$."],"url":"http://arxiv.org/abs/2402.04752v1","category":"math.PR"}
{"created":"2024-02-07 10:55:18","title":"Interference Simulator for the Whole HF Band: Application to CW-Morse","abstract":"In this paper, we use jointly a model of narrow band interference and a congestion model to model and implement an interference simulator for the whole HF band. The result is a model to generate interfering signals that could be found in a given frequency allocation, at a given time (past, present, or future) and for a given location. Our model does not require measurements and it is characterized by its ease of use and the freedom it offers to choose scene (modulation, location, week, year, etc.). In addition, we have defined a generic modulating function and the conditions to model a contact continuous wave (CW)-Morse, which meets the usual standards of contest. Consequently, our interference model in conjunction with the CW-Morse modulating function designed results in a specific CW-Morse model for amateur contests. As an example of the simulation model, we simulate the CW-Morse communications on the contest ARR ARRL Field Day 2011.","sentences":["In this paper, we use jointly a model of narrow band interference and a congestion model to model and implement an interference simulator for the whole HF band.","The result is a model to generate interfering signals that could be found in a given frequency allocation, at a given time (past, present, or future) and for a given location.","Our model does not require measurements and it is characterized by its ease of use and the freedom it offers to choose scene (modulation, location, week, year, etc.).","In addition, we have defined a generic modulating function and the conditions to model a contact continuous wave (CW)-Morse, which meets the usual standards of contest.","Consequently, our interference model in conjunction with the CW-Morse modulating function designed results in a specific CW-Morse model for amateur contests.","As an example of the simulation model, we simulate the CW-Morse communications on the contest ARR ARRL Field Day 2011."],"url":"http://arxiv.org/abs/2402.04742v1","category":"eess.SP"}
{"created":"2024-02-07 10:55:18","title":"Temperature evolution in the Early Universe and freeze-in at stronger coupling","abstract":"Dark matter freeze-in at stronger coupling is operative when the Standard Model (SM) bath temperature never exceeds the dark matter mass. An attractive feature of this scenario is that it can be probed by direct detection experiments as well as at the LHC. In this work, we show how the mechanism can be realized in a simple UV complete framework, emphasizing the role of the maximal temperature of the SM thermal bath. We demonstrate that the maximal temperature can coincide with the reheating temperature or be close to it such that dark matter production is always Boltzmann-suppressed. This possibility is realized, for example, if the inflaton decays primarily into feebly interacting right-handed neutrinos, which subsequently generate the SM thermal bath. In this case, the SM sector temperature remains constant over cosmological times prior to reheating.","sentences":["Dark matter freeze-in at stronger coupling is operative when the Standard Model (SM) bath temperature never exceeds the dark matter mass.","An attractive feature of this scenario is that it can be probed by direct detection experiments as well as at the LHC.","In this work, we show how the mechanism can be realized in a simple UV complete framework, emphasizing the role of the maximal temperature of the SM thermal bath.","We demonstrate that the maximal temperature can coincide with the reheating temperature or be close to it such that dark matter production is always Boltzmann-suppressed.","This possibility is realized, for example, if the inflaton decays primarily into feebly interacting right-handed neutrinos, which subsequently generate the SM thermal bath.","In this case, the SM sector temperature remains constant over cosmological times prior to reheating."],"url":"http://arxiv.org/abs/2402.04743v1","category":"hep-ph"}
{"created":"2024-02-07 10:42:39","title":"Randomized algorithms to generate hypergraphs with given degree sequences","abstract":"The question whether there exists a hypergraph whose degrees are equal to a given sequence of integers is a well-known reconstruction problem in graph theory, which is motivated by discrete tomography. In this paper we approach the problem by randomized algorithms which generate the required hypergraph with positive probability if the sequence satisfies certain constraints.","sentences":["The question whether there exists a hypergraph whose degrees are equal to a given sequence of integers is a well-known reconstruction problem in graph theory, which is motivated by discrete tomography.","In this paper we approach the problem by randomized algorithms which generate the required hypergraph with positive probability if the sequence satisfies certain constraints."],"url":"http://arxiv.org/abs/2402.04737v1","category":"math.CO"}
{"created":"2024-02-07 10:33:04","title":"Cuts and semidefinite liftings for the complex cut polytope","abstract":"We consider the complex cut polytope: the convex hull of Hermitian rank 1 matrices $xx^{\\mathrm{H}}$, where the elements of $x \\in \\mathbb{C}^n$ are $m$th unit roots. These polytopes have applications in ${\\text{MAX-3-CUT}}$, digital communication technology, angular synchronization and more generally, complex quadratic programming. For ${m=2}$, the complex cut polytope corresponds to the well-known cut polytope. We generalize valid cuts for this polytope to cuts for any complex cut polytope with finite $m>2$ and provide a framework to compare them. Further, we consider a second semidefinite lifting of the complex cut polytope for $m=\\infty$. This lifting is proven to be equivalent to other complex Lasserre-type liftings of the same order proposed in the literature, while being of smaller size. We also prove that a second semidefinite lifting of the complex cut polytope for $n = m=3$ is exact. Our theoretical findings are supported by numerical experiments on various optimization problems.","sentences":["We consider the complex cut polytope: the convex hull of Hermitian rank 1 matrices $xx^{\\mathrm{H}}$, where the elements of $x \\in \\mathbb{C}^n$ are $m$th unit roots.","These polytopes have applications in ${\\text{MAX-3-CUT}}$, digital communication technology, angular synchronization and more generally, complex quadratic programming.","For ${m=2}$, the complex cut polytope corresponds to the well-known cut polytope.","We generalize valid cuts for this polytope to cuts for any complex cut polytope with finite $m>2$ and provide a framework to compare them.","Further, we consider a second semidefinite lifting of the complex cut polytope for $m=\\infty$. This lifting is proven to be equivalent to other complex Lasserre-type liftings of the same order proposed in the literature, while being of smaller size.","We also prove that a second semidefinite lifting of the complex cut polytope for $n = m=3$ is exact.","Our theoretical findings are supported by numerical experiments on various optimization problems."],"url":"http://arxiv.org/abs/2402.04731v1","category":"math.OC"}
{"created":"2024-02-07 10:28:19","title":"Data-driven Bayesian estimation of Monod kinetics","abstract":"In this paper, we consider the well known problem of non-linear identification of the rates of the reactions involved in cells with Monod functions. In bioprocesses, generating data is very expensive and long and so it is important to incorporate prior knowledge on the Monod kinetic parameters. Bayesian estimation is an elegant estimation technique which deals with parameter estimation with prior knowledge modeled as probability density functions. However, we might not have an accurate knowledge of the kinetic parameters such as interval bounds, especially for newly developed cell lines. Hence, we consider the case when there is no accurate prior information on the kinetic parameters except qualitative knowledge such that their non-negativity. A log-Gaussian prior distribution is considered for the parameters and the mean and variances of these distribution are tuned using the Expectation Maximization algorithm. The algorithm requires to use Metropolis Hastings within Gibbs sampling which can be computationally expensive. We develop a novel variant of the Metropolis-Hastings within Gibbs sampling sampling scheme in order to accelerate and improve on the hyperparameter tuning. We show that it can give better modeling performances on a relatively large-scale simulation example compared to available methods in the literature.","sentences":["In this paper, we consider the well known problem of non-linear identification of the rates of the reactions involved in cells with Monod functions.","In bioprocesses, generating data is very expensive and long and so it is important to incorporate prior knowledge on the Monod kinetic parameters.","Bayesian estimation is an elegant estimation technique which deals with parameter estimation with prior knowledge modeled as probability density functions.","However, we might not have an accurate knowledge of the kinetic parameters such as interval bounds, especially for newly developed cell lines.","Hence, we consider the case when there is no accurate prior information on the kinetic parameters except qualitative knowledge such that their non-negativity.","A log-Gaussian prior distribution is considered for the parameters and the mean and variances of these distribution are tuned using the Expectation Maximization algorithm.","The algorithm requires to use Metropolis Hastings within Gibbs sampling which can be computationally expensive.","We develop a novel variant of the Metropolis-Hastings within Gibbs sampling sampling scheme in order to accelerate and improve on the hyperparameter tuning.","We show that it can give better modeling performances on a relatively large-scale simulation example compared to available methods in the literature."],"url":"http://arxiv.org/abs/2402.04727v1","category":"stat.ME"}
{"created":"2024-02-07 10:23:24","title":"Out of the Dark: WISPs in String Theory and the Early Universe","abstract":"New light hidden sector degrees of freedom represent one of the most approaches to going beyond the Standard Model. I give a short account of how such WISP candidates naturally appear in string compactifications and some descriptions of ways that they can affect early universe cosmology.","sentences":["New light hidden sector degrees of freedom represent one of the most approaches to going beyond the Standard Model.","I give a short account of how such WISP candidates naturally appear in string compactifications and some descriptions of ways that they can affect early universe cosmology."],"url":"http://arxiv.org/abs/2402.04725v1","category":"hep-th"}
{"created":"2024-02-07 10:16:20","title":"Ten simple rules for teaching sustainable software engineering","abstract":"Computational methods and associated software implementations are central to every field of scientific investigation. Modern biological research, particularly within systems biology, has relied heavily on the development of software tools to process and organize increasingly large datasets, simulate complex mechanistic models, provide tools for the analysis and management of data, and visualize and organize outputs. However, developing high-quality research software requires scientists to develop a host of software development skills, and teaching these skills to students is challenging. There has been a growing importance placed on ensuring reproducibility and good development practices in computational research. However, less attention has been devoted to informing the specific teaching strategies which are effective at nurturing in researchers the complex skillset required to produce high-quality software that, increasingly, is required to underpin both academic and industrial biomedical research. Recent articles in the Ten Simple Rules collection have discussed the teaching of foundational computer science and coding techniques to biology students. We advance this discussion by describing the specific steps for effectively teaching the necessary skills scientists need to develop sustainable software packages which are fit for (re-)use in academic research or more widely. Although our advice is likely to be applicable to all students and researchers hoping to improve their software development skills, our guidelines are directed towards an audience of students that have some programming literacy but little formal training in software development or engineering, typical of early doctoral students. These practices are also applicable outside of doctoral training environments, and we believe they should form a key part of postgraduate training schemes more generally in the life sciences.","sentences":["Computational methods and associated software implementations are central to every field of scientific investigation.","Modern biological research, particularly within systems biology, has relied heavily on the development of software tools to process and organize increasingly large datasets, simulate complex mechanistic models, provide tools for the analysis and management of data, and visualize and organize outputs.","However, developing high-quality research software requires scientists to develop a host of software development skills, and teaching these skills to students is challenging.","There has been a growing importance placed on ensuring reproducibility and good development practices in computational research.","However, less attention has been devoted to informing the specific teaching strategies which are effective at nurturing in researchers the complex skillset required to produce high-quality software that, increasingly, is required to underpin both academic and industrial biomedical research.","Recent articles in the Ten Simple Rules collection have discussed the teaching of foundational computer science and coding techniques to biology students.","We advance this discussion by describing the specific steps for effectively teaching the necessary skills scientists need to develop sustainable software packages which are fit for (re-)use in academic research or more widely.","Although our advice is likely to be applicable to all students and researchers hoping to improve their software development skills, our guidelines are directed towards an audience of students that have some programming literacy but little formal training in software development or engineering, typical of early doctoral students.","These practices are also applicable outside of doctoral training environments, and we believe they should form a key part of postgraduate training schemes more generally in the life sciences."],"url":"http://arxiv.org/abs/2402.04722v1","category":"cs.CY"}
{"created":"2024-02-07 10:09:00","title":"InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior","abstract":"Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce InstructScene, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis. The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and multimodal models. Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin. Thorough ablation studies confirm the efficacy of crucial design components. Project page: https://chenguolin.github.io/projects/InstructScene.","sentences":["Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems.","Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation.","We introduce InstructScene, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis.","The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a zero-shot manner.","To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and multimodal models.","Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin.","Thorough ablation studies confirm the efficacy of crucial design components.","Project page: https://chenguolin.github.io/projects/InstructScene."],"url":"http://arxiv.org/abs/2402.04717v1","category":"cs.CV"}
{"created":"2024-02-07 10:08:53","title":"Observation of scale-free localized states induced by non-Hermitian defects","abstract":"Wave localization is a fundamental phenomenon that appears universally in both natural materials and artificial structures and plays a crucial role in understanding the various physical properties of a system. Usually, a localized state has an exponential profile with a localization length independent of the system size. Here, we experimentally demonstrate a new class of localized states called scale-free localized states, which has an unfixed localization length scaling linearly with the system size. Using circuit lattices, we observe that a non-Hermitian defect added to a Hermitian lattice induces an extensive number of states with scale-free localization. Furthermore, we demonstrate that, in a lattice with a parity-time-symmetric non-Hermitian defect, the scale-free localization emerges because of spontaneous parity-time symmetry breaking. Our results uncover a new type of localized states and extend the study of defect physics to the non-Hermitian regime.","sentences":["Wave localization is a fundamental phenomenon that appears universally in both natural materials and artificial structures and plays a crucial role in understanding the various physical properties of a system.","Usually, a localized state has an exponential profile with a localization length independent of the system size.","Here, we experimentally demonstrate a new class of localized states called scale-free localized states, which has an unfixed localization length scaling linearly with the system size.","Using circuit lattices, we observe that a non-Hermitian defect added to a Hermitian lattice induces an extensive number of states with scale-free localization.","Furthermore, we demonstrate that, in a lattice with a parity-time-symmetric non-Hermitian defect, the scale-free localization emerges because of spontaneous parity-time symmetry breaking.","Our results uncover a new type of localized states and extend the study of defect physics to the non-Hermitian regime."],"url":"http://arxiv.org/abs/2402.04716v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 10:05:42","title":"Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search","abstract":"We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS). We introduce novel concepts: $b\\textit{-monotonic path}$ and $B\\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET. We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work. Empirically, we validate the method's effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with out-of-distribution data and hard instances. Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications.","sentences":["We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS).","We introduce novel concepts: $b\\textit{-monotonic path}$ and $B\\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET.","We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work.","Empirically, we validate the method's effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with out-of-distribution data and hard instances.","Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications."],"url":"http://arxiv.org/abs/2402.04713v1","category":"cs.IR"}
{"created":"2024-02-07 10:02:22","title":"A note on Weyl gauge symmetry in gravity","abstract":"A scale invariant theory of gravity, containing at most two derivatives, requires, in addition to the Riemannian metric, a scalar field and (or) a gauge field. The gauge field is usualy used to construct the affine connection of Weyl geometry. In this note, we incorporate both the gauge field and the scalar field to build a generalised scale invariant Weyl affine connection. The Ricci tensor and the Ricci scalar made out of this generalised Weyl affine connection contain, naturally, kinetic terms for the scalar field and the gauge field. This provides a geometric interpretation for these terms. It is also shown that scale invariance in the presence of a cosmological constant and mass terms is not completely lost. It becomes a duality transformation relating various fields.","sentences":["A scale invariant theory of gravity, containing at most two derivatives, requires, in addition to the Riemannian metric, a scalar field and (or) a gauge field.","The gauge field is usualy used to construct the affine connection of Weyl geometry.","In this note, we incorporate both the gauge field and the scalar field to build a generalised scale invariant Weyl affine connection.","The Ricci tensor and the Ricci scalar made out of this generalised Weyl affine connection contain, naturally, kinetic terms for the scalar field and the gauge field.","This provides a geometric interpretation for these terms.","It is also shown that scale invariance in the presence of a cosmological constant and mass terms is not completely lost.","It becomes a duality transformation relating various fields."],"url":"http://arxiv.org/abs/2402.04712v1","category":"hep-th"}
{"created":"2024-02-07 09:52:49","title":"Anatomy of $\\tilde{\\mathbb{E}}$","abstract":"We present a detailed general framework to describe the forcing $\\tilde{\\mathbb{E}}$, defined by Kellner, Shelah and Tan\\u{a}sie to prove the consistency with ZFC of an alternative order of Cicho\\'n's maximum. Our presentation is close to the framework of tree-creature forcing notions from Horowitz and Shelah. We show that the posets in this class have strong FAM limits for intervals (in recent terminology, they are $\\sigma$-FAM-linked) and, furthermore, that they also have strong ultrafilter limits for intervals.","sentences":["We present a detailed general framework to describe the forcing $\\tilde{\\mathbb{E}}$, defined by Kellner, Shelah and Tan\\u{a}sie to prove the consistency with ZFC of an alternative order of Cicho\\'n's maximum.","Our presentation is close to the framework of tree-creature forcing notions from Horowitz and Shelah.","We show that the posets in this class have strong FAM limits for intervals (in recent terminology, they are $\\sigma$-FAM-linked) and, furthermore, that they also have strong ultrafilter limits for intervals."],"url":"http://arxiv.org/abs/2402.04706v1","category":"math.LO"}
{"created":"2024-02-07 09:39:29","title":"EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions","abstract":"Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms. We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios. Project Website can be accessed at \\url{https://shashankkotyan.github.io/EvoSeed}.","sentences":["Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified.","Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution.","To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples.","Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting.","We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model.","Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers.","Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms.","We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios.","Project Website can be accessed at \\url{https://shashankkotyan.github.io/EvoSeed}."],"url":"http://arxiv.org/abs/2402.04699v1","category":"cs.CV"}
{"created":"2024-02-07 09:37:14","title":"Run-and-tumble motion of ellipsoidal swimmers","abstract":"A hallmark of bacteria is their so-called \"run-and-tumble\" motion, consisting of a sequence of linear directed \"runs\" and random rotations that constantly alternate due to biochemical feedback. It plays a crucial role in the ability of bacteria to move through chemical gradients and inspired a fundamental active particle model. Nevertheless, synthetic active particles generally do not exhibit run-and-tumble motion but rather active Brownian motion. We show in experiments that ellipsoidal thermophoretic Janus particles, propelling along their short axis, can yield run-and-tumble-like motion even without feedback. Their hydrodynamic wall interactions under strong confinement give rise to an effective double-well potential for the declination of the short axis. The geometry-induced timescale separation of the in-plane rotational dynamics and noise-induced transitions in the potential then yields run-and-tumble-like motion.","sentences":["A hallmark of bacteria is their so-called \"run-and-tumble\" motion, consisting of a sequence of linear directed \"runs\" and random rotations that constantly alternate due to biochemical feedback.","It plays a crucial role in the ability of bacteria to move through chemical gradients and inspired a fundamental active particle model.","Nevertheless, synthetic active particles generally do not exhibit run-and-tumble motion but rather active Brownian motion.","We show in experiments that ellipsoidal thermophoretic Janus particles, propelling along their short axis, can yield run-and-tumble-like motion even without feedback.","Their hydrodynamic wall interactions under strong confinement give rise to an effective double-well potential for the declination of the short axis.","The geometry-induced timescale separation of the in-plane rotational dynamics and noise-induced transitions in the potential then yields run-and-tumble-like motion."],"url":"http://arxiv.org/abs/2402.04697v1","category":"cond-mat.soft"}
{"created":"2024-02-07 09:35:58","title":"Nash Equilibria in Reverse Temporal Voronoi Games","abstract":"We study Voronoi games on temporal graphs as introduced by Boehmer et al. (IJCAI 2021) where two players each select a vertex in a temporal graph with the goal of reaching the other vertices earlier than the other player. In this work, we consider the reverse temporal Voronoi game, that is, a player wants to maximize the number of vertices reaching her earlier than the other player. Since temporal distances in temporal graphs are not symmetric in general, this yields a different game. We investigate the difference between the two games with respect to the existence of Nash equilibria in various temporal graph classes including temporal trees, cycles, grids, cliques and split graphs. Our extensive results show that the two games indeed behave quite differently depending on the considered temporal graph class.","sentences":["We study Voronoi games on temporal graphs as introduced by Boehmer et al.","(IJCAI 2021) where two players each select a vertex in a temporal graph with the goal of reaching the other vertices earlier than the other player.","In this work, we consider the reverse temporal Voronoi game, that is, a player wants to maximize the number of vertices reaching her earlier than the other player.","Since temporal distances in temporal graphs are not symmetric in general, this yields a different game.","We investigate the difference between the two games with respect to the existence of Nash equilibria in various temporal graph classes including temporal trees, cycles, grids, cliques and split graphs.","Our extensive results show that the two games indeed behave quite differently depending on the considered temporal graph class."],"url":"http://arxiv.org/abs/2402.04696v1","category":"cs.GT"}
{"created":"2024-02-07 09:33:05","title":"Positron lifetime calculations for defects in Zn","abstract":"The effect of the lattice relaxation at vacancy clusters and interstitial-type dislocation loops on the lifetime of positrons in Zn has been studied. Defective relaxed structures have been generated for the lifetime calculations by using a many-body potential for Zn. From the results, it is inferred that the effect of the atomic relaxation is mainly significant for small vacancy clusters. The lifetime associated with interstitial-type loops is very sensitive to the loop structure and its surroundings. Previous experimental results are compared with the theoretical calculations.","sentences":["The effect of the lattice relaxation at vacancy clusters and interstitial-type dislocation loops on the lifetime of positrons in Zn has been studied.","Defective relaxed structures have been generated for the lifetime calculations by using a many-body potential for Zn.","From the results, it is inferred that the effect of the atomic relaxation is mainly significant for small vacancy clusters.","The lifetime associated with interstitial-type loops is very sensitive to the loop structure and its surroundings.","Previous experimental results are compared with the theoretical calculations."],"url":"http://arxiv.org/abs/2402.04693v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 09:31:01","title":"Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces","abstract":"This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing literature.","sentences":["This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces.","We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity.","Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm.","It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning.","We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator.","Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing literature."],"url":"http://arxiv.org/abs/2402.04691v1","category":"stat.ML"}
{"created":"2024-02-07 09:14:12","title":"Gravitational decoupling and aerodynamics: black holes and analog gravity in a jet propulsion lab","abstract":"A connection is established between transonic sound waves propagating along a de Laval nozzle and quasinormal modes emitted from hairy black holes obtained with the gravitational decoupling method applied to the Reissner-Nordstr\\\"om geometry. Aerodynamical features provide an analogue setup to experimentally test fluid flow perturbations in a de Laval nozzle producing quasinormal modes. In particular, nozzle shape, pressure, Mach number, temperature, density, and thrust coefficient profiles are determined as functions of the black hole parameters for several multipole numbers. The black hole quasinormal mode frequencies are also investigated for different overtones, evaluating the quality factor of the nozzle.","sentences":["A connection is established between transonic sound waves propagating along a de Laval nozzle and quasinormal modes emitted from hairy black holes obtained with the gravitational decoupling method applied to the Reissner-Nordstr\\\"om geometry.","Aerodynamical features provide an analogue setup to experimentally test fluid flow perturbations in a de Laval nozzle producing quasinormal modes.","In particular, nozzle shape, pressure, Mach number, temperature, density, and thrust coefficient profiles are determined as functions of the black hole parameters for several multipole numbers.","The black hole quasinormal mode frequencies are also investigated for different overtones, evaluating the quality factor of the nozzle."],"url":"http://arxiv.org/abs/2402.04682v1","category":"gr-qc"}
{"created":"2024-02-07 09:13:26","title":"Architectural Design Decisions for Self-Serve Data Platforms in Data Meshes","abstract":"Data mesh is an emerging decentralized approach to managing and generating value from analytical enterprise data at scale. It shifts the ownership of the data to the business domains closest to the data, promotes sharing and managing data as autonomous products, and uses a federated and automated data governance model. The data mesh relies on a managed data platform that offers services to domain and governance teams to build, share, and manage data products efficiently. However, designing and implementing a self-serve data platform is challenging, and the platform engineers and architects must understand and choose the appropriate design options to ensure the platform will enhance the experience of domain and governance teams. For these reasons, this paper proposes a catalog of architectural design decisions and their corresponding decision options by systematically reviewing 43 industrial gray literature articles on self-serve data platforms in data mesh. Moreover, we used semi-structured interviews with six data engineering experts with data mesh experience to validate, refine, and extend the findings from the literature. Such a catalog of design decisions and options drawn from the state of practice shall aid practitioners in building data meshes while providing a baseline for further research on data mesh architectures.","sentences":["Data mesh is an emerging decentralized approach to managing and generating value from analytical enterprise data at scale.","It shifts the ownership of the data to the business domains closest to the data, promotes sharing and managing data as autonomous products, and uses a federated and automated data governance model.","The data mesh relies on a managed data platform that offers services to domain and governance teams to build, share, and manage data products efficiently.","However, designing and implementing a self-serve data platform is challenging, and the platform engineers and architects must understand and choose the appropriate design options to ensure the platform will enhance the experience of domain and governance teams.","For these reasons, this paper proposes a catalog of architectural design decisions and their corresponding decision options by systematically reviewing 43 industrial gray literature articles on self-serve data platforms in data mesh.","Moreover, we used semi-structured interviews with six data engineering experts with data mesh experience to validate, refine, and extend the findings from the literature.","Such a catalog of design decisions and options drawn from the state of practice shall aid practitioners in building data meshes while providing a baseline for further research on data mesh architectures."],"url":"http://arxiv.org/abs/2402.04681v1","category":"cs.SE"}
{"created":"2024-02-07 09:11:05","title":"2-categorical approach to unifying constructions of precoverings and its applications","abstract":"Throughout this paper $G$ is a fixed group, and $k$ is a fixed field. All categories are assumed to be $k$-linear. First we give a systematic way to induce $G$-precoverings by adjoint functors using a 2-categorical machinery, which unifies many similar constructions of $G$-precoverings.   Now let $\\mathcal{C}$ be a skeletally small category with a $G$-action, $\\mathcal{C}/G$ the orbit category of $\\mathcal{C}$, $(P, \\phi) : \\mathcal{C} \\rightarrow \\mathcal{C}/G$ the canonical $G$-covering, and $\\mathrm{mod}\\mbox{-} \\mathcal{C}$, $\\mathrm{mod}\\mbox{-} (\\mathcal{C}/G)$ the categories of finitely generated modules over $\\mathcal{C}, \\mathcal{C}/G$, respectively. Then it is well known that there exists a canonical G-precovering $(P., \\phi.) : \\mathrm{mod}\\mbox{-} \\mathcal{C} \\rightarrow \\mathrm{mod}\\mbox{-} (\\mathcal{C}/G)$. By applying the machinery above to this $(P., \\phi.)$, new $G$-precoverings $(\\mathrm{mod}\\mbox{-} \\mathcal{C}) / S \\rightarrow (\\mathrm{mod}\\mbox{-} \\mathcal{C}/G)/S'$ are induced between the factor categories or localizations of $\\mathrm{mod}\\mbox{-} \\mathcal{C}$ and $\\mathrm{mod}\\mbox{-} \\mathcal{C}/G$, respectively.   This is further applied to the morphism category $\\mathrm{H}(\\mathrm{mod}\\mbox{-} \\mathcal{C})$ of $\\mathrm{mod}\\mbox{-} \\mathcal{C}$ to have a $G$-precovering $\\mathrm{fp}(\\mathcal{K}) \\rightarrow \\mathrm{fp}(\\mathcal{K}')$ between the categories of finitely presented modules over suitable subcategories $\\mathcal{K}$ and $\\mathcal{K}'$ of $\\mathrm{mod}\\mbox{-}\\mathcal{C}$ and $ \\mathrm{mod}\\mbox{-} \\mathcal{C}/G$, respectively.","sentences":["Throughout this paper $G$ is a fixed group, and $k$ is a fixed field.","All categories are assumed to be $k$-linear.","First we give a systematic way to induce $G$-precoverings by adjoint functors using a 2-categorical machinery, which unifies many similar constructions of $G$-precoverings.   ","Now let $\\mathcal{C}$ be a skeletally small category with a $G$-action, $\\mathcal{C}/G$ the orbit category of $\\mathcal{C}$, $(P, \\phi) : \\mathcal{C} \\rightarrow \\mathcal{C}/G$ the canonical $G$-covering, and $\\mathrm{mod}\\mbox{-} \\mathcal{C}$, $\\mathrm{mod}\\mbox{-} (\\mathcal{C}/G)$ the categories of finitely generated modules over $\\mathcal{C}, \\mathcal{C}/G$, respectively.","Then it is well known that there exists a canonical G-precovering $(P., \\phi.)",": \\mathrm{mod}\\mbox{-} \\mathcal{C} \\rightarrow \\mathrm{mod}\\mbox{-} (\\mathcal{C}/G)$. By applying the machinery above to this $(P., \\phi.)$, new $G$-precoverings $(\\mathrm{mod}\\mbox{-} \\mathcal{C}) / S \\rightarrow (\\mathrm{mod}\\mbox{-} \\mathcal{C}/G)/S'$ are induced between the factor categories or localizations of $\\mathrm{mod}\\mbox{-} \\mathcal{C}$ and $\\mathrm{mod}\\mbox{-} \\mathcal{C}/G$, respectively.   ","This is further applied to the morphism category $\\mathrm{H}(\\mathrm{mod}\\mbox{-} \\mathcal{C})$ of $\\mathrm{mod}\\mbox{-} \\mathcal{C}$ to have","a $G$-precovering $\\mathrm{fp}(\\mathcal{K}) \\rightarrow \\mathrm{fp}(\\mathcal{K}')$ between the categories of finitely presented modules over suitable subcategories $\\mathcal{K}$ and $\\mathcal{K}'$ of $\\mathrm{mod}\\mbox{-}\\mathcal{C}$ and $ \\mathrm{mod}\\mbox{-} \\mathcal{C}/G$, respectively."],"url":"http://arxiv.org/abs/2402.04680v1","category":"math.RT"}
{"created":"2024-02-07 09:09:21","title":"Field transformations in functional integral, effective action and functional flow equations","abstract":"Field transformations for the quantum effective action lead to different pictures of a given physical situation, as describing a given evolution of the universe by different geometries. Field transformations for functional flow equations can reveal important physical features, as the appearance of bound states. They also allow for technical simplifications. We make a critical assessment of the virtues and shortcomings of different versions of field transformations. Key issues are demonstrated by the quantum field theory for scalars with a field-dependent coefficient of the kinetic term. Our findings confirm the principle of field relativity for gravity and cosmology based on the quantum effective action.","sentences":["Field transformations for the quantum effective action lead to different pictures of a given physical situation, as describing a given evolution of the universe by different geometries.","Field transformations for functional flow equations can reveal important physical features, as the appearance of bound states.","They also allow for technical simplifications.","We make a critical assessment of the virtues and shortcomings of different versions of field transformations.","Key issues are demonstrated by the quantum field theory for scalars with a field-dependent coefficient of the kinetic term.","Our findings confirm the principle of field relativity for gravity and cosmology based on the quantum effective action."],"url":"http://arxiv.org/abs/2402.04679v1","category":"hep-th"}
{"created":"2024-02-07 09:09:14","title":"Large Language Models As Faithful Explainers","abstract":"Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the faithfulness scores. Experiments conducted on three NLU datasets demonstrate that xLLM can significantly improve the faithfulness of generated explanations, which are in alignment with the behaviors of LLMs.","sentences":["Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability.","Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs.","Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format.","However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs.","In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs.","Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the faithfulness scores.","Experiments conducted on three NLU datasets demonstrate that xLLM can significantly improve the faithfulness of generated explanations, which are in alignment with the behaviors of LLMs."],"url":"http://arxiv.org/abs/2402.04678v1","category":"cs.CL"}
{"created":"2024-02-07 09:09:09","title":"Source Identification in Abstractive Summarization","abstract":"Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as $\\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings. Our code and data are available at https://github.com/suhara/sourcesum.","sentences":["Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries.","In this paper, we define input sentences that contain essential information in the generated summary as $\\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences.","To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets.","We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task.","Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings.","Our code and data are available at https://github.com/suhara/sourcesum."],"url":"http://arxiv.org/abs/2402.04677v1","category":"cs.CL"}
{"created":"2024-02-07 09:03:04","title":"Group Distributionally Robust Dataset Distillation with Risk Minimization","abstract":"Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from regions with low population density? Here, the representativeness and coverage of the dataset become salient over the guaranteed training error at inference. Drawing inspiration from distributionally robust optimization, we introduce an algorithm that combines clustering with the minimization of a risk measure on the loss to conduct DD. We provide a theoretical rationale for our approach and demonstrate its effective generalization and robustness across subgroups through numerical experiments.","sentences":["Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models.","Its applications span various domains, including transfer learning, federated learning, and neural architecture search.","The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset.","However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest.","Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups.","That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from regions with low population density?","Here, the representativeness and coverage of the dataset become salient over the guaranteed training error at inference.","Drawing inspiration from distributionally robust optimization, we introduce an algorithm that combines clustering with the minimization of a risk measure on the loss to conduct DD.","We provide a theoretical rationale for our approach and demonstrate its effective generalization and robustness across subgroups through numerical experiments."],"url":"http://arxiv.org/abs/2402.04676v1","category":"cs.LG"}
{"created":"2024-02-07 08:57:59","title":"G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection","abstract":"In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS.","sentences":["In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains.","In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity.","Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting","and we propose to leverage Differentiable NAS to solve S-DGOD.","However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data.","Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains.","To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS.","Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods.","Codes are available at https://github.com/wufan-cse/G-NAS."],"url":"http://arxiv.org/abs/2402.04672v1","category":"cs.CV"}
{"created":"2024-02-07 08:55:57","title":"V2VSSC: A 3D Semantic Scene Completion Benchmark for Perception with Vehicle to Vehicle Communication","abstract":"Semantic scene completion (SSC) has recently gained popularity because it can provide both semantic and geometric information that can be used directly for autonomous vehicle navigation. However, there are still challenges to overcome. SSC is often hampered by occlusion and short-range perception due to sensor limitations, which can pose safety risks. This paper proposes a fundamental solution to this problem by leveraging vehicle-to-vehicle (V2V) communication. We propose the first generalized collaborative SSC framework that allows autonomous vehicles to share sensing information from different sensor views to jointly perform SSC tasks. To validate the proposed framework, we further build V2VSSC, the first V2V SSC benchmark, on top of the large-scale V2V perception dataset OPV2V. Extensive experiments demonstrate that by leveraging V2V communication, the SSC performance can be increased by 8.3% on geometric metric IoU and 6.0% mIOU.","sentences":["Semantic scene completion (SSC) has recently gained popularity because it can provide both semantic and geometric information that can be used directly for autonomous vehicle navigation.","However, there are still challenges to overcome.","SSC is often hampered by occlusion and short-range perception due to sensor limitations, which can pose safety risks.","This paper proposes a fundamental solution to this problem by leveraging vehicle-to-vehicle (V2V) communication.","We propose the first generalized collaborative SSC framework that allows autonomous vehicles to share sensing information from different sensor views to jointly perform SSC tasks.","To validate the proposed framework, we further build V2VSSC, the first V2V SSC benchmark, on top of the large-scale V2V perception dataset OPV2V. Extensive experiments demonstrate that by leveraging V2V communication, the SSC performance can be increased by 8.3% on geometric metric IoU and 6.0% mIOU."],"url":"http://arxiv.org/abs/2402.04671v1","category":"cs.CV"}
{"created":"2024-02-07 08:51:57","title":"CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks","abstract":"Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information. However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism. The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output. CLIF is hyperparameter-free and features broad applicability. Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models. Moreover, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions.","sentences":["Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models.","Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information.","However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism.","The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart.","We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs.","Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF)","Neuron.","CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output.","CLIF is hyperparameter-free and features broad applicability.","Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models.","Moreover, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions."],"url":"http://arxiv.org/abs/2402.04663v1","category":"cs.NE"}
{"created":"2024-02-07 08:51:50","title":"Token vs Equity for Startup Financing","abstract":"Why would a blockchain-based startup and its venture capital investors choose to finance by issuing tokens instead of equity? What would be their rates of return for each asset? This paper focuses on the liquidity difference between the two fundraising methods. I build a three-period model of an entrepreneur, two types of investors, and users. Some investors have unforeseen liquidity needs in the middle period that can only be met with tokens. The entrepreneur obtains higher payoff by issuing tokens instead of equity, and the payoff difference increases with investors risk-aversion and need for liquidity in the middle period, as well as the depth of the token market.","sentences":["Why would a blockchain-based startup and its venture capital investors choose to finance by issuing tokens instead of equity?","What would be their rates of return for each asset?","This paper focuses on the liquidity difference between the two fundraising methods.","I build a three-period model of an entrepreneur, two types of investors, and users.","Some investors have unforeseen liquidity needs in the middle period that can only be met with tokens.","The entrepreneur obtains higher payoff by issuing tokens instead of equity, and the payoff difference increases with investors risk-aversion and need for liquidity in the middle period, as well as the depth of the token market."],"url":"http://arxiv.org/abs/2402.04662v1","category":"q-fin.GN"}
{"created":"2024-02-07 08:49:33","title":"Adversarial Robustness Through Artifact Design","abstract":"Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) and their colors. We found that, combined with adversarial training, our approach led to up to 25.18\\% higher robust accuracy compared to state-of-the-art methods against two adversary types, while further increasing accuracy on benign inputs.","sentences":["Adversarial examples arose as a challenge for machine learning.","To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing).","Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples.","Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness.","Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples.","We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it.","We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) and their colors.","We found that, combined with adversarial training, our approach led to up to 25.18\\% higher robust accuracy compared to state-of-the-art methods against two adversary types, while further increasing accuracy on benign inputs."],"url":"http://arxiv.org/abs/2402.04660v1","category":"cs.CR"}
{"created":"2024-02-07 08:44:01","title":"SPLEND1D, a reduced one-dimensional model to investigate the physics of plasma detachment","abstract":"Studying the process of divertor detachment and the associated complex interplay of plasma dynamics and atomic physics processes is of utmost importance for future fusion reactors. Whilst simplified analytical models exist to interpret the general features of detachment, they are limited in their predictive power, and complex 2D or even 3D codes are generally required to provide a self-consistent picture of the divertor. As an intermediate step, 1D models of the Scrape-Off Layer (SOL) can be particularly insightful as the dynamics are greatly simplified, while still self-consistently including various source and sink terms at play, as well as additional important effects such as flows. These codes can be used to shed light on the physics at play, to perform fast parameter scans, or to interpret experiments. In this paper, we introduce the SPLEND1D (Simulator of PLasma ENabling Detachment in 1D) code: a fast and versatile 1D SOL model. We present in detail the model that is implemented in SPLEND1D. We then employ the code to explore various elements of detachment physics for parameters typical of the Tokamak \\`a Configuration Variable (TCV), including the atomic physics and other processes behind power and momentum losses, and explore the various hypotheses and free parameters of the model.","sentences":["Studying the process of divertor detachment and the associated complex interplay of plasma dynamics and atomic physics processes is of utmost importance for future fusion reactors.","Whilst simplified analytical models exist to interpret the general features of detachment, they are limited in their predictive power, and complex 2D or even 3D codes are generally required to provide a self-consistent picture of the divertor.","As an intermediate step, 1D models of the Scrape-Off Layer (SOL) can be particularly insightful as the dynamics are greatly simplified, while still self-consistently including various source and sink terms at play, as well as additional important effects such as flows.","These codes can be used to shed light on the physics at play, to perform fast parameter scans, or to interpret experiments.","In this paper, we introduce the SPLEND1D (Simulator of PLasma ENabling Detachment in 1D) code: a fast and versatile 1D SOL model.","We present in detail the model that is implemented in SPLEND1D.","We then employ the code to explore various elements of detachment physics for parameters typical of the Tokamak \\`a Configuration Variable (TCV), including the atomic physics and other processes behind power and momentum losses, and explore the various hypotheses and free parameters of the model."],"url":"http://arxiv.org/abs/2402.04656v1","category":"physics.plasm-ph"}
{"created":"2024-02-07 08:42:48","title":"Open-Vocabulary Calibration for Vision-Language Models","abstract":"Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed.","sentences":["Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few.","In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning.","However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world.","This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting.","To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes.","The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed."],"url":"http://arxiv.org/abs/2402.04655v1","category":"cs.LG"}
{"created":"2024-02-07 08:36:29","title":"Quantum Correlation Sharing: A Review On Recent Progress From Nonlocality To Other Non-Classical Correlations","abstract":"This review offers a comprehensive exploration and synthesis of recent advancements in the domain of quantum correlation sharing facilitated through sequential measurements. We initiate our inquiry by delving into the interpretation of the joint probability, laying the foundation for an examination of quantum correlations within the context of specific measurement methods. The subsequent section meticulously explores nonlocal sharing under diverse measurement strategies and scenarios, with a specific focus on investigating the impact of these strategies on the dissemination of quantum nonlocality. Key perspectives such as \"asymmetry\" and \"weak value\" are scrutinized through detailed analyses across various scenarios, allowing us to evaluate the potential of nonlocality sharing. We also provide a retrospective overview of experimental endeavors associated with this phenomenon. The third part of our exploration presents research findings on steering sharing, offering clarity on the feasibility of steering sharing and summarizing the distinctive properties of quantum steering sharing in different scenarios. Continuing our journey, the fourth section delves into discussions on the sharing of diverse quantum correlations, encompassing network nonlocality, quantum entanglement, and quantum contextuality. Moving forward, the fifth section conducts a comprehensive review of the progress in the application of quantum correlation sharing, specifically based on sequential measurement strategies. Applications such as quantum random access coding, random number generation, and self-testing tasks are highlighted. Finally, we discuss and list some of the key unresolved issues in this research field, and conclude the entire article.","sentences":["This review offers a comprehensive exploration and synthesis of recent advancements in the domain of quantum correlation sharing facilitated through sequential measurements.","We initiate our inquiry by delving into the interpretation of the joint probability, laying the foundation for an examination of quantum correlations within the context of specific measurement methods.","The subsequent section meticulously explores nonlocal sharing under diverse measurement strategies and scenarios, with a specific focus on investigating the impact of these strategies on the dissemination of quantum nonlocality.","Key perspectives such as \"asymmetry\" and \"weak value\" are scrutinized through detailed analyses across various scenarios, allowing us to evaluate the potential of nonlocality sharing.","We also provide a retrospective overview of experimental endeavors associated with this phenomenon.","The third part of our exploration presents research findings on steering sharing, offering clarity on the feasibility of steering sharing and summarizing the distinctive properties of quantum steering sharing in different scenarios.","Continuing our journey, the fourth section delves into discussions on the sharing of diverse quantum correlations, encompassing network nonlocality, quantum entanglement, and quantum contextuality.","Moving forward, the fifth section conducts a comprehensive review of the progress in the application of quantum correlation sharing, specifically based on sequential measurement strategies.","Applications such as quantum random access coding, random number generation, and self-testing tasks are highlighted.","Finally, we discuss and list some of the key unresolved issues in this research field, and conclude the entire article."],"url":"http://arxiv.org/abs/2402.04652v1","category":"quant-ph"}
{"created":"2024-02-07 08:24:35","title":"An analysis of the noise schedule for score-based generative models","abstract":"Score-based generative models (SGMs) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target. Recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the Kullback-Leibler (KL) divergence and Wasserstein distances. All existing results have been obtained so far for time-homogeneous speed of the noise schedule. Under mild assumptions on the data distribution, we establish an upper bound for the KL divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule. Assuming that the score is Lipschitz continuous, we provide an improved error bound in Wasserstein distance, taking advantage of favourable underlying contraction mechanisms. We also propose an algorithm to automatically tune the noise schedule using the proposed upper bound. We illustrate empirically the performance of the noise schedule optimization in comparison to standard choices in the literature.","sentences":["Score-based generative models (SGMs) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target.","Recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the Kullback-Leibler (KL) divergence and Wasserstein distances.","All existing results have been obtained so far for time-homogeneous speed of the noise schedule.","Under mild assumptions on the data distribution, we establish an upper bound for the KL divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule.","Assuming that the score is Lipschitz continuous, we provide an improved error bound in Wasserstein distance, taking advantage of favourable underlying contraction mechanisms.","We also propose an algorithm to automatically tune the noise schedule using the proposed upper bound.","We illustrate empirically the performance of the noise schedule optimization in comparison to standard choices in the literature."],"url":"http://arxiv.org/abs/2402.04650v1","category":"math.ST"}
{"created":"2024-02-07 08:19:57","title":"OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding","abstract":"The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness.","sentences":["The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes.","Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus.","However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP.","To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies.","First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning.","Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics.","Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views.","Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively.","Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness."],"url":"http://arxiv.org/abs/2402.04648v1","category":"cs.CV"}
{"created":"2024-02-07 08:18:09","title":"Latent Plan Transformer: Planning as Latent Variable Inference","abstract":"In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories. It achieves competitive performance across several benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting capabilities of nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. These results validate that latent variable inference can be a strong alternative to step-wise reward prompting.","sentences":["In tasks aiming for long-term returns, planning becomes necessary.","We study generative modeling for planning with datasets repurposed from offline reinforcement learning.","Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge.","We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return.","LPT can be learned with maximum likelihood estimation on trajectory-return pairs.","In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context.","During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference.","It then guides the autoregressive policy throughout the episode, functioning as a plan.","Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories.","It achieves competitive performance across several benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting capabilities of nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies.","These results validate that latent variable inference can be a strong alternative to step-wise reward prompting."],"url":"http://arxiv.org/abs/2402.04647v1","category":"cs.LG"}
{"created":"2024-02-07 08:16:40","title":"LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views","abstract":"Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a novel generalizable fine-tuning method LEVI, where the pre-trained model is adaptively ensembled layer-wise with a small task-specific model, while preserving training and inference efficiencies. By combining two complementing models, LEVI effectively suppresses problematic features in both the fine-tuning data and pre-trained model and preserves useful features for new tasks. Broad experiments with large language and vision models show that LEVI greatly improves fine-tuning generalization via emphasizing different views from fine-tuning data and pre-trained features.","sentences":["Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks.","While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD).","To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data.","However, potential limitations in the pre-training data and models are often ignored.","In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization.","It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data.","To address the issues in both pre-training and fine-tuning data, we propose a novel generalizable fine-tuning method LEVI, where the pre-trained model is adaptively ensembled layer-wise with a small task-specific model, while preserving training and inference efficiencies.","By combining two complementing models, LEVI effectively suppresses problematic features in both the fine-tuning data and pre-trained model and preserves useful features for new tasks.","Broad experiments with large language and vision models show that LEVI greatly improves fine-tuning generalization via emphasizing different views from fine-tuning data and pre-trained features."],"url":"http://arxiv.org/abs/2402.04644v1","category":"cs.LG"}
{"created":"2024-02-07 08:11:34","title":"The advance of Mercury's perihelion","abstract":"A very famous ``test'' of the General Theory of Relativity (GTR) is the advance of Mercury's perihelion (and of other planets too). To be more precise, this is not a prediction of General Relativity, since the anomaly was known in the XIXth century, but no consistent explanation had been found yet at the time GTR was elaborated. Einstein came up with a solution to the problem in 1914.   In the case of Mercury, the closest planet to the Sun, the effect is more pronounced than for other planets, and observed from Earth; there is an advance of the perihelion of Mercury of about 5550~arc seconds per century (as/cy). Among these, about $5000$ are due to the equinox precession (the precise value is {$5025.645$}~as/cy) and about $500$ ({$531.54$}) to the influence of the external planets. The remaining, about $50$~as/cy ({$42.56$}), are not understood within Newtonian mechanics. Here, we revisit the problem in some detail for a presentation at the undergraduate level.","sentences":["A very famous ``test'' of the General Theory of Relativity (GTR) is the advance of Mercury's perihelion (and of other planets too).","To be more precise, this is not a prediction of General Relativity, since the anomaly was known in the XIXth century, but no consistent explanation had been found yet at the time GTR was elaborated.","Einstein came up with a solution to the problem in 1914.   ","In the case of Mercury, the closest planet to the Sun, the effect is more pronounced than for other planets, and observed from Earth; there is an advance of the perihelion of Mercury of about 5550~arc seconds per century (as/cy).","Among these, about $5000$ are due to the equinox precession (the precise value is {$5025.645$}~as/cy) and about $500$ ({$531.54$}) to the influence of the external planets.","The remaining, about $50$~as/cy ({$42.56$}), are not understood within Newtonian mechanics.","Here, we revisit the problem in some detail for a presentation at the undergraduate level."],"url":"http://arxiv.org/abs/2402.04643v1","category":"gr-qc"}
{"created":"2024-02-07 08:10:51","title":"On the Mathematical foundations of Diffusion Monte Carlo","abstract":"The Diffusion Monte Carlo method with constant number of walkers, also called Stochastic Reconfiguration as well as Sequential Monte Carlo, is a widely used Monte Carlo methodology for computing the ground-state energy and wave function of quantum systems. In this study, we present the first mathematically rigorous analysis of this class of stochastic methods on non necessarily compact state spaces, including linear diffusions evolving in quadratic absorbing potentials, yielding what seems to be the first result of this type for this class of models. We present a novel and general mathematical framework with easily checked Lyapunov stability conditions that ensure the uniform-in-time convergence of Diffusion Monte Carlo estimates towards the top of the spectrum of Schr\\\"odinger operators. For transient free evolutions, we also present a divergence blow up of the estimates w.r.t. the time horizon even when the asymptotic fluctuation variances are uniformly bounded. We also illustrate the impact of these results in the context of generalized coupled quantum harmonic oscillators with non necessarily reversible nor stable diffusive particle and a quadratic energy absorbing well associated with a semi-definite positive matrix force.","sentences":["The Diffusion Monte Carlo method with constant number of walkers, also called Stochastic Reconfiguration as well as Sequential Monte Carlo, is a widely used Monte Carlo methodology for computing the ground-state energy and wave function of quantum systems.","In this study, we present the first mathematically rigorous analysis of this class of stochastic methods on non necessarily compact state spaces, including linear diffusions evolving in quadratic absorbing potentials, yielding what seems to be the first result of this type for this class of models.","We present a novel and general mathematical framework with easily checked Lyapunov stability conditions that ensure the uniform-in-time convergence of Diffusion Monte Carlo estimates towards the top of the spectrum of Schr\\\"odinger operators.","For transient free evolutions, we also present a divergence blow up of the estimates w.r.t.","the time horizon even when the asymptotic fluctuation variances are uniformly bounded.","We also illustrate the impact of these results in the context of generalized coupled quantum harmonic oscillators with non necessarily reversible nor stable diffusive particle and a quadratic energy absorbing well associated with a semi-definite positive matrix force."],"url":"http://arxiv.org/abs/2402.04642v1","category":"math.ST"}
{"created":"2024-02-07 07:57:43","title":"Domain Bridge: Generative model-based domain forensic for black-box models","abstract":"In forensic investigations of machine learning models, techniques that determine a model's data domain play an essential role, with prior work relying on large-scale corpora like ImageNet to approximate the target model's domain. Although such methods are effective in finding broad domains, they often struggle in identifying finer-grained classes within those domains. In this paper, we introduce an enhanced approach to determine not just the general data domain (e.g., human face) but also its specific attributes (e.g., wearing glasses). Our approach uses an image embedding model as the encoder and a generative model as the decoder. Beginning with a coarse-grained description, the decoder generates a set of images, which are then presented to the unknown target model. Successful classifications by the model guide the encoder to refine the description, which in turn, are used to produce a more specific set of images in the subsequent iteration. This iterative refinement narrows down the exact class of interest. A key strength of our approach lies in leveraging the expansive dataset, LAION-5B, on which the generative model Stable Diffusion is trained. This enlarges our search space beyond traditional corpora, such as ImageNet. Empirical results showcase our method's performance in identifying specific attributes of a model's input domain, paving the way for more detailed forensic analyses of deep learning models.","sentences":["In forensic investigations of machine learning models, techniques that determine a model's data domain play an essential role, with prior work relying on large-scale corpora like ImageNet to approximate the target model's domain.","Although such methods are effective in finding broad domains, they often struggle in identifying finer-grained classes within those domains.","In this paper, we introduce an enhanced approach to determine not just the general data domain (e.g., human face) but also its specific attributes (e.g., wearing glasses).","Our approach uses an image embedding model as the encoder and a generative model as the decoder.","Beginning with a coarse-grained description, the decoder generates a set of images, which are then presented to the unknown target model.","Successful classifications by the model guide the encoder to refine the description, which in turn, are used to produce a more specific set of images in the subsequent iteration.","This iterative refinement narrows down the exact class of interest.","A key strength of our approach lies in leveraging the expansive dataset, LAION-5B, on which the generative model Stable Diffusion is trained.","This enlarges our search space beyond traditional corpora, such as ImageNet.","Empirical results showcase our method's performance in identifying specific attributes of a model's input domain, paving the way for more detailed forensic analyses of deep learning models."],"url":"http://arxiv.org/abs/2402.04640v1","category":"cs.LG"}
{"created":"2024-02-07 07:46:42","title":"CIRCUS: an autonomous control system for antimatter, atomic and quantum physics experiments","abstract":"A powerful and robust control system is a crucial, often neglected, pillar of any modern, complex physics experiment that requires the management of a multitude of different devices and their precise time synchronisation. The AEgIS collaboration presents CIRCUS, a novel, autonomous control system optimised for time-critical experiments such as those at CERN's Antiproton Decelerator and, more broadly, in atomic and quantum physics research. Its setup is based on Sinara/ARTIQ and TALOS, integrating the ALPACA analysis pipeline, the last two developed entirely in AEgIS. It is suitable for strict synchronicity requirements and repeatable, automated operation of experiments, culminating in autonomous parameter optimisation via feedback from real-time data analysis. CIRCUS has been successfully deployed and tested in AEgIS; being experiment-agnostic and released open-source, other experiments can leverage its capabilities.","sentences":["A powerful and robust control system is a crucial, often neglected, pillar of any modern, complex physics experiment that requires the management of a multitude of different devices and their precise time synchronisation.","The AEgIS collaboration presents CIRCUS, a novel, autonomous control system optimised for time-critical experiments such as those at CERN's Antiproton Decelerator and, more broadly, in atomic and quantum physics research.","Its setup is based on Sinara/ARTIQ and TALOS, integrating the ALPACA analysis pipeline, the last two developed entirely in AEgIS.","It is suitable for strict synchronicity requirements and repeatable, automated operation of experiments, culminating in autonomous parameter optimisation via feedback from real-time data analysis.","CIRCUS has been successfully deployed and tested in AEgIS; being experiment-agnostic and released open-source, other experiments can leverage its capabilities."],"url":"http://arxiv.org/abs/2402.04637v1","category":"quant-ph"}
{"created":"2024-02-07 18:58:50","title":"Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding","abstract":"To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively. Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding.","sentences":["To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework.","To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model.","One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states.","To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation.","In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy.","Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads.","We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively.","Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding."],"url":"http://arxiv.org/abs/2402.05109v1","category":"cs.LG"}
{"created":"2024-02-07 18:47:32","title":"Motile bacteria crossing liquid-liquid interfaces","abstract":"Real-life bacteria often swim in complex fluids, but our understanding of the interactions between bacteria and complex surroundings is still evolving. In this work, rod-like $\\textit{Bacillus subtilis}$ swims in a quasi-2D environment with aqueous liquid-liquid interfaces, i.e., the isotropic-nematic coexistence phase of an aqueous chromonic liquid crystal. Focusing on the bacteria motion near and at the liquid-liquid interfaces, we collect and quantify bacterial trajectories from the isotropic to the nematic phase. Despite its small magnitude, the interfacial tension of the order of 10 $\\mathrm{\\mu N/m}$ at the isotropic-nematic interface justifies our observations that bacteria swimming more perpendicular to the interface have a higher probability of crossing the interface. Our force-balance model, considering the interfacial tension, further predicts how the length and speed of the bacteria affect their crossing behaviors.","sentences":["Real-life bacteria often swim in complex fluids, but our understanding of the interactions between bacteria and complex surroundings is still evolving.","In this work, rod-like $\\textit{Bacillus subtilis}$ swims in a quasi-2D environment with aqueous liquid-liquid interfaces, i.e., the isotropic-nematic coexistence phase of an aqueous chromonic liquid crystal.","Focusing on the bacteria motion near and at the liquid-liquid interfaces, we collect and quantify bacterial trajectories from the isotropic to the nematic phase.","Despite its small magnitude, the interfacial tension of the order of 10 $\\mathrm{\\mu N/m}$ at the isotropic-nematic interface justifies our observations that bacteria swimming more perpendicular to the interface have a higher probability of crossing the interface.","Our force-balance model, considering the interfacial tension, further predicts how the length and speed of the bacteria affect their crossing behaviors."],"url":"http://arxiv.org/abs/2402.05095v1","category":"cond-mat.soft"}
{"created":"2024-02-07 18:30:04","title":"Markovian Analysis of Information Cascades with Fake Agents","abstract":"People often learn from other's actions when they make decisions while doing online shopping. This kind of observational learning may lead to information cascades, which means agents might ignore their own signals and follow the 'trend' created collectively by the actions of their predecessors. It is well-known that with rational agents, such a cascade model can result in either correct or incorrect cascades. In this paper, we additionally consider the presence of fake agents who always take fixed actions and we investigate their influence on the outcome of these cascades. We propose an infinite Markov Chain sequence structure and a tree structure to analyze how the fraction and the type of such fake agents impacts behavior of the upcoming agents. We show that an increase in the fraction of fake agents may reduce the chances of their preferred outcome, and also there is a certain lower bound for the probability of a wrong cascade. In particular, we discuss the probability of an agent being fake tends to 1 and the effect of a constant portion of fake agents.","sentences":["People often learn from other's actions when they make decisions while doing online shopping.","This kind of observational learning may lead to information cascades, which means agents might ignore their own signals and follow the 'trend' created collectively by the actions of their predecessors.","It is well-known that with rational agents, such a cascade model can result in either correct or incorrect cascades.","In this paper, we additionally consider the presence of fake agents who always take fixed actions and we investigate their influence on the outcome of these cascades.","We propose an infinite Markov Chain sequence structure and a tree structure to analyze how the fraction and the type of such fake agents impacts behavior of the upcoming agents.","We show that an increase in the fraction of fake agents may reduce the chances of their preferred outcome, and also there is a certain lower bound for the probability of a wrong cascade.","In particular, we discuss the probability of an agent being fake tends to 1 and the effect of a constant portion of fake agents."],"url":"http://arxiv.org/abs/2402.05076v1","category":"cs.SI"}
{"created":"2024-02-07 16:43:27","title":"Once-in-a-lifetime encounter models for neutrino media: From coherent oscillation to flavor equilibration","abstract":"Collective neutrino oscillations are typically studied using the lowest-order quantum kinetic equation, also known as the mean-field approximation. However, some recent quantum many-body simulations suggest that quantum entanglement among neutrinos may be important and may result in flavor equilibration of the neutrino gas. In this work, we develop new quantum many-body models for neutrino gases in which any pair of neutrinos can interact at most once in their lifetimes. A key parameter of our models is $\\gamma=\\mu \\Delta z$, where $\\mu$ is the neutrino coupling strength, which is proportional to the neutrino density, and $\\Delta z$ is the duration over which a pair of neutrinos can interact each time. Our models reduce to the mean-field approach in the limit $\\gamma\\to0$ and achieve flavor equilibration in time $t \\gg (\\gamma\\mu)^{-1}$. These models demonstrate the emergence of coherent flavor oscillations from the particle perspective and may help elucidate the role of quantum entanglement in collective neutrino oscillations.","sentences":["Collective neutrino oscillations are typically studied using the lowest-order quantum kinetic equation, also known as the mean-field approximation.","However, some recent quantum many-body simulations suggest that quantum entanglement among neutrinos may be important and may result in flavor equilibration of the neutrino gas.","In this work, we develop new quantum many-body models for neutrino gases in which any pair of neutrinos can interact at most once in their lifetimes.","A key parameter of our models is $\\gamma=\\mu \\Delta z$, where $\\mu$ is the neutrino coupling strength, which is proportional to the neutrino density, and $\\Delta z$ is the duration over which a pair of neutrinos can interact each time.","Our models reduce to the mean-field approach in the limit $\\gamma\\to0$ and achieve flavor equilibration in time $t \\gg (\\gamma\\mu)^{-1}$. These models demonstrate the emergence of coherent flavor oscillations from the particle perspective and may help elucidate the role of quantum entanglement in collective neutrino oscillations."],"url":"http://arxiv.org/abs/2402.05022v1","category":"hep-ph"}
{"created":"2024-02-07 16:32:55","title":"When the Body Became Data: Historical Data Cultures and Anatomical Illustration","abstract":"With changing attitudes around knowledge, medicine, art, and technology, the human body has become a source of information and, ultimately, shareable and analyzable data. Centuries of illustrations and visualizations of the body occur within particular historical, social, and political contexts. These contexts are enmeshed in different so-called data cultures: ways that data, knowledge, and information are conceptualized and collected, structured and shared. In this work, we explore how information about the body was collected as well as the circulation, impact, and persuasive force of the resulting images. We show how mindfulness of data cultural influences remain crucial for today's designers, researchers, and consumers of visualizations. We conclude with a call for the field to reflect on how visualizations are not timeless and contextless mirrors on objective data, but as much a product of our time and place as the visualizations of the past.","sentences":["With changing attitudes around knowledge, medicine, art, and technology, the human body has become a source of information and, ultimately, shareable and analyzable data.","Centuries of illustrations and visualizations of the body occur within particular historical, social, and political contexts.","These contexts are enmeshed in different so-called data cultures: ways that data, knowledge, and information are conceptualized and collected, structured and shared.","In this work, we explore how information about the body was collected as well as the circulation, impact, and persuasive force of the resulting images.","We show how mindfulness of data cultural influences remain crucial for today's designers, researchers, and consumers of visualizations.","We conclude with a call for the field to reflect on how visualizations are not timeless and contextless mirrors on objective data, but as much a product of our time and place as the visualizations of the past."],"url":"http://arxiv.org/abs/2402.05014v1","category":"cs.HC"}
{"created":"2024-02-07 16:29:39","title":"A Review on Trajectory Datasets on Advanced Driver Assistance System","abstract":"This paper presents a comprehensive review of trajectory data of Advanced Driver Assistance System equipped-vehicle, with the aim of precisely model of Autonomous Vehicles (AVs) behavior. This study emphasizes the importance of trajectory data in the development of AV models, especially in car-following scenarios. We introduce and evaluate several datasets: the OpenACC Dataset, the Connected & Autonomous Transportation Systems Laboratory Open Dataset, the Vanderbilt ACC Dataset, the Central Ohio Dataset, and the Waymo Open Dataset. Each dataset offers unique insights into AV behaviors, yet they share common challenges in terms of data availability, processing, and standardization. After a series of data cleaning, outlier removal and statistical analysis, this paper transforms datasets of varied formats into a uniform standard, thereby improving their applicability for modeling AV car-following behavior. Key contributions of this study include: 1. the transformation of all datasets into a unified standard format, enhancing their utility for broad research applications; 2. a comparative analysis of these datasets, highlighting their distinct characteristics and implications for car-following model development; 3. the provision of guidelines for future data collection projects, along with the open-source release of all processed data and code for use by the research community.","sentences":["This paper presents a comprehensive review of trajectory data of Advanced Driver Assistance System equipped-vehicle, with the aim of precisely model of Autonomous Vehicles (AVs) behavior.","This study emphasizes the importance of trajectory data in the development of AV models, especially in car-following scenarios.","We introduce and evaluate several datasets: the OpenACC Dataset, the Connected & Autonomous Transportation Systems Laboratory Open Dataset, the Vanderbilt ACC Dataset, the Central Ohio Dataset, and the Waymo Open Dataset.","Each dataset offers unique insights into AV behaviors, yet they share common challenges in terms of data availability, processing, and standardization.","After a series of data cleaning, outlier removal and statistical analysis, this paper transforms datasets of varied formats into a uniform standard, thereby improving their applicability for modeling AV car-following behavior.","Key contributions of this study include: 1.","the transformation of all datasets into a unified standard format, enhancing their utility for broad research applications; 2. a comparative analysis of these datasets, highlighting their distinct characteristics and implications for car-following model development; 3.","the provision of guidelines for future data collection projects, along with the open-source release of all processed data and code for use by the research community."],"url":"http://arxiv.org/abs/2402.05009v1","category":"stat.AP"}
{"created":"2024-02-07 15:15:14","title":"Charting the COVID Long Haul Experience -- A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence","abstract":"COVID Long Haul (CLH) is an emerging chronic illness with varied patient experiences. Our understanding of CLH is often limited to data from electronic health records (EHRs), such as diagnoses or problem lists, which do not capture the volatility and severity of symptoms or their impact. To better understand the unique presentation of CLH, we conducted a 3-month long cohort study with 14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective (weekly surveys, interviews) data. Our findings reveal a complex presentation of symptoms, associated uncertainty, and the ensuing impact CLH has on patients' personal and professional lives. We identify patient needs, practices, and challenges around adhering to clinical recommendations, engaging with health data, and establishing \"new normals\" post COVID. We reflect on the potential found at the intersection of these various data streams and the persuasive heuristics possible when designing for this new population and their specific needs.","sentences":["COVID Long Haul (CLH) is an emerging chronic illness with varied patient experiences.","Our understanding of CLH is often limited to data from electronic health records (EHRs), such as diagnoses or problem lists, which do not capture the volatility and severity of symptoms or their impact.","To better understand the unique presentation of CLH, we conducted a 3-month long cohort study with 14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective (weekly surveys, interviews) data.","Our findings reveal a complex presentation of symptoms, associated uncertainty, and the ensuing impact CLH has on patients' personal and professional lives.","We identify patient needs, practices, and challenges around adhering to clinical recommendations, engaging with health data, and establishing \"new normals\" post COVID.","We reflect on the potential found at the intersection of these various data streams and the persuasive heuristics possible when designing for this new population and their specific needs."],"url":"http://arxiv.org/abs/2402.04937v1","category":"cs.HC"}
{"created":"2024-02-07 14:24:41","title":"Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning","abstract":"Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experiments show that our method enables more efficient target detection compared to state-of-the-art learning and non-learning baselines. We also show the applicability of our approach for orchard monitoring using an unmanned aerial vehicle in a photorealistic simulator.","sentences":["Autonomous robots are often employed for data collection due to their efficiency and low labour costs.","A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life.","Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions.","To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments.","A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest.","For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest.","Our experiments show that our method enables more efficient target detection compared to state-of-the-art learning and non-learning baselines.","We also show the applicability of our approach for orchard monitoring using an unmanned aerial vehicle in a photorealistic simulator."],"url":"http://arxiv.org/abs/2402.04894v1","category":"cs.RO"}
{"created":"2024-02-07 14:19:03","title":"Comparing Methods for Creating a National Random Sample of Twitter Users","abstract":"Twitter data has been widely used by researchers across various social and computer science disciplines. A common aim when working with Twitter data is the construction of a random sample of users from a given country. However, while several methods have been proposed in the literature, their comparative performance is mostly unexplored. In this paper, we implement four methods to collect a random sample of Twitter users in the US: 1% Stream, Bounding Box, Location Query, and Language Query. Then, we compare the methods according to their tweet- and user-level metrics as well as their accuracy in estimating US population with and without using inclusion probabilities of various demographics. Our results show that the 1% Stream method performs differently than others and best for the construction of a population representative sample, though its statistical significance is questionable due to large confidence intervals. We discuss the conditions under which the 1% Stream method may not be suitable and suggest the Bounding Box method as the second-best method to use.","sentences":["Twitter data has been widely used by researchers across various social and computer science disciplines.","A common aim when working with Twitter data is the construction of a random sample of users from a given country.","However, while several methods have been proposed in the literature, their comparative performance is mostly unexplored.","In this paper, we implement four methods to collect a random sample of Twitter users in the US: 1% Stream, Bounding Box, Location Query, and Language Query.","Then, we compare the methods according to their tweet- and user-level metrics as well as their accuracy in estimating US population with and without using inclusion probabilities of various demographics.","Our results show that the 1% Stream method performs differently than others and best for the construction of a population representative sample, though its statistical significance is questionable due to large confidence intervals.","We discuss the conditions under which the 1% Stream method may not be suitable and suggest the Bounding Box method as the second-best method to use."],"url":"http://arxiv.org/abs/2402.04879v1","category":"cs.SI"}
{"created":"2024-02-07 13:06:11","title":"Microwave control of collective quantum jump statistics of a dissipative Rydberg gas","abstract":"Quantum many-body systems near phase transitions respond collectively to externally applied perturbations. We explore this phenomenon in a laser-driven dissipative Rydberg gas that is tuned to a bistable regime. Here two metastable phases coexist, which feature a low and high density of Rydberg atoms, respectively. The ensuing collective dynamics, which we monitor in situ, is characterized by stochastic collective jumps between these two macroscopically distinct many-body phases. We show that the statistics of these jumps can be controlled using a dual-tone microwave field. In particular, we find that the distribution of jump times develops peaks corresponding to subharmonics of the relative microwave detuning. Our study demonstrates the control of collective statistical properties of dissipative quantum many-body systems without the necessity of fine-tuning or of ultra cold temperatures. Such robust phenomena may find technological applications in quantum sensing and metrology.","sentences":["Quantum many-body systems near phase transitions respond collectively to externally applied perturbations.","We explore this phenomenon in a laser-driven dissipative Rydberg gas that is tuned to a bistable regime.","Here two metastable phases coexist, which feature a low and high density of Rydberg atoms, respectively.","The ensuing collective dynamics, which we monitor in situ, is characterized by stochastic collective jumps between these two macroscopically distinct many-body phases.","We show that the statistics of these jumps can be controlled using a dual-tone microwave field.","In particular, we find that the distribution of jump times develops peaks corresponding to subharmonics of the relative microwave detuning.","Our study demonstrates the control of collective statistical properties of dissipative quantum many-body systems without the necessity of fine-tuning or of ultra cold temperatures.","Such robust phenomena may find technological applications in quantum sensing and metrology."],"url":"http://arxiv.org/abs/2402.04815v1","category":"quant-ph"}
{"created":"2024-02-07 12:42:38","title":"Back action suppression for levitated dipolar scatterers","abstract":"Levitated dipolar scatterers exhibit exceptional performance as optomechanical systems for observing quantum mechanics at the mesoscopic scale. However, their tendency to scatter light in almost any direction poses experimental challenges, in particular limiting light collection efficiencies and, consequently, the information extractable from the system. In this article, we present a setup designed to enhance the information gleaned from optomechanical measurements by constraining the back action to a specific spatial direction. This approach facilitates achieving Heisenberg-limited detection at any given numerical aperture. The setup consists of a hollow hemispherical mirror that controls the light scattered by the dipolar emitter, particularly at high scattering angles, thereby focusing the obtained information. This mirror is compatible with existing setups commonly employed in levitated optomechanics, including confocal lenses and optical resonators.","sentences":["Levitated dipolar scatterers exhibit exceptional performance as optomechanical systems for observing quantum mechanics at the mesoscopic scale.","However, their tendency to scatter light in almost any direction poses experimental challenges, in particular limiting light collection efficiencies and, consequently, the information extractable from the system.","In this article, we present a setup designed to enhance the information gleaned from optomechanical measurements by constraining the back action to a specific spatial direction.","This approach facilitates achieving Heisenberg-limited detection at any given numerical aperture.","The setup consists of a hollow hemispherical mirror that controls the light scattered by the dipolar emitter, particularly at high scattering angles, thereby focusing the obtained information.","This mirror is compatible with existing setups commonly employed in levitated optomechanics, including confocal lenses and optical resonators."],"url":"http://arxiv.org/abs/2402.04802v1","category":"quant-ph"}
{"created":"2024-02-07 11:01:26","title":"Black Hole Search in Dynamic Tori","abstract":"We investigate the black hole search problem by a set of mobile agents in a dynamic torus. Black hole is defined to be a dangerous stationary node which has the capability to destroy any number of incoming agents without leaving any trace of its existence. A torus of size $n\\times m$ ($3\\leq n \\leq m$) is a collection of $n$ row rings and $m$ column rings, and the dynamicity is such that each ring is considered to be 1-interval connected, i.e., in other words at most one edge can be missing from each ring at any round. The parameters which define the efficiency of any black hole search algorithm are: the number of agents and the number of rounds (or \\textit{time}) for termination. We consider two initial configurations of mobile agents: first, the agents are co-located and second, the agents are scattered. In each case, we establish lower and upper bounds on the number of agents and on the amount of time required to solve the black hole search problem.","sentences":["We investigate the black hole search problem by a set of mobile agents in a dynamic torus.","Black hole is defined to be a dangerous stationary node which has the capability to destroy any number of incoming agents without leaving any trace of its existence.","A torus of size $n\\times m$ ($3\\leq n \\leq m$) is a collection of $n$ row rings and $m$ column rings, and the dynamicity is such that each ring is considered to be 1-interval connected, i.e., in other words at most one edge can be missing from each ring at any round.","The parameters which define the efficiency of any black hole search algorithm are: the number of agents and the number of rounds (or \\textit{time}) for termination.","We consider two initial configurations of mobile agents: first, the agents are co-located and second, the agents are scattered.","In each case, we establish lower and upper bounds on the number of agents and on the amount of time required to solve the black hole search problem."],"url":"http://arxiv.org/abs/2402.04746v1","category":"cs.DC"}
{"created":"2024-02-07 10:15:11","title":"Robust policy iteration for continuous-time stochastic $H_\\infty$ control problem with unknown dynamics","abstract":"In this article, we study a continuous-time stochastic $H_\\infty$ control problem based on reinforcement learning (RL) techniques that can be viewed as solving a stochastic linear-quadratic two-person zero-sum differential game (LQZSG). First, we propose an RL algorithm that can iteratively solve stochastic game algebraic Riccati equation based on collected state and control data when all dynamic system information is unknown. In addition, the algorithm only needs to collect data once during the iteration process. Then, we discuss the robustness and convergence of the inner and outer loops of the policy iteration algorithm, respectively, and show that when the error of each iteration is within a certain range, the algorithm can converge to a small neighborhood of the saddle point of the stochastic LQZSG problem. Finally, we applied the proposed RL algorithm to two simulation examples to verify the effectiveness of the algorithm.","sentences":["In this article, we study a continuous-time stochastic $H_\\infty$ control problem based on reinforcement learning (RL) techniques that can be viewed as solving a stochastic linear-quadratic two-person zero-sum differential game (LQZSG).","First, we propose an RL algorithm that can iteratively solve stochastic game algebraic Riccati equation based on collected state and control data when all dynamic system information is unknown.","In addition, the algorithm only needs to collect data once during the iteration process.","Then, we discuss the robustness and convergence of the inner and outer loops of the policy iteration algorithm, respectively, and show that when the error of each iteration is within a certain range, the algorithm can converge to a small neighborhood of the saddle point of the stochastic LQZSG problem.","Finally, we applied the proposed RL algorithm to two simulation examples to verify the effectiveness of the algorithm."],"url":"http://arxiv.org/abs/2402.04721v1","category":"math.OC"}
{"created":"2024-02-07 09:38:47","title":"Filling the radio transients gap (or: The case for a dedicated radio transients monitoring array in the southern hemisphere)","abstract":"In this short paper we outline the case for a small radio telescope array in the southern hemisphere with operations dedicated to rapid follow-up and monitoring of astrophysical transients. We argue that the science harvest from such a facility would be very large, using AMI-LA as an outstanding example of how such a programme is already being operated in the north with an enormous track record of success. A southern radio transients facility would in turn take pressure off the Square Kilometre Array and the other world class larger arrays with 10-100 times more collecting area, which will never have the programme time available to comprehensively pursue this science. We discuss comparisons with the development of transient surveys and follow up in optical astronomy, and also how single millimetre dishes can contribute to radio transients science in the south. This paper is not a funding proposal aimed at any particular body, but rather a concept and discussion piece, and the authors welcome comments and feedback.","sentences":["In this short paper we outline the case for a small radio telescope array in the southern hemisphere with operations dedicated to rapid follow-up and monitoring of astrophysical transients.","We argue that the science harvest from such a facility would be very large, using AMI-LA as an outstanding example of how such a programme is already being operated in the north with an enormous track record of success.","A southern radio transients facility would in turn take pressure off the Square Kilometre Array and the other world class larger arrays with 10-100 times more collecting area, which will never have the programme time available to comprehensively pursue this science.","We discuss comparisons with the development of transient surveys and follow up in optical astronomy, and also how single millimetre dishes can contribute to radio transients science in the south.","This paper is not a funding proposal aimed at any particular body, but rather a concept and discussion piece, and the authors welcome comments and feedback."],"url":"http://arxiv.org/abs/2402.04698v1","category":"astro-ph.IM"}
{"created":"2024-02-07 08:52:59","title":"Gaussian Process-Based Nonlinear Moving Horizon Estimation","abstract":"In this paper, we propose a novel Gaussian process-based moving horizon estimation (MHE) framework for unknown nonlinear systems. In the proposed scheme, we take advantage of the properties of Gaussian processes. On the one hand, we approximate the system dynamics by the posterior means of the learned Gaussian processes (GPs). On the other hand, we exploit the posterior variances of the Gaussian processes to design the weighting matrices in the MHE cost function and account for the uncertainty in the learned system dynamics. The data collection and the tuning of the hyperparameters are done offline. We prove robust stability of the GP-based MHE scheme using a Lyapunov-based proof technique. Furthermore, as additional contribution, we analyze under which conditions incremental input/output-to-state stability (a nonlinear detectability notion) is preserved when approximating the system dynamics using, e.g., machine learning techniques. Finally, we illustrate the performance of the GP-based MHE scheme in a simulation case study and show how the chosen weighting matrices can lead to an improved performance compared to standard cost functions.","sentences":["In this paper, we propose a novel Gaussian process-based moving horizon estimation (MHE) framework for unknown nonlinear systems.","In the proposed scheme, we take advantage of the properties of Gaussian processes.","On the one hand, we approximate the system dynamics by the posterior means of the learned Gaussian processes (GPs).","On the other hand, we exploit the posterior variances of the Gaussian processes to design the weighting matrices in the MHE cost function and account for the uncertainty in the learned system dynamics.","The data collection and the tuning of the hyperparameters are done offline.","We prove robust stability of the GP-based MHE scheme using a Lyapunov-based proof technique.","Furthermore, as additional contribution, we analyze under which conditions incremental input/output-to-state stability (a nonlinear detectability notion) is preserved when approximating the system dynamics using, e.g., machine learning techniques.","Finally, we illustrate the performance of the GP-based MHE scheme in a simulation case study and show how the chosen weighting matrices can lead to an improved performance compared to standard cost functions."],"url":"http://arxiv.org/abs/2402.04665v1","category":"eess.SY"}
{"created":"2024-02-07 07:28:34","title":"The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends","abstract":"Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. We humans tend to persuade others to change their viewpoints, attitudes or behaviors through conversations in various scenarios (e.g., persuasion for social good, arguing in online platforms). Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue system. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers.","sentences":["Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems.","We humans tend to persuade others to change their viewpoints, attitudes or behaviors through conversations in various scenarios (e.g., persuasion for social good, arguing in online platforms).","Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue system.","Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation.","However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion.","Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm.","To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy.","Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent.","Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics.","Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers."],"url":"http://arxiv.org/abs/2402.04631v1","category":"cs.CL"}
{"created":"2024-02-07 07:24:01","title":"SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph","abstract":"The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs. In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic \"clues\" in the queries, such as meaningful variable names and inline comments. Finally, we evaluate our approach over the real-world Bgee gene expression knowledge graph and we show that semantic clues can improve model performance by up to 33% compared to a baseline with random variable names and no comments included.","sentences":["The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs.","However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs.","To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs.","In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce.","In this context, we also investigate the role of semantic \"clues\" in the queries, such as meaningful variable names and inline comments.","Finally, we evaluate our approach over the real-world Bgee gene expression knowledge graph and we show that semantic clues can improve model performance by up to 33% compared to a baseline with random variable names and no comments included."],"url":"http://arxiv.org/abs/2402.04627v1","category":"cs.AI"}
{"created":"2024-02-07 06:50:42","title":"InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory","abstract":"Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining the ability to capture long-distance dependencies. Without any training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies.","sentences":["Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents.","However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues.","To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences.","Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics.","This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences.","Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation.","Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining the ability to capture long-distance dependencies.","Without any training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these LLMs on long sequences.","Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies."],"url":"http://arxiv.org/abs/2402.04617v1","category":"cs.CL"}
{"created":"2024-02-07 06:48:24","title":"TinyLLM: Learning a Small Student from Multiple Large Language Models","abstract":"Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform large teacher LLMs significantly, despite having a considerably smaller model size.","sentences":["Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense.","Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization.","However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information.","To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs.","In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers.","Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs.","We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios.","Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method.","Results show that TinyLLM can outperform large teacher LLMs significantly, despite having a considerably smaller model size."],"url":"http://arxiv.org/abs/2402.04616v1","category":"cs.CL"}
{"created":"2024-02-07 06:42:33","title":"ScreenAI: A Vision-Language Model for UI and Infographics Understanding","abstract":"Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and InfographicVQA) compared to models of similar size. Finally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering.","sentences":["Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction.","We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding.","Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets.","At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements.","We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale.","We run ablation studies to demonstrate the impact of these design choices.","At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and InfographicVQA) compared to models of similar size.","Finally, we release three new datasets: one focused on the screen annotation task and two others focused on question answering."],"url":"http://arxiv.org/abs/2402.04615v1","category":"cs.CV"}
{"created":"2024-02-07 05:56:54","title":"Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector","abstract":"Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection. Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance.","sentences":["Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs).","While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs.","In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs.","Our method first trains a correction model to generate an initial correction of the source sentence.","Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection.","Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction.","Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection.","Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance."],"url":"http://arxiv.org/abs/2402.04601v1","category":"cs.CL"}
{"created":"2024-02-07 05:47:31","title":"Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment","abstract":"Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be matched to the query temporal block with the same or adjacent (next) temporal index, and adjacent camera views to achieve joint local temporal-viewpoint warping. JEANIE selects the smallest distance among matching paths with different temporal-viewpoint warping patterns, an advantage over DTW which only performs temporal alignment. We also propose an unsupervised FSAR akin to clustering of sequences with JEANIE as a distance measure. JEANIE achieves state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II on supervised and unsupervised FSAR, and their meta-learning inspired fusion.","sentences":["Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences.","Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs.","In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D.","We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes.","Given a query sequence, we create its several views by simulating several camera locations.","For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW).","Specifically, each support temporal block can be matched to the query temporal block with the same or adjacent (next) temporal index, and adjacent camera views to achieve joint local temporal-viewpoint warping.","JEANIE selects the smallest distance among matching paths with different temporal-viewpoint warping patterns, an advantage over DTW which only performs temporal alignment.","We also propose an unsupervised FSAR akin to clustering of sequences with JEANIE as a distance measure.","JEANIE achieves state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II on supervised and unsupervised FSAR, and their meta-learning inspired fusion."],"url":"http://arxiv.org/abs/2402.04599v1","category":"cs.CV"}
{"created":"2024-02-07 05:43:57","title":"CMSA algorithm for solving the prioritized pairwise test data generation problem in software product lines","abstract":"In Software Product Lines (SPLs) it may be difficult or even impossible to test all the products of the family because of the large number of valid feature combinations that may exist. Thus, we want to find a minimal subset of the product family that allows us to test all these possible combinations (pairwise). Furthermore, when testing a single product is a great effort, it is desirable to first test products composed of a set of priority features. This problem is called Prioritized Pairwise Test Data Generation Problem.   State-of-the-art algorithms based on Integer Linear Programming for this problema are faster enough for small and medium instances. However, there exists some real instances that are too large to be computed with these algorithms in a reasonable time because of the exponential growth of the number of candidate solutions. Also, these heuristics not always lead us to the best solutions. In this work we propose a new approach based on a hybrid metaheuristic algorithm called Construct, Merge, Solve & Adapt. We compare this matheuristic with four algorithms: a Hybrid algorithm based on Integer Linear Programming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming (HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm called prioritized-ICPL. The analysis reveals that CMSA results in statistically significantly better quality solutions in most instances and for most levels of weighted coverage, although it requires more execution time.","sentences":["In Software Product Lines (SPLs) it may be difficult or even impossible to test all the products of the family because of the large number of valid feature combinations that may exist.","Thus, we want to find a minimal subset of the product family that allows us to test all these possible combinations (pairwise).","Furthermore, when testing a single product is a great effort, it is desirable to first test products composed of a set of priority features.","This problem is called Prioritized Pairwise Test Data Generation Problem.   ","State-of-the-art algorithms based on Integer Linear Programming for this problema are faster enough for small and medium instances.","However, there exists some real instances that are too large to be computed with these algorithms in a reasonable time because of the exponential growth of the number of candidate solutions.","Also, these heuristics not always lead us to the best solutions.","In this work we propose a new approach based on a hybrid metaheuristic algorithm called Construct, Merge, Solve & Adapt.","We compare this matheuristic with four algorithms: a Hybrid algorithm based on Integer Linear Programming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming (HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm called prioritized-ICPL.","The analysis reveals that CMSA results in statistically significantly better quality solutions in most instances and for most levels of weighted coverage, although it requires more execution time."],"url":"http://arxiv.org/abs/2402.04597v1","category":"cs.AI"}
{"created":"2024-02-07 05:38:53","title":"Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)","abstract":"Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance of the model by making it more robust to data imbalance. A modified F1 score is presented to evaluate the effectiveness of the proposed loss function in handling imbalance. Experiments on several benchmark multi-label datasets show that DOSA trained with the proposed loss function shows improved robustness to data imbalance and obtains better continual multi-label learning performance than CIFDM, a previous state-of-the-art algorithm.","sentences":["Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time.","This motivates the study of task-agnostic continual multi-label learning problems.","While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy.","Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning.","Also, accurately determining multiple labels with SNNs is still an open research problem.","This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps.","A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance of the model by making it more robust to data imbalance.","A modified F1 score is presented to evaluate the effectiveness of the proposed loss function in handling imbalance.","Experiments on several benchmark multi-label datasets show that DOSA trained with the proposed loss function shows improved robustness to data imbalance and obtains better continual multi-label learning performance than CIFDM, a previous state-of-the-art algorithm."],"url":"http://arxiv.org/abs/2402.04596v1","category":"cs.LG"}
{"created":"2024-02-07 04:55:57","title":"Troublemaker Learning for Low-Light Image Enhancement","abstract":"Low-light image enhancement (LLIE) restores the color and brightness of underexposed images. Supervised methods suffer from high costs in collecting low/normal-light image pairs. Unsupervised methods invest substantial effort in crafting complex loss functions. We address these two challenges through the proposed TroubleMaker Learning (TML) strategy, which employs normal-light images as inputs for training. TML is simple: we first dim the input and then increase its brightness. TML is based on two core components. First, the troublemaker model (TM) constructs pseudo low-light images from normal images to relieve the cost of pairwise data. Second, the predicting model (PM) enhances the brightness of pseudo low-light images. Additionally, we incorporate an enhancing model (EM) to further improve the visual performance of PM outputs. Moreover, in LLIE tasks, characterizing global element correlations is important because more information on the same object can be captured. CNN cannot achieve this well, and self-attention has high time complexity. Accordingly, we propose Global Dynamic Convolution (GDC) with O(n) time complexity, which essentially imitates the partial calculation process of self-attention to formulate elementwise correlations. Based on the GDC module, we build the UGDC model. Extensive quantitative and qualitative experiments demonstrate that UGDC trained with TML can achieve competitive performance against state-of-the-art approaches on public datasets. The code is available at https://github.com/Rainbowman0/TML_LLIE.","sentences":["Low-light image enhancement (LLIE) restores the color and brightness of underexposed images.","Supervised methods suffer from high costs in collecting low/normal-light image pairs.","Unsupervised methods invest substantial effort in crafting complex loss functions.","We address these two challenges through the proposed TroubleMaker Learning (TML) strategy, which employs normal-light images as inputs for training.","TML is simple: we first dim the input and then increase its brightness.","TML is based on two core components.","First, the troublemaker model (TM) constructs pseudo low-light images from normal images to relieve the cost of pairwise data.","Second, the predicting model (PM) enhances the brightness of pseudo low-light images.","Additionally, we incorporate an enhancing model (EM) to further improve the visual performance of PM outputs.","Moreover, in LLIE tasks, characterizing global element correlations is important because more information on the same object can be captured.","CNN cannot achieve this well, and self-attention has high time complexity.","Accordingly, we propose Global Dynamic Convolution (GDC) with O(n) time complexity, which essentially imitates the partial calculation process of self-attention to formulate elementwise correlations.","Based on the GDC module, we build the UGDC model.","Extensive quantitative and qualitative experiments demonstrate that UGDC trained with TML can achieve competitive performance against state-of-the-art approaches on public datasets.","The code is available at https://github.com/Rainbowman0/TML_LLIE."],"url":"http://arxiv.org/abs/2402.04584v1","category":"eess.IV"}
{"created":"2024-02-07 04:44:41","title":"Boosting Reinforcement Learning Algorithms in Continuous Robotic Reaching Tasks using Adaptive Potential Functions","abstract":"In reinforcement learning, reward shaping is an efficient way to guide the learning process of an agent, as the reward can indicate the optimal policy of the task. The potential-based reward shaping framework was proposed to guarantee policy invariance after reward shaping, where a potential function is used to calculate the shaping reward. In former work, we proposed a novel adaptive potential function (APF) method to learn the potential function concurrently with training the agent based on information collected by the agent during the training process, and examined the APF method in discrete action space scenarios. This paper investigates the feasibility of using APF in solving continuous-reaching tasks in a real-world robotic scenario with continuous action space. We combine the Deep Deterministic Policy Gradient (DDPG) algorithm and our proposed method to form a new algorithm called APF-DDPG. To compare APF-DDPG with DDPG, we designed a task where the agent learns to control Baxter's right arm to reach a goal position. The experimental results show that the APF-DDPG algorithm outperforms the DDPG algorithm on both learning speed and robustness.","sentences":["In reinforcement learning, reward shaping is an efficient way to guide the learning process of an agent, as the reward can indicate the optimal policy of the task.","The potential-based reward shaping framework was proposed to guarantee policy invariance after reward shaping, where a potential function is used to calculate the shaping reward.","In former work, we proposed a novel adaptive potential function (APF) method to learn the potential function concurrently with training the agent based on information collected by the agent during the training process, and examined the APF method in discrete action space scenarios.","This paper investigates the feasibility of using APF in solving continuous-reaching tasks in a real-world robotic scenario with continuous action space.","We combine the Deep Deterministic Policy Gradient (DDPG) algorithm and our proposed method to form a new algorithm called APF-DDPG.","To compare APF-DDPG with DDPG, we designed a task where the agent learns to control Baxter's right arm to reach a goal position.","The experimental results show that the APF-DDPG algorithm outperforms the DDPG algorithm on both learning speed and robustness."],"url":"http://arxiv.org/abs/2402.04581v1","category":"cs.RO"}
{"created":"2024-02-07 04:43:41","title":"A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents","abstract":"The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used in cross-domain policy transfer problems. Lastly, we summarize the open challenges that lie beyond the capabilities of current paradigms and discuss potential future directions in this field.","sentences":["The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data.","However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements.","Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration.","Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches.","In this paper, we conduct a systematic review of existing cross-domain policy transfer methods.","Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting.","We also provide a high-level discussion about the key methodologies used in cross-domain policy transfer problems.","Lastly, we summarize the open challenges that lie beyond the capabilities of current paradigms and discuss potential future directions in this field."],"url":"http://arxiv.org/abs/2402.04580v1","category":"cs.RO"}
{"created":"2024-02-07 04:39:23","title":"Collective Counterfactual Explanations via Optimal Transport","abstract":"Counterfactual explanations provide individuals with cost-optimal actions that can alter their labels to desired classes. However, if substantial instances seek state modification, such individual-centric methods can lead to new competitions and unanticipated costs. Furthermore, these recommendations, disregarding the underlying data distribution, may suggest actions that users perceive as outliers. To address these issues, our work proposes a collective approach for formulating counterfactual explanations, with an emphasis on utilizing the current density of the individuals to inform the recommended actions. Our problem naturally casts as an optimal transport problem. Leveraging the extensive literature on optimal transport, we illustrate how this collective method improves upon the desiderata of classical counterfactual explanations. We support our proposal with numerical simulations, illustrating the effectiveness of the proposed approach and its relation to classic methods.","sentences":["Counterfactual explanations provide individuals with cost-optimal actions that can alter their labels to desired classes.","However, if substantial instances seek state modification, such individual-centric methods can lead to new competitions and unanticipated costs.","Furthermore, these recommendations, disregarding the underlying data distribution, may suggest actions that users perceive as outliers.","To address these issues, our work proposes a collective approach for formulating counterfactual explanations, with an emphasis on utilizing the current density of the individuals to inform the recommended actions.","Our problem naturally casts as an optimal transport problem.","Leveraging the extensive literature on optimal transport, we illustrate how this collective method improves upon the desiderata of classical counterfactual explanations.","We support our proposal with numerical simulations, illustrating the effectiveness of the proposed approach and its relation to classic methods."],"url":"http://arxiv.org/abs/2402.04579v1","category":"cs.LG"}
{"created":"2024-02-07 04:36:31","title":"S-Agents: self-organizing agents in open-ended environment","abstract":"Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a \"tree of agents\" structure for dynamic workflow, an \"hourglass agent architecture\" for balancing information priorities, and a \"non-obstructive collaboration\" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection in the Minecraft environment, validating their effectiveness.","sentences":["Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks.","In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments.","Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures.","Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a \"tree of agents\" structure for dynamic workflow, an \"hourglass agent architecture\" for balancing information priorities, and a \"non-obstructive collaboration\" method to allow asynchronous task execution among agents.","This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention.","Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection in the Minecraft environment, validating their effectiveness."],"url":"http://arxiv.org/abs/2402.04578v1","category":"cs.AI"}
{"created":"2024-02-07 04:09:21","title":"RIS-NOMA integrated low-complexity transceiver architecture: Sum rate and energy efficiency perspective","abstract":"This paper aims to explore reconfigurable intelligent surface (RIS) integration in a millimeter wave (mmWave) communication system with low-complexity transceiver architecture under imperfect CSI assumption. Towards this, we propose a RIS-aided system with a fully analog (FA) architecture at the base station. However, to overcome the disadvantage of single-user transmission due to the single RF-chain, we employ NOMA. For such a system, we formulate sum rate (SR) and energy efficiency (EE) maximization problems to obtain the joint transmit beamformer, RIS phase shift matrix, and power allocation solutions under minimum rate constraint. We first tackle the fractional objectives of both problems by reformulating the SR and EE maximization problems into equivalent quadratic forms using the quadratic transform. On the other hand, we employ successive convex approximation and the semi-definite relaxation technique to handle the non-convex minimum rate and unit modulus constraint of the RIS phase shifts, respectively. Next, we propose an alternating optimization-based algorithm that iterates over the transmit beamformer, power allocation, and RIS phase shift subproblems. Further, we also show that the quadratic reformulation is equivalent to the WMSE-based reformulation for the case of SR maximization problem. Our numerical results show that the proposed RIS-NOMA integrated FA architecture system outperforms the optimally configured fully digital architecture in terms of SR at low SNR and EE for a wide range of SNR while still maintaining low hardware complexity and cost. Finally, we present the numerical performance analysis of the RIS-NOMA integrated low-complexity system for various system configuration parameters.","sentences":["This paper aims to explore reconfigurable intelligent surface (RIS) integration in a millimeter wave (mmWave) communication system with low-complexity transceiver architecture under imperfect CSI assumption.","Towards this, we propose a RIS-aided system with a fully analog (FA) architecture at the base station.","However, to overcome the disadvantage of single-user transmission due to the single RF-chain, we employ NOMA.","For such a system, we formulate sum rate (SR) and energy efficiency (EE) maximization problems to obtain the joint transmit beamformer, RIS phase shift matrix, and power allocation solutions under minimum rate constraint.","We first tackle the fractional objectives of both problems by reformulating the SR and EE maximization problems into equivalent quadratic forms using the quadratic transform.","On the other hand, we employ successive convex approximation and the semi-definite relaxation technique to handle the non-convex minimum rate and unit modulus constraint of the RIS phase shifts, respectively.","Next, we propose an alternating optimization-based algorithm that iterates over the transmit beamformer, power allocation, and RIS phase shift subproblems.","Further, we also show that the quadratic reformulation is equivalent to the WMSE-based reformulation for the case of SR maximization problem.","Our numerical results show that the proposed RIS-NOMA integrated FA architecture system outperforms the optimally configured fully digital architecture in terms of SR at low SNR and EE for a wide range of SNR while still maintaining low hardware complexity and cost.","Finally, we present the numerical performance analysis of the RIS-NOMA integrated low-complexity system for various system configuration parameters."],"url":"http://arxiv.org/abs/2402.04570v1","category":"cs.IT"}
{"created":"2024-02-07 04:06:53","title":"OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences","abstract":"Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about agents' behavioural data, from which we derive two features for anomaly detection. The intuition behind our method is that the action optimality feature derived from the Q function can differentiate the optimal action from others at each local state, and the sequential association feature derived from the state value function has the potential to maintain the temporal correlations between decisions (state-action pairs). Our experiments show that OIL-AD can achieve outstanding online anomaly detection performance with up to 34.8% improvement in F1 score over comparable baselines.","sentences":["Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task.","Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment.","To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association.","Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories.","We propose that the Q function and the state value function can provide sufficient information about agents' behavioural data, from which we derive two features for anomaly detection.","The intuition behind our method is that the action optimality feature derived from the Q function can differentiate the optimal action from others at each local state, and the sequential association feature derived from the state value function has the potential to maintain the temporal correlations between decisions (state-action pairs).","Our experiments show that OIL-AD can achieve outstanding online anomaly detection performance with up to 34.8% improvement in F1 score over comparable baselines."],"url":"http://arxiv.org/abs/2402.04567v1","category":"cs.LG"}
{"created":"2024-02-07 03:43:56","title":"Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention","abstract":"Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.","sentences":["Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks.","In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure.","In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision.","Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image.","These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores.","They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism.","This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels.","As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object.","Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test."],"url":"http://arxiv.org/abs/2402.04563v1","category":"cs.CV"}
{"created":"2024-02-07 03:37:19","title":"Can Large Language Model Agents Simulate Human Trust Behaviors?","abstract":"Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications for various scenarios where trust is paramount. Our study represents a significant step in understanding the behaviors of LLM agents and the LLM-human analogy.","sentences":["Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science.","However, one fundamental question remains: can LLM agents really simulate human behaviors?","In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors.","We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics.","Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents.","In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans.","We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations.","We further offer important implications for various scenarios where trust is paramount.","Our study represents a significant step in understanding the behaviors of LLM agents and the LLM-human analogy."],"url":"http://arxiv.org/abs/2402.04559v1","category":"cs.AI"}
{"created":"2024-02-07 03:25:08","title":"An Artificial Intelligence (AI) workflow for catalyst design and optimization","abstract":"In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space. The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques. However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis. To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization. Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable parameters for practical experimentation and optimization. In this article, we demonstrate the application of this AI workflow in the optimization of catalyst synthesis for ammonia production. The results underscore the workflow's ability to streamline the catalyst development process, offering a swift, resource-efficient, and high-precision alternative to conventional methods.","sentences":["In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space.","The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques.","However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis.","To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates Large Language Models (LLMs), Bayesian optimization, and an active learning loop to expedite and enhance catalyst optimization.","Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable parameters for practical experimentation and optimization.","In this article, we demonstrate the application of this AI workflow in the optimization of catalyst synthesis for ammonia production.","The results underscore the workflow's ability to streamline the catalyst development process, offering a swift, resource-efficient, and high-precision alternative to conventional methods."],"url":"http://arxiv.org/abs/2402.04557v1","category":"physics.chem-ph"}
{"created":"2024-02-07 03:05:54","title":"NORMY: Non-Uniform History Modeling for Open Retrieval Conversational Question Answering","abstract":"Open Retrieval Conversational Question Answering (OrConvQA) answers a question given a conversation as context and a document collection. A typical OrConvQA pipeline consists of three modules: a Retriever to retrieve relevant documents from the collection, a Reranker to rerank them given the question and the context, and a Reader to extract an answer span. The conversational turns can provide valuable context to answer the final query. State-of-the-art OrConvQA systems use the same history modeling for all three modules of the pipeline. We hypothesize this as suboptimal. Specifically, we argue that a broader context is needed in the first modules of the pipeline to not miss relevant documents, while a narrower context is needed in the last modules to identify the exact answer span. We propose NORMY, the first unsupervised non-uniform history modeling pipeline which generates the best conversational history for each module. We further propose a novel Retriever for NORMY, which employs keyphrase extraction on the conversation history, and leverages passages retrieved in previous turns as additional context. We also created a new dataset for OrConvQA, by expanding the doc2dial dataset. We implemented various state-of-the-art history modeling techniques and comprehensively evaluated them separately for each module of the pipeline on three datasets: OR-QUAC, our doc2dial extension, and ConvMix. Our extensive experiments show that NORMY outperforms the state-of-the-art in the individual modules and in the end-to-end system.","sentences":["Open Retrieval Conversational Question Answering (OrConvQA) answers a question given a conversation as context and a document collection.","A typical OrConvQA pipeline consists of three modules: a Retriever to retrieve relevant documents from the collection, a Reranker to rerank them given the question and the context, and a Reader to extract an answer span.","The conversational turns can provide valuable context to answer the final query.","State-of-the-art OrConvQA systems use the same history modeling for all three modules of the pipeline.","We hypothesize this as suboptimal.","Specifically, we argue that a broader context is needed in the first modules of the pipeline to not miss relevant documents, while a narrower context is needed in the last modules to identify the exact answer span.","We propose NORMY, the first unsupervised non-uniform history modeling pipeline which generates the best conversational history for each module.","We further propose a novel Retriever for NORMY, which employs keyphrase extraction on the conversation history, and leverages passages retrieved in previous turns as additional context.","We also created a new dataset for OrConvQA, by expanding the doc2dial dataset.","We implemented various state-of-the-art history modeling techniques and comprehensively evaluated them separately for each module of the pipeline on three datasets: OR-QUAC, our doc2dial extension, and ConvMix.","Our extensive experiments show that NORMY outperforms the state-of-the-art in the individual modules and in the end-to-end system."],"url":"http://arxiv.org/abs/2402.04548v1","category":"cs.IR"}
{"created":"2024-02-07 02:53:50","title":"Learning Diverse Policies with Soft Self-Generated Guidance","abstract":"Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still being able to learn without rewards and approach optimality. Furthermore, a novel diversity measurement is introduced to maintain the team's diversity and regulate exploration. The proposed algorithm is evaluated on discrete and continuous control tasks with sparse and deceptive rewards. Compared with the existing RL methods, the experimental results indicate that our proposed algorithm is significantly better than the baseline methods regarding diverse exploration and avoiding local optima.","sentences":["Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained.","Hence, the gradient calculated by the agent can be stochastic and without valid information.","Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process.","However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors.","This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded.","The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data.","The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still being able to learn without rewards and approach optimality.","Furthermore, a novel diversity measurement is introduced to maintain the team's diversity and regulate exploration.","The proposed algorithm is evaluated on discrete and continuous control tasks with sparse and deceptive rewards.","Compared with the existing RL methods, the experimental results indicate that our proposed algorithm is significantly better than the baseline methods regarding diverse exploration and avoiding local optima."],"url":"http://arxiv.org/abs/2402.04539v1","category":"cs.LG"}
{"created":"2024-02-07 02:50:56","title":"Tactile-based Object Retrieval From Granular Media","abstract":"We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first method to reliably retrieve a number of different objects from a granular environment, doing so on real hardware and with integrated tactile sensing. Videos and additional information can be found at https://jxu.ai/geotact.","sentences":["We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media.","This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision.","Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings.","To address these challenges, we use a learning method trained end-to-end with simulated sensor noise.","We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings.","We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware.","To the best of our knowledge, GEOTACT is the first method to reliably retrieve a number of different objects from a granular environment, doing so on real hardware and with integrated tactile sensing.","Videos and additional information can be found at https://jxu.ai/geotact."],"url":"http://arxiv.org/abs/2402.04536v1","category":"cs.RO"}
{"created":"2024-02-07 02:42:40","title":"Joint Beamforming Design for Double Active RIS-assisted Radar-Communication Coexistence Systems","abstract":"Integrated sensing and communication (ISAC) technology has been considered as one of the key candidate technologies in the next-generation wireless communication systems. However, when radar and communication equipment coexist in the same system, i.e. radar-communication coexistence (RCC), the interference from communication systems to radar can be large and cannot be ignored. Recently, reconfigurable intelligent surface (RIS) has been introduced into RCC systems to reduce the interference. However, the \"multiplicative fading\" effect introduced by passive RIS limits its performance. To tackle this issue, we consider a double active RIS-assisted RCC system, which focuses on the design of the radar's beamforming vector and the active RISs' reflecting coefficient matrices, to maximize the achievable data rate of the communication system. The considered system needs to meet the radar detection constraint and the power budgets at the radar and the RISs. Since the problem is non-convex, we propose an algorithm based on the penalty dual decomposition (PDD) framework. Specifically, we initially introduce auxiliary variables to reformulate the coupled variables into equation constraints and incorporate these constraints into the objective function through the PDD framework. Then, we decouple the equivalent problem into several subproblems by invoking the block coordinate descent (BCD) method. Furthermore, we employ the Lagrange dual method to alternately optimize these subproblems. Simulation results verify the effectiveness of the proposed algorithm. Furthermore, the results also show that under the same power budget, deploying double active RISs in RCC systems can achieve higher data rate than those with single active RIS and double passive RISs.","sentences":["Integrated sensing and communication (ISAC) technology has been considered as one of the key candidate technologies in the next-generation wireless communication systems.","However, when radar and communication equipment coexist in the same system, i.e. radar-communication coexistence (RCC), the interference from communication systems to radar can be large and cannot be ignored.","Recently, reconfigurable intelligent surface (RIS) has been introduced into RCC systems to reduce the interference.","However, the \"multiplicative fading\" effect introduced by passive RIS limits its performance.","To tackle this issue, we consider a double active RIS-assisted RCC system, which focuses on the design of the radar's beamforming vector and the active RISs' reflecting coefficient matrices, to maximize the achievable data rate of the communication system.","The considered system needs to meet the radar detection constraint and the power budgets at the radar and the RISs.","Since the problem is non-convex, we propose an algorithm based on the penalty dual decomposition (PDD) framework.","Specifically, we initially introduce auxiliary variables to reformulate the coupled variables into equation constraints and incorporate these constraints into the objective function through the PDD framework.","Then, we decouple the equivalent problem into several subproblems by invoking the block coordinate descent (BCD) method.","Furthermore, we employ the Lagrange dual method to alternately optimize these subproblems.","Simulation results verify the effectiveness of the proposed algorithm.","Furthermore, the results also show that under the same power budget, deploying double active RISs in RCC systems can achieve higher data rate than those with single active RIS and double passive RISs."],"url":"http://arxiv.org/abs/2402.04532v1","category":"eess.SP"}
{"created":"2024-02-07 02:14:58","title":"RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation","abstract":"Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures. Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperforms current state-of-the-art methods, achieving up to 3.0% absolute HitRate@100 improvements while utilizing less than 10x training data.","sentences":["Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS.","Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness.","To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner.","In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures.","Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment.","Extensive experiments demonstrate RA-Rec substantially outperforms current state-of-the-art methods, achieving up to 3.0% absolute HitRate@100 improvements while utilizing less than 10x training data."],"url":"http://arxiv.org/abs/2402.04527v1","category":"cs.IR"}
{"created":"2024-02-07 01:58:21","title":"On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis","abstract":"We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\\Max\\{$# of stored memory patterns, length of input query sequence$\\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.","sentences":["We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis.","Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns.","Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns.","Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH).","To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds.","This includes a derivation of a lower bound on the computational time, scaling linearly with $\\Max\\{$# of stored memory patterns, length of input query sequence$\\}$.","In addition, we prove its memory retrieval error bound and exponential memory capacity."],"url":"http://arxiv.org/abs/2402.04520v1","category":"cs.LG"}
{"created":"2024-02-07 01:48:29","title":"A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks","abstract":"Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics. The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability. However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms. These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks. In this paper, we design and develop a deep reinforcement learning (DRL) approach for adaptive traffic routing. We design a deep graph convolutional neural network (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes. We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need for a labeled training dataset, enabling the framework to quickly adapt to traffic dynamics. The model leverages q-value estimates to select the routing path for every traffic flow request, balancing exploration and exploitation. We perform extensive experiments with various traffic patterns and compare the performance of the proposed approach with the Open Shortest Path First (OSPF) protocol. The experimental results show the effectiveness and adaptiveness of the proposed framework by increasing the network throughput by up to 7.8% and reducing the traffic delay by up to 16.1% compared to OSPF.","sentences":["Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics.","The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability.","However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms.","These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks.","In this paper, we design and develop a deep reinforcement learning (DRL) approach for adaptive traffic routing.","We design a deep graph convolutional neural network (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes.","We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need for a labeled training dataset, enabling the framework to quickly adapt to traffic dynamics.","The model leverages q-value estimates to select the routing path for every traffic flow request, balancing exploration and exploitation.","We perform extensive experiments with various traffic patterns and compare the performance of the proposed approach with the Open Shortest Path First (OSPF) protocol.","The experimental results show the effectiveness and adaptiveness of the proposed framework by increasing the network throughput by up to 7.8% and reducing the traffic delay by up to 16.1% compared to OSPF."],"url":"http://arxiv.org/abs/2402.04515v1","category":"cs.NI"}
{"created":"2024-02-07 01:23:59","title":"Processing All-Sky Images At Scale On The Amazon Cloud: A HiPS Example","abstract":"We report here on a project that has developed a practical approach to processing all-sky image collections on cloud platforms, using as an exemplar application the creation of three-color Hierarchical Progressive Survey (HiPS) maps of the 2MASS data set with the Montage Image Mosaic Engine on Amazon Web Services. We will emphasize issues that must be considered by scientists wishing to use cloud platforms to perform such parallel processing, so providing a guide for scientists wishing to exploit cloud platforms for similar large-scale processing. A HiPS map is based on the HEALPix sky-tiling scheme. Progressive zooming of a HiPS map reveals an image sampled at ever smaller or larger spatial scales that are defined by the HEALPix standard. Briefly, the approach used by Montage involves creating a base mosaic at the lowest required HEALPix level, usually chosen to match as closely as possible the spatial sampling of the input images, then cutting out the HiPS cells in PNG format from this mosaic. The process is repeated at successive HEALPix levels to create a nested collection of FITS files, from which PNG files are created that are shown in HiPS viewers. Stretching FITS files to produce PNGs is based on an image histogram. For composite regions (up and including the whole sky), the histograms for each tile can be combined to create a composite histogram for the region. Using this single histogram for each of the individual FITS files means all the PNGs are on the same brightness scale and displaying them side by side in a HiPS viewer produces a continuous uniform map across the entire sky.","sentences":["We report here on a project that has developed a practical approach to processing all-sky image collections on cloud platforms, using as an exemplar application the creation of three-color Hierarchical Progressive Survey (HiPS) maps of the 2MASS data set with the Montage Image Mosaic Engine on Amazon Web Services.","We will emphasize issues that must be considered by scientists wishing to use cloud platforms to perform such parallel processing, so providing a guide for scientists wishing to exploit cloud platforms for similar large-scale processing.","A HiPS map is based on the HEALPix sky-tiling scheme.","Progressive zooming of a HiPS map reveals an image sampled at ever smaller or larger spatial scales that are defined by the HEALPix standard.","Briefly, the approach used by Montage involves creating a base mosaic at the lowest required HEALPix level, usually chosen to match as closely as possible the spatial sampling of the input images, then cutting out the HiPS cells in PNG format from this mosaic.","The process is repeated at successive HEALPix levels to create a nested collection of FITS files, from which PNG files are created that are shown in HiPS viewers.","Stretching FITS files to produce PNGs is based on an image histogram.","For composite regions (up and including the whole sky), the histograms for each tile can be combined to create a composite histogram for the region.","Using this single histogram for each of the individual FITS files means all the PNGs are on the same brightness scale and displaying them side by side in a HiPS viewer produces a continuous uniform map across the entire sky."],"url":"http://arxiv.org/abs/2402.04506v1","category":"astro-ph.IM"}
{"created":"2024-02-07 00:36:24","title":"Grandmaster-Level Chess Without Search","abstract":"The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.","sentences":["The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale.","This paper investigates the impact of training at scale for chess.","Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games.","We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points.","Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms.","We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct.","A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale.","To validate our results, we perform an extensive series of ablations of design choices and hyperparameters."],"url":"http://arxiv.org/abs/2402.04494v1","category":"cs.LG"}
{"created":"2024-02-07 00:33:11","title":"A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs","abstract":"Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\\epsilon^{-2})$ for finding an $\\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals.","sentences":["Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset.","Offline RL with low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\\epsilon^{-2})$ for finding an $\\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient.","In this paper, we propose a primal dual algorithm for offline RL with low-rank MDPs in the discounted infinite-horizon setting.","Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\\epsilon^{-2})$ with partial data coverage assumption.","This improves upon a recent work that requires $O(\\epsilon^{-4})$ samples.","Moreover, our algorithm extends the previous work to the offline constrained RL setting by supporting constraints on additional reward signals."],"url":"http://arxiv.org/abs/2402.04493v1","category":"stat.ML"}
{"created":"2024-02-07 00:31:40","title":"Modeling and Characterizing Service Interference in Dynamic Infrastructures","abstract":"Performance interference can occur when various services are executed over the same physical infrastructure in a cloud system. This can lead to performance degradation compared to the execution of services in isolation. This work proposes a Confirmatory Factor Analysis (CFA)-based model to estimate performance interference across containers, caused by the use of CPU, memory and IO across a number of co-hosted applications. The approach provides resource characterization through human comprehensible indices expressed as time series, so the interference in the entire execution lifetime of a service can be analyzed. Our experiments, based on the combination of real services with different profiles executed in Docker containers, suggest that our model can accurately predict the overall execution time, for different service combinations. The approach can be used by a service designer to identify phases, during the execution life-cycle of a service, that are likely to lead to a greater degree of interference, and to ensure that only complementary services are hosted on the same physical machine. Interference-awareness of this kind will enable more intelligent resource management and scheduling for cloud systems, and may be used to dynamically modify scheduling decisions.","sentences":["Performance interference can occur when various services are executed over the same physical infrastructure in a cloud system.","This can lead to performance degradation compared to the execution of services in isolation.","This work proposes a Confirmatory Factor Analysis (CFA)-based model to estimate performance interference across containers, caused by the use of CPU, memory and IO across a number of co-hosted applications.","The approach provides resource characterization through human comprehensible indices expressed as time series, so the interference in the entire execution lifetime of a service can be analyzed.","Our experiments, based on the combination of real services with different profiles executed in Docker containers, suggest that our model can accurately predict the overall execution time, for different service combinations.","The approach can be used by a service designer to identify phases, during the execution life-cycle of a service, that are likely to lead to a greater degree of interference, and to ensure that only complementary services are hosted on the same physical machine.","Interference-awareness of this kind will enable more intelligent resource management and scheduling for cloud systems, and may be used to dynamically modify scheduling decisions."],"url":"http://arxiv.org/abs/2402.04491v1","category":"cs.DC"}
{"created":"2024-02-06 23:52:58","title":"Detecting Mode Collapse in Language Models via Narration","abstract":"No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby overfitting the model during alignment constrains it from generalizing over authorship: models suffering from mode collapse become unable to assume a multiplicity of perspectives. Our method and results are significant for researchers seeking to employ language models in sociological simulations.","sentences":["No two authors write alike.","Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text.","Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives.","Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny.","By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby overfitting the model during alignment constrains it from generalizing over authorship: models suffering from mode collapse become unable to assume a multiplicity of perspectives.","Our method and results are significant for researchers seeking to employ language models in sociological simulations."],"url":"http://arxiv.org/abs/2402.04477v1","category":"cs.CL"}
{"created":"2024-02-06 23:52:10","title":"Dual-View Visual Contextualization for Web Navigation","abstract":"Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their \"dual views\" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We validate our method on the recently released Mind2Web dataset, which features diverse navigation domains and tasks on real-world websites. Our method consistently outperforms the baseline in all the scenarios, including cross-task, cross-website, and cross-domain ones.","sentences":["Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites.","Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages.","Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions.","In this paper, we propose to contextualize HTML elements through their \"dual views\" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot.","We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features.","The resulting representations of HTML elements are more informative for the agent to take action.","We validate our method on the recently released Mind2Web dataset, which features diverse navigation domains and tasks on real-world websites.","Our method consistently outperforms the baseline in all the scenarios, including cross-task, cross-website, and cross-domain ones."],"url":"http://arxiv.org/abs/2402.04476v1","category":"cs.CV"}
{"created":"2024-02-06 23:42:55","title":"Performance of a Kinetic Inductance Phonon-Mediated Detector at the NEXUS Cryogenic Facility","abstract":"Microcalorimeters that leverage microwave kinetic inductance detectors to read out phonon signals in the particle-absorbing target, referred to as kinetic inductance phonon-mediated (KIPM) detectors, offer an attractive detector architecture to probe dark matter (DM) down to the fermionic thermal relic mass limit. A prototype KIPM detector featuring a single aluminum resonator patterned onto a 1-gram silicon substrate was operated in the NEXUS low-background facility at Fermilab for characterization and evaluation of this detector architecture's efficacy for a dark matter search. An energy calibration was performed by exposing the bare substrate to a pulsed source of 470 nm photons, resulting in a baseline resolution on the energy absorbed by the phonon sensor of $2.1\\pm0.2$ eV, a factor of two better than the current state-of-the-art, enabled by millisecond-scale quasiparticle lifetimes. However, due to the sub-percent phonon collection efficiency, the resolution on energy deposited in the substrate is limited to $\\sigma_E=318 \\pm 28$ eV. We further model the signal pulse shape as a function of device temperature to extract quasiparticle lifetimes, as well as the observed noise spectra, both of which impact the baseline resolution of the sensor.","sentences":["Microcalorimeters that leverage microwave kinetic inductance detectors to read out phonon signals in the particle-absorbing target, referred to as kinetic inductance phonon-mediated (KIPM) detectors, offer an attractive detector architecture to probe dark matter (DM) down to the fermionic thermal relic mass limit.","A prototype KIPM detector featuring a single aluminum resonator patterned onto a 1-gram silicon substrate was operated in the NEXUS low-background facility at Fermilab for characterization and evaluation of this detector architecture's efficacy for a dark matter search.","An energy calibration was performed by exposing the bare substrate to a pulsed source of 470 nm photons, resulting in a baseline resolution on the energy absorbed by the phonon sensor of $2.1\\pm0.2$ eV, a factor of two better than the current state-of-the-art, enabled by millisecond-scale quasiparticle lifetimes.","However, due to the sub-percent phonon collection efficiency, the resolution on energy deposited in the substrate is limited to $\\sigma_E=318 \\pm 28$ eV. We further model the signal pulse shape as a function of device temperature to extract quasiparticle lifetimes, as well as the observed noise spectra, both of which impact the baseline resolution of the sensor."],"url":"http://arxiv.org/abs/2402.04473v1","category":"physics.ins-det"}
{"created":"2024-02-06 23:20:34","title":"Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan","abstract":"The introduction of AI and ML technologies into medical devices has revolutionized healthcare diagnostics and treatments. Medical device manufacturers are keen to maximize the advantages afforded by AI and ML by consolidating multiple applications onto a single platform. However, concurrent execution of several AI applications, each with its own visualization components, leads to unpredictable end-to-end latency, primarily due to GPU resource contentions. To mitigate this, manufacturers typically deploy separate workstations for distinct AI applications, thereby increasing financial, energy, and maintenance costs. This paper addresses these challenges within the context of NVIDIA's Holoscan platform, a real-time AI system for streaming sensor data and images. We propose a system design optimized for heterogeneous GPU workloads, encompassing both compute and graphics tasks. Our design leverages CUDA MPS for spatial partitioning of compute workloads and isolates compute and graphics processing onto separate GPUs. We demonstrate significant performance improvements across various end-to-end latency determinism metrics through empirical evaluation with real-world Holoscan medical device applications. For instance, the proposed design reduces maximum latency by 21-30% and improves latency distribution flatness by 17-25% for up to five concurrent endoscopy tool tracking AI applications, compared to a single-GPU baseline. Against a default multi-GPU setup, our optimizations decrease maximum latency by 35% for up to six concurrent applications by improving GPU utilization by 42%. This paper provides clear design insights for AI applications in the edge-computing domain including medical systems, where performance predictability of concurrent and heterogeneous GPU workloads is a critical requirement.","sentences":["The introduction of AI and ML technologies into medical devices has revolutionized healthcare diagnostics and treatments.","Medical device manufacturers are keen to maximize the advantages afforded by AI and ML by consolidating multiple applications onto a single platform.","However, concurrent execution of several AI applications, each with its own visualization components, leads to unpredictable end-to-end latency, primarily due to GPU resource contentions.","To mitigate this, manufacturers typically deploy separate workstations for distinct AI applications, thereby increasing financial, energy, and maintenance costs.","This paper addresses these challenges within the context of NVIDIA's Holoscan platform, a real-time AI system for streaming sensor data and images.","We propose a system design optimized for heterogeneous GPU workloads, encompassing both compute and graphics tasks.","Our design leverages CUDA MPS for spatial partitioning of compute workloads and isolates compute and graphics processing onto separate GPUs.","We demonstrate significant performance improvements across various end-to-end latency determinism metrics through empirical evaluation with real-world Holoscan medical device applications.","For instance, the proposed design reduces maximum latency by 21-30% and improves latency distribution flatness by 17-25% for up to five concurrent endoscopy tool tracking AI applications, compared to a single-GPU baseline.","Against a default multi-GPU setup, our optimizations decrease maximum latency by 35% for up to six concurrent applications by improving GPU utilization by 42%.","This paper provides clear design insights for AI applications in the edge-computing domain including medical systems, where performance predictability of concurrent and heterogeneous GPU workloads is a critical requirement."],"url":"http://arxiv.org/abs/2402.04466v1","category":"cs.SE"}
{"created":"2024-02-06 23:16:41","title":"Ten Hard Problems in Artificial Intelligence We Must Get Right","abstract":"We explore the AI2050 \"hard problems\" that block the promise of AI and cause AI risks: (1) developing general capabilities of the systems; (2) assuring the performance of AI systems and their training processes; (3) aligning system goals with human goals; (4) enabling great applications of AI in real life; (5) addressing economic disruptions; (6) ensuring the participation of all; (7) at the same time ensuring socially responsible deployment; (8) addressing any geopolitical disruptions that AI causes; (9) promoting sound governance of the technology; and (10) managing the philosophical disruptions for humans living in the age of AI. For each problem, we outline the area, identify significant recent work, and suggest ways forward. [Note: this paper reviews literature through January 2023.]","sentences":["We explore the AI2050 \"hard problems\" that block the promise of AI and cause AI risks: (1) developing general capabilities of the systems; (2) assuring the performance of AI systems and their training processes; (3) aligning system goals with human goals; (4) enabling great applications of AI in real life; (5) addressing economic disruptions; (6) ensuring the participation of all; (7) at the same time ensuring socially responsible deployment; (8) addressing any geopolitical disruptions that AI causes; (9) promoting sound governance of the technology; and (10) managing the philosophical disruptions for humans living in the age of AI.","For each problem, we outline the area, identify significant recent work, and suggest ways forward.","[Note: this paper reviews literature through January 2023.]"],"url":"http://arxiv.org/abs/2402.04464v1","category":"cs.AI"}
{"created":"2024-02-06 23:10:31","title":"Combinatorial Optimization and Machine Learning for Dynamic Inventory Routing","abstract":"We introduce a combinatorial optimization-enriched machine learning pipeline and a novel learning paradigm to solve inventory routing problems with stochastic demand and dynamic inventory updates. After each inventory update, our approach reduces replenishment and routing decisions to an optimal solution of a capacitated prize-collecting traveling salesman problem for which well-established algorithms exist. Discovering good prize parametrizations is non-trivial; therefore, we have developed a machine learning approach. We evaluate the performance of our pipeline in settings with steady-state and more complex demand patterns. Compared to previous works, the policy generated by our algorithm leads to significant cost savings, achieves lower inference time, and can even leverage contextual information.","sentences":["We introduce a combinatorial optimization-enriched machine learning pipeline and a novel learning paradigm to solve inventory routing problems with stochastic demand and dynamic inventory updates.","After each inventory update, our approach reduces replenishment and routing decisions to an optimal solution of a capacitated prize-collecting traveling salesman problem for which well-established algorithms exist.","Discovering good prize parametrizations is non-trivial; therefore, we have developed a machine learning approach.","We evaluate the performance of our pipeline in settings with steady-state and more complex demand patterns.","Compared to previous works, the policy generated by our algorithm leads to significant cost savings, achieves lower inference time, and can even leverage contextual information."],"url":"http://arxiv.org/abs/2402.04463v1","category":"math.OC"}
{"created":"2024-02-06 22:35:04","title":"Failure Analysis in Next-Generation Critical Cellular Communication Infrastructures","abstract":"The advent of communication technologies marks a transformative phase in critical infrastructure construction, where the meticulous analysis of failures becomes paramount in achieving the fundamental objectives of continuity, security, and availability. This survey enriches the discourse on failures, failure analysis, and countermeasures in the context of the next-generation critical communication infrastructures. Through an exhaustive examination of existing literature, we discern and categorize prominent research orientations with focuses on, namely resource depletion, security vulnerabilities, and system availability concerns. We also analyze constructive countermeasures tailored to address identified failure scenarios and their prevention. Furthermore, the survey emphasizes the imperative for standardization in addressing failures related to Artificial Intelligence (AI) within the ambit of the sixth-generation (6G) networks, accounting for the forward-looking perspective for the envisioned intelligence of 6G network architecture. By identifying new challenges and delineating future research directions, this survey can help guide stakeholders toward unexplored territories, fostering innovation and resilience in critical communication infrastructure development and failure prevention.","sentences":["The advent of communication technologies marks a transformative phase in critical infrastructure construction, where the meticulous analysis of failures becomes paramount in achieving the fundamental objectives of continuity, security, and availability.","This survey enriches the discourse on failures, failure analysis, and countermeasures in the context of the next-generation critical communication infrastructures.","Through an exhaustive examination of existing literature, we discern and categorize prominent research orientations with focuses on, namely resource depletion, security vulnerabilities, and system availability concerns.","We also analyze constructive countermeasures tailored to address identified failure scenarios and their prevention.","Furthermore, the survey emphasizes the imperative for standardization in addressing failures related to Artificial Intelligence (AI) within the ambit of the sixth-generation (6G) networks, accounting for the forward-looking perspective for the envisioned intelligence of 6G network architecture.","By identifying new challenges and delineating future research directions, this survey can help guide stakeholders toward unexplored territories, fostering innovation and resilience in critical communication infrastructure development and failure prevention."],"url":"http://arxiv.org/abs/2402.04448v1","category":"eess.SY"}
{"created":"2024-02-06 22:34:57","title":"Context-Aware Spectrum Coexistence of Terrestrial Beyond 5G Networks in Satellite Bands","abstract":"Spectrum sharing between terrestrial 5G and incumbent networks in the satellite bands presents a promising avenue to satisfy the ever-increasing bandwidth demand of the next-generation wireless networks. However, protecting incumbent operations from harmful interference poses a fundamental challenge in accommodating terrestrial broadband cellular networks in the satellite bands. State-of-the-art spectrum-sharing policies usually consider several worst-case assumptions and ignore site-specific contextual factors in making spectrum-sharing decisions, and thus, often results in under-utilization of the shared band for the secondary licensees. To address such limitations, this paper introduces CAT3S (Context-Aware Terrestrial-Satellite Spectrum Sharing) framework that empowers the coexisting terrestrial 5G network to maximize utilization of the shared satellite band without creating harmful interference to the incumbent links by exploiting the contextual factors. CAT3S consists of the following two components: (i) context-acquisition unit to collect and process essential contextual information for spectrum sharing and (ii) context-aware base station (BS) control unit to optimize the set of operational BSs and their operation parameters (i.e., transmit power and active beams per sector). To evaluate the performance of the CAT3S, a realistic spectrum coexistence case study over the 12 GHz band is considered. Experiment results demonstrate that the proposed CAT3S achieves notably higher spectrum utilization than state-of-the-art spectrum-sharing policies in different weather contexts.","sentences":["Spectrum sharing between terrestrial 5G and incumbent networks in the satellite bands presents a promising avenue to satisfy the ever-increasing bandwidth demand of the next-generation wireless networks.","However, protecting incumbent operations from harmful interference poses a fundamental challenge in accommodating terrestrial broadband cellular networks in the satellite bands.","State-of-the-art spectrum-sharing policies usually consider several worst-case assumptions and ignore site-specific contextual factors in making spectrum-sharing decisions, and thus, often results in under-utilization of the shared band for the secondary licensees.","To address such limitations, this paper introduces CAT3S (Context-Aware Terrestrial-Satellite Spectrum Sharing) framework that empowers the coexisting terrestrial 5G network to maximize utilization of the shared satellite band without creating harmful interference to the incumbent links by exploiting the contextual factors.","CAT3S consists of the following two components: (i) context-acquisition unit to collect and process essential contextual information for spectrum sharing and (ii) context-aware base station (BS) control unit to optimize the set of operational BSs and their operation parameters (i.e., transmit power and active beams per sector).","To evaluate the performance of the CAT3S, a realistic spectrum coexistence case study over the 12 GHz band is considered.","Experiment results demonstrate that the proposed CAT3S achieves notably higher spectrum utilization than state-of-the-art spectrum-sharing policies in different weather contexts."],"url":"http://arxiv.org/abs/2402.04447v1","category":"cs.NI"}
{"created":"2024-02-06 22:13:49","title":"PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection","abstract":"Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and extensive experiments show the effectiveness of {\\method} in IP protection and maintaining high-performance for downstream tasks.","sentences":["Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks.","As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner.","However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks.","Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models.","Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space.","PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder.","A finetuning-resistant watermark injection is further deployed.","Theoretical analysis and extensive experiments show the effectiveness of {\\method} in IP protection and maintaining high-performance for downstream tasks."],"url":"http://arxiv.org/abs/2402.04435v1","category":"cs.LG"}
{"created":"2024-02-06 22:08:59","title":"ARMAN: A Reconfigurable Monolithic 3D Accelerator Architecture for Convolutional Neural Networks","abstract":"The Convolutional Neural Network (CNN) has emerged as a powerful and versatile tool for artificial intelligence (AI) applications. Conventional computing architectures face challenges in meeting the demanding processing requirements of compute-intensive CNN applications, as they suffer from limited throughput and low utilization. To this end, specialized accelerators have been developed to speed up CNN computations. However, as we demonstrate in this paper via extensive design space exploration, different neural network models have different characteristics, which calls for different accelerator architectures and configurations to match their computing demand. We show that a one-size-fits-all fixed architecture does not guarantee optimal power/energy/performance trade-off. To overcome this challenge, this paper proposes ARMAN, a novel reconfigurable systolic-array-based accelerator architecture based on Monolithic 3D (M3D) technology for CNN inference. The proposed accelerator offers the flexibility to reconfigure among different scale-up or scale-out arrangements depending on the neural network structure, providing the optimal trade-off across power, energy, and performance for various neural network models. We demonstrate the effectiveness of our approach through evaluations of multiple benchmarks. The results demonstrate that the proposed accelerator exhibits up to 2x, 2.24x, 1.48x, and 2x improvements in terms of execution cycles, power, energy, and EDP respectively, over the non-configurable architecture.","sentences":["The Convolutional Neural Network (CNN) has emerged as a powerful and versatile tool for artificial intelligence (AI) applications.","Conventional computing architectures face challenges in meeting the demanding processing requirements of compute-intensive CNN applications, as they suffer from limited throughput and low utilization.","To this end, specialized accelerators have been developed to speed up CNN computations.","However, as we demonstrate in this paper via extensive design space exploration, different neural network models have different characteristics, which calls for different accelerator architectures and configurations to match their computing demand.","We show that a one-size-fits-all fixed architecture does not guarantee optimal power/energy/performance trade-off.","To overcome this challenge, this paper proposes ARMAN, a novel reconfigurable systolic-array-based accelerator architecture based on Monolithic 3D (M3D) technology for CNN inference.","The proposed accelerator offers the flexibility to reconfigure among different scale-up or scale-out arrangements depending on the neural network structure, providing the optimal trade-off across power, energy, and performance for various neural network models.","We demonstrate the effectiveness of our approach through evaluations of multiple benchmarks.","The results demonstrate that the proposed accelerator exhibits up to 2x, 2.24x, 1.48x, and 2x improvements in terms of execution cycles, power, energy, and EDP respectively, over the non-configurable architecture."],"url":"http://arxiv.org/abs/2402.04431v1","category":"cs.AR"}
{"created":"2024-02-06 21:39:55","title":"Studying Vulnerable Code Entities in R","abstract":"Pre-trained Code Language Models (Code-PLMs) have shown many advancements and achieved state-of-the-art results for many software engineering tasks in the past few years. These models are mainly targeted for popular programming languages such as Java and Python, leaving out many other ones like R. Though R has a wide community of developers and users, there is little known about the applicability of Code-PLMs for R. In this preliminary study, we aim to investigate the vulnerability of Code-PLMs for code entities in R. For this purpose, we use an R dataset of code and comment pairs and then apply CodeAttack, a black-box attack model that uses the structure of code to generate adversarial code samples. We investigate how the model can attack different entities in R. This is the first step towards understanding the importance of R token types, compared to popular programming languages (e.g., Java). We limit our study to code summarization. Our results show that the most vulnerable code entity is the identifier, followed by some syntax tokens specific to R. The results can shed light on the importance of token types and help in developing models for code summarization and method name prediction for the R language.","sentences":["Pre-trained Code Language Models (Code-PLMs) have shown many advancements and achieved state-of-the-art results for many software engineering tasks in the past few years.","These models are mainly targeted for popular programming languages such as Java and Python, leaving out many other ones like R. Though R has a wide community of developers and users, there is little known about the applicability of Code-PLMs for R.","In this preliminary study, we aim to investigate the vulnerability of Code-PLMs for code entities in R. For this purpose, we use an R dataset of code and comment pairs and then apply CodeAttack, a black-box attack model that uses the structure of code to generate adversarial code samples.","We investigate how the model can attack different entities in R. This is the first step towards understanding the importance of R token types, compared to popular programming languages (e.g., Java).","We limit our study to code summarization.","Our results show that the most vulnerable code entity is the identifier, followed by some syntax tokens specific to R. The results can shed light on the importance of token types and help in developing models for code summarization and method name prediction for the R language."],"url":"http://arxiv.org/abs/2402.04421v1","category":"cs.SE"}
{"created":"2024-02-06 21:39:13","title":"Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways","abstract":"As machine learning applications proliferate, we need an understanding of their potential for harm. However, current fairness metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conclude that harm cannot be the sole guide in fairness mitigation, and propose a nuanced perspective depending on who is experiencing what harm and why.","sentences":["As machine learning applications proliferate, we need an understanding of their potential for harm.","However, current fairness metrics are rarely grounded in human psychological experiences of harm.","Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors.","First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful.","Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors.","We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors.","This experiential harm impacts women more than men.","However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity.","We conclude that harm cannot be the sole guide in fairness mitigation, and propose a nuanced perspective depending on who is experiencing what harm and why."],"url":"http://arxiv.org/abs/2402.04420v1","category":"cs.CY"}
{"created":"2024-02-06 21:18:34","title":"The VampPrior Mixture Model","abstract":"Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.","sentences":["Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations.","Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously.","We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs.","We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters.","Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets.","Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters."],"url":"http://arxiv.org/abs/2402.04412v1","category":"cs.LG"}
{"created":"2024-02-06 21:07:12","title":"Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning","abstract":"The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requires no validation operations or datasets. Our experimental results show that FRECA can accurately and efficiently quantify client contributions in a robust manner.","sentences":["The performance of clients in Federated Learning (FL) can vary due to various reasons.","Assessing the contributions of each client is crucial for client selection and compensation.","It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates.","The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset.","In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL.","FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones.","This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm.","FRECA is also efficient, as it operates solely on local model updates and requires no validation operations or datasets.","Our experimental results show that FRECA can accurately and efficiently quantify client contributions in a robust manner."],"url":"http://arxiv.org/abs/2402.04409v1","category":"cs.LG"}
{"created":"2024-02-06 20:58:36","title":"CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines","abstract":"Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format.","sentences":["Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data.","Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication.","Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data.","This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation.","In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format."],"url":"http://arxiv.org/abs/2402.04400v1","category":"cs.LG"}
{"created":"2024-02-06 20:56:31","title":"Learning from Time Series under Temporal Label Noise","abstract":"Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.","sentences":["Many sequential classification tasks are affected by label noise that varies over time.","Such noise can cause label quality to improve, worsen, or periodically change over time.","We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series.","In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function.","We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform.","We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data.","We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data."],"url":"http://arxiv.org/abs/2402.04398v1","category":"cs.LG"}
{"created":"2024-02-06 20:52:12","title":"QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks","abstract":"Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.","sentences":["Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision.","In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\\le$ 4 bits per weight) using three novel techniques.","First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties.","Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing.","Third, QuIP# uses fine-tuning to improve fidelity to the original model.","Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference."],"url":"http://arxiv.org/abs/2402.04396v1","category":"cs.LG"}
{"created":"2024-02-06 20:45:31","title":"Densely Multiplied Physics Informed Neural Network","abstract":"Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of the DM-PINN in both accuracy and efficiency.","sentences":["Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes.","Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN.","We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers.","Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs.","The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation).","Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of the DM-PINN in both accuracy and efficiency."],"url":"http://arxiv.org/abs/2402.04390v1","category":"cs.LG"}
{"created":"2024-02-06 20:44:37","title":"Control of seizure-like dynamics in neuronal populations with excitability adaptation related to ketogenic diet","abstract":"We consider a heterogeneous, globally coupled population of excitatory quadratic integrate-and-fire neurons with excitability adaptation due to a metabolic feedback associated with ketogenic diet, a form of therapy for epilepsy. Bifurcation analysis of a three-dimensional mean-field system derived in the framework of next-generation neural mass models allows us to explain the scenarios and suggest control strategies for the transitions between the neurophysiologically desired asynchronous states and the synchronous, seizure-like states featuring collective oscillations. We reveal two qualitatively different scenarios for the onset of synchrony. For weaker couplings, a bistability region between the lower- and the higher-activity asynchronous states unfolds from the cusp point, and the collective oscillations emerge via a supercritical Hopf bifurcation. For stronger couplings, one finds seven co-dimension two bifurcation points, including pairs of Bogdanov-Takens and generalized Hopf points, such that both lower- and higher-activity asynchronous states undergo transitions to collective oscillations, with hysteresis and jump-like behavior observed in vicinity of subcritical Hopf bifurcations. We demonstrate three control mechanisms for switching between asynchronous and synchronous states, involving parametric perturbation of the adenosine triphosphate (ATP) production rate, external stimulation currents, or pulse-like ATP shocks, and indicate a potential therapeutic advantage of hysteretic scenarios.","sentences":["We consider a heterogeneous, globally coupled population of excitatory quadratic integrate-and-fire neurons with excitability adaptation due to a metabolic feedback associated with ketogenic diet, a form of therapy for epilepsy.","Bifurcation analysis of a three-dimensional mean-field system derived in the framework of next-generation neural mass models allows us to explain the scenarios and suggest control strategies for the transitions between the neurophysiologically desired asynchronous states and the synchronous, seizure-like states featuring collective oscillations.","We reveal two qualitatively different scenarios for the onset of synchrony.","For weaker couplings, a bistability region between the lower- and the higher-activity asynchronous states unfolds from the cusp point, and the collective oscillations emerge via a supercritical Hopf bifurcation.","For stronger couplings, one finds seven co-dimension two bifurcation points, including pairs of Bogdanov-Takens and generalized Hopf points, such that both lower- and higher-activity asynchronous states undergo transitions to collective oscillations, with hysteresis and jump-like behavior observed in vicinity of subcritical Hopf bifurcations.","We demonstrate three control mechanisms for switching between asynchronous and synchronous states, involving parametric perturbation of the adenosine triphosphate (ATP) production rate, external stimulation currents, or pulse-like ATP shocks, and indicate a potential therapeutic advantage of hysteretic scenarios."],"url":"http://arxiv.org/abs/2402.04388v1","category":"nlin.AO"}
{"created":"2024-02-06 20:39:49","title":"Counterfactual Generation with Answer Set Programming","abstract":"Machine learning models that automate decision-making are increasingly being used in consequential areas such as loan approvals, pretrial bail approval, hiring, and many more. Unfortunately, most of these models are black-boxes, i.e., they are unable to reveal how they reach these prediction decisions. A need for transparency demands justification for such predictions. An affected individual might also desire explanations to understand why a decision was made. Ethical and legal considerations may further require informing the individual of changes in the input attribute that could be made to produce a desirable outcome. This paper focuses on the latter problem of automatically generating counterfactual explanations. We propose a framework Counterfactual Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and the s(CASP) goal-directed ASP system to automatically generate counterfactual explanations from rules generated by rule-based machine learning (RBML) algorithms. In our framework, we show how counterfactual explanations are computed and justified by imagining worlds where some or all factual assumptions are altered/changed. More importantly, we show how we can navigate between these worlds, namely, go from our original world/scenario where we obtain an undesired outcome to the imagined world/scenario where we obtain a desired/favourable outcome.","sentences":["Machine learning models that automate decision-making are increasingly being used in consequential areas such as loan approvals, pretrial bail approval, hiring, and many more.","Unfortunately, most of these models are black-boxes, i.e., they are unable to reveal how they reach these prediction decisions.","A need for transparency demands justification for such predictions.","An affected individual might also desire explanations to understand why a decision was made.","Ethical and legal considerations may further require informing the individual of changes in the input attribute that could be made to produce a desirable outcome.","This paper focuses on the latter problem of automatically generating counterfactual explanations.","We propose a framework Counterfactual Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and the s(CASP) goal-directed ASP system to automatically generate counterfactual explanations from rules generated by rule-based machine learning (RBML) algorithms.","In our framework, we show how counterfactual explanations are computed and justified by imagining worlds where some or all factual assumptions are altered/changed.","More importantly, we show how we can navigate between these worlds, namely, go from our original world/scenario where we obtain an undesired outcome to the imagined world/scenario where we obtain a desired/favourable outcome."],"url":"http://arxiv.org/abs/2402.04382v1","category":"cs.AI"}
{"created":"2024-02-06 20:30:19","title":"Scaling laws for learning with real and surrogate data","abstract":"Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gain from surrogate data.","sentences":["Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning.","One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models.","Blurring distinctions, we refer to such data as `surrogate data'.   ","We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior.","Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law.","This can be used to predict the optimal weighting and the gain from surrogate data."],"url":"http://arxiv.org/abs/2402.04376v1","category":"cs.LG"}
{"created":"2024-02-06 20:18:32","title":"The World of Generative AI: Deepfakes and Large Language Models","abstract":"We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in particular, pose an alarming threat to society as they are capable of spreading misinformation and changing the truth. LLMs are powerful language models that generate general-purpose language. However due to its generative aspect, it can also be a risk for people if used with ill intentions. The ethical use of these technologies is a big concern. This short article tries to find out the interrelationship between them.","sentences":["We live in the era of Generative Artificial Intelligence (GenAI).","Deepfakes and Large Language Models (LLMs) are two examples of GenAI.","Deepfakes, in particular, pose an alarming threat to society as they are capable of spreading misinformation and changing the truth.","LLMs are powerful language models that generate general-purpose language.","However due to its generative aspect, it can also be a risk for people if used with ill intentions.","The ethical use of these technologies is a big concern.","This short article tries to find out the interrelationship between them."],"url":"http://arxiv.org/abs/2402.04373v1","category":"cs.CY"}
{"created":"2024-02-06 20:13:34","title":"Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception","abstract":"This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality. It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations. While previous models of pedestrian behaviour have been either 'black-box' machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches. Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use reinforcement learning to learn bounded optimal behaviour policy. The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timing of the stopping distance of the yielding vehicle. Notably, our findings suggest that behaviours previously framed as 'biases' in decision-making, such as speed-dependent gap acceptance, might instead be a product of rational adaptation to the constraints of visual perception. Our approach also permits fitting the parameters of cognitive constraints and rewards per individual, to better account for individual differences. To conclude, by leveraging both RL and mechanistic modelling, our model offers novel insights about pedestrian behaviour, and may provide a useful foundation for more accurate and scalable pedestrian models.","sentences":["This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality.","It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations.","While previous models of pedestrian behaviour have been either 'black-box' machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches.","Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use reinforcement learning to learn bounded optimal behaviour policy.","The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timing of the stopping distance of the yielding vehicle.","Notably, our findings suggest that behaviours previously framed as 'biases' in decision-making, such as speed-dependent gap acceptance, might instead be a product of rational adaptation to the constraints of visual perception.","Our approach also permits fitting the parameters of cognitive constraints and rewards per individual, to better account for individual differences.","To conclude, by leveraging both RL and mechanistic modelling, our model offers novel insights about pedestrian behaviour, and may provide a useful foundation for more accurate and scalable pedestrian models."],"url":"http://arxiv.org/abs/2402.04370v1","category":"cs.AI"}
{"created":"2024-02-06 20:08:41","title":"A method to observe field-region oxide charge and inter-electrode isolation from $CV$-characteristics of $n$-on-$p$ devices","abstract":"$N$-on-$p$ silicon sensors will be utilized in the Compact Muon Solenoid (CMS) detector's tracker and High Granularity Calorimeter (HGCAL) in the High Luminosity upgrade of the Large Hadron Collider (HL-LHC). Among their several advantages in terms of radiation hardness over the traditional $p$-on-$n$ sensors in the extreme radiation environment of the HL-LHC are electron collection instead of holes and overlapping maxima of weighting and electric fields at the charge-collecting electrodes. The disadvantage of the multi-channel SiO$_2$-passivated $n$-on-$p$ sensors is the generation of an inversion layer under the Si/SiO$_2$-interface by a positive interface-oxide-charge ($N_\\textrm{ox}$) that at high densities can compromise the position resolution by creating a conduction channel between the electrodes. This issue is typically addressed by including additional isolation implants ($p$-stop, $p$-spray) between $n^+$-electrodes. Focusing on the guard-ring regions of $n$-on-$p$ sensors where no isolation implants are applied between the electrodes, a capacitance-voltage ($CV$) characterization study of both 6-inch wafer test diodes and 8-inch HGCAL prototype and pre-series sensors showed a distinct threshold voltage ($V_\\textrm{th,iso}$) in the $CV$-characteristics of a biased $n^+$-electrode when its enclosing guard-ring was left floating. When reproduced by simulations, the measured $V_\\textrm{th,iso}$ was found to contain information on the field-region $N_\\textrm{ox}$ and indicate the threshold where the two electrodes become electrically isolated by the influence of the reverse bias voltage. Together with previous studies on the inter-electrode isolation of irradiated $n$-on-$p$ sensors, the results indicate that position sensitive $n$-on-$p$ sensors without isolation implants may be feasible in future HEP experiments.","sentences":["$N$-on-$p$ silicon sensors will be utilized in the Compact Muon Solenoid (CMS) detector's tracker and High Granularity Calorimeter (HGCAL) in the High Luminosity upgrade of the Large Hadron Collider (HL-LHC).","Among their several advantages in terms of radiation hardness over the traditional $p$-on-$n$ sensors in the extreme radiation environment of the HL-LHC are electron collection instead of holes and overlapping maxima of weighting and electric fields at the charge-collecting electrodes.","The disadvantage of the multi-channel SiO$_2$-passivated $n$-on-$p$ sensors is the generation of an inversion layer under the Si/SiO$_2$-interface by a positive interface-oxide-charge ($N_\\textrm{ox}$) that at high densities can compromise the position resolution by creating a conduction channel between the electrodes.","This issue is typically addressed by including additional isolation implants ($p$-stop, $p$-spray) between $n^+$-electrodes.","Focusing on the guard-ring regions of $n$-on-$p$ sensors where no isolation implants are applied between the electrodes, a capacitance-voltage ($CV$) characterization study of both 6-inch wafer test diodes and 8-inch HGCAL prototype and pre-series sensors showed a distinct threshold voltage ($V_\\textrm{th,iso}$) in the $CV$-characteristics of a biased $n^+$-electrode when its enclosing guard-ring was left floating.","When reproduced by simulations, the measured $V_\\textrm{th,iso}$ was found to contain information on the field-region $N_\\textrm{ox}$ and indicate the threshold where the two electrodes become electrically isolated by the influence of the reverse bias voltage.","Together with previous studies on the inter-electrode isolation of irradiated $n$-on-$p$ sensors, the results indicate that position sensitive $n$-on-$p$ sensors without isolation implants may be feasible in future HEP experiments."],"url":"http://arxiv.org/abs/2402.04365v1","category":"physics.ins-det"}
{"created":"2024-02-06 19:39:26","title":"PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation","abstract":"We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) across various sub-regions within the data space.","sentences":["We propose a comprehensive sample-based method for assessing the quality of generative models.","The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset.","This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region.","The method only requires samples from the generative model and the test data.","It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction.","Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models.","Instead, it focuses on approximating the integral of the density (probability mass) across various sub-regions within the data space."],"url":"http://arxiv.org/abs/2402.04355v1","category":"stat.ML"}
{"created":"2024-02-06 19:23:29","title":"CausalMetaR: An R package for performing causally interpretable meta-analyses","abstract":"Researchers would often like to leverage data from a collection of sources (e.g., primary studies in a meta-analysis) to estimate causal effects in a target population of interest. However, traditional meta-analytic methods do not produce causally interpretable estimates for a well-defined target population. In this paper, we present the CausalMetaR R package, which implements efficient and robust methods to estimate causal effects in a given internal or external target population using multi-source data. The package includes estimators of average and subgroup treatment effects for the entire target population. To produce efficient and robust estimates of causal effects, the package implements doubly robust and non-parametric efficient estimators and supports using flexible data-adaptive (e.g., machine learning techniques) methods and cross-fitting techniques to estimate the nuisance models (e.g., the treatment model, the outcome model). We describe the key features of the package and demonstrate how to use the package through an example.","sentences":["Researchers would often like to leverage data from a collection of sources (e.g., primary studies in a meta-analysis) to estimate causal effects in a target population of interest.","However, traditional meta-analytic methods do not produce causally interpretable estimates for a well-defined target population.","In this paper, we present the CausalMetaR R package, which implements efficient and robust methods to estimate causal effects in a given internal or external target population using multi-source data.","The package includes estimators of average and subgroup treatment effects for the entire target population.","To produce efficient and robust estimates of causal effects, the package implements doubly robust and non-parametric efficient estimators and supports using flexible data-adaptive (e.g., machine learning techniques) methods and cross-fitting techniques to estimate the nuisance models (e.g., the treatment model, the outcome model).","We describe the key features of the package and demonstrate how to use the package through an example."],"url":"http://arxiv.org/abs/2402.04341v1","category":"stat.ME"}
{"created":"2024-02-06 19:20:58","title":"Logical recognition method for solving the problem of identification in the Internet of Things","abstract":"A new area of application of methods of algebra of logic and to valued logic, which has emerged recently, is the problem of recognizing a variety of objects and phenomena, medical or technical diagnostics, constructing modern machines, checking test problems, etc., which can be reduced to constructing an optimal extension of the logical function to the entire feature space. For example, in logical recognition systems, logical methods based on discrete analysis and propositional calculus based on it are used to build their own recognition algorithms. In the general case, the use of a logical recognition method provides for the presence of logical connections expressed by the optimal continuation of a k-valued function over the entire feature space, in which the variables are the logical features of the objects or phenomena being recognized. The goal of this work is to develop a logical method for object recognition consisting of a reference table with logical features and classes of non-intersecting objects, which are specified as vectors from a given feature space. The method consists of considering the reference table as a logical function that is not defined everywhere and constructing an optimal continuation of the logical function to the entire feature space, which determines the extension of classes to the entire space.","sentences":["A new area of application of methods of algebra of logic and to valued logic, which has emerged recently, is the problem of recognizing a variety of objects and phenomena, medical or technical diagnostics, constructing modern machines, checking test problems, etc., which can be reduced to constructing an optimal extension of the logical function to the entire feature space.","For example, in logical recognition systems, logical methods based on discrete analysis and propositional calculus based on it are used to build their own recognition algorithms.","In the general case, the use of a logical recognition method provides for the presence of logical connections expressed by the optimal continuation of a k-valued function over the entire feature space, in which the variables are the logical features of the objects or phenomena being recognized.","The goal of this work is to develop a logical method for object recognition consisting of a reference table with logical features and classes of non-intersecting objects, which are specified as vectors from a given feature space.","The method consists of considering the reference table as a logical function that is not defined everywhere and constructing an optimal continuation of the logical function to the entire feature space, which determines the extension of classes to the entire space."],"url":"http://arxiv.org/abs/2402.04338v1","category":"cs.AI"}
{"created":"2024-02-06 19:18:56","title":"LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text","abstract":"In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\\% (violation identification) and 81.02\\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).","sentences":["In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals.","We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators.","Both tasks were designed specifically for the context of class-action cases.","The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs.","Our results, with an F1-score of 62.69\\% (violation identification) and 81.02\\% (associating victims), show that our datasets and setups can be used for both tasks.","Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP)."],"url":"http://arxiv.org/abs/2402.04335v1","category":"cs.CL"}
{"created":"2024-02-06 19:18:51","title":"Home Automation System based on Intelligent Transducer Enablers","abstract":"This paper presents a novel home automation system named HASITE (Home Automation System based on Intelligent Transducer Enablers), which has been specifically designed to identify and configure transducers easily and quickly. These features are especially useful in situations where many transducers are deployed, since their setup becomes a cumbersome task that consumes a significant amount of time and human resources. HASITE simplifies the deployment of a home automation system by using wireless networks and both self-configuration and self-registration protocols. Thanks to the application of these three elements, HASITE is able to add new transducers by just powering them up. According to the tests performed in different realistic scenarios, a transducer is ready to be used in less than 13 s. Moreover, all HASITE functionalities can be accessed through an API, which also allows for the integration of third-party systems. As an example, an Android application based on the API is presented. Remote users can use it to interact with transducers by just using a regular smartphone or a tablet.","sentences":["This paper presents a novel home automation system named HASITE (Home Automation System based on Intelligent Transducer Enablers), which has been specifically designed to identify and configure transducers easily and quickly.","These features are especially useful in situations where many transducers are deployed, since their setup becomes a cumbersome task that consumes a significant amount of time and human resources.","HASITE simplifies the deployment of a home automation system by using wireless networks and both self-configuration and self-registration protocols.","Thanks to the application of these three elements, HASITE is able to add new transducers by just powering them up.","According to the tests performed in different realistic scenarios, a transducer is ready to be used in less than 13 s.","Moreover, all HASITE functionalities can be accessed through an API, which also allows for the integration of third-party systems.","As an example, an Android application based on the API is presented.","Remote users can use it to interact with transducers by just using a regular smartphone or a tablet."],"url":"http://arxiv.org/abs/2402.04334v1","category":"eess.SY"}
{"created":"2024-02-06 19:18:04","title":"LESS: Selecting Influential Data for Targeted Instruction Tuning","abstract":"Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application.","sentences":["Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots.","However, real-world applications often require a specialized suite of skills (e.g., reasoning).","The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning.","We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt","Similarity Search for instruction data selection.","Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data.","LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability.","Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks.","Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families.","Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application."],"url":"http://arxiv.org/abs/2402.04333v1","category":"cs.CL"}
{"created":"2024-02-06 19:10:30","title":"On the estimation and interpretation of effect size metrics","abstract":"Effect size estimates are thought to capture the collective, two-way response to an intervention or exposure in a three-way problem among the intervention/exposure, various confounders and the outcome. For meaningful causal inference from the estimated effect size, the joint distribution of observed confounders must be identical across all intervention/exposure groups. However, real-world observational studies and even randomized clinical trials often lack such structural symmetry. To address this issue, various methods have been proposed and widely utilized. Recently, elementary combinatorics and information theory have motivated a consistent way to completely eliminate observed confounding in any given study. In this work, we leverage these new techniques to evaluate conventional methods based on their ability to (a) consistently differentiate between collective and individual responses to intervention/exposure and (b) establish the desired structural parity for sensible effect size estimation. Our findings reveal that a straightforward application of logistic regression homogenizes the three-way stratified analysis, but fails to restore structural symmetry leaving in particular the two-way effect size estimate unadjusted. Conversely, the Mantel-Haenszel estimator struggles to separate three-way effects from the two-way effect of intervention/exposure, leading to inconsistencies in interpreting pooled estimates as two-way risk metrics.","sentences":["Effect size estimates are thought to capture the collective, two-way response to an intervention or exposure in a three-way problem among the intervention/exposure, various confounders and the outcome.","For meaningful causal inference from the estimated effect size, the joint distribution of observed confounders must be identical across all intervention/exposure groups.","However, real-world observational studies and even randomized clinical trials often lack such structural symmetry.","To address this issue, various methods have been proposed and widely utilized.","Recently, elementary combinatorics and information theory have motivated a consistent way to completely eliminate observed confounding in any given study.","In this work, we leverage these new techniques to evaluate conventional methods based on their ability to (a) consistently differentiate between collective and individual responses to intervention/exposure and (b) establish the desired structural parity for sensible effect size estimation.","Our findings reveal that a straightforward application of logistic regression homogenizes the three-way stratified analysis, but fails to restore structural symmetry leaving in particular the two-way effect size estimate unadjusted.","Conversely, the Mantel-Haenszel estimator struggles to separate three-way effects from the two-way effect of intervention/exposure, leading to inconsistencies in interpreting pooled estimates as two-way risk metrics."],"url":"http://arxiv.org/abs/2402.04327v1","category":"stat.ME"}
{"created":"2024-02-06 19:09:44","title":"Personality Trait Recognition using ECG Spectrograms and Deep Learning","abstract":"This paper presents an innovative approach to recognizing personality traits using deep learning (DL) methods applied to electrocardiogram (ECG) signals. Within the framework of detecting the big five personality traits model encompassing extra-version, neuroticism, agreeableness, conscientiousness, and openness, the research explores the potential of ECG-derived spectrograms as informative features. Optimal window sizes for spectrogram generation are determined, and a convolutional neural network (CNN), specifically Resnet-18, and visual transformer (ViT) are employed for feature extraction and personality trait classification. The study utilizes the publicly available ASCERTAIN dataset, which comprises various physiological signals, including ECG recordings, collected from 58 participants during the presentation of video stimuli categorized by valence and arousal levels. The outcomes of this study demonstrate noteworthy performance in personality trait classification, consistently achieving F1-scores exceeding 0.9 across different window sizes and personality traits. These results emphasize the viability of ECG signal spectrograms as a valuable modality for personality trait recognition, with Resnet-18 exhibiting effectiveness in discerning distinct personality traits.","sentences":["This paper presents an innovative approach to recognizing personality traits using deep learning (DL) methods applied to electrocardiogram (ECG) signals.","Within the framework of detecting the big five personality traits model encompassing extra-version, neuroticism, agreeableness, conscientiousness, and openness, the research explores the potential of ECG-derived spectrograms as informative features.","Optimal window sizes for spectrogram generation are determined, and a convolutional neural network (CNN), specifically Resnet-18, and visual transformer (ViT) are employed for feature extraction and personality trait classification.","The study utilizes the publicly available ASCERTAIN dataset, which comprises various physiological signals, including ECG recordings, collected from 58 participants during the presentation of video stimuli categorized by valence and arousal levels.","The outcomes of this study demonstrate noteworthy performance in personality trait classification, consistently achieving F1-scores exceeding 0.9 across different window sizes and personality traits.","These results emphasize the viability of ECG signal spectrograms as a valuable modality for personality trait recognition, with Resnet-18 exhibiting effectiveness in discerning distinct personality traits."],"url":"http://arxiv.org/abs/2402.04326v1","category":"cs.HC"}
{"created":"2024-02-06 19:09:32","title":"Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons","abstract":"Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robustness and efficiency across several attack scenarios, model architectures, and datasets.","sentences":["Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making.","Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models.","In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency.","Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks.","By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons.","Our experimental results demonstrate that our method successfully enhances both robustness and efficiency across several attack scenarios, model architectures, and datasets."],"url":"http://arxiv.org/abs/2402.04325v1","category":"cs.LG"}
{"created":"2024-02-06 19:00:06","title":"Probing magnetism in moir\u00e9 heterostructures with quantum twisting microscopes","abstract":"Spin-ordered states close to metal-insulator transitions are poorly understood theoretically and challenging to probe in experiments. Here, we propose that the quantum twisting microscope, which provides direct access to the energy-momentum resolved spectrum of single-particle and collective excitations, can be used as a novel tool to distinguish between different types of magnetic order. To this end, we calculate the single-particle spectral function and the dynamical spin-structure factor for both a ferromagnetic and antiferromagnetic generalized Wigner crystal formed in fractionally filled moir\\'e superlattices of transition metal dichalcogenide heterostructures. We demonstrate that magnetic order can be clearly identified in these response functions. Furthermore, we explore signatures of quantum phase transitions in the quantum twisting microscope response. We focus on the specific case of triangular moir\\'e lattices at half filling, which have been proposed to host a topological phase transition between a chiral spin liquid and a 120 degree ordered state. Our work demonstrates the potential for quantum twisting microscopes to characterize quantum magnetism in moir\\'e heterostructures.","sentences":["Spin-ordered states close to metal-insulator transitions are poorly understood theoretically and challenging to probe in experiments.","Here, we propose that the quantum twisting microscope, which provides direct access to the energy-momentum resolved spectrum of single-particle and collective excitations, can be used as a novel tool to distinguish between different types of magnetic order.","To this end, we calculate the single-particle spectral function and the dynamical spin-structure factor for both a ferromagnetic and antiferromagnetic generalized Wigner crystal formed in fractionally filled moir\\'e superlattices of transition metal dichalcogenide heterostructures.","We demonstrate that magnetic order can be clearly identified in these response functions.","Furthermore, we explore signatures of quantum phase transitions in the quantum twisting microscope response.","We focus on the specific case of triangular moir\\'e lattices at half filling, which have been proposed to host a topological phase transition between a chiral spin liquid and a 120 degree ordered state.","Our work demonstrates the potential for quantum twisting microscopes to characterize quantum magnetism in moir\\'e heterostructures."],"url":"http://arxiv.org/abs/2402.04311v1","category":"cond-mat.str-el"}
{"created":"2024-02-06 12:42:21","title":"Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI Claims","abstract":"Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.","sentences":["Humans have a tendency to see 'human'-like qualities in objects around them.","We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do.","This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs).","In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs.","In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome.","Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes."],"url":"http://arxiv.org/abs/2402.03962v2","category":"cs.AI"}
{"created":"2024-02-06 11:23:33","title":"Embedding Knowledge Graphs in Degenerate Clifford Algebras","abstract":"Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture embeddings better. Our comparison against the state of the art suggests that our approach generalizes better than other approaches on all datasets w.r.t. the MRR it achieves on validation data. We also show that a greedy search suffices to discover values of $p$, $q$ and $r$ that are close to optimal.","sentences":["Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions.","So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings.","We propose to consider nilpotent base vectors with a nilpotency index of two.","In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings.","We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks.","The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture embeddings better.","Our comparison against the state of the art suggests that our approach generalizes better than other approaches on all datasets w.r.t.","the MRR it achieves on validation data.","We also show that a greedy search suffices to discover values of $p$, $q$ and $r$ that are close to optimal."],"url":"http://arxiv.org/abs/2402.04870v1","category":"cs.AI"}
{"created":"2024-02-06 10:15:38","title":"AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies","abstract":"Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution is uni-modal. Our comprehensive empirical evaluation shows that AdaFlow achieves high performance across all dimensions, including success rate, behavioral diversity, and inference speed. The code is available at https://github.com/hxixixh/AdaFlow","sentences":["Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process.","It urges us to design efficient policy generators while keeping the ability to generate diverse actions.","To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling.","AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows.","We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs.","With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity.","Interestingly, it automatically reduces to a one-step generator when the action distribution is uni-modal.","Our comprehensive empirical evaluation shows that AdaFlow achieves high performance across all dimensions, including success rate, behavioral diversity, and inference speed.","The code is available at https://github.com/hxixixh/AdaFlow"],"url":"http://arxiv.org/abs/2402.04292v1","category":"cs.LG"}
{"created":"2024-02-06 09:26:34","title":"BiLLM: Pushing the Limit of Post-Training Quantization for LLMs","abstract":"Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLMs families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency.","sentences":["Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources.","As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements.","However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths.","In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs.","Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy.","Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately.","BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLMs families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins.","Moreover, BiLLM enables the binarization process of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency."],"url":"http://arxiv.org/abs/2402.04291v1","category":"cs.LG"}
{"created":"2024-02-06 08:30:47","title":"CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling","abstract":"Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competitive performance. Especially, CasCast significantly surpasses the baseline (up to +91.8%) for regional extreme-precipitation nowcasting.","sentences":["Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management.","Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation.","In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns.","Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs.","Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competitive performance.","Especially, CasCast significantly surpasses the baseline (up to +91.8%) for regional extreme-precipitation nowcasting."],"url":"http://arxiv.org/abs/2402.04290v1","category":"cs.LG"}
{"created":"2024-02-07 18:59:54","title":"Vortex spin liquid with neutral Fermi surface and fractional quantum spin Hall effect at odd integer filling of moir\u00e9 Chern band","abstract":"There have been many progress in realizing integer and fractional quantum anomalous Hall (QAH) phases in moir\\'e systems with nearly flat Chern band. More recently there is even observation of a time reversal invariant fractional phase at filling $n=3$ in twisted MoTe$_2$ bilayer at small twist angle, indicating completely new physics beyond the conventional quantum Hall systems. While an obvious interpretation is from decoupled FQAH phases in the two valleys, the absence of similar phases at fillings such as $n=\\frac{2}{3}+\\frac{2}{3}$ is at odds with it and suggests physics unique to the odd integer filling $n=\\frac{1}{2}+\\frac{1}{2}$. In this work we start from a pair of composite Fermi liquids (CFL) with opposite chiralities in the two valleys and consider the possible proximate phase after adding the inter-valley correlation. We propose a new fractional insulator through the exciton pairing of the composite fermions. This insulating phase hosts neutral Fermi surfaces and fractional quantum spin Hall (FQSH) effect. The neutral fermion is also spinless, while there is gapless spin (locked to valley) excitation from the internal flux of an U(1) gauge field. We dub the phase as a vortex spin liquid (VSL) because it can be reached from proliferating fermionic vortex of a nearby inter-valley-coherent (IVC) insulator. The phase in the bulk is essentially a quantum spin liquid as in a Mott insulator. Because the charge gap suppresses the double occupancy from the two valleys in the same region, the VSL phase may likely be energetically better than the decoupled FQSH phase. As a spin liquid, this special FQSH candidate exists only at odd integer filling, consistent with the experiment. We also discuss the potential routes from the VSL phase to competing symmetry breaking phases. \\end{abstract}","sentences":["There have been many progress in realizing integer and fractional quantum anomalous Hall (QAH) phases in moir\\'e systems with nearly flat Chern band.","More recently there is even observation of a time reversal invariant fractional phase at filling $n=3$ in twisted MoTe$_2$ bilayer at small twist angle, indicating completely new physics beyond the conventional quantum Hall systems.","While an obvious interpretation is from decoupled FQAH phases in the two valleys, the absence of similar phases at fillings such as $n=\\frac{2}{3}+\\frac{2}{3}$ is at odds with it and suggests physics unique to the odd integer filling $n=\\frac{1}{2}+\\frac{1}{2}$. In this work we start from a pair of composite Fermi liquids (CFL) with opposite chiralities in the two valleys and consider the possible proximate phase after adding the inter-valley correlation.","We propose a new fractional insulator through the exciton pairing of the composite fermions.","This insulating phase hosts neutral Fermi surfaces and fractional quantum spin Hall (FQSH) effect.","The neutral fermion is also spinless, while there is gapless spin (locked to valley) excitation from the internal flux of an U(1) gauge field.","We dub the phase as a vortex spin liquid (VSL) because it can be reached from proliferating fermionic vortex of a nearby inter-valley-coherent (IVC) insulator.","The phase in the bulk is essentially a quantum spin liquid as in a Mott insulator.","Because the charge gap suppresses the double occupancy from the two valleys in the same region, the VSL phase may likely be energetically better than the decoupled FQSH phase.","As a spin liquid, this special FQSH candidate exists only at odd integer filling, consistent with the experiment.","We also discuss the potential routes from the VSL phase to competing symmetry breaking phases.","\\end{abstract}"],"url":"http://arxiv.org/abs/2402.05112v1","category":"cond-mat.str-el"}
{"created":"2024-02-07 18:58:38","title":"A topological algorithm for the Fourier transform of Stokes data at infinity","abstract":"We reinterpret a result of T. Mochizuki about the Fourier transform of Stokes data of irregular connections on the Riemann sphere in the language of Stokes local systems due to P. Boalch. We thus obtain a clean topological description of the Stokes matrices of the Fourier transform from infinity to infinity in a large number of cases of one level. In particular, this induces explicit isomorphisms between wild character varieties, in a much larger range of examples than those for which such isomorphisms have previously been written down. We conjecture that these isomorphisms are compatible with the quasi-Hamiltonian structure of the wild character varieties.","sentences":["We reinterpret a result of T. Mochizuki about the Fourier transform of Stokes data of irregular connections on the Riemann sphere in the language of Stokes local systems due to P. Boalch.","We thus obtain a clean topological description of the Stokes matrices of the Fourier transform from infinity to infinity in a large number of cases of one level.","In particular, this induces explicit isomorphisms between wild character varieties, in a much larger range of examples than those for which such isomorphisms have previously been written down.","We conjecture that these isomorphisms are compatible with the quasi-Hamiltonian structure of the wild character varieties."],"url":"http://arxiv.org/abs/2402.05108v1","category":"math.AG"}
{"created":"2024-02-07 18:46:47","title":"Interacting particle approximation of cross-diffusion systems","abstract":"We derive a class of multi-species cross-diffusion systems from stochastic interacting particles. We prove existence of weak solutions of the limiting cross-diffusion system as well as the propagation of chaos by means of nonlocal approximation of the nonlinear diffusion terms, coupling methods and compactness arguments. We show that these equations capture the macroscopic behavior of the interacting particle system if the localisation parameter is chosen logarithmically with respect to the number of particles.","sentences":["We derive a class of multi-species cross-diffusion systems from stochastic interacting particles.","We prove existence of weak solutions of the limiting cross-diffusion system as well as the propagation of chaos by means of nonlocal approximation of the nonlinear diffusion terms, coupling methods and compactness arguments.","We show that these equations capture the macroscopic behavior of the interacting particle system if the localisation parameter is chosen logarithmically with respect to the number of particles."],"url":"http://arxiv.org/abs/2402.05094v1","category":"math.AP"}
{"created":"2024-02-07 18:46:46","title":"Moduli Parameters of Complex Singularities with Non-Degenerate Newton Boundary","abstract":"Our recent extension of Arnold's classification includes all singularities of corank up to two equivalent to a germ with a non-degenerate Newton boundary, thus broadening the classification's scope significantly by a class which is unbounded with respect to modality and Milnor number. This method is based on proving that all right-equivalence classes within a mu-constant stratum can be represented by a single normal form derived from a regular basis of a suitably selected special fiber. While both Arnold's and our preceding work on normal forms addresses the determination of a normal form family containing the given germ, this paper takes the next natural step: We present an algorithm for computing for a given germ the values of the moduli parameters in its normal form family, that is, a normal form equation in its stable equivalence class. This algorithm will be crucial for understanding the moduli stacks of such singularities. The implementation of this algorithm, along with the foundational classification techniques, is implemented in the library arnold.lib for the computer algebra system Singular.","sentences":["Our recent extension of Arnold's classification includes all singularities of corank up to two equivalent to a germ with a non-degenerate Newton boundary, thus broadening the classification's scope significantly by a class which is unbounded with respect to modality and Milnor number.","This method is based on proving that all right-equivalence classes within a mu-constant stratum can be represented by a single normal form derived from a regular basis of a suitably selected special fiber.","While both Arnold's and our preceding work on normal forms addresses the determination of a normal form family containing the given germ, this paper takes the next natural step: We present an algorithm for computing for a given germ the values of the moduli parameters in its normal form family, that is, a normal form equation in its stable equivalence class.","This algorithm will be crucial for understanding the moduli stacks of such singularities.","The implementation of this algorithm, along with the foundational classification techniques, is implemented in the library arnold.lib for the computer algebra system Singular."],"url":"http://arxiv.org/abs/2402.05093v1","category":"math.AG"}
{"created":"2024-02-07 18:46:34","title":"Extreme value statistics of non-Markovian processes from a new class of integrable nonlinear differential equation","abstract":"This research presents a universal mapping between two key concepts in the study of stochastic processes: nonequilibrium steady-states under confinement and extreme value statistics, focusing on one-dimensional systems. The mapping holds irrespectively of the statistics of the noise driving the dynamics. This result is based on a novel approach that provides an exact trajectory-wise solution for first-order dynamics near a hard-wall boundary. I first demonstrate the applicability of this method by efficiently reproducing known results from Brownian motion theory, such as the distribution of the running maximum, and by deriving new ones, such as the survival probability below a specific curve with conditioning on the endpoint value. I then extend the analysis to non-Markovian processes, focusing on run-and-tumble particles. The mapping to a steady-state allows to compute several quantities of interest, such as the long-time probability that run-and-tumble and Brownian particles do not cross each other.","sentences":["This research presents a universal mapping between two key concepts in the study of stochastic processes: nonequilibrium steady-states under confinement and extreme value statistics, focusing on one-dimensional systems.","The mapping holds irrespectively of the statistics of the noise driving the dynamics.","This result is based on a novel approach that provides an exact trajectory-wise solution for first-order dynamics near a hard-wall boundary.","I first demonstrate the applicability of this method by efficiently reproducing known results from Brownian motion theory, such as the distribution of the running maximum, and by deriving new ones, such as the survival probability below a specific curve with conditioning on the endpoint value.","I then extend the analysis to non-Markovian processes, focusing on run-and-tumble particles.","The mapping to a steady-state allows to compute several quantities of interest, such as the long-time probability that run-and-tumble and Brownian particles do not cross each other."],"url":"http://arxiv.org/abs/2402.05091v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 18:37:17","title":"Non-Markovian Quantum Control via Model Maximum Likelihood Estimation and Reinforcement Learning","abstract":"Reinforcement Learning (RL) techniques have been increasingly applied in optimizing control systems. However, their application in quantum systems is hampered by the challenge of performing closed-loop control due to the difficulty in measuring these systems. This often leads to reliance on assumed models, introducing model bias, a problem that is exacerbated in open quantum dynamics where Markovian approximations are not valid. To address these challenges, we propose a novel approach that incorporates the non-Markovian nature of the environment into a low-dimensional effective reservoir. By initially employing a series of measurements as a 'dataset', we utilize machine learning techniques to learn the effective quantum dynamics more efficiently than traditional tomographic methods. Our methodology aims to demonstrates that by integrating reinforcement learning with model learning, it is possible to devise control policies and models that can counteract decoherence in a spin-boson system. This approach may not only mitigates the issues of model bias but also provides a more accurate representation of quantum dynamics, paving the way for more effective quantum control strategies.","sentences":["Reinforcement Learning (RL) techniques have been increasingly applied in optimizing control systems.","However, their application in quantum systems is hampered by the challenge of performing closed-loop control due to the difficulty in measuring these systems.","This often leads to reliance on assumed models, introducing model bias, a problem that is exacerbated in open quantum dynamics where Markovian approximations are not valid.","To address these challenges, we propose a novel approach that incorporates the non-Markovian nature of the environment into a low-dimensional effective reservoir.","By initially employing a series of measurements as a 'dataset', we utilize machine learning techniques to learn the effective quantum dynamics more efficiently than traditional tomographic methods.","Our methodology aims to demonstrates that by integrating reinforcement learning with model learning, it is possible to devise control policies and models that can counteract decoherence in a spin-boson system.","This approach may not only mitigates the issues of model bias but also provides a more accurate representation of quantum dynamics, paving the way for more effective quantum control strategies."],"url":"http://arxiv.org/abs/2402.05084v1","category":"quant-ph"}
{"created":"2024-02-07 18:34:34","title":"Information and Configurational Entropy in Glassy Systems","abstract":"It is often stated that if one is presented with a snapshot of the positions of the molecules of a glass and one of a liquid, one is unable to tell the difference. Here we argue instead that given several such snapshots taken over a time-interval, even without specifying the times, there is a definite procedure to assess precisely the level of glassiness: it suffices to concatenate the snapshots side-by-side, and to subject the joint picture to a lossless compression protocol. We argue that the size of the compressed file yields a direct and unambiguous measure of the `vibrational' and `configurational' entropies, and may be used to study the associated glass length scale in or out of equilibrium through the size and frequency of the repeated motifs essential to the compression, a quantity that would diverge at a putative glass transition.","sentences":["It is often stated that if one is presented with a snapshot of the positions of the molecules of a glass and one of a liquid, one is unable to tell the difference.","Here we argue instead that given several such snapshots taken over a time-interval, even without specifying the times, there is a definite procedure to assess precisely the level of glassiness: it suffices to concatenate the snapshots side-by-side, and to subject the joint picture to a lossless compression protocol.","We argue that the size of the compressed file yields a direct and unambiguous measure of the `vibrational' and `configurational' entropies, and may be used to study the associated glass length scale in or out of equilibrium through the size and frequency of the repeated motifs essential to the compression, a quantity that would diverge at a putative glass transition."],"url":"http://arxiv.org/abs/2402.05081v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-07 18:23:09","title":"Fluctuating hydrodynamics of active particles interacting via chemotaxis and quorum sensing: static and dynamics","abstract":"In this article we derive and test the fluctuating hydrodynamic description of active particles interacting via taxis and quorum sensing, both for mono-disperse systems and for mixtures of co-existing species of active particles. We compute the average steady-state density profile in the presence of spatial motility regulation, as well as the structure factor and intermediate scattering function for interacting systems. By comparing our predictions to microscopic numerical simulations, we show that our fluctuating hydrodynamics correctly predicts the large-scale static and dynamical properties of the system. We also discuss how the theory breaks down when structures emerge at scales smaller or comparable to the persistence length of the particles. When the density field is the unique hydrodynamic mode of the system, we show that active Brownian particles, run-and-tumble particles and active Ornstein-Uhlenbeck particles, interacting via quorum-sensing or chemotactic interactions, display undistinguishable large-scale properties. This form of universality implies an interesting robustness of the predicted physics but also that large-scale observations of patterns are insufficient to assess their microscopic origins.","sentences":["In this article we derive and test the fluctuating hydrodynamic description of active particles interacting via taxis and quorum sensing, both for mono-disperse systems and for mixtures of co-existing species of active particles.","We compute the average steady-state density profile in the presence of spatial motility regulation, as well as the structure factor and intermediate scattering function for interacting systems.","By comparing our predictions to microscopic numerical simulations, we show that our fluctuating hydrodynamics correctly predicts the large-scale static and dynamical properties of the system.","We also discuss how the theory breaks down when structures emerge at scales smaller or comparable to the persistence length of the particles.","When the density field is the unique hydrodynamic mode of the system, we show that active Brownian particles, run-and-tumble particles and active Ornstein-Uhlenbeck particles, interacting via quorum-sensing or chemotactic interactions, display undistinguishable large-scale properties.","This form of universality implies an interesting robustness of the predicted physics but also that large-scale observations of patterns are insufficient to assess their microscopic origins."],"url":"http://arxiv.org/abs/2402.05072v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 18:22:41","title":"Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity","abstract":"We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\\rho$-cohypomonotonicity or admitting a solution to the $\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\\rho>0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate value of $\\rho$ no larger than $\\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\\rho < \\frac{1}{L}$. The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\\u{\\i}-Mann (KM) iterations. We also provide algorithms and complexity guarantees in the stochastic case with the same range on $\\rho$. Our main insight for the improvements in the convergence analyses is to harness the recently proposed \"conic nonexpansiveness\" property of operators. As byproducts, we provide a refined analysis for inexact Halpern iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator.","sentences":["We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\\rho$-cohypomonotonicity or admitting a solution to the $\\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\\rho>0$ correspond to a greater degree of nonconvexity.","These problem classes include examples in two player reinforcement learning, interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail.","It has been conjectured that first-order methods can tolerate value of $\\rho$ no larger than $\\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\\rho < \\frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\\rho < \\frac{1}{L}$.","The algorithms we analyze are inexact variants of Halpern and Krasnosel'ski\\u{\\i}-Mann (KM) iterations.","We also provide algorithms and complexity guarantees in the stochastic case with the same range on $\\rho$. Our main insight for the improvements in the convergence analyses is to harness the recently proposed \"conic nonexpansiveness\" property of operators.","As byproducts, we provide a refined analysis for inexact Halpern iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator."],"url":"http://arxiv.org/abs/2402.05071v1","category":"math.OC"}
{"created":"2024-02-07 18:19:51","title":"Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems","abstract":"Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method. By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. The discussions about these scenarios contribute to a comprehensive understanding of the method's capabilities and limitations. This novel decoupling approach simplifies the analysis and prediction of spatiotemporal systems, where large-scale data can be obtained with low computational demands, followed by Spectral PINNs for capturing small-scale dynamics with improved efficiency and accuracy.","sentences":["Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems.","In this paper, a novel solving mode is proposed for characterizing multiscale dynamics through a decoupling method.","By modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system, a Spectral PINN is developed to approach the small-scale system in an orthogonal basis functional space.","The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional Navier-Stokes (NS) equations, showcasing its versatility in addressing problems of fluid dynamics.","Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics.","The discussions about these scenarios contribute to a comprehensive understanding of the method's capabilities and limitations.","This novel decoupling approach simplifies the analysis and prediction of spatiotemporal systems, where large-scale data can be obtained with low computational demands, followed by Spectral PINNs for capturing small-scale dynamics with improved efficiency and accuracy."],"url":"http://arxiv.org/abs/2402.05067v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 18:15:28","title":"Tuning the feedback controller gains is a simple way to improve autonomous driving performance","abstract":"Typical autonomous driving systems are a combination of machine learning algorithms (often involving neural networks) and classical feedback controllers. Whilst significant progress has been made in recent years on the neural network side of these systems, only limited progress has been made on the feedback controller side. Often, the feedback control gains are simply passed from paper to paper with little re-tuning taking place, even though the changes to the neural networks can alter the vehicle's closed loop dynamics. The aim of this paper is to highlight the limitations of this approach; it is shown that re-tuning the feedback controller can be a simple way to improve autonomous driving performance. To demonstrate this, the PID gains of the longitudinal controller in the TCP autonomous vehicle algorithm are tuned. This causes the driving score in CARLA to increase from 73.21 to 77.38, with the results averaged over 16 driving scenarios. Moreover, it was observed that the performance benefits were most apparent during challenging driving scenarios, such as during rain or night time, as the tuned controller led to a more assertive driving style. These results demonstrate the value of developing both the neural network and feedback control policies of autonomous driving systems simultaneously, as this can be a simple and methodical way to improve autonomous driving system performance and robustness.","sentences":["Typical autonomous driving systems are a combination of machine learning algorithms (often involving neural networks) and classical feedback controllers.","Whilst significant progress has been made in recent years on the neural network side of these systems, only limited progress has been made on the feedback controller side.","Often, the feedback control gains are simply passed from paper to paper with little re-tuning taking place, even though the changes to the neural networks can alter the vehicle's closed loop dynamics.","The aim of this paper is to highlight the limitations of this approach; it is shown that re-tuning the feedback controller can be a simple way to improve autonomous driving performance.","To demonstrate this, the PID gains of the longitudinal controller in the TCP autonomous vehicle algorithm are tuned.","This causes the driving score in CARLA to increase from 73.21 to 77.38, with the results averaged over 16 driving scenarios.","Moreover, it was observed that the performance benefits were most apparent during challenging driving scenarios, such as during rain or night time, as the tuned controller led to a more assertive driving style.","These results demonstrate the value of developing both the neural network and feedback control policies of autonomous driving systems simultaneously, as this can be a simple and methodical way to improve autonomous driving system performance and robustness."],"url":"http://arxiv.org/abs/2402.05064v1","category":"eess.SY"}
{"created":"2024-02-07 18:14:55","title":"Critical behavior of a phase transition in the dynamics of interacting populations","abstract":"Many-variable differential equations with random coefficients provide powerful models for the dynamics of many interacting species in ecology. These models are known to exhibit a dynamical phase transition from a phase where population sizes reach a fixed point, to a phase where they fluctuate indefinitely. This transition has parallels with models developed in other fields, but also distinct features that stem from the requirement that the variables represent non-negative population sizes.   Abstract Here we provide a theory for the critical behavior close to the phase transition. We show that there are three different universality classes, depending on the distance from the critical point, and the migration rate which couples the system to its surroundings. We derive scaling relations for two parameters, the size of the temporal fluctuations, and the correlation timescale. We show that the temporal fluctuations grow continuously upon crossing the transition, and that timescales diverge near the transition (a critical slowing down). We define and calculate the corresponding critical exponents.","sentences":["Many-variable differential equations with random coefficients provide powerful models for the dynamics of many interacting species in ecology.","These models are known to exhibit a dynamical phase transition from a phase where population sizes reach a fixed point, to a phase where they fluctuate indefinitely.","This transition has parallels with models developed in other fields, but also distinct features that stem from the requirement that the variables represent non-negative population sizes.   ","Abstract Here we provide a theory for the critical behavior close to the phase transition.","We show that there are three different universality classes, depending on the distance from the critical point, and the migration rate which couples the system to its surroundings.","We derive scaling relations for two parameters, the size of the temporal fluctuations, and the correlation timescale.","We show that the temporal fluctuations grow continuously upon crossing the transition, and that timescales diverge near the transition (a critical slowing down).","We define and calculate the corresponding critical exponents."],"url":"http://arxiv.org/abs/2402.05063v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 18:13:04","title":"$H_{\\infty}$-Optimal Estimator Synthesis for Coupled Linear 2D PDEs using Convex Optimization","abstract":"It has recently been shown that, any suitably well-posed PDE in one or two spatial dimensions can be equivalently represented as a Partial Integral Equation (PIE), expressing the dynamics in terms of Partial Integral (PI) operators. Parameterizing storage functionals by PI operators as well, $L_2$-gain analysis of the PDE can then be posed as a linear operator inequality on PI operators, which can be solved using convex optimization. In this paper, we build on this result to derive a convex-optimization-based test for constructing an $H_{\\infty}$-optimal estimator for 2D PDEs, extending a similar result for 1D PDEs. In particular, we first show how a well-posed 2D PDE with infinite-dimensional outputs can be equivalently represented as a PIE, and we parameterize an associated Luenberger-type estimator by a 2D PI operator $\\mathcal{L}$. Parameterizing a storage functional for the resulting error dynamics by another PI operator $\\mathcal{P}$, we prove that the $H_{\\infty}$-norm of the error dynamics can be minimized by solving a linear operator inequality on PI operator variables $\\mathcal{P}$ and $\\mathcal{W}:=\\mathcal{P}\\mathcal{L}$. Finally, we derive an explicit expression for the inverse of the operator $\\mathcal{P}$, and propose a parameterization of variables $\\mathcal{P}$ and $\\mathcal{W}$ by matrices, posing the problem of finding an $H_{\\infty}$-optimal estimator gain $\\mathcal{L}=\\mathcal{P}^{-1}\\mathcal{W}$ as a convex optimization problem. We implement this test in the PIETOOLS software suite, and apply this software to construct an estimator for a 2D heat equation with state observations along the boundary.","sentences":["It has recently been shown that, any suitably well-posed PDE in one or two spatial dimensions can be equivalently represented as a Partial Integral Equation (PIE), expressing the dynamics in terms of Partial Integral (PI) operators.","Parameterizing storage functionals by PI operators as well, $L_2$-gain analysis of the PDE can then be posed as a linear operator inequality on PI operators, which can be solved using convex optimization.","In this paper, we build on this result to derive a convex-optimization-based test for constructing an $H_{\\infty}$-optimal estimator for 2D PDEs, extending a similar result for 1D PDEs.","In particular, we first show how a well-posed 2D PDE with infinite-dimensional outputs can be equivalently represented as a PIE, and we parameterize an associated Luenberger-type estimator by a 2D PI operator $\\mathcal{L}$. Parameterizing a storage functional for the resulting error dynamics by another PI operator $\\mathcal{P}$, we prove that the $H_{\\infty}$-norm of the error dynamics can be minimized by solving a linear operator inequality on PI operator variables $\\mathcal{P}$ and $\\mathcal{W}:=\\mathcal{P}\\mathcal{L}$. Finally, we derive an explicit expression for the inverse of the operator $\\mathcal{P}$, and propose a parameterization of variables $\\mathcal{P}$ and $\\mathcal{W}$ by matrices, posing the problem of finding an $H_{\\infty}$-optimal estimator gain $\\mathcal{L}=\\mathcal{P}^{-1}\\mathcal{W}$ as a convex optimization problem.","We implement this test in the PIETOOLS software suite, and apply this software to construct an estimator for a 2D heat equation with state observations along the boundary."],"url":"http://arxiv.org/abs/2402.05061v1","category":"math.OC"}
{"created":"2024-02-07 18:10:26","title":"A modified J model for efficiently calculating the electromagnetic fields of ReBCO no-insulation pancake coils using an explicit-implicit hybrid algorithm","abstract":"Rare-earth (Re)Ba2Cu3O7-x (ReBCO) no-insulation (NI) coil is widely concerned due to its excellent electromagnetic and thermal properties. However, the presence of the turn-to-turn shunts in NI coils leads to that complexity of numerical simulation is increased. In this paper, a modified J model is proposed and the corresponding explicit-implicit hybrid algorithm is designed to calculate NI coils. The numerical results are in good agreement with the experimental data and the circuit model. The homogenization model is also proposed to simulate the large-scale NI coils in the background magnets. The modified J model has good accuracy and fast calculation speed, which can also be used to solve electromagnetic fields of insulation coils efficiently.","sentences":["Rare-earth (Re)Ba2Cu3O7-x (ReBCO) no-insulation (NI) coil is widely concerned due to its excellent electromagnetic and thermal properties.","However, the presence of the turn-to-turn shunts in NI coils leads to that complexity of numerical simulation is increased.","In this paper, a modified J model is proposed and the corresponding explicit-implicit hybrid algorithm is designed to calculate NI coils.","The numerical results are in good agreement with the experimental data and the circuit model.","The homogenization model is also proposed to simulate the large-scale NI coils in the background magnets.","The modified J model has good accuracy and fast calculation speed, which can also be used to solve electromagnetic fields of insulation coils efficiently."],"url":"http://arxiv.org/abs/2402.05058v1","category":"physics.app-ph"}
{"created":"2024-02-07 18:04:46","title":"Approximate Integrity Constraints in Incomplete Databases With Limited Domains","abstract":"In case of incomplete database tables, a possible world is obtained by replacing any missing value by a value from the corresponding attribute's domain that can be infinite. A possible key or possible functional dependency constraint is satisfied by an incomplete table if we can obtain a possible world that satisfies the given key or functional dependency. On the other hand, a certain key or certain functional dependency holds if all possible worlds satisfy the constraint, A strongly possible constraint is an intermediate concept between possible and certain constraints, based on the strongly possible world approach (a strongly possible world is obtained by replacing \\nul's by a value from the ones appearing in the corresponding attribute of the table). A strongly possible key or functional dependency holds in an incomplete table if there exists a strongly possible world that satisfies the given constraint. In the present paper, we introduce strongly possible versions of multivalued dependencies and cross joins, and we analyse the complexity of checking the validity of a given strongly possible cross joins. We also study approximation measures of strongly possible keys (spKeys), functional dependencies (spFDs), multivalued dependencies (spMVDs) and cross joins (spCJs). We also treat complexity questions of determination of the approximation values.","sentences":["In case of incomplete database tables, a possible world is obtained by replacing any missing value by a value from the corresponding attribute's domain that can be infinite.","A possible key or possible functional dependency constraint is satisfied by an incomplete table if we can obtain a possible world that satisfies the given key or functional dependency.","On the other hand, a certain key or certain functional dependency holds if all possible worlds satisfy the constraint, A strongly possible constraint is an intermediate concept between possible and certain constraints, based on the strongly possible world approach (a strongly possible world is obtained by replacing \\nul's by a value from the ones appearing in the corresponding attribute of the table).","A strongly possible key or functional dependency holds in an incomplete table if there exists a strongly possible world that satisfies the given constraint.","In the present paper, we introduce strongly possible versions of multivalued dependencies and cross joins, and we analyse the complexity of checking the validity of a given strongly possible cross joins.","We also study approximation measures of strongly possible keys (spKeys), functional dependencies (spFDs), multivalued dependencies (spMVDs) and cross joins (spCJs).","We also treat complexity questions of determination of the approximation values."],"url":"http://arxiv.org/abs/2402.05057v1","category":"cs.DB"}
{"created":"2024-02-07 17:50:23","title":"Generalised Gabriel-Roiter measure and thin representations","abstract":"For Dynkin and Euclidean quivers, it is shown that Gabriel-Roiter measures of thin representations equal the induced chain length functions on the corresponding system of subquivers. This allows a combinatorial procedure to find a GR filtration of thin representations, showing that GR measures of thin representations are field-independent. It is proved that an indecomposable filtration of a thin representation is a GR filtration for a suitable choice of a length function on the category of finite-dimensional representations.","sentences":["For Dynkin and Euclidean quivers, it is shown that Gabriel-Roiter measures of thin representations equal the induced chain length functions on the corresponding system of subquivers.","This allows a combinatorial procedure to find a GR filtration of thin representations, showing that GR measures of thin representations are field-independent.","It is proved that an indecomposable filtration of a thin representation is a GR filtration for a suitable choice of a length function on the category of finite-dimensional representations."],"url":"http://arxiv.org/abs/2402.05051v1","category":"math.RT"}
{"created":"2024-02-07 17:40:22","title":"On the log-hyperconvexity index of pseudoconvex H$\\ddot{o}$lder domains in $\\mathbb{C}^n$","abstract":"In this note we prove that every bounded pseudoconvex domain in $\\mathbb{C}^n$ with H$\\ddot{o}$lder boundary has positive log-hyperconvexity index which gives a positive answer posed by Boyong Chen.","sentences":["In this note we prove that every bounded pseudoconvex domain in $\\mathbb{C}^n$ with H$\\ddot{o}$lder boundary has positive log-hyperconvexity index which gives a positive answer posed by Boyong Chen."],"url":"http://arxiv.org/abs/2402.05047v1","category":"math.CV"}
{"created":"2024-02-07 17:38:00","title":"Monitoring the energy of a cavity by observing the emission of a repeatedly excited qubit","abstract":"The number of excitations in a large quantum system (harmonic oscillator or qudit) can be measured in a quantum non demolition manner using a dispersively coupled qubit. It typically requires a series of qubit pulses that encode various binary questions about the photon number. Recently, a method based on the fluorescence measurement of a qubit driven by a train of identical pulses was introduced to track the photon number in a cavity, hence simplifying its monitoring and raising interesting questions about the measurement backaction of this scheme. A first realization with superconducting circuits demonstrated how the average number of photons could be measured in this way. Here we present an experiment that reaches single shot photocounting and number tracking owing to a cavity decay rate 4 orders of magnitude smaller than both the dispersive coupling rate and the qubit emission rate. An innovative notch filter and pogo-pin based galvanic contact makes possible these seemingly incompatible features. The qubit dynamics under the pulse train is characterized. We observe quantum jumps by monitoring the photon number via the qubit fluorescence as photons leave the cavity one at a time. Besides, we extract the measurement rate and induced dephasing rate and compare them to theoretical models. Our method could be applied to quantum error correction protocols on bosonic codes or qudits.","sentences":["The number of excitations in a large quantum system (harmonic oscillator or qudit) can be measured in a quantum non demolition manner using a dispersively coupled qubit.","It typically requires a series of qubit pulses that encode various binary questions about the photon number.","Recently, a method based on the fluorescence measurement of a qubit driven by a train of identical pulses was introduced to track the photon number in a cavity, hence simplifying its monitoring and raising interesting questions about the measurement backaction of this scheme.","A first realization with superconducting circuits demonstrated how the average number of photons could be measured in this way.","Here we present an experiment that reaches single shot photocounting and number tracking owing to a cavity decay rate 4 orders of magnitude smaller than both the dispersive coupling rate and the qubit emission rate.","An innovative notch filter and pogo-pin based galvanic contact makes possible these seemingly incompatible features.","The qubit dynamics under the pulse train is characterized.","We observe quantum jumps by monitoring the photon number via the qubit fluorescence as photons leave the cavity one at a time.","Besides, we extract the measurement rate and induced dephasing rate and compare them to theoretical models.","Our method could be applied to quantum error correction protocols on bosonic codes or qudits."],"url":"http://arxiv.org/abs/2402.05046v1","category":"quant-ph"}
{"created":"2024-02-07 17:28:09","title":"Sticky Fingers: Resilience of Satellite Fingerprinting against Jamming Attacks","abstract":"In the wake of increasing numbers of attacks on radio communication systems, a range of techniques are being deployed to increase the security of these systems. One such technique is radio fingerprinting, in which the transmitter can be identified and authenticated by observing small hardware differences expressed in the signal. Fingerprinting has been explored in particular in the defense of satellite systems, many of which are insecure and cannot be retrofitted with cryptographic security.   In this paper, we evaluate the effectiveness of radio fingerprinting techniques under interference and jamming attacks, usually intended to deny service. By taking a pre-trained fingerprinting model and gathering a new dataset in which different levels of Gaussian noise and tone jamming have been added to the legitimate signal, we assess the attacker power required in order to disrupt the transmitter fingerprint such that it can no longer be recognized. We compare this to Gaussian jamming on the data portion of the signal, obtaining the remarkable result that transmitter fingerprints are still recognizable even in the presence of moderate levels of noise. Through deeper analysis of the results, we conclude that it takes a similar amount of jamming power in order to disrupt the fingerprint as it does to jam the message contents itself, so it is safe to include a fingerprinting system to authenticate satellite communication without opening up the system to easier denial-of-service attacks.","sentences":["In the wake of increasing numbers of attacks on radio communication systems, a range of techniques are being deployed to increase the security of these systems.","One such technique is radio fingerprinting, in which the transmitter can be identified and authenticated by observing small hardware differences expressed in the signal.","Fingerprinting has been explored in particular in the defense of satellite systems, many of which are insecure and cannot be retrofitted with cryptographic security.   ","In this paper, we evaluate the effectiveness of radio fingerprinting techniques under interference and jamming attacks, usually intended to deny service.","By taking a pre-trained fingerprinting model and gathering a new dataset in which different levels of Gaussian noise and tone jamming have been added to the legitimate signal, we assess the attacker power required in order to disrupt the transmitter fingerprint such that it can no longer be recognized.","We compare this to Gaussian jamming on the data portion of the signal, obtaining the remarkable result that transmitter fingerprints are still recognizable even in the presence of moderate levels of noise.","Through deeper analysis of the results, we conclude that it takes a similar amount of jamming power in order to disrupt the fingerprint as it does to jam the message contents itself, so it is safe to include a fingerprinting system to authenticate satellite communication without opening up the system to easier denial-of-service attacks."],"url":"http://arxiv.org/abs/2402.05042v1","category":"cs.CR"}
{"created":"2024-02-07 17:17:25","title":"Smooth real-time motion planning based on a cascade dual-quaternion screw-geometry MPC","abstract":"This paper investigates the tracking problem of a smooth coordinate-invariant trajectory using dual quaternion algebra. The proposed architecture consists of a cascade structure in which the outer-loop MPC performs real-time smoothing of the manipulator's end-effector twist while an inner-loop kinematic controller ensures tracking of the instantaneous desired end-effector pose. Experiments on a $7$-DoF Franka Emika Panda robotic manipulator validate the proposed method demonstrating its application to constraint the robot twists, accelerations and jerks within prescribed bounds.","sentences":["This paper investigates the tracking problem of a smooth coordinate-invariant trajectory using dual quaternion algebra.","The proposed architecture consists of a cascade structure in which the outer-loop MPC performs real-time smoothing of the manipulator's end-effector twist while an inner-loop kinematic controller ensures tracking of the instantaneous desired end-effector pose.","Experiments on a $7$-DoF Franka Emika Panda robotic manipulator validate the proposed method demonstrating its application to constraint the robot twists, accelerations and jerks within prescribed bounds."],"url":"http://arxiv.org/abs/2402.05037v1","category":"cs.RO"}
{"created":"2024-02-07 17:03:20","title":"Optimal input reverberation and homeostatic self-organization towards the edge of synchronization","abstract":"Transient or partial synchronization can be used to do computations, although a fully synchronized network is frequently related to epileptic seizures. Here, we propose a homeostatic mechanism that is capable of maintaining a neuronal network at the edge of a synchronization transition, thereby avoiding the harmful consequences of a fully synchronized network. We model neurons by maps since they are dynamically richer than integrate-and-fire models and more computationally efficient than conductance-based approaches. We first describe the synchronization phase transition of a dense network of neurons with different tonic spiking frequencies coupled by gap junctions. We show that at the transition critical point, inputs optimally reverberate through the network activity through transient synchronization. Then, we introduce a local homeostatic dynamic in the synaptic coupling and show that it produces a robust self-organization toward the edge of this phase transition. We discuss the potential biological consequences of this self-organization process, such as its relation to the Brain Criticality hypothesis, its input processing capacity, and how its malfunction could lead to pathological synchronization.","sentences":["Transient or partial synchronization can be used to do computations, although a fully synchronized network is frequently related to epileptic seizures.","Here, we propose a homeostatic mechanism that is capable of maintaining a neuronal network at the edge of a synchronization transition, thereby avoiding the harmful consequences of a fully synchronized network.","We model neurons by maps since they are dynamically richer than integrate-and-fire models and more computationally efficient than conductance-based approaches.","We first describe the synchronization phase transition of a dense network of neurons with different tonic spiking frequencies coupled by gap junctions.","We show that at the transition critical point, inputs optimally reverberate through the network activity through transient synchronization.","Then, we introduce a local homeostatic dynamic in the synaptic coupling and show that it produces a robust self-organization toward the edge of this phase transition.","We discuss the potential biological consequences of this self-organization process, such as its relation to the Brain Criticality hypothesis, its input processing capacity, and how its malfunction could lead to pathological synchronization."],"url":"http://arxiv.org/abs/2402.05032v1","category":"nlin.AO"}
{"created":"2024-02-07 17:00:09","title":"Realization of Wess-Zumino-Witten transitions with levels $k=6$ and $k=4$ in a frustrated spin-3 chain","abstract":"We study dimerization transitions in a frustrated spin-3 chain with next-nearest neighbor and three-site interactions. We show that two independent coupling constants of the model are sufficient to fine-tune the system to the critical point in the Wess-Zumino-Witten SU(2)$_6$ universality class. This critical point appears as the end point of an extended SU(2)$_4$ critical line. This implies that the renormalization group flow lead to the critical theory with the largest level $k$ such that the number of relevant operators is reduced by one and the parity of the level is preserved. We also report the appearance of non-magnetic Ising transition between the topologically trivial uniform and dimerized phases. This transition takes place within the singlet sector, while magnetic gap remains open.","sentences":["We study dimerization transitions in a frustrated spin-3 chain with next-nearest neighbor and three-site interactions.","We show that two independent coupling constants of the model are sufficient to fine-tune the system to the critical point in the Wess-Zumino-Witten SU(2)$_6$ universality class.","This critical point appears as the end point of an extended SU(2)$_4$ critical line.","This implies that the renormalization group flow lead to the critical theory with the largest level $k$ such that the number of relevant operators is reduced by one and the parity of the level is preserved.","We also report the appearance of non-magnetic Ising transition between the topologically trivial uniform and dimerized phases.","This transition takes place within the singlet sector, while magnetic gap remains open."],"url":"http://arxiv.org/abs/2402.05031v1","category":"cond-mat.str-el"}
{"created":"2024-02-07 16:59:00","title":"Inference for Two-Stage Extremum Estimators","abstract":"We present a simulation-based approach to approximate the asymptotic variance and asymptotic distribution function of two-stage estimators. We focus on extremum estimators in the second stage and consider a large class of estimators in the first stage. This class includes extremum estimators, high-dimensional estimators, and other types of estimators (e.g., Bayesian estimators). We accommodate scenarios where the asymptotic distributions of both the first- and second-stage estimators are non-normal. We also allow for the second-stage estimator to exhibit a significant bias due to the first-stage sampling error. We introduce a debiased plug-in estimator and establish its limiting distribution. Our method is readily implementable with complex models. Unlike resampling methods, we eliminate the need for multiple computations of the plug-in estimator. Monte Carlo simulations confirm the effectiveness of our approach in finite samples. We present an empirical application with peer effects on adolescent fast-food consumption habits, where we employ the proposed method to address the issue of biased instrumental variable estimates resulting from the presence of many weak instruments.","sentences":["We present a simulation-based approach to approximate the asymptotic variance and asymptotic distribution function of two-stage estimators.","We focus on extremum estimators in the second stage and consider a large class of estimators in the first stage.","This class includes extremum estimators, high-dimensional estimators, and other types of estimators (e.g., Bayesian estimators).","We accommodate scenarios where the asymptotic distributions of both the first- and second-stage estimators are non-normal.","We also allow for the second-stage estimator to exhibit a significant bias due to the first-stage sampling error.","We introduce a debiased plug-in estimator and establish its limiting distribution.","Our method is readily implementable with complex models.","Unlike resampling methods, we eliminate the need for multiple computations of the plug-in estimator.","Monte Carlo simulations confirm the effectiveness of our approach in finite samples.","We present an empirical application with peer effects on adolescent fast-food consumption habits, where we employ the proposed method to address the issue of biased instrumental variable estimates resulting from the presence of many weak instruments."],"url":"http://arxiv.org/abs/2402.05030v1","category":"econ.EM"}
{"created":"2024-02-07 16:36:47","title":"Quantum Tensor Product Decomposition from Choi State Tomography","abstract":"The Schmidt decomposition is the go-to tool for measuring bipartite entanglement of pure quantum states. Similarly, it is possible to study the entangling features of a quantum operation using its operator-Schmidt, or tensor product decomposition. While quantum technological implementations of the former are thoroughly studied, entangling properties on the operator level are harder to extract in the quantum computational framework because of the exponential nature of sample complexity. Here we present an algorithm for unbalanced partitions into a small subsystem and a large one (the environment) to compute the tensor product decomposition of a unitary whose effect on the small subsystem is captured in classical memory while the effect on the environment is accessible as a quantum resource. This quantum algorithm may be used to make predictions about operator non-locality, effective open quantum dynamics on a subsystem, as well as for finding low-rank approximations and low-depth compilations of quantum circuit unitaries. We demonstrate the method and its applications on a time-evolution unitary of an isotropic Heisenberg model in two dimensions.","sentences":["The Schmidt decomposition is the go-to tool for measuring bipartite entanglement of pure quantum states.","Similarly, it is possible to study the entangling features of a quantum operation using its operator-Schmidt, or tensor product decomposition.","While quantum technological implementations of the former are thoroughly studied, entangling properties on the operator level are harder to extract in the quantum computational framework because of the exponential nature of sample complexity.","Here we present an algorithm for unbalanced partitions into a small subsystem and a large one (the environment) to compute the tensor product decomposition of a unitary whose effect on the small subsystem is captured in classical memory while the effect on the environment is accessible as a quantum resource.","This quantum algorithm may be used to make predictions about operator non-locality, effective open quantum dynamics on a subsystem, as well as for finding low-rank approximations and low-depth compilations of quantum circuit unitaries.","We demonstrate the method and its applications on a time-evolution unitary of an isotropic Heisenberg model in two dimensions."],"url":"http://arxiv.org/abs/2402.05018v1","category":"quant-ph"}
{"created":"2024-02-07 16:32:02","title":"Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching","abstract":"Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \\textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching. Moreover, we design a loss function to further extract knowledge from the expert trajectories. Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets. Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM.","sentences":["Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs.","Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation.","To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one.","This significantly limits both the scale and efficacy of the condensed graph.","In this paper, we make the first attempt toward \\textit{lossless graph condensation} by bridging the previously neglected supervision signals.","Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching.","Moreover, we design a loss function to further extract knowledge from the expert trajectories.","Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets.","Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM."],"url":"http://arxiv.org/abs/2402.05011v1","category":"cs.LG"}
{"created":"2024-02-07 16:30:05","title":"Exhaust Gas Optimization of Modern Scooters by Velocity Control","abstract":"This paper investigates the optimization of the exhaust gas composition by applying a velocity-controlled Throttle-by-Wire-System on modern 50 cc scooters (Euro 5). Nowadays combustion-powered scooters are still inefficiently restricted, resulting in an unreasonably high fuel consumption and unfavorable exhaust emissions. The velocity control prevents restriction by negatively shifting the ignition timing and regulates the throttle valve opening instead. Injection quantity, engine speed, ignition timing, cylinder wall temperature, exhaust gas temperature, oxygen sensor data, crankshaft position and in-cylinder pressure were acquired to measure engine parameters. At the same time, vehicle data on the CAN bus, such as throttle opening angle, the rider's acceleration command and vehicle velocity were recorded. For determination of the exhaust gas composition, five probes were sensing CO, CO2, NOx, O2 and HC in addition to the temperature and mass flow. A Peugeot Kisbee 50 4T (Euro 5) serves as test vehicle. The original and the optimized restriction were subjected to various gradients on a roller dynamometer at top speed. Thus, a statement can be made about all operating points of restriction. The resistance parameters required, were previously determined in a coast down test. When driving on level ground, a difference of 50% in the throttle opening leads to a 17% improvement in fuel economy. By measuring the engine parameters, optimum ignition timing could be proven with increasing internal cylinder pressure. Further, 17% reduction in exhaust gas flow was demonstrated. CO emissions decreased by a factor of 8.4, CO2 by 1.17 and HC by 2.1 while NOx increased by a factor of 3.","sentences":["This paper investigates the optimization of the exhaust gas composition by applying a velocity-controlled Throttle-by-Wire-System on modern 50 cc scooters (Euro 5).","Nowadays combustion-powered scooters are still inefficiently restricted, resulting in an unreasonably high fuel consumption and unfavorable exhaust emissions.","The velocity control prevents restriction by negatively shifting the ignition timing and regulates the throttle valve opening instead.","Injection quantity, engine speed, ignition timing, cylinder wall temperature, exhaust gas temperature, oxygen sensor data, crankshaft position and in-cylinder pressure were acquired to measure engine parameters.","At the same time, vehicle data on the CAN bus, such as throttle opening angle, the rider's acceleration command and vehicle velocity were recorded.","For determination of the exhaust gas composition, five probes were sensing CO, CO2, NOx, O2 and HC in addition to the temperature and mass flow.","A Peugeot Kisbee 50 4T (Euro 5) serves as test vehicle.","The original and the optimized restriction were subjected to various gradients on a roller dynamometer at top speed.","Thus, a statement can be made about all operating points of restriction.","The resistance parameters required, were previously determined in a coast down test.","When driving on level ground, a difference of 50% in the throttle opening leads to a 17% improvement in fuel economy.","By measuring the engine parameters, optimum ignition timing could be proven with increasing internal cylinder pressure.","Further, 17% reduction in exhaust gas flow was demonstrated.","CO emissions decreased by a factor of 8.4, CO2 by 1.17 and HC by 2.1 while NOx increased by a factor of 3."],"url":"http://arxiv.org/abs/2402.05010v1","category":"eess.SY"}
{"created":"2024-02-07 16:19:50","title":"Efficient Invariant Kalman Filter for Inertial-based Odometry with Large-sample Environmental Measurements","abstract":"A filter for inertial-based odometry is a recursive method used to estimate the pose from measurements of ego-motion and relative pose. Currently, there is no known filter that guarantees the computation of a globally optimal solution for the non-linear measurement model. In this paper, we demonstrate that an innovative filter, with the state being $SE_2(3)$ and the $\\sqrt{n}$-\\textit{consistent} pose as the initialization, efficiently achieves \\textit{asymptotic optimality} in terms of minimum mean square error. This approach is tailored for real-time SLAM and inertial-based odometry applications.   Our first contribution is that we propose an iterative filtering method based on the Gauss-Newton method on Lie groups which is numerically to solve the estimation of states from a priori and non-linear measurements. The filtering stands out due to its iterative mechanism and adaptive initialization. Second, when dealing with environmental measurements of the surroundings, we utilize a $\\sqrt{n}$-consistent pose as the initial value for the update step in a single iteration. The solution is closed in form and has computational complexity $O(n)$. Third, we theoretically show that the approach can achieve asymptotic optimality in the sense of minimum mean square error from the a priori and virtual relative pose measurements (see Problem~\\ref{prob:new update problem}). Finally, to validate our method, we carry out extensive numerical and experimental evaluations. Our results consistently demonstrate that our approach outperforms other state-of-the-art filter-based methods, including the iterated extended Kalman filter and the invariant extended Kalman filter, in terms of accuracy and running time.","sentences":["A filter for inertial-based odometry is a recursive method used to estimate the pose from measurements of ego-motion and relative pose.","Currently, there is no known filter that guarantees the computation of a globally optimal solution for the non-linear measurement model.","In this paper, we demonstrate that an innovative filter, with the state being $SE_2(3)$ and the $\\sqrt{n}$-\\textit{consistent} pose as the initialization, efficiently achieves \\textit{asymptotic optimality} in terms of minimum mean square error.","This approach is tailored for real-time SLAM and inertial-based odometry applications.   ","Our first contribution is that we propose an iterative filtering method based on the Gauss-Newton method on Lie groups which is numerically to solve the estimation of states from a priori and non-linear measurements.","The filtering stands out due to its iterative mechanism and adaptive initialization.","Second, when dealing with environmental measurements of the surroundings, we utilize a $\\sqrt{n}$-consistent pose as the initial value for the update step in a single iteration.","The solution is closed in form and has computational complexity $O(n)$. Third, we theoretically show that the approach can achieve asymptotic optimality in the sense of minimum mean square error from the a priori and virtual relative pose measurements (see Problem~\\ref{prob:new update problem}).","Finally, to validate our method, we carry out extensive numerical and experimental evaluations.","Our results consistently demonstrate that our approach outperforms other state-of-the-art filter-based methods, including the iterated extended Kalman filter and the invariant extended Kalman filter, in terms of accuracy and running time."],"url":"http://arxiv.org/abs/2402.05003v1","category":"cs.RO"}
{"created":"2024-02-07 16:18:59","title":"Randomized Confidence Bounds for Stochastic Partial Monitoring","abstract":"The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback. On each round, a learning agent plays an action while the environment simultaneously chooses an outcome. The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome. The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss. In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action on each round. In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes. We introduce a new class of strategies based on the randomization of deterministic confidence bounds, that extend regret guarantees to settings where existing stochastic strategies are not applicable. Our experiments show that the proposed RandCBP and RandCBPside* strategies improve state-of-the-art baselines in PM games. To encourage the adoption of the PM framework, we design a use case on the real-world problem of monitoring the error rate of any deployed classification system.","sentences":["The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback.","On each round, a learning agent plays an action while the environment simultaneously chooses an outcome.","The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome.","The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss.","In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action on each round.","In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes.","We introduce a new class of strategies based on the randomization of deterministic confidence bounds, that extend regret guarantees to settings where existing stochastic strategies are not applicable.","Our experiments show that the proposed RandCBP and RandCBPside* strategies improve state-of-the-art baselines in PM games.","To encourage the adoption of the PM framework, we design a use case on the real-world problem of monitoring the error rate of any deployed classification system."],"url":"http://arxiv.org/abs/2402.05002v1","category":"cs.LG"}
{"created":"2024-02-07 16:15:59","title":"Pedagogical Alignment of Large Language Models","abstract":"In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM behaviour. Building on this perspective, we propose a novel approach for constructing a reward dataset specifically designed for the pedagogical alignment of LLMs. We apply three state-of-the-art RLHF algorithms and find that they outperform SFT significantly. Our qualitative analyses across model differences and hyperparameter sensitivity further validate the superiority of RLHF over SFT. Also, our study sheds light on the potential of online feedback for enhancing the performance of pedagogically-aligned LLMs, thus providing valuable insights for the advancement of these models in educational settings.","sentences":["In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts.","Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints.","The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter.","Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods.","This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM behaviour.","Building on this perspective, we propose a novel approach for constructing a reward dataset specifically designed for the pedagogical alignment of LLMs.","We apply three state-of-the-art RLHF algorithms and find that they outperform SFT significantly.","Our qualitative analyses across model differences and hyperparameter sensitivity further validate the superiority of RLHF over SFT.","Also, our study sheds light on the potential of online feedback for enhancing the performance of pedagogically-aligned LLMs, thus providing valuable insights for the advancement of these models in educational settings."],"url":"http://arxiv.org/abs/2402.05000v1","category":"cs.CL"}
{"created":"2024-02-07 16:12:49","title":"Continuous operation of large-scale atom arrays in optical lattices","abstract":"Scaling the size of assembled neutral-atom arrays trapped in optical lattices or optical tweezers is an enabling step for a number of applications ranging from quantum simulations to quantum metrology. However, preparation times increase with system size and constitute a severe bottleneck in the bottom-up assembly of large ordered arrays from stochastically loaded optical traps. Here, we demonstrate a novel method to circumvent this bottleneck by recycling atoms from one experimental run to the next, while continuously reloading and adding atoms to the array. Using this approach, we achieve densely-packed arrays with more than 1000 atoms stored in an optical lattice, continuously refilled with a net 2.5 seconds cycle time and about 130 atoms reloaded during each cycle. Furthermore, we show that we can continuously maintain such large arrays by simply reloading atoms that are lost from one cycle to the next. Our approach paves the way towards quantum science with large ordered atomic arrays containing thousands of atoms in continuous operation.","sentences":["Scaling the size of assembled neutral-atom arrays trapped in optical lattices or optical tweezers is an enabling step for a number of applications ranging from quantum simulations to quantum metrology.","However, preparation times increase with system size and constitute a severe bottleneck in the bottom-up assembly of large ordered arrays from stochastically loaded optical traps.","Here, we demonstrate a novel method to circumvent this bottleneck by recycling atoms from one experimental run to the next, while continuously reloading and adding atoms to the array.","Using this approach, we achieve densely-packed arrays with more than 1000 atoms stored in an optical lattice, continuously refilled with a net 2.5 seconds cycle time and about 130 atoms reloaded during each cycle.","Furthermore, we show that we can continuously maintain such large arrays by simply reloading atoms that are lost from one cycle to the next.","Our approach paves the way towards quantum science with large ordered atomic arrays containing thousands of atoms in continuous operation."],"url":"http://arxiv.org/abs/2402.04994v1","category":"quant-ph"}
{"created":"2024-02-07 16:10:13","title":"Challenges and opportunities in the supervised learning of quantum circuit outputs","abstract":"Recently, deep neural networks have proven capable of predicting some output properties of relevant random quantum circuits, indicating a strategy to emulate quantum computers alternative to direct simulation methods such as, e.g., tensor-network methods. However, the reach of this alternative strategy is not yet clear. Here we investigate if and to what extent neural networks can learn to predict the output expectation values of circuits often employed in variational quantum algorithms, namely, circuits formed by layers of CNOT gates alternated with random single-qubit rotations. On the one hand, we find that the computational cost of supervised learning scales exponentially with the inter-layer variance of the random angles. This allows entering a regime where quantum computers can easily outperform classical neural networks. On the other hand, circuits featuring only inter-qubit angle variations are easily emulated. In fact, thanks to a suitable scalable design, neural networks accurately predict the output of larger and deeper circuits than those used for training, even reaching circuit sizes which turn out to be intractable for the most common simulation libraries, considering both state-vector and tensor-network algorithms. We provide a repository of testing data in this regime, to be used for future benchmarking of quantum devices and novel classical algorithms.","sentences":["Recently, deep neural networks have proven capable of predicting some output properties of relevant random quantum circuits, indicating a strategy to emulate quantum computers alternative to direct simulation methods such as, e.g., tensor-network methods.","However, the reach of this alternative strategy is not yet clear.","Here we investigate if and to what extent neural networks can learn to predict the output expectation values of circuits often employed in variational quantum algorithms, namely, circuits formed by layers of CNOT gates alternated with random single-qubit rotations.","On the one hand, we find that the computational cost of supervised learning scales exponentially with the inter-layer variance of the random angles.","This allows entering a regime where quantum computers can easily outperform classical neural networks.","On the other hand, circuits featuring only inter-qubit angle variations are easily emulated.","In fact, thanks to a suitable scalable design, neural networks accurately predict the output of larger and deeper circuits than those used for training, even reaching circuit sizes which turn out to be intractable for the most common simulation libraries, considering both state-vector and tensor-network algorithms.","We provide a repository of testing data in this regime, to be used for future benchmarking of quantum devices and novel classical algorithms."],"url":"http://arxiv.org/abs/2402.04992v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-07 16:08:36","title":"The possibility of panspermia in the deep cosmos by means of the planetary dust grains","abstract":"By obtaining the assumption that planetary dust particles can escape from the gravitational attraction of a planet, we consider the possibility for the dust grains to leave the star's system by means of the radiation pressure. By taking the typical dust parameters into account, we consider their dynamics and show that they can reach the deep cosmos, taking part in panspermia. It has been shown that, during $5$ billion years, the dust grains will reach $10^5$ stellar systems, and by taking the Drake equation into account, it has been shown that the whole galaxy will be full of planetary dust particles.","sentences":["By obtaining the assumption that planetary dust particles can escape from the gravitational attraction of a planet, we consider the possibility for the dust grains to leave the star's system by means of the radiation pressure.","By taking the typical dust parameters into account, we consider their dynamics and show that they can reach the deep cosmos, taking part in panspermia.","It has been shown that, during $5$ billion years, the dust grains will reach $10^5$ stellar systems, and by taking the Drake equation into account, it has been shown that the whole galaxy will be full of planetary dust particles."],"url":"http://arxiv.org/abs/2402.04990v1","category":"astro-ph.EP"}
{"created":"2024-02-07 16:01:53","title":"Hovering Flight in Flapping Insects and Hummingbirds: A Natural Real-Time and Stable Extremum Seeking Feedback System","abstract":"In this paper, we show that the physical phenomenon of hovering flight is comprehensively characterized and captured if considered and treated as an extremum-seeking (ES) feedback system. Said novel characterization solves all the puzzle pieces of hovering flight that existed for decades in previous literature: it provides a simple model-free, real-time, stable feedback system. Consistent with natural observations and biological experiments, hovering via ES is simply achievable by the natural oscillations of the wing angles and measuring (sensing) altitude and/or power. We provide simulation trials to demonstrate the effectiveness of our results on dronefly, hawkmoth, bumblebee, fruitfly insects, and hummingbird.","sentences":["In this paper, we show that the physical phenomenon of hovering flight is comprehensively characterized and captured if considered and treated as an extremum-seeking (ES) feedback system.","Said novel characterization solves all the puzzle pieces of hovering flight that existed for decades in previous literature: it provides a simple model-free, real-time, stable feedback system.","Consistent with natural observations and biological experiments, hovering via ES is simply achievable by the natural oscillations of the wing angles and measuring (sensing) altitude and/or power.","We provide simulation trials to demonstrate the effectiveness of our results on dronefly, hawkmoth, bumblebee, fruitfly insects, and hummingbird."],"url":"http://arxiv.org/abs/2402.04985v1","category":"math.OC"}
{"created":"2024-02-07 15:57:54","title":"SWAP algorithm for lattice spin models","abstract":"We adapted the SWAP molecular dynamics algorithm for use in lattice Ising spin models. We dressed the spins with a randomly distributed length and we alternated long-range spin exchanges with conventional single spin flip Monte Carlo updates, both accepted with a stochastic rule which respects detailed balance. We show that this algorithm, when applied to the bidimensional Edwards-Anderson model, speeds up significantly the relaxation at low temperatures and manages to find ground states with high efficiency and little computational cost. The exploration of spin models should help in understanding why SWAP accelerates the evolution of particle systems and shed light on relations between dynamics and free-energy landscapes.","sentences":["We adapted the SWAP molecular dynamics algorithm for use in lattice Ising spin models.","We dressed the spins with a randomly distributed length and we alternated long-range spin exchanges with conventional single spin flip Monte Carlo updates, both accepted with a stochastic rule which respects detailed balance.","We show that this algorithm, when applied to the bidimensional Edwards-Anderson model, speeds up significantly the relaxation at low temperatures and manages to find ground states with high efficiency and little computational cost.","The exploration of spin models should help in understanding why SWAP accelerates the evolution of particle systems and shed light on relations between dynamics and free-energy landscapes."],"url":"http://arxiv.org/abs/2402.04981v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 15:41:24","title":"Non-relativistic trace anomaly and equation of state in dense fermionic matter","abstract":"We theoretically investigate a non-relativistic trace anomaly and its impact on the low-temperature equation of state in spatially one-dimensional three-component fermionic systems with a three-body interaction, which exhibit a non-trivial three-body crossover from a bound trimer gas to dense fermionic matter with increasing density. By applying the $G$-matrix approach to the three-body interaction, we obtain the analytical expression for the ground-state equation of state relevant to the high-density degenerate regime and thereby address how the three-body contact or, equivalently, the trace anomaly emerges. The analytical results are compared with the recent quantum Monte Carlo data. Our study of the trace anomaly and the sound speed could have some relevance to the physics of hadron-quark crossover in compact stars.","sentences":["We theoretically investigate a non-relativistic trace anomaly and its impact on the low-temperature equation of state in spatially one-dimensional three-component fermionic systems with a three-body interaction, which exhibit a non-trivial three-body crossover from a bound trimer gas to dense fermionic matter with increasing density.","By applying the $G$-matrix approach to the three-body interaction, we obtain the analytical expression for the ground-state equation of state relevant to the high-density degenerate regime and thereby address how the three-body contact or, equivalently, the trace anomaly emerges.","The analytical results are compared with the recent quantum Monte Carlo data.","Our study of the trace anomaly and the sound speed could have some relevance to the physics of hadron-quark crossover in compact stars."],"url":"http://arxiv.org/abs/2402.04960v1","category":"hep-ph"}
{"created":"2024-02-07 15:41:14","title":"Margin Propagation based XOR-SAT Solvers for Decoding of LDPC Codes","abstract":"Decoding of Low-Density Parity Check (LDPC) codes can be viewed as a special case of XOR-SAT problems, for which low-computational complexity bit-flipping algorithms have been proposed in the literature. However, a performance gap exists between the bit-flipping LDPC decoding algorithms and the benchmark LDPC decoding algorithms, such as the Sum-Product Algorithm (SPA). In this paper, we propose an XOR-SAT solver using log-sum-exponential functions and demonstrate its advantages for LDPC decoding. This is then approximated using the Margin Propagation formulation to attain a low-complexity LDPC decoder. The proposed algorithm uses soft information to decide the bit-flips that maximize the number of parity check constraints satisfied over an optimization function. The proposed solver can achieve results that are within $0.1$dB of the Sum-Product Algorithm for the same number of code iterations. It is also at least 10x lesser than other Gradient-Descent Bit Flipping decoding algorithms, which are also bit-flipping algorithms based on optimization functions. The approximation using the Margin Propagation formulation does not require any multipliers, resulting in significantly lower computational complexity than other soft-decision Bit-Flipping LDPC decoders.","sentences":["Decoding of Low-Density Parity Check (LDPC) codes can be viewed as a special case of XOR-SAT problems, for which low-computational complexity bit-flipping algorithms have been proposed in the literature.","However, a performance gap exists between the bit-flipping LDPC decoding algorithms and the benchmark LDPC decoding algorithms, such as the Sum-Product Algorithm (SPA).","In this paper, we propose an XOR-SAT solver using log-sum-exponential functions and demonstrate its advantages for LDPC decoding.","This is then approximated using the Margin Propagation formulation to attain a low-complexity LDPC decoder.","The proposed algorithm uses soft information to decide the bit-flips that maximize the number of parity check constraints satisfied over an optimization function.","The proposed solver can achieve results that are within $0.1$dB of the Sum-Product Algorithm for the same number of code iterations.","It is also at least 10x lesser than other Gradient-Descent Bit Flipping decoding algorithms, which are also bit-flipping algorithms based on optimization functions.","The approximation using the Margin Propagation formulation does not require any multipliers, resulting in significantly lower computational complexity than other soft-decision Bit-Flipping LDPC decoders."],"url":"http://arxiv.org/abs/2402.04959v1","category":"cs.IT"}
{"created":"2024-02-07 15:34:28","title":"Global hypoellipticity for a class of first order evolution equations on compact Lie groups","abstract":"We present necessary and sufficient conditions to have global hypoellipticity for a class of complex-valued coefficient first order evolution equations defined on $\\mathbb{T}^1 \\times G$, where $G$ is a compact Lie group. First, we show that the global hypoellipticity of the constant coefficient operator related to this operator is a necessary condition, but not a sufficient condition. Under certain hypothesis, we show that the global hypoellipticity of this class of operator is completely characterized by Nirenberg-Treves' condition $(\\mathcal{P})$.","sentences":["We present necessary and sufficient conditions to have global hypoellipticity for a class of complex-valued coefficient first order evolution equations defined on $\\mathbb{T}^1 \\times G$, where $G$ is a compact Lie group.","First, we show that the global hypoellipticity of the constant coefficient operator related to this operator is a necessary condition, but not a sufficient condition.","Under certain hypothesis, we show that the global hypoellipticity of this class of operator is completely characterized by Nirenberg-Treves' condition $(\\mathcal{P})$."],"url":"http://arxiv.org/abs/2402.04950v1","category":"math.AP"}
{"created":"2024-02-07 15:31:16","title":"Critical axis of Ruelle resonances for Anosov flow with a potential","abstract":"We combine methods from microlocal analysis and dimension theory to study resonances with largest real part for an Anosov flow with smooth real valued potential. We show that the resonant states are closely related to special systems of measures supported on the stable manifolds introduced by Climenhaga. As a result, we relate the presence of the resonances on the critical axis to mixing properties of the flow with respect to certain equilibrium measures and show that these equilibrium measures can be reconstructed from the spectral theory of the Anosov flow.","sentences":["We combine methods from microlocal analysis and dimension theory to study resonances with largest real part for an Anosov flow with smooth real valued potential.","We show that the resonant states are closely related to special systems of measures supported on the stable manifolds introduced by Climenhaga.","As a result, we relate the presence of the resonances on the critical axis to mixing properties of the flow with respect to certain equilibrium measures and show that these equilibrium measures can be reconstructed from the spectral theory of the Anosov flow."],"url":"http://arxiv.org/abs/2402.04948v1","category":"math.DS"}
{"created":"2024-02-07 15:14:41","title":"Quantum control by effective counterdiabatic driving","abstract":"We review a scheme for the systematic design of quantum control protocols based on shortcuts to adiabaticity in few-level quantum systems. The adiabatic dynamics is accelerated by introducing high-frequency modulations in the control Hamiltonian, which mimic a time-dependent counterdiabatic correction. We present a number of applications for the high-fidelity realization of quantum state transfers and quantum gates based on effective counterdiabatic driving, in platforms ranging from superconducting circuits to Rydberg atoms.","sentences":["We review a scheme for the systematic design of quantum control protocols based on shortcuts to adiabaticity in few-level quantum systems.","The adiabatic dynamics is accelerated by introducing high-frequency modulations in the control Hamiltonian, which mimic a time-dependent counterdiabatic correction.","We present a number of applications for the high-fidelity realization of quantum state transfers and quantum gates based on effective counterdiabatic driving, in platforms ranging from superconducting circuits to Rydberg atoms."],"url":"http://arxiv.org/abs/2402.04936v1","category":"quant-ph"}
{"created":"2024-02-07 15:11:37","title":"A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health","abstract":"Restless multi-armed bandits (RMABs) are used to model sequential resource allocation in public health intervention programs. In these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (RL). However, existing methods in online RL for RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs. A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons. Empirically, we show that BCoR achieves substantially higher finite-sample performance than existing approaches over a range of experimental settings, including one constructed from a real-world public health campaign in India.","sentences":["Restless multi-armed bandits (RMABs) are used to model sequential resource allocation in public health intervention programs.","In these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (RL).","However, existing methods in online RL for RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity.","We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs.","A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons.","Empirically, we show that BCoR achieves substantially higher finite-sample performance than existing approaches over a range of experimental settings, including one constructed from a real-world public health campaign in India."],"url":"http://arxiv.org/abs/2402.04933v1","category":"cs.LG"}
{"created":"2024-02-07 15:01:15","title":"Complexity of the (Connected) Cluster Vertex Deletion problem on $H$-free graphs","abstract":"The well-known Cluster Vertex Deletion problem (CVD) asks for a given graph $G$ and an integer $k$ whether it is possible to delete a set $S$ of at most $k$ vertices of $G$ such that the resulting graph $G-S$ is a cluster graph (a disjoint union of cliques). We give a complete characterization of graphs $H$ for which CVD on $H$-free graphs is polynomially solvable and for which it is NP-complete. Moreover, in the NP-completeness cases, CVD cannot be solved in sub-exponential time in the vertex number of the $H$-free input graphs unless the Exponential-Time Hypothesis fails. We also consider the connected variant of CVD, the Connected Cluster Vertex Deletion problem (CCVD), in which the set $S$ has to induce a connected subgraph of $G$. It turns out that CCVD admits the same complexity dichotomy for $H$-free graphs. Our results enlarge a list of rare dichotomy theorems for well-studied problems on $H$-free graphs.","sentences":["The well-known Cluster Vertex Deletion problem (CVD) asks for a given graph $G$ and an integer $k$ whether it is possible to delete a set $S$ of at most $k$ vertices of $G$ such that the resulting graph $G-S$ is a cluster graph (a disjoint union of cliques).","We give a complete characterization of graphs $H$ for which CVD on $H$-free graphs is polynomially solvable and for which it is NP-complete.","Moreover, in the NP-completeness cases, CVD cannot be solved in sub-exponential time in the vertex number of the $H$-free input graphs unless the Exponential-Time Hypothesis fails.","We also consider the connected variant of CVD, the Connected Cluster Vertex Deletion problem (CCVD), in which the set $S$ has to induce a connected subgraph of $G$. It turns out that CCVD admits the same complexity dichotomy for $H$-free graphs.","Our results enlarge a list of rare dichotomy theorems for well-studied problems on $H$-free graphs."],"url":"http://arxiv.org/abs/2402.04931v1","category":"cs.DM"}
{"created":"2024-02-07 14:43:48","title":"Maximal displacement of a time-inhomogeneous N(T)-particles branching Brownian motion","abstract":"The $N$-particles branching Brownian motion ($N$-BBM) is a branching Markov process which describes the evolution of a population of particles undergoing reproduction and selection. It shares many properties with the $N$-particles branching random walk ($N$-BRW), which itself is strongly related to physical $p$-spin models, or to Derrida's Random Energy Model. The $N$-BRW can also be seen as the realization of an optimization algorithm over hierarchical data, which is often called beam search. More precisely, the maximal displacement of the $N$-BRW (or $N$-BBM) can be seen as the output of the beam search algorithm; and the population size $N$ is the ``width'' of the beam, and (almost) matches the computational complexity of the algorithm.   In this paper, we investigate the maximal displacement at time $T$ of an $N$-BBM, where $N=N(T)$ is picked arbitrarily depending on $T$ and the diffusion of the process $\\sigma(\\cdot)$ is inhomogeneous in time. We prove the existence of a transition in the second order of the maximal displacement when $\\log N(T)$ is of order $T^{1/3}$. When $\\log N(T)\\ll T^{1/3}$, the maximal displacement behaves according to the Brunet-Derrida correction which has already been studied for $N$ a large constant and for $\\sigma$ constant. When $\\log N(T)\\gg T^{1/3}$, the output of the algorithm (i.e. the maximal displacement) is subject to two phenomena: on the one hand it begins to grow very slowly (logarithmically) in terms of the complexity $N$; and on the other hand its dependency in the time-inhomogeneity $\\sigma(\\cdot)$ becomes more intricate. The transition at $\\log N(T)\\approx T^{1/3}$ can be interpreted as an ``efficiency ceiling'' in the output of the beam search algorithm, which extends previous results from Addario-Berry and Maillard regarding an algorithm hardness threshold for optimization over the Continuous Random Energy Model.","sentences":["The $N$-particles branching Brownian motion ($N$-BBM) is a branching Markov process which describes the evolution of a population of particles undergoing reproduction and selection.","It shares many properties with the $N$-particles branching random walk ($N$-BRW), which itself is strongly related to physical $p$-spin models, or to Derrida's Random Energy Model.","The $N$-BRW can also be seen as the realization of an optimization algorithm over hierarchical data, which is often called beam search.","More precisely, the maximal displacement of the $N$-BRW (or $N$-BBM) can be seen as the output of the beam search algorithm; and the population size $N$ is the ``width'' of the beam, and (almost) matches the computational complexity of the algorithm.   ","In this paper, we investigate the maximal displacement at time $T$ of an $N$-BBM, where $N=N(T)$ is picked arbitrarily depending on $T$ and the diffusion of the process $\\sigma(\\cdot)$ is inhomogeneous in time.","We prove the existence of a transition in the second order of the maximal displacement when $\\log N(T)$ is of order $T^{1/3}$. When $\\log N(T)\\ll T^{1/3}$, the maximal displacement behaves according to the Brunet-Derrida correction which has already been studied for $N$ a large constant and for $\\sigma$ constant.","When $\\log N(T)\\gg T^{1/3}$, the output of the algorithm (i.e. the maximal displacement) is subject to two phenomena: on the one hand it begins to grow very slowly (logarithmically) in terms of the complexity $N$; and on the other hand its dependency in the time-inhomogeneity $\\sigma(\\cdot)$ becomes more intricate.","The transition at $\\log N(T)\\approx T^{1/3}$ can be interpreted as an ``efficiency ceiling'' in the output of the beam search algorithm, which extends previous results from Addario-Berry and Maillard regarding an algorithm hardness threshold for optimization over the Continuous Random Energy Model."],"url":"http://arxiv.org/abs/2402.04917v1","category":"math.PR"}
{"created":"2024-02-07 14:43:46","title":"Simple inexpensive vertex and edge invariants distinguishing dataset strongly regular graphs","abstract":"While standard Weisfeiler-Leman vertex labels are not able to distinguish even vertices of regular graphs, there is proposed and tested family of inexpensive polynomial time vertex and edge invariants, distinguishing much more difficult SRGs (strongly regular graphs), also often their vertices. Among 43717 SRGs from dataset by Edward Spence, proposed vertex invariants alone were able to distinguish all but 4 pairs of graphs, which were easily distinguished by further application of proposed edge invariants. Specifically, proposed vertex invariants are traces or sorted diagonals of $(A|_{N_a})^p$ adjacency matrix $A$ restricted to $N_a$ neighborhood of vertex $a$, already for $p=3$ distinguishing all SRGs from 6 out of 13 sets in this dataset, 8 if adding $p=4$. Proposed edge invariants are analogously traces or diagonals of powers of $\\bar{A}_{ab,cd}=A_{ab} A_{ac} A_{bd}$, nonzero for $(a,b)$ being edges. As SRGs are considered the most difficult cases for graph isomorphism problem, such algebraic-combinatorial invariants bring hope that this problem is polynomial time.","sentences":["While standard Weisfeiler-Leman vertex labels are not able to distinguish even vertices of regular graphs, there is proposed and tested family of inexpensive polynomial time vertex and edge invariants, distinguishing much more difficult SRGs (strongly regular graphs), also often their vertices.","Among 43717 SRGs from dataset by Edward Spence, proposed vertex invariants alone were able to distinguish all but 4 pairs of graphs, which were easily distinguished by further application of proposed edge invariants.","Specifically, proposed vertex invariants are traces or sorted diagonals of $(A|_{N_a})^p$ adjacency matrix $A$ restricted to $N_a$ neighborhood of vertex $a$, already for $p=3$ distinguishing all SRGs from 6 out of 13 sets in this dataset, 8 if adding $p=4$. Proposed edge invariants are analogously traces or diagonals of powers of $\\bar{A}_{ab,cd}=A_{ab} A_{ac} A_{bd}$, nonzero for $(a,b)$ being edges.","As SRGs are considered the most difficult cases for graph isomorphism problem, such algebraic-combinatorial invariants bring hope that this problem is polynomial time."],"url":"http://arxiv.org/abs/2402.04916v1","category":"cs.CC"}
{"created":"2024-02-07 14:35:25","title":"Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects","abstract":"Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.","sentences":["Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making.","Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead.","Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making.","Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions.","Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect."],"url":"http://arxiv.org/abs/2402.04906v1","category":"cs.LG"}
{"created":"2024-02-07 14:35:11","title":"Predicting Interstellar Object Chemodynamics with Gaia","abstract":"The interstellar object population of the Milky Way is a product of its stars. However, what is in fact a complex structure in the Solar neighbourhood has traditionally in ISO studies been described as smoothly distributed. Using a debiased stellar population derived from the Gaia DR3 stellar sample, we infer that the velocity distribution of ISOs is far more textured than a smooth Gaussian. The moving groups caused by Galactic resonances dominate the distribution. 1I/`Oumuamua and 2I/Borisov have entirely normal places within these distributions; 1I is within the non-coeval moving group that includes the Matariki (Pleiades) cluster, and 2I within the Coma Berenices moving group. We show that for the composition of planetesimals formed beyond the ice line, these velocity structures also have a chemodynamic component. This variation will be visible on the sky. We predict that this richly textured distribution will be differentiable from smooth Gaussians in samples that are within the expected discovery capacity of the Vera C. Rubin Observatory. Solar neighbourhood ISOs will be of all ages and come from a dynamic mix of many different populations of stars, reflecting their origins from all around the Galactic disk.","sentences":["The interstellar object population of the Milky Way is a product of its stars.","However, what is in fact a complex structure in the Solar neighbourhood has traditionally in ISO studies been described as smoothly distributed.","Using a debiased stellar population derived from the Gaia DR3 stellar sample, we infer that the velocity distribution of ISOs is far more textured than a smooth Gaussian.","The moving groups caused by Galactic resonances dominate the distribution.","1I/`Oumuamua and 2I/Borisov have entirely normal places within these distributions; 1I is within the non-coeval moving group that includes the Matariki (Pleiades) cluster, and 2I within the Coma Berenices moving group.","We show that for the composition of planetesimals formed beyond the ice line, these velocity structures also have a chemodynamic component.","This variation will be visible on the sky.","We predict that this richly textured distribution will be differentiable from smooth Gaussians in samples that are within the expected discovery capacity of the Vera C. Rubin Observatory.","Solar neighbourhood ISOs will be of all ages and come from a dynamic mix of many different populations of stars, reflecting their origins from all around the Galactic disk."],"url":"http://arxiv.org/abs/2402.04904v1","category":"astro-ph.EP"}
{"created":"2024-02-07 14:32:35","title":"Research on Mobile Network High-precision Absolute Time Synchronization based on TAP","abstract":"With the development of mobile communication and industrial internet technologies, the demand for robust absolute time synchronization based on network for diverse scenarios is significantly growing. TAP is a novel network timing method that aims to achieve sub-microsecond synchronization over air interface. This paper investigates the improvement and end-to-end realization of TAP. This paper first analyzes the effectiveness and deficiencies of TAP by establishing an equivalent clock model which evaluates TAP from timing error composition and allan variance. Second, this paper proposes a detailed base station and terminal design and the corresponding improvement of TAP. Both hardware compensation and protocol software design are taken into account so as to minimize timing error and system cost while maximizing compatibility with 3GPP. Finally, this paper presents a TAP end-to-end 5G prototype system developed based on software defined radio base station and COTS baseband module. The field test results show that the proposed scheme effectively solves the problems of TAP in application and robustly achieves 200ns level timing accuracy in various situations. The average accuracy with long observations can reach 1 nanosecond. It is 2$\\sim$3 orders of magnitude better than common network timing methods, including NTP, PTP and the original TAP.","sentences":["With the development of mobile communication and industrial internet technologies, the demand for robust absolute time synchronization based on network for diverse scenarios is significantly growing.","TAP is a novel network timing method that aims to achieve sub-microsecond synchronization over air interface.","This paper investigates the improvement and end-to-end realization of TAP.","This paper first analyzes the effectiveness and deficiencies of TAP by establishing an equivalent clock model which evaluates TAP from timing error composition and allan variance.","Second, this paper proposes a detailed base station and terminal design and the corresponding improvement of TAP.","Both hardware compensation and protocol software design are taken into account so as to minimize timing error and system cost while maximizing compatibility with 3GPP.","Finally, this paper presents a TAP end-to-end 5G prototype system developed based on software defined radio base station and COTS baseband module.","The field test results show that the proposed scheme effectively solves the problems of TAP in application and robustly achieves 200ns level timing accuracy in various situations.","The average accuracy with long observations can reach 1 nanosecond.","It is 2$\\sim$3 orders of magnitude better than common network timing methods, including NTP, PTP and the original TAP."],"url":"http://arxiv.org/abs/2402.04901v1","category":"cs.NI"}
{"created":"2024-02-07 14:25:20","title":"Dynamic Coalition Portfolio Selection with Recursive Utility","abstract":"In this paper, we consider a dynamic coalition portfolio selection problem, with each agent's objective given by an Epstein--Zin recursive utility. To find a Pareto optimum, the coalition's problem is formulated as an optimization problem evolved by a multi-dimensional forward-backward SDE. Since the evolution system has a forward-backward structure, the problem is intrinsically time-inconsistent. With the dynamic-game point of view, we rigorously develop an approach to finding the equilibrium Pareto investment-consumption strategy. We find that the relationship between risk aversion and EIS has more influence on the coalition's problem than that on the one-agent problem. More interestingly, we show that the equilibrium Pareto consumption strategy associated with the recursive utility is much more effective than that associated with the CRRA expected utility, which highlights the feature of recursive utilities that the marginal benefit of consumption can depend on the future consumption.","sentences":["In this paper, we consider a dynamic coalition portfolio selection problem, with each agent's objective given by an Epstein--Zin recursive utility.","To find a Pareto optimum, the coalition's problem is formulated as an optimization problem evolved by a multi-dimensional forward-backward SDE.","Since the evolution system has a forward-backward structure, the problem is intrinsically time-inconsistent.","With the dynamic-game point of view, we rigorously develop an approach to finding the equilibrium Pareto investment-consumption strategy.","We find that the relationship between risk aversion and EIS has more influence on the coalition's problem than that on the one-agent problem.","More interestingly, we show that the equilibrium Pareto consumption strategy associated with the recursive utility is much more effective than that associated with the CRRA expected utility, which highlights the feature of recursive utilities that the marginal benefit of consumption can depend on the future consumption."],"url":"http://arxiv.org/abs/2402.04895v1","category":"math.OC"}
{"created":"2024-02-07 14:24:06","title":"The Category of Iterative Sets in Homotopy Type Theory and Univalent Foundations","abstract":"When working in Homotopy Type Theory and Univalent Foundations, the traditional role of the category of sets, Set, is replaced by the category hSet of homotopy sets (h-sets); types with h-propositional identity types. Many of the properties of Set hold for hSet ((co)completeness, exactness, local cartesian closure, etc.). Notably, however, the univalence axiom implies that Ob(hSet) is not itself an h-set, but an h-groupoid. This is expected in univalent foundations, but it is sometimes useful to also have a stricter universe of sets, for example when constructing internal models of type theory. In this work, we equip the type of iterative sets V0, due to Gylterud (2018) as a refinement of the pioneering work of Aczel (1978) on universes of sets in type theory, with the structure of a Tarski universe and show that it satisfies many of the good properties of h-sets. In particular, we organize V0 into a (non-univalent strict) category and prove that it is locally cartesian closed. This enables us to organize it into a category with families with the structure necessary to model extensional type theory internally in HoTT/UF. We do this in a rather minimal univalent type theory with W-types, in particular we do not rely on any HITs, or other complex extensions of type theory. Furthermore, the construction of V0 and the model is fully constructive and predicative, while still being very convenient to work with as the decoding from V0 into h-sets commutes definitionally for all type constructors. Almost all of the paper has been formalized in Agda using the agda-unimath library of univalent mathematics.","sentences":["When working in Homotopy Type Theory and Univalent Foundations, the traditional role of the category of sets, Set, is replaced by the category hSet of homotopy sets (h-sets); types with h-propositional identity types.","Many of the properties of Set hold for hSet ((co)completeness, exactness, local cartesian closure, etc.).","Notably, however, the univalence axiom implies that Ob(hSet) is not itself an h-set, but an h-groupoid.","This is expected in univalent foundations, but it is sometimes useful to also have a stricter universe of sets, for example when constructing internal models of type theory.","In this work, we equip the type of iterative sets V0, due to Gylterud (2018) as a refinement of the pioneering work of Aczel (1978) on universes of sets in type theory, with the structure of a Tarski universe and show that it satisfies many of the good properties of h-sets.","In particular, we organize V0 into a (non-univalent strict) category and prove that it is locally cartesian closed.","This enables us to organize it into a category with families with the structure necessary to model extensional type theory internally in HoTT/UF.","We do this in a rather minimal univalent type theory with W-types, in particular we do not rely on any HITs, or other complex extensions of type theory.","Furthermore, the construction of V0 and the model is fully constructive and predicative, while still being very convenient to work with as the decoding from V0 into h-sets commutes definitionally for all type constructors.","Almost all of the paper has been formalized in Agda using the agda-unimath library of univalent mathematics."],"url":"http://arxiv.org/abs/2402.04893v1","category":"cs.LO"}
{"created":"2024-02-07 14:23:58","title":"Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource management in multi-user video transcoding","abstract":"The coexistence of parallel applications in shared computing nodes, each one featuring different Quality of Service (QoS) requirements, carries out new challenges to improve resource occupation while keeping acceptable rates in terms of QoS. As more application-specific and system-wide metrics are included as QoS dimensions, or under situations in which resource-usage limits are strict, building and serving the most appropriate set of actions (application control knobs and system resource assignment) to concurrent applications in an automatic and optimal fashion becomes mandatory. In this paper, we propose strategies to build and serve this type of knowledge to concurrent applications by leveraging Reinforcement Learning techniques. Taking multi-user video transcoding as a driving example, our experimental results reveal an excellent adaptation of resource and knob management to heterogeneous QoS requests, and increases in the amount of concurrently served users up to 1.24x compared with alternative approaches considering homogeneous QoS requests.","sentences":["The coexistence of parallel applications in shared computing nodes, each one featuring different Quality of Service (QoS) requirements, carries out new challenges to improve resource occupation while keeping acceptable rates in terms of QoS.","As more application-specific and system-wide metrics are included as QoS dimensions, or under situations in which resource-usage limits are strict, building and serving the most appropriate set of actions (application control knobs and system resource assignment) to concurrent applications in an automatic and optimal fashion becomes mandatory.","In this paper, we propose strategies to build and serve this type of knowledge to concurrent applications by leveraging Reinforcement Learning techniques.","Taking multi-user video transcoding as a driving example, our experimental results reveal an excellent adaptation of resource and knob management to heterogeneous QoS requests, and increases in the amount of concurrently served users up to 1.24x compared with alternative approaches considering homogeneous QoS requests."],"url":"http://arxiv.org/abs/2402.04891v1","category":"cs.DC"}
{"created":"2024-02-07 14:21:12","title":"Epistral Network: Revolutionizing Media Curation and Consumption through Decentralization","abstract":"Blockchain technology has revolutionized media consumption and distribution in the digital age, allowing creators, consumers, and regulators to participate in a decentralized, fair, and engaging media environment. Epistral, an innovative media network that leverages blockchain technology, aims to be the world's first anti-mimetic media curation and consumption network, addressing the core challenges facing today's digital media landscape: unfair treatment of creators and manipulative consumer algorithms, and the complex task of effective regulation. This paper delves into the conceptualization, design, and potential impact of epistral and explores how it embodies McLuhan's and Girard's theories within the realm of blockchain technology and draws from Hayden's critique of democratic representation. The paper analyzes the challenges and opportunities presented by this new network, providing a broader discourse on the future of media consumption, distribution, and regulation.","sentences":["Blockchain technology has revolutionized media consumption and distribution in the digital age, allowing creators, consumers, and regulators to participate in a decentralized, fair, and engaging media environment.","Epistral, an innovative media network that leverages blockchain technology, aims to be the world's first anti-mimetic media curation and consumption network, addressing the core challenges facing today's digital media landscape: unfair treatment of creators and manipulative consumer algorithms, and the complex task of effective regulation.","This paper delves into the conceptualization, design, and potential impact of epistral and explores how it embodies McLuhan's and Girard's theories within the realm of blockchain technology and draws from Hayden's critique of democratic representation.","The paper analyzes the challenges and opportunities presented by this new network, providing a broader discourse on the future of media consumption, distribution, and regulation."],"url":"http://arxiv.org/abs/2402.04881v1","category":"cs.CR"}
{"created":"2024-02-07 14:16:33","title":"Understanding the Nature of the Ultra-Steep Spectrum Diffuse Radio Source in the Galaxy Cluster Abell 272","abstract":"Ultra-steep spectrum (USS) radio sources with complex filamentary morphologies are a poorly understood subclass of diffuse radio source found in galaxy clusters. They are characterised by power law spectra with spectral indices less than -1.5, and are typically located in merging clusters. We present X-ray and radio observations of the galaxy cluster A272, containing a USS diffuse radio source. The system is an ongoing major cluster merger with an extended region of bright X-ray emission south of the core. Surface brightness analysis yields a $3\\sigma$ detection of a merger shock front in this region. We obtain shock Mach numbers $M_\\rho = 1.20 \\pm 0.09$ and $M_T = 1.7 \\pm 0.3$ from the density and temperature jumps, respectively. Optical data reveals that the system is a merger between a northern cool core cluster and a southern non-cool core cluster. We find that the USS source, with spectral index $\\alpha^{\\text{74 MHz}}_{\\text{1.4 GHz}} = -1.9 \\pm 0.1$, is located in the bright southern region. Radio observations show that the source has a double-lobed structure with complex filaments, and is centred on the brightest cluster galaxy of the southern subcluster. We provide two suggestions for the origin of this source; the first posits the source as an AGN relic that has been re-energised by the passing of a merger shock front, while the second interprets the complex structure as the result of two overlapping AGN radio outbursts. We also present constraints on the inverse Compton emission at the location of the source.","sentences":["Ultra-steep spectrum (USS) radio sources with complex filamentary morphologies are a poorly understood subclass of diffuse radio source found in galaxy clusters.","They are characterised by power law spectra with spectral indices less than -1.5, and are typically located in merging clusters.","We present X-ray and radio observations of the galaxy cluster A272, containing a USS diffuse radio source.","The system is an ongoing major cluster merger with an extended region of bright X-ray emission south of the core.","Surface brightness analysis yields a $3\\sigma$ detection of a merger shock front in this region.","We obtain shock Mach numbers $M_\\rho = 1.20 \\pm 0.09$ and $M_T = 1.7 \\pm 0.3$ from the density and temperature jumps, respectively.","Optical data reveals that the system is a merger between a northern cool core cluster and a southern non-cool core cluster.","We find that the USS source, with spectral index $\\alpha^{\\text{74 MHz}}_{\\text{1.4 GHz}} = -1.9 \\pm 0.1$, is located in the bright southern region.","Radio observations show that the source has a double-lobed structure with complex filaments, and is centred on the brightest cluster galaxy of the southern subcluster.","We provide two suggestions for the origin of this source; the first posits the source as an AGN relic that has been re-energised by the passing of a merger shock front, while the second interprets the complex structure as the result of two overlapping AGN radio outbursts.","We also present constraints on the inverse Compton emission at the location of the source."],"url":"http://arxiv.org/abs/2402.04876v1","category":"astro-ph.CO"}
{"created":"2024-02-07 14:09:44","title":"Fine-Tuning of the Excitonic Response in Monolayer WS2 Domes via Coupled Pressure and Strain Variation","abstract":"We present a spectroscopic investigation into the vibrational and optoelectronic properties of WS2 domes in the 0-0.65 GPa range. The pressure evolution of the system morphology, deduced by the combined analysis of Raman and photoluminescence spectra, revealed a significant variation in the dome's aspect ratio. The modification of the dome shape caused major changes in the mechanical properties of the system resulting in a sizable increase of the out-of-plane compressive strain while keeping the in-plane tensile strain unchanged. The variation of the strain gradients drives a non-linear behavior in both the exciton energy and radiative recombination intensity, interpreted as the consequence of a hybridization mechanism between the electronic states of two distinct minima in the conduction band. Our results indicate that pressure and strain can be efficiently combined in low dimensional systems with unconventional morphology to obtain modulations of the electronic band structure not achievable in planar crystals.","sentences":["We present a spectroscopic investigation into the vibrational and optoelectronic properties of WS2 domes in the 0-0.65 GPa range.","The pressure evolution of the system morphology, deduced by the combined analysis of Raman and photoluminescence spectra, revealed a significant variation in the dome's aspect ratio.","The modification of the dome shape caused major changes in the mechanical properties of the system resulting in a sizable increase of the out-of-plane compressive strain while keeping the in-plane tensile strain unchanged.","The variation of the strain gradients drives a non-linear behavior in both the exciton energy and radiative recombination intensity, interpreted as the consequence of a hybridization mechanism between the electronic states of two distinct minima in the conduction band.","Our results indicate that pressure and strain can be efficiently combined in low dimensional systems with unconventional morphology to obtain modulations of the electronic band structure not achievable in planar crystals."],"url":"http://arxiv.org/abs/2402.04872v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 14:03:14","title":"Collaborative Computing in Non-Terrestrial Networks: A Multi-Time-Scale Deep Reinforcement Learning Approach","abstract":"Constructing earth-fixed cells with low-earth orbit (LEO) satellites in non-terrestrial networks (NTNs) has been the most promising paradigm to enable global coverage. The limited computing capabilities on LEO satellites however render tackling resource optimization within a short duration a critical challenge. Although the sufficient computing capabilities of the ground infrastructures can be utilized to assist the LEO satellite, different time-scale control cycles and coupling decisions between the space- and ground-segments still obstruct the joint optimization design for computing agents at different segments. To address the above challenges, in this paper, a multi-time-scale deep reinforcement learning (DRL) scheme is developed for achieving the radio resource optimization in NTNs, in which the LEO satellite and user equipment (UE) collaborate with each other to perform individual decision-making tasks with different control cycles. Specifically, the UE updates its policy toward improving value functions of both the satellite and UE, while the LEO satellite only performs finite-step rollout for decision-makings based on the reference decision trajectory provided by the UE. Most importantly, rigorous analysis to guarantee the performance convergence of the proposed scheme is provided. Comprehensive simulations are conducted to justify the effectiveness of the proposed scheme in balancing the transmission performance and computational complexity.","sentences":["Constructing earth-fixed cells with low-earth orbit (LEO) satellites in non-terrestrial networks (NTNs) has been the most promising paradigm to enable global coverage.","The limited computing capabilities on LEO satellites however render tackling resource optimization within a short duration a critical challenge.","Although the sufficient computing capabilities of the ground infrastructures can be utilized to assist the LEO satellite, different time-scale control cycles and coupling decisions between the space- and ground-segments still obstruct the joint optimization design for computing agents at different segments.","To address the above challenges, in this paper, a multi-time-scale deep reinforcement learning (DRL) scheme is developed for achieving the radio resource optimization in NTNs, in which the LEO satellite and user equipment (UE) collaborate with each other to perform individual decision-making tasks with different control cycles.","Specifically, the UE updates its policy toward improving value functions of both the satellite and UE, while the LEO satellite only performs finite-step rollout for decision-makings based on the reference decision trajectory provided by the UE.","Most importantly, rigorous analysis to guarantee the performance convergence of the proposed scheme is provided.","Comprehensive simulations are conducted to justify the effectiveness of the proposed scheme in balancing the transmission performance and computational complexity."],"url":"http://arxiv.org/abs/2402.04865v1","category":"eess.SP"}
{"created":"2024-02-07 13:57:52","title":"Tactile Ergodic Control Using Diffusion and Geometric Algebra","abstract":"Continuous physical interaction between robots and their environment is a requirement in many industrial and household tasks, such as sanding and cleaning. Due to the complex tactile information, these tasks are notoriously difficult to model and to sense. In this article, we introduce a closed-loop control method that is constrained to surfaces. The applications that we target have in common that they can be represented by probability distributions on the surface that correlate to the time the robot should spend in a region. These surfaces can easily be captured jointly with the target distributions using coloured point clouds. We present the extension of an ergodic control approach that can be used with point clouds, based on heat equation-driven area coverage (HEDAC). Our method enables closed-loop exploration by measuring the actual coverage using vision. Unlike existing approaches, we approximate the potential field from non-stationary diffusion using spectral acceleration, which does not require complex preprocessing steps and achieves real-time closed-loop control frequencies. We exploit geometric algebra to stay in contact with the target surface by tracking a line while simultaneously exerting a desired force along that line. Our approach is suitable for fully autonomous and human-robot interaction settings where the robot can either directly measure the coverage of the target with its sensors or by being guided online by markings or annotations of a human expert. We tested the performance of the approach in kinematic simulation using point clouds, ranging from the Stanford bunny to a variety of kitchen utensils. Our real-world experiments demonstrate that the proposed approach can successfully be used to wash kitchenware with curved surfaces, by cleaning the dirt detected by vision in an online manner. Website: https://geometric-algebra.tobiloew.ch/tactile_ergodic_control","sentences":["Continuous physical interaction between robots and their environment is a requirement in many industrial and household tasks, such as sanding and cleaning.","Due to the complex tactile information, these tasks are notoriously difficult to model and to sense.","In this article, we introduce a closed-loop control method that is constrained to surfaces.","The applications that we target have in common that they can be represented by probability distributions on the surface that correlate to the time the robot should spend in a region.","These surfaces can easily be captured jointly with the target distributions using coloured point clouds.","We present the extension of an ergodic control approach that can be used with point clouds, based on heat equation-driven area coverage (HEDAC).","Our method enables closed-loop exploration by measuring the actual coverage using vision.","Unlike existing approaches, we approximate the potential field from non-stationary diffusion using spectral acceleration, which does not require complex preprocessing steps and achieves real-time closed-loop control frequencies.","We exploit geometric algebra to stay in contact with the target surface by tracking a line while simultaneously exerting a desired force along that line.","Our approach is suitable for fully autonomous and human-robot interaction settings where the robot can either directly measure the coverage of the target with its sensors or by being guided online by markings or annotations of a human expert.","We tested the performance of the approach in kinematic simulation using point clouds, ranging from the Stanford bunny to a variety of kitchen utensils.","Our real-world experiments demonstrate that the proposed approach can successfully be used to wash kitchenware with curved surfaces, by cleaning the dirt detected by vision in an online manner.","Website: https://geometric-algebra.tobiloew.ch/tactile_ergodic_control"],"url":"http://arxiv.org/abs/2402.04862v1","category":"cs.RO"}
{"created":"2024-02-07 13:56:17","title":"Inverted Microstrip Gap Waveguide Coplanar EBG Filter for Antenna Applications","abstract":"The possibility of making compact stopband filters using coplanar-coupled EBG resonators in inverted microstrip gap waveguide technology is studied in this work. To do this, the filtering characteristics of different configurations of mushroom-type elements are shown in which the short-circuit element is placed on the edge of the resonator of the patch. The behavior of the structure as well as its main advantages such as: low losses, self-packaging, low level of complexity, flexibility and easy design are illustrated in the paper. To evaluate the possibility of integrating these structures in gap waveguide planar antennas feeding networks, a 5-cell EBG filter was designed and built at the X band. The proposed filter reached a maximum rejection level of minus 35.4 dB, had a stopband centered at 9 GHz and a relative fractional bandwidth below minus 20 dB of 10.6 percent. The new compact filter presented a flat passband in which it was well matched and had low insertion losses that, including the connectors, were close to 1.5 dB in most of the band. These results are enough to improve low-complexity future antenna designs with filter functionalities in this technology.","sentences":["The possibility of making compact stopband filters using coplanar-coupled EBG resonators in inverted microstrip gap waveguide technology is studied in this work.","To do this, the filtering characteristics of different configurations of mushroom-type elements are shown in which the short-circuit element is placed on the edge of the resonator of the patch.","The behavior of the structure as well as its main advantages such as: low losses, self-packaging, low level of complexity, flexibility and easy design are illustrated in the paper.","To evaluate the possibility of integrating these structures in gap waveguide planar antennas feeding networks, a 5-cell EBG filter was designed and built at the X band.","The proposed filter reached a maximum rejection level of minus 35.4 dB, had a stopband centered at 9 GHz and a relative fractional bandwidth below minus 20 dB of 10.6 percent.","The new compact filter presented a flat passband in which it was well matched and had low insertion losses that, including the connectors, were close to 1.5 dB in most of the band.","These results are enough to improve low-complexity future antenna designs with filter functionalities in this technology."],"url":"http://arxiv.org/abs/2402.04860v1","category":"physics.app-ph"}
{"created":"2024-02-07 13:43:18","title":"Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions","abstract":"The performance of optimization methods is often tied to the spectrum of the objective Hessian. Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements -- particularly not for non-convex problems. Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity. This allows to partition the class of non-convex problems into a nested chain of subclasses. Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses. As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade. Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian. Our theory is validated by numerical experiments executed on multiple practical machine learning problems.","sentences":["The performance of optimization methods is often tied to the spectrum of the objective Hessian.","Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements -- particularly not for non-convex problems.","Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity.","This allows to partition the class of non-convex problems into a nested chain of subclasses.","Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses.","As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade.","Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian.","Our theory is validated by numerical experiments executed on multiple practical machine learning problems."],"url":"http://arxiv.org/abs/2402.04843v1","category":"math.OC"}
{"created":"2024-02-07 13:21:41","title":"E(3)-Equivariant Mesh Neural Networks","abstract":"Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have address the need for geometric deep learning on 3D mesh. However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric graphs are competitive in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing.","sentences":["Triangular meshes are widely used to represent three-dimensional objects.","As a result, many recent works have address the need for geometric deep learning on 3D mesh.","However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric graphs are competitive in practice.","Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy.","The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing."],"url":"http://arxiv.org/abs/2402.04821v1","category":"cs.LG"}
{"created":"2024-02-07 13:17:35","title":"An advanced scheme for queue management inTCP/IP networks","abstract":"Active Queue Management (AQM) is a key congestion control scheme that aims to find a balance between keeping high link utilization, minimizing queuing delays, and ensuring a fair share of the bandwidth between the competing flows. Traditional AQM mechanisms use only information that is present at the intermediate nodes (routers). They do not take into account the particularities of the flows composing the traffic. In this paper, we make use of a mechanism, called Explicit RTT Notification (ERN), that shares with routers information about the Round Trip Times (RTTs) of the flows. We propose a new fuzzy logic based AQM controller that relies on the RTTs of the flows to improve fairness between them. The performances of the new proposed method, FuzzyRTT, is examined and compared to existing schemes via simulation experiments.","sentences":["Active Queue Management (AQM) is a key congestion control scheme that aims to find a balance between keeping high link utilization, minimizing queuing delays, and ensuring a fair share of the bandwidth between the competing flows.","Traditional AQM mechanisms use only information that is present at the intermediate nodes (routers).","They do not take into account the particularities of the flows composing the traffic.","In this paper, we make use of a mechanism, called Explicit RTT Notification (ERN), that shares with routers information about the Round Trip Times (RTTs) of the flows.","We propose a new fuzzy logic based AQM controller that relies on the RTTs of the flows to improve fairness between them.","The performances of the new proposed method, FuzzyRTT, is examined and compared to existing schemes via simulation experiments."],"url":"http://arxiv.org/abs/2402.04818v1","category":"cs.NI"}
{"created":"2024-02-07 12:55:24","title":"Hausdorff dimension of recurrence sets for matrix transformations of tori","abstract":"Let $T\\colon\\mathbb{T}^d\\to \\mathbb{T}^d$, defined by $T x=Ax(\\bmod 1)$, where $A$ is a $d\\times d$ integer matrix with eigenvalues $1<|\\lambda_1|\\le|\\lambda_2|\\le\\dots\\le|\\lambda_d|$. We investigate the Hausdorff dimension of the recurrence set   \\[R(\\psi):=\\{x\\in\\mathbb{T}^d\\colon T^nx\\in B(x,\\psi(n)) {\\rm ~for~infinitely~ many~}n\\}\\]   for $\\alpha\\ge\\log|\\lambda_d/\\lambda_1|$, where $\\psi$ is a positive decreasing function defined on $\\mathbb{N}$ and its lower order at infinity is $\\alpha=\\liminf\\limits_{n\\to\\infty}\\frac{-\\log \\psi(n)}{n}$. In the case that $A$ is diagonalizable over $\\mathbb{Q}$ with integral eigenvalues, we obtain the dimension formula.","sentences":["Let $T\\colon\\mathbb{T}^d\\to \\mathbb{T}^d$, defined by $T x=Ax(\\bmod 1)$, where $A$ is a $d\\times d$ integer matrix with eigenvalues $1<|\\lambda_1|\\le|\\lambda_2|\\le\\dots\\le|\\lambda_d|$. We investigate the Hausdorff dimension of the recurrence set   \\[R(\\psi):=\\{x\\in\\mathbb{T}^d\\colon T^nx\\in B(x,\\psi(n))","{\\rm ~for~infinitely~ many~}n\\}\\]   for $\\alpha\\ge\\log|\\lambda_d/\\lambda_1|$, where $\\psi$ is a positive decreasing function defined on $\\mathbb{N}$ and its lower order at infinity is $\\alpha=\\liminf\\limits_{n\\to\\infty}\\frac{-\\log \\psi(n)}{n}$. In the case that $A$ is diagonalizable over $\\mathbb{Q}$ with integral eigenvalues, we obtain the dimension formula."],"url":"http://arxiv.org/abs/2402.04810v1","category":"math.DS"}
{"created":"2024-02-07 12:53:49","title":"Long term dynamics around the Didymos-Dimorphos binary asteroid of boulders ejected after the DART impact","abstract":"In 2022 the DART mission spacecraft impacted the asteroid Dimorphos, the secondary body of the binary Didymos system, ejecting a large number of dust particles, rocks and boulders. The ESA Hera mission will reach the system in 2026 for post--impact studies and possible detection of orbiting fragments. We investigate the long term dynamics of the large boulders ejected by DART to test if any of these objects survive in orbit until the arrival of the Hera mission. To model the dynamics of the boulders we use a numerical model which includes the gravity of non-spherical Didymos and Dimorphos, the solar gravity and the radiation pressure. The SPICE kernels are used to define the correct reference frame for the integration. The dynamics of the boulders is highly chaotic and 1% of the initial boulders survive at least for 4 years on quasi--stable orbits. These orbits are characterised by wide oscillations in eccentricity in antiphase with those in inclination (including spin flips), a mechanism similar to the Kozai one. This behaviour may protect these bodies from close encounters with both asteroids. We also compute the distribution on the surfaces of the asteroids of sesquinary impacts which may influence the dust emission, after the initial DART impact, and the surface composition of the asteroids. The probability of observing boulders by the mission Hera is small but not negligible and an almost constant flux of escaping boulders is expected in the coming years since their lifetime after the DART impact covers a large time interval. Most of re--impacts on Dimorphos occur in the hemisphere opposite to the impact site, preferentially close to the equatorial plane.","sentences":["In 2022 the DART mission spacecraft impacted the asteroid Dimorphos, the secondary body of the binary Didymos system, ejecting a large number of dust particles, rocks and boulders.","The ESA Hera mission will reach the system in 2026 for post--impact studies and possible detection of orbiting fragments.","We investigate the long term dynamics of the large boulders ejected by DART to test if any of these objects survive in orbit until the arrival of the Hera mission.","To model the dynamics of the boulders we use a numerical model which includes the gravity of non-spherical Didymos and Dimorphos, the solar gravity and the radiation pressure.","The SPICE kernels are used to define the correct reference frame for the integration.","The dynamics of the boulders is highly chaotic and 1% of the initial boulders survive at least for 4 years on quasi--stable orbits.","These orbits are characterised by wide oscillations in eccentricity in antiphase with those in inclination (including spin flips), a mechanism similar to the Kozai one.","This behaviour may protect these bodies from close encounters with both asteroids.","We also compute the distribution on the surfaces of the asteroids of sesquinary impacts which may influence the dust emission, after the initial DART impact, and the surface composition of the asteroids.","The probability of observing boulders by the mission Hera is small but not negligible and an almost constant flux of escaping boulders is expected in the coming years since their lifetime after the DART impact covers a large time interval.","Most of re--impacts on Dimorphos occur in the hemisphere opposite to the impact site, preferentially close to the equatorial plane."],"url":"http://arxiv.org/abs/2402.04807v1","category":"astro-ph.EP"}
{"created":"2024-02-07 12:53:37","title":"Time-domain constraints for Positive Real functions: Applications to the dielectric response of a passive material","abstract":"This paper presents a systematic approach to derive physical bounds for Positive Real (PR) functions directly in the Time-Domain (TD). The theory is based on Cauer's representation of an arbitrary PR function together with associated sum rules (moments of the measure) and exploits the unilateral Laplace transform to derive rigorous bounds on the TD response of a passive system. The existence of useful sum rules and related physical bounds relies heavily on an assumption about the PR function having a low- or high-frequency asymptotic expansion at least of odd order 1. As a canonical example, we explore the time-domain dielectric step response of a passive material, either with or without a given pulse raise time. As a particular numerical example, we consider here the electric susceptibility of gold (Au) which is commonly modeled by well established Drude or Brendel Bormann models. An explicit physical bound on the early-time step response of the material is then given in terms of a quadratic function in time which is completely determined by the plasma frequency of the metal.","sentences":["This paper presents a systematic approach to derive physical bounds for Positive Real (PR) functions directly in the Time-Domain (TD).","The theory is based on Cauer's representation of an arbitrary PR function together with associated sum rules (moments of the measure) and exploits the unilateral Laplace transform to derive rigorous bounds on the TD response of a passive system.","The existence of useful sum rules and related physical bounds relies heavily on an assumption about the PR function having a low- or high-frequency asymptotic expansion at least of odd order 1.","As a canonical example, we explore the time-domain dielectric step response of a passive material, either with or without a given pulse raise time.","As a particular numerical example, we consider here the electric susceptibility of gold (Au) which is commonly modeled by well established Drude or Brendel Bormann models.","An explicit physical bound on the early-time step response of the material is then given in terms of a quadratic function in time which is completely determined by the plasma frequency of the metal."],"url":"http://arxiv.org/abs/2402.04806v1","category":"math-ph"}
{"created":"2024-02-07 12:51:49","title":"Ab initio study of subsurface diffusion of Cu on the H-passivated Si(001) surface","abstract":"In this paper we use density-functional theory calculations to analyze both the stability and diffusion of Cu adatoms near and on the H-passivated Si(001) surface. Two different Cu sources are considered: depositing Cu from vacuum, and contaminating Cu outdiffusing from bulk Si. Deposited Cu from vacuum quickly moves subsurface to an interstitial site in the third Si layer (T2). Once there, Cu adatoms enter a subsurface zigzag migration route between T2 and another subsurface site, T2->HSL->T2, along the dimer row direction. Contaminating Cu outdiffusing from bulk is found to be a fast diffuser along both parallel and perpendicular directions to the dimer row when far from the surface. It is attracted to the layers close to the surface and becomes trapped at an interstitial site located at the sixth Si layer (T3). As the outdiffusing Cu atoms get closer to the surface, a channeling zigzag diffusion along the dimer row direction, similar to that one followed by deposited Cu from vacuum, is favoured over diffusion along the perpendicular direction. These results are consistent with previous experimental work done on similar systems and will motivate further experiments on the interesting interaction between Cu and Si surfaces.","sentences":["In this paper we use density-functional theory calculations to analyze both the stability and diffusion of Cu adatoms near and on the H-passivated Si(001) surface.","Two different Cu sources are considered: depositing Cu from vacuum, and contaminating Cu outdiffusing from bulk Si.","Deposited Cu from vacuum quickly moves subsurface to an interstitial site in the third Si layer (T2).","Once there, Cu adatoms enter a subsurface zigzag migration route between T2 and another subsurface site, T2->HSL->T2, along the dimer row direction.","Contaminating Cu outdiffusing from bulk is found to be a fast diffuser along both parallel and perpendicular directions to the dimer row when far from the surface.","It is attracted to the layers close to the surface and becomes trapped at an interstitial site located at the sixth Si layer (T3).","As the outdiffusing Cu atoms get closer to the surface, a channeling zigzag diffusion along the dimer row direction, similar to that one followed by deposited Cu from vacuum, is favoured over diffusion along the perpendicular direction.","These results are consistent with previous experimental work done on similar systems and will motivate further experiments on the interesting interaction between Cu and Si surfaces."],"url":"http://arxiv.org/abs/2402.04804v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 12:36:19","title":"Stability under dwell time constraints: Discretization revisited","abstract":"We decide the stability and compute the Lyapunov exponent of continuous-time linear switching systems with a guaranteed dwell time. The main result asserts that the discretization method with step size~$h$ approximates the Lyapunov exponent with the precision~$C\\,h^2$, where~$C$ is a constant. Let us stress that without the dwell time assumption, the approximation rate is known to be linear in~$h$. Moreover, for every system, the constant~$C$ can be explicitly evaluated. In turn, the discretized system can be treated by computing the Markovian joint spectral radius of a certain system on a graph. This gives the value of the Lyapunov exponent with a high accuracy. The method is efficient for dimensions up to, approximately, ten; for positive systems, the dimensions can be much higher, up to several hundreds.","sentences":["We decide the stability and compute the Lyapunov exponent of continuous-time linear switching systems with a guaranteed dwell time.","The main result asserts that the discretization method with step size~$h$ approximates the Lyapunov exponent with the precision~$C\\,h^2$, where~$C$ is a constant.","Let us stress that without the dwell time assumption, the approximation rate is known to be linear in~$h$.","Moreover, for every system, the constant~$C$ can be explicitly evaluated.","In turn, the discretized system can be treated by computing the Markovian joint spectral radius of a certain system on a graph.","This gives the value of the Lyapunov exponent with a high accuracy.","The method is efficient for dimensions up to, approximately, ten; for positive systems, the dimensions can be much higher, up to several hundreds."],"url":"http://arxiv.org/abs/2402.04795v1","category":"math.DS"}
{"created":"2024-02-07 12:15:56","title":"Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity","abstract":"We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method-Shadowheart SGD-that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.","sentences":["We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers.","Using an unbiassed compression technique, we develop a new method-Shadowheart SGD-that provably improves the time complexities of all previous centralized methods.","Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication.","We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method."],"url":"http://arxiv.org/abs/2402.04785v1","category":"math.OC"}
{"created":"2024-02-07 11:55:53","title":"Entanglement Hamiltonian in the non-Hermitian SSH model","abstract":"Entanglement Hamiltonians provide the most comprehensive characterisation of entanglement in extended quantum systems. A key result in unitary quantum field theories is the Bisognano-Wichmann theorem, which establishes the locality of the entanglement Hamiltonian. In this work, our focus is on the non-Hermitian Su-Schrieffer-Heeger (SSH) chain. We study the entanglement Hamiltonian both in a gapped phase and at criticality. In the gapped phase we find that the lattice entanglement Hamiltonian is compatible with a lattice Bisognano-Wichmann result, with an entanglement temperature linear in the lattice index. At the critical point, we identify a new imaginary chemical potential term absent in unitary models. This operator is responsible for the negative entanglement entropy observed in the non-Hermitian SSH chain at criticality.","sentences":["Entanglement Hamiltonians provide the most comprehensive characterisation of entanglement in extended quantum systems.","A key result in unitary quantum field theories is the Bisognano-Wichmann theorem, which establishes the locality of the entanglement Hamiltonian.","In this work, our focus is on the non-Hermitian Su-Schrieffer-Heeger (SSH) chain.","We study the entanglement Hamiltonian both in a gapped phase and at criticality.","In the gapped phase we find that the lattice entanglement Hamiltonian is compatible with a lattice Bisognano-Wichmann result, with an entanglement temperature linear in the lattice index.","At the critical point, we identify a new imaginary chemical potential term absent in unitary models.","This operator is responsible for the negative entanglement entropy observed in the non-Hermitian SSH chain at criticality."],"url":"http://arxiv.org/abs/2402.04776v1","category":"quant-ph"}
{"created":"2024-02-07 11:43:21","title":"Stochastic Data-Driven Bouligand Landweber Method for Solving Non-smooth Inverse Problems","abstract":"In this study, we present and analyze a novel variant of the stochastic gradient descent method, referred as Stochastic data-driven Bouligand Landweber iteration tailored for addressing the system of non-smooth ill-posed inverse problems. Our method incorporates the utilization of training data, using a bounded linear operator, which guides the iterative procedure. At each iteration step, the method randomly chooses one equation from the nonlinear system with data-driven term. When dealing with the precise or exact data, it has been established that mean square iteration error converges to zero. However, when confronted with the noisy data, we employ our approach in conjunction with a predefined stopping criterion, which we refer to as an \\textit{a-priori} stopping rule. We provide a comprehensive theoretical foundation, establishing convergence and stability for this scheme within the realm of infinite-dimensional Hilbert spaces. These theoretical underpinnings are further bolstered by discussing an example that fulfills assumptions of the paper.","sentences":["In this study, we present and analyze a novel variant of the stochastic gradient descent method, referred as Stochastic data-driven Bouligand Landweber iteration tailored for addressing the system of non-smooth ill-posed inverse problems.","Our method incorporates the utilization of training data, using a bounded linear operator, which guides the iterative procedure.","At each iteration step, the method randomly chooses one equation from the nonlinear system with data-driven term.","When dealing with the precise or exact data, it has been established that mean square iteration error converges to zero.","However, when confronted with the noisy data, we employ our approach in conjunction with a predefined stopping criterion, which we refer to as an \\textit{a-priori} stopping rule.","We provide a comprehensive theoretical foundation, establishing convergence and stability for this scheme within the realm of infinite-dimensional Hilbert spaces.","These theoretical underpinnings are further bolstered by discussing an example that fulfills assumptions of the paper."],"url":"http://arxiv.org/abs/2402.04772v1","category":"math.FA"}
{"created":"2024-02-07 11:09:10","title":"Asymptotic Dynamics of Alternating Minimization for Non-Convex Optimization","abstract":"This study investigates the asymptotic dynamics of alternating minimization applied to optimize a bilinear non-convex function with normally distributed covariates. We employ the replica method from statistical physics in a multi-step approach to precisely trace the algorithm's evolution. Our findings indicate that the dynamics can be described effectively by a two--dimensional discrete stochastic process, where each step depends on all previous time steps, revealing a memory dependency in the procedure. The theoretical framework developed in this work is broadly applicable for the analysis of various iterative algorithms, extending beyond the scope of alternating minimization.","sentences":["This study investigates the asymptotic dynamics of alternating minimization applied to optimize a bilinear non-convex function with normally distributed covariates.","We employ the replica method from statistical physics in a multi-step approach to precisely trace the algorithm's evolution.","Our findings indicate that the dynamics can be described effectively by a two--dimensional discrete stochastic process, where each step depends on all previous time steps, revealing a memory dependency in the procedure.","The theoretical framework developed in this work is broadly applicable for the analysis of various iterative algorithms, extending beyond the scope of alternating minimization."],"url":"http://arxiv.org/abs/2402.04751v1","category":"math.OC"}
{"created":"2024-02-07 11:08:05","title":"AINS: Affordable Indoor Navigation Solution via Line Color Identification Using Mono-Camera for Autonomous Vehicles","abstract":"Recently, researchers have been exploring various ways to improve the effectiveness and efficiency of autonomous vehicles by researching new methods, especially for indoor scenarios. Autonomous Vehicles in indoor navigation systems possess many challenges especially the limited accuracy of GPS in indoor scenarios. Several, robust methods have been explored for autonomous vehicles in indoor scenarios to solve this problem, but the ineffectiveness of the proposed methods is the high deployment cost. To address the above-mentioned problems we have presented A low-cost indoor navigation method for autonomous vehicles called Affordable Indoor Navigation Solution (AINS) which is based on based on Monocular Camera. Our proposed solution is mainly based on a mono camera without relying on various huge or power-inefficient sensors to find the path, such as range finders and other navigation sensors. Our proposed method shows that we can deploy autonomous vehicles indoor navigation systems while taking into consideration the cost. We can observe that the results shown by our solution are better than existing solutions and we can reduce the estimated error and time consumption.","sentences":["Recently, researchers have been exploring various ways to improve the effectiveness and efficiency of autonomous vehicles by researching new methods, especially for indoor scenarios.","Autonomous Vehicles in indoor navigation systems possess many challenges especially the limited accuracy of GPS in indoor scenarios.","Several, robust methods have been explored for autonomous vehicles in indoor scenarios to solve this problem, but the ineffectiveness of the proposed methods is the high deployment cost.","To address the above-mentioned problems we have presented A low-cost indoor navigation method for autonomous vehicles called Affordable Indoor Navigation Solution (AINS) which is based on based on Monocular Camera.","Our proposed solution is mainly based on a mono camera without relying on various huge or power-inefficient sensors to find the path, such as range finders and other navigation sensors.","Our proposed method shows that we can deploy autonomous vehicles indoor navigation systems while taking into consideration the cost.","We can observe that the results shown by our solution are better than existing solutions and we can reduce the estimated error and time consumption."],"url":"http://arxiv.org/abs/2402.04750v1","category":"cs.RO"}
{"created":"2024-02-07 10:58:25","title":"Emergent Berezinskii-Kosterlitz-Thouless and Kugel-Khomskii physics in the triangular lattice bilayer colbaltate","abstract":"Motivated by the experiments on the triangular lattice bilayer colbaltate K$_2$Co$_2$(SeO$_3$)$_3$, we formulate a theory to explore the underlying physics from a couple observation. The model is composed of interacting Co$^{2+}$ dimers on the triangular lattice, where the Co$^{2+}$ ion provides an effective spin-1/2 local moment via the spin-orbit coupling and the crystal field effect. The intra-dimer interaction is dominant and would simply favor the local spin singlet, and the inter-dimer interactions compete with the inter-dimer interaction, leading to rich behaviors. With the easy-axis anisotropy, it is shown that, in the ground state manifold of the intra-dimer Ising interaction, the system realizes an effective transverse field Ising model, where the ground state is either a three-sublattice order or Ising disordered. The finite temperature regime naturally realizes the Berezinskii-Kosterlitz-Thouless physics. To explore the full excitations, we incorporate the excited state manifold of the intra-dimer Ising interaction and establish the emergent Kugel-Khomskii physics. Thus, the triangular lattice bilayer colbaltate is an excellent platform to explore the interplay between geometrical frustration and anisotropic interactions as well as the emergent effective models and the resulting physics.","sentences":["Motivated by the experiments on the triangular lattice bilayer colbaltate K$_2$Co$_2$(SeO$_3$)$_3$, we formulate a theory to explore the underlying physics from a couple observation.","The model is composed of interacting Co$^{2+}$ dimers on the triangular lattice, where the Co$^{2+}$ ion provides an effective spin-1/2 local moment via the spin-orbit coupling and the crystal field effect.","The intra-dimer interaction is dominant and would simply favor the local spin singlet, and the inter-dimer interactions compete with the inter-dimer interaction, leading to rich behaviors.","With the easy-axis anisotropy, it is shown that, in the ground state manifold of the intra-dimer Ising interaction, the system realizes an effective transverse field Ising model, where the ground state is either a three-sublattice order or Ising disordered.","The finite temperature regime naturally realizes the Berezinskii-Kosterlitz-Thouless physics.","To explore the full excitations, we incorporate the excited state manifold of the intra-dimer Ising interaction and establish the emergent Kugel-Khomskii physics.","Thus, the triangular lattice bilayer colbaltate is an excellent platform to explore the interplay between geometrical frustration and anisotropic interactions as well as the emergent effective models and the resulting physics."],"url":"http://arxiv.org/abs/2402.04745v1","category":"cond-mat.str-el"}
{"created":"2024-02-07 10:32:13","title":"A Video-Aware FEC-Based Unequal Loss Protection System for Video Streaming over RTP","abstract":"A video-aware unequal loss protection (ULP) system for protecting RTP video streaming in bursty packet loss networks is proposed. Considering the relevance of the frame, the state of the channel, and the bitrate constraints of the protection bitstream, our algorithm selects in real time the most suitable frames to be protected through forward error protection (FEC) techniques. It benefits from a wise RTP encapsulation that allows working at a frame level without requiring any further process than that of parsing RTP headers. This makes our system straightforward and fast, perfectly suitable to be included in commercial video streaming servers. Simulation results show how our technique outperforms other proposed ULP schemes.","sentences":["A video-aware unequal loss protection (ULP) system for protecting RTP video streaming in bursty packet loss networks is proposed.","Considering the relevance of the frame, the state of the channel, and the bitrate constraints of the protection bitstream, our algorithm selects in real time the most suitable frames to be protected through forward error protection (FEC) techniques.","It benefits from a wise RTP encapsulation that allows working at a frame level without requiring any further process than that of parsing RTP headers.","This makes our system straightforward and fast, perfectly suitable to be included in commercial video streaming servers.","Simulation results show how our technique outperforms other proposed ULP schemes."],"url":"http://arxiv.org/abs/2402.04729v1","category":"eess.IV"}
{"created":"2024-02-07 10:27:16","title":"Phase stability and mechanical property trends for MAB phases by high-throughput ab initio calculations","abstract":"MAB phases (MABs) are atomically-thin laminates of ceramic/metallic-like layers, having made a breakthrough in the development of 2D materials. Though theoretically offering a vast chemical and phase space, relatively few MABs have yet been synthesised. To guide experiments, we perform a systematic high-throughput {\\it{ab initio}} screening of MABs that combine group 4--7 transition metals (M); Al, Si, Ga, Ge, or In (A); and boron (B) focusing on their phase stability trends and mechanical properties. Considering the 1:1:1, 2:1:1, 2:1:2, 3:1:2, 3:1:3, and 3:1:4 M:A:B ratios and 10 phase prototypes, possible stabilisation of a single-phase compound for each elemental combination is assessed through formation energy spectra of the competing mechanically and dynamically stable MABs. Based on the volumetric proximity of energetically-close phases, we identify systems in which volume-changing deformations may facilitate transformation toughening. Subsequently, chemistry- and phase-structure-related trends in the elastic stiffness and ductility are predicted using elastic-constants-based descriptors. The analysis of directional Cauchy pressures and Young's moduli allows comparing mechanical response parallel and normal to M--B/A layers. Among the suggested most promising MABs are Nb$_3$AlB$_4$, Cr$_2$SiB$_2$, Mn$_2$SiB$_2$ or the already synthesised MoAlB.","sentences":["MAB phases (MABs) are atomically-thin laminates of ceramic/metallic-like layers, having made a breakthrough in the development of 2D materials.","Though theoretically offering a vast chemical and phase space, relatively few MABs have yet been synthesised.","To guide experiments, we perform a systematic high-throughput {\\it{ab initio}} screening of MABs that combine group 4--7 transition metals (M); Al, Si, Ga, Ge, or In (A); and boron (B) focusing on their phase stability trends and mechanical properties.","Considering the 1:1:1, 2:1:1, 2:1:2, 3:1:2, 3:1:3, and 3:1:4 M:A:B ratios and 10 phase prototypes, possible stabilisation of a single-phase compound for each elemental combination is assessed through formation energy spectra of the competing mechanically and dynamically stable MABs.","Based on the volumetric proximity of energetically-close phases, we identify systems in which volume-changing deformations may facilitate transformation toughening.","Subsequently, chemistry- and phase-structure-related trends in the elastic stiffness and ductility are predicted using elastic-constants-based descriptors.","The analysis of directional Cauchy pressures and Young's moduli allows comparing mechanical response parallel and normal to M--B/A layers.","Among the suggested most promising MABs are Nb$_3$AlB$_4$, Cr$_2$SiB$_2$, Mn$_2$SiB$_2$ or the already synthesised MoAlB."],"url":"http://arxiv.org/abs/2402.04726v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 10:23:17","title":"The Interaction between Surface Acoustic Wave and Quantum Hall Effects","abstract":"Surface Acoustic Wave (SAW) is a powerful technique for investigating quantum phases appearing in two-dimensional electron systems. The electrons respond to the piezoelectric field of SAWthrough screening, attenuating its amplitude and shifting its velocity, which is described by the relaxation model. In this work, we systematically study this interaction using orders of magnitude lower SAW amplitude than that in previous studies. At high magnetic fields when electrons form highly correlated states such as the quantum Hall effect, we observe an anomalously large attenuation of SAW while the acoustic speed remains considerably high, inconsistent with the conventional relaxation model. This anomaly exists only when the SAW power is sufficiently low.","sentences":["Surface Acoustic Wave (SAW) is a powerful technique for investigating quantum phases appearing in two-dimensional electron systems.","The electrons respond to the piezoelectric field of SAWthrough screening, attenuating its amplitude and shifting its velocity, which is described by the relaxation model.","In this work, we systematically study this interaction using orders of magnitude lower SAW amplitude than that in previous studies.","At high magnetic fields when electrons form highly correlated states such as the quantum Hall effect, we observe an anomalously large attenuation of SAW while the acoustic speed remains considerably high, inconsistent with the conventional relaxation model.","This anomaly exists only when the SAW power is sufficiently low."],"url":"http://arxiv.org/abs/2402.04724v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 10:18:39","title":"On the well-posedness of the Cauchy problem for the two-component peakon system in $C^k\\cap W^{k,1}$","abstract":"This study focuses on the Cauchy problem associated with the two-component peakon system featuring a cubic nonlinearity, constrained to the class $(m,n)\\in C^{k}(\\mathbb{R}) \\cap W^{k,1}(\\mathbb{R})$ with $k\\in\\mathbb{N}\\cup\\{0\\}$.This system extends the celebrated Fokas-Olver-Rosenau-Qiao equation, and the following nonlocal (two-place) counterpart proposed by Lou and Qiao: $$ \\partial_t m(t,x)= \\partial_x[m(t,x)(u(t,x)-\\partial_xu(t,x)) (u(-t,-x)+\\partial_x(u(-t,-x)))], $$ where $m(t,x)=\\left(1-\\partial_{x}^2\\right)u(t,x)$. Employing an approach based on Lagrangian coordinates, we establish the local existence, uniqueness, and Lipschitz continuity of the data-to-solution map in the class $C^k\\cap W^{k,1}$. Moreover, we derive criteria for blow-up of the local solution in this class.","sentences":["This study focuses on the Cauchy problem associated with the two-component peakon system featuring a cubic nonlinearity, constrained to the class $(m,n)\\in C^{k}(\\mathbb{R}) \\cap W^{k,1}(\\mathbb{R})$ with $k\\in\\mathbb{N}\\cup\\{0\\}$.This system extends the celebrated Fokas-Olver-Rosenau-Qiao equation, and the following nonlocal (two-place) counterpart proposed by Lou and Qiao: $$ \\partial_t m(t,x)= \\partial_x[m(t,x)(u(t,x)-\\partial_xu(t,x))","(u(-t,-x)+\\partial_x(u(-t,-x)))], $$ where $m(t,x)=\\left(1-\\partial_{x}^2\\right)u(t,x)$.","Employing an approach based on Lagrangian coordinates, we establish the local existence, uniqueness, and Lipschitz continuity of the data-to-solution map in the class $C^k\\cap W^{k,1}$. Moreover, we derive criteria for blow-up of the local solution in this class."],"url":"http://arxiv.org/abs/2402.04723v1","category":"math.AP"}
{"created":"2024-02-07 10:12:46","title":"Investigating Driving Interactions: A Robust Multi-Agent Simulation Framework for Autonomous Vehicles","abstract":"Current validation methods often rely on recorded data and basic functional checks, which may not be sufficient to encompass the scenarios an autonomous vehicle might encounter. In addition, there is a growing need for complex scenarios with changing vehicle interactions for comprehensive validation. This work introduces a novel synchronous multi-agent simulation framework for autonomous vehicles in interactive scenarios. Our approach creates an interactive scenario and incorporates publicly available edge-case scenarios wherein simulated vehicles are replaced by agents navigating to predefined destinations. We provide a platform that enables the integration of different autonomous driving planning methodologies and includes a set of evaluation metrics to assess autonomous driving behavior. Our study explores different planning setups and adjusts simulation complexity to test the framework's adaptability and performance. Results highlight the critical role of simulating vehicle interactions to enhance autonomous driving systems. Our setup offers unique insights for developing advanced algorithms for complex driving tasks to accelerate future investigations and developments in this field. The multi-agent simulation framework is available as open-source software: https://github.com/TUM-AVS/Frenetix-Motion-Planner","sentences":["Current validation methods often rely on recorded data and basic functional checks, which may not be sufficient to encompass the scenarios an autonomous vehicle might encounter.","In addition, there is a growing need for complex scenarios with changing vehicle interactions for comprehensive validation.","This work introduces a novel synchronous multi-agent simulation framework for autonomous vehicles in interactive scenarios.","Our approach creates an interactive scenario and incorporates publicly available edge-case scenarios wherein simulated vehicles are replaced by agents navigating to predefined destinations.","We provide a platform that enables the integration of different autonomous driving planning methodologies and includes a set of evaluation metrics to assess autonomous driving behavior.","Our study explores different planning setups and adjusts simulation complexity to test the framework's adaptability and performance.","Results highlight the critical role of simulating vehicle interactions to enhance autonomous driving systems.","Our setup offers unique insights for developing advanced algorithms for complex driving tasks to accelerate future investigations and developments in this field.","The multi-agent simulation framework is available as open-source software:","https://github.com/TUM-AVS/Frenetix-Motion-Planner"],"url":"http://arxiv.org/abs/2402.04720v1","category":"cs.RO"}
{"created":"2024-02-07 10:10:06","title":"Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying","abstract":"This paper investigates the efficiency of nonsingular fast terminal sliding mode and adaptive smooth control method for the distributed space telescope demonstration mission. The distributed space telescope has a flexible focal length that corresponds to the relative position of the formation flying concept. The precise formation flying technology by CubeSats enhances the utility of distributed space systems with low costs. The propulsion systems for CubeSats usually have restricted degrees of freedom. Since the scientific mission requires continuous orbit control, the attitude and orbit control system mutually affect the control performance. The nonsingular fast terminal sliding mode has the advantage of a fast convergence rate and is able to improve the control performance. The adaptive smooth controller designed for the SISO system is expanded and applied to the attitude and orbit control system. The simulation results verify the efficiency of the adaptive smooth controller based on the nonsingular fast terminal sliding mode.","sentences":["This paper investigates the efficiency of nonsingular fast terminal sliding mode and adaptive smooth control method for the distributed space telescope demonstration mission.","The distributed space telescope has a flexible focal length that corresponds to the relative position of the formation flying concept.","The precise formation flying technology by CubeSats enhances the utility of distributed space systems with low costs.","The propulsion systems for CubeSats usually have restricted degrees of freedom.","Since the scientific mission requires continuous orbit control, the attitude and orbit control system mutually affect the control performance.","The nonsingular fast terminal sliding mode has the advantage of a fast convergence rate and is able to improve the control performance.","The adaptive smooth controller designed for the SISO system is expanded and applied to the attitude and orbit control system.","The simulation results verify the efficiency of the adaptive smooth controller based on the nonsingular fast terminal sliding mode."],"url":"http://arxiv.org/abs/2402.04718v1","category":"eess.SY"}
{"created":"2024-02-07 09:54:11","title":"Embedding memory-efficient stochastic simulators as quantum trajectories","abstract":"By exploiting the complexity intrinsic to quantum dynamics, quantum technologies promise a whole host of computational advantages. One such advantage lies in the field of stochastic modelling, where it has been shown that quantum stochastic simulators can operate with a lower memory overhead than their best classical counterparts. This advantage is particularly pronounced for continuous-time stochastic processes; however, the corresponding quantum stochastic simulators heretofore prescribed operate only on a quasi-continuous-time basis, and suffer an ever-increasing circuit complexity with increasing temporal resolution. Here, by establishing a correspondence with quantum trajectories -- a method for modelling open quantum systems -- we show how truly continuous-time quantum stochastic simulators can be embedded in such open quantum systems, bridging this gap and obviating previous constraints. We further show how such an embedding can be made for discrete-time stochastic processes, which manifest as jump-only trajectories, and discuss how viewing the correspondence in the reverse direction provides new means of studying structural complexity in quantum systems themselves.","sentences":["By exploiting the complexity intrinsic to quantum dynamics, quantum technologies promise a whole host of computational advantages.","One such advantage lies in the field of stochastic modelling, where it has been shown that quantum stochastic simulators can operate with a lower memory overhead than their best classical counterparts.","This advantage is particularly pronounced for continuous-time stochastic processes; however, the corresponding quantum stochastic simulators heretofore prescribed operate only on a quasi-continuous-time basis, and suffer an ever-increasing circuit complexity with increasing temporal resolution.","Here, by establishing a correspondence with quantum trajectories -- a method for modelling open quantum systems -- we show how truly continuous-time quantum stochastic simulators can be embedded in such open quantum systems, bridging this gap and obviating previous constraints.","We further show how such an embedding can be made for discrete-time stochastic processes, which manifest as jump-only trajectories, and discuss how viewing the correspondence in the reverse direction provides new means of studying structural complexity in quantum systems themselves."],"url":"http://arxiv.org/abs/2402.04708v1","category":"quant-ph"}
{"created":"2024-02-07 09:50:00","title":"Decoherence Rate in Random Lindblad Dynamics","abstract":"Open quantum systems undergo decoherence, responsible for the transition from quantum to classical behavior. The time scale in which decoherence takes place can be analyzed using upper limits to its rate. We examine the dynamics of open chaotic quantum systems governed by random Lindblad operators, sourced from Gaussian and Ginibre ensembles with Wigner-Dyson symmetry classes. In these systems, the ensemble-averaged purity decays monotonically as function of time. This decay is governed by the decoherence rate, which is upper bounded by the dimension of their Hilbert space, and is independent of the ensemble symmetry. These findings hold upon mixing different ensembles, indicating the universal character of the decoherence rate limit. Moreover, our findings reveal that open chaotic quantum systems, governed by random Lindbladians, inherently tend to exhibit the most rapid decoherence, regardless of the initial state. This phenomenon is associated with the concentration of the decoherence rate near its upper bound. Our work identifies primary features of decoherence in dissipative quantum chaos, with applications ranging from quantum foundations to high-energy physics and quantum technologies.","sentences":["Open quantum systems undergo decoherence, responsible for the transition from quantum to classical behavior.","The time scale in which decoherence takes place can be analyzed using upper limits to its rate.","We examine the dynamics of open chaotic quantum systems governed by random Lindblad operators, sourced from Gaussian and Ginibre ensembles with Wigner-Dyson symmetry classes.","In these systems, the ensemble-averaged purity decays monotonically as function of time.","This decay is governed by the decoherence rate, which is upper bounded by the dimension of their Hilbert space, and is independent of the ensemble symmetry.","These findings hold upon mixing different ensembles, indicating the universal character of the decoherence rate limit.","Moreover, our findings reveal that open chaotic quantum systems, governed by random Lindbladians, inherently tend to exhibit the most rapid decoherence, regardless of the initial state.","This phenomenon is associated with the concentration of the decoherence rate near its upper bound.","Our work identifies primary features of decoherence in dissipative quantum chaos, with applications ranging from quantum foundations to high-energy physics and quantum technologies."],"url":"http://arxiv.org/abs/2402.04705v1","category":"quant-ph"}
{"created":"2024-02-07 09:48:54","title":"Nonuniversal Equation of State of a Quasi-2D Bose Gas in Dimensional Crossover","abstract":"Equation of state (EOS) for a pure two-dimensional (2D) Bose gas exhibits a logarithmic dependence on the s-wave scattering length [L. Salasnich, Phys. Rev. Lett. 118, 130402 (2017)]. The pronounced disparity between the EOS of a 2D Bose gas and its 3D counterpart underscores the significance of exploring the dimensional crossover between these two distinct dimensions. In this work, we are motivated to deduce nonuniversal corrections to EOS for an optically trapped Bose gas along the dimensional crossover from 3D to 2D, incorporating the finite-range effects of the interatomic potential. Employing the framework of effective field theory, we derive the analytical expressions for both the ground state energy and quantum depletion. The introduction of the lattice induces a transition from a 3D to a quasi-2D regime. In particular, we systematically analyze the asymptotic behaviors of both the 2D and 3D aspects of the model system, with a specific focus on the nonuniversal effects on the EOS arising from finite-range interactions. The nonuniversal effects proposed in this study along the dimensional crossover represent a significant stride toward unraveling the intricate interplay between dimensionality and quantum fluctuations.","sentences":["Equation of state (EOS) for a pure two-dimensional (2D)","Bose gas exhibits a logarithmic dependence on the s-wave scattering length [L. Salasnich, Phys. Rev. Lett.","118, 130402 (2017)].","The pronounced disparity between the EOS of a 2D Bose gas and its 3D counterpart underscores the significance of exploring the dimensional crossover between these two distinct dimensions.","In this work, we are motivated to deduce nonuniversal corrections to EOS for an optically trapped Bose gas along the dimensional crossover from 3D to 2D, incorporating the finite-range effects of the interatomic potential.","Employing the framework of effective field theory, we derive the analytical expressions for both the ground state energy and quantum depletion.","The introduction of the lattice induces a transition from a 3D to a quasi-2D regime.","In particular, we systematically analyze the asymptotic behaviors of both the 2D and 3D aspects of the model system, with a specific focus on the nonuniversal effects on the EOS arising from finite-range interactions.","The nonuniversal effects proposed in this study along the dimensional crossover represent a significant stride toward unraveling the intricate interplay between dimensionality and quantum fluctuations."],"url":"http://arxiv.org/abs/2402.04703v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-07 09:46:41","title":"Frequency-Modulation Mode-Locked Laser with GHz Spectral Width Tunable in the 2-3 um Region","abstract":"A narrow-bandwidth actively mode-locked laser using a Cr:ZnS gain medium has been successfully demonstrated. A free-space electro-optic phase modulator is employed in the solid-state laser resonator to achieve frequency-modulation (FM) mode-locking, which achieves a narrow spectral width of ~1 GHz and a pulse duration of ~500 ps over a wide tuning range of 1947-2445 nm. The operation frequency of the modulator determines the repetition rate of the mode-locked pulse train and can stabilize it to millihertz-level without any additional feedback loop systems. We also study the theoretical expression of pulse duration and spectral width in a FM mode-locking in a laser cavity that contains considerable group-delay dispersion. The results indicates that larger intracavity dispersion can only stabilize the laser operation by avoiding mode switching, but also narrow the spectral width and increase the pulse duration. The proposed laser features a narrow spectral width at a desired mid-infrared wavelength and a comb-like spectral structure with stabilized longitudinal mode spacing, providing a powerful tool for sensing and control of molecules.","sentences":["A narrow-bandwidth actively mode-locked laser using a Cr:ZnS gain medium has been successfully demonstrated.","A free-space electro-optic phase modulator is employed in the solid-state laser resonator to achieve frequency-modulation (FM) mode-locking, which achieves a narrow spectral width of ~1 GHz and a pulse duration of ~500 ps over a wide tuning range of 1947-2445 nm.","The operation frequency of the modulator determines the repetition rate of the mode-locked pulse train and can stabilize it to millihertz-level without any additional feedback loop systems.","We also study the theoretical expression of pulse duration and spectral width in a FM mode-locking in a laser cavity that contains considerable group-delay dispersion.","The results indicates that larger intracavity dispersion can only stabilize the laser operation by avoiding mode switching, but also narrow the spectral width and increase the pulse duration.","The proposed laser features a narrow spectral width at a desired mid-infrared wavelength and a comb-like spectral structure with stabilized longitudinal mode spacing, providing a powerful tool for sensing and control of molecules."],"url":"http://arxiv.org/abs/2402.04702v1","category":"physics.optics"}
{"created":"2024-02-07 09:40:43","title":"Exhaustive Classification and Quantification of Coupling Modes in Power Systems with Power Electronics","abstract":"Due to the energy transition, today's electrical networks include synchronous machines and inverter-based resources interfacing renewable energies such as wind turbines, solar panels, and Battery Energy Storage Systems to the grid. In such systems, interactions known as coupling modes or dynamic interactions, between synchronous machines and inverter-based resources may arise. This paper conducts a clear and exhaustive study on a proposed benchmark, in order to analyze, quantify and classify these new types of modes. Detailed models representing electromagnetic transient phenomena are developed and linearized, then used for conducting modal analysis to fully characterize the small-signal stability of the system. Also, a sensitivity analysis is presented to evaluate the impact of key parameters on the detected modes of oscillation. Besides the exhaustive classification of the possible coupling modes, the proposed benchmark and methodology can be used to study any given power system in a minimal order modeling. The case of a fully detailed power grid based on the IEEE 39 bus system was studied as an illustrative example.","sentences":["Due to the energy transition, today's electrical networks include synchronous machines and inverter-based resources interfacing renewable energies such as wind turbines, solar panels, and Battery Energy Storage Systems to the grid.","In such systems, interactions known as coupling modes or dynamic interactions, between synchronous machines and inverter-based resources may arise.","This paper conducts a clear and exhaustive study on a proposed benchmark, in order to analyze, quantify and classify these new types of modes.","Detailed models representing electromagnetic transient phenomena are developed and linearized, then used for conducting modal analysis to fully characterize the small-signal stability of the system.","Also, a sensitivity analysis is presented to evaluate the impact of key parameters on the detected modes of oscillation.","Besides the exhaustive classification of the possible coupling modes, the proposed benchmark and methodology can be used to study any given power system in a minimal order modeling.","The case of a fully detailed power grid based on the IEEE 39 bus system was studied as an illustrative example."],"url":"http://arxiv.org/abs/2402.04701v1","category":"eess.SY"}
{"created":"2024-02-07 09:34:39","title":"A duality method for mean-field limits with singular interactions","abstract":"We introduce a new approach to justify mean-field limits for first-and second-order particle systems with singular interactions. It is based on a duality approach combined with the analysis of linearized dual correlations, and it allows to cover for the first time arbitrary square-integrable interaction forces at possibly vanishing temperature. In case of first-order systems, it allows to recover in particular the mean-field limit to the 2d Euler and Navier-Stokes equations. We postpone to a forthcoming work the development of quantitative estimates and the extension to more singular interactions.","sentences":["We introduce a new approach to justify mean-field limits for first-and second-order particle systems with singular interactions.","It is based on a duality approach combined with the analysis of linearized dual correlations, and it allows to cover for the first time arbitrary square-integrable interaction forces at possibly vanishing temperature.","In case of first-order systems, it allows to recover in particular the mean-field limit to the 2d Euler and Navier-Stokes equations.","We postpone to a forthcoming work the development of quantitative estimates and the extension to more singular interactions."],"url":"http://arxiv.org/abs/2402.04695v1","category":"math.AP"}
{"created":"2024-02-07 09:33:48","title":"The impact of non-equilibrium plasma distributions on solar wind measurements by Vigil's Plasma Analyser","abstract":"In order to protect society from space weather impacts, we must monitor space weather and obtain early warnings for extreme events if possible. For this purpose, the European Space Agency is currently preparing to launch the Vigil mission towards the end of this decade as a space-weather monitor at the fifth Lagrange point of the Sun--Earth system. Vigil will carry, amongst other instruments, the Plasma Analyzer (PLA) to provide quasi-continuous measurements of solar wind ions. We model the performance of the PLA instrument, considering typical solar wind plasma conditions, to compare the expected observations of PLA with the assumed input conditions of the solar wind. We evaluate the instrument performance under realistic, non-equilibrium plasma conditions, accounting for temperature anisotropies, proton beams, and the contributions from drifting $\\alpha$-particles. We examine the accuracy of the instrument's performance over a range of input solar wind moments. We identify sources of potential errors due to non-equilibrium plasma conditions and link these to instrument characteristics such as its angular and energy resolution and its field of view. We demonstrate the limitations of the instrument and potential improvements such as applying ground-based fitting techniques to obtain more accurate measurements of the solar wind even under non-equilibrium plasma conditions. The use of ground processing of plasma moments instead of on-board processing is crucial for the extraction of reliable measurements.","sentences":["In order to protect society from space weather impacts, we must monitor space weather and obtain early warnings for extreme events if possible.","For this purpose, the European Space Agency is currently preparing to launch the Vigil mission towards the end of this decade as a space-weather monitor at the fifth Lagrange point of the Sun--Earth system.","Vigil will carry, amongst other instruments, the Plasma Analyzer (PLA) to provide quasi-continuous measurements of solar wind ions.","We model the performance of the PLA instrument, considering typical solar wind plasma conditions, to compare the expected observations of PLA with the assumed input conditions of the solar wind.","We evaluate the instrument performance under realistic, non-equilibrium plasma conditions, accounting for temperature anisotropies, proton beams, and the contributions from drifting $\\alpha$-particles.","We examine the accuracy of the instrument's performance over a range of input solar wind moments.","We identify sources of potential errors due to non-equilibrium plasma conditions and link these to instrument characteristics such as its angular and energy resolution and its field of view.","We demonstrate the limitations of the instrument and potential improvements such as applying ground-based fitting techniques to obtain more accurate measurements of the solar wind even under non-equilibrium plasma conditions.","The use of ground processing of plasma moments instead of on-board processing is crucial for the extraction of reliable measurements."],"url":"http://arxiv.org/abs/2402.04694v1","category":"physics.space-ph"}
{"created":"2024-02-07 09:20:46","title":"Sub-Lorentzian extremals defined by an antinorm","abstract":"We consider a left-invariant (sub-)Lorentzian structure on a Lie group. We assume that this structure is defined by a closed convex salient cone in the corresponding Lie algebra and a continuous antinorm associated with this cone. We derive the Hamiltonian system for (sub-)Lorentzian extremals and give conditions under that normal extremal trajectories keep their causal type. Tangent vectors of abnormal extremal trajectories are either light-like or tangent vectors of sub-Riemannian extremal trajectories for the sub-Riemannian distribution spanned by the cone.","sentences":["We consider a left-invariant (sub-)Lorentzian structure on a Lie group.","We assume that this structure is defined by a closed convex salient cone in the corresponding Lie algebra and a continuous antinorm associated with this cone.","We derive the Hamiltonian system for (sub-)Lorentzian extremals and give conditions under that normal extremal trajectories keep their causal type.","Tangent vectors of abnormal extremal trajectories are either light-like or tangent vectors of sub-Riemannian extremal trajectories for the sub-Riemannian distribution spanned by the cone."],"url":"http://arxiv.org/abs/2402.04687v1","category":"math.OC"}
{"created":"2024-02-07 09:19:21","title":"Robust Symbol-Level Precoding for Massive MIMO Communication Under Channel Aging","abstract":"This paper investigates the robust design of symbol-level precoding (SLP) for multiuser multiple-input multiple-output (MIMO) downlink transmission with imperfect channel state information (CSI) caused by channel aging. By utilizing the a posteriori channel model based on the widely adopted jointly correlated channel model, the imperfect CSI is modeled as the statistical CSI incorporating the channel mean and channel variance information with spatial correlation. With the signal model in the presence of channel aging, we formulate the signal-to-noise-plus-interference ratio (SINR) balancing and minimum mean square error (MMSE) problems for robust SLP design. The former targets to maximize the minimum SINR across users, while the latter minimizes the mean square error between the received signal and the target constellation point. When it comes to massive MIMO scenarios, the increment in the number of antennas poses a computational complexity challenge, limiting the deployment of SLP schemes. To address such a challenge, we simplify the objective function of the SINR balancing problem and further derive a closed-form SLP scheme. Besides, by approximating the matrix involved in the computation, we modify the proposed algorithm and develop an MMSE-based SLP scheme with lower computation complexity. Simulation results confirm the superiority of the proposed schemes over the state-of-the-art SLP schemes.","sentences":["This paper investigates the robust design of symbol-level precoding (SLP) for multiuser multiple-input multiple-output (MIMO) downlink transmission with imperfect channel state information (CSI) caused by channel aging.","By utilizing the a posteriori channel model based on the widely adopted jointly correlated channel model, the imperfect CSI is modeled as the statistical CSI incorporating the channel mean and channel variance information with spatial correlation.","With the signal model in the presence of channel aging, we formulate the signal-to-noise-plus-interference ratio (SINR) balancing and minimum mean square error (MMSE) problems for robust SLP design.","The former targets to maximize the minimum SINR across users, while the latter minimizes the mean square error between the received signal and the target constellation point.","When it comes to massive MIMO scenarios, the increment in the number of antennas poses a computational complexity challenge, limiting the deployment of SLP schemes.","To address such a challenge, we simplify the objective function of the SINR balancing problem and further derive a closed-form SLP scheme.","Besides, by approximating the matrix involved in the computation, we modify the proposed algorithm and develop an MMSE-based SLP scheme with lower computation complexity.","Simulation results confirm the superiority of the proposed schemes over the state-of-the-art SLP schemes."],"url":"http://arxiv.org/abs/2402.04685v1","category":"eess.SP"}
{"created":"2024-02-07 08:53:59","title":"Stochastic Schr\u00f6dinger-Korteweg de Vries systems driven by multiplicative noises","abstract":"In this paper, we consider the well-posedness of stochastic S-KdV in $H_x^1\\times H_x^1$ driven by multiplicative noises. We first develop the bilinear and trilinear estimates of the Bourgain space with $b\\in(0,1/2)$ to get the local well-posedness. Then, using sequences of approximation equations with localized nonlinear terms cut-off in both the physical and the Fourier space, we can get a priori estimate of the initial system. The proof of the short time global well-posedness interprets that our method, which can handle the global well-posedness with complicate conversation laws such as the $H_x^1\\times H_x^1$ case, is essentially different from existing methods.","sentences":["In this paper, we consider the well-posedness of stochastic S-KdV in $H_x^1\\times H_x^1$ driven by multiplicative noises.","We first develop the bilinear and trilinear estimates of the Bourgain space with $b\\in(0,1/2)$ to get the local well-posedness.","Then, using sequences of approximation equations with localized nonlinear terms cut-off in both the physical and the Fourier space, we can get a priori estimate of the initial system.","The proof of the short time global well-posedness interprets that our method, which can handle the global well-posedness with complicate conversation laws such as the $H_x^1\\times H_x^1$ case, is essentially different from existing methods."],"url":"http://arxiv.org/abs/2402.04669v1","category":"math.PR"}
{"created":"2024-02-07 08:53:34","title":"A Comparative Study of Sensitivity Computations in ESDIRK-Based Optimal Control Problems","abstract":"In this paper, we compare the impact of iterated and direct approaches to sensitivity computation in fixed-step explicit singly diagonally-implicit Runge-Kutta (ESDIRK) methods when applied to optimal control problems (OCPs). We use the principle of internal numerical differentiation (IND) strictly for the iterated approach, i.e., reusing the iteration matrix factorizations, the number of Newton-type iterations, and Newton iterates, to compute the sensitivities. The direct method computes the sensitivities without using the Newton schemes. We compare the impact of the iterated and direct sensitivity computations in OCPs for the quadruple tank system. We benchmark the iterated and direct approaches with a base case. This base case is an OCP that applies an ESDIRK method that refactorizes the iteration matrix in every Newton iteration and uses a direct approach for sensitivity computations. In these OCPs, we vary the number of integration steps between control intervals and we evaluate the performance based on the number of SQP and QPs iterations, KKT violations, and the total number of function evaluations, Jacobian updates, and iteration matrix factorizations. The results indicate that the iterated approach outperforms the direct approach but yields similar performance to the base case.","sentences":["In this paper, we compare the impact of iterated and direct approaches to sensitivity computation in fixed-step explicit singly diagonally-implicit Runge-Kutta (ESDIRK) methods when applied to optimal control problems (OCPs).","We use the principle of internal numerical differentiation (IND) strictly for the iterated approach, i.e., reusing the iteration matrix factorizations, the number of Newton-type iterations, and Newton iterates, to compute the sensitivities.","The direct method computes the sensitivities without using the Newton schemes.","We compare the impact of the iterated and direct sensitivity computations in OCPs for the quadruple tank system.","We benchmark the iterated and direct approaches with a base case.","This base case is an OCP that applies an ESDIRK method that refactorizes the iteration matrix in every Newton iteration and uses a direct approach for sensitivity computations.","In these OCPs, we vary the number of integration steps between control intervals and we evaluate the performance based on the number of SQP and QPs iterations, KKT violations, and the total number of function evaluations, Jacobian updates, and iteration matrix factorizations.","The results indicate that the iterated approach outperforms the direct approach but yields similar performance to the base case."],"url":"http://arxiv.org/abs/2402.04667v1","category":"eess.SY"}
{"created":"2024-02-07 08:48:21","title":"On the Formation of Double Neutron Stars in the Milky Way: Influence of Key Parameters","abstract":"The detection of gravitational wave events has stimulated theoretical modeling of the formation and evolution of double compact objects (DCOs). However, even for the most studied isolated binary evolution channel, there exist large uncertainties in the input parameters and treatments of the binary evolution process. So far, double neutron stars (DNSs) are the only DCOs for which direct observations are available through traditional electromagnetic astronomy. In this work, we adopt a population synthesis method to investigate the formation and evolution of Galactic DNSs. We construct 324 models for the formation of Galactic DNSs, taking into account various possible combinations of critical input parameters and processes such as mass transfer efficiency, supernova type, common envelope efficiency, neutron star kick velocity, and pulsar selection effect. We employ Bayesian analysis to evaluate the adopted models by comparing with observations. We also compare the expected DNS merger rate in the Galaxy with that inferred from the known Galactic population of Pulsar-NS systems. Based on these analyses we derive favorable range of the aforementioned key parameters.","sentences":["The detection of gravitational wave events has stimulated theoretical modeling of the formation and evolution of double compact objects (DCOs).","However, even for the most studied isolated binary evolution channel, there exist large uncertainties in the input parameters and treatments of the binary evolution process.","So far, double neutron stars (DNSs) are the only DCOs for which direct observations are available through traditional electromagnetic astronomy.","In this work, we adopt a population synthesis method to investigate the formation and evolution of Galactic DNSs.","We construct 324 models for the formation of Galactic DNSs, taking into account various possible combinations of critical input parameters and processes such as mass transfer efficiency, supernova type, common envelope efficiency, neutron star kick velocity, and pulsar selection effect.","We employ Bayesian analysis to evaluate the adopted models by comparing with observations.","We also compare the expected DNS merger rate in the Galaxy with that inferred from the known Galactic population of Pulsar-NS systems.","Based on these analyses we derive favorable range of the aforementioned key parameters."],"url":"http://arxiv.org/abs/2402.04658v1","category":"astro-ph.HE"}
{"created":"2024-02-07 07:54:12","title":"Macroscopic Magnetic Dynamics","abstract":"Ferromagnetic metals and spin-polarized $^{3}$He are spin 1/2 systems with the same macroscopic symmetry, and thus should have macroscopic magnetic dynamics with the same structure. Using Onsager's irreversible thermodynamics, we develop a theory for these systems that contains two relaxation times (one for the magnetization $\\vec{M}$ and the other for the spin current $\\vec{J}_{i}$), a magnetic compressibility, and a mean-field parameter. Currently spintronics data on metallic ferromagnets are analyzed using a complex decay length from a theory employing a diffusion constant, a lifetime, and a mean-field parameter. The present theory leads to a complex decay length with the same structure. On neglecting decay of $\\vec{M}$, the present theory applies to liquids and gases. For macroscopic equations the particle statistics is not relevant, so the theory also applies to bosons. The theory predicts a longitudinal spin wave whose velocity we estimate for liquid $^{3}$He and for paramagnetic metals; but such a wave should also occur for ferromagnets and for gases.","sentences":["Ferromagnetic metals and spin-polarized $^{3}$He are spin 1/2 systems with the same macroscopic symmetry, and thus should have macroscopic magnetic dynamics with the same structure.","Using Onsager's irreversible thermodynamics, we develop a theory for these systems that contains two relaxation times (one for the magnetization $\\vec{M}$ and the other for the spin current $\\vec{J}_{i}$), a magnetic compressibility, and a mean-field parameter.","Currently spintronics data on metallic ferromagnets are analyzed using a complex decay length from a theory employing a diffusion constant, a lifetime, and a mean-field parameter.","The present theory leads to a complex decay length with the same structure.","On neglecting decay of $\\vec{M}$, the present theory applies to liquids and gases.","For macroscopic equations the particle statistics is not relevant, so the theory also applies to bosons.","The theory predicts a longitudinal spin wave whose velocity we estimate for liquid $^{3}$He and for paramagnetic metals; but such a wave should also occur for ferromagnets and for gases."],"url":"http://arxiv.org/abs/2402.04639v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 07:51:27","title":"An efficient unconditional energy stable scheme for the simulation of droplet formation","abstract":"We have developed an efficient and unconditionally energy-stable method for simulating droplet formation dynamics. Our approach involves a novel time-marching scheme based on the scalar auxiliary variable technique, specifically designed for solving the Cahn-Hilliard-Navier-Stokes phase field model with variable density and viscosity. We have successfully applied this method to simulate droplet formation in scenarios where a Newtonian fluid is injected through a vertical tube into another immiscible Newtonian fluid. To tackle the challenges posed by nonhomogeneous Dirichlet boundary conditions at the tube entrance, we have introduced additional nonlocal auxiliary variables and associated ordinary differential equations. These additions effectively eliminate the influence of boundary terms. Moreover, we have incorporated stabilization terms into the scheme to enhance its numerical effectiveness. Notably, our resulting scheme is fully decoupled, requiring the solution of only linear systems at each time step. We have also demonstrated the energy decaying property of the scheme, with suitable modifications. To assess the accuracy and stability of our algorithm, we have conducted extensive numerical simulations. Additionally, we have examined the dynamics of droplet formation and explored the impact of dimensionless parameters on the process. Overall, our work presents a refined method for simulating droplet formation dynamics, offering improved efficiency, energy stability, and accuracy.","sentences":["We have developed an efficient and unconditionally energy-stable method for simulating droplet formation dynamics.","Our approach involves a novel time-marching scheme based on the scalar auxiliary variable technique, specifically designed for solving the Cahn-Hilliard-Navier-Stokes phase field model with variable density and viscosity.","We have successfully applied this method to simulate droplet formation in scenarios where a Newtonian fluid is injected through a vertical tube into another immiscible Newtonian fluid.","To tackle the challenges posed by nonhomogeneous Dirichlet boundary conditions at the tube entrance, we have introduced additional nonlocal auxiliary variables and associated ordinary differential equations.","These additions effectively eliminate the influence of boundary terms.","Moreover, we have incorporated stabilization terms into the scheme to enhance its numerical effectiveness.","Notably, our resulting scheme is fully decoupled, requiring the solution of only linear systems at each time step.","We have also demonstrated the energy decaying property of the scheme, with suitable modifications.","To assess the accuracy and stability of our algorithm, we have conducted extensive numerical simulations.","Additionally, we have examined the dynamics of droplet formation and explored the impact of dimensionless parameters on the process.","Overall, our work presents a refined method for simulating droplet formation dynamics, offering improved efficiency, energy stability, and accuracy."],"url":"http://arxiv.org/abs/2402.04638v1","category":"math.NA"}
{"created":"2024-02-07 07:39:27","title":"TransLLaMa: LLM-based Simultaneous Translation System","abstract":"Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \"wait\" token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.","sentences":["Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning.","Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers.","This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special \"wait\" token.","This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines.","We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems."],"url":"http://arxiv.org/abs/2402.04636v1","category":"cs.CL"}
{"created":"2024-02-07 07:24:14","title":"Distinguishing pure and thermal states by Landauer's principle in open systems","abstract":"Starting from Polchinski's thought experiment on how to distinguish between pure and thermal states, we construct a specific system to study the interaction between qubit and cavity quantum field theory (QFT) in order to provide a more operational point of view. Without imposing any restrictions on the initial states of qubit and cavity QFT, we compute the evolution of the system order by order by the perturbation method. We choose Landauer's principle, an important bound in quantum computation and quantum measurement, as the basis for the determination of the thermal state. By backtracking the initial state form, we obtain the conditions that must be satisfied by the cavity QFT: the expectation value of the annihilation operator should be zero, and the expectation value of the particle number operator should satisfy the Bose-Einstein distribution. We also discuss the difference between the thermal state and a possible alternative to the thermal state: the canonical thermal pure quantum (CTPQ) state.","sentences":["Starting from Polchinski's thought experiment on how to distinguish between pure and thermal states, we construct a specific system to study the interaction between qubit and cavity quantum field theory (QFT) in order to provide a more operational point of view.","Without imposing any restrictions on the initial states of qubit and cavity QFT, we compute the evolution of the system order by order by the perturbation method.","We choose Landauer's principle, an important bound in quantum computation and quantum measurement, as the basis for the determination of the thermal state.","By backtracking the initial state form, we obtain the conditions that must be satisfied by the cavity QFT: the expectation value of the annihilation operator should be zero, and the expectation value of the particle number operator should satisfy the Bose-Einstein distribution.","We also discuss the difference between the thermal state and a possible alternative to the thermal state: the canonical thermal pure quantum (CTPQ) state."],"url":"http://arxiv.org/abs/2402.04628v1","category":"quant-ph"}
{"created":"2024-02-07 07:19:47","title":"Breaking surface plasmon excitation constraint via surface spin waves","abstract":"Surface plasmons in two-dimensional (2D) electron systems have attracted great attention for their promising light-matter applications. However, the excitation of a surface plasmon, in particular, transverse-electric (TE) surface plasmon, remains an outstanding challenge due to the difficulty to conserve energy and momentum simultaneously in the normal 2D materials. Here we show that the TE surface plasmons ranging from gigahertz to terahertz regime can be effectively excited and manipulated in a hybrid dielectric, 2D material and magnet structure. The essential physics is that the surface spin wave supplements an additional freedom of surface plasmon excitation and thus greatly enhances the electric field in the 2D medium. Based on widely-used magnetic materials like yttrium iron garnet (YIG) and manganese difluoride ($\\mathrm{MnF}_2$), we further show that the plasmon excitation manifests itself as a measurable dip in the reflection spectrum of the hybrid system while the dip position and the dip depth can be well controlled by the electric gating on the 2D layer and an external magnetic field. Our findings should bridge the fields of low-dimensional physics, plasmonics and spintronics and open a novel route to integrate plasmonic and spintronic devices.","sentences":["Surface plasmons in two-dimensional (2D) electron systems have attracted great attention for their promising light-matter applications.","However, the excitation of a surface plasmon, in particular, transverse-electric (TE) surface plasmon, remains an outstanding challenge due to the difficulty to conserve energy and momentum simultaneously in the normal 2D materials.","Here we show that the TE surface plasmons ranging from gigahertz to terahertz regime can be effectively excited and manipulated in a hybrid dielectric, 2D material and magnet structure.","The essential physics is that the surface spin wave supplements an additional freedom of surface plasmon excitation and thus greatly enhances the electric field in the 2D medium.","Based on widely-used magnetic materials like yttrium iron garnet (YIG) and manganese difluoride ($\\mathrm{MnF}_2$), we further show that the plasmon excitation manifests itself as a measurable dip in the reflection spectrum of the hybrid system while the dip position and the dip depth can be well controlled by the electric gating on the 2D layer and an external magnetic field.","Our findings should bridge the fields of low-dimensional physics, plasmonics and spintronics and open a novel route to integrate plasmonic and spintronic devices."],"url":"http://arxiv.org/abs/2402.04626v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 07:05:39","title":"Discontinuous harvesting policy in a Filippov system involving prey refuge","abstract":"In this article, a non-smooth predator-prey dynamical system is considered. Here, we discuss about sustainable harvesting in a Filippov predator-prey system, which can produce yield and at the same time prevent over-exploitation of bioresources. The local and global stability analysis of the two subsystems, with and without harvesting, are studied. Furthermore, for the Filippov system, we have performed bifurcation analysis for several key parameters like predation rate, threshold quantity and prey refuge. Some local sliding bifurcations are also observed for the system. Numerical simulations are presented to illustrate the dynamical behaviour of the system.","sentences":["In this article, a non-smooth predator-prey dynamical system is considered.","Here, we discuss about sustainable harvesting in a Filippov predator-prey system, which can produce yield and at the same time prevent over-exploitation of bioresources.","The local and global stability analysis of the two subsystems, with and without harvesting, are studied.","Furthermore, for the Filippov system, we have performed bifurcation analysis for several key parameters like predation rate, threshold quantity and prey refuge.","Some local sliding bifurcations are also observed for the system.","Numerical simulations are presented to illustrate the dynamical behaviour of the system."],"url":"http://arxiv.org/abs/2402.04619v1","category":"math.DS"}
{"created":"2024-02-07 06:28:08","title":"An inevitably aging world -- Analysis on the evolutionary pattern of age structure in 200 countries","abstract":"Ignoring the differences between countries, human reproductive and dispersal behaviors can be described by some standardized models, so whether there is a universal law of population growth hidden in the abundant and unstructured data from various countries remains unclear. The age-specific population data constitute a three-dimensional tensor containing more comprehensive information. The existing literature often describes the characteristics of global or regional population evolution by subregion aggregation and statistical analysis, which makes it challenging to identify the underlying rules by ignoring national or structural details. Statistical physics can be used to summarize the macro characteristics and evolution laws of complex systems based on the attributes and motions of masses of individuals by decomposing high-dimensional tensors. Specifically, it can be used to assess the evolution of age structure in various countries over the past approximately 70 years, rather than simply focusing on the regions where aging has become apparent. It provides a universal scheme for the growing elderly and working age populations, indicating that the demographics on all continents are inevitably moving towards an aging population, including the current \"young\" continents of Africa, and Asia, South America with a recent \"demographic dividend\". It is a force derived from the \"life cycle\", and most countries have been unable to avoid this universal evolutionary path in the foreseeable future.","sentences":["Ignoring the differences between countries, human reproductive and dispersal behaviors can be described by some standardized models, so whether there is a universal law of population growth hidden in the abundant and unstructured data from various countries remains unclear.","The age-specific population data constitute a three-dimensional tensor containing more comprehensive information.","The existing literature often describes the characteristics of global or regional population evolution by subregion aggregation and statistical analysis, which makes it challenging to identify the underlying rules by ignoring national or structural details.","Statistical physics can be used to summarize the macro characteristics and evolution laws of complex systems based on the attributes and motions of masses of individuals by decomposing high-dimensional tensors.","Specifically, it can be used to assess the evolution of age structure in various countries over the past approximately 70 years, rather than simply focusing on the regions where aging has become apparent.","It provides a universal scheme for the growing elderly and working age populations, indicating that the demographics on all continents are inevitably moving towards an aging population, including the current \"young\" continents of Africa, and Asia, South America with a recent \"demographic dividend\".","It is a force derived from the \"life cycle\", and most countries have been unable to avoid this universal evolutionary path in the foreseeable future."],"url":"http://arxiv.org/abs/2402.04612v1","category":"physics.soc-ph"}
{"created":"2024-02-07 05:58:53","title":"The passage among the subcategories of weakly approximable triangulated categories","abstract":"In this article we prove that all the inclusions between the 'classical' and naturally defined full triangulated subcategories of a weakly approximable triangulated category are intrinsic (in one case under a technical condition). This extends all the existing results about subcategories of weakly approximable triangulated categories. Together with a forthcoming paper about uniqueness of enhancements, our result allows us to generalize a celebrated theorem by Rickard which asserts that if $R$ and $S$ are left coherent rings, then a derived equivalence of $R$ and $S$ is \"independent of the decorations\". That is, if $D^?(R\\text{-}\\square)$ and $D^?(S\\text{-}\\square)$ are equivalent as triangulated categories for some choice of decorations $?$ and $\\square$, then they are equivalent for every choice of decorations. But our theorem is much more general, and applies also to quasi-compact and quasi-separated schemes -- even to the relative version, in which the derived categories consist of complexes with cohomology supported on a given closed subscheme with quasi-compact complement.","sentences":["In this article we prove that all the inclusions between the 'classical' and naturally defined full triangulated subcategories of a weakly approximable triangulated category are intrinsic (in one case under a technical condition).","This extends all the existing results about subcategories of weakly approximable triangulated categories.","Together with a forthcoming paper about uniqueness of enhancements, our result allows us to generalize a celebrated theorem by Rickard which asserts that if $R$ and $S$ are left coherent rings, then a derived equivalence of $R$ and $S$ is \"independent of the decorations\".","That is, if $D^?(R\\text{-}\\square)$ and $D^?(S\\text{-}\\square)$ are equivalent as triangulated categories for some choice of decorations $?$ and $\\square$, then they are equivalent for every choice of decorations.","But our theorem is much more general, and applies also to quasi-compact and quasi-separated schemes -- even to the relative version, in which the derived categories consist of complexes with cohomology supported on a given closed subscheme with quasi-compact complement."],"url":"http://arxiv.org/abs/2402.04605v1","category":"math.AG"}
{"created":"2024-02-07 05:58:18","title":"Persistent anomaly in dynamical quantum phase transition in long-range non-Hermitian $p$-wave Kitaev chian","abstract":"Considering a non-Hermitian version of $p$-wave Kitaev chain in the presence of additional second nearest neighbour tunneling, we study dynamical quantum phase transition (DQPT) which accounts for the vanishing Loschmidt amplitude. The locus of the Fisher's zero traces a continuous path on the complex time plane for the Hermitian case while it becomes discontinuous for non-Hermitian cases. This further leads to the half-unit jumps in the winding number characterizing a dynamical topological aspect of DQPT for non-Hermitian Hamiltonian. Uncovering the interplay between non-Hermiticity and long-range tunneling, we find these features to be universally present irrespective of the additional second nearest neighbour tunneling terms as long as non-Hermiticity is preserved.","sentences":["Considering a non-Hermitian version of $p$-wave Kitaev chain in the presence of additional second nearest neighbour tunneling, we study dynamical quantum phase transition (DQPT) which accounts for the vanishing Loschmidt amplitude.","The locus of the Fisher's zero traces a continuous path on the complex time plane for the Hermitian case while it becomes discontinuous for non-Hermitian cases.","This further leads to the half-unit jumps in the winding number characterizing a dynamical topological aspect of DQPT for non-Hermitian Hamiltonian.","Uncovering the interplay between non-Hermiticity and long-range tunneling, we find these features to be universally present irrespective of the additional second nearest neighbour tunneling terms as long as non-Hermiticity is preserved."],"url":"http://arxiv.org/abs/2402.04603v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 05:36:06","title":"Ransomware Detection Dynamics: Insights and Implications","abstract":"The rise of ransomware attacks has necessitated the development of effective strategies for identifying and mitigating these threats. This research investigates the utilization of a feature selection algorithm for distinguishing ransomware-related and benign transactions in both Bitcoin (BTC) and United States Dollar (USD). Leveraging the UGRansome dataset, a comprehensive repository of ransomware related BTC and USD transactions, we propose a set of novel features designed to capture the distinct characteristics of ransomware activity within the cryptocurrency ecosystem. These features encompass transaction metadata, ransom analysis, and behavioral patterns, offering a multifaceted view of ransomware-related financial transactions. Through rigorous experimentation and evaluation, we demonstrate the effectiveness of our feature set in accurately extracting BTC and USD transactions, thereby aiding in the early detection and prevention of ransomware-related financial flows. We introduce a Ransomware Feature Selection Algorithm (RFSA) based on Gini Impurity and Mutual Information (MI) for selecting crucial ransomware features from the UGRansome dataset. Insights from the visualization highlight the potential of Gini Impurity and MI-based feature selection to enhance ransomware detection systems by effectively discriminating between ransomware classes. The analysis reveals that approximately 68% of ransomware incidents involve BTC transactions within the range of 1.46 to 2.56, with an average of 2.01 BTC transactions per attack. The findings emphasize the dynamic and adaptable nature of ransomware demands, suggesting that there is no fixed amount for specific cyberattacks, highlighting the evolving landscape of ransomware threats.","sentences":["The rise of ransomware attacks has necessitated the development of effective strategies for identifying and mitigating these threats.","This research investigates the utilization of a feature selection algorithm for distinguishing ransomware-related and benign transactions in both Bitcoin (BTC) and United States Dollar (USD).","Leveraging the UGRansome dataset, a comprehensive repository of ransomware related BTC and USD transactions, we propose a set of novel features designed to capture the distinct characteristics of ransomware activity within the cryptocurrency ecosystem.","These features encompass transaction metadata, ransom analysis, and behavioral patterns, offering a multifaceted view of ransomware-related financial transactions.","Through rigorous experimentation and evaluation, we demonstrate the effectiveness of our feature set in accurately extracting BTC and USD transactions, thereby aiding in the early detection and prevention of ransomware-related financial flows.","We introduce a Ransomware Feature Selection Algorithm (RFSA) based on Gini Impurity and Mutual Information (MI) for selecting crucial ransomware features from the UGRansome dataset.","Insights from the visualization highlight the potential of Gini Impurity and MI-based feature selection to enhance ransomware detection systems by effectively discriminating between ransomware classes.","The analysis reveals that approximately 68% of ransomware incidents involve BTC transactions within the range of 1.46 to 2.56, with an average of 2.01 BTC transactions per attack.","The findings emphasize the dynamic and adaptable nature of ransomware demands, suggesting that there is no fixed amount for specific cyberattacks, highlighting the evolving landscape of ransomware threats."],"url":"http://arxiv.org/abs/2402.04594v1","category":"cs.CR"}
{"created":"2024-02-07 05:18:48","title":"Concurrent Strategies on Games with Algebras","abstract":"Probabilistic concurrent/distributed strategies have so far not been investigated thoroughly in the context of imperfect information, where the Player has only partial knowledge of the moves made by the Opponent. In a situation where the Player and Opponent can make concurrent moves according to the game, and the Player cannot see the move of the Opponent, the move of the Player should be probabilistically independent of the move of the Opponent. What has been achieved is showing a bijection between strategies on a game with algebra and strategies on a regular (albeit more complex) game. We also succeeded in showing the results holds with neutral events. However it is still unclear if a well-formed bicategory of concurrent games with algebras can be defined. Our attempts to compose these strategies while managing the added structure didn't pan out. Concerning the other classic extensions of concurrent games the first results we presented show promise of a more general usage of games with algebra.","sentences":["Probabilistic concurrent/distributed strategies have so far not been investigated thoroughly in the context of imperfect information, where the Player has only partial knowledge of the moves made by the Opponent.","In a situation where the Player and Opponent can make concurrent moves according to the game, and the Player cannot see the move of the Opponent, the move of the Player should be probabilistically independent of the move of the Opponent.","What has been achieved is showing a bijection between strategies on a game with algebra and strategies on a regular (albeit more complex) game.","We also succeeded in showing the results holds with neutral events.","However it is still unclear if a well-formed bicategory of concurrent games with algebras can be defined.","Our attempts to compose these strategies while managing the added structure didn't pan out.","Concerning the other classic extensions of concurrent games the first results we presented show promise of a more general usage of games with algebra."],"url":"http://arxiv.org/abs/2402.04590v1","category":"cs.GT"}
{"created":"2024-02-07 05:17:43","title":"Long Short-Term Memory for Early Warning Detection of Gravitational Waves","abstract":"The pre-merger detection of gravitational waves from the early inspiral phase of compact binary coalescence events would allow the observation of the earlier stages of the merger in the electromagnetic band. This would significantly impact multi-messenger astronomy, giving astronomers potential access to rich new information. Here, we introduce a proof-of-concept deep-learning-based approach to produce pre-merger early-warning alerts for binary black hole systems. We show the possibility of using a Long Short-Term Memory network trained on the whitened detector strain in the time domain to detect and classify compact binary events. In this work, we consider a single advanced Laser Interferometer Gravitational-Wave Observatory detector at design sensitivity and make approximate sensitivity and early warning capability comparisons with approximations to traditional matched filtering approaches. We find that our model is competitive in both aspects, and when applied to a simulated test dataset was able to produce an early alert up to four seconds before the merger.","sentences":["The pre-merger detection of gravitational waves from the early inspiral phase of compact binary coalescence events would allow the observation of the earlier stages of the merger in the electromagnetic band.","This would significantly impact multi-messenger astronomy, giving astronomers potential access to rich new information.","Here, we introduce a proof-of-concept deep-learning-based approach to produce pre-merger early-warning alerts for binary black hole systems.","We show the possibility of using a Long Short-Term Memory network trained on the whitened detector strain in the time domain to detect and classify compact binary events.","In this work, we consider a single advanced Laser Interferometer Gravitational-Wave Observatory detector at design sensitivity and make approximate sensitivity and early warning capability comparisons with approximations to traditional matched filtering approaches.","We find that our model is competitive in both aspects, and when applied to a simulated test dataset was able to produce an early alert up to four seconds before the merger."],"url":"http://arxiv.org/abs/2402.04589v1","category":"gr-qc"}
{"created":"2024-02-07 05:02:25","title":"A Physics-Informed Auto-Learning Framework for Developing Stochastic Conceptual Models for ENSO Diversity","abstract":"Understanding ENSO dynamics has tremendously improved over the past decades. However, one aspect still poorly understood or represented in conceptual models is the ENSO diversity in spatial pattern, peak intensity, and temporal evolution. In this paper, a physics-informed auto-learning framework is developed to derive ENSO stochastic conceptual models with varying degrees of freedom. The framework is computationally efficient and easy to apply. Once the state vector of the target model is set, causal inference is exploited to build the right-hand side of the equations based on a mathematical function library. Fundamentally different from standard nonlinear regression, the auto-learning framework provides a parsimonious model by retaining only terms that improve the dynamical consistency with observations. It can also identify crucial latent variables and provide physical explanations. Exploiting a realistic six-dimensional reference recharge oscillator-based ENSO model, a hierarchy of three- to six-dimensional models is derived using the auto-learning framework and is systematically validated by a unified set of validation criteria assessing the dynamical and statistical features of the ENSO diversity. It is shown that the minimum model characterizing ENSO diversity is four-dimensional, with three interannual variables describing the western Pacific thermocline depth, the eastern and central Pacific sea surface temperatures (SSTs), and one intraseasonal variable for westerly wind events. Without the intraseasonal variable, the resulting three-dimensional model underestimates extreme events and is too regular. The limited number of weak nonlinearities in the model are essential in reproducing the observed extreme El Ni\\~nos and nonlinear relationship between the eastern and western Pacific SSTs.","sentences":["Understanding ENSO dynamics has tremendously improved over the past decades.","However, one aspect still poorly understood or represented in conceptual models is the ENSO diversity in spatial pattern, peak intensity, and temporal evolution.","In this paper, a physics-informed auto-learning framework is developed to derive ENSO stochastic conceptual models with varying degrees of freedom.","The framework is computationally efficient and easy to apply.","Once the state vector of the target model is set, causal inference is exploited to build the right-hand side of the equations based on a mathematical function library.","Fundamentally different from standard nonlinear regression, the auto-learning framework provides a parsimonious model by retaining only terms that improve the dynamical consistency with observations.","It can also identify crucial latent variables and provide physical explanations.","Exploiting a realistic six-dimensional reference recharge oscillator-based ENSO model, a hierarchy of three- to six-dimensional models is derived using the auto-learning framework and is systematically validated by a unified set of validation criteria assessing the dynamical and statistical features of the ENSO diversity.","It is shown that the minimum model characterizing ENSO diversity is four-dimensional, with three interannual variables describing the western Pacific thermocline depth, the eastern and central Pacific sea surface temperatures (SSTs), and one intraseasonal variable for westerly wind events.","Without the intraseasonal variable, the resulting three-dimensional model underestimates extreme events and is too regular.","The limited number of weak nonlinearities in the model are essential in reproducing the observed extreme El Ni\\~nos and nonlinear relationship between the eastern and western Pacific SSTs."],"url":"http://arxiv.org/abs/2402.04585v1","category":"math.DS"}
{"created":"2024-02-07 04:33:44","title":"A note on complex hyperbolic lattices and strict hyperbolization","abstract":"We study the connection between the fundamental groups of complex hyperbolic manifolds and those of spaces arising from the (relative) strict hyperbolization process due to Charney--Davis and Davis--Januszkiewicz--Weinberger. Viewing a non-uniform lattice $\\Gamma$ in $\\text{PU}(n,1)$ as a relatively hyperbolic group with respect to its cusp subgroups in the usual way, we show that when $n\\geq 2$, $\\Gamma$ is not isomorphic to any relatively hyperbolic group arising from the relative strict hyperbolization process, via work of Lafont--Ruffoni. We also prove that a uniform lattice in $\\text{PU}(n,1)$ is not the fundamental group of a Charney-Davis strict hyperbolization when $n\\geq 2$, assuming the initial complex satisfies some mild conditions.","sentences":["We study the connection between the fundamental groups of complex hyperbolic manifolds and those of spaces arising from the (relative) strict hyperbolization process due to Charney--Davis and Davis--Januszkiewicz--Weinberger.","Viewing a non-uniform lattice $\\Gamma$ in $\\text{PU}(n,1)$ as a relatively hyperbolic group with respect to its cusp subgroups in the usual way, we show that when $n\\geq 2$, $\\Gamma$ is not isomorphic to any relatively hyperbolic group arising from the relative strict hyperbolization process, via work of Lafont--Ruffoni.","We also prove that a uniform lattice in $\\text{PU}(n,1)$ is not the fundamental group of a Charney-Davis strict hyperbolization when $n\\geq 2$, assuming the initial complex satisfies some mild conditions."],"url":"http://arxiv.org/abs/2402.04576v1","category":"math.GT"}
{"created":"2024-02-07 04:17:39","title":"Predicting the Dominant Formation Mechanism of Multi-Planetary Systems","abstract":"Most, if not all, sun-like stars host one or more planets, making multi-planetary systems commonplace in our galaxy. We utilize hundreds of multi-planet simulations to explore the origin of such systems, focusing on their orbital architecture. The first set of simulations assumes in-situ assembly of planetary embryos, while the second explores planetary migration. After applying observational biases to the simulations, we compare them to 250+ observed multi-planetary systems, including 13 systems with planets in the habitable zone. For all of the systems, we calculate two of the so-called statistical measures: the mass concentration ($S_{c}$) and orbital spacing ($S_{s}$). After analytic and empirical analyses, we find that the measures are related to first-order with a power law: $S_{c} \\sim S_{s}^\\beta$. The in-situ systems exhibit steeper power-law relations relative to the migration systems. We show that different formation scenarios cover different regions in the $S_{s} - S_{c}$ diagram with some overlap. Furthermore, we discover that observed systems with $S_{s} < 30$ are likely dominated by the migration scenario, while those with $S_{s} \\geq 30$ are likely dominated by the in-situ scenario. We apply these criteria to determine that a majority (62%) of observed multi-planetary systems formed via migration, whereas most systems with currently observed habitable planets formed via in-situ assembly. This work provides methods of leveraging the statistical measures ($S_{s}$ and $S_{c}$) to disentangle the formation history of observed multi-planetary systems based on their present-day architectures.","sentences":["Most, if not all, sun-like stars host one or more planets, making multi-planetary systems commonplace in our galaxy.","We utilize hundreds of multi-planet simulations to explore the origin of such systems, focusing on their orbital architecture.","The first set of simulations assumes in-situ assembly of planetary embryos, while the second explores planetary migration.","After applying observational biases to the simulations, we compare them to 250+ observed multi-planetary systems, including 13 systems with planets in the habitable zone.","For all of the systems, we calculate two of the so-called statistical measures: the mass concentration ($S_{c}$) and orbital spacing ($S_{s}$).","After analytic and empirical analyses, we find that the measures are related to first-order with a power law:","$S_{c} \\sim S_{s}^\\beta$. The in-situ systems exhibit steeper power-law relations relative to the migration systems.","We show that different formation scenarios cover different regions in the $S_{s} - S_{c}$ diagram with some overlap.","Furthermore, we discover that observed systems with $S_{s} < 30$ are likely dominated by the migration scenario, while those with $S_{s} \\geq 30$ are likely dominated by the in-situ scenario.","We apply these criteria to determine that a majority (62%) of observed multi-planetary systems formed via migration, whereas most systems with currently observed habitable planets formed via in-situ assembly.","This work provides methods of leveraging the statistical measures ($S_{s}$ and $S_{c}$) to disentangle the formation history of observed multi-planetary systems based on their present-day architectures."],"url":"http://arxiv.org/abs/2402.04574v1","category":"astro-ph.EP"}
{"created":"2024-02-07 04:07:54","title":"Algebraic Montgomery-Yang problem and smooth obstructions","abstract":"Let $S$ be a rational homology complex projective plane with quotient singularities. The algebraic Montgomery-Yang problem conjectures that the number of singular points of $S$ is at most three if its smooth locus is simply-connected. In this paper, we leverage results from the study of smooth 4-manifolds, including the Donaldson diagonalization theorem and Heegaard Floer correction terms, to establish additional conditions for $S$. As a result, we eliminate the possibility of a rational homology complex projective plane of specific types with four singularities. Moreover, we identify large families encompassing infinitely many types of singularities that satisfy the orbifold BMY inequality, a key property in algebraic geometry, yet are obstructed from being a rational homology complex projective plane due to smooth conditions. Additionally, we discuss computational results related to this problem, offering new insights into the algebraic Montgomery-Yang problem.","sentences":["Let $S$ be a rational homology complex projective plane with quotient singularities.","The algebraic Montgomery-Yang problem conjectures that the number of singular points of $S$ is at most three if its smooth locus is simply-connected.","In this paper, we leverage results from the study of smooth 4-manifolds, including the Donaldson diagonalization theorem and Heegaard Floer correction terms, to establish additional conditions for $S$. As a result, we eliminate the possibility of a rational homology complex projective plane of specific types with four singularities.","Moreover, we identify large families encompassing infinitely many types of singularities that satisfy the orbifold BMY inequality, a key property in algebraic geometry, yet are obstructed from being a rational homology complex projective plane due to smooth conditions.","Additionally, we discuss computational results related to this problem, offering new insights into the algebraic Montgomery-Yang problem."],"url":"http://arxiv.org/abs/2402.04569v1","category":"math.GT"}
{"created":"2024-02-07 03:49:40","title":"On the space-time analyticity of the Keller-Segel-Navier-Stokes system","abstract":"In this paper, we study the coupled Keller-Segel-Navier-Stokes system, which models chemotaxis occuring in ambient viscous fluid. We consider this nonlinear, nonlocal system on a periodic strip, equipped with homogeneous Neumann boundary conditions for the Keller-Segel part and no-slip boundary condition for the fluid part. We prove the simultaneous space-time analyticity of the solution up to the boundary based on energy methods.","sentences":["In this paper, we study the coupled Keller-Segel-Navier-Stokes system, which models chemotaxis occuring in ambient viscous fluid.","We consider this nonlinear, nonlocal system on a periodic strip, equipped with homogeneous Neumann boundary conditions for the Keller-Segel part and no-slip boundary condition for the fluid part.","We prove the simultaneous space-time analyticity of the solution up to the boundary based on energy methods."],"url":"http://arxiv.org/abs/2402.04564v1","category":"math.AP"}
{"created":"2024-02-07 03:19:02","title":"FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models","abstract":"Semantic mapping based on the supervised object detectors is sensitive to image distribution. In real-world environments, the object detection and segmentation performance can lead to a major drop, preventing the use of semantic mapping in a wider domain. On the other hand, the development of vision-language foundation models demonstrates a strong zero-shot transferability across data distribution. It provides an opportunity to construct generalizable instance-aware semantic maps. Hence, this work explores how to boost instance-aware semantic mapping from object detection generated from foundation models. We propose a probabilistic label fusion method to predict close-set semantic classes from open-set label measurements. An instance refinement module merges the over-segmented instances caused by inconsistent segmentation. We integrate all the modules into a unified semantic mapping system. Reading a sequence of RGB-D input, our work incrementally reconstructs an instance-aware semantic map. We evaluate the zero-shot performance of our method in ScanNet and SceneNN datasets. Our method achieves 40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation task. It outperforms the traditional semantic mapping method significantly.","sentences":["Semantic mapping based on the supervised object detectors is sensitive to image distribution.","In real-world environments, the object detection and segmentation performance can lead to a major drop, preventing the use of semantic mapping in a wider domain.","On the other hand, the development of vision-language foundation models demonstrates a strong zero-shot transferability across data distribution.","It provides an opportunity to construct generalizable instance-aware semantic maps.","Hence, this work explores how to boost instance-aware semantic mapping from object detection generated from foundation models.","We propose a probabilistic label fusion method to predict close-set semantic classes from open-set label measurements.","An instance refinement module merges the over-segmented instances caused by inconsistent segmentation.","We integrate all the modules into a unified semantic mapping system.","Reading a sequence of RGB-D input, our work incrementally reconstructs an instance-aware semantic map.","We evaluate the zero-shot performance of our method in ScanNet and SceneNN datasets.","Our method achieves 40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation task.","It outperforms the traditional semantic mapping method significantly."],"url":"http://arxiv.org/abs/2402.04555v1","category":"cs.CV"}
{"created":"2024-02-07 03:17:48","title":"Convexity restoration from hairy black hole in Einstein-Maxwell-charged scalar system in AdS","abstract":"In the Einstein-Maxwell-charged scalar system with a negative cosmological constant in arbitrary dimensions higher than three, there exists a horizonless charged soliton solution, which we construct explicitly for an arbitrary mass of the scalar in perturbative series in small charge. We find that the stability of the soliton is determined by the validity of the AdS weak gravity conjecture. The existence of a stable soliton might endanger the convexity of the (free) energy as a function of the charge because the phase transition between the soliton and the extremal Reissner-Nordstrom black hole would be discontinuous. We, however, argue that the existence of the hairy black hole solution circumvents the violation of convexity. The thermodynamic properties of the hairy black hole show that the phase transition becomes continuous irrespective of whether the AdS weak gravity conjecture holds. When it holds, the phase transition occurs between the soliton and the hairy black hole, and when it is violated, the phase transition occurs between the extremal Reissner-Nordstrom black hole and the hairy black hole.","sentences":["In the Einstein-Maxwell-charged scalar system with a negative cosmological constant in arbitrary dimensions higher than three, there exists a horizonless charged soliton solution, which we construct explicitly for an arbitrary mass of the scalar in perturbative series in small charge.","We find that the stability of the soliton is determined by the validity of the AdS weak gravity conjecture.","The existence of a stable soliton might endanger the convexity of the (free) energy as a function of the charge because the phase transition between the soliton and the extremal Reissner-Nordstrom black hole would be discontinuous.","We, however, argue that the existence of the hairy black hole solution circumvents the violation of convexity.","The thermodynamic properties of the hairy black hole show that the phase transition becomes continuous irrespective of whether the AdS weak gravity conjecture holds.","When it holds, the phase transition occurs between the soliton and the hairy black hole, and when it is violated, the phase transition occurs between the extremal Reissner-Nordstrom black hole and the hairy black hole."],"url":"http://arxiv.org/abs/2402.04552v1","category":"hep-th"}
{"created":"2024-02-07 03:01:57","title":"Efficient Quantum Digital Signatures over Long Distances with Likely Bit Strings","abstract":"Quantum digital signatures (QDSs) can provide information-theoretic security of messages against forgery and repudiation. Compared with previous QDS protocols that focus on signing one-bit messages, hash function-based QDS protocols can save quantum resources and are able to sign messages of arbitrary length. Using the idea of likely bit strings, we propose an efficient QDS protocol with hash functions over long distances. Our method of likely bit strings can be applied to any quantum key distribution-based QDS protocol to significantly improve the signature rate and dramatically increase the secure signature distance of QDS protocols. In order to save computing resources, we propose an improved method where Alice participates in the verification process of Bob and Charlie. This eliminates the computational complexity relating to the huge number of all likely strings. We demonstrate the advantages of our method and our improved method with the example of sending-or-not-sending QDS. Under typical parameters, both our method and our improved method can improve the signature rate by more than 100 times and increase the signature distance by about 150 km compared with hash function-based QDS protocols without likely bit strings.","sentences":["Quantum digital signatures (QDSs) can provide information-theoretic security of messages against forgery and repudiation.","Compared with previous QDS protocols that focus on signing one-bit messages, hash function-based QDS protocols can save quantum resources and are able to sign messages of arbitrary length.","Using the idea of likely bit strings, we propose an efficient QDS protocol with hash functions over long distances.","Our method of likely bit strings can be applied to any quantum key distribution-based QDS protocol to significantly improve the signature rate and dramatically increase the secure signature distance of QDS protocols.","In order to save computing resources, we propose an improved method where Alice participates in the verification process of Bob and Charlie.","This eliminates the computational complexity relating to the huge number of all likely strings.","We demonstrate the advantages of our method and our improved method with the example of sending-or-not-sending QDS.","Under typical parameters, both our method and our improved method can improve the signature rate by more than 100 times and increase the signature distance by about 150 km compared with hash function-based QDS protocols without likely bit strings."],"url":"http://arxiv.org/abs/2402.04544v1","category":"quant-ph"}
{"created":"2024-02-07 02:52:18","title":"LQ Optimal Control of First-Order Hyperbolic PDE Systems with Final State Constraints","abstract":"This paper studies the linear-quadratic (LQ) optimal control problem of a class of systems governed by the first-order hyperbolic partial differential equations (PDEs) with final state constraints. The main contribution is to present the solvability condition and the corresponding explicit optimal controller by using the Lagrange multiplier method and the technique of solving forward and backward partial differential equations (FBPDEs). In particular, the result is reduced to the case with zero-valued final state constraints. Several numerical examples are provided to demonstrate the performance of the designed optimal controller.","sentences":["This paper studies the linear-quadratic (LQ) optimal control problem of a class of systems governed by the first-order hyperbolic partial differential equations (PDEs) with final state constraints.","The main contribution is to present the solvability condition and the corresponding explicit optimal controller by using the Lagrange multiplier method and the technique of solving forward and backward partial differential equations (FBPDEs).","In particular, the result is reduced to the case with zero-valued final state constraints.","Several numerical examples are provided to demonstrate the performance of the designed optimal controller."],"url":"http://arxiv.org/abs/2402.04537v1","category":"math.OC"}
{"created":"2024-02-07 02:34:16","title":"Hyperbolic Space Spectral Characteristics in a Network of Mechanical Linkages","abstract":"We investigate the dynamic properties of elastic lattices defined by tessellations of a curved hyperbolic space. The lattices are obtained by projecting nodes of a regular hyperbolic tessellation onto a flat disk and then connecting those sites with simple linkages. Numerical and experimental investigations illustrate how their vibrational spectral properties are characterized by a high density of modes that are localized at the domain boundaries. Such properties govern the propagation of waves induced by broadband inputs. This suggests the potential for applications seeking the protection of bulk media from boundary-incident perturbations. We uncover the boundary-dominated nature of an exemplary hyperbolic lattice through the evaluation and analysis of its integrated density of states and vibrational spectrum. The dynamics of the lattice are also contextualized by comparing them with those of continuous disks characterized by Euclidean and hyperbolic property distributions, which confirms that the lattice retains the boundary-dominated spectrum observed in the hyperbolic plane. We then numerically investigate the response of the lattice to transient pulses incident on the boundary and find that edge-confined wave propagation occurs. The modal and transient pulse propagation behavior of the lattice is experimentally validated in a milled aluminum sample. By leveraging hyperbolic geometry, our mechanical lattice ushers in a novel class of mechanical metamaterials with boundary-dominated wave phenomena reminiscent of topologically protected systems suitable for applications in advanced wave control.","sentences":["We investigate the dynamic properties of elastic lattices defined by tessellations of a curved hyperbolic space.","The lattices are obtained by projecting nodes of a regular hyperbolic tessellation onto a flat disk and then connecting those sites with simple linkages.","Numerical and experimental investigations illustrate how their vibrational spectral properties are characterized by a high density of modes that are localized at the domain boundaries.","Such properties govern the propagation of waves induced by broadband inputs.","This suggests the potential for applications seeking the protection of bulk media from boundary-incident perturbations.","We uncover the boundary-dominated nature of an exemplary hyperbolic lattice through the evaluation and analysis of its integrated density of states and vibrational spectrum.","The dynamics of the lattice are also contextualized by comparing them with those of continuous disks characterized by Euclidean and hyperbolic property distributions, which confirms that the lattice retains the boundary-dominated spectrum observed in the hyperbolic plane.","We then numerically investigate the response of the lattice to transient pulses incident on the boundary and find that edge-confined wave propagation occurs.","The modal and transient pulse propagation behavior of the lattice is experimentally validated in a milled aluminum sample.","By leveraging hyperbolic geometry, our mechanical lattice ushers in a novel class of mechanical metamaterials with boundary-dominated wave phenomena reminiscent of topologically protected systems suitable for applications in advanced wave control."],"url":"http://arxiv.org/abs/2402.04531v1","category":"physics.app-ph"}
{"created":"2024-02-07 02:15:34","title":"Stairway to equilibrium entropy","abstract":"We compute the time evolution of the non-equilibrium entropy in the homogeneous isotropization dynamics of the 1RCBH model, corresponding to a top-down holographic construction describing a strongly coupled $\\mathcal{N}=4$ Supersymmetric Yang-Mills fluid charged under an Abelian $U(1)$ subgroup of the global $SU(4)$ R-symmetry. The model has a critical point in its conformal phase diagram and for the analyzed set of initial data we also evaluate the time evolution of the pressure anisotropy and the scalar condensate of the medium. As found previously for the Bjorken flow of the same model, we observe that for some initial data satisfying all the energy conditions, dynamical transient violations of the dominant and the weak energy conditions take place when the fluid is still far from equilibrium. However, a more complex structure than in Bjorken flow is observed in the formation of transient plateaus during the time evolution of the entropy density in the homogeneous isotropization dynamics. In fact, a new feature disclosed in this work is the formation of a periodic sequence of several close plateaus in the form of a stairway for the entropy density near thermodynamic equilibrium, which is observed for all the initial data analyzed.","sentences":["We compute the time evolution of the non-equilibrium entropy in the homogeneous isotropization dynamics of the 1RCBH model, corresponding to a top-down holographic construction describing a strongly coupled $\\mathcal{N}=4$ Supersymmetric Yang-Mills fluid charged under an Abelian $U(1)$ subgroup of the global $SU(4)$ R-symmetry.","The model has a critical point in its conformal phase diagram and for the analyzed set of initial data we also evaluate the time evolution of the pressure anisotropy and the scalar condensate of the medium.","As found previously for the Bjorken flow of the same model, we observe that for some initial data satisfying all the energy conditions, dynamical transient violations of the dominant and the weak energy conditions take place when the fluid is still far from equilibrium.","However, a more complex structure than in Bjorken flow is observed in the formation of transient plateaus during the time evolution of the entropy density in the homogeneous isotropization dynamics.","In fact, a new feature disclosed in this work is the formation of a periodic sequence of several close plateaus in the form of a stairway for the entropy density near thermodynamic equilibrium, which is observed for all the initial data analyzed."],"url":"http://arxiv.org/abs/2402.04529v1","category":"hep-th"}
{"created":"2024-02-07 02:06:51","title":"Understanding multiple timescales in quantum dissipative dynamics: Insights from quantum trajectories","abstract":"Open quantum systems with nearly degenerate energy levels have been shown to exhibit long-lived metastable states in the approach to equilibrium, even when modelled with certain Lindblad-form quantum master equations. This is a result of dramatic separation of timescales due to differences between Liouvillian eigenvalues. These metastable states often have nonzero coherences which die off only in the long time limit once the system reaches thermal equilibrium. We examine two distinct situations that give rise to this effect: one in which dissipative dynamics couple together states only within a nearly degenerate subspace, and one in which they give rise to jumps over finite energy splittings, between separate nearly degenerate subspaces. We find, in each case, that a change of basis can often lead to a representation which more naturally captures the impact of the system-bath interaction than does the energy eigenbasis, revealing that separate timescales are associated with separate processes (e.g. decoherence into a non-energy eigenbasis, decay of population correlations to the initial state). This approach is paired with the inspection of quantum trajectories, which further provide intuition as to how open system evolution is characterized when coherent oscillations, thermal relaxation, and decoherence all occur simultaneously.","sentences":["Open quantum systems with nearly degenerate energy levels have been shown to exhibit long-lived metastable states in the approach to equilibrium, even when modelled with certain Lindblad-form quantum master equations.","This is a result of dramatic separation of timescales due to differences between Liouvillian eigenvalues.","These metastable states often have nonzero coherences which die off only in the long time limit once the system reaches thermal equilibrium.","We examine two distinct situations that give rise to this effect: one in which dissipative dynamics couple together states only within a nearly degenerate subspace, and one in which they give rise to jumps over finite energy splittings, between separate nearly degenerate subspaces.","We find, in each case, that a change of basis can often lead to a representation which more naturally captures the impact of the system-bath interaction than does the energy eigenbasis, revealing that separate timescales are associated with separate processes (e.g. decoherence into a non-energy eigenbasis, decay of population correlations to the initial state).","This approach is paired with the inspection of quantum trajectories, which further provide intuition as to how open system evolution is characterized when coherent oscillations, thermal relaxation, and decoherence all occur simultaneously."],"url":"http://arxiv.org/abs/2402.04524v1","category":"quant-ph"}
{"created":"2024-02-07 02:06:48","title":"SumRec: A Framework for Recommendation using Open-Domain Dialogue","abstract":"Chat dialogues contain considerable useful information about a speaker's interests, preferences, and experiences.Thus, knowledge from open-domain chat dialogue can be used to personalize various systems and offer recommendations for advanced information.This study proposed a novel framework SumRec for recommending information from open-domain chat dialogue.The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation. To extract the speaker and item characteristics, the SumRec framework employs a large language model (LLM) to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.The speaker and item information are then input into a score estimation model, generating a recommendation score.Experimental results show that the SumRec framework provides better recommendations than the baseline method of using dialogues and item descriptions in their original form. Our dataset and code is publicly available at https://github.com/Ryutaro-A/SumRec","sentences":["Chat dialogues contain considerable useful information about a speaker's interests, preferences, and experiences.","Thus, knowledge from open-domain chat dialogue can be used to personalize various systems and offer recommendations for advanced information.","This study proposed a novel framework SumRec for recommending information from open-domain chat dialogue.","The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation.","To extract the speaker and item characteristics, the SumRec framework employs a large language model (LLM) to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.","The speaker and item information are then input into a score estimation model, generating a recommendation score.","Experimental results show that the SumRec framework provides better recommendations than the baseline method of using dialogues and item descriptions in their original form.","Our dataset and code is publicly available at https://github.com/Ryutaro-A/SumRec"],"url":"http://arxiv.org/abs/2402.04523v1","category":"cs.LG"}
{"created":"2024-02-07 18:59:12","title":"Opening the AI black box: program synthesis via mechanistic interpretability","abstract":"We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.","sentences":["We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code.","We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30).","MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm.","As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub.","We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy."],"url":"http://arxiv.org/abs/2402.05110v1","category":"cs.LG"}
{"created":"2024-02-07 18:55:22","title":"Tighter Generalisation Bounds via Interpolation","abstract":"This paper contains a recipe for deriving new PAC-Bayes generalisation bounds based on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to KL, Wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties. We explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases. We also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances.","sentences":["This paper contains a recipe for deriving new PAC-Bayes generalisation bounds based on the $(f, \\Gamma)$-divergence, and, in addition, presents PAC-Bayes generalisation bounds where we interpolate between a series of probability divergences (including but not limited to KL, Wasserstein, and total variation), making the best out of many worlds depending on the posterior distributions properties.","We explore the tightness of these bounds and connect them to earlier results from statistical learning, which are specific cases.","We also instantiate our bounds as training objectives, yielding non-trivial guarantees and practical performances."],"url":"http://arxiv.org/abs/2402.05101v1","category":"stat.ML"}
{"created":"2024-02-07 18:40:02","title":"Electronic structure and magnetic tendencies of trilayer La$_4$Ni$_3$O$_{10}$ under pressure: structural transition, molecular orbitals, and layer differentiation","abstract":"Motivated by the recent observation of superconductivity in the pressurized trilayer La$_4$Ni$_3$O$_{10}$ Ruddlesden-Popper (RP) nickelate, we explore its structural, electronic, and magnetic properties as a function of hydrostatic pressure from first-principles calculations. We find that in both the bilayer and trilayer nickelates, an orthorhombic(monoclinic)-to-tetragonal transition under pressure takes place concomitantly with the onset of superconductivity. The electronic structure of La$_4$Ni$_3$O$_{10}$ can be understood using a molecular trimer basis wherein $n$ molecular subbands arise as the $d_{z^2}$ orbitals hybridize strongly along the $c$-axis within the trilayer. The magnetic tendencies indicate that the ground state at ambient pressure is formed by nonmagnetic inner planes and stripe-ordered outer planes that are antiferromagnetically coupled along the $c$ axis, resulting in an unusual $\\uparrow$, 0, $\\downarrow$ stacking that is consistent with the spin density wave model suggested by neutron diffraction. Such a state is destabilized by the pressures wherein superconductivity arises. Despite the presence of $d_{z^2}$ states at the Fermi level, the $d_{x^2-y^2}$ orbitals also play a key role in the electronic structure of La$_4$Ni$_3$O$_{10}$. This active role of the $d_{x^2-y^2}$ states in the low-energy physics of the trilayer RP nickelate, together with the distinct electronic behavior of inner and outer planes, resembles the physics of multilayer cuprates.","sentences":["Motivated by the recent observation of superconductivity in the pressurized trilayer La$_4$Ni$_3$O$_{10}$ Ruddlesden-Popper (RP) nickelate, we explore its structural, electronic, and magnetic properties as a function of hydrostatic pressure from first-principles calculations.","We find that in both the bilayer and trilayer nickelates, an orthorhombic(monoclinic)-to-tetragonal transition under pressure takes place concomitantly with the onset of superconductivity.","The electronic structure of La$_4$Ni$_3$O$_{10}$ can be understood using a molecular trimer basis wherein $n$ molecular subbands arise as the $d_{z^2}$ orbitals hybridize strongly along the $c$-axis within the trilayer.","The magnetic tendencies indicate that the ground state at ambient pressure is formed by nonmagnetic inner planes and stripe-ordered outer planes that are antiferromagnetically coupled along the $c$ axis, resulting in an unusual $\\uparrow$, 0, $\\downarrow$ stacking that is consistent with the spin density wave model suggested by neutron diffraction.","Such a state is destabilized by the pressures wherein superconductivity arises.","Despite the presence of $d_{z^2}$ states at the Fermi level, the $d_{x^2-y^2}$ orbitals also play a key role in the electronic structure of La$_4$Ni$_3$O$_{10}$. This active role of the $d_{x^2-y^2}$ states in the low-energy physics of the trilayer RP nickelate, together with the distinct electronic behavior of inner and outer planes, resembles the physics of multilayer cuprates."],"url":"http://arxiv.org/abs/2402.05085v1","category":"cond-mat.supr-con"}
{"created":"2024-02-07 18:33:04","title":"Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation","abstract":"In recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks. While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability. Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.","sentences":["In recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks.","While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms.","However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation.","Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba's capability.","Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network.","This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images.","We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance.","We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset.","The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting.","The source code and baseline implementations are available."],"url":"http://arxiv.org/abs/2402.05079v1","category":"eess.IV"}
{"created":"2024-02-07 18:31:06","title":"Cycle-factors in oriented graphs","abstract":"Let $k$ be a positive integer. A $k$-cycle-factor of an oriented graph is a set of disjoint cycles of length $k$ that covers all vertices of the graph. In this paper, we prove that there exists a positive constant $c$ such that for $n$ sufficiently large, any oriented graph on $n$ vertices with both minimum out-degree and minimum in-degree at least $(1/2-c)n$ contains a $k$-cycle-factor for any $k\\geq4$. Additionally, under the same hypotheses, we also show that for any sequence $n_1, \\ldots, n_t$ with $\\sum^t_{i=1}n_i=n$ and the number of the $n_i$ equal to $3$ is $\\alpha n$, where $\\alpha$ is any real number with $0<\\alpha<1/3$, the oriented graph $D$ contains $t$ disjoint cycles of lengths $n_1, \\ldots, n_t$. This conclusion is the best possible in some sense and refines a result of Keevash and Sudakov.","sentences":["Let $k$ be a positive integer.","A $k$-cycle-factor of an oriented graph is a set of disjoint cycles of length $k$ that covers all vertices of the graph.","In this paper, we prove that there exists a positive constant $c$ such that for $n$ sufficiently large, any oriented graph on $n$ vertices with both minimum out-degree and minimum in-degree at least $(1/2-c)n$ contains a $k$-cycle-factor for any $k\\geq4$. Additionally, under the same hypotheses, we also show that for any sequence $n_1, \\ldots, n_t$ with $\\sum^t_{i=1}n_i=n$ and the number of the $n_i$ equal to $3$ is $\\alpha n$, where $\\alpha$ is any real number with $0<\\alpha<1/3$, the oriented graph $D$ contains $t$ disjoint cycles of lengths $n_1, \\ldots, n_t$.","This conclusion is the best possible in some sense and refines a result of Keevash and Sudakov."],"url":"http://arxiv.org/abs/2402.05077v1","category":"math.CO"}
{"created":"2024-02-07 18:29:38","title":"ARCollab: Towards Multi-User Interactive Cardiovascular Surgical Planning in Mobile Augmented Reality","abstract":"Surgical planning for congenital heart diseases requires a collaborative approach, traditionally involving the 3D-printing of physical heart models for inspection by surgeons and cardiologists. Recent advancements in mobile augmented reality (AR) technologies have offered a promising alternative, noted for their ease-of-use and portability. Despite this progress, there remains a gap in research exploring the use of multi-user mobile AR environments for facilitating collaborative cardiovascular surgical planning. We are developing ARCollab, an iOS AR application designed to allow multiple surgeons and cardiologists to interact with patient-specific 3D heart models in a shared environment. ARCollab allows surgeons and cardiologists to import heart models, perform gestures to manipulate the heart, and collaborate with other users without having to produce a physical heart model. We are excited by the potential for ARCollab to make long-term real-world impact, thanks to the ubiquity of iOS devices that will allow for ARCollab's easy distribution, deployment and adoption.","sentences":["Surgical planning for congenital heart diseases requires a collaborative approach, traditionally involving the 3D-printing of physical heart models for inspection by surgeons and cardiologists.","Recent advancements in mobile augmented reality (AR) technologies have offered a promising alternative, noted for their ease-of-use and portability.","Despite this progress, there remains a gap in research exploring the use of multi-user mobile AR environments for facilitating collaborative cardiovascular surgical planning.","We are developing ARCollab, an iOS AR application designed to allow multiple surgeons and cardiologists to interact with patient-specific 3D heart models in a shared environment.","ARCollab allows surgeons and cardiologists to import heart models, perform gestures to manipulate the heart, and collaborate with other users without having to produce a physical heart model.","We are excited by the potential for ARCollab to make long-term real-world impact, thanks to the ubiquity of iOS devices that will allow for ARCollab's easy distribution, deployment and adoption."],"url":"http://arxiv.org/abs/2402.05075v1","category":"cs.HC"}
{"created":"2024-02-07 17:34:32","title":"Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty","abstract":"Multi-modal sensor data fusion takes advantage of complementary or reinforcing information from each sensor and can boost overall performance in applications such as scene classification and target detection. This paper presents a new method for fusing multi-modal and multi-resolution remote sensor data without requiring pixel-level training labels, which can be difficult to obtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion (MIMRF) framework that addresses label uncertainty for fusion, but it can be slow to train due to the large search space for the fuzzy measures used to integrate sensor data sources. We propose a new method based on binary fuzzy measures, which reduces the search space and significantly improves the efficiency of the MIMRF framework. We present experimental results on synthetic data and a real-world remote sensing detection task and show that the proposed MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution fusion given remote sensing data with uncertainty.","sentences":["Multi-modal sensor data fusion takes advantage of complementary or reinforcing information from each sensor and can boost overall performance in applications such as scene classification and target detection.","This paper presents a new method for fusing multi-modal and multi-resolution remote sensor data without requiring pixel-level training labels, which can be difficult to obtain.","Previously, we developed a Multiple Instance Multi-Resolution Fusion (MIMRF) framework that addresses label uncertainty for fusion, but it can be slow to train due to the large search space for the fuzzy measures used to integrate sensor data sources.","We propose a new method based on binary fuzzy measures, which reduces the search space and significantly improves the efficiency of the MIMRF framework.","We present experimental results on synthetic data and a real-world remote sensing detection task and show that the proposed MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution fusion given remote sensing data with uncertainty."],"url":"http://arxiv.org/abs/2402.05045v1","category":"cs.CV"}
{"created":"2024-02-07 17:23:50","title":"Blow-up of solutions for a semilinear parabolic equation with nonlinear memory and absorption under nonlinear nonlocal boundary condition","abstract":"In this paper we consider initial boundary value problem for a parabolic equation with nonlinear memory and absorption under nonlinear nonlocal boundary condition. We prove global existence and blow-up of solutions.","sentences":["In this paper we consider initial boundary value problem for a parabolic equation with nonlinear memory and absorption under nonlinear nonlocal boundary condition.","We prove global existence and blow-up of solutions."],"url":"http://arxiv.org/abs/2402.05040v1","category":"math.AP"}
{"created":"2024-02-07 16:54:52","title":"Community detection problem based on polarization measures:an application to Twitter: the COVID-19 case in Spain","abstract":"In this paper, we address one of the most important topics in the field of Social Networks Analysis: the community detection problem with additional information. That additional information is modeled by a fuzzy measure that represents the risk of polarization. Particularly, we are interested in dealing with the problem of taking into account the polarization of nodes in the community detection problem. Adding this type of information to the community detection problem makes it more realistic, as a community is more likely to be defined if the corresponding elements are willing to maintain a peaceful dialogue. The polarization capacity is modeled by a fuzzy measure based on the JDJpol measure of polarization related to two poles. We also present an efficient algorithm for finding groups whose elements are no polarized. Hereafter, we work in a real case. It is a network obtained from Twitter, concerning the political position against the Spanish government taken by several influential users. We analyze how the partitions obtained change when some additional information related to how polarized that society is, is added to the problem.","sentences":["In this paper, we address one of the most important topics in the field of Social Networks Analysis: the community detection problem with additional information.","That additional information is modeled by a fuzzy measure that represents the risk of polarization.","Particularly, we are interested in dealing with the problem of taking into account the polarization of nodes in the community detection problem.","Adding this type of information to the community detection problem makes it more realistic, as a community is more likely to be defined if the corresponding elements are willing to maintain a peaceful dialogue.","The polarization capacity is modeled by a fuzzy measure based on the JDJpol measure of polarization related to two poles.","We also present an efficient algorithm for finding groups whose elements are no polarized.","Hereafter, we work in a real case.","It is a network obtained from Twitter, concerning the political position against the Spanish government taken by several influential users.","We analyze how the partitions obtained change when some additional information related to how polarized that society is, is added to the problem."],"url":"http://arxiv.org/abs/2402.05028v1","category":"cs.SI"}
{"created":"2024-02-07 16:39:19","title":"A catalogue of dual-field interferometric binary calibrators","abstract":"Dual-field interferometric observations with VLTI/GRAVITY sometimes require the use of a \"binary calibrator\", a binary star whose individual components remain unresolved by the interferometer, with a separation between 400 and 2000 mas for observations with the Units Telescopes (UTs), or 1200 to 3000 mas for the Auxiliary Telescopes (ATs). The separation vector also needs to be predictable to within 10 mas for proper pointing of the instrument. Up until now, no list of properly vetted calibrators was available for dual-field observations with VLTI/GRAVITY on the UTs. Our objective is to compile such a list, and make it available to the community. We identify a list of candidates from the Washington Double Star (WDS) catalogue, all with appropriate separations and brightness, scattered over the Southern sky. We observe them as part of a dedicated calibration programme, and determine whether these objects are true binaries (excluding higher multiplicities resolved interferometrically but unseen by imaging), and extract measurements of the separation vectors. We combine these new measurements with those available in the WDS to determine updated orbital parameters for all our vetted calibrators. We compile a list of 13 vetted binary calibrators for observations with VLTI/GRAVITY on the UTs, and provide orbital estimates and astrometric predictions for each of them. We show that our list guarantees that there are always at least two binary calibrators at airmass < 2 in the sky over the Paranal observatory, at any point in time. Any Principal Investigator wishing to use the dual-field mode of VLTI/GRAVITY with the UTs can now refer to this list to select an appropriate calibrator. We encourage the use of \"whereistheplanet\" to predict the astrometry of these calibrators, which seamlessly integrates with \"p2Gravity\" for VLTI/GRAVITY dual-field observing material preparation.","sentences":["Dual-field interferometric observations with VLTI/GRAVITY sometimes require the use of a \"binary calibrator\", a binary star whose individual components remain unresolved by the interferometer, with a separation between 400 and 2000 mas for observations with the Units Telescopes (UTs), or 1200 to 3000 mas for the Auxiliary Telescopes (ATs).","The separation vector also needs to be predictable to within 10 mas for proper pointing of the instrument.","Up until now, no list of properly vetted calibrators was available for dual-field observations with VLTI/GRAVITY on the UTs.","Our objective is to compile such a list, and make it available to the community.","We identify a list of candidates from the Washington Double Star (WDS) catalogue, all with appropriate separations and brightness, scattered over the Southern sky.","We observe them as part of a dedicated calibration programme, and determine whether these objects are true binaries (excluding higher multiplicities resolved interferometrically but unseen by imaging), and extract measurements of the separation vectors.","We combine these new measurements with those available in the WDS to determine updated orbital parameters for all our vetted calibrators.","We compile a list of 13 vetted binary calibrators for observations with VLTI/GRAVITY on the UTs, and provide orbital estimates and astrometric predictions for each of them.","We show that our list guarantees that there are always at least two binary calibrators at airmass < 2 in the sky over the Paranal observatory, at any point in time.","Any Principal Investigator wishing to use the dual-field mode of VLTI/GRAVITY with the UTs can now refer to this list to select an appropriate calibrator.","We encourage the use of \"whereistheplanet\" to predict the astrometry of these calibrators, which seamlessly integrates with \"p2Gravity\" for VLTI/GRAVITY dual-field observing material preparation."],"url":"http://arxiv.org/abs/2402.05019v1","category":"astro-ph.IM"}
{"created":"2024-02-07 16:32:58","title":"A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?","abstract":"Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate. Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data.","sentences":["Automation is one of the cornerstones of contemporary material discovery.","Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space.","While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs).","However, existing work thus far has only explored LLMs for heuristic materials searches.","Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs.","In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space.","We take a sober, dispassionate stance in answering this question.","This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate.","Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data."],"url":"http://arxiv.org/abs/2402.05015v1","category":"cs.LG"}
{"created":"2024-02-07 16:15:49","title":"Revealing the KH2PO4 soft-mode coupling mechanism with infrared spectroscopy under pressure","abstract":"We measured the far-infrared reflectivity of a KH2PO4 single crystal up to pressures of 2 GPa in the ferroelectric and paraelectric phases. We find that the nu4 vibrational mode of the PO4 tetrahedron is strongly affected by the applied pressure. At ambient pressure this phonon is destabilized by the presence of the H ions and hence shows a highly damped character, beyond the phonon propagation threshold. Applying a pressure close to 0.6 GPa makes this phonon clearly underdamped. Its behavior closely follows the soft-mode behavior observed in Raman spectroscopy. Our results solve a long standing open problem, demonstrating that the nu4 mode is the excitation mediating the coupling of the hydrogen network to the lattice modes that create the ferroelectic polarization in KH2PO4.","sentences":["We measured the far-infrared reflectivity of a KH2PO4 single crystal up to pressures of 2 GPa in the ferroelectric and paraelectric phases.","We find that the nu4 vibrational mode of the PO4 tetrahedron is strongly affected by the applied pressure.","At ambient pressure this phonon is destabilized by the presence of the H ions and hence shows a highly damped character, beyond the phonon propagation threshold.","Applying a pressure close to 0.6 GPa makes this phonon clearly underdamped.","Its behavior closely follows the soft-mode behavior observed in Raman spectroscopy.","Our results solve a long standing open problem, demonstrating that the nu4 mode is the excitation mediating the coupling of the hydrogen network to the lattice modes that create the ferroelectic polarization in KH2PO4."],"url":"http://arxiv.org/abs/2402.04998v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 15:53:49","title":"Existence of infinitely many solutions for a critical Hartree type equation with potential: local Poho\u017eaev identities methods","abstract":"This paper deals with the following equation $$-\\Delta u =K(|x'|, x'')\\Big(|x|^{-\\alpha}\\ast (K(|x'|, x'')|u|^{2^{\\ast}_{\\alpha}})\\Big) |u|^{2^{\\ast}_{\\alpha}-2}u\\quad\\mbox{in}\\ \\mathbb{R}^N,$$ where $N\\geq5$, $\\alpha>5-\\frac{6}{N-2}$, $2^{\\ast}_{\\alpha}=\\frac{2N-\\alpha}{N-2}$ is the so-called upper critical exponent in the Hardy-Littlewood-Sobolev inequality and $K(|x'|, x'')$, where $(x',x'')\\in \\mathbb{R}^2\\times\\mathbb{R}^{N-2}$, is bounded and nonnegative. Under proper assumptions on the potential function $K$, we obtain the existence of infinitely many solutions for the nonlocal critical equation by using a finite dimensional reduction argument and local Poho\\v{z}aev identities. It is a remarkable fact that the order of the Riesz potential influences the existence/non-existence of solutions.","sentences":["This paper deals with the following equation $$-\\Delta u =K(|x'|, x'')\\Big(|x|^{-\\alpha}\\ast (K(|x'|, x'')|u|^{2^{\\ast}_{\\alpha}})\\Big) |u|^{2^{\\ast}_{\\alpha}-2}u\\quad\\mbox{in}\\ \\mathbb{R}^N,$$ where $N\\geq5$, $\\alpha>5-\\frac{6}{N-2}$, $2^{\\ast}_{\\alpha}=\\frac{2N-\\alpha}{N-2}$ is the so-called upper critical exponent in the Hardy-Littlewood-Sobolev inequality and $K(|x'|, x'')$, where $(x',x'')\\in \\mathbb{R}^2\\times\\mathbb{R}^{N-2}$, is bounded and nonnegative.","Under proper assumptions on the potential function $K$, we obtain the existence of infinitely many solutions for the nonlocal critical equation by using a finite dimensional reduction argument and local Poho\\v{z}aev identities.","It is a remarkable fact that the order of the Riesz potential influences the existence/non-existence of solutions."],"url":"http://arxiv.org/abs/2402.04974v1","category":"math.AP"}
{"created":"2024-02-07 15:46:17","title":"A global study of $\u03b1$-clusters decay in heavy and superheavy nuclei with half-life and preformation factor","abstract":"A detailed study of $\\alpha$-clusters decay is exhibited by incorporating crucial microscopic nuclear structure information into the estimations of half-life and preformation factor. For the first time, using the k-cross validation approach, two semi-empirical formulas for (i) $\\alpha$-decay half-life and (ii) $\\alpha$-particle preformation factor, are picked out and subsequently modified by including shell, odd-nucleon blocking, and asymmetry effects along with the usual dependence on $\\alpha$-decay energy ($Q_{\\alpha}$) and angular momentum of $\\alpha$-particle. Both the formulas are fitted for the two different regions separated by neutron number N$=$126, as from the experimental systematics the role of N$=$126 shell closure is found decisive in determining the trends of $Q_{\\alpha}$, $\\alpha$-decay half-life, and $\\alpha$-particle preformation factor. It is found that the inclusion of the above-mentioned degrees of freedom significantly reduces the errors in the estimations when compared with several other similar modified/refitted semi-empirical relations indicating the robustness of the proposed formulas. The predictions of $\\alpha$-decay half-life throughout the periodic chart have been made including the unknown territory, future probable decay chain of self-conjugate nucleus $^{112}$Ba terminated on $^{100}$Sn, decay chain of $^{208}$Pa through new isotope $^{204}$Ac as well as decay chains of awaiting superheavy nuclei $^{298}$Og and $^{299}$120.","sentences":["A detailed study of $\\alpha$-clusters decay is exhibited by incorporating crucial microscopic nuclear structure information into the estimations of half-life and preformation factor.","For the first time, using the k-cross validation approach, two semi-empirical formulas for (i) $\\alpha$-decay half-life and (ii) $\\alpha$-particle preformation factor, are picked out and subsequently modified by including shell, odd-nucleon blocking, and asymmetry effects along with the usual dependence on $\\alpha$-decay energy ($Q_{\\alpha}$) and angular momentum of $\\alpha$-particle.","Both the formulas are fitted for the two different regions separated by neutron number N$=$126, as from the experimental systematics the role of N$=$126 shell closure is found decisive in determining the trends of $Q_{\\alpha}$, $\\alpha$-decay half-life, and $\\alpha$-particle preformation factor.","It is found that the inclusion of the above-mentioned degrees of freedom significantly reduces the errors in the estimations when compared with several other similar modified/refitted semi-empirical relations indicating the robustness of the proposed formulas.","The predictions of $\\alpha$-decay half-life throughout the periodic chart have been made including the unknown territory, future probable decay chain of self-conjugate nucleus $^{112}$Ba terminated on $^{100}$Sn, decay chain of $^{208}$Pa through new isotope $^{204}$Ac as well as decay chains of awaiting superheavy nuclei $^{298}$Og and $^{299}$120."],"url":"http://arxiv.org/abs/2402.04970v1","category":"nucl-th"}
{"created":"2024-02-07 15:45:00","title":"Excited-Band Coherent Delocalization for Improved Optical Lattice Clock Performance","abstract":"We implement coherent delocalization as a tool for improving the two primary metrics of atomic clock performance: systematic uncertainty and instability. By decreasing atomic density with coherent delocalization, we suppress cold-collision shifts and two-body losses. Atom loss attributed to Landau-Zener tunneling in the ground lattice band would compromise coherent delocalization at low trap depths for our \\textsuperscript{171}Yb atoms; hence, we implement for the first time delocalization in excited lattice bands. Doing so increases the spatial distribution of atoms trapped in the vertically-oriented optical lattice by $\\sim7$ times. At the same time we observe a reduction of the cold-collision shift by 6.5(8) times, while also making inelastic two-body loss negligible. With these advantages, we measure the trap-light-induced quenching rate and natural lifetime of the $^3$P$_0$ excited-state as $5.7(7)\\times10^{-4}$ $E_r$\\textsuperscript{$-1$}s\\textsuperscript{$-1$} and 19(2) s, respectively.","sentences":["We implement coherent delocalization as a tool for improving the two primary metrics of atomic clock performance: systematic uncertainty and instability.","By decreasing atomic density with coherent delocalization, we suppress cold-collision shifts and two-body losses.","Atom loss attributed to Landau-Zener tunneling in the ground lattice band would compromise coherent delocalization at low trap depths for our \\textsuperscript{171}Yb atoms; hence, we implement for the first time delocalization in excited lattice bands.","Doing so increases the spatial distribution of atoms trapped in the vertically-oriented optical lattice by $\\sim7$ times.","At the same time we observe a reduction of the cold-collision shift by 6.5(8) times, while also making inelastic two-body loss negligible.","With these advantages, we measure the trap-light-induced quenching rate and natural lifetime of the $^3$P$_0$ excited-state as $5.7(7)\\times10^{-4}$ $E_r$\\textsuperscript{$-1$}s\\textsuperscript{$-1$} and 19(2) s, respectively."],"url":"http://arxiv.org/abs/2402.04968v1","category":"physics.atom-ph"}
{"created":"2024-02-07 15:36:15","title":"Gradient continuity for the parabolic $(1,\\,p)$-Laplace equation under the subcritical case","abstract":"This paper is concerned with the gradient continuity for the parabolic $(1,\\,p)$-Laplace equation. In the supercritical case $\\frac{2n}{n+2}<p<\\infty$, where $n\\ge 2$ denotes the space dimension, this gradient regularity result has been proved recently by the author. In this paper, we would like to prove that the same regularity holds even for the subcritical case $1<p\\le \\frac{2n}{n+2}$ with $n\\ge 3$, on the condition that a weak solution admits the $L^{s}$-integrability with $s>n(2-p)/p$. The gradient continuity is proved, similarly to the supercritical case, once the local gradient bounds of solutions are verified. Hence, this paper mainly aims to show the local boundedness of a solution and its gradient by Moser's iteration. The proof is completed by considering a parabolic approximate problem, and showing a priori gradient estimates of a bounded weak solution to the relaxed equation.","sentences":["This paper is concerned with the gradient continuity for the parabolic $(1,\\,p)$-Laplace equation.","In the supercritical case $\\frac{2n}{n+2}<p<\\infty$, where $n\\ge 2$ denotes the space dimension, this gradient regularity result has been proved recently by the author.","In this paper, we would like to prove that the same regularity holds even for the subcritical case $1<p\\le \\frac{2n}{n+2}$ with $n\\ge 3$, on the condition that a weak solution admits the $L^{s}$-integrability with $s>n(2-p)/p$.","The gradient continuity is proved, similarly to the supercritical case, once the local gradient bounds of solutions are verified.","Hence, this paper mainly aims to show the local boundedness of a solution and its gradient by Moser's iteration.","The proof is completed by considering a parabolic approximate problem, and showing a priori gradient estimates of a bounded weak solution to the relaxed equation."],"url":"http://arxiv.org/abs/2402.04951v1","category":"math.AP"}
{"created":"2024-02-07 15:25:20","title":"Elastic Analysis of Augmented Curves and Constrained Surfaces","abstract":"The square root velocity transformation is crucial for efficiently employing the elastic approach in functional and shape data analysis of curves. We study fundamental geometric properties of curves under this transformation. Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks.","sentences":["The square root velocity transformation is crucial for efficiently employing the elastic approach in functional and shape data analysis of curves.","We study fundamental geometric properties of curves under this transformation.","Moreover, utilizing natural geometric constructions, we employ the approach for intrinsic comparison within several classes of surfaces and augmented curves, which arise in the real world applications such as tubes, ruled surfaces spherical strips, protein molecules and hurricane tracks."],"url":"http://arxiv.org/abs/2402.04944v1","category":"math.DG"}
{"created":"2024-02-07 15:25:20","title":"Modulation Mechanism of Ionic Transport through Short Nanopores by Charged Exterior Surfaces","abstract":"Short nanopores have various applications in biosensing, desalination, and energy conversion. Here, the modulation of charged exterior surfaces on ionic transport is investigated through simulations with sub-200 nm long nanopores under applied voltages. Detailed analysis of ionic current, electric field strength, and fluid flow inside and outside nanopores reveals that charged exterior surfaces can increase ionic conductance by increasing both the concentration and migration speed of charge carriers. The electric double layers near charged exterior surfaces provide an ion pool and an additional passageway for counterions, which lead to enhanced exterior surface conductance and ionic concentrations at pore entrances and inside the nanopore. We also report that charges on the membrane surfaces increase electric field strengths inside nanopores. The effective width of a ring with surface charges placed at pore entrances (Lcs) is considered as well by studying the dependence of the current on Lcs. We find a linear relationship between the effective Lcs and the surface charge density and voltage, and an inverse relationship between the geometrical pore length and salt concentration. Our results elucidate the modulation mechanism of charged exterior surfaces on ionic transport through short nanopores, which is important for the design and fabrication of porous membranes.","sentences":["Short nanopores have various applications in biosensing, desalination, and energy conversion.","Here, the modulation of charged exterior surfaces on ionic transport is investigated through simulations with sub-200 nm long nanopores under applied voltages.","Detailed analysis of ionic current, electric field strength, and fluid flow inside and outside nanopores reveals that charged exterior surfaces can increase ionic conductance by increasing both the concentration and migration speed of charge carriers.","The electric double layers near charged exterior surfaces provide an ion pool and an additional passageway for counterions, which lead to enhanced exterior surface conductance and ionic concentrations at pore entrances and inside the nanopore.","We also report that charges on the membrane surfaces increase electric field strengths inside nanopores.","The effective width of a ring with surface charges placed at pore entrances (Lcs) is considered as well by studying the dependence of the current on Lcs.","We find a linear relationship between the effective Lcs and the surface charge density and voltage, and an inverse relationship between the geometrical pore length and salt concentration.","Our results elucidate the modulation mechanism of charged exterior surfaces on ionic transport through short nanopores, which is important for the design and fabrication of porous membranes."],"url":"http://arxiv.org/abs/2402.04945v1","category":"cond-mat.soft"}
{"created":"2024-02-07 15:19:25","title":"Influences of Electroosmotic Flow on Ionic Current through Nanopores: a Comprehensive Understanding","abstract":"Continuum simulations become an important tool to uncover the mysteries in nanofluidic experiments. However, fluid flow in simulation models is usually unconsidered. Here, systematical simulations are conducted to provide a quantitative understanding of influences from electroosmotic flow (EOF) on ionic transport through nanopores by both types of models with and without consideration of EOF. In nanopores of less than ~10 nm in diameter, counterions dominate ionic current which is always promoted obviously by the convective effect of EOF. In the diameter range from ~10 to ~30 nm, strong EOF induces ion concentration polarization or ion depletion inside nanopores which causes significant decreases in ionic current. For nanopores larger than ~30 nm, due to convective promotion and inhibition of EOF on the transport of counterions and anions, considerable nanopore selectivity to counterions maintains in cases with EOF. Though the difference in total current between both cases decreases with further pore size increasing, the difference in cation/anion current is still considerable. From our results under various pore parameters and applied conditions, the fluid flow should be considered in the simulation cases when EOF is strong. Our work may provide useful guidance for simulation conductance.","sentences":["Continuum simulations become an important tool to uncover the mysteries in nanofluidic experiments.","However, fluid flow in simulation models is usually unconsidered.","Here, systematical simulations are conducted to provide a quantitative understanding of influences from electroosmotic flow (EOF) on ionic transport through nanopores by both types of models with and without consideration of EOF.","In nanopores of less than ~10 nm in diameter, counterions dominate ionic current which is always promoted obviously by the convective effect of EOF.","In the diameter range from ~10 to ~30 nm, strong EOF induces ion concentration polarization or ion depletion inside nanopores which causes significant decreases in ionic current.","For nanopores larger than ~30 nm, due to convective promotion and inhibition of EOF on the transport of counterions and anions, considerable nanopore selectivity to counterions maintains in cases with EOF.","Though the difference in total current between both cases decreases with further pore size increasing, the difference in cation/anion current is still considerable.","From our results under various pore parameters and applied conditions, the fluid flow should be considered in the simulation cases when EOF is strong.","Our work may provide useful guidance for simulation conductance."],"url":"http://arxiv.org/abs/2402.04941v1","category":"physics.chem-ph"}
{"created":"2024-02-07 15:14:38","title":"Convergence of Approximate and Packet Routing Equilibria to Nash Flows Over Time","abstract":"We consider a dynamic model of traffic that has received a lot of attention in the past few years. Infinitesimally small agents aim to travel from a source to a destination as quickly as possible. Flow patterns vary over time, and congestion effects are modeled via queues, which form based on the deterministic queueing model whenever the inflow into a link exceeds its capacity. Are equilibria in this model meaningful as a prediction of traffic behavior? For this to be the case, a certain notion of stability under ongoing perturbations is needed. Real traffic consists of discrete, atomic ''packets'', rather than being a continuous flow of non-atomic agents. Users may not choose an absolutely quickest route available, if there are multiple routes with very similar travel times. We would hope that in both these situations -- a discrete packet model, with packet size going to 0, and $\\epsilon$-equilibria, with $\\epsilon$ going to 0 -- equilibria converge to dynamic equilibria in the flow over time model. No such convergence results were known. We show that such a convergence result does hold in single-commodity instances for both of these settings, in a unified way. More precisely, we introduce a notion of ''strict'' $\\epsilon$-equilibria, and show that these must converge to the exact dynamic equilibrium in the limit as $\\epsilon \\to 0$. We then show that results for the two settings mentioned can be deduced from this with only moderate further technical effort.","sentences":["We consider a dynamic model of traffic that has received a lot of attention in the past few years.","Infinitesimally small agents aim to travel from a source to a destination as quickly as possible.","Flow patterns vary over time, and congestion effects are modeled via queues, which form based on the deterministic queueing model whenever the inflow into a link exceeds its capacity.","Are equilibria in this model meaningful as a prediction of traffic behavior?","For this to be the case, a certain notion of stability under ongoing perturbations is needed.","Real traffic consists of discrete, atomic ''packets'', rather than being a continuous flow of non-atomic agents.","Users may not choose an absolutely quickest route available, if there are multiple routes with very similar travel times.","We would hope that in both these situations -- a discrete packet model, with packet size going to 0, and $\\epsilon$-equilibria, with $\\epsilon$ going to 0 -- equilibria converge to dynamic equilibria in the flow over time model.","No such convergence results were known.","We show that such a convergence result does hold in single-commodity instances for both of these settings, in a unified way.","More precisely, we introduce a notion of ''strict'' $\\epsilon$-equilibria, and show that these must converge to the exact dynamic equilibrium in the limit as $\\epsilon \\to 0$.","We then show that results for the two settings mentioned can be deduced from this with only moderate further technical effort."],"url":"http://arxiv.org/abs/2402.04935v1","category":"cs.GT"}
{"created":"2024-02-07 15:04:01","title":"Growth in the universal cover under large simplicial volume","abstract":"Consider a closed manifold $M$ with two Riemannian metrics: one hyperbolic metric, and one other metric $g$. What hypotheses on $g$ guarantee that for a given radius $r$, there are balls of radius $r$ in the universal cover of $(M, g)$ with greather-than-hyperbolic volumes? We show that this conclusion holds for all $r \\geq 1$ if $(\\mathrm{Vol} (M, g))^2$ is less than a small constant times the hyperbolic volume of $M$. This strengthens a theorem of Sabourau and is partial progress toward a conjecture of Guth.","sentences":["Consider a closed manifold $M$ with two Riemannian metrics: one hyperbolic metric, and one other metric $g$. What hypotheses on $g$ guarantee that for a given radius $r$, there are balls of radius $r$ in the universal cover of $(M, g)$ with greather-than-hyperbolic volumes?","We show that this conclusion holds for all $r \\geq 1$ if $(\\mathrm{Vol} (M, g))^2$ is less than a small constant times the hyperbolic volume of $M$. This strengthens a theorem of Sabourau and is partial progress toward a conjecture of Guth."],"url":"http://arxiv.org/abs/2402.04932v1","category":"math.DG"}
{"created":"2024-02-07 14:46:28","title":"Characterization of the Surface Charge Property and Porosity of Track-etched Polymer Membranes","abstract":"As an important property of porous membranes, the surface charge property determines many ionic behaviors of nanopores, such as ionic conductance and selectivity. Based on the dependence of electric double layers on bulk concentrations, ionic conductance through nanopores at high and low concentrations is governed by the bulk conductance and surface charge density, respectively. Here, through the investigation of ionic conductance inside track-etched single polyethylene terephthalate (PET) nanopores under various concentrations, the surface charge density of PET membranes is extracted as around 0.021 C per m2 at pH 10 over measurements with 40 PET nanopores. Simulations show that surface roughness can cause underestimation in surface charge density due to the inhibited electroosmotic flow. Then, the averaged pore size and porosity of track-etched multipore PET membranes are characterized by the developed ionic conductance method. Through coupled theoretical predictions in ionic conductance under high and low concentrations, the averaged pore size and porosity of porous membranes can be obtained simultaneously. Our method provides a simple and precise way to characterize the pore size and porosity of multipore membranes, especially for those with sub-100 nm pores and low porosities.","sentences":["As an important property of porous membranes, the surface charge property determines many ionic behaviors of nanopores, such as ionic conductance and selectivity.","Based on the dependence of electric double layers on bulk concentrations, ionic conductance through nanopores at high and low concentrations is governed by the bulk conductance and surface charge density, respectively.","Here, through the investigation of ionic conductance inside track-etched single polyethylene terephthalate (PET) nanopores under various concentrations, the surface charge density of PET membranes is extracted as around 0.021 C per m2 at pH 10 over measurements with 40 PET nanopores.","Simulations show that surface roughness can cause underestimation in surface charge density due to the inhibited electroosmotic flow.","Then, the averaged pore size and porosity of track-etched multipore PET membranes are characterized by the developed ionic conductance method.","Through coupled theoretical predictions in ionic conductance under high and low concentrations, the averaged pore size and porosity of porous membranes can be obtained simultaneously.","Our method provides a simple and precise way to characterize the pore size and porosity of multipore membranes, especially for those with sub-100 nm pores and low porosities."],"url":"http://arxiv.org/abs/2402.04920v1","category":"physics.chem-ph"}
{"created":"2024-02-07 14:39:09","title":"What Values Do ImageNet-trained Classifiers Enact?","abstract":"We identify \"values\" as actions that classifiers take that speak to open questions of significant social concern. Investigating a classifier's values builds on studies of social bias that uncover how classifiers participate in social processes beyond their creators' forethought. In our case, this participation involves what counts as nutritious, what it means to be modest, and more. Unlike AI social bias, however, a classifier's values are not necessarily morally loathsome. Attending to image classifiers' values can facilitate public debate and introspection about the future of society. To substantiate these claims, we report on an extensive examination of both ImageNet training/validation data and ImageNet-trained classifiers with custom testing data. We identify perceptual decision boundaries in 118 categories that address open questions in society, and through quantitative testing of rival datasets we find that ImageNet-trained classifiers enact at least 7 values through their perceptual decisions. To contextualize these results, we develop a conceptual framework that integrates values, social bias, and accuracy, and we describe a rhetorical method for identifying how context affects the values that a classifier enacts. We also discover that classifier performance does not straightforwardly reflect the proportions of subgroups in a training set. Our findings bring a rich sense of the social world to ML researchers that can be applied to other domains beyond computer vision.","sentences":["We identify \"values\" as actions that classifiers take that speak to open questions of significant social concern.","Investigating a classifier's values builds on studies of social bias that uncover how classifiers participate in social processes beyond their creators' forethought.","In our case, this participation involves what counts as nutritious, what it means to be modest, and more.","Unlike AI social bias, however, a classifier's values are not necessarily morally loathsome.","Attending to image classifiers' values can facilitate public debate and introspection about the future of society.","To substantiate these claims, we report on an extensive examination of both ImageNet training/validation data and ImageNet-trained classifiers with custom testing data.","We identify perceptual decision boundaries in 118 categories that address open questions in society, and through quantitative testing of rival datasets we find that ImageNet-trained classifiers enact at least 7 values through their perceptual decisions.","To contextualize these results, we develop a conceptual framework that integrates values, social bias, and accuracy, and we describe a rhetorical method for identifying how context affects the values that a classifier enacts.","We also discover that classifier performance does not straightforwardly reflect the proportions of subgroups in a training set.","Our findings bring a rich sense of the social world to ML researchers that can be applied to other domains beyond computer vision."],"url":"http://arxiv.org/abs/2402.04911v1","category":"cs.CY"}
{"created":"2024-02-07 14:22:43","title":"Controllable gas sensitivity of Mg/CuO nanocomposite films measured in methanol vapor","abstract":"The gas sensitivity of Mg/CuO nanocomposite films, characterized by varying mass ratios of Mg:CuO, was assessed under exposure to 1000 ppm of methanol vapor. Films were fabricated by the doctor blade method on conductive and nonconductive glass substrates. Structural and optical analyses were conducted using XRD and UV-Visible spectrums. The XRD patterns facilitated the estimation of crystallite size, dislocation density and strain. The optical band gap of the samples was determined from UV-Visible spectrums. Despite variations in crystallite size, dislocation density and strain in response to changing Mg concentrations in nanocomposites, no discernible shift in the band gap was observed. The mass percentage of Mg in nanocomposite was incrementally altered from 10% to 20% in steps of 5%. Due to the adsorption of methanol vapor, the resistivity of the sample decreased significantly. Gas sensitivity exhibited variance ranging from 3.79 for pure CuO to 1.23 for nanocomposite with 20%Mg. The sample with 10%Mg quickly responded to methanol vapor compared to pure CuO and other nanocomposites.   Keywords: CuO, nanocomposites, gas sensors, XRD, UV-Visible spectrums","sentences":["The gas sensitivity of Mg/CuO nanocomposite films, characterized by varying mass ratios of Mg:CuO, was assessed under exposure to 1000 ppm of methanol vapor.","Films were fabricated by the doctor blade method on conductive and nonconductive glass substrates.","Structural and optical analyses were conducted using XRD and UV-Visible spectrums.","The XRD patterns facilitated the estimation of crystallite size, dislocation density and strain.","The optical band gap of the samples was determined from UV-Visible spectrums.","Despite variations in crystallite size, dislocation density and strain in response to changing Mg concentrations in nanocomposites, no discernible shift in the band gap was observed.","The mass percentage of Mg in nanocomposite was incrementally altered from 10% to 20% in steps of 5%.","Due to the adsorption of methanol vapor, the resistivity of the sample decreased significantly.","Gas sensitivity exhibited variance ranging from 3.79 for pure CuO to 1.23 for nanocomposite with 20%Mg.","The sample with 10%Mg quickly responded to methanol vapor compared to pure CuO and other nanocomposites.   ","Keywords: CuO, nanocomposites, gas sensors, XRD, UV-Visible spectrums"],"url":"http://arxiv.org/abs/2402.04886v1","category":"physics.app-ph"}
{"created":"2024-02-07 14:12:44","title":"Seebeck coefficient of ionic conductors from Bayesian regression analysis","abstract":"We propose a novel approach to evaluating the ionic Seebeck coefficient in electrolytes from relatively short equilibrium molecular dynamics simulations, based on the Green-Kubo theory of linear response and Bayesian regression analysis. By exploiting the probability distribution of the off-diagonal elements of a Wishart matrix, we develop a consistent and unbiased estimator for the Seebeck coefficient whose statistical uncertainty can be arbitrarily reduced in the long-time limit. To validate the effectiveness of our method, we benchmark it against extensive equilibrium molecular dynamics simulations conducted on molten $\\mathrm{CsF}$ using empirical force fields. We then employ this procedure to calculate the Seebeck coefficient of molten $\\mathrm{NaCl}$, $\\mathrm{KCl}$ and $\\mathrm{LiCl}$ using neural-network force fields trained on \\textit{ab initio} data over a range of pressure-temperature conditions.","sentences":["We propose a novel approach to evaluating the ionic Seebeck coefficient in electrolytes from relatively short equilibrium molecular dynamics simulations, based on the Green-Kubo theory of linear response and Bayesian regression analysis.","By exploiting the probability distribution of the off-diagonal elements of a Wishart matrix, we develop a consistent and unbiased estimator for the Seebeck coefficient whose statistical uncertainty can be arbitrarily reduced in the long-time limit.","To validate the effectiveness of our method, we benchmark it against extensive equilibrium molecular dynamics simulations conducted on molten $\\mathrm{CsF}$ using empirical force fields.","We then employ this procedure to calculate the Seebeck coefficient of molten $\\mathrm{NaCl}$, $\\mathrm{KCl}$ and $\\mathrm{LiCl}$ using neural-network force fields trained on \\textit{ab initio} data over a range of pressure-temperature conditions."],"url":"http://arxiv.org/abs/2402.04873v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 13:55:33","title":"Conditionality principle under unconstrained randomness","abstract":"A very simple example demonstrates that Fisher's application of the conditionality principle to regression (\"fixed $x$ regression\"), endorsed by Sprott and many other followers, makes prediction impossible in the context of statistical learning theory. On the other hand, relaxing the requirement of conditionality makes it possible via, e.g., conformal prediction.","sentences":["A very simple example demonstrates that Fisher's application of the conditionality principle to regression (\"fixed $x$ regression\"), endorsed by Sprott and many other followers, makes prediction impossible in the context of statistical learning theory.","On the other hand, relaxing the requirement of conditionality makes it possible via, e.g., conformal prediction."],"url":"http://arxiv.org/abs/2402.04859v1","category":"math.ST"}
{"created":"2024-02-07 13:54:56","title":"Advancing Anomaly Detection: An Adaptation Model and a New Dataset","abstract":"Industry surveillance is widely applicable in sectors like retail, manufacturing, education, and smart cities, each presenting unique anomalies requiring specialized detection. However, adapting anomaly detection models to novel viewpoints within the same scenario poses challenges. Extending these models to entirely new scenarios necessitates retraining or fine-tuning, a process that can be time consuming. To address these challenges, we propose the Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the few-shot learning framework for faster adaptation of pre-trained models to new concepts. Despite this approach, a significant challenge emerges from the absence of a comprehensive dataset with diverse scenarios and camera views. In response, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14 distinct scenarios captured from various camera views. This real-world dataset is the first high-resolution anomaly detection dataset, offering a solid foundation for training superior models. MSAD includes diverse normal motion patterns, incorporating challenging variations like different lighting and weather conditions. Through experimentation, we validate the efficacy of SA2D, particularly when trained on the MSAD dataset. Our results show that SA2D not only excels under novel viewpoints within the same scenario but also demonstrates competitive performance when faced with entirely new scenarios. This highlights our method's potential in addressing challenges in detecting anomalies across diverse and evolving surveillance scenarios.","sentences":["Industry surveillance is widely applicable in sectors like retail, manufacturing, education, and smart cities, each presenting unique anomalies requiring specialized detection.","However, adapting anomaly detection models to novel viewpoints within the same scenario poses challenges.","Extending these models to entirely new scenarios necessitates retraining or fine-tuning, a process that can be time consuming.","To address these challenges, we propose the Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the few-shot learning framework for faster adaptation of pre-trained models to new concepts.","Despite this approach, a significant challenge emerges from the absence of a comprehensive dataset with diverse scenarios and camera views.","In response, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14 distinct scenarios captured from various camera views.","This real-world dataset is the first high-resolution anomaly detection dataset, offering a solid foundation for training superior models.","MSAD includes diverse normal motion patterns, incorporating challenging variations like different lighting and weather conditions.","Through experimentation, we validate the efficacy of SA2D, particularly when trained on the MSAD dataset.","Our results show that SA2D not only excels under novel viewpoints within the same scenario but also demonstrates competitive performance when faced with entirely new scenarios.","This highlights our method's potential in addressing challenges in detecting anomalies across diverse and evolving surveillance scenarios."],"url":"http://arxiv.org/abs/2402.04857v1","category":"cs.CV"}
{"created":"2024-02-07 13:52:11","title":"Leveraging LLMs for Unsupervised Dense Retriever Ranking","abstract":"This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus. Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus. The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set. The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable. Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge. Existing methodologies for ranking dense retrievers fall short in addressing these domain shift scenarios.   To tackle this, our method capitalizes on LLMs to create pseudo-relevant queries, labels, and reference lists by analyzing a subset of documents from the target corpus. This allows for the ranking of dense retrievers based on their performance with these pseudo-relevant signals. Significantly, this strategy is the first to depend exclusively on the target corpus data, removing the necessity for training data and test labels. We assessed the effectiveness of our approach by compiling a comprehensive pool of cutting-edge dense retrievers and comparing our method against traditional dense retriever selection benchmarks. The findings reveal that our proposed solution surpasses the existing benchmarks in both the selection and ranking of dense retrievers.","sentences":["This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus.","Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus.","The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set.","The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable.","Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge.","Existing methodologies for ranking dense retrievers fall short in addressing these domain shift scenarios.   ","To tackle this, our method capitalizes on LLMs to create pseudo-relevant queries, labels, and reference lists by analyzing a subset of documents from the target corpus.","This allows for the ranking of dense retrievers based on their performance with these pseudo-relevant signals.","Significantly, this strategy is the first to depend exclusively on the target corpus data, removing the necessity for training data and test labels.","We assessed the effectiveness of our approach by compiling a comprehensive pool of cutting-edge dense retrievers and comparing our method against traditional dense retriever selection benchmarks.","The findings reveal that our proposed solution surpasses the existing benchmarks in both the selection and ranking of dense retrievers."],"url":"http://arxiv.org/abs/2402.04853v1","category":"cs.IR"}
{"created":"2024-02-07 13:45:00","title":"Investigation of photosensitive and photodetector characteristics of n-TPA-IFA/p-Si heterojunction structure","abstract":"n-TPA-IFA organic material was synthesized and deposited on p-Si by spin coating method to produce n-TPA-IFA/p-Si heterojunction diode. We determined that the dielectric constant and energy band gap of TPA-IFA organic material were 3.91 and 3.37 eV by DFT/B3LYP/6-311G(d,p) method using on Gaussian 09 W, respectively and the carrier type of TPA-IFA organic semiconductor material was also n-type at room temperature from temperature-dependent hall effect measurements. Using forward and reverse bias I-V measurements in the dark and under various light intensities, we examined the key electrical characteristics of the n-TPA-IFA/p-Si heterojunction diodeincluding, Qb and n. It was determined that the rectification ratio (RR) was approximately 104. The reverse current's observed increasing behavior with increasing light indicates that the produced heterojunction diode can be utilized as a photo-diode, detector, or sensor. The photodetector properties of n-TPA-IFA/p-Si heterostructure were explored at different light intensities, and the photoresponsivity (R), photosensitivity (PS), specific detectivity (D), and linear dynamic range (LDR) of the heterojunction found to be changed with reverse voltage and light intensity. It was found that as light intensity increased, the linear dynamic range (LDR), a crucial characteristic for image sensors, increased as well (10.15 dB and 18.84 dB for 20 and 100 mW/cm2). Ultimately, the findings confirmed that the TPA-IFA-based heterojunction diode could be obtained for the photodetector application.","sentences":["n-TPA-IFA organic material was synthesized and deposited on p-Si by spin coating method to produce n-TPA-IFA/p-Si heterojunction diode.","We determined that the dielectric constant and energy band gap of TPA-IFA organic material were 3.91 and 3.37 eV by DFT/B3LYP/6-311G(d,p) method using on Gaussian 09 W, respectively and the carrier type of TPA-IFA organic semiconductor material was also n-type at room temperature from temperature-dependent hall effect measurements.","Using forward and reverse bias I-V measurements in the dark and under various light intensities, we examined the key electrical characteristics of the n-TPA-IFA/p-Si heterojunction diodeincluding, Qb and n.","It was determined that the rectification ratio (RR) was approximately 104.","The reverse current's observed increasing behavior with increasing light indicates that the produced heterojunction diode can be utilized as a photo-diode, detector, or sensor.","The photodetector properties of n-TPA-IFA/p-Si heterostructure were explored at different light intensities, and the photoresponsivity (R), photosensitivity (PS), specific detectivity (D), and linear dynamic range (LDR) of the heterojunction found to be changed with reverse voltage and light intensity.","It was found that as light intensity increased, the linear dynamic range (LDR), a crucial characteristic for image sensors, increased as well (10.15 dB and 18.84 dB for 20 and 100 mW/cm2).","Ultimately, the findings confirmed that the TPA-IFA-based heterojunction diode could be obtained for the photodetector application."],"url":"http://arxiv.org/abs/2402.04846v1","category":"physics.app-ph"}
{"created":"2024-02-07 12:55:14","title":"Basis expansion approaches for functional analysis of variance with repeated measures","abstract":"The methodological contribution in this paper is motivated by biomechanical studies where data characterizing human movement are waveform curves representing joint measures such as flexion angles, velocity, acceleration, and so on. In many cases the aim consists of detecting differences in gait patterns when several independent samples of subjects walk or run under different conditions (repeated measures). Classic kinematic studies often analyse discrete summaries of the sample curves discarding important information and providing biased results. As the sample data are obviously curves, a Functional Data Analysis approach is proposed to solve the problem of testing the equality of the mean curves of a functional variable observed on several independent groups under different treatments or time periods. A novel approach for Functional Analysis of Variance (FANOVA) for repeated measures that takes into account the complete curves is introduced. By assuming a basis expansion for each sample curve, two-way FANOVA problem is reduced to Multivariate ANOVA for the multivariate response of basis coefficients. Then, two different approaches for MANOVA with repeated measures are considered. Besides, an extensive simulation study is developed to check their performance. Finally, two applications with gait data are developed.","sentences":["The methodological contribution in this paper is motivated by biomechanical studies where data characterizing human movement are waveform curves representing joint measures such as flexion angles, velocity, acceleration, and so on.","In many cases the aim consists of detecting differences in gait patterns when several independent samples of subjects walk or run under different conditions (repeated measures).","Classic kinematic studies often analyse discrete summaries of the sample curves discarding important information and providing biased results.","As the sample data are obviously curves, a Functional Data Analysis approach is proposed to solve the problem of testing the equality of the mean curves of a functional variable observed on several independent groups under different treatments or time periods.","A novel approach for Functional Analysis of Variance (FANOVA) for repeated measures that takes into account the complete curves is introduced.","By assuming a basis expansion for each sample curve, two-way FANOVA problem is reduced to Multivariate ANOVA for the multivariate response of basis coefficients.","Then, two different approaches for MANOVA with repeated measures are considered.","Besides, an extensive simulation study is developed to check their performance.","Finally, two applications with gait data are developed."],"url":"http://arxiv.org/abs/2402.04808v1","category":"stat.ME"}
{"created":"2024-02-07 12:40:08","title":"Euclid: Identifying the reddest high-redshift galaxies in the Euclid Deep Fields with gradient-boosted trees","abstract":"Dusty, distant, massive ($M_*\\gtrsim 10^{11}\\,\\rm M_\\odot$) galaxies are usually found to show a remarkable star-formation activity, contributing on the order of $25\\%$ of the cosmic star-formation rate density at $z\\approx3$--$5$, and up to $30\\%$ at $z\\sim7$ from ALMA observations. Nonetheless, they are elusive in classical optical surveys, and current near-infrared surveys are able to detect them only in very small sky areas. Since these objects have low space densities, deep and wide surveys are necessary to obtain statistically relevant results about them. Euclid will be potentially capable of delivering the required information, but, given the lack of spectroscopic features at these distances within its bands, it is still unclear if it will be possible to identify and characterize these objects. The goal of this work is to assess the capability of Euclid, together with ancillary optical and near-infrared data, to identify these distant, dusty and massive galaxies, based on broadband photometry. We used a gradient-boosting algorithm to predict both the redshift and spectral type of objects at high $z$. To perform such an analysis we make use of simulated photometric observations derived using the SPRITZ software. The gradient-boosting algorithm was found to be accurate in predicting both the redshift and spectral type of objects within the Euclid Deep Survey simulated catalog at $z>2$. In particular, we study the analog of HIEROs (i.e. sources with $H-[4.5]>2.25$), combining Euclid and Spitzer data at the depth of the Deep Fields. We found that the dusty population at $3\\lesssim z\\lesssim 7$ is well identified, with a redshift RMS and OLF of only $0.55$ and $8.5\\%$ ($H_E\\leq26$), respectively. Our findings suggest that with Euclid we will obtain meaningful insights into the role of massive and dusty galaxies in the cosmic star-formation rate over time.","sentences":["Dusty, distant, massive ($M_*\\gtrsim 10^{11}\\,\\rm M_\\odot$) galaxies are usually found to show a remarkable star-formation activity, contributing on the order of $25\\%$ of the cosmic star-formation rate density at $z\\approx3$--$5$, and up to $30\\%$ at $z\\sim7$ from ALMA observations.","Nonetheless, they are elusive in classical optical surveys, and current near-infrared surveys are able to detect them only in very small sky areas.","Since these objects have low space densities, deep and wide surveys are necessary to obtain statistically relevant results about them.","Euclid will be potentially capable of delivering the required information, but, given the lack of spectroscopic features at these distances within its bands, it is still unclear if it will be possible to identify and characterize these objects.","The goal of this work is to assess the capability of Euclid, together with ancillary optical and near-infrared data, to identify these distant, dusty and massive galaxies, based on broadband photometry.","We used a gradient-boosting algorithm to predict both the redshift and spectral type of objects at high $z$. To perform such an analysis we make use of simulated photometric observations derived using the SPRITZ software.","The gradient-boosting algorithm was found to be accurate in predicting both the redshift and spectral type of objects within the Euclid Deep Survey simulated catalog at $z>2$. In particular, we study the analog of HIEROs (i.e. sources with $H-[4.5]>2.25$), combining Euclid and Spitzer data at the depth of the Deep Fields.","We found that the dusty population at $3\\lesssim z\\lesssim 7$ is well identified, with a redshift RMS and OLF of only $0.55$ and $8.5\\%$ ($H_E\\leq26$), respectively.","Our findings suggest that with Euclid we will obtain meaningful insights into the role of massive and dusty galaxies in the cosmic star-formation rate over time."],"url":"http://arxiv.org/abs/2402.04800v1","category":"astro-ph.CO"}
{"created":"2024-02-07 12:32:52","title":"Global motions for nonhomogeneous Navier-Stokes equations with large flux","abstract":"The nonhomogeneous Navier-Stokes equations are considered in a cylindrical domain in ${\\mathbb R}^3$, parallel to the $x_3$-axis with large inflow and outflow on the top and the bottom. Moreover, on the lateral part of the cylinder the slip boundary conditions are assumed. The global existence of regular solutions is proved under assumptions that inflow and outflow are close to homogeneous and norms of derivative with respect to $x_3$ of the external force and initial velocity are sufficiently small. The key point of this paper is to verify that $x_3$-coordinate of velocity remains positive.","sentences":["The nonhomogeneous Navier-Stokes equations are considered in a cylindrical domain in ${\\mathbb R}^3$, parallel to the $x_3$-axis with large inflow and outflow on the top and the bottom.","Moreover, on the lateral part of the cylinder the slip boundary conditions are assumed.","The global existence of regular solutions is proved under assumptions that inflow and outflow are close to homogeneous and norms of derivative with respect to $x_3$ of the external force and initial velocity are sufficiently small.","The key point of this paper is to verify that $x_3$-coordinate of velocity remains positive."],"url":"http://arxiv.org/abs/2402.04793v1","category":"math.AP"}
{"created":"2024-02-07 12:03:03","title":"Performance at maximum figure of merit for a Brownian Carnot refrigerator","abstract":"This paper focuses on the coefficient of performance (COP) at maximum figure of merit $\\chi$ for a Brownian Carnot-like refrigerator, within the context of symmetric Low-Dissipation approach. Our proposal is based on the Langevin equation for a Brownian particle bounded to a harmonic potential trap, which can perform Carnot-like cycles at finite time. We show that under quasistatic conditions the COP has the same expression as the macroscopic Carnot refrigerator. However, for irreversible cycles at finite time and under symmetric dissipation, the optimal COP is the counterpart of Curzon-Ahlborn efficiency for irreversible macroscopic refrigerators.","sentences":["This paper focuses on the coefficient of performance (COP) at maximum figure of merit $\\chi$ for a Brownian Carnot-like refrigerator, within the context of symmetric Low-Dissipation approach.","Our proposal is based on the Langevin equation for a Brownian particle bounded to a harmonic potential trap, which can perform Carnot-like cycles at finite time.","We show that under quasistatic conditions the COP has the same expression as the macroscopic Carnot refrigerator.","However, for irreversible cycles at finite time and under symmetric dissipation, the optimal COP is the counterpart of Curzon-Ahlborn efficiency for irreversible macroscopic refrigerators."],"url":"http://arxiv.org/abs/2402.04780v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 11:59:51","title":"Suppression of nucleation density in twisted graphene domains grown on graphene/SiC template by sequential thermal process","abstract":"We investigated the growth of twisted graphene on graphene/silicon carbide (SiC-G) templates by metal-free chemical vapor deposition (CVD) through a sequential thermal (ST) process, which exploits the ultraclean surface of SiC-G without exposing the surface to air before CVD. By conducting control experiments with SiC-G templates exposed to air (AirE process), structural analysis by atomic force microscopy revealed that the nucleation density of CVD graphene (CVD-G) was significantly suppressed in the ST process under the same growth condition. The nucleation behavior on SiC-G surfaces is observed to be very sensitive to carbon source concentration and process temperature. The nucleation on the ultraclean surface of SiC-G prepared by the ST process requires higher partial pressure of carbon source compared with that on the surface by the AirE process. Moreover, analysis of CVD-G growth over a wide temperature range indicates that nucleation phenomena change dramatically with a threshold temperature of 1300{\\deg}C, possibly due to arising of etching effects. The successful synthesis of twisted few-layer graphene (tFLG) was affirmed by Raman spectroscopy, in which analysis of the G' band proves a high ratio of twisted structure in CVD-G. These results demonstrate that metal-free CVD utilizing ultraclean templates is an effective approach for the scalable production of large-domain tFLG that is valuable for electronic applications.","sentences":["We investigated the growth of twisted graphene on graphene/silicon carbide (SiC-G) templates by metal-free chemical vapor deposition (CVD) through a sequential thermal (ST) process, which exploits the ultraclean surface of SiC-G without exposing the surface to air before CVD.","By conducting control experiments with SiC-G templates exposed to air (AirE process), structural analysis by atomic force microscopy revealed that the nucleation density of CVD graphene (CVD-G) was significantly suppressed in the ST process under the same growth condition.","The nucleation behavior on SiC-G surfaces is observed to be very sensitive to carbon source concentration and process temperature.","The nucleation on the ultraclean surface of SiC-G prepared by the ST process requires higher partial pressure of carbon source compared with that on the surface by the AirE process.","Moreover, analysis of CVD-G growth over a wide temperature range indicates that nucleation phenomena change dramatically with a threshold temperature of 1300{\\deg}C, possibly due to arising of etching effects.","The successful synthesis of twisted few-layer graphene (tFLG) was affirmed by Raman spectroscopy, in which analysis of the G' band proves a high ratio of twisted structure in CVD-G. These results demonstrate that metal-free CVD utilizing ultraclean templates is an effective approach for the scalable production of large-domain tFLG that is valuable for electronic applications."],"url":"http://arxiv.org/abs/2402.04778v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 11:41:08","title":"Stable thrust on a finite-sized magnet above a Meissner superconducting torus","abstract":"Forces and torques exerted by a superconducting torus on a permanent magnet have been mapped. It is demonstrated that stable orbits exist. Moreover, provided that the magnet remains in any of these orbits, the first critical field in the superconductor is never overpassed and the superconductor remains in the Meissner state. The consequent absence of hysteresis makes these kinds of device perfect candidates for non-frictional bearings or gyroscopes.","sentences":["Forces and torques exerted by a superconducting torus on a permanent magnet have been mapped.","It is demonstrated that stable orbits exist.","Moreover, provided that the magnet remains in any of these orbits, the first critical field in the superconductor is never overpassed and the superconductor remains in the Meissner state.","The consequent absence of hysteresis makes these kinds of device perfect candidates for non-frictional bearings or gyroscopes."],"url":"http://arxiv.org/abs/2402.04771v1","category":"cond-mat.supr-con"}
{"created":"2024-02-07 11:18:21","title":"Rotators-Translators to Mean Curvature Flow in $\\mathbb H^2\\times\\mathbb R$","abstract":"We establish the existence of one-parameter families of helicoidal surfaces of $\\mathbb H^2\\times\\mathbb R$ which, under mean curvature flow, simultaneously rotate about a vertical axis and translate vertically.","sentences":["We establish the existence of one-parameter families of helicoidal surfaces of $\\mathbb H^2\\times\\mathbb R$ which, under mean curvature flow, simultaneously rotate about a vertical axis and translate vertically."],"url":"http://arxiv.org/abs/2402.04757v1","category":"math.DG"}
{"created":"2024-02-07 10:53:48","title":"Systematic local simulations of fast neutrino flavor conversions with scattering effects","abstract":"We investigate the dynamics of fast neutrino flavor conversions (FFCs) in the one-dimensional (1D) inhomogeneous and the homogeneous models as post processes by employing snapshots obtained by our self-consistent, realistic Boltzmann simulations in two spatial dimensions (2D). We show that the FFC growth rate is considerably larger in the inhomogeneous model than in the homogeneous model, as expected from the previous linear analysis results. We find that the momentum space dimension does not significantly influence the neutrino transition probability under inhomogeneous conditions. On the other hand, in the homogeneous model without collisions, the FFC depends on the momentum space, and the azimuthal angle dependence breaks the periodicity of the FFC. Our study demonstrates that collision-induced enhancement occurs on a long time scale in the inhomogeneous model. Despite that collision-induced enhancement does not appear on the short time scale, that should be taken into account to predict the final conversion probability.","sentences":["We investigate the dynamics of fast neutrino flavor conversions (FFCs) in the one-dimensional (1D) inhomogeneous and the homogeneous models as post processes by employing snapshots obtained by our self-consistent, realistic Boltzmann simulations in two spatial dimensions (2D).","We show that the FFC growth rate is considerably larger in the inhomogeneous model than in the homogeneous model, as expected from the previous linear analysis results.","We find that the momentum space dimension does not significantly influence the neutrino transition probability under inhomogeneous conditions.","On the other hand, in the homogeneous model without collisions, the FFC depends on the momentum space, and the azimuthal angle dependence breaks the periodicity of the FFC.","Our study demonstrates that collision-induced enhancement occurs on a long time scale in the inhomogeneous model.","Despite that collision-induced enhancement does not appear on the short time scale, that should be taken into account to predict the final conversion probability."],"url":"http://arxiv.org/abs/2402.04741v1","category":"hep-ph"}
{"created":"2024-02-07 10:41:41","title":"Comparing Observed with Simulated Solar Disk Center Scattering Polarization in the Sr I 4607 \u00c5 line","abstract":"Solar magnetic fields alter scattering polarization in spectral lines like Sr I at 4607 {\\AA} via the Hanle effect, making it a potential diagnostic for small-scale mixed-polarity photospheric magnetic fields. Recently, observational evidence for scattering polarization in the Sr I 4607 {\\AA} at the solar disk center was found. Here, we investigate the reliability of the reconstruction method making possible this detection. To this end, we apply it to linear polarization profiles of the Sr I 4607 {\\AA} line radiation emerging at the disk center obtained from a detailed 3D radiative transfer calculation in a magneto-hydrodynamic simulation snapshot with a small-scale dynamo contribution. The reconstruction method systematically reduces the scattering amplitudes by up to a factor of two, depending on the noise level. We demonstrate that the decrease can be attributed to two systematic errors: first, the physical constraint that underlies our assumptions regarding the dependence of scattering polarization on the quadrupolar moment of the radiation field, and second, the limitations of our method in accurately determining the sign of the radiation field tensor from the observed intensity image. However, consistently applying the reconstruction process and after taking into account image degradation effects due to the temporally variable image quality, such as imposed by seeing, observed and synthesized polarization signals show remarkable agreement. We thus conclude that the observed scattering polarization at solar disk center is consistent with that emerging from magneto-hydrodynamic model of the solar photosphere with an average magnetic field of 170 G at the visible surface.","sentences":["Solar magnetic fields alter scattering polarization in spectral lines like Sr I at 4607 {\\AA} via the Hanle effect, making it a potential diagnostic for small-scale mixed-polarity photospheric magnetic fields.","Recently, observational evidence for scattering polarization in the Sr I 4607 {\\AA} at the solar disk center was found.","Here, we investigate the reliability of the reconstruction method making possible this detection.","To this end, we apply it to linear polarization profiles of the Sr I 4607 {\\AA} line radiation emerging at the disk center obtained from a detailed 3D radiative transfer calculation in a magneto-hydrodynamic simulation snapshot with a small-scale dynamo contribution.","The reconstruction method systematically reduces the scattering amplitudes by up to a factor of two, depending on the noise level.","We demonstrate that the decrease can be attributed to two systematic errors: first, the physical constraint that underlies our assumptions regarding the dependence of scattering polarization on the quadrupolar moment of the radiation field, and second, the limitations of our method in accurately determining the sign of the radiation field tensor from the observed intensity image.","However, consistently applying the reconstruction process and after taking into account image degradation effects due to the temporally variable image quality, such as imposed by seeing, observed and synthesized polarization signals show remarkable agreement.","We thus conclude that the observed scattering polarization at solar disk center is consistent with that emerging from magneto-hydrodynamic model of the solar photosphere with an average magnetic field of 170 G at the visible surface."],"url":"http://arxiv.org/abs/2402.04736v1","category":"astro-ph.SR"}
{"created":"2024-02-07 10:33:09","title":"Graph Cuts with Arbitrary Size Constraints Through Optimal Transport","abstract":"A common way of partitioning graphs is through minimum cuts. One drawback of classical minimum cut methods is that they tend to produce small groups, which is why more balanced variants such as normalized and ratio cuts have seen more success. However, we believe that with these variants, the balance constraints can be too restrictive for some applications like for clustering of imbalanced datasets, while not being restrictive enough for when searching for perfectly balanced partitions. Here, we propose a new graph cut algorithm for partitioning graphs under arbitrary size constraints. We formulate the graph cut problem as a regularized Gromov-Wasserstein problem. We then propose to solve it using accelerated proximal GD algorithm which has global convergence guarantees, results in sparse solutions and only incurs an additional ratio of $\\mathcal{O}(\\log(n))$ compared to the classical spectral clustering algorithm but was seen to be more efficient.","sentences":["A common way of partitioning graphs is through minimum cuts.","One drawback of classical minimum cut methods is that they tend to produce small groups, which is why more balanced variants such as normalized and ratio cuts have seen more success.","However, we believe that with these variants, the balance constraints can be too restrictive for some applications like for clustering of imbalanced datasets, while not being restrictive enough for when searching for perfectly balanced partitions.","Here, we propose a new graph cut algorithm for partitioning graphs under arbitrary size constraints.","We formulate the graph cut problem as a regularized Gromov-Wasserstein problem.","We then propose to solve it using accelerated proximal GD algorithm which has global convergence guarantees, results in sparse solutions and only incurs an additional ratio of $\\mathcal{O}(\\log(n))$ compared to the classical spectral clustering algorithm but was seen to be more efficient."],"url":"http://arxiv.org/abs/2402.04732v1","category":"cs.LG"}
{"created":"2024-02-07 09:39:53","title":"Confinement enhanced viscosity vs shear thinning in lubricated ice friction","abstract":"The ice surface is known for presenting a very small kinetic friction coefficient, but the origin of this property remains highly controversial to date. In this work, we revisit recent computer simulations of ice sliding on atomically smooth substrates, using newly calculated bulk viscosities for the TIP4P/Ice water model. The results show that spontaneously formed premelting films in static conditions exhibit an effective viscosity which is about twice the bulk viscosity. However, upon approaching sliding speeds in the order of m/s, the shear rate becomes very large, and the viscosities decrease by several orders of magnitude. This shows that premelting films can act as an efficient lubrication layer despite their small thickness, and illustrates an interesting interplay between confinement enhanced viscosities, and shear thinning. Our results suggest that the strongly thinned viscosities that operate under the high speed skating regime could largely reduce the amount of frictional heating.","sentences":["The ice surface is known for presenting a very small kinetic friction coefficient, but the origin of this property remains highly controversial to date.","In this work, we revisit recent computer simulations of ice sliding on atomically smooth substrates, using newly calculated bulk viscosities for the TIP4P/Ice water model.","The results show that spontaneously formed premelting films in static conditions exhibit an effective viscosity which is about twice the bulk viscosity.","However, upon approaching sliding speeds in the order of m/s, the shear rate becomes very large, and the viscosities decrease by several orders of magnitude.","This shows that premelting films can act as an efficient lubrication layer despite their small thickness, and illustrates an interesting interplay between confinement enhanced viscosities, and shear thinning.","Our results suggest that the strongly thinned viscosities that operate under the high speed skating regime could largely reduce the amount of frictional heating."],"url":"http://arxiv.org/abs/2402.04700v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 09:28:35","title":"Stein Boltzmann Sampling: A Variational Approach for Global Optimization","abstract":"In this paper, we introduce a new flow-based method for global optimization of Lipschitz functions, called Stein Boltzmann Sampling (SBS). Our method samples from the Boltzmann distribution that becomes asymptotically uniform over the set of the minimizers of the function to be optimized. Candidate solutions are sampled via the \\emph{Stein Variational Gradient Descent} algorithm. We prove the asymptotic convergence of our method, introduce two SBS variants, and provide a detailed comparison with several state-of-the-art global optimization algorithms on various benchmark functions. The design of our method, the theoretical results, and our experiments, suggest that SBS is particularly well-suited to be used as a continuation of efficient global optimization methods as it can produce better solutions while making a good use of the budget.","sentences":["In this paper, we introduce a new flow-based method for global optimization of Lipschitz functions, called Stein Boltzmann Sampling (SBS).","Our method samples from the Boltzmann distribution that becomes asymptotically uniform over the set of the minimizers of the function to be optimized.","Candidate solutions are sampled via the \\emph{Stein Variational Gradient Descent} algorithm.","We prove the asymptotic convergence of our method, introduce two SBS variants, and provide a detailed comparison with several state-of-the-art global optimization algorithms on various benchmark functions.","The design of our method, the theoretical results, and our experiments, suggest that SBS is particularly well-suited to be used as a continuation of efficient global optimization methods as it can produce better solutions while making a good use of the budget."],"url":"http://arxiv.org/abs/2402.04689v1","category":"math.OC"}
{"created":"2024-02-07 09:19:01","title":"Towards a Parallel Summation Algorithm","abstract":"We propose a summation analog of the paradigm of parallel integration. Using this paradigm, we make some first steps towards an indefinite summation algorithm applicable to summands that rationally depend on the summation index and a P-recursive sequence and its shifts. Under the assumption that the corresponding difference field has no unnatural constants, we are able to compute a bound on the normal part of the denominator of a potential closed form. We can also handle the numerator. Our algorithm is incomplete so far as we cannot predict the special part of the denominator. However, we do have some structural results about special polynomials for the setting under consideration.","sentences":["We propose a summation analog of the paradigm of parallel integration.","Using this paradigm, we make some first steps towards an indefinite summation algorithm applicable to summands that rationally depend on the summation index and a P-recursive sequence and its shifts.","Under the assumption that the corresponding difference field has no unnatural constants, we are able to compute a bound on the normal part of the denominator of a potential closed form.","We can also handle the numerator.","Our algorithm is incomplete so far as we cannot predict the special part of the denominator.","However, we do have some structural results about special polynomials for the setting under consideration."],"url":"http://arxiv.org/abs/2402.04684v1","category":"math.CO"}
{"created":"2024-02-07 09:01:51","title":"Hyperparameter Tuning for Causal Inference with Double Machine Learning: A Simulation Study","abstract":"Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks. While there is an extensive literature on tuning ML learners for prediction, there is only little guidance available on tuning ML learners for causal machine learning and how to select among different ML learners. In this paper, we empirically assess the relationship between the predictive performance of ML methods and the resulting causal estimation based on the Double Machine Learning (DML) approach by Chernozhukov et al. (2018). DML relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge. We provide empirical insights on the role of hyperparameter tuning and other practical decisions for causal estimation with DML. First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning. Second, we investigate how the choice of ML methods and hyperparameters, including recent AutoML frameworks, impacts the estimation performance for a causal parameter of interest. Third, we assess to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics.","sentences":["Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks.","While there is an extensive literature on tuning ML learners for prediction, there is only little guidance available on tuning ML learners for causal machine learning and how to select among different ML learners.","In this paper, we empirically assess the relationship between the predictive performance of ML methods and the resulting causal estimation based on the Double Machine Learning (DML) approach by Chernozhukov et al.","(2018).","DML relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter.","We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge.","We provide empirical insights on the role of hyperparameter tuning and other practical decisions for causal estimation with DML.","First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning.","Second, we investigate how the choice of ML methods and hyperparameters, including recent AutoML frameworks, impacts the estimation performance for a causal parameter of interest.","Third, we assess to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics."],"url":"http://arxiv.org/abs/2402.04674v1","category":"econ.EM"}
{"created":"2024-02-07 08:59:28","title":"Streamlined Hybrid Annotation Framework using Scalable Codestream for Bandwidth-Restricted UAV Object Detection","abstract":"Emergency response missions depend on the fast relay of visual information, a task to which unmanned aerial vehicles are well adapted. However, the effective use of unmanned aerial vehicles is often compromised by bandwidth limitations that impede fast data transmission, thereby delaying the quick decision-making necessary in emergency situations. To address these challenges, this paper presents a streamlined hybrid annotation framework that utilizes the JPEG 2000 compression algorithm to facilitate object detection under limited bandwidth. The proposed framework employs a fine-tuned deep learning network for initial image annotation at lower resolutions and uses JPEG 2000's scalable codestream to selectively enhance the image resolution in critical areas that require human expert annotation. We show that our proposed hybrid framework reduces the response time by a factor of 34 in emergency situations compared to a baseline approach.","sentences":["Emergency response missions depend on the fast relay of visual information, a task to which unmanned aerial vehicles are well adapted.","However, the effective use of unmanned aerial vehicles is often compromised by bandwidth limitations that impede fast data transmission, thereby delaying the quick decision-making necessary in emergency situations.","To address these challenges, this paper presents a streamlined hybrid annotation framework that utilizes the JPEG 2000 compression algorithm to facilitate object detection under limited bandwidth.","The proposed framework employs a fine-tuned deep learning network for initial image annotation at lower resolutions and uses JPEG 2000's scalable codestream to selectively enhance the image resolution in critical areas that require human expert annotation.","We show that our proposed hybrid framework reduces the response time by a factor of 34 in emergency situations compared to a baseline approach."],"url":"http://arxiv.org/abs/2402.04673v1","category":"eess.IV"}
{"created":"2024-02-07 08:53:46","title":"A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data","abstract":"The burden of diseases is rising worldwide, with unequal treatment efficacy for patient populations that are underrepresented in clinical trials. Healthcare, however, is driven by the average population effect of medical treatments and, therefore, operates in a \"one-size-fits-all\" approach, not necessarily what best fits each patient. These facts suggest a pressing need for methodologies to study individualized treatment effects (ITE) to drive personalized treatment. Despite the increased interest in machine-learning-driven ITE estimation models, the vast majority focus on tabular data with limited review and understanding of methodologies proposed for time-series electronic health records (EHRs). To this end, this work provides an overview of ITE works for time-series data and insights into future research. The work summarizes the latest work in the literature and reviews it in light of theoretical assumptions, types of treatment settings, and computational frameworks. Furthermore, this work discusses challenges and future research directions for ITEs in a time-series setting. We hope this work opens new directions and serves as a resource for understanding one of the exciting yet under-studied research areas.","sentences":["The burden of diseases is rising worldwide, with unequal treatment efficacy for patient populations that are underrepresented in clinical trials.","Healthcare, however, is driven by the average population effect of medical treatments and, therefore, operates in a \"one-size-fits-all\" approach, not necessarily what best fits each patient.","These facts suggest a pressing need for methodologies to study individualized treatment effects (ITE) to drive personalized treatment.","Despite the increased interest in machine-learning-driven ITE estimation models, the vast majority focus on tabular data with limited review and understanding of methodologies proposed for time-series electronic health records (EHRs).","To this end, this work provides an overview of ITE works for time-series data and insights into future research.","The work summarizes the latest work in the literature and reviews it in light of theoretical assumptions, types of treatment settings, and computational frameworks.","Furthermore, this work discusses challenges and future research directions for ITEs in a time-series setting.","We hope this work opens new directions and serves as a resource for understanding one of the exciting yet under-studied research areas."],"url":"http://arxiv.org/abs/2402.04668v1","category":"cs.LG"}
{"created":"2024-02-07 08:49:01","title":"Interpretation of AMS-02 beryllium isotope fluxes using data-driven production cross sections","abstract":"The Be isotopic measurements preliminarily reported by the AMS-02 Collaboration have reached an unprecedented energy of 12 GeV/$n$. As secondary cosmic rays (CRs), the Be isotopes include both stable and unstable species, which are crucial for constraining the propagation parameters of Galactic CRs. However, uncertainties in their production cross sections can skew the interpretation of the CR data, especially when cross-section measurements are of significantly lower quality than CR measurements. In this work, we consider the uncertainties of the cross sections to interpret the Be isotopic data by adopting a cross-section parametrization that fully utilizes the available experimental data. Owing to the high-quality measurements of the $^7$Be production cross section, we innovatively employ $^7$Be instead of $^9$Be to constrain propagation parameters. Notably, the diffusion halo thickness is constrained to $5.67\\pm0.76$~kpc, representing a moderate value compared to previous analogous works. Combining the well-constrained CR propagation model and the precise CR measurements of $^9$Be, we conversely constrain the major production cross section of $^9$Be and find that it ought to be remarkably lower than previously thought. Our analysis also questions the reliability of certain cross sections measured by some experiments, potentially marking the first time CR data has been used to identify dubious nucleon production cross sections. The method presented in this work holds promise for analyzing upcoming isotopic data from other nuclei.","sentences":["The Be isotopic measurements preliminarily reported by the AMS-02 Collaboration have reached an unprecedented energy of 12 GeV/$n$. As secondary cosmic rays (CRs), the Be isotopes include both stable and unstable species, which are crucial for constraining the propagation parameters of Galactic CRs.","However, uncertainties in their production cross sections can skew the interpretation of the CR data, especially when cross-section measurements are of significantly lower quality than CR measurements.","In this work, we consider the uncertainties of the cross sections to interpret the Be isotopic data by adopting a cross-section parametrization that fully utilizes the available experimental data.","Owing to the high-quality measurements of the $^7$Be production cross section, we innovatively employ $^7$Be instead of $^9$Be to constrain propagation parameters.","Notably, the diffusion halo thickness is constrained to $5.67\\pm0.76$~kpc, representing a moderate value compared to previous analogous works.","Combining the well-constrained CR propagation model and the precise CR measurements of $^9$Be, we conversely constrain the major production cross section of $^9$Be and find that it ought to be remarkably lower than previously thought.","Our analysis also questions the reliability of certain cross sections measured by some experiments, potentially marking the first time CR data has been used to identify dubious nucleon production cross sections.","The method presented in this work holds promise for analyzing upcoming isotopic data from other nuclei."],"url":"http://arxiv.org/abs/2402.04659v1","category":"astro-ph.HE"}
{"created":"2024-02-07 08:44:54","title":"A domain wall and chiral edge current in holographic chiral phase transitions","abstract":"We investigate spatially inhomogeneous solutions in a top-down holographic model: the D3/D7 model which provides a holographic description of the chiral phase transition for a finite external magnetic field, chemical potential, and temperature. We numerically find a domain wall (or kink) solution in the three dimensional space, which incorporates between the chiral symmetry broken phase at the spatial infinity, under the homogeneous sources. Along with the inhomogeneity of the chiral condensate, the charge density is also spatially modulated. The modulated charge density and finite magnetic field lead to the chiral edge current close to the domain wall. We explore the dependences of those profiles on the chemical potential and temperature near the first and second order phase transition points. Our results indicate that the inhomogeneous solutions we found are in good agreement with those obtained by the Ginzburg--Landau theory in the vicinity of the transition points.","sentences":["We investigate spatially inhomogeneous solutions in a top-down holographic model: the D3/D7 model which provides a holographic description of the chiral phase transition for a finite external magnetic field, chemical potential, and temperature.","We numerically find a domain wall (or kink) solution in the three dimensional space, which incorporates between the chiral symmetry broken phase at the spatial infinity, under the homogeneous sources.","Along with the inhomogeneity of the chiral condensate, the charge density is also spatially modulated.","The modulated charge density and finite magnetic field lead to the chiral edge current close to the domain wall.","We explore the dependences of those profiles on the chemical potential and temperature near the first and second order phase transition points.","Our results indicate that the inhomogeneous solutions we found are in good agreement with those obtained by the Ginzburg--Landau theory in the vicinity of the transition points."],"url":"http://arxiv.org/abs/2402.04657v1","category":"hep-th"}
{"created":"2024-02-07 08:17:00","title":"Capacity Modification in the Stable Matching Problem","abstract":"We study the problem of capacity modification in the many-to-one stable matching of workers and firms. Our goal is to systematically study how the set of stable matchings changes when some seats are added to or removed from the firms. We make three main contributions: First, we examine whether firms and workers can improve or worsen upon changing the capacities under worker-proposing and firm-proposing deferred acceptance algorithms. Second, we study the computational problem of adding or removing seats to either match a fixed worker-firm pair in some stable matching or make a fixed matching stable with respect to the modified problem. We develop polynomial-time algorithms for these problems when only the overall change in the firms' capacities is restricted, and show NP-hardness when there are additional constraints for individual firms. Lastly, we compare capacity modification with the classical model of preference manipulation by firms and identify scenarios under which one mode of manipulation outperforms the other. We find that a threshold on a given firm's capacity, which we call its peak, crucially determines the effectiveness of different manipulation actions.","sentences":["We study the problem of capacity modification in the many-to-one stable matching of workers and firms.","Our goal is to systematically study how the set of stable matchings changes when some seats are added to or removed from the firms.","We make three main contributions: First, we examine whether firms and workers can improve or worsen upon changing the capacities under worker-proposing and firm-proposing deferred acceptance algorithms.","Second, we study the computational problem of adding or removing seats to either match a fixed worker-firm pair in some stable matching or make a fixed matching stable with respect to the modified problem.","We develop polynomial-time algorithms for these problems when only the overall change in the firms' capacities is restricted, and show NP-hardness when there are additional constraints for individual firms.","Lastly, we compare capacity modification with the classical model of preference manipulation by firms and identify scenarios under which one mode of manipulation outperforms the other.","We find that a threshold on a given firm's capacity, which we call its peak, crucially determines the effectiveness of different manipulation actions."],"url":"http://arxiv.org/abs/2402.04645v1","category":"cs.GT"}
{"created":"2024-02-07 07:24:35","title":"Iterated satellite operators on the knot concordance group","abstract":"We show that for a winding number zero satellite operator $P$ on the knot concordance group, if the axis of $P$ has nontrivial self-pairing under the Blanchfield form of the pattern, then the image of the iteration $P^n$ generates an infinite rank subgroup for each $n$. Furthermore, the graded quotients of the filtration of the knot concordance group associated with $P$ have infinite rank at all levels. This gives an affirmative answer to a question of Hedden and Pinz\\'{o}n-Caicedo in many cases. We also show that under the same hypotheses, $P^n$ is not a homomorphism on the knot concordance group for each $n$. We use amenable $L^2$-signatures to prove these results.","sentences":["We show that for a winding number zero satellite operator $P$ on the knot concordance group, if the axis of $P$ has nontrivial self-pairing under the Blanchfield form of the pattern, then the image of the iteration $P^n$ generates an infinite rank subgroup for each $n$. Furthermore, the graded quotients of the filtration of the knot concordance group associated with $P$ have infinite rank at all levels.","This gives an affirmative answer to a question of Hedden and Pinz\\'{o}n-Caicedo in many cases.","We also show that under the same hypotheses, $P^n$ is not a homomorphism on the knot concordance group for each $n$. We use amenable $L^2$-signatures to prove these results."],"url":"http://arxiv.org/abs/2402.04629v1","category":"math.GT"}
{"created":"2024-02-07 07:14:11","title":"MEMORYLLM: Towards Self-Updatable Large Language Models","abstract":"Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.","sentences":["Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model.","We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently.","To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer.","MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier.","Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks.","Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks.","MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates."],"url":"http://arxiv.org/abs/2402.04624v1","category":"cs.CL"}
{"created":"2024-02-07 07:10:18","title":"On rigidity of hypersurfaces with constant shifted curvature functions in hyperbolic space","abstract":"In this paper, we first give some new characterizations of geodesic sphere in hyperbolic space by the condition that hypersurface has constant weighted shifted mean curvatures, or constant weighted shifted mean curvature ratio, which generalize the result of Hu-Wei-Zhou \\cite{HWZ23}. Secondly, we investigate several rigidity problems for hypersurfaces in the hyperbolic space with constant linear combinations of weighted shifted mean curvatures as well as radially symmetric shifted mean curvatures. As applications, we obtain the rigidity results for hypersurfaces with constant linear combinations of mean curvatures in a general form and constant Gauss-Bonnet curvature $L_k$ under weaker conditions, which extend the work of the third author and Xia \\cite{WX14}.","sentences":["In this paper, we first give some new characterizations of geodesic sphere in hyperbolic space by the condition that hypersurface has constant weighted shifted mean curvatures, or constant weighted shifted mean curvature ratio, which generalize the result of Hu-Wei-Zhou \\cite{HWZ23}.","Secondly, we investigate several rigidity problems for hypersurfaces in the hyperbolic space with constant linear combinations of weighted shifted mean curvatures as well as radially symmetric shifted mean curvatures.","As applications, we obtain the rigidity results for hypersurfaces with constant linear combinations of mean curvatures in a general form and constant Gauss-Bonnet curvature $L_k$ under weaker conditions, which extend the work of the third author and Xia \\cite{WX14}."],"url":"http://arxiv.org/abs/2402.04622v1","category":"math.DG"}
{"created":"2024-02-07 06:32:50","title":"Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models","abstract":"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we urge the community to identify the faithfulness requirements of real-world applications and ensure explanations meet those needs. Finally, we propose some directions for future work, emphasizing the need for novel methodologies and frameworks that can enhance the faithfulness of self-explanations without compromising their plausibility, essential for the transparent deployment of LLMs in diverse high-stakes domains.","sentences":["Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications.","Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior.","Self-explanations have seen widespread adoption owing to their conversational and plausible nature.","However, there is little to no understanding of their faithfulness.","In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs.","We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness.","We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness.","We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making.","Moreover, we urge the community to identify the faithfulness requirements of real-world applications and ensure explanations meet those needs.","Finally, we propose some directions for future work, emphasizing the need for novel methodologies and frameworks that can enhance the faithfulness of self-explanations without compromising their plausibility, essential for the transparent deployment of LLMs in diverse high-stakes domains."],"url":"http://arxiv.org/abs/2402.04614v1","category":"cs.CL"}
{"created":"2024-02-07 06:05:11","title":"SBoTFlow: A Scalable framework using lattice Boltzmann method and Topology-confined mesh refinement for moving-body applications","abstract":"This paper proposes a scalable lattice-Boltzmann computational framework (SBoTFlow) for simulations of flexible moving objects in an incompressible fluid flow. Behavior of fluid flow formed from moving boundaries of flexible-object motions is obtained through the multidirect forcing immersed boundary scheme associated with the lattice Boltzmann equation with a parallel topology-confined block refinement framework. We first demonstrate that the hydrodynamic quantities computed in this manner for standard benchmarks, including the Tayler-Green vortex flow and flow over an obstacle-embedded lid-driven cavity and an isolated circular cylinder, agree well with those previously published in the literature. We then exploit the framework to probe the underlying dynamic properties contributing to fluid flow under flexible motions at different Reynolds numbers by simulating large-scale flapping wing motions of both amplitude and frequency. The analysis shows that the proposed numerical framework for pitching and flapping motions has a strong ability to accurately capture high amplitudes, specifically up to $64^\\circ$, and a frequency of $f=1/2.5\\pi$. This suggests that the present parallel numerical framework has the potential to be used in studying flexible motions, such as insect flight or wing aerodynamics.","sentences":["This paper proposes a scalable lattice-Boltzmann computational framework (SBoTFlow) for simulations of flexible moving objects in an incompressible fluid flow.","Behavior of fluid flow formed from moving boundaries of flexible-object motions is obtained through the multidirect forcing immersed boundary scheme associated with the lattice Boltzmann equation with a parallel topology-confined block refinement framework.","We first demonstrate that the hydrodynamic quantities computed in this manner for standard benchmarks, including the Tayler-Green vortex flow and flow over an obstacle-embedded lid-driven cavity and an isolated circular cylinder, agree well with those previously published in the literature.","We then exploit the framework to probe the underlying dynamic properties contributing to fluid flow under flexible motions at different Reynolds numbers by simulating large-scale flapping wing motions of both amplitude and frequency.","The analysis shows that the proposed numerical framework for pitching and flapping motions has a strong ability to accurately capture high amplitudes, specifically up to $64^\\circ$, and a frequency of $f=1/2.5\\pi$. This suggests that the present parallel numerical framework has the potential to be used in studying flexible motions, such as insect flight or wing aerodynamics."],"url":"http://arxiv.org/abs/2402.04606v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 05:58:30","title":"Symmetric bilinear Forms and Galois Theory","abstract":"Let $ K$ be a field admitting a Galois extension $L$ of degree $n$, denoting the Galois group as $G = \\gal(L/K)$. Our focus lies on the space $\\sym_K(L)$ of symmetric $K$-bilinear forms on $L$. We establish a decomposition of $\\sym_K(L)$ into direct sum of $K$-subspaces $A^{\\sigma_i}$, where $\\sigma_i \\in G$. Notably, these subspaces $ A^{\\sigma_i}$ exhibit nice constant rank properties. The central contribution of this paper is a decomposition theorem for $\\sym_K(L)$, revealing a direct sum of $\\frac{(n+1)}{2}$ constant rank $n$-subspaces, each having dimension of $n$. This holds particularly when $G$ is cyclic, represented as $G = \\gal(L/K) = \\langle\\sigma\\rangle$. For cyclic extensions of even degree $n = 2m$, we present slightly less precise but analogous results. In this scenario, we enhance and enrich these constant results and show that, the component $ A^{\\sigma}$ often decomposes directly into a constant rank subspaces. Remarkably, this decomposition is universally valid when $-1 \\notin L^{2}$. Consequently, we derive a decomposition of $\\sym_K(L)$ into subspaces of constant rank under several situations. Moreover, leveraging these decompositions, we investigate the maximum dimension of an $n$-subspace inside $M(n,K)$ and $S(n,K)$ for various field $K$ where $M(n,K)$ and $ S(n,K)$ denote the vector spaces $(n \\times n)$ matrices and symmetric matrices over $K$, respectively.","sentences":["Let $ K$ be a field admitting a Galois extension $L$ of degree $n$, denoting the Galois group as $G = \\gal(L/K)$. Our focus lies on the space $\\sym_K(L)$ of symmetric $K$-bilinear forms on $L$. We establish a decomposition of $\\sym_K(L)$ into direct sum of $K$-subspaces $A^{\\sigma_i}$, where $\\sigma_i \\in G$. Notably, these subspaces $ A^{\\sigma_i}$ exhibit nice constant rank properties.","The central contribution of this paper is a decomposition theorem for $\\sym_K(L)$, revealing a direct sum of $\\frac{(n+1)}{2}$ constant rank $n$-subspaces, each having dimension of $n$. This holds particularly when $G$ is cyclic, represented as $G = \\gal(L/K) = \\langle\\sigma\\rangle$. For cyclic extensions of even degree $n = 2m$, we present slightly less precise but analogous results.","In this scenario, we enhance and enrich these constant results and show that, the component $ A^{\\sigma}$ often decomposes directly into a constant rank subspaces.","Remarkably, this decomposition is universally valid when $-1 \\notin L^{2}$.","Consequently, we derive a decomposition of $\\sym_K(L)$ into subspaces of constant rank under several situations.","Moreover, leveraging these decompositions, we investigate the maximum dimension of an $n$-subspace inside $M(n,K)$ and $S(n,K)$ for various field $K$ where $M(n,K)$ and $ S(n,K)$ denote the vector spaces $(n \\times n)$ matrices and symmetric matrices over $K$, respectively."],"url":"http://arxiv.org/abs/2402.04604v1","category":"math.NT"}
{"created":"2024-02-07 05:57:42","title":"Online Quantile Regression","abstract":"This paper tackles the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of covariates is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only short-term sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for square loss, we also briefly discuss its properties and proper schemes. Extensive simulations support our theoretical findings.","sentences":["This paper tackles the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of covariates is allowed to grow with the number of observations, the horizon is unknown, and memory is limited.","We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance.","In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario.","Our method ensures long-term optimal estimation irrespective of the chosen update strategy.","Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only short-term sensitivity to initial errors.","A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets.","This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations.","Additionally, when the initial error rate is well controlled, there is a trade-off between short-term error rate and long-term optimality.","Due to the lack of delicate statistical analysis for square loss, we also briefly discuss its properties and proper schemes.","Extensive simulations support our theoretical findings."],"url":"http://arxiv.org/abs/2402.04602v1","category":"math.ST"}
{"created":"2024-02-07 05:05:53","title":"UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset","abstract":"Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data without any performance degradation, making the SFT process more efficient. The resulting UltraLink dataset comprises approximately 1 million samples across five languages, and the proposed data construction method can also be easily extended to other languages. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks.","sentences":["Open-source large language models (LLMs) have gained significant strength across diverse fields.","Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning.","In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset.","Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs.","For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries.","For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary.","Consequently, we can substantially prune the language-agnostic SFT data without any performance degradation, making the SFT process more efficient.","The resulting UltraLink dataset comprises approximately 1 million samples across five languages, and the proposed data construction method can also be easily extended to other languages.","UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks."],"url":"http://arxiv.org/abs/2402.04588v1","category":"cs.CL"}
{"created":"2024-02-07 05:03:31","title":"Efficient anytime algorithms to solve the bi-objective Next Release Problem","abstract":"The Next Release Problem consists in selecting a subset of requirements to develop in the next release of a software product. The selection should be done in a way that maximizes the satisfaction of the stakeholders while the development cost is minimized and the constraints of the requirements are fulfilled. Recent works have solved the problem using exact methods based on Integer Linear Programming. In practice, there is no need to compute all the efficient solutions of the problem; a well-spread set in the objective space is more convenient for the decision maker. The exact methods used in the past to find the complete Pareto front explore the objective space in a lexicographic order or use a weighted sum of the objectives to solve a single-objective problem, finding only supported solutions. In this work, we propose five new methods that maintain a well-spread set of solutions at any time during the search, so that the decision maker can stop the algorithm when a large enough set of solutions is found. The methods are called anytime due to this feature. They find both supported and non-supported solutions, and can complete the whole Pareto front if the time provided is long enough.","sentences":["The Next Release Problem consists in selecting a subset of requirements to develop in the next release of a software product.","The selection should be done in a way that maximizes the satisfaction of the stakeholders while the development cost is minimized and the constraints of the requirements are fulfilled.","Recent works have solved the problem using exact methods based on Integer Linear Programming.","In practice, there is no need to compute all the efficient solutions of the problem; a well-spread set in the objective space is more convenient for the decision maker.","The exact methods used in the past to find the complete Pareto front explore the objective space in a lexicographic order or use a weighted sum of the objectives to solve a single-objective problem, finding only supported solutions.","In this work, we propose five new methods that maintain a well-spread set of solutions at any time during the search, so that the decision maker can stop the algorithm when a large enough set of solutions is found.","The methods are called anytime due to this feature.","They find both supported and non-supported solutions, and can complete the whole Pareto front if the time provided is long enough."],"url":"http://arxiv.org/abs/2402.04586v1","category":"cs.SE"}
{"created":"2024-02-07 04:51:14","title":"A Psychological Study: Importance of Contrast and Luminance in Color to Grayscale Mapping","abstract":"Grayscale images are essential in image processing and computer vision tasks. They effectively emphasize luminance and contrast, highlighting important visual features, while also being easily compatible with other algorithms. Moreover, their simplified representation makes them efficient for storage and transmission purposes. While preserving contrast is important for maintaining visual quality, other factors such as preserving information relevant to the specific application or task at hand may be more critical for achieving optimal performance. To evaluate and compare different decolorization algorithms, we designed a psychological experiment. During the experiment, participants were instructed to imagine color images in a hypothetical \"colorless world\" and select the grayscale image that best resembled their mental visualization. We conducted a comparison between two types of algorithms: (i) perceptual-based simple color space conversion algorithms, and (ii) spatial contrast-based algorithms, including iteration-based methods. Our experimental findings indicate that CIELAB exhibited superior performance on average, providing further evidence for the effectiveness of perception-based decolorization algorithms. On the other hand, the spatial contrast-based algorithms showed relatively poorer performance, possibly due to factors such as DC-offset and artificial contrast generation. However, these algorithms demonstrated shorter selection times. Notably, no single algorithm consistently outperformed the others across all test images. In this paper, we will delve into a comprehensive discussion on the significance of contrast and luminance in color-to-grayscale mapping based on our experimental results and analysis.","sentences":["Grayscale images are essential in image processing and computer vision tasks.","They effectively emphasize luminance and contrast, highlighting important visual features, while also being easily compatible with other algorithms.","Moreover, their simplified representation makes them efficient for storage and transmission purposes.","While preserving contrast is important for maintaining visual quality, other factors such as preserving information relevant to the specific application or task at hand may be more critical for achieving optimal performance.","To evaluate and compare different decolorization algorithms, we designed a psychological experiment.","During the experiment, participants were instructed to imagine color images in a hypothetical \"colorless world\" and select the grayscale image that best resembled their mental visualization.","We conducted a comparison between two types of algorithms: (i) perceptual-based simple color space conversion algorithms, and (ii) spatial contrast-based algorithms, including iteration-based methods.","Our experimental findings indicate that CIELAB exhibited superior performance on average, providing further evidence for the effectiveness of perception-based decolorization algorithms.","On the other hand, the spatial contrast-based algorithms showed relatively poorer performance, possibly due to factors such as DC-offset and artificial contrast generation.","However, these algorithms demonstrated shorter selection times.","Notably, no single algorithm consistently outperformed the others across all test images.","In this paper, we will delve into a comprehensive discussion on the significance of contrast and luminance in color-to-grayscale mapping based on our experimental results and analysis."],"url":"http://arxiv.org/abs/2402.04583v1","category":"cs.CV"}
{"created":"2024-02-07 04:47:19","title":"Dimensionality reduction can be used as a surrogate model for high-dimensional forward uncertainty quantification","abstract":"We introduce a method to construct a stochastic surrogate model from the results of dimensionality reduction in forward uncertainty quantification. The hypothesis is that the high-dimensional input augmented by the output of a computational model admits a low-dimensional representation. This assumption can be met by numerous uncertainty quantification applications with physics-based computational models. The proposed approach differs from a sequential application of dimensionality reduction followed by surrogate modeling, as we \"extract\" a surrogate model from the results of dimensionality reduction in the input-output space. This feature becomes desirable when the input space is genuinely high-dimensional. The proposed method also diverges from the Probabilistic Learning on Manifold, as a reconstruction mapping from the feature space to the input-output space is circumvented. The final product of the proposed method is a stochastic simulator that propagates a deterministic input into a stochastic output, preserving the convenience of a sequential \"dimensionality reduction + Gaussian process regression\" approach while overcoming some of its limitations. The proposed method is demonstrated through two uncertainty quantification problems characterized by high-dimensional input uncertainties.","sentences":["We introduce a method to construct a stochastic surrogate model from the results of dimensionality reduction in forward uncertainty quantification.","The hypothesis is that the high-dimensional input augmented by the output of a computational model admits a low-dimensional representation.","This assumption can be met by numerous uncertainty quantification applications with physics-based computational models.","The proposed approach differs from a sequential application of dimensionality reduction followed by surrogate modeling, as we \"extract\" a surrogate model from the results of dimensionality reduction in the input-output space.","This feature becomes desirable when the input space is genuinely high-dimensional.","The proposed method also diverges from the Probabilistic Learning on Manifold, as a reconstruction mapping from the feature space to the input-output space is circumvented.","The final product of the proposed method is a stochastic simulator that propagates a deterministic input into a stochastic output, preserving the convenience of a sequential \"dimensionality reduction + Gaussian process regression\" approach while overcoming some of its limitations.","The proposed method is demonstrated through two uncertainty quantification problems characterized by high-dimensional input uncertainties."],"url":"http://arxiv.org/abs/2402.04582v1","category":"stat.AP"}
{"created":"2024-02-07 04:33:52","title":"The Bondi-Sachs formalism for the Einstein scalar field equations with the zero cosmological constant","abstract":"Inspired by interaction of gravitational waves and dark matters, we study the Bondi-Sachs formalism for Einstein massless scalar field with zero cosmological constant. We provide asymptotic expansions for the Bondi-Sachs metrics as well as the scalar fields and prove the peeling property. We also prove the positivity of the Bondi energy-momentum under conditions ensuring certain asymptotically null hypersurfaces are of order 2.","sentences":["Inspired by interaction of gravitational waves and dark matters, we study the Bondi-Sachs formalism for Einstein massless scalar field with zero cosmological constant.","We provide asymptotic expansions for the Bondi-Sachs metrics as well as the scalar fields and prove the peeling property.","We also prove the positivity of the Bondi energy-momentum under conditions ensuring certain asymptotically null hypersurfaces are of order 2."],"url":"http://arxiv.org/abs/2402.04577v1","category":"gr-qc"}
{"created":"2024-02-07 04:11:19","title":"A conjecture implying Thomassen's chord conjecture in graph theory","abstract":"Thomassen's chord conjecture from 1976 states that every longest cycle in a $3$-connected graph has a chord. This is one of the most important unsolved problems in graph theory. We pose a new conjecture which implies Thomassen's conjecture. It involves bound vertices in a longest path between two vertices in a $k$-connected graph. We also give supporting evidence and analyze a special case. The purpose of making this new conjecture is to explore the surroundings of Thomassen's conjecture.","sentences":["Thomassen's chord conjecture from 1976 states that every longest cycle in a $3$-connected graph has a chord.","This is one of the most important unsolved problems in graph theory.","We pose a new conjecture which implies Thomassen's conjecture.","It involves bound vertices in a longest path between two vertices in a $k$-connected graph.","We also give supporting evidence and analyze a special case.","The purpose of making this new conjecture is to explore the surroundings of Thomassen's conjecture."],"url":"http://arxiv.org/abs/2402.04572v1","category":"math.CO"}
{"created":"2024-02-07 04:05:29","title":"Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy","abstract":"Radiotherapy is a primary treatment for cancers with the aim of applying sufficient radiation dose to the planning target volume (PTV) while minimizing dose hazards to the organs at risk (OARs). Convolutional neural networks (CNNs) have automated the radiotherapy plan-making by predicting the dose maps. However, current CNN-based methods ignore the remarkable dose difference in the dose map, i.e., high dose value in the interior PTV while low value in the exterior PTV, leading to a suboptimal prediction. In this paper, we propose a triplet-constraint transformer (TCtrans) with multi-scale refinement to predict the high-quality dose distribution. Concretely, a novel PTV-guided triplet constraint is designed to refine dose feature representations in the interior and exterior PTV by utilizing the explicit geometry of PTV. Furthermore, we introduce a multi-scale refinement (MSR) module to effectively fulfill the triplet constraint in different decoding layers with multiple scales. Besides, a transformer encoder is devised to learn the important global dosimetric knowledge. Experiments on a clinical cervical cancer dataset demonstrate the superiority of our method.","sentences":["Radiotherapy is a primary treatment for cancers with the aim of applying sufficient radiation dose to the planning target volume (PTV) while minimizing dose hazards to the organs at risk (OARs).","Convolutional neural networks (CNNs) have automated the radiotherapy plan-making by predicting the dose maps.","However, current CNN-based methods ignore the remarkable dose difference in the dose map, i.e., high dose value in the interior PTV while low value in the exterior PTV, leading to a suboptimal prediction.","In this paper, we propose a triplet-constraint transformer (TCtrans) with multi-scale refinement to predict the high-quality dose distribution.","Concretely, a novel PTV-guided triplet constraint is designed to refine dose feature representations in the interior and exterior PTV by utilizing the explicit geometry of PTV.","Furthermore, we introduce a multi-scale refinement (MSR) module to effectively fulfill the triplet constraint in different decoding layers with multiple scales.","Besides, a transformer encoder is devised to learn the important global dosimetric knowledge.","Experiments on a clinical cervical cancer dataset demonstrate the superiority of our method."],"url":"http://arxiv.org/abs/2402.04566v1","category":"eess.IV"}
{"created":"2024-02-07 03:13:11","title":"Riemann-Lebesgue Forest for Regression","abstract":"We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \\cite{Vaart} and Stein's method \\cite{Chen2010NormalAB}. When the underlying function $Y=f(\\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \\cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \\cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets.","sentences":["We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression.","The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals.","With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\\mathbf{X}$ at each non-terminal node.","We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \\cite{Vaart} and Stein's method \\cite{Chen2010NormalAB}.","When the underlying function $Y=f(\\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \\cite{Scornet2014ConsistencyOR}.","The competitive performance of RLF against original random forest \\cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and real world datasets."],"url":"http://arxiv.org/abs/2402.04550v1","category":"stat.ML"}
{"created":"2024-02-07 03:02:57","title":"Insight into the solar plage chromosphere with DKIST","abstract":"The strongly coupled hydrodynamic, magnetic, and radiation properties of the plasma in the solar chromosphere makes it a region of the Sun's atmosphere that is poorly understood. We use data obtained with the high-resolution Visible Broadband Imager (VBI) equipped with an H$\\beta$ filter and the Visible Spectro-Polarimeter (ViSP) at the Daniel K. Inouye Solar Telescope to investigate the fine-scale structure of the plage chromosphere. To aid the interpretation of the VBI imaging data, we also analyze spectra from the CHROMospheric Imaging Spectrometer on the Swedish Solar Telescope. The analysis of spectral properties, such as enhanced line widths and line depths explains the high contrast of the fibrils relative to the background atmosphere demonstrating that H$\\beta$ is an excellent diagnostic for the enigmatic fine-scale structure of the chromosphere. A correlation between the parameters of the H$\\beta$ line indicates that opacity broadening created by overdense fibrils could be the main reason for the spectral line broadening observed frequently in chromospheric fine-scale structures. Spectropolarimetric inversions of the ViSP data in the Ca II 8542 {\\AA} and Fe I 6301/6302 {\\AA} lines are used to construct semiempirical models of the plage atmosphere. Inversion outputs indicate the existence of dense fibrils in the Ca II 8542 {\\AA} line. The analyses of the ViSP data show that the morphological characteristics, such as orientation, inclination and length of fibrils are defined by the topology of the magnetic field in the photosphere. Chromospheric maps reveal a prominent magnetic canopy in the area where fibrils are directed towards the observer.","sentences":["The strongly coupled hydrodynamic, magnetic, and radiation properties of the plasma in the solar chromosphere makes it a region of the Sun's atmosphere that is poorly understood.","We use data obtained with the high-resolution Visible Broadband Imager (VBI) equipped with an H$\\beta$ filter and the Visible Spectro-Polarimeter (ViSP) at the Daniel K. Inouye Solar Telescope to investigate the fine-scale structure of the plage chromosphere.","To aid the interpretation of the VBI imaging data, we also analyze spectra from the CHROMospheric Imaging Spectrometer on the Swedish Solar Telescope.","The analysis of spectral properties, such as enhanced line widths and line depths explains the high contrast of the fibrils relative to the background atmosphere demonstrating that H$\\beta$ is an excellent diagnostic for the enigmatic fine-scale structure of the chromosphere.","A correlation between the parameters of the H$\\beta$ line indicates that opacity broadening created by overdense fibrils could be the main reason for the spectral line broadening observed frequently in chromospheric fine-scale structures.","Spectropolarimetric inversions of the ViSP data in the Ca II 8542 {\\AA} and Fe I 6301/6302 {\\AA} lines are used to construct semiempirical models of the plage atmosphere.","Inversion outputs indicate the existence of dense fibrils in the Ca II 8542 {\\AA} line.","The analyses of the ViSP data show that the morphological characteristics, such as orientation, inclination and length of fibrils are defined by the topology of the magnetic field in the photosphere.","Chromospheric maps reveal a prominent magnetic canopy in the area where fibrils are directed towards the observer."],"url":"http://arxiv.org/abs/2402.04545v1","category":"astro-ph.SR"}
{"created":"2024-02-07 02:48:20","title":"M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual Dataset","abstract":"Forest monitoring and education are key to forest protection, education and management, which is an effective way to measure the progress of a country's forest and climate commitments. Due to the lack of a large-scale wild forest monitoring benchmark, the common practice is to train the model on a common outdoor benchmark (e.g., KITTI) and evaluate it on real forest datasets (e.g., CanaTree100). However, there is a large domain gap in this setting, which makes the evaluation and deployment difficult. In this paper, we propose a new photorealistic virtual forest dataset and a multimodal transformer-based algorithm for tree detection and instance segmentation. To the best of our knowledge, it is the first time that a multimodal detection and segmentation algorithm is applied to large-scale forest scenes. We believe that the proposed dataset and method will inspire the simulation, computer vision, education, and forestry communities towards a more comprehensive multi-modal understanding.","sentences":["Forest monitoring and education are key to forest protection, education and management, which is an effective way to measure the progress of a country's forest and climate commitments.","Due to the lack of a large-scale wild forest monitoring benchmark, the common practice is to train the model on a common outdoor benchmark (e.g., KITTI) and evaluate it on real forest datasets (e.g., CanaTree100).","However, there is a large domain gap in this setting, which makes the evaluation and deployment difficult.","In this paper, we propose a new photorealistic virtual forest dataset and a multimodal transformer-based algorithm for tree detection and instance segmentation.","To the best of our knowledge, it is the first time that a multimodal detection and segmentation algorithm is applied to large-scale forest scenes.","We believe that the proposed dataset and method will inspire the simulation, computer vision, education, and forestry communities towards a more comprehensive multi-modal understanding."],"url":"http://arxiv.org/abs/2402.04534v1","category":"cs.GR"}
{"created":"2024-02-07 01:57:39","title":"FLAGRED -- Fuzzy Logic-based Algorithm Generalizing Risk Estimation for Drones","abstract":"Accurately estimating risk in real-time is essential for ensuring the safety and efficiency of many applications involving autonomous robot systems. This paper presents a novel, generalizable algorithm for the real-time estimation of risks created by external disturbances on multirotors. Unlike conventional approaches, our method requires no additional sensors, accurate drone models, or large datasets. It employs motor command data in a fuzzy logic system, overcoming barriers to real-world implementation. Inherently adaptable, it utilizes fundamental drone characteristics, making it applicable to diverse drone models. The efficiency of the algorithm has been confirmed through comprehensive real-world testing on various platforms. It proficiently discerned between high and low-risk scenarios resulting from diverse wind disturbances and varying thrust-to-weight ratios. The algorithm surpassed the widely-recognized ArduCopter wind estimation algorithm in performance and demonstrated its capability to promptly detect brief gusts.","sentences":["Accurately estimating risk in real-time is essential for ensuring the safety and efficiency of many applications involving autonomous robot systems.","This paper presents a novel, generalizable algorithm for the real-time estimation of risks created by external disturbances on multirotors.","Unlike conventional approaches, our method requires no additional sensors, accurate drone models, or large datasets.","It employs motor command data in a fuzzy logic system, overcoming barriers to real-world implementation.","Inherently adaptable, it utilizes fundamental drone characteristics, making it applicable to diverse drone models.","The efficiency of the algorithm has been confirmed through comprehensive real-world testing on various platforms.","It proficiently discerned between high and low-risk scenarios resulting from diverse wind disturbances and varying thrust-to-weight ratios.","The algorithm surpassed the widely-recognized ArduCopter wind estimation algorithm in performance and demonstrated its capability to promptly detect brief gusts."],"url":"http://arxiv.org/abs/2402.04518v1","category":"cs.RO"}
{"created":"2024-02-07 01:49:03","title":"Generalized Sobolev Transport for Probability Measures on a Graph","abstract":"We study the optimal transport (OT) problem for measures supported on a graph metric space. Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation. However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures. In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function. An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \\emph{Orlicz geometric structure}. Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches. Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation. In this work, we leverage a specific class of convex functions for Orlicz structure to propose the generalized Sobolev transport (GST). GST encompasses the ST as its special case, and can be utilized for prior structures beyond the $L^p$ geometry. In connection with the OW, we show that one only needs to simply solve a univariate optimization problem to compute the GST, unlike the complex two-level optimization problem in OW. We empirically illustrate that GST is several-order faster than the OW. Moreover, we provide preliminary evidences on the advantages of GST for document classification and for several tasks in topological data analysis.","sentences":["We study the optimal transport (OT) problem for measures supported on a graph metric space.","Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation.","However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures.","In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function.","An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \\emph{Orlicz geometric structure}.","Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches.","Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation.","In this work, we leverage a specific class of convex functions for Orlicz structure to propose the generalized Sobolev transport (GST).","GST encompasses the ST as its special case, and can be utilized for prior structures beyond the $L^p$ geometry.","In connection with the OW, we show that one only needs to simply solve a univariate optimization problem to compute the GST, unlike the complex two-level optimization problem in OW.","We empirically illustrate that GST is several-order faster than the OW.","Moreover, we provide preliminary evidences on the advantages of GST for document classification and for several tasks in topological data analysis."],"url":"http://arxiv.org/abs/2402.04516v1","category":"stat.ML"}
{"created":"2024-02-07 01:46:50","title":"Online Cascade Learning for Efficient Inference over Streams","abstract":"Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a \"cascade\" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.","sentences":["Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks.","We propose online cascade learning, the first approach to addressing this challenge.","The objective here is to learn a \"cascade\" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input.","We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem.","Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing."],"url":"http://arxiv.org/abs/2402.04513v1","category":"cs.LG"}
{"created":"2024-02-07 01:18:49","title":"Text2Street: Controllable Text-to-image Generation for Street Views","abstract":"Text-to-image generation has made remarkable progress with the emergence of diffusion models. However, it is still a difficult task to generate images for street views based on text, mainly because the road topology of street scenes is complex, the traffic status is diverse and the weather condition is various, which makes conventional text-to-image models difficult to deal with. To address these challenges, we propose a novel controllable text-to-image framework, named \\textbf{Text2Street}. In the framework, we first introduce the lane-aware road topology generator, which achieves text-to-map generation with the accurate road structure and lane lines armed with the counting adapter, realizing the controllable road topology generation. Then, the position-based object layout generator is proposed to obtain text-to-layout generation through an object-level bounding box diffusion strategy, realizing the controllable traffic object layout generation. Finally, the multiple control image generator is designed to integrate the road topology, object layout and weather description to realize controllable street-view image generation. Extensive experiments show that the proposed approach achieves controllable street-view text-to-image generation and validates the effectiveness of the Text2Street framework for street views.","sentences":["Text-to-image generation has made remarkable progress with the emergence of diffusion models.","However, it is still a difficult task to generate images for street views based on text, mainly because the road topology of street scenes is complex, the traffic status is diverse and the weather condition is various, which makes conventional text-to-image models difficult to deal with.","To address these challenges, we propose a novel controllable text-to-image framework, named \\textbf{Text2Street}.","In the framework, we first introduce the lane-aware road topology generator, which achieves text-to-map generation with the accurate road structure and lane lines armed with the counting adapter, realizing the controllable road topology generation.","Then, the position-based object layout generator is proposed to obtain text-to-layout generation through an object-level bounding box diffusion strategy, realizing the controllable traffic object layout generation.","Finally, the multiple control image generator is designed to integrate the road topology, object layout and weather description to realize controllable street-view image generation.","Extensive experiments show that the proposed approach achieves controllable street-view text-to-image generation and validates the effectiveness of the Text2Street framework for street views."],"url":"http://arxiv.org/abs/2402.04504v1","category":"cs.CV"}
{"created":"2024-02-07 01:09:16","title":"Multiplicity of electron- and photon-seeded electromagnetic showers at multi-petawatt laser facilities","abstract":"Electromagnetic showers developing from the collision of an ultra-intense laser pulse with a beam of high-energy electrons or photons are investigated under conditions relevant to future experiments on multi-petawatt laser facilities. A semi-analytical model is derived that predicts the shower multiplicity, i.e. the number of pairs produced per incident seed particle (electron or gamma photon). The model is benchmarked against particle-in-cell simulations and shown to be accurate over a wide range of seed particle energies (100 MeV - 40 GeV), laser relativistic field strengths ($10 < a_0 < 1000$), and quantum parameter $\\chi_0$ (ranging from 1 to 40). It is shown that, for experiments expected in the next decade, only the first generations of pairs contribute to the shower while multiplicities larger than unity are predicted. Guidelines for forthcoming experiments are discussed considering laser facilities such as Apollon and ELI Beamlines. The difference between electron- and photon seeding and the influence of the laser pulse duration are investigated.","sentences":["Electromagnetic showers developing from the collision of an ultra-intense laser pulse with a beam of high-energy electrons or photons are investigated under conditions relevant to future experiments on multi-petawatt laser facilities.","A semi-analytical model is derived that predicts the shower multiplicity, i.e. the number of pairs produced per incident seed particle (electron or gamma photon).","The model is benchmarked against particle-in-cell simulations and shown to be accurate over a wide range of seed particle energies (100 MeV - 40 GeV), laser relativistic field strengths ($10 < a_0 < 1000$), and quantum parameter $\\chi_0$ (ranging from 1 to 40).","It is shown that, for experiments expected in the next decade, only the first generations of pairs contribute to the shower while multiplicities larger than unity are predicted.","Guidelines for forthcoming experiments are discussed considering laser facilities such as Apollon and ELI Beamlines.","The difference between electron- and photon seeding and the influence of the laser pulse duration are investigated."],"url":"http://arxiv.org/abs/2402.04501v1","category":"physics.plasm-ph"}
{"created":"2024-02-07 00:54:35","title":"Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data","abstract":"Kalman Filter (KF) is an optimal linear state prediction algorithm, with applications in fields as diverse as engineering, economics, robotics, and space exploration. Here, we develop an extension of the KF, called a Pathspace Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties associated with the underlying data and prior knowledge, and b) take as input an entire trajectory and an underlying mechanistic model, and using a Bayesian methodology quantify the different sources of uncertainty. An application of this algorithm is to automatically detect temporal windows where the internal mechanistic model deviates from the data in a time-dependent manner. First, we provide theorems characterizing the convergence of the PKF algorithm. Then, we numerically demonstrate that the PKF outperforms conventional KF methods on a synthetic dataset lowering the mean-squared-error by several orders of magnitude. Finally, we apply this method to biological time-course dataset involving over 1.8 million gene expression measurements.","sentences":["Kalman Filter (KF) is an optimal linear state prediction algorithm, with applications in fields as diverse as engineering, economics, robotics, and space exploration.","Here, we develop an extension of the KF, called a Pathspace Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties associated with the underlying data and prior knowledge, and b) take as input an entire trajectory and an underlying mechanistic model, and using a Bayesian methodology quantify the different sources of uncertainty.","An application of this algorithm is to automatically detect temporal windows where the internal mechanistic model deviates from the data in a time-dependent manner.","First, we provide theorems characterizing the convergence of the PKF algorithm.","Then, we numerically demonstrate that the PKF outperforms conventional KF methods on a synthetic dataset lowering the mean-squared-error by several orders of magnitude.","Finally, we apply this method to biological time-course dataset involving over 1.8 million gene expression measurements."],"url":"http://arxiv.org/abs/2402.04498v1","category":"stat.ML"}
{"created":"2024-02-07 00:24:10","title":"Item-Level Heterogeneous Treatment Effects of Selective Serotonin Reuptake Inhibitors (SSRIs) on Depression: Implications for Inference, Generalizability, and Identification","abstract":"In analysis of randomized controlled trials (RCTs) with patient-reported outcome measures (PROMs), Item Response Theory (IRT) models that allow for heterogeneity in the treatment effect at the item level merit consideration. These models for ``item-level heterogeneous treatment effects'' (IL-HTE) can provide more accurate statistical inference, allow researchers to better generalize their results, and resolve critical identification problems in the estimation of interaction effects. In this study, we extend the IL-HTE model to polytomous data and apply the model to determine how the effect of selective serotonin reuptake inhibitors (SSRIs) on depression varies across the items on a depression rating scale. We first conduct a Monte Carlo simulation study to assess the performance of the polytomous IL-HTE model under a range of conditions. We then apply the IL-HTE model to item-level data from 28 RCTs measuring the effect of SSRIs on depression using the 17-item Hamilton Depression Rating Scale (HDRS-17) and estimate potential heterogeneity by subscale (HDRS-6). Our results show that the IL-HTE model provides more accurate statistical inference, allows for generalizability of results to out-of-sample items, and resolves identification problems in the estimation of interaction effects. Our empirical application shows that while the average effect of SSRIs on depression is beneficial (i.e., negative) and statistically significant, there is substantial IL-HTE, with estimates of the standard deviation of item-level effects nearly as large as the average effect. We show that this substantial IL-HTE is driven primarily by systematically larger effects on the HDRS-6 subscale items. The IL-HTE model has the potential to provide new insights for the inference, generalizability, and identification of treatment effects in clinical trials using patient reported outcome measures.","sentences":["In analysis of randomized controlled trials (RCTs) with patient-reported outcome measures (PROMs), Item Response Theory (IRT) models that allow for heterogeneity in the treatment effect at the item level merit consideration.","These models for ``item-level heterogeneous treatment effects'' (IL-HTE) can provide more accurate statistical inference, allow researchers to better generalize their results, and resolve critical identification problems in the estimation of interaction effects.","In this study, we extend the IL-HTE model to polytomous data and apply the model to determine how the effect of selective serotonin reuptake inhibitors (SSRIs) on depression varies across the items on a depression rating scale.","We first conduct a Monte Carlo simulation study to assess the performance of the polytomous IL-HTE model under a range of conditions.","We then apply the IL-HTE model to item-level data from 28 RCTs measuring the effect of SSRIs on depression using the 17-item Hamilton Depression Rating Scale (HDRS-17) and estimate potential heterogeneity by subscale (HDRS-6).","Our results show that the IL-HTE model provides more accurate statistical inference, allows for generalizability of results to out-of-sample items, and resolves identification problems in the estimation of interaction effects.","Our empirical application shows that while the average effect of SSRIs on depression is beneficial (i.e., negative) and statistically significant, there is substantial IL-HTE, with estimates of the standard deviation of item-level effects nearly as large as the average effect.","We show that this substantial IL-HTE is driven primarily by systematically larger effects on the HDRS-6 subscale items.","The IL-HTE model has the potential to provide new insights for the inference, generalizability, and identification of treatment effects in clinical trials using patient reported outcome measures."],"url":"http://arxiv.org/abs/2402.04487v1","category":"stat.ME"}
{"created":"2024-02-07 00:14:32","title":"BEBLID: Boosted efficient binary local image descriptor","abstract":"Efficient matching of local image features is a fundamental task in many computer vision applications. However, the real-time performance of top matching algorithms is compromised in computationally limited devices, such as mobile phones or drones, due to the simplicity of their hardware and their finite energy supply. In this paper we introduce BEBLID, an efficient learned binary image descriptor. It improves our previous real-valued descriptor, BELID, making it both more efficient for matching and more accurate. To this end we use AdaBoost with an improved weak-learner training scheme that produces better local descriptions. Further, we binarize our descriptor by forcing all weak-learners to have the same weight in the strong learner combination and train it in an unbalanced data set to address the asymmetries arising in matching and retrieval tasks. In our experiments BEBLID achieves an accuracy close to SIFT and better computational efficiency than ORB, the fastest algorithm in the literature.","sentences":["Efficient matching of local image features is a fundamental task in many computer vision applications.","However, the real-time performance of top matching algorithms is compromised in computationally limited devices, such as mobile phones or drones, due to the simplicity of their hardware and their finite energy supply.","In this paper we introduce BEBLID, an efficient learned binary image descriptor.","It improves our previous real-valued descriptor, BELID, making it both more efficient for matching and more accurate.","To this end we use AdaBoost with an improved weak-learner training scheme that produces better local descriptions.","Further, we binarize our descriptor by forcing all weak-learners to have the same weight in the strong learner combination and train it in an unbalanced data set to address the asymmetries arising in matching and retrieval tasks.","In our experiments BEBLID achieves an accuracy close to SIFT and better computational efficiency than ORB, the fastest algorithm in the literature."],"url":"http://arxiv.org/abs/2402.04482v1","category":"cs.CV"}
{"created":"2024-02-06 23:45:20","title":"Quantum vortex phases of charged pion condensates induced by rotation in a magnetic field","abstract":"Using the relativistic complex scalar field model with a repulsive self-interaction, we discuss the ground state structure of charged pion condensation under the coexistence of parallel rotation and magnetic field. Our previous study found that the density distribution profile of the condensates is a supergiant quantum vortex phase and change with rotational speed and coupling constant. In this work, we further discover vortex lattice structures in the condensates under conditions of small rotation and strong coupling constant. This mechanism can be thought of as electrical superconductivity: Vortex lattices are created to better adapt to changes in rotation and interaction. Furthermore, large rotation and weak coupling constant are more likely to cause the vortex lattices to be destroyed and form a giant quantum vortex similar to a doughnut. We expect this phenomenon can be observed in the relativistic non-central heavy ion collisions with large rotation and strong magnetic field.","sentences":["Using the relativistic complex scalar field model with a repulsive self-interaction, we discuss the ground state structure of charged pion condensation under the coexistence of parallel rotation and magnetic field.","Our previous study found that the density distribution profile of the condensates is a supergiant quantum vortex phase and change with rotational speed and coupling constant.","In this work, we further discover vortex lattice structures in the condensates under conditions of small rotation and strong coupling constant.","This mechanism can be thought of as electrical superconductivity: Vortex lattices are created to better adapt to changes in rotation and interaction.","Furthermore, large rotation and weak coupling constant are more likely to cause the vortex lattices to be destroyed and form a giant quantum vortex similar to a doughnut.","We expect this phenomenon can be observed in the relativistic non-central heavy ion collisions with large rotation and strong magnetic field."],"url":"http://arxiv.org/abs/2402.04475v1","category":"hep-ph"}
{"created":"2024-02-06 23:41:28","title":"Healthcare Quality by Specialists under a Mixed Compensation System: an Empirical Analysis","abstract":"We analyze the effects of a mixed compensation (MC) scheme for specialists on the quality of their healthcare services. We exploit a reform implemented in Quebec (Canada) in 1999. The government introduced a payment mechanism combining a per diem with a reduced fee per clinical service. Using a large patient/physician panel dataset, we estimate a multi-state multi-spell hazard model analogous to a difference-in-differences approach. We compute quality indicators from our model. Our results suggest that the reform reduced the quality of MC specialist services measured by the risk of re-hospitalization and mortality after discharge. These effects vary across specialties.","sentences":["We analyze the effects of a mixed compensation (MC) scheme for specialists on the quality of their healthcare services.","We exploit a reform implemented in Quebec (Canada) in 1999.","The government introduced a payment mechanism combining a per diem with a reduced fee per clinical service.","Using a large patient/physician panel dataset, we estimate a multi-state multi-spell hazard model analogous to a difference-in-differences approach.","We compute quality indicators from our model.","Our results suggest that the reform reduced the quality of MC specialist services measured by the risk of re-hospitalization and mortality after discharge.","These effects vary across specialties."],"url":"http://arxiv.org/abs/2402.04472v1","category":"econ.GN"}
{"created":"2024-02-06 23:28:15","title":"IoT Network Traffic Analysis with Deep Learning","abstract":"As IoT networks become more complex and generate massive amounts of dynamic data, it is difficult to monitor and detect anomalies using traditional statistical methods and machine learning methods. Deep learning algorithms can process and learn from large amounts of data and can also be trained using unsupervised learning techniques, meaning they don't require labelled data to detect anomalies. This makes it possible to detect new and unknown anomalies that may not have been detected before. Also, deep learning algorithms can be automated and highly scalable; thereby, they can run continuously in the backend and make it achievable to monitor large IoT networks instantly. In this work, we conduct a literature review on the most recent works using deep learning techniques and implement a model using ensemble techniques on the KDD Cup 99 dataset. The experimental results showcase the impressive performance of our deep anomaly detection model, achieving an accuracy of over 98\\%.","sentences":["As IoT networks become more complex and generate massive amounts of dynamic data, it is difficult to monitor and detect anomalies using traditional statistical methods and machine learning methods.","Deep learning algorithms can process and learn from large amounts of data and can also be trained using unsupervised learning techniques, meaning they don't require labelled data to detect anomalies.","This makes it possible to detect new and unknown anomalies that may not have been detected before.","Also, deep learning algorithms can be automated and highly scalable; thereby, they can run continuously in the backend and make it achievable to monitor large IoT networks instantly.","In this work, we conduct a literature review on the most recent works using deep learning techniques and implement a model using ensemble techniques on the KDD Cup 99 dataset.","The experimental results showcase the impressive performance of our deep anomaly detection model, achieving an accuracy of over 98\\%."],"url":"http://arxiv.org/abs/2402.04469v1","category":"cs.LG"}
{"created":"2024-02-06 23:26:12","title":"DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems","abstract":"Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics. However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system. In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases. We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives. Our Dynamics Stable Learning by Invariant Measures (DySLIM) objective enables model training that achieves better point-wise tracking and long-term statistical accuracy relative to other learning objectives. By targeting the distribution with a scalable regularization term, we hope that this approach can be extended to more complex systems exhibiting slowly-variant distributions, such as weather and climate models.","sentences":["Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics.","However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system.","In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases.","We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives.","Our Dynamics Stable Learning by Invariant Measures (DySLIM) objective enables model training that achieves better point-wise tracking and long-term statistical accuracy relative to other learning objectives.","By targeting the distribution with a scalable regularization term, we hope that this approach can be extended to more complex systems exhibiting slowly-variant distributions, such as weather and climate models."],"url":"http://arxiv.org/abs/2402.04467v1","category":"cs.LG"}
{"created":"2024-02-06 23:18:29","title":"BAdaCost: Multi-class Boosting with Costs","abstract":"We present BAdaCost, a multi-class cost-sensitive classification algorithm. It combines a set of cost-sensitive multi-class weak learners to obtain a strong classification rule within the Boosting framework. To derive the algorithm we introduce CMEL, a Cost-sensitive Multi-class Exponential Loss that generalizes the losses optimized in various classification algorithms such as AdaBoost, SAMME, Cost-sensitive AdaBoost and PIBoost. Hence unifying them under a common theoretical framework. In the experiments performed we prove that BAdaCost achieves significant gains in performance when compared to previous multi-class cost-sensitive approaches. The advantages of the proposed algorithm in asymmetric multi-class classification are also evaluated in practical multi-view face and car detection problems.","sentences":["We present BAdaCost, a multi-class cost-sensitive classification algorithm.","It combines a set of cost-sensitive multi-class weak learners to obtain a strong classification rule within the Boosting framework.","To derive the algorithm we introduce CMEL, a Cost-sensitive Multi-class Exponential Loss that generalizes the losses optimized in various classification algorithms such as AdaBoost, SAMME, Cost-sensitive AdaBoost and PIBoost.","Hence unifying them under a common theoretical framework.","In the experiments performed we prove that BAdaCost achieves significant gains in performance when compared to previous multi-class cost-sensitive approaches.","The advantages of the proposed algorithm in asymmetric multi-class classification are also evaluated in practical multi-view face and car detection problems."],"url":"http://arxiv.org/abs/2402.04465v1","category":"cs.CV"}
{"created":"2024-02-06 23:07:36","title":"On Data Analysis Pipelines and Modular Bayesian Modeling","abstract":"Data analysis pipelines, where quantities estimated in upstream modules are used as inputs to downstream ones, are common in many application areas. The most common approach to implementing data analysis pipelines involves obtaining point estimates from the upstream module(s) and then treating these as known quantities when working with the downstream module(s). This approach is straightforward, but is likely to underestimate the overall uncertainty associated with any final estimates. An alternative approach involves estimating parameters from the modules jointly using a Bayesian hierarchical model, which has the advantage of propagating upstream uncertainty into the downstream estimates. However in the case where one of the modules suffers from misspecification, such a joint model can result in the misspecification from one module corrupting the estimates from the remaining modules. Furthermore, hierarchical models require the development of ad-hoc implementations that can be time consuming to create and require large amounts of computational effort. So-called cut inference modifies the posterior distribution in such a way that prevents information flow between certain modules and provides a third alternative for statistical inference in data analysis pipelines. This paper presents a unified framework that encompasses all three modeling approaches (two-step, cut, and joint) in the context of data analysis pipelines with two modules and uses two examples to illustrate the trade offs associated with these three approaches. Our work shows that cut inference provides both robustness and ease of implementation for data analysis pipelines at a lower cost in terms of statistical inference than two-step procedures.","sentences":["Data analysis pipelines, where quantities estimated in upstream modules are used as inputs to downstream ones, are common in many application areas.","The most common approach to implementing data analysis pipelines involves obtaining point estimates from the upstream module(s) and then treating these as known quantities when working with the downstream module(s).","This approach is straightforward, but is likely to underestimate the overall uncertainty associated with any final estimates.","An alternative approach involves estimating parameters from the modules jointly using a Bayesian hierarchical model, which has the advantage of propagating upstream uncertainty into the downstream estimates.","However in the case where one of the modules suffers from misspecification, such a joint model can result in the misspecification from one module corrupting the estimates from the remaining modules.","Furthermore, hierarchical models require the development of ad-hoc implementations that can be time consuming to create and require large amounts of computational effort.","So-called cut inference modifies the posterior distribution in such a way that prevents information flow between certain modules and provides a third alternative for statistical inference in data analysis pipelines.","This paper presents a unified framework that encompasses all three modeling approaches (two-step, cut, and joint) in the context of data analysis pipelines with two modules and uses two examples to illustrate the trade offs associated with these three approaches.","Our work shows that cut inference provides both robustness and ease of implementation for data analysis pipelines at a lower cost in terms of statistical inference than two-step procedures."],"url":"http://arxiv.org/abs/2402.04461v1","category":"stat.ME"}
{"created":"2024-02-06 23:07:26","title":"A novel two-field pure K-essence for inflation, dark matter, dark energy and black holes","abstract":"K-essence theories are usually studied in the framework of one scalar field $\\phi$. Namely, the Lagrangian of K-essence is the function of scalar field $\\phi$ and its covariant derivative. However, in this paper, we explore a two-field pure K-essence, i.e. the corresponding Lagrangian is the function of covariant derivatives of two scalar fields without the dependency of scalar fields themselves. That is why we call it pure K-essence. The novelty of this K-essence is that its Lagrangian contains the quotient term of the kinetic energies from the two scalar fields. This results in the presence of many interesting features, for example, the equation of state can be arbitrarily small and arbitrarily large. As a comparison, the range for equation of state of quintessence is from $-1$ to $+1$. Interestingly, this novel K-essence can play the role of inflation field, dark matter and dark energy. Finally, the absence of the scalar fields themselves in the equations of motion makes the study considerable simple such that even the exact black hole solutions can be found.","sentences":["K-essence theories are usually studied in the framework of one scalar field $\\phi$. Namely, the Lagrangian of K-essence is the function of scalar field $\\phi$ and its covariant derivative.","However, in this paper, we explore a two-field pure K-essence, i.e. the corresponding Lagrangian is the function of covariant derivatives of two scalar fields without the dependency of scalar fields themselves.","That is why we call it pure K-essence.","The novelty of this K-essence is that its Lagrangian contains the quotient term of the kinetic energies from the two scalar fields.","This results in the presence of many interesting features, for example, the equation of state can be arbitrarily small and arbitrarily large.","As a comparison, the range for equation of state of quintessence is from $-1$ to $+1$. Interestingly, this novel K-essence can play the role of inflation field, dark matter and dark energy.","Finally, the absence of the scalar fields themselves in the equations of motion makes the study considerable simple such that even the exact black hole solutions can be found."],"url":"http://arxiv.org/abs/2402.04460v1","category":"gr-qc"}
{"created":"2024-02-06 22:55:14","title":"On complete trapped submanifolds in globally hyperbolic spacetimes","abstract":"The aim of this manuscript is to obtain rigidity and non-existence results for parabolic spacelike submanifolds with causal mean curvature vector field in orthogonally splitted spacetimes, and in particular, in globally hyperbolic spacetimes. We also obtain results regarding the geometry of submanifolds by ensuring, under some mild hypothesis, the non-existence of local minima or maxima of certain distinguished function. Furthermore, in this last case the submanifold does not need to be parabolic or even complete.   As an application in General Relativity, we obtain several nice results regarding (non-necessarily closed) trapped surfaces in a huge family of spacetimes. In fact, we show how our technique allows us to recover some relevant previous results for trapped surfaces in both, standard static spacetimes and Generalized Robertson-Walker spacetimes.","sentences":["The aim of this manuscript is to obtain rigidity and non-existence results for parabolic spacelike submanifolds with causal mean curvature vector field in orthogonally splitted spacetimes, and in particular, in globally hyperbolic spacetimes.","We also obtain results regarding the geometry of submanifolds by ensuring, under some mild hypothesis, the non-existence of local minima or maxima of certain distinguished function.","Furthermore, in this last case the submanifold does not need to be parabolic or even complete.   ","As an application in General Relativity, we obtain several nice results regarding (non-necessarily closed) trapped surfaces in a huge family of spacetimes.","In fact, we show how our technique allows us to recover some relevant previous results for trapped surfaces in both, standard static spacetimes and Generalized Robertson-Walker spacetimes."],"url":"http://arxiv.org/abs/2402.04458v1","category":"math.DG"}
{"created":"2024-02-06 22:44:27","title":"Evolving Mobile Cloud Gaming with 5G Standalone Network Telemetry","abstract":"Mobile cloud gaming places the simultaneous demands of high capacity and low latency on the wireless network, demands that Private and Metropolitan-Area Standalone 5G networks are poised to meet. However, lacking introspection into the 5G Radio Access Network (RAN), cloud gaming servers are ill-poised to cope with the vagaries of the wireless last hop to a mobile client, while 5G network operators run mostly closed networks, limiting their potential for co-design with the wider internet and user applications. This paper presents Telesa, a passive, incrementally-deployable, and independently-deployable Standalone 5G network telemetry system that streams fine-grained RAN capacity, latency, and retransmission information to application servers to enable better millisecond scale, application-level decisions on offered load and bit rate adaptation than end-to-end latency measurements or end-to-end packet losses currently permit. We design, implement, and evaluate a Telesa telemetry-enhanced game streaming platform, demonstrating exact congestion-control that can better adapt game video bitrate while simultaneously controlling end-to-end latency, thus maximizing game quality of experience. Our experimental evaluation on a production 5G Standalone network demonstrates a 178-249% Quality of Experience improvement versus two state-of-the-art cloud gaming applications.","sentences":["Mobile cloud gaming places the simultaneous demands of high capacity and low latency on the wireless network, demands that Private and Metropolitan-Area","Standalone 5G networks are poised to meet.","However, lacking introspection into the 5G Radio Access Network (RAN), cloud gaming servers are ill-poised to cope with the vagaries of the wireless last hop to a mobile client, while 5G network operators run mostly closed networks, limiting their potential for co-design with the wider internet and user applications.","This paper presents Telesa, a passive, incrementally-deployable, and independently-deployable Standalone 5G network telemetry system that streams fine-grained RAN capacity, latency, and retransmission information to application servers to enable better millisecond scale, application-level decisions on offered load and bit rate adaptation than end-to-end latency measurements or end-to-end packet losses currently permit.","We design, implement, and evaluate a Telesa telemetry-enhanced game streaming platform, demonstrating exact congestion-control that can better adapt game video bitrate while simultaneously controlling end-to-end latency, thus maximizing game quality of experience.","Our experimental evaluation on a production 5G Standalone network demonstrates a 178-249% Quality of Experience improvement versus two state-of-the-art cloud gaming applications."],"url":"http://arxiv.org/abs/2402.04454v1","category":"cs.NI"}
{"created":"2024-02-06 22:26:42","title":"Equitable Networked Microgrid Topology Reconfiguration for Wildfire Risk Mitigation","abstract":"Increasing amount of wildfires in recent years consistently challenges the safe and reliable operations of power systems. To prevent power lines and other electrical components from causing wildfires under extreme conditions, electric utilities often deploy public safety power shutoffs (PSPS) to mitigate the wildfire risks therein. Although PSPS are effective countermeasures against wildfires, uncoordinated strategies can cause disruptions in electricity supply and even lead to cascading failures. Meanwhile, it is extremely important to consider mitigating biased decisions on different communities and populations during the implementation of shutoff actions. In this work, we primarily focus on the dynamic reconfiguration problem of networked microgrids with distributed energy resources. In particular, we formulate a rolling horizon optimization problem allowing for flexible network reconfiguration at each time interval to mitigate wildfire risks. To promote equity and fairness during the span of shutoffs, we further enforce a range of constraints associated with load shedding to discourage disproportionate impact on individual load blocks. Numerical studies on the modified IEEE 13-bus system and a larger-sized Smart-DS system demonstrate the performance of the proposed algorithm towards more equitable power shutoff operations.","sentences":["Increasing amount of wildfires in recent years consistently challenges the safe and reliable operations of power systems.","To prevent power lines and other electrical components from causing wildfires under extreme conditions, electric utilities often deploy public safety power shutoffs (PSPS) to mitigate the wildfire risks therein.","Although PSPS are effective countermeasures against wildfires, uncoordinated strategies can cause disruptions in electricity supply and even lead to cascading failures.","Meanwhile, it is extremely important to consider mitigating biased decisions on different communities and populations during the implementation of shutoff actions.","In this work, we primarily focus on the dynamic reconfiguration problem of networked microgrids with distributed energy resources.","In particular, we formulate a rolling horizon optimization problem allowing for flexible network reconfiguration at each time interval to mitigate wildfire risks.","To promote equity and fairness during the span of shutoffs, we further enforce a range of constraints associated with load shedding to discourage disproportionate impact on individual load blocks.","Numerical studies on the modified IEEE 13-bus system and a larger-sized Smart-DS system demonstrate the performance of the proposed algorithm towards more equitable power shutoff operations."],"url":"http://arxiv.org/abs/2402.04444v1","category":"eess.SY"}
{"created":"2024-02-06 22:22:43","title":"Galois invariants of finite abelian descent and Brauer sets","abstract":"For a variety over a global field, one can consider subsets of the set of adelic points of the variety cut out by finite abelian descent or Brauer-Manin obstructions. Given a Galois extension of the ground field one can consider similar sets over the extension and take Galois invariants. In this paper, we study under which circumstances the Galois invariants recover the obstruction sets over the ground field. As an application of our results, we study finite abelian descent and Brauer-Manin obstructions for isotrivial curves over function fields and extend results obtained by the first and last authors for constant curves to the isotrivial case.","sentences":["For a variety over a global field, one can consider subsets of the set of adelic points of the variety cut out by finite abelian descent or Brauer-Manin obstructions.","Given a Galois extension of the ground field one can consider similar sets over the extension and take Galois invariants.","In this paper, we study under which circumstances the Galois invariants recover the obstruction sets over the ground field.","As an application of our results, we study finite abelian descent and Brauer-Manin obstructions for isotrivial curves over function fields and extend results obtained by the first and last authors for constant curves to the isotrivial case."],"url":"http://arxiv.org/abs/2402.04441v1","category":"math.NT"}
{"created":"2024-02-06 22:13:48","title":"Upper efficiency limit of Sb2Se3 solar cells","abstract":"Antimony selenide (Sb2Se3) is at the forefront of an emerging class of sustainable photovoltaic materials. Despite notable developments over the past decade, the light-to-electricity conversion efficiency of Sb2Se3 has reached a plateau of ~10%. Is this an intrinsic limitation of the material or is there scope to rival the success of metal halide perovskite solar cells? Here we assess the trap-limited conversion efficiency of Sb2Se3. First-principles defect analysis of the hole and electron capture rates for point defects demonstrates the critical role of vacancies as active recombination centres. We predict an upper limit of 25% efficiency in Sb2Se3 grown under optimal equilibrium conditions where the concentrations of charged vacancies are minimised. We further reveal how the detrimental effect of Se vacancies can be reduced by extrinsic oxygen passivation, highlighting a pathway to achieve high-performance metal selenide solar cells close to the thermodynamic limit.","sentences":["Antimony selenide (Sb2Se3) is at the forefront of an emerging class of sustainable photovoltaic materials.","Despite notable developments over the past decade, the light-to-electricity conversion efficiency of Sb2Se3 has reached a plateau of ~10%.","Is this an intrinsic limitation of the material or is there scope to rival the success of metal halide perovskite solar cells?","Here we assess the trap-limited conversion efficiency of Sb2Se3.","First-principles defect analysis of the hole and electron capture rates for point defects demonstrates the critical role of vacancies as active recombination centres.","We predict an upper limit of 25% efficiency in Sb2Se3 grown under optimal equilibrium conditions where the concentrations of charged vacancies are minimised.","We further reveal how the detrimental effect of Se vacancies can be reduced by extrinsic oxygen passivation, highlighting a pathway to achieve high-performance metal selenide solar cells close to the thermodynamic limit."],"url":"http://arxiv.org/abs/2402.04434v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-06 22:12:14","title":"Fast Online Changepoint Detection","abstract":"We study online changepoint detection in the context of a linear regression model. We propose a class of heavily weighted statistics based on the CUSUM process of the regression residuals, which are specifically designed to ensure timely detection of breaks occurring early on during the monitoring horizon. We subsequently propose a class of composite statistics, constructed using different weighing schemes; the decision rule to mark a changepoint is based on the largest statistic across the various weights, thus effectively working like a veto-based voting mechanism, which ensures fast detection irrespective of the location of the changepoint. Our theory is derived under a very general form of weak dependence, thus being able to apply our tests to virtually all time series encountered in economics, medicine, and other applied sciences. Monte Carlo simulations show that our methodologies are able to control the procedure-wise Type I Error, and have short detection delays in the presence of breaks.","sentences":["We study online changepoint detection in the context of a linear regression model.","We propose a class of heavily weighted statistics based on the CUSUM process of the regression residuals, which are specifically designed to ensure timely detection of breaks occurring early on during the monitoring horizon.","We subsequently propose a class of composite statistics, constructed using different weighing schemes; the decision rule to mark a changepoint is based on the largest statistic across the various weights, thus effectively working like a veto-based voting mechanism, which ensures fast detection irrespective of the location of the changepoint.","Our theory is derived under a very general form of weak dependence, thus being able to apply our tests to virtually all time series encountered in economics, medicine, and other applied sciences.","Monte Carlo simulations show that our methodologies are able to control the procedure-wise Type I Error, and have short detection delays in the presence of breaks."],"url":"http://arxiv.org/abs/2402.04433v1","category":"stat.ME"}
{"created":"2024-02-06 22:11:01","title":"Comprehensive Forecasting of California's Energy Consumption: A Multi-Source and Sectoral Analysis Using ARIMA and ARIMAX Models","abstract":"California's significant role as the second-largest consumer of energy in the United States underscores the importance of accurate energy consumption predictions. With a thriving industrial sector, a burgeoning population, and ambitious environmental goals, the state's energy landscape is dynamic and complex. This paper presents a comprehensive analysis of California's energy consumption trends and provides detailed forecasting models for different energy sources and sectors. The study leverages ARIMA and ARIMAX models, considering both historical consumption data and exogenous variables. We address the unique challenges posed by the COVID-19 pandemic and the limited data for 2022, highlighting the resilience of these models in the face of uncertainty. Our analysis reveals that while fossil fuels continue to dominate California's energy landscape, renewable energy sources, particularly solar and biomass, are experiencing substantial growth. Hydroelectric power, while sensitive to precipitation, remains a significant contributor to renewable energy consumption. Furthermore, we anticipate ongoing efforts to reduce fossil fuel consumption. The forecasts for energy consumption by sector suggest continued growth in the commercial and residential sectors, reflecting California's expanding economy and population. In contrast, the industrial sector is expected to experience more moderate changes, while the transportation sector remains the largest energy consumer.","sentences":["California's significant role as the second-largest consumer of energy in the United States underscores the importance of accurate energy consumption predictions.","With a thriving industrial sector, a burgeoning population, and ambitious environmental goals, the state's energy landscape is dynamic and complex.","This paper presents a comprehensive analysis of California's energy consumption trends and provides detailed forecasting models for different energy sources and sectors.","The study leverages ARIMA and ARIMAX models, considering both historical consumption data and exogenous variables.","We address the unique challenges posed by the COVID-19 pandemic and the limited data for 2022, highlighting the resilience of these models in the face of uncertainty.","Our analysis reveals that while fossil fuels continue to dominate California's energy landscape, renewable energy sources, particularly solar and biomass, are experiencing substantial growth.","Hydroelectric power, while sensitive to precipitation, remains a significant contributor to renewable energy consumption.","Furthermore, we anticipate ongoing efforts to reduce fossil fuel consumption.","The forecasts for energy consumption by sector suggest continued growth in the commercial and residential sectors, reflecting California's expanding economy and population.","In contrast, the industrial sector is expected to experience more moderate changes, while the transportation sector remains the largest energy consumer."],"url":"http://arxiv.org/abs/2402.04432v1","category":"stat.AP"}
{"created":"2024-02-06 22:04:37","title":"Field-dependent magnetic relaxation times of magnetic nanoparticle systems: analytic approximations supported by numerical simulations","abstract":"Many estimates for the magnetic relaxation time of magnetic nanoparticle systems neglect the effect of the applied field strength. This is despite many applications of magnetic nanoparticles involving relaxation dynamics under the influence of applied fields. Here, an analytic approximation for the field-dependent Brownian relaxation time of single-domain, spherical magnetic nanoparticles in an external applied field is developed mathematically. This expression is validated by comparison with existing empirically-derived expressions and by comparison to particle-level simulations that allow particle rotations. Our approximation works particularly well for larger particles. We then use the developed expression to analytically calculate the total magnetic relaxation time when both Brownian and N\\'eel relaxation mechanisms are at play. Again, we show that the results match those found using particle-level simulations, this time with both particle rotations and internal magnetization dynamics allowed. However, for some particle parameters and for large field strengths, our simulations reveal that the Brownian and N\\'eel relaxation mechanisms are decoupled and it is not appropriate to combine these to calculate a total relaxation time.","sentences":["Many estimates for the magnetic relaxation time of magnetic nanoparticle systems neglect the effect of the applied field strength.","This is despite many applications of magnetic nanoparticles involving relaxation dynamics under the influence of applied fields.","Here, an analytic approximation for the field-dependent Brownian relaxation time of single-domain, spherical magnetic nanoparticles in an external applied field is developed mathematically.","This expression is validated by comparison with existing empirically-derived expressions and by comparison to particle-level simulations that allow particle rotations.","Our approximation works particularly well for larger particles.","We then use the developed expression to analytically calculate the total magnetic relaxation time when both Brownian and N\\'eel relaxation mechanisms are at play.","Again, we show that the results match those found using particle-level simulations, this time with both particle rotations and internal magnetization dynamics allowed.","However, for some particle parameters and for large field strengths, our simulations reveal that the Brownian and N\\'eel relaxation mechanisms are decoupled and it is not appropriate to combine these to calculate a total relaxation time."],"url":"http://arxiv.org/abs/2402.04427v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-06 21:53:31","title":"Optimal Binary Signaling for a Two Sensor Gaussian MAC Network","abstract":"We consider a two sensor distributed detection system transmitting a binary non-uniform source over a Gaussian multiple access channel (MAC). We model the network via binary sensors whose outputs are generated by binary symmetric channels of different noise levels. We prove an optimal one dimensional constellation design under individual sensor power constraints which minimizes the error probability of detecting the source. Three distinct cases arise for this optimization based on the parameters in the problem setup. In the most notable case (Case III), the optimal signaling design is to not necessarily use all of the power allocated to the more noisy sensor (with less correlation to the source). We compare the error performance of the optimal one dimensional constellation to orthogonal signaling. The results show that the optimal one dimensional constellation achieves lower error probability than using orthogonal channels.","sentences":["We consider a two sensor distributed detection system transmitting a binary non-uniform source over a Gaussian multiple access channel (MAC).","We model the network via binary sensors whose outputs are generated by binary symmetric channels of different noise levels.","We prove an optimal one dimensional constellation design under individual sensor power constraints which minimizes the error probability of detecting the source.","Three distinct cases arise for this optimization based on the parameters in the problem setup.","In the most notable case (Case III), the optimal signaling design is to not necessarily use all of the power allocated to the more noisy sensor (with less correlation to the source).","We compare the error performance of the optimal one dimensional constellation to orthogonal signaling.","The results show that the optimal one dimensional constellation achieves lower error probability than using orthogonal channels."],"url":"http://arxiv.org/abs/2402.04424v1","category":"cs.IT"}
{"created":"2024-02-06 21:48:39","title":"Smart Pipe System for a Shipyard 4.0","abstract":"As a result of the progressive implantation of the Industry 4.0 paradigm, many industries are experimenting a revolution that shipyards cannot ignore. Therefore, the application of the principles of Industry 4.0 to shipyards are leading to the creation of Shipyards 4.0. Due to this, Navantia, one of the 10 largest shipbuilders in the world, is updating its whole inner workings to keep up with the near-future challenges that a Shipyard 4.0 will have to face. Such challenges can be divided into three groups: the vertical integration of production systems, the horizontal integration of a new generation of value creation networks, and the re-engineering of the entire production chain, making changes that affect the entire life cycle of each piece of a ship. Pipes, which exist in a huge number and varied typology on a ship, are one of the key pieces, and its monitoring constitutes a prospective cyber-physical system. Their improved identification, traceability, and indoor location, from production and through their life, can enhance shipyard productivity and safety. In order to perform such tasks, this article first conducts a thorough analysis of the shipyard environment. From this analysis, the essential hardware and software technical requirements are determined. Next, the concept of smart pipe is presented and defined as an object able to transmit signals periodically that allows for providing enhanced services in a shipyard. In order to build a smart pipe system, different technologies are selected and evaluated, concluding that passive and active RFID are currently the most appropriate technologies to create it. Furthermore, some promising indoor positioning results obtained in a pipe workshop are presented, showing that multi-antenna algorithms and Kalman filtering can help to stabilize Received Signal Strength (RSS) and improve the overall accuracy of the system.","sentences":["As a result of the progressive implantation of the Industry 4.0 paradigm, many industries are experimenting a revolution that shipyards cannot ignore.","Therefore, the application of the principles of Industry 4.0 to shipyards are leading to the creation of Shipyards 4.0.","Due to this, Navantia, one of the 10 largest shipbuilders in the world, is updating its whole inner workings to keep up with the near-future challenges that a Shipyard 4.0 will have to face.","Such challenges can be divided into three groups: the vertical integration of production systems, the horizontal integration of a new generation of value creation networks, and the re-engineering of the entire production chain, making changes that affect the entire life cycle of each piece of a ship.","Pipes, which exist in a huge number and varied typology on a ship, are one of the key pieces, and its monitoring constitutes a prospective cyber-physical system.","Their improved identification, traceability, and indoor location, from production and through their life, can enhance shipyard productivity and safety.","In order to perform such tasks, this article first conducts a thorough analysis of the shipyard environment.","From this analysis, the essential hardware and software technical requirements are determined.","Next, the concept of smart pipe is presented and defined as an object able to transmit signals periodically that allows for providing enhanced services in a shipyard.","In order to build a smart pipe system, different technologies are selected and evaluated, concluding that passive and active RFID are currently the most appropriate technologies to create it.","Furthermore, some promising indoor positioning results obtained in a pipe workshop are presented, showing that multi-antenna algorithms and Kalman filtering can help to stabilize Received Signal Strength (RSS) and improve the overall accuracy of the system."],"url":"http://arxiv.org/abs/2402.04423v1","category":"eess.SY"}
{"created":"2024-02-06 21:34:18","title":"A Survey of Offline and Online Learning-Based Algorithms for Multirotor UAVs","abstract":"Multirotor UAVs are used for a wide spectrum of civilian and public domain applications. Navigation controllers endowed with different attributes and onboard sensor suites enable multirotor autonomous or semi-autonomous, safe flight, operation, and functionality under nominal and detrimental conditions and external disturbances, even when flying in uncertain and dynamically changing environments. During the last decade, given the faster-than-exponential increase of available computational power, different learning-based algorithms have been derived, implemented, and tested to navigate and control, among other systems, multirotor UAVs. Learning algorithms have been, and are used to derive data-driven based models, to identify parameters, to track objects, to develop navigation controllers, and to learn the environment in which multirotors operate. Learning algorithms combined with model-based control techniques have been proven beneficial when applied to multirotors. This survey summarizes published research since 2015, dividing algorithms, techniques, and methodologies into offline and online learning categories, and then, further classifying them into machine learning, deep learning, and reinforcement learning sub-categories. An integral part and focus of this survey are on online learning algorithms as applied to multirotors with the aim to register the type of learning techniques that are either hard or almost hard real-time implementable, as well as to understand what information is learned, why, and how, and how fast. The outcome of the survey offers a clear understanding of the recent state-of-the-art and of the type and kind of learning-based algorithms that may be implemented, tested, and executed in real-time.","sentences":["Multirotor UAVs are used for a wide spectrum of civilian and public domain applications.","Navigation controllers endowed with different attributes and onboard sensor suites enable multirotor autonomous or semi-autonomous, safe flight, operation, and functionality under nominal and detrimental conditions and external disturbances, even when flying in uncertain and dynamically changing environments.","During the last decade, given the faster-than-exponential increase of available computational power, different learning-based algorithms have been derived, implemented, and tested to navigate and control, among other systems, multirotor UAVs.","Learning algorithms have been, and are used to derive data-driven based models, to identify parameters, to track objects, to develop navigation controllers, and to learn the environment in which multirotors operate.","Learning algorithms combined with model-based control techniques have been proven beneficial when applied to multirotors.","This survey summarizes published research since 2015, dividing algorithms, techniques, and methodologies into offline and online learning categories, and then, further classifying them into machine learning, deep learning, and reinforcement learning sub-categories.","An integral part and focus of this survey are on online learning algorithms as applied to multirotors with the aim to register the type of learning techniques that are either hard or almost hard real-time implementable, as well as to understand what information is learned, why, and how, and how fast.","The outcome of the survey offers a clear understanding of the recent state-of-the-art and of the type and kind of learning-based algorithms that may be implemented, tested, and executed in real-time."],"url":"http://arxiv.org/abs/2402.04418v1","category":"cs.RO"}
{"created":"2024-02-06 21:29:37","title":"A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data","abstract":"Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space. For this multimodal UDG setting, we propose a novel method to build a small ($<$100K) subset of the source data in three simple steps: (1) diversified retrieval using label names as queries, (2) rank pseudo-labeling, and (3) clustering to find representative samples. To demonstrate the value of studying the multimodal UDG problem, we compare our results against state-of-the-art source-free DG and zero-shot (ZS) methods on their respective benchmarks and show up to 10% improvement in accuracy on 20 diverse target datasets. Additionally, our multi-stage dataset construction method achieves 3% improvement on average over nearest neighbors retrieval. Code is available: https://github.com/Chris210634/mudg","sentences":["Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces.","However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive.","For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning.","Our framework does not explicitly assume any relationship between the source dataset and target task.","Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space.","For this multimodal UDG setting, we propose a novel method to build a small ($<$100K) subset of the source data in three simple steps: (1) diversified retrieval using label names as queries, (2) rank pseudo-labeling, and (3) clustering to find representative samples.","To demonstrate the value of studying the multimodal UDG problem, we compare our results against state-of-the-art source-free DG and zero-shot (ZS) methods on their respective benchmarks and show up to 10% improvement in accuracy on 20 diverse target datasets.","Additionally, our multi-stage dataset construction method achieves 3% improvement on average over nearest neighbors retrieval.","Code is available: https://github.com/Chris210634/mudg"],"url":"http://arxiv.org/abs/2402.04416v1","category":"cs.CV"}
{"created":"2024-02-06 21:05:37","title":"Regularized MIP Model for Optimal Power Flow with Energy Storage Systems and its Applications","abstract":"Incorporating energy storage systems (ESS) into power systems has been studied in many recent works, where binary variables are often introduced to model the complementary nature of battery charging and discharging. A conventional approach for these ESS optimization problems is to relax binary variables and convert the problem into a linear program. However, such linear programming relaxation models can yield unrealistic fractional solutions, such as simultaneous charging and discharging. In this paper, we develop a regularized Mixed-Integer Programming (MIP) model for the ESS optimal power flow (OPF) problem. We prove that under mild conditions, the proposed regularized model admits a zero integrality gap with its linear programming relaxation; hence, it can be solved efficiently. By studying the properties of the regularized MIP model, we show that its optimal solution is also near-optimal to the original ESS OPF problem, thereby providing a valid and tight upper bound for the ESS OPF problem. The use of the regularized MIP model allows us to solve two intractable problems: a two-stage stochastic ESS OPF problem and a trilevel network contingency problem.","sentences":["Incorporating energy storage systems (ESS) into power systems has been studied in many recent works, where binary variables are often introduced to model the complementary nature of battery charging and discharging.","A conventional approach for these ESS optimization problems is to relax binary variables and convert the problem into a linear program.","However, such linear programming relaxation models can yield unrealistic fractional solutions, such as simultaneous charging and discharging.","In this paper, we develop a regularized Mixed-Integer Programming (MIP) model for the ESS optimal power flow (OPF) problem.","We prove that under mild conditions, the proposed regularized model admits a zero integrality gap with its linear programming relaxation; hence, it can be solved efficiently.","By studying the properties of the regularized MIP model, we show that its optimal solution is also near-optimal to the original ESS OPF problem, thereby providing a valid and tight upper bound for the ESS OPF problem.","The use of the regularized MIP model allows us to solve two intractable problems: a two-stage stochastic ESS OPF problem and a trilevel network contingency problem."],"url":"http://arxiv.org/abs/2402.04406v1","category":"math.OC"}
{"created":"2024-02-06 20:44:25","title":"Novel Methods for Load Estimation in Cell Switching in HAPS-Assisted Sustainable 6G Networks","abstract":"In the evolving landscape of vertical heterogeneous networks, the practice of cell switching particularly for small base stations faces a significant challenge due to the lack of accurate data on the traffic load of sleeping SBSs. This information gap is crucial as it hinders the feasibility and applicability of existing power consumption optimization methods; however, the studies in the literature predominantly assume perfect knowledge about the traffic load of sleeping SBSs. Addressing this critical issue, our study introduces innovative methodologies for estimating the traffic load of sleeping SBSs in a vHetNet including the integration of a high altitude platform as a super macro base station into the terrestrial network. We propose three distinct spatial interpolation-based estimation schemes: clustering-based, distance based, and random neighboring selection. Employing a real data set for empirical validations, we compare the estimation performance of the developed traffic load estimation schemes and assess the impact of estimation errors. Our findings demonstrate that accurate estimation of sleeping SBSs' traffic loads is essential for making network power consumption optimization methods both feasible and applicable in vHetNets.","sentences":["In the evolving landscape of vertical heterogeneous networks, the practice of cell switching particularly for small base stations faces a significant challenge due to the lack of accurate data on the traffic load of sleeping SBSs.","This information gap is crucial as it hinders the feasibility and applicability of existing power consumption optimization methods; however, the studies in the literature predominantly assume perfect knowledge about the traffic load of sleeping SBSs.","Addressing this critical issue, our study introduces innovative methodologies for estimating the traffic load of sleeping SBSs in a vHetNet including the integration of a high altitude platform as a super macro base station into the terrestrial network.","We propose three distinct spatial interpolation-based estimation schemes: clustering-based, distance based, and random neighboring selection.","Employing a real data set for empirical validations, we compare the estimation performance of the developed traffic load estimation schemes and assess the impact of estimation errors.","Our findings demonstrate that accurate estimation of sleeping SBSs' traffic loads is essential for making network power consumption optimization methods both feasible and applicable in vHetNets."],"url":"http://arxiv.org/abs/2402.04386v1","category":"cs.NI"}
{"created":"2024-02-06 20:43:00","title":"FairWire: Fair Graph Generation","abstract":"Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for the deployment of them in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on the structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generation models that is brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model. Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs.","sentences":["Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems.","However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for the deployment of them in real-world decision systems.","In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on the structural bias has not yet been investigated.","Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs.","Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations.","To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use.","Faced with the bias amplification in graph generation models that is brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model.","Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs."],"url":"http://arxiv.org/abs/2402.04383v1","category":"cs.LG"}
{"created":"2024-02-06 20:38:51","title":"Biomimetic hydrogel based on HET-s amylo\u00efd fibers for long-term culture of primary hippocampal neurons","abstract":"Historically, amyloid fibers (AF) in research has always been linked to degenerative diseases. However, HET-s AF, by their morphology and function, have only little in common to pathogenic amyloid fibers such as {\\alpha}-synuclein or a\\b{eta} and they have appeared as promising candidate for biocoating since few years. Here we have shown than HET-s amyloid fibers hydrogel is an extremely polyvalent coating material for the in vitro culture of primary hippocampal neurons. First, the non-cytotoxicity was demonstrated in vitro using standardized ISO protocols. Then, it is shown that in vitro culture of primary hippocampal neurons on HET-s AF hydrogels could last more than 45 days with clear signatures of spontaneous network activity, with which is a feat that not many other coatings have achieved yet. Finally, interactions between the cells, the dendrites and the hydrogels are highlighted, showing that dendrites might be able to penetrate the hydrogels in depth, therefore allowing recordings even within micrometer-thick hydrogels. In the end, those properties combined with group functionalization using standard biochemistry techniques, makes HET-s hydrogels ideal candidates to be used for the long-term growth of neurons as well as other types of cells. This versatility and easiness to use are definitely still unheard, especially for protein material. Due to its ability to transform from dry films to hydrogel when in contact with the extracellular matrix (ECM), it could also be used for in vivo implants, solving the issue of hydrogel damaging during the implant surgery.","sentences":["Historically, amyloid fibers (AF) in research has always been linked to degenerative diseases.","However, HET-s AF, by their morphology and function, have only little in common to pathogenic amyloid fibers such as {\\alpha}-synuclein or a\\b{eta} and they have appeared as promising candidate for biocoating since few years.","Here we have shown than HET-s amyloid fibers hydrogel is an extremely polyvalent coating material for the in vitro culture of primary hippocampal neurons.","First, the non-cytotoxicity was demonstrated in vitro using standardized ISO protocols.","Then, it is shown that in vitro culture of primary hippocampal neurons on HET-s AF hydrogels could last more than 45 days with clear signatures of spontaneous network activity, with which is a feat that not many other coatings have achieved yet.","Finally, interactions between the cells, the dendrites and the hydrogels are highlighted, showing that dendrites might be able to penetrate the hydrogels in depth, therefore allowing recordings even within micrometer-thick hydrogels.","In the end, those properties combined with group functionalization using standard biochemistry techniques, makes HET-s hydrogels ideal candidates to be used for the long-term growth of neurons as well as other types of cells.","This versatility and easiness to use are definitely still unheard, especially for protein material.","Due to its ability to transform from dry films to hydrogel when in contact with the extracellular matrix (ECM), it could also be used for in vivo implants, solving the issue of hydrogel damaging during the implant surgery."],"url":"http://arxiv.org/abs/2402.04381v1","category":"physics.bio-ph"}
{"created":"2024-02-06 20:00:11","title":"Resonance Cascades and Number Theory","abstract":"In this article, we are interested in situations where the existence of a contiguous cascade of quantum resonant transitions is predicated on the validity of a particular statement in number theory. Unexpectedly, the principal challenge was posed by the design of the perturbation potential: as we have reported elsewhere, a non-uniform distribution of the transition matrix elements leads to a localization that arrests mobility. A significant portion of our paper is devoted to ensuring that uniformity.   As a case study, we look at the following trivial statement: \"Any power of $3$ is an integer.\" Consequently, we \"test\" this statement in a numerical experiment where we demonstrate an un-impeded upward mobility along an equidistant, $\\ln(3)$-spaced subsequence of the energy levels of a potential with a log-natural spectrum, under a frequency $\\ln(3)$ time-periodic perturbation. We further show when we \"remove\" $9$ from the set of integers -- by excluding the corresponding energy level from the spectrum -- the cascade halts abruptly.","sentences":["In this article, we are interested in situations where the existence of a contiguous cascade of quantum resonant transitions is predicated on the validity of a particular statement in number theory.","Unexpectedly, the principal challenge was posed by the design of the perturbation potential: as we have reported elsewhere, a non-uniform distribution of the transition matrix elements leads to a localization that arrests mobility.","A significant portion of our paper is devoted to ensuring that uniformity.   ","As a case study, we look at the following trivial statement: \"Any power of $3$ is an integer.\"","Consequently, we \"test\" this statement in a numerical experiment where we demonstrate an un-impeded upward mobility along an equidistant, $\\ln(3)$-spaced subsequence of the energy levels of a potential with a log-natural spectrum, under a frequency $\\ln(3)$ time-periodic perturbation.","We further show when we \"remove\" $9$ from the set of integers -- by excluding the corresponding energy level from the spectrum -- the cascade halts abruptly."],"url":"http://arxiv.org/abs/2402.04361v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-07 17:46:37","title":"Federated Learning Can Find Friends That Are Beneficial","abstract":"In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.","sentences":["In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges.","While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental.","In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective.","We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution.","Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches.","This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years."],"url":"http://arxiv.org/abs/2402.05050v1","category":"cs.LG"}
{"created":"2024-02-07 15:43:50","title":"ConvLoRA and AdaBN based Domain Adaptation via Self-Training","abstract":"Existing domain adaptation (DA) methods often involve pre-training on the source domain and fine-tuning on the target domain. For multi-target domain adaptation, having a dedicated/separate fine-tuned network for each target domain, that retain all the pre-trained model parameters, is prohibitively expensive. To address this limitation, we propose Convolutional Low-Rank Adaptation (ConvLoRA). ConvLoRA freezes pre-trained model weights, adds trainable low-rank decomposition matrices to convolutional layers, and backpropagates the gradient through these matrices thus greatly reducing the number of trainable parameters. To further boost adaptation, we utilize Adaptive Batch Normalization (AdaBN) which computes target-specific running statistics and use it along with ConvLoRA. Our method has fewer trainable parameters and performs better or on-par with large independent fine-tuned networks (with less than 0.9% trainable parameters of the total base model) when tested on the segmentation of Calgary-Campinas dataset containing brain MRI images. Our approach is simple, yet effective and can be applied to any deep learning-based architecture which uses convolutional and batch normalization layers. Code is available at: https://github.com/aleemsidra/ConvLoRA.","sentences":["Existing domain adaptation (DA) methods often involve pre-training on the source domain and fine-tuning on the target domain.","For multi-target domain adaptation, having a dedicated/separate fine-tuned network for each target domain, that retain all the pre-trained model parameters, is prohibitively expensive.","To address this limitation, we propose Convolutional Low-Rank Adaptation (ConvLoRA).","ConvLoRA freezes pre-trained model weights, adds trainable low-rank decomposition matrices to convolutional layers, and backpropagates the gradient through these matrices thus greatly reducing the number of trainable parameters.","To further boost adaptation, we utilize Adaptive Batch Normalization (AdaBN) which computes target-specific running statistics and use it along with ConvLoRA.","Our method has fewer trainable parameters and performs better or on-par with large independent fine-tuned networks (with less than 0.9% trainable parameters of the total base model) when tested on the segmentation of Calgary-Campinas dataset containing brain MRI images.","Our approach is simple, yet effective and can be applied to any deep learning-based architecture which uses convolutional and batch normalization layers.","Code is available at: https://github.com/aleemsidra/ConvLoRA."],"url":"http://arxiv.org/abs/2402.04964v1","category":"cs.CV"}
{"created":"2024-02-07 13:51:26","title":"Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning","abstract":"In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis.","sentences":["In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning.","Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively.","Our strategy encompasses two-stage training: (i).","a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii).","fine-tuning for multi-patch prediction in the targeted time-series context.","A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding.","Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations.","aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis."],"url":"http://arxiv.org/abs/2402.04852v1","category":"cs.LG"}
{"created":"2024-02-07 12:52:22","title":"Progressive unsupervised domain adaptation for ASR using ensemble models and multi-stage training","abstract":"In Automatic Speech Recognition (ASR), teacher-student (T/S) training has shown to perform well for domain adaptation with small amount of training data. However, adaption without ground-truth labels is still challenging. A previous study has shown the effectiveness of using ensemble teacher models in T/S training for unsupervised domain adaptation (UDA) but its performance still lags behind compared to the model trained on in-domain data. This paper proposes a method to yield better UDA by training multi-stage students with ensemble teacher models. Initially, multiple teacher models are trained on labelled data from read and meeting domains. These teachers are used to train a student model on unlabelled out-of-domain telephone speech data. To improve the adaptation, subsequent student models are trained sequentially considering previously trained model as their teacher. Experiments are conducted with three teachers trained on AMI, WSJ and LibriSpeech and three stages of students on SwitchBoard data. Results shown on eval00 test set show significant WER improvement with multi-stage training with an absolute gain of 9.8%, 7.7% and 3.3% at each stage.","sentences":["In Automatic Speech Recognition (ASR), teacher-student (T/S) training has shown to perform well for domain adaptation with small amount of training data.","However, adaption without ground-truth labels is still challenging.","A previous study has shown the effectiveness of using ensemble teacher models in T/S training for unsupervised domain adaptation (UDA) but its performance still lags behind compared to the model trained on in-domain data.","This paper proposes a method to yield better UDA by training multi-stage students with ensemble teacher models.","Initially, multiple teacher models are trained on labelled data from read and meeting domains.","These teachers are used to train a student model on unlabelled out-of-domain telephone speech data.","To improve the adaptation, subsequent student models are trained sequentially considering previously trained model as their teacher.","Experiments are conducted with three teachers trained on AMI, WSJ and LibriSpeech and three stages of students on SwitchBoard data.","Results shown on eval00 test set show significant WER improvement with multi-stage training with an absolute gain of 9.8%, 7.7% and 3.3% at each stage."],"url":"http://arxiv.org/abs/2402.04805v1","category":"eess.AS"}
{"created":"2024-02-07 10:49:23","title":"Application-Layer FEC Scheme Configuration Optimization via Hybrid Simulated Annealing","abstract":"An optimization technique based on an adapted combination of simulated annealing (SA) and tabu search (TS) is presented. This method aims at finding near-optimal unequal error protection (UEP) application-layer FEC code configurations. This approach is intended to smartly protect audio and video transmission over IP networks when hard time restrictions apply. The considered code is a UEP version of the widely-used Pro-MPEG COP3 codes enabling the use of several matrices of dissimilar size and thus of unequal recovery capability. Finding the optimal configuration frequently requires the evaluation of a large solution space. So, to fulfill the imposed constraints, SA is adapted to the specifics of the scenario. In particular, the annealing schedule is conditioned by the real-time restrictions. Furthermore, solution neighborhood structures are determined by a proposed definition of distance between protection configurations, which, jointly with TS, conditions the selection of candidate solutions. Experimental results show a significantly improved performance of the optimization process, which invariably fulfills imposed timing constraints, at the expense of a very low distortion increase, when compared to using exhaustive search. These results allow the use of UEP Pro-MPEG COP3 codes for protecting video and audio transmission, which distinctly outperforms the standard code in a wide range of scenarios.","sentences":["An optimization technique based on an adapted combination of simulated annealing (SA) and tabu search (TS) is presented.","This method aims at finding near-optimal unequal error protection (UEP) application-layer FEC code configurations.","This approach is intended to smartly protect audio and video transmission over IP networks when hard time restrictions apply.","The considered code is a UEP version of the widely-used Pro-MPEG COP3 codes enabling the use of several matrices of dissimilar size and thus of unequal recovery capability.","Finding the optimal configuration frequently requires the evaluation of a large solution space.","So, to fulfill the imposed constraints, SA is adapted to the specifics of the scenario.","In particular, the annealing schedule is conditioned by the real-time restrictions.","Furthermore, solution neighborhood structures are determined by a proposed definition of distance between protection configurations, which, jointly with TS, conditions the selection of candidate solutions.","Experimental results show a significantly improved performance of the optimization process, which invariably fulfills imposed timing constraints, at the expense of a very low distortion increase, when compared to using exhaustive search.","These results allow the use of UEP Pro-MPEG COP3 codes for protecting video and audio transmission, which distinctly outperforms the standard code in a wide range of scenarios."],"url":"http://arxiv.org/abs/2402.04739v1","category":"eess.IV"}
{"created":"2024-02-07 10:41:14","title":"Review of Cetacean's click detection algorithms","abstract":"The detection of echolocation clicks is key in understanding the intricate behaviors of cetaceans and monitoring their populations. Cetacean species relying on clicks for navigation, foraging and even communications are sperm whales (Physeter macrocephalus) and a variety of dolphin groups. Echolocation clicks are wideband signals of short duration that are often emitted in sequences of varying inter-click-intervals. While datasets and models for clicks exist, the detection and classification of clicks present a significant challenge, mostly due to the diversity of clicks' structures, overlapping signals from simultaneously emitting animals, and the abundance of noise transients from, for example, snapping shrimps and shipping cavitation noise. This paper provides a survey of the many detection and classification methodologies of clicks, ranging from 2002 to 2023. We divide the surveyed techniques into categories by their methodology. Specifically, feature analysis (e.g., phase, ICI and duration), frequency content, energy based detection, supervised and unsupervised machine learning, template matching and adaptive detection approaches. Also surveyed are open access platforms for click detections, and databases openly available for testing. Details of the method applied for each paper are given along with advantages and limitations, and for each category we analyze the remaining challenges. The paper also includes a performance comparison for several schemes over a shared database. Finally, we provide tables summarizing the existing detection schemes in terms of challenges address, methods, detection and classification tools applied, features used and applications.","sentences":["The detection of echolocation clicks is key in understanding the intricate behaviors of cetaceans and monitoring their populations.","Cetacean species relying on clicks for navigation, foraging and even communications are sperm whales (Physeter macrocephalus) and a variety of dolphin groups.","Echolocation clicks are wideband signals of short duration that are often emitted in sequences of varying inter-click-intervals.","While datasets and models for clicks exist, the detection and classification of clicks present a significant challenge, mostly due to the diversity of clicks' structures, overlapping signals from simultaneously emitting animals, and the abundance of noise transients from, for example, snapping shrimps and shipping cavitation noise.","This paper provides a survey of the many detection and classification methodologies of clicks, ranging from 2002 to 2023.","We divide the surveyed techniques into categories by their methodology.","Specifically, feature analysis (e.g., phase, ICI and duration), frequency content, energy based detection, supervised and unsupervised machine learning, template matching and adaptive detection approaches.","Also surveyed are open access platforms for click detections, and databases openly available for testing.","Details of the method applied for each paper are given along with advantages and limitations, and for each category we analyze the remaining challenges.","The paper also includes a performance comparison for several schemes over a shared database.","Finally, we provide tables summarizing the existing detection schemes in terms of challenges address, methods, detection and classification tools applied, features used and applications."],"url":"http://arxiv.org/abs/2402.04735v1","category":"cs.SD"}
{"created":"2024-02-07 10:35:31","title":"The Galactic center excess at the highest energies: morphology and photon-count statistics","abstract":"The nature of the GeV gamma-ray Galactic center excess (GCE) in the data of Fermi-Large Area Telescope (LAT) is still to be unveiled. We present a new analysis of the inner Galaxy Fermi-LAT data at energies above 10 GeV, based on an innovative method which combines the skyFACT adaptive template fitting with and the 1pPDF pixel-count statistics. We find a strong evidence for the GCE also at high energies, $\\sigma > 5$ regardless of the GCE spatial template. Remarkably, our fits prefer the bulge morphological model over the dark matter one at high significance, and show no evidence for an additional dark matter template on top of the bulge component. Through the 1pPDF analysis, we find that the model best describing the gamma-ray data requires a smooth, diffuse GCE following a bulge morphology, together with sub-threshold point sources. The 1pPDF fit reconstructs a consistent population of faint point sources down at least to $10^{-12}$ ph cm$^{-2}$ s$^{-1}$. Between $10^{-12}$ ph cm$^{-2}$ s$^{-1}$ and $10^{-11}$ ph cm$^{-2}$ s$^{-1}$ the 1pPDF measures a number of point sources significantly higher than the ones in the Fermi 4FGL catalog. The robustness of our results brings further support to the attempt of explaining, at least partially, the high-energy tail of the GCE in terms of a population of point sources, likely corresponding to millisecond pulsars.","sentences":["The nature of the GeV gamma-ray Galactic center excess (GCE) in the data of Fermi-Large Area Telescope (LAT) is still to be unveiled.","We present a new analysis of the inner Galaxy Fermi-LAT data at energies above 10 GeV, based on an innovative method which combines the skyFACT adaptive template fitting with and the 1pPDF pixel-count statistics.","We find a strong evidence for the GCE also at high energies, $\\sigma > 5$ regardless of the GCE spatial template.","Remarkably, our fits prefer the bulge morphological model over the dark matter one at high significance, and show no evidence for an additional dark matter template on top of the bulge component.","Through the 1pPDF analysis, we find that the model best describing the gamma-ray data requires a smooth, diffuse GCE following a bulge morphology, together with sub-threshold point sources.","The 1pPDF fit reconstructs a consistent population of faint point sources down at least to $10^{-12}$ ph cm$^{-2}$ s$^{-1}$. Between $10^{-12}$ ph cm$^{-2}$ s$^{-1}$ and $10^{-11}$ ph cm$^{-2}$ s$^{-1}$ the 1pPDF measures a number of point sources significantly higher than the ones in the Fermi 4FGL catalog.","The robustness of our results brings further support to the attempt of explaining, at least partially, the high-energy tail of the GCE in terms of a population of point sources, likely corresponding to millisecond pulsars."],"url":"http://arxiv.org/abs/2402.04733v1","category":"astro-ph.HE"}
{"created":"2024-02-07 10:32:40","title":"Model Predictive Trajectory Optimization With Dynamically Changing Waypoints for Serial Manipulators","abstract":"Systematically including dynamically changing waypoints as desired discrete actions, for instance, resulting from superordinate task planning, has been challenging for online model predictive trajectory optimization with short planning horizons. This paper presents a novel waypoint model predictive control (wMPC) concept for online replanning tasks. The main idea is to split the planning horizon at the waypoint when it becomes reachable within the current planning horizon and reduce the horizon length towards the waypoints and goal points. This approach keeps the computational load low and provides flexibility in adapting to changing conditions in real time. The presented approach achieves competitive path lengths and trajectory durations compared to (global) offline RRT-type planners in a multi-waypoint scenario. Moreover, the ability of wMPC to dynamically replan tasks online is experimentally demonstrated on a KUKA LBR iiwa 14 R820 robot in a dynamic pick-and-place scenario.","sentences":["Systematically including dynamically changing waypoints as desired discrete actions, for instance, resulting from superordinate task planning, has been challenging for online model predictive trajectory optimization with short planning horizons.","This paper presents a novel waypoint model predictive control (wMPC) concept for online replanning tasks.","The main idea is to split the planning horizon at the waypoint when it becomes reachable within the current planning horizon and reduce the horizon length towards the waypoints and goal points.","This approach keeps the computational load low and provides flexibility in adapting to changing conditions in real time.","The presented approach achieves competitive path lengths and trajectory durations compared to (global) offline RRT-type planners in a multi-waypoint scenario.","Moreover, the ability of wMPC to dynamically replan tasks online is experimentally demonstrated on a KUKA LBR iiwa 14 R820 robot in a dynamic pick-and-place scenario."],"url":"http://arxiv.org/abs/2402.04730v1","category":"cs.RO"}
{"created":"2024-02-07 10:10:21","title":"Quantum Theory of Spin-Transfer and Spin-Pumping in Collinear Antiferromagnets and Ferrimagnets","abstract":"Antiferromagnets are promising candidates as active components in spintronic applications. They share features with ferrimagnets in that opposing spin orientations exist in two or more sublattices. Spin transfer torque and spin pumping are essential ingredients in antiferromagnetic and ferrimagnet spintronics. This paper develops an out-of-equilibrium quantum theory of the spin dynamics of collinear magnets containing many spins coupled to normal metal reservoirs. At equilibrium, the spins are parallel or antiparallel to the easy axis. The theory, therefore, covers collinear antiferromagnets and ferrimagnets. We focus on the resulting semi-classical spin dynamics. The dissipation in the spin dynamics is enhanced due to spin-pumping. Spin accumulations in the normal metals induce deterministic spin-transfer torques on the magnet. Additionally, each electron's discrete spin angular momentum causes stochastic fluctuating torques on the antiferromagnet or ferrimagnet. We derive these fluctuating torques. The fluctuation-dissipation theorem holds at high temperatures, including the effects of spin-pumping. At low temperatures, we derive shot noise contributions to the fluctuations.","sentences":["Antiferromagnets are promising candidates as active components in spintronic applications.","They share features with ferrimagnets in that opposing spin orientations exist in two or more sublattices.","Spin transfer torque and spin pumping are essential ingredients in antiferromagnetic and ferrimagnet spintronics.","This paper develops an out-of-equilibrium quantum theory of the spin dynamics of collinear magnets containing many spins coupled to normal metal reservoirs.","At equilibrium, the spins are parallel or antiparallel to the easy axis.","The theory, therefore, covers collinear antiferromagnets and ferrimagnets.","We focus on the resulting semi-classical spin dynamics.","The dissipation in the spin dynamics is enhanced due to spin-pumping.","Spin accumulations in the normal metals induce deterministic spin-transfer torques on the magnet.","Additionally, each electron's discrete spin angular momentum causes stochastic fluctuating torques on the antiferromagnet or ferrimagnet.","We derive these fluctuating torques.","The fluctuation-dissipation theorem holds at high temperatures, including the effects of spin-pumping.","At low temperatures, we derive shot noise contributions to the fluctuations."],"url":"http://arxiv.org/abs/2402.04719v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 09:58:52","title":"High-dimensional multidisciplinary design optimization for aircraft eco-design / Optimisation multi-disciplinaire en grande dimension pour l'\u00e9co-conception avion en avant-projet","abstract":"The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the \"DRAGON\" hybrid airplane to reduce their climate impact.","sentences":["The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables).","The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models.","EGO is a heuristic BO method that performs well in terms of solution quality.","However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases.","For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out.","The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space.","In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables.","Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the \"DRAGON\" hybrid airplane to reduce their climate impact."],"url":"http://arxiv.org/abs/2402.04711v1","category":"math.OC"}
{"created":"2024-02-07 09:30:15","title":"Near-equilibrium growth of vapor bubbles","abstract":"This study delves into the near-equilibrium dynamics of vapor bubbles. Utilizing a regular perturbation method, we derive analytical solutions to capture the bubble's initial growth stages, wherein surface tension and viscous dissipation forces impede the bubble's growth. The accuracy of these solutions is validated with numeric solutions of the complete Rayleigh-Plesset equation. Although limited to near-critical bubble radii, our analytical solutions predict the initial delay period as a function of two nondimensional parameters, signifying the initial deviation from the critical radius and the surface tension to viscosity ratio. We found a transition in the dominant delay mechanism, marking a shift from surface tension to viscous damping dominance. Our findings emphasize the significance of considering the surface tension delay, particularly for short timescales, and highlight its crucial role in accurately modeling the bubble's initial growth dynamics. The derived analytical solutions and the obtained correlation for surface-tension-induced delay may prove a practical tool and could be integrated into existing models of vapor bubble growth.","sentences":["This study delves into the near-equilibrium dynamics of vapor bubbles.","Utilizing a regular perturbation method, we derive analytical solutions to capture the bubble's initial growth stages, wherein surface tension and viscous dissipation forces impede the bubble's growth.","The accuracy of these solutions is validated with numeric solutions of the complete Rayleigh-Plesset equation.","Although limited to near-critical bubble radii, our analytical solutions predict the initial delay period as a function of two nondimensional parameters, signifying the initial deviation from the critical radius and the surface tension to viscosity ratio.","We found a transition in the dominant delay mechanism, marking a shift from surface tension to viscous damping dominance.","Our findings emphasize the significance of considering the surface tension delay, particularly for short timescales, and highlight its crucial role in accurately modeling the bubble's initial growth dynamics.","The derived analytical solutions and the obtained correlation for surface-tension-induced delay may prove a practical tool and could be integrated into existing models of vapor bubble growth."],"url":"http://arxiv.org/abs/2402.04690v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 08:18:06","title":"Learning with Diversification from Block Sparse Signal","abstract":"This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.","sentences":["This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data.","By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting.","Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation.","Moreover, we establish the global and local optimality theory of our model.","Experiments validate the advantages of DivSBL over existing algorithms."],"url":"http://arxiv.org/abs/2402.04646v1","category":"cs.LG"}
{"created":"2024-02-07 07:16:12","title":"Noise Map Guidance: Inversion with Spatial Context for Real Image Editing","abstract":"Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images. However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity. Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization. Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing. Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality. Our empirical investigations highlight NMG's adaptability across various editing techniques and its robustness to variants of DDIM inversions.","sentences":["Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images.","However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity.","Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization.","Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing.","Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality.","Our empirical investigations highlight NMG's adaptability across various editing techniques and its robustness to variants of DDIM inversions."],"url":"http://arxiv.org/abs/2402.04625v1","category":"cs.CV"}
{"created":"2024-02-07 07:01:08","title":"Multi-Scale Semantic Segmentation with Modified MBConv Blocks","abstract":"Recently, MBConv blocks, initially designed for efficiency in resource-limited settings and later adapted for cutting-edge image classification performances, have demonstrated significant potential in image classification tasks. Despite their success, their application in semantic segmentation has remained relatively unexplored. This paper introduces a novel adaptation of MBConv blocks specifically tailored for semantic segmentation. Our modification stems from the insight that semantic segmentation requires the extraction of more detailed spatial information than image classification. We argue that to effectively perform multi-scale semantic segmentation, each branch of a U-Net architecture, regardless of its resolution, should possess equivalent segmentation capabilities. By implementing these changes, our approach achieves impressive mean Intersection over Union (IoU) scores of 84.5% and 84.0% on the Cityscapes test and validation datasets, respectively, demonstrating the efficacy of our proposed modifications in enhancing semantic segmentation performance.","sentences":["Recently, MBConv blocks, initially designed for efficiency in resource-limited settings and later adapted for cutting-edge image classification performances, have demonstrated significant potential in image classification tasks.","Despite their success, their application in semantic segmentation has remained relatively unexplored.","This paper introduces a novel adaptation of MBConv blocks specifically tailored for semantic segmentation.","Our modification stems from the insight that semantic segmentation requires the extraction of more detailed spatial information than image classification.","We argue that to effectively perform multi-scale semantic segmentation, each branch of a U-Net architecture, regardless of its resolution, should possess equivalent segmentation capabilities.","By implementing these changes, our approach achieves impressive mean Intersection over Union (IoU) scores of 84.5% and 84.0% on the Cityscapes test and validation datasets, respectively, demonstrating the efficacy of our proposed modifications in enhancing semantic segmentation performance."],"url":"http://arxiv.org/abs/2402.04618v1","category":"cs.CV"}
{"created":"2024-02-07 04:11:25","title":"Progressive Conservative Adaptation for Evolving Target Domains","abstract":"Conventional domain adaptation typically transfers knowledge from a source domain to a stationary target domain. However, in many real-world cases, target data usually emerge sequentially and have continuously evolving distributions. Restoring and adapting to such target data results in escalating computational and resource consumption over time. Hence, it is vital to devise algorithms to address the evolving domain adaptation (EDA) problem, \\emph{i.e.,} adapting models to evolving target domains without access to historic target domains. To achieve this goal, we propose a simple yet effective approach, termed progressive conservative adaptation (PCAda). To manage new target data that diverges from previous distributions, we fine-tune the classifier head based on the progressively updated class prototypes. Moreover, as adjusting to the most recent target domain can interfere with the features learned from previous target domains, we develop a conservative sparse attention mechanism. This mechanism restricts feature adaptation within essential dimensions, thus easing the inference related to historical knowledge. The proposed PCAda is implemented with a meta-learning framework, which achieves the fast adaptation of the classifier with the help of the progressively updated class prototypes in the inner loop and learns a generalized feature without severely interfering with the historic knowledge via the conservative sparse attention in the outer loop. Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate the effectiveness of our method.","sentences":["Conventional domain adaptation typically transfers knowledge from a source domain to a stationary target domain.","However, in many real-world cases, target data usually emerge sequentially and have continuously evolving distributions.","Restoring and adapting to such target data results in escalating computational and resource consumption over time.","Hence, it is vital to devise algorithms to address the evolving domain adaptation (EDA) problem, \\emph{i.e.,} adapting models to evolving target domains without access to historic target domains.","To achieve this goal, we propose a simple yet effective approach, termed progressive conservative adaptation (PCAda).","To manage new target data that diverges from previous distributions, we fine-tune the classifier head based on the progressively updated class prototypes.","Moreover, as adjusting to the most recent target domain can interfere with the features learned from previous target domains, we develop a conservative sparse attention mechanism.","This mechanism restricts feature adaptation within essential dimensions, thus easing the inference related to historical knowledge.","The proposed PCAda is implemented with a meta-learning framework, which achieves the fast adaptation of the classifier with the help of the progressively updated class prototypes in the inner loop and learns a generalized feature without severely interfering with the historic knowledge via the conservative sparse attention in the outer loop.","Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.04573v1","category":"cs.CV"}
{"created":"2024-02-07 03:18:34","title":"BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial Imagery","abstract":"In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality.","sentences":["In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery.","Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models.","(2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs.","(3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity.","Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene.","This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments.","Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process.","Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results.","We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality."],"url":"http://arxiv.org/abs/2402.04554v1","category":"cs.CV"}
{"created":"2024-02-06 22:24:56","title":"Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations","abstract":"Effective communication between healthcare providers and patients is crucial to providing high-quality patient care. In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems. By analyzing embeddings such as bag-of-words, character n-grams, Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations. Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner. Overall, Word2Vec, GloVe and Character n-grams embeddings performed well, indicating their suitability for modeling targeted to this task. GPT2 embedding also shows notable performance, indicating its suitability for models tailored to this task as well. Our machine learning architectures significantly improved the quality of health conversations when training data are scarce, improving communication between patients and healthcare providers.","sentences":["Effective communication between healthcare providers and patients is crucial to providing high-quality patient care.","In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems.","By analyzing embeddings such as bag-of-words, character n-grams, Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations.","Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner.","Overall, Word2Vec, GloVe and Character n-grams embeddings performed well, indicating their suitability for modeling targeted to this task.","GPT2 embedding also shows notable performance, indicating its suitability for models tailored to this task as well.","Our machine learning architectures significantly improved the quality of health conversations when training data are scarce, improving communication between patients and healthcare providers."],"url":"http://arxiv.org/abs/2402.04442v1","category":"cs.CL"}
{"created":"2024-02-06 21:03:52","title":"Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning","abstract":"Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users' personal PEFT parameters, they can own and use their LLMs personally. OPPU integrates parametric user knowledge in the personal PEFT parameters with the non-parametric knowledge acquired through retrieval and profile. This integration adapts individual LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further in-depth studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different active levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.","sentences":["Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences.","Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles.","However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues.","Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic.","To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences.","By plugging in users' personal PEFT parameters, they can own and use their LLMs personally.","OPPU integrates parametric user knowledge in the personal PEFT parameters with the non-parametric knowledge acquired through retrieval and profile.","This integration adapts individual LLMs to user behavior shifts.","Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark.","Further in-depth studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different active levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods."],"url":"http://arxiv.org/abs/2402.04401v1","category":"cs.CL"}
{"created":"2024-02-06 20:57:16","title":"A Repeated Auction Model for Load-Aware Dynamic Resource Allocation in Multi-Access Edge Computing","abstract":"Multi-access edge computing (MEC) is one of the enabling technologies for high-performance computing at the edge of the 6 G networks, supporting high data rates and ultra-low service latency. Although MEC is a remedy to meet the growing demand for computation-intensive applications, the scarcity of resources at the MEC servers degrades its performance. Hence, effective resource management is essential; nevertheless, state-of-the-art research lacks efficient economic models to support the exponential growth of the MEC-enabled applications market. We focus on designing a MEC offloading service market based on a repeated auction model with multiple resource sellers (e.g., network operators and service providers) that compete to sell their computing resources to the offloading users. We design a computationally-efficient modified Generalized Second Price (GSP)-based algorithm that decides on pricing and resource allocation by considering the dynamic offloading requests arrival and the servers' computational workloads. Besides, we propose adaptive best-response bidding strategies for the resource sellers, satisfying the symmetric Nash equilibrium (SNE) and individual rationality properties. Finally, via intensive numerical results, we show the effectiveness of our proposed resource allocation mechanism.","sentences":["Multi-access edge computing (MEC) is one of the enabling technologies for high-performance computing at the edge of the 6 G networks, supporting high data rates and ultra-low service latency.","Although MEC is a remedy to meet the growing demand for computation-intensive applications, the scarcity of resources at the MEC servers degrades its performance.","Hence, effective resource management is essential; nevertheless, state-of-the-art research lacks efficient economic models to support the exponential growth of the MEC-enabled applications market.","We focus on designing a MEC offloading service market based on a repeated auction model with multiple resource sellers (e.g., network operators and service providers) that compete to sell their computing resources to the offloading users.","We design a computationally-efficient modified Generalized Second Price (GSP)-based algorithm that decides on pricing and resource allocation by considering the dynamic offloading requests arrival and the servers' computational workloads.","Besides, we propose adaptive best-response bidding strategies for the resource sellers, satisfying the symmetric Nash equilibrium (SNE) and individual rationality properties.","Finally, via intensive numerical results, we show the effectiveness of our proposed resource allocation mechanism."],"url":"http://arxiv.org/abs/2402.04399v1","category":"cs.GT"}
{"created":"2024-02-06 20:47:58","title":"Factorial Basis Method for q-Series Applications","abstract":"The Factorial Basis method, initially designed for quasi-triangular, shift-compatible factorial bases, provides solutions to linear recurrence equations in the form of definite-sums. This paper extends the Factorial Basis method to its q-analog, enabling its application in q-calculus. We demonstrate the adaptation of the method to q-sequences and its utility in the realm of q-combinatorics. The extended technique is employed to automatically prove established identities and unveil novel ones, particularly some associated with the Rogers-Ramanujan identities.","sentences":["The Factorial Basis method, initially designed for quasi-triangular, shift-compatible factorial bases, provides solutions to linear recurrence equations in the form of definite-sums.","This paper extends the Factorial Basis method to its q-analog, enabling its application in q-calculus.","We demonstrate the adaptation of the method to q-sequences and its utility in the realm of q-combinatorics.","The extended technique is employed to automatically prove established identities and unveil novel ones, particularly some associated with the Rogers-Ramanujan identities."],"url":"http://arxiv.org/abs/2402.04392v1","category":"cs.SC"}
{"created":"2024-02-06 19:49:23","title":"Adaptive Inference: Theoretical Limits and Unexplored Opportunities","abstract":"This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms. We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties. Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces.","sentences":["This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms.","We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties.","Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces."],"url":"http://arxiv.org/abs/2402.04359v1","category":"cs.LG"}
{"created":"2024-02-06 19:42:18","title":"Bidirectional Autoregressive Diffusion Model for Dance Generation","abstract":"Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge. Recently, diffusion models have showcased remarkable generative abilities across various domains. They hold promise for human motion generation due to their adaptable many-to-many nature. Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement. When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions. To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions. To make the generated dance motion smoother, a local information decoder is built for local motion enhancement. The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions. To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements. Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation.","sentences":["Dance serves as a powerful medium for expressing human emotions, but the lifelike generation of dance is still a considerable challenge.","Recently, diffusion models have showcased remarkable generative abilities across various domains.","They hold promise for human motion generation due to their adaptable many-to-many nature.","Nonetheless, current diffusion-based motion generation models often create entire motion sequences directly and unidirectionally, lacking focus on the motion with local and bidirectional enhancement.","When choreographing high-quality dance movements, people need to take into account not only the musical context but also the nearby music-aligned dance motions.","To authentically capture human behavior, we propose a Bidirectional Autoregressive Diffusion Model (BADM) for music-to-dance generation, where a bidirectional encoder is built to enforce that the generated dance is harmonious in both the forward and backward directions.","To make the generated dance motion smoother, a local information decoder is built for local motion enhancement.","The proposed framework is able to generate new motions based on the input conditions and nearby motions, which foresees individual motion slices iteratively and consolidates all predictions.","To further refine the synchronicity between the generated dance and the beat, the beat information is incorporated as an input to generate better music-aligned dance movements.","Experimental results demonstrate that the proposed model achieves state-of-the-art performance compared to existing unidirectional approaches on the prominent benchmark for music-to-dance generation."],"url":"http://arxiv.org/abs/2402.04356v1","category":"cs.SD"}
{"created":"2024-02-06 19:37:05","title":"3D printer-controlled syringe pumps for dual, active, regulable and simultaneous dispensing of reagents. Manufacturing of immunochromatographic test strips","abstract":"Lateral flow immunoassays (LFIA) are widely used worldwide for the detection of different analytes because they combine multiple advantages such as low production cost, simplicity, and portability, which allows biomarkers detection without requiring infrastructure or highly trained personnel. Here we propose to provide solutions to the manufacturing process of LFIA at laboratory-scale, particularly to the controlled and active dispensing of the reagents in the form the Test Lines (TL) and the Control Lines (CL). To accomplish this task, we adapted a 3D printer to also control Syringe Pumps (SP), since the proposed adaptation of a 3D printer is easy, free and many laboratories already have it in their infrastructure. In turn, the standard function of the 3D printer can be easily restored by disconnecting the SPs and reconnecting the extruder. Additionally, the unified control of the 3D printer enables dual, active, regulable and simultaneous dispensing, four features that are typically found only in certain high-cost commercial equipment. With the proposed setup, the challenge of dispensing simultaneously at least 2 lines (CL and TL) with SPs controlled by a 3D printer was addressed, including regulation in the width of dispensed lines within experimental limits. Also, the construction of a LFIA for the detection of leptospirosis is shown as a practical example of automatized reagent dispensing.","sentences":["Lateral flow immunoassays (LFIA) are widely used worldwide for the detection of different analytes because they combine multiple advantages such as low production cost, simplicity, and portability, which allows biomarkers detection without requiring infrastructure or highly trained personnel.","Here we propose to provide solutions to the manufacturing process of LFIA at laboratory-scale, particularly to the controlled and active dispensing of the reagents in the form the Test Lines (TL) and the Control Lines (CL).","To accomplish this task, we adapted a 3D printer to also control Syringe Pumps (SP), since the proposed adaptation of a 3D printer is easy, free and many laboratories already have it in their infrastructure.","In turn, the standard function of the 3D printer can be easily restored by disconnecting the SPs and reconnecting the extruder.","Additionally, the unified control of the 3D printer enables dual, active, regulable and simultaneous dispensing, four features that are typically found only in certain high-cost commercial equipment.","With the proposed setup, the challenge of dispensing simultaneously at least 2 lines (CL and TL) with SPs controlled by a 3D printer was addressed, including regulation in the width of dispensed lines within experimental limits.","Also, the construction of a LFIA for the detection of leptospirosis is shown as a practical example of automatized reagent dispensing."],"url":"http://arxiv.org/abs/2402.04354v1","category":"cs.RO"}
{"created":"2024-02-06 19:31:26","title":"The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry","abstract":"Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) \"finetuned-conversion\" of task-specific Transformers into linear versions that recover task performance, and (3) \"pretrained-conversion\" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or \"spiky\") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.","sentences":["Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length.","This holds exciting promise for (1) training linear Transformers from scratch, (2) \"finetuned-conversion\" of task-specific Transformers into linear versions that recover task performance, and (3) \"pretrained-conversion\" of Transformers such as large language models into linear versions finetunable on downstream tasks.","However, linear attentions often underperform standard softmax attention in quality.","To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or \"spiky\") weights and dot-product monotonicity.","We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention.","We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity.","Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention.","Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs.","Hedgehog also enables pretrained-conversion.","Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models.","We finally turn a pretrained Llama-2 7B into a viable linear attention Llama.","With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops."],"url":"http://arxiv.org/abs/2402.04347v1","category":"cs.LG"}
{"created":"2024-02-06 19:16:25","title":"Proactive Blockage Prediction for UAV assisted Handover in Future Wireless Network","abstract":"The future wireless communication applications demand seamless connectivity, higher throughput, and low latency, for which the millimeter-wave (mmWave) band is considered a potential technology. Nevertheless, line-of-sight (LoS) is often mandatory for mmWave band communication, and it renders these waves sensitive to sudden changes in the environment. Therefore, it is necessary to maintain the LoS link for a reliable connection. One such technique to maintain LoS is using proactive handover (HO). However, proactive HO is challenging, requiring continuous information about the surrounding wireless network to anticipate potential blockage. This paper presents a proactive blockage prediction mechanism where an unmanned aerial vehicle (UAV) is used as the base station for HO. The proposed scheme uses computer vision (CV) to obtain potential blocking objects, user speed, and location. To assess the effectiveness of the proposed scheme, the system is evaluated using a publicly available dataset for blockage prediction. The study integrates scenarios from Vision-based Wireless (ViWi) and UAV channel modeling, generating wireless data samples relevant to UAVs. The antenna modeling on the UAV end incorporates a polarization-matched scenario to optimize signal reception. The results demonstrate that UAV-assisted Handover not only ensures seamless connectivity but also enhances overall network performance by 20%. This research contributes to the advancement of proactive blockage mitigation strategies in wireless networks, showcasing the potential of UAVs as dynamic and adaptable base stations.","sentences":["The future wireless communication applications demand seamless connectivity, higher throughput, and low latency, for which the millimeter-wave (mmWave) band is considered a potential technology.","Nevertheless, line-of-sight (LoS) is often mandatory for mmWave band communication, and it renders these waves sensitive to sudden changes in the environment.","Therefore, it is necessary to maintain the LoS link for a reliable connection.","One such technique to maintain LoS is using proactive handover (HO).","However, proactive HO is challenging, requiring continuous information about the surrounding wireless network to anticipate potential blockage.","This paper presents a proactive blockage prediction mechanism where an unmanned aerial vehicle (UAV) is used as the base station for HO.","The proposed scheme uses computer vision (CV) to obtain potential blocking objects, user speed, and location.","To assess the effectiveness of the proposed scheme, the system is evaluated using a publicly available dataset for blockage prediction.","The study integrates scenarios from Vision-based Wireless (ViWi) and UAV channel modeling, generating wireless data samples relevant to UAVs.","The antenna modeling on the UAV end incorporates a polarization-matched scenario to optimize signal reception.","The results demonstrate that UAV-assisted Handover not only ensures seamless connectivity but also enhances overall network performance by 20%.","This research contributes to the advancement of proactive blockage mitigation strategies in wireless networks, showcasing the potential of UAVs as dynamic and adaptable base stations."],"url":"http://arxiv.org/abs/2402.04332v1","category":"eess.SP"}
{"created":"2024-02-06 19:05:22","title":"Homogeneity problem for basis expansion of functional data with applications to resistive memories","abstract":"The homogeneity problem for testing if more than two different samples come from the same population is considered for the case of functional data. The methodological results are motivated by the study of homogeneity of electronic devices fabricated by different materials and active layer thicknesses. In the case of normality distribution of the stochastic processes associated with each sample, this problem is known as Functional ANOVA problem and is reduced to test the equality of the mean group functions (FANOVA). The problem is that the current/voltage curves associated with Resistive Random Access Memories (RRAM) are not generated by a Gaussian process so that a different approach is necessary for testing homogeneity. To solve this problem two different parametric and nonparametric approaches based on basis expansion of the sample curves are proposed. The first consists of testing multivariate homogeneity tests on a vector of basis coefficients of the sample curves. The second is based on dimension reduction by using functional principal component analysis of the sample curves (FPCA) and testing multivariate homogeneity on a vector of principal components scores. Different approximation numerical techniques are employed to adapt the experimental data for the statistical study. An extensive simulation study is developed for analyzing the performance of both approaches in the parametric and non-parametric cases. Finally, the proposed methodologies are applied on three samples of experimental reset curves measured in three different RRAM technologies.","sentences":["The homogeneity problem for testing if more than two different samples come from the same population is considered for the case of functional data.","The methodological results are motivated by the study of homogeneity of electronic devices fabricated by different materials and active layer thicknesses.","In the case of normality distribution of the stochastic processes associated with each sample, this problem is known as Functional ANOVA problem and is reduced to test the equality of the mean group functions (FANOVA).","The problem is that the current/voltage curves associated with Resistive Random Access Memories (RRAM) are not generated by a Gaussian process so that a different approach is necessary for testing homogeneity.","To solve this problem two different parametric and nonparametric approaches based on basis expansion of the sample curves are proposed.","The first consists of testing multivariate homogeneity tests on a vector of basis coefficients of the sample curves.","The second is based on dimension reduction by using functional principal component analysis of the sample curves (FPCA) and testing multivariate homogeneity on a vector of principal components scores.","Different approximation numerical techniques are employed to adapt the experimental data for the statistical study.","An extensive simulation study is developed for analyzing the performance of both approaches in the parametric and non-parametric cases.","Finally, the proposed methodologies are applied on three samples of experimental reset curves measured in three different RRAM technologies."],"url":"http://arxiv.org/abs/2402.04321v1","category":"stat.ME"}
{"created":"2024-02-06 19:02:33","title":"Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments","abstract":"In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments. Traditional approaches often rely on computational methods such as time-series analysis. Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs. We introduce a novel \"adaptive visual sector\" mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. Additionally, we develop a \"dynamic traffic graph\" using Convolutional Neural Networks (CNN) and Graph Attention Networks (GAT) to capture spatio-temporal dependencies among agents. Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively. Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs. The code for the proposed model is available at our Github.","sentences":["In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments.","Traditional approaches often rely on computational methods such as time-series analysis.","Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs.","We introduce a novel \"adaptive visual sector\" mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed.","Additionally, we develop a \"dynamic traffic graph\" using Convolutional Neural Networks (CNN) and Graph Attention Networks (GAT) to capture spatio-temporal dependencies among agents.","Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively.","Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs.","The code for the proposed model is available at our Github."],"url":"http://arxiv.org/abs/2402.04318v1","category":"cs.RO"}
{"created":"2024-02-07 15:39:14","title":"The fractional Hopf differential and a weak formulation of stationarity for the half Dirichlet energy","abstract":"We obtain a weak formulation of the stationarity condition for the half Dirichlet energy, which can be expressed in terms of a fractional analogous to the Hopf differential. As an application we show that conformal harmonic maps from the disc are precisely the harmonic extensions of stationary points of the half Dirichlet energy on the circle. We also derive a Noether theorem and a Pohozaev identity for stationary points of the half Dirichlet energy.","sentences":["We obtain a weak formulation of the stationarity condition for the half Dirichlet energy, which can be expressed in terms of a fractional analogous to the Hopf differential.","As an application we show that conformal harmonic maps from the disc are precisely the harmonic extensions of stationary points of the half Dirichlet energy on the circle.","We also derive a Noether theorem and a Pohozaev identity for stationary points of the half Dirichlet energy."],"url":"http://arxiv.org/abs/2402.04956v1","category":"math.AP"}
{"created":"2024-02-07 14:47:23","title":"Jet transport coefficients by elastic and radiative scatterings in the strongly interacting quark-gluon plasma","abstract":"We extend the investigation on jet transport coefficients within the effective Dynamical QuasiParticle Model (DQPM) -- constructed for the description of non-perturbative QCD phenomena of the strongly interacting quark-gluon plasma (sQGP) in line with the lattice QCD equation-of-state -- by accounting for inelastic $2\\to 3$ reactions with gluon radiation additionally to the elastic scattering of partons. The elastic and inelastic reactions are calculated explicitly within leading-order Feynman diagrams with effective propagators and vertices from the DQPM by accounting for all channels and their interferences. We present the results for the jet transport coefficients such as the transverse momentum transfer squared $\\hat{q}$ per unit length as well as the energy loss $\\Delta E = dE/dx$ per unit length in the sQGP and investigate their dependence on the temperature $T$ and momentum of the jet parton depending on the choice of the strong coupling constant $\\alpha_s$ in thermal, jet parton and radiative vertices. For the latter we consider different scenarios used in the literature and find a very strong dependence of $\\hat q$ and $\\Delta E$ on the choice of $\\alpha_s$. Moreover, we explore the relation of $\\hat{q}/T^3$ to the ratio of specific shear viscosity to entropy density $\\eta/s$ and show that the ratio $T^3/\\hat{q}$ to $\\eta/s$ has a strong $T$ dependence -- especially when approaching to $T_c$ -- on the choice of $\\alpha_s$ in scattering vertices.","sentences":["We extend the investigation on jet transport coefficients within the effective Dynamical QuasiParticle Model (DQPM) -- constructed for the description of non-perturbative QCD phenomena of the strongly interacting quark-gluon plasma (sQGP) in line with the lattice QCD equation-of-state -- by accounting for inelastic $2\\to 3$ reactions with gluon radiation additionally to the elastic scattering of partons.","The elastic and inelastic reactions are calculated explicitly within leading-order Feynman diagrams with effective propagators and vertices from the DQPM by accounting for all channels and their interferences.","We present the results for the jet transport coefficients such as the transverse momentum transfer squared $\\hat{q}$ per unit length as well as the energy loss $\\Delta E = dE/dx$ per unit length in the sQGP and investigate their dependence on the temperature $T$ and momentum of the jet parton depending on the choice of the strong coupling constant $\\alpha_s$ in thermal, jet parton and radiative vertices.","For the latter we consider different scenarios used in the literature and find a very strong dependence of $\\hat q$ and $\\Delta E$ on the choice of $\\alpha_s$. Moreover, we explore the relation of $\\hat{q}/T^3$ to the ratio of specific shear viscosity to entropy density $\\eta/s$ and show that the ratio $T^3/\\hat{q}$ to $\\eta/s$ has a strong $T$ dependence -- especially when approaching to $T_c$ -- on the choice of $\\alpha_s$ in scattering vertices."],"url":"http://arxiv.org/abs/2402.04923v1","category":"hep-ph"}
{"created":"2024-02-07 14:44:50","title":"Modeling and Calibration of Gaia, Hipparcos, and Tycho-2 astrometric data for the detection of dark companions","abstract":"Hidden within the Gaia satellite's multiple data releases lies a valuable cache of dark companions. To facilitate the efficient and reliable detection of these companions via combined analyses involving Gaia, Hipparcos, and Tycho-2 catalogs, we introduce an astrometric modeling framework. This method incorporates analytical least square minimization and nonlinear parameter optimization techniques to a set of common calibration sources across the different space-based astrometric catalogues. This enables us to discern the error inflation, astrometric jitter, differential parallax zero-point, and frame rotation of various catalogues relative to Gaia DR3. Our findings yield the most precise Gaia DR2 calibration parameters to date, revealing notable dependencies on magnitude and color. Intriguingly, we identify sub-mas frame rotation between Gaia DR1 and DR3, along with an estimated astrometric jitter of 2.16 mas for the revised Hipparcos catalog. In a thorough comparative analysis with previous studies, we offer recommendations on calibrating and utilizing different catalogs for companion detection. Furthermore, we provide a user-friendly pipeline (https://github.com/ruiyicheng/Download_HIP_Gaia_GOST) for catalog download and bias correction, enhancing accessibility and usability within the scientific community.","sentences":["Hidden within the Gaia satellite's multiple data releases lies a valuable cache of dark companions.","To facilitate the efficient and reliable detection of these companions via combined analyses involving Gaia, Hipparcos, and Tycho-2 catalogs, we introduce an astrometric modeling framework.","This method incorporates analytical least square minimization and nonlinear parameter optimization techniques to a set of common calibration sources across the different space-based astrometric catalogues.","This enables us to discern the error inflation, astrometric jitter, differential parallax zero-point, and frame rotation of various catalogues relative to Gaia DR3.","Our findings yield the most precise Gaia DR2 calibration parameters to date, revealing notable dependencies on magnitude and color.","Intriguingly, we identify sub-mas frame rotation between Gaia DR1 and DR3, along with an estimated astrometric jitter of 2.16 mas for the revised Hipparcos catalog.","In a thorough comparative analysis with previous studies, we offer recommendations on calibrating and utilizing different catalogs for companion detection.","Furthermore, we provide a user-friendly pipeline (https://github.com/ruiyicheng/Download_HIP_Gaia_GOST) for catalog download and bias correction, enhancing accessibility and usability within the scientific community."],"url":"http://arxiv.org/abs/2402.04919v1","category":"astro-ph.SR"}
{"created":"2024-02-07 13:46:47","title":"T-W relation and free energy of the antiperiodic XXZ chain with \u03b7=i\u03c0/3 at a finite temperature","abstract":"We study the thermodynamics of the antiperiodic XXZ chain with anisotropy parameter {\\eta}=i{\\pi}/3 by means of the t-W method. We parameterize the eigenvalues of both the transfer matrix and the corresponding fused transfer matrix by their zero points instead of Bethe roots. Based on the patterns of the zero points distribution and the reconstructed entropy, we obtain the nonlinear integral equations (NLIEs) describing the thermodynamics of the model and compute its free energy at a finite temperature.","sentences":["We study the thermodynamics of the antiperiodic XXZ chain with anisotropy parameter {\\eta}=i{\\pi}/3 by means of the t-W method.","We parameterize the eigenvalues of both the transfer matrix and the corresponding fused transfer matrix by their zero points instead of Bethe roots.","Based on the patterns of the zero points distribution and the reconstructed entropy, we obtain the nonlinear integral equations (NLIEs) describing the thermodynamics of the model and compute its free energy at a finite temperature."],"url":"http://arxiv.org/abs/2402.04849v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 13:41:45","title":"Efficient Estimation of a Gaussian Mean with Local Differential Privacy","abstract":"In this paper we study the problem of estimating the unknown mean $\\theta$ of a unit variance Gaussian distribution in a locally differentially private (LDP) way. In the high-privacy regime ($\\epsilon\\le 0.67$), we identify the exact optimal privacy mechanism that minimizes the variance of the estimator asymptotically. It turns out to be the extraordinarily simple sign mechanism that applies randomized response to the sign of $X_i-\\theta$. However, since this optimal mechanism depends on the unknown mean $\\theta$, we employ a two-stage LDP parameter estimation procedure which requires splitting agents into two groups. The first $n_1$ observations are used to consistently but not necessarily efficiently estimate the parameter $\\theta$ by $\\tilde{\\theta}_{n_1}$. Then this estimate is updated by applying the sign mechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining $n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown mean.","sentences":["In this paper we study the problem of estimating the unknown mean $\\theta$ of a unit variance Gaussian distribution in a locally differentially private (LDP) way.","In the high-privacy regime ($\\epsilon\\le 0.67$), we identify the exact optimal privacy mechanism that minimizes the variance of the estimator asymptotically.","It turns out to be the extraordinarily simple sign mechanism that applies randomized response to the sign of $X_i-\\theta$. However, since this optimal mechanism depends on the unknown mean $\\theta$, we employ a two-stage LDP parameter estimation procedure which requires splitting agents into two groups.","The first $n_1$ observations are used to consistently but not necessarily efficiently estimate the parameter $\\theta$ by $\\tilde{\\theta}_{n_1}$. Then this estimate is updated by applying the sign mechanism with $\\tilde{\\theta}_{n_1}$ instead of $\\theta$ to the remaining $n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown mean."],"url":"http://arxiv.org/abs/2402.04840v1","category":"math.ST"}
{"created":"2024-02-07 13:32:47","title":"SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning","abstract":"Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on seven datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in almost all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings.","sentences":["Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label.","Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem.","Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm.","These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques.","The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels.","SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature.","We perform thorough experiments on seven datasets and compare SARI against nine NPLL and PLL methods from the prior art.","SARI achieves state-of-the-art results in almost all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings."],"url":"http://arxiv.org/abs/2402.04835v1","category":"cs.CV"}
{"created":"2024-02-07 13:23:48","title":"Accurate Zernike-Corrected Phase Screens for Arbitrary Power Spectra","abstract":"Wave propagation through random continuous media remains an important fundamental problem with applications ranging from remote sensing to quantum communication. Typically, such media are characterized by smooth refractive index fluctuations whose impact on the wave can be captured by the stochastic parabolic equation. The latter can be solved numerically by means of a split-step method, which replaces the continuous medium with a number of discrete phase screens derived from the medium's power spectrum. We introduce and benchmark highly accurate and efficient hybrid phase screens for arbitrary power spectra which are based on the combination of Zernike and Fourier phase screens.","sentences":["Wave propagation through random continuous media remains an important fundamental problem with applications ranging from remote sensing to quantum communication.","Typically, such media are characterized by smooth refractive index fluctuations whose impact on the wave can be captured by the stochastic parabolic equation.","The latter can be solved numerically by means of a split-step method, which replaces the continuous medium with a number of discrete phase screens derived from the medium's power spectrum.","We introduce and benchmark highly accurate and efficient hybrid phase screens for arbitrary power spectra which are based on the combination of Zernike and Fourier phase screens."],"url":"http://arxiv.org/abs/2402.04826v1","category":"physics.optics"}
{"created":"2024-02-07 11:26:00","title":"Color Recognition in Challenging Lighting Environments: CNN Approach","abstract":"Light plays a vital role in vision either human or machine vision, the perceived color is always based on the lighting conditions of the surroundings. Researchers are working to enhance the color detection techniques for the application of computer vision. They have implemented proposed several methods using different color detection approaches but still, there is a gap that can be filled. To address this issue, a color detection method, which is based on a Convolutional Neural Network (CNN), is proposed. Firstly, image segmentation is performed using the edge detection segmentation technique to specify the object and then the segmented object is fed to the Convolutional Neural Network trained to detect the color of an object in different lighting conditions. It is experimentally verified that our method can substantially enhance the robustness of color detection in different lighting conditions, and our method performed better results than existing methods.","sentences":["Light plays a vital role in vision either human or machine vision, the perceived color is always based on the lighting conditions of the surroundings.","Researchers are working to enhance the color detection techniques for the application of computer vision.","They have implemented proposed several methods using different color detection approaches but still, there is a gap that can be filled.","To address this issue, a color detection method, which is based on a Convolutional Neural Network (CNN), is proposed.","Firstly, image segmentation is performed using the edge detection segmentation technique to specify the object and then the segmented object is fed to the Convolutional Neural Network trained to detect the color of an object in different lighting conditions.","It is experimentally verified that our method can substantially enhance the robustness of color detection in different lighting conditions, and our method performed better results than existing methods."],"url":"http://arxiv.org/abs/2402.04762v1","category":"cs.CV"}
{"created":"2024-02-07 10:51:11","title":"Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes","abstract":"An extension of the Hawkes process, the Marked Hawkes process distinguishes itself by featuring variable jump size across each event, in contrast to the constant jump size observed in a Hawkes process without marks. While extensive literature has been dedicated to the non-parametric estimation of both the linear and non-linear Hawkes process, there remains a significant gap in the literature regarding the marked Hawkes process. In response to this, we propose a methodology for estimating the conditional intensity of the marked Hawkes process. We introduce two distinct models: \\textit{Shallow Neural Hawkes with marks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these approaches take the past arrival times and their corresponding marks as the input to obtain the arrival intensity. This approach is entirely non-parametric, preserving the interpretability associated with the marked Hawkes process. To validate the efficacy of our method, we subject the method to synthetic datasets with known ground truth. Additionally, we apply our method to model cryptocurrency order book data, demonstrating its applicability to real-world scenarios.","sentences":["An extension of the Hawkes process, the Marked Hawkes process distinguishes itself by featuring variable jump size across each event, in contrast to the constant jump size observed in a Hawkes process without marks.","While extensive literature has been dedicated to the non-parametric estimation of both the linear and non-linear Hawkes process, there remains a significant gap in the literature regarding the marked Hawkes process.","In response to this, we propose a methodology for estimating the conditional intensity of the marked Hawkes process.","We introduce two distinct models: \\textit{Shallow Neural Hawkes with marks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes.","Both these approaches take the past arrival times and their corresponding marks as the input to obtain the arrival intensity.","This approach is entirely non-parametric, preserving the interpretability associated with the marked Hawkes process.","To validate the efficacy of our method, we subject the method to synthetic datasets with known ground truth.","Additionally, we apply our method to model cryptocurrency order book data, demonstrating its applicability to real-world scenarios."],"url":"http://arxiv.org/abs/2402.04740v1","category":"stat.ML"}
{"created":"2024-02-07 10:47:36","title":"Unveiling a crystal's entropy of disorder via electron diffraction. A statistical mechanics approach","abstract":"Upon melting, the molecules in the crystal explore numerous configurations, reflecting an increase in disorder. The molar entropy of disorder can be defined by Bolzmann's formula dSd = Rln(Wd) where Wd is the increase in the number of microscopic states, so far inaccessible experimentally. We found that the Arrhenius frequency factor A of the electron diffraction signal decay provides Wd via an experimental equation A = AINTWd where AINT is an inelastic scattering cross-section. The method connects Clausius and Boltzmann experimentally and supplements the Clausius approach, being applicable to a femtogram quantity of thermally unstable and biomolecular crystals. The data also showed that crystal disordering and crystallization of melt are reciprocal, both governed by the entropy change, but manifesting in opposite directions.","sentences":["Upon melting, the molecules in the crystal explore numerous configurations, reflecting an increase in disorder.","The molar entropy of disorder can be defined by Bolzmann's formula dSd = Rln(Wd) where Wd is the increase in the number of microscopic states, so far inaccessible experimentally.","We found that the Arrhenius frequency factor A of the electron diffraction signal decay provides Wd via an experimental equation A = AINTWd where AINT is an inelastic scattering cross-section.","The method connects Clausius and Boltzmann experimentally and supplements the Clausius approach, being applicable to a femtogram quantity of thermally unstable and biomolecular crystals.","The data also showed that crystal disordering and crystallization of melt are reciprocal, both governed by the entropy change, but manifesting in opposite directions."],"url":"http://arxiv.org/abs/2402.04738v1","category":"physics.chem-ph"}
{"created":"2024-02-07 09:57:39","title":"Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have gained considerable traction for their capability to effectively process topological data, yet their interpretability remains a critical concern. Current interpretation methods are dominated by post-hoc explanations to provide a transparent and intuitive understanding of GNNs. However, they have limited performance in interpreting complicated subgraphs and can't utilize the explanation to advance GNN predictions. On the other hand, transparent GNN models are proposed to capture critical subgraphs. While such methods could improve GNN predictions, they usually don't perform well on explanations. Thus, it is desired for a new strategy to better couple GNN explanation and prediction. In this study, we have developed a novel interpretable causal GNN framework that incorporates retrieval-based causal learning with Graph Information Bottleneck (GIB) theory. The framework could semi-parametrically retrieve crucial subgraphs detected by GIB and compress the explanatory subgraphs via a causal module. The framework was demonstrated to consistently outperform state-of-the-art methods, and to achieve 32.71\\% higher precision on real-world explanation scenarios with diverse explanation types. More importantly, the learned explanations were shown able to also improve GNN prediction performance.","sentences":["Graph Neural Networks (GNNs) have gained considerable traction for their capability to effectively process topological data, yet their interpretability remains a critical concern.","Current interpretation methods are dominated by post-hoc explanations to provide a transparent and intuitive understanding of GNNs.","However, they have limited performance in interpreting complicated subgraphs and can't utilize the explanation to advance GNN predictions.","On the other hand, transparent GNN models are proposed to capture critical subgraphs.","While such methods could improve GNN predictions, they usually don't perform well on explanations.","Thus, it is desired for a new strategy to better couple GNN explanation and prediction.","In this study, we have developed a novel interpretable causal GNN framework that incorporates retrieval-based causal learning with Graph Information Bottleneck (GIB) theory.","The framework could semi-parametrically retrieve crucial subgraphs detected by GIB and compress the explanatory subgraphs via a causal module.","The framework was demonstrated to consistently outperform state-of-the-art methods, and to achieve 32.71\\% higher precision on real-world explanation scenarios with diverse explanation types.","More importantly, the learned explanations were shown able to also improve GNN prediction performance."],"url":"http://arxiv.org/abs/2402.04710v1","category":"cs.LG"}
{"created":"2024-02-07 09:32:32","title":"From explained variance of correlated components to PCA without orthogonality constraints","abstract":"Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of AZ 2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by 1 regularization, due to the difficulty of taking care of both the orthogonality constraint on loadings and the non differentiable 1 penalty. Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ. So we propose first a comprehensive study of mathematical and numerical properties of expvar(Y) for two existing definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions. Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints.","sentences":["Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of AZ 2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by 1 regularization, due to the difficulty of taking care of both the orthogonality constraint on loadings and the non differentiable 1 penalty.","Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ.","So we propose first a comprehensive study of mathematical and numerical properties of expvar(Y) for two existing definitions Zou et al.","[2006], Shen and Huang","[2008] and four new definitions.","Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints."],"url":"http://arxiv.org/abs/2402.04692v1","category":"stat.ML"}
{"created":"2024-02-07 09:02:22","title":"Quantitative isoperimetric inequalities for classical capillarity problems","abstract":"We consider capillarity functionals which measure the perimeter of sets contained in a Euclidean half-space assigning a constant weight $\\lambda \\in (-1,1)$ to the portion of the boundary that touches the boundary of the half-space. Depending on $\\lambda$, sets that minimize this capillarity perimeter among those with fixed volume are known to be suitable truncated balls lying on the boundary of the half-space.   We prove two quantitative isoperimetric inequalities for this class of capillarity problems: a first sharp inequality estimates the Fraenkel asymmetry of a competitor with respect to the optimal bubbles in terms of the energy deficit; a second inequality estimates a notion of asymmetry for the part of the boundary of a competitor that touches the boundary of the half-space in terms of the energy deficit.   After a symmetrization procedure, the inequalities follow from a novel combination of a quantitative ABP method with a selection-type argument.","sentences":["We consider capillarity functionals which measure the perimeter of sets contained in a Euclidean half-space assigning a constant weight $\\lambda \\in (-1,1)$ to the portion of the boundary that touches the boundary of the half-space.","Depending on $\\lambda$, sets that minimize this capillarity perimeter among those with fixed volume are known to be suitable truncated balls lying on the boundary of the half-space.   ","We prove two quantitative isoperimetric inequalities for this class of capillarity problems: a first sharp inequality estimates the Fraenkel asymmetry of a competitor with respect to the optimal bubbles in terms of the energy deficit; a second inequality estimates a notion of asymmetry for the part of the boundary of a competitor that touches the boundary of the half-space in terms of the energy deficit.   ","After a symmetrization procedure, the inequalities follow from a novel combination of a quantitative ABP method with a selection-type argument."],"url":"http://arxiv.org/abs/2402.04675v1","category":"math.AP"}
{"created":"2024-02-07 08:55:07","title":"A comparison of different approaches to compute surface tension contribution in incompressible two-phase flows","abstract":"We perform a quantitative assessment of different strategies to compute the contribution due to surface tension in incompressible two-phase flows using a conservative level set (CLS) method. More specifically, we compare classical approaches, such as the direct computation of the curvature from the level set or the Laplace-Beltrami operator, with an evolution equation for the mean curvature recently proposed in literature. We consider the test case of a static bubble, for which an exact solution for the pressure jump across the interface is available, and the test case of an oscillating bubble, showing pros and cons of the different approaches.","sentences":["We perform a quantitative assessment of different strategies to compute the contribution due to surface tension in incompressible two-phase flows using a conservative level set (CLS) method.","More specifically, we compare classical approaches, such as the direct computation of the curvature from the level set or the Laplace-Beltrami operator, with an evolution equation for the mean curvature recently proposed in literature.","We consider the test case of a static bubble, for which an exact solution for the pressure jump across the interface is available, and the test case of an oscillating bubble, showing pros and cons of the different approaches."],"url":"http://arxiv.org/abs/2402.04670v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 08:38:51","title":"Willmore surfaces and Hopf tori in homogeneous 3-manifolds","abstract":"Some classification results for closed surfaces in Berger spheres are presented. On the one hand, a Willmore functional for isometrically immersed surfaces into an homogeneous space $\\mathbb{E}^{3}(\\kappa,\\tau)$ with isometry group of dimension $4$ is defined and its first variational formula is computed. Then, we characterize Clifford and Hopf tori as the only Willmore surfaces satifying a sharp Simons-type integral inequality. On the other hand, we also obtain some integral inequalities for closed surfaces with constant extrinsic curvature in $\\mathbb{E}^3(\\kappa,\\tau)$, becoming equalities if and only if the surface is a Hopf torus in a Berger sphere.","sentences":["Some classification results for closed surfaces in Berger spheres are presented.","On the one hand, a Willmore functional for isometrically immersed surfaces into an homogeneous space $\\mathbb{E}^{3}(\\kappa,\\tau)$ with isometry group of dimension $4$ is defined and its first variational formula is computed.","Then, we characterize Clifford and Hopf tori as the only Willmore surfaces satifying a sharp Simons-type integral inequality.","On the other hand, we also obtain some integral inequalities for closed surfaces with constant extrinsic curvature in $\\mathbb{E}^3(\\kappa,\\tau)$, becoming equalities if and only if the surface is a Hopf torus in a Berger sphere."],"url":"http://arxiv.org/abs/2402.04654v1","category":"math.DG"}
{"created":"2024-02-07 07:29:59","title":"A stability result for Riemannian foliations","abstract":"We show that a Riemannian foliation F on a compact manifold M is stable, provided that the cohomology group H^1(F,NF) vanishes. Stability means that any foliation on M close enough to F is conjugate to F by means of a diffeomorphism.","sentences":["We show that a Riemannian foliation F on a compact manifold M is stable, provided that the cohomology group H^1(F,NF) vanishes.","Stability means that any foliation on M close enough to F is conjugate to F by means of a diffeomorphism."],"url":"http://arxiv.org/abs/2402.04633v1","category":"math.DG"}
{"created":"2024-02-07 07:29:50","title":"GSN: Generalisable Segmentation in Neural Radiance Field","abstract":"Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: https://vinayak-vg.github.io/GSN/","sentences":["Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene.","Semantic feature fields have been added to RFs to facilitate several segmentation tasks.","Generalised RF representations learn the principles of view interpolation.","A generalised RF can render new views of an unknown and untrained scene, given a few views.","We present a way to distil feature fields into the generalised GNT representation.","Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features.","This enables multi-view segmentation of arbitrary new scenes.","We show different semantic features being distilled into generalised RFs.","Our multi-view segmentation results are on par with methods that use traditional RFs.","GSN closes the gap between standard and generalisable RF methods significantly.","Project Page: https://vinayak-vg.github.io/GSN/"],"url":"http://arxiv.org/abs/2402.04632v1","category":"cs.CV"}
{"created":"2024-02-07 07:09:15","title":"Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective","abstract":"How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that aligns with the theory. We conclude that A-X dependence mediates the effect of graph convolution, such that smaller dependence improves GNN-based node classification.","sentences":["How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)?","The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from.","Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle.","Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon.","Thus, we raise two research questions.","First, how should A-X dependence be measured, while controlling for potential confounds?","Second, how does A-X dependence affect GNNs?","In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that aligns with the theory.","We conclude that A-X dependence mediates the effect of graph convolution, such that smaller dependence improves GNN-based node classification."],"url":"http://arxiv.org/abs/2402.04621v1","category":"cs.LG"}
{"created":"2024-02-07 06:16:49","title":"Early Stopping of Untrained Convolutional Neural Networks","abstract":"In recent years, new regularization methods based on (deep) neural networks have shown very promising empirical performance for the numerical solution of ill-posed problems, such as in medical imaging and imaging science. Due to the nonlinearity of neural networks, these methods often lack satisfactory theoretical justification. In this work, we rigorously discuss the convergence of a successful unsupervised approach that utilizes untrained convolutional neural networks to represent solutions to linear ill-posed problems. Untrained neural networks have particular appeal for many applications because they do not require paired training data. The regularization property of the approach relies solely on the architecture of the neural network instead. Due to the vast over-parameterization of the employed neural network, suitable early stopping is essential for the success of the method. We establish that the classical discrepancy principle is an adequate method for early stopping of two-layer untrained convolutional neural networks learned by gradient descent, and furthermore, it yields an approximation with minimax optimal convergence rates. Numerical results are also presented to illustrate the theoretical findings.","sentences":["In recent years, new regularization methods based on (deep) neural networks have shown very promising empirical performance for the numerical solution of ill-posed problems, such as in medical imaging and imaging science.","Due to the nonlinearity of neural networks, these methods often lack satisfactory theoretical justification.","In this work, we rigorously discuss the convergence of a successful unsupervised approach that utilizes untrained convolutional neural networks to represent solutions to linear ill-posed problems.","Untrained neural networks have particular appeal for many applications because they do not require paired training data.","The regularization property of the approach relies solely on the architecture of the neural network instead.","Due to the vast over-parameterization of the employed neural network, suitable early stopping is essential for the success of the method.","We establish that the classical discrepancy principle is an adequate method for early stopping of two-layer untrained convolutional neural networks learned by gradient descent, and furthermore, it yields an approximation with minimax optimal convergence rates.","Numerical results are also presented to illustrate the theoretical findings."],"url":"http://arxiv.org/abs/2402.04610v1","category":"math.NA"}
{"created":"2024-02-07 06:13:14","title":"Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach","abstract":"Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.","sentences":["Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive.","However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains.","Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks.","To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output.","The editing actions in this framework are specifically devised for text generation.","Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings."],"url":"http://arxiv.org/abs/2402.04609v1","category":"cs.CL"}
{"created":"2024-02-07 04:10:46","title":"$K$-theoretic wall-crossing formulas and multiple basic hypergeometric series","abstract":"We study $K$-theoretic integrals over famed quiver moduli via wall-crossing phenomena. We study the chainsaw quiver varieties, and consider generating functions defined by two types of $K$-theoretic classes. In particular, we focus on integrals over the handsaw quiver varieties of type $A_{1}$, and get functional equations for each of them. We also give explicit formula for these partition functions. In particular, we obtain geometric interpretation of transformation formulas for multiple basic hypergeometric series including the Kajihara transformation formula, and the one studied by Langer-Schlosser-Warnaar and Halln\\\"as-Langman-Noumi-Rosengren.","sentences":["We study $K$-theoretic integrals over famed quiver moduli via wall-crossing phenomena.","We study the chainsaw quiver varieties, and consider generating functions defined by two types of $K$-theoretic classes.","In particular, we focus on integrals over the handsaw quiver varieties of type $A_{1}$, and get functional equations for each of them.","We also give explicit formula for these partition functions.","In particular, we obtain geometric interpretation of transformation formulas for multiple basic hypergeometric series including the Kajihara transformation formula, and the one studied by Langer-Schlosser-Warnaar and Halln\\\"as-Langman-Noumi-Rosengren."],"url":"http://arxiv.org/abs/2402.04571v1","category":"math.AG"}
{"created":"2024-02-07 03:06:06","title":"Applying Simulation-Based Inference to Spectral and Spatial Information from the Galactic Center Gamma-Ray Excess","abstract":"The two most favored explanations of the Fermi Galactic Center gamma-ray excess (GCE) are millisecond pulsars and self annihilation of the smooth dark matter halo of the galaxy. In order to distinguish between these possibilities, we would like to optimally use all information in the available data, including photon direction and energy information. To date, analyses of the GCE have generally treated directional and energy information separately, or have ignored one or the other completely. Here, we develop a method for analyzing the GCE that relies on simulation-based inference with neural posterior models to jointly analyze photon directional and spectral information while correctly accounting for the spatial and energy resolution of the telescope, here assumed to be the Fermi Large Area Telescope (LAT). Our results also have implications for analyses of the diffuse gamma-ray background, which we discuss.","sentences":["The two most favored explanations of the Fermi Galactic Center gamma-ray excess (GCE) are millisecond pulsars and self annihilation of the smooth dark matter halo of the galaxy.","In order to distinguish between these possibilities, we would like to optimally use all information in the available data, including photon direction and energy information.","To date, analyses of the GCE have generally treated directional and energy information separately, or have ignored one or the other completely.","Here, we develop a method for analyzing the GCE that relies on simulation-based inference with neural posterior models to jointly analyze photon directional and spectral information while correctly accounting for the spatial and energy resolution of the telescope, here assumed to be the Fermi Large Area Telescope (LAT).","Our results also have implications for analyses of the diffuse gamma-ray background, which we discuss."],"url":"http://arxiv.org/abs/2402.04549v1","category":"astro-ph.HE"}
{"created":"2024-02-07 02:57:40","title":"BRI3L: A Brightness Illusion Image Dataset for Identification and Localization of Regions of Illusory Perception","abstract":"Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated using this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset. To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization. We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization. The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions. We also test the ability of state-of-theart diffusion models to generate brightness illusions. We have provided all the code, dataset, instructions etc in the github repo: https://github.com/aniket004/BRI3L","sentences":["Visual illusions play a significant role in understanding visual perception.","Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic.","To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches.","The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image.","Hence, both the classification and segmentation task can be evaluated using this dataset.","We follow the standard psychophysical experiments involving human subjects to validate the dataset.","To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization.","We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5)","Induced Grating illusion.","Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization.","The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions.","We also test the ability of state-of-theart diffusion models to generate brightness illusions.","We have provided all the code, dataset, instructions etc in the github repo: https://github.com/aniket004/BRI3L"],"url":"http://arxiv.org/abs/2402.04541v1","category":"cs.CV"}
{"created":"2024-02-07 02:16:05","title":"On the minimax robustness against correlation and heteroscedasticity of ordinary least squares among generalized least squares estimators of regression","abstract":"We present a result according to which certain functions of covariance matrices are maximized at scalar multiples of the identity matrix. This is used to show that the ordinary least squares (OLS) estimate of regression is minimax, in the class of generalized least squares estimates, when the maximum is taken over certain classes of error covariance structures and the loss function possesses a natural monotonicity property. We then consider regression models in which the response function is possibly misspecified, and show that OLS is no longer minimax. We argue that the gains from a minimax estimate are however often outweighed by the simplicity of OLS. We also investigate the interplay between minimax precision matrices and minimax designs. We find that the design has by far the major influence on efficiency and that, when the two are combined, OLS is generally at least 'almost' minimax, and often exactly so.","sentences":["We present a result according to which certain functions of covariance matrices are maximized at scalar multiples of the identity matrix.","This is used to show that the ordinary least squares (OLS) estimate of regression is minimax, in the class of generalized least squares estimates, when the maximum is taken over certain classes of error covariance structures and the loss function possesses a natural monotonicity property.","We then consider regression models in which the response function is possibly misspecified, and show that OLS is no longer minimax.","We argue that the gains from a minimax estimate are however often outweighed by the simplicity of OLS.","We also investigate the interplay between minimax precision matrices and minimax designs.","We find that the design has by far the major influence on efficiency and that, when the two are combined, OLS is generally at least 'almost' minimax, and often exactly so."],"url":"http://arxiv.org/abs/2402.04530v1","category":"math.ST"}
{"created":"2024-02-07 02:00:37","title":"Mean curvature flow with multiplicity $2$ convergence in closed manifolds","abstract":"We construct new examples of immortal mean curvature flow of smooth embedded connected hypersurfaces in closed manifolds, which converge to minimal hypersurfaces with multiplicity $2$ as time approaches infinity.","sentences":["We construct new examples of immortal mean curvature flow of smooth embedded connected hypersurfaces in closed manifolds, which converge to minimal hypersurfaces with multiplicity $2$ as time approaches infinity."],"url":"http://arxiv.org/abs/2402.04521v1","category":"math.DG"}
{"created":"2024-02-07 01:12:49","title":"Asymptotically AdS black hole with a conformally-coupled scalar field in the first-order formalism of gravity","abstract":"We present a novel asymptotically anti-de Sitter black hole solution with conformally-coupled scalar fields in the first-order formalism of gravity in four dimensions. To do so, we consider a one-parameter extension of conformal transformations by exploiting the fact that the vielbein and spin connection are regarded as independent fields. We solve the field equations analytically and obtain a static black hole solution with nontrivial torsion sourced by the conformal coupling between the scalar field and geometry. The presence of torsion renders the scalar field everywhere regular, while the curvature and torsion singularities coalesce into the origin. We show that this configuration is continuously connected to previously reported solutions in the limit of vanishing torsion and analyze its main properties, focusing on the consequences of the torsional singularity.","sentences":["We present a novel asymptotically anti-de Sitter black hole solution with conformally-coupled scalar fields in the first-order formalism of gravity in four dimensions.","To do so, we consider a one-parameter extension of conformal transformations by exploiting the fact that the vielbein and spin connection are regarded as independent fields.","We solve the field equations analytically and obtain a static black hole solution with nontrivial torsion sourced by the conformal coupling between the scalar field and geometry.","The presence of torsion renders the scalar field everywhere regular, while the curvature and torsion singularities coalesce into the origin.","We show that this configuration is continuously connected to previously reported solutions in the limit of vanishing torsion and analyze its main properties, focusing on the consequences of the torsional singularity."],"url":"http://arxiv.org/abs/2402.04503v1","category":"gr-qc"}
{"created":"2024-02-07 00:30:58","title":"De-amplifying Bias from Differential Privacy in Language Model Fine-tuning","abstract":"Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. Fairness aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model. The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy.","sentences":["Fairness and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models.","Fairness aims to reduce model bias for social/demographic sub-groups.","Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual's training data on the resulting model.","The trade-offs between privacy and fairness goals of trustworthy ML pose a challenge to those wishing to address both.","We show that DP amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs), producing models more biased than ones fine-tuned without DP.","We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups.","Through the case of binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA), a known method for addressing bias, also mitigates bias amplification by DP.","As a consequence, DP and CDA together can be used to fine-tune models while maintaining both fairness and privacy."],"url":"http://arxiv.org/abs/2402.04489v1","category":"cs.LG"}
{"created":"2024-02-07 00:29:55","title":"Quarkonium dynamics in the quantum Brownian regime with non-abelian quantum master equations","abstract":"We present numerical solutions in a one-dimensional setting of quantum master equations that have been recently derived. We focus on the dynamics of a single heavy quark-antiquark pair in a Quark-Gluon Plasma in thermal equilibrium, in the so-called quantum Brownian regime where the temperature of the plasma is large in comparison with the spacing between the energy levels of the $Q\\bar{Q}$ system. The one-dimensional potential used in the calculations has been adjusted so as to produce numbers that are relevant for the phenomenology of the charmonium. The equations are solved using different initial states and medium configurations. Various temperature regimes are studied and the effects of screening and collisions thoroughly analyzed. Technical features of the equations are analyzed. The contributions of the different operators that control the evolution are discussed as a function of the temperature. Some phenomenological consequences are addressed.","sentences":["We present numerical solutions in a one-dimensional setting of quantum master equations that have been recently derived.","We focus on the dynamics of a single heavy quark-antiquark pair in a Quark-Gluon Plasma in thermal equilibrium, in the so-called quantum Brownian regime where the temperature of the plasma is large in comparison with the spacing between the energy levels of the $Q\\bar{Q}$ system.","The one-dimensional potential used in the calculations has been adjusted so as to produce numbers that are relevant for the phenomenology of the charmonium.","The equations are solved using different initial states and medium configurations.","Various temperature regimes are studied and the effects of screening and collisions thoroughly analyzed.","Technical features of the equations are analyzed.","The contributions of the different operators that control the evolution are discussed as a function of the temperature.","Some phenomenological consequences are addressed."],"url":"http://arxiv.org/abs/2402.04488v1","category":"hep-ph"}
{"created":"2024-02-07 00:22:28","title":"Lie symmetries, conservation laws and exact solutions of a generalized quasilinear KdV equation with degenerate dispersion","abstract":"We provide a complete classification of point symmetries and low-order local conservation laws of the generalized quasilinear KdV equation in terms of the arbitrary function. The corresponding interpretation of symmetry transformation groups are given. In addition, a physical description of the conserved quantities is included. Finally, few travelling wave solutions have been obtained.","sentences":["We provide a complete classification of point symmetries and low-order local conservation laws of the generalized quasilinear KdV equation in terms of the arbitrary function.","The corresponding interpretation of symmetry transformation groups are given.","In addition, a physical description of the conserved quantities is included.","Finally, few travelling wave solutions have been obtained."],"url":"http://arxiv.org/abs/2402.04484v1","category":"nlin.SI"}
{"created":"2024-02-06 23:28:01","title":"Combinatorial 2d higher topological quantum field theory from a local cyclic $A_\\infty$ algebra","abstract":"We construct combinatorial analogs of 2d higher topological quantum field theories. We consider triangulations as vertices of a certain CW complex $\\Xi$. In the \"flip theory,\" cells of $\\Xi_\\mathrm{flip}$ correspond to polygonal decompositions obtained by erasing the edges in a triangulation. These theories assign to a cobordism $\\Sigma$ a cochain $Z$ on $\\Xi_\\mathrm{flip}$ constructed as a contraction of structure tensors of a cyclic $A_\\infty$ algebra $V$ assigned to polygons. The cyclic $A_\\infty$ equations imply the closedness equation $(\\delta+Q)Z=0$. In this context we define combinatorial BV operators and give examples with coefficients in $\\mathbb{Z}_2$.   In the \"secondary polytope theory,\" $\\Xi_\\mathrm{sp}$ is the secondary polytope (due to Gelfand-Kapranov-Zelevinsky) and the cyclic $A_\\infty$ algebra has to be replaced by an appropriate refinement that we call an $\\widehat{A}_\\infty$ algebra.   We conjecture the existence of a good Pachner CW complex $\\Xi$ for any cobordism, whose local combinatorics is descibed by secondary polytopes and the homotopy type is that of Zwiebach's moduli space of complex structures. Depending on this conjecture, one has an \"ideal model\" of combinatorial 2d HTQFT determined by a local $\\widehat{A}_\\infty$ algebra.","sentences":["We construct combinatorial analogs of 2d higher topological quantum field theories.","We consider triangulations as vertices of a certain CW complex $\\Xi$. In the \"flip theory,\" cells of $\\Xi_\\mathrm{flip}$ correspond to polygonal decompositions obtained by erasing the edges in a triangulation.","These theories assign to a cobordism $\\Sigma$ a cochain $Z$ on $\\Xi_\\mathrm{flip}$ constructed as a contraction of structure tensors of a cyclic $A_\\infty$ algebra $V$ assigned to polygons.","The cyclic $A_\\infty$ equations imply the closedness equation $(\\delta+Q)Z=0$. In this context we define combinatorial BV operators and give examples with coefficients in $\\mathbb{Z}_2$.   In the \"secondary polytope theory,\" $\\Xi_\\mathrm{sp}$ is the secondary polytope (due to Gelfand-Kapranov-Zelevinsky) and the cyclic $A_\\infty$ algebra has to be replaced by an appropriate refinement that we call an $\\widehat{A}_\\infty$ algebra.   ","We conjecture the existence of a good Pachner CW complex $\\Xi$ for any cobordism, whose local combinatorics is descibed by secondary polytopes and the homotopy type is that of Zwiebach's moduli space of complex structures.","Depending on this conjecture, one has an \"ideal model\" of combinatorial 2d HTQFT determined by a local $\\widehat{A}_\\infty$ algebra."],"url":"http://arxiv.org/abs/2402.04468v1","category":"math-ph"}
{"created":"2024-02-06 22:44:49","title":"A Moser-Bernstein problem for Riemannian warped products","abstract":"In this work we deal with an elliptic non-linear problem, which arises naturally from Riemannian geometry. This problem has clasically been studied in the the Euclidean $n$-dimensional space and it is known as the Moser-Bernstein problem. Nevertheless we solve this type of problems in a wide family of Riemannian manifolds, constructed as Riemannian warped products. More precicely, we study the entire solutions to the minimal hypersurface equation in a Riemannian warped product $M=P\\times_h\\mathbb{R}$, where $P$ is a complete Riemannian parabolic manifold and $h$ a positive smooth function on $P$.","sentences":["In this work we deal with an elliptic non-linear problem, which arises naturally from Riemannian geometry.","This problem has clasically been studied in the the Euclidean $n$-dimensional space and it is known as the Moser-Bernstein problem.","Nevertheless we solve this type of problems in a wide family of Riemannian manifolds, constructed as Riemannian warped products.","More precicely, we study the entire solutions to the minimal hypersurface equation in a Riemannian warped product $M=P\\times_h\\mathbb{R}$, where $P$ is a complete Riemannian parabolic manifold and $h$ a positive smooth function on $P$."],"url":"http://arxiv.org/abs/2402.04455v1","category":"math.DG"}
{"created":"2024-02-06 22:42:21","title":"Symbolic Computation of Sequential Equilibria","abstract":"The sequential equilibrium is a standard solution concept for extensive-form games with imperfect information that includes an explicit representation of the players' beliefs. An assessment consisting of a strategy and a belief is a sequential equilibrium if it satisfies the properties of sequential rationality and consistency.   Our main result is that both properties together can be written as a single finite system of polynomial equations and inequalities. The solutions to this system are exactly the sequential equilibria of the game. We construct this system explicitly and describe an implementation that solves it using cylindrical algebraic decomposition. To write consistency as a finite system of equations, we need to compute the extreme directions of a set of polyhedral cones. We propose a modified version of the double description method, optimized for this specific purpose. To the best of our knowledge, our implementation is the first to symbolically solve general finite imperfect information games for sequential equilibria.","sentences":["The sequential equilibrium is a standard solution concept for extensive-form games with imperfect information that includes an explicit representation of the players' beliefs.","An assessment consisting of a strategy and a belief is a sequential equilibrium if it satisfies the properties of sequential rationality and consistency.   ","Our main result is that both properties together can be written as a single finite system of polynomial equations and inequalities.","The solutions to this system are exactly the sequential equilibria of the game.","We construct this system explicitly and describe an implementation that solves it using cylindrical algebraic decomposition.","To write consistency as a finite system of equations, we need to compute the extreme directions of a set of polyhedral cones.","We propose a modified version of the double description method, optimized for this specific purpose.","To the best of our knowledge, our implementation is the first to symbolically solve general finite imperfect information games for sequential equilibria."],"url":"http://arxiv.org/abs/2402.04452v1","category":"cs.GT"}
{"created":"2024-02-06 22:36:52","title":"Vectorial active matter on the lattice: polar condensates and nematic filaments","abstract":"We introduce a novel lattice-gas cellular automaton (LGCA) for compressible vectorial active matter with polar and nematic velocity alignment. Interactions are, by construction, zero-range. For polar alignment, we show the system undergoes a phase transition that promotes aggregation with strong resemblance to the classic zero-range process. We find that above a critical point, the states of a macroscopic fraction of the particles in the system coalesce into the same state, sharing the same position and momentum (polar condensate). For nematic alignment, the system also exhibits condensation, but there exist fundamental differences: a macroscopic fraction of the particles in the system collapses into a filament, where particles possess only two possible momenta. Furthermore, we derive hydrodynamic equations for the active LGCA model to understand the phase transitions and condensation that undergoes the system. We also show that generically the discrete lattice symmetries -- e.g. of a square or hexagonal lattice -- affect drastically the emergent large-scale properties of on-lattice active systems. The study puts in evidence that aligning active matter on the lattice displays new behavior, including phase transitions to states that share similarities to condensation models.","sentences":["We introduce a novel lattice-gas cellular automaton (LGCA) for compressible vectorial active matter with polar and nematic velocity alignment.","Interactions are, by construction, zero-range.","For polar alignment, we show the system undergoes a phase transition that promotes aggregation with strong resemblance to the classic zero-range process.","We find that above a critical point, the states of a macroscopic fraction of the particles in the system coalesce into the same state, sharing the same position and momentum (polar condensate).","For nematic alignment, the system also exhibits condensation, but there exist fundamental differences: a macroscopic fraction of the particles in the system collapses into a filament, where particles possess only two possible momenta.","Furthermore, we derive hydrodynamic equations for the active LGCA model to understand the phase transitions and condensation that undergoes the system.","We also show that generically the discrete lattice symmetries -- e.g. of a square or hexagonal lattice -- affect drastically the emergent large-scale properties of on-lattice active systems.","The study puts in evidence that aligning active matter on the lattice displays new behavior, including phase transitions to states that share similarities to condensation models."],"url":"http://arxiv.org/abs/2402.04450v1","category":"cond-mat.soft"}
{"created":"2024-02-06 22:20:53","title":"Exploring higher-order neural network node interactions with total correlation","abstract":"In domains such as ecological systems, collaborations, and the human brain the variables interact in complex ways. Yet accurately characterizing higher-order variable interactions (HOIs) is a difficult problem that is further exacerbated when the HOIs change across the data. To solve this problem we propose a new method called Local Correlation Explanation (CorEx) to capture HOIs at a local scale by first clustering data points based on their proximity on the data manifold. We then use a multivariate version of the mutual information called the total correlation, to construct a latent factor representation of the data within each cluster to learn the local HOIs. We use Local CorEx to explore HOIs in synthetic and real world data to extract hidden insights about the data structure. Lastly, we demonstrate Local CorEx's suitability to explore and interpret the inner workings of trained neural networks.","sentences":["In domains such as ecological systems, collaborations, and the human brain the variables interact in complex ways.","Yet accurately characterizing higher-order variable interactions (HOIs) is a difficult problem that is further exacerbated when the HOIs change across the data.","To solve this problem we propose a new method called Local Correlation Explanation (CorEx) to capture HOIs at a local scale by first clustering data points based on their proximity on the data manifold.","We then use a multivariate version of the mutual information called the total correlation, to construct a latent factor representation of the data within each cluster to learn the local HOIs.","We use Local CorEx to explore HOIs in synthetic and real world data to extract hidden insights about the data structure.","Lastly, we demonstrate Local CorEx's suitability to explore and interpret the inner workings of trained neural networks."],"url":"http://arxiv.org/abs/2402.04440v1","category":"cs.LG"}
{"created":"2024-02-06 22:08:18","title":"Local Index Theory for the Rarita-Schwinger Operator","abstract":"We prove the local index theorem for the Rarita-Schwinger operator and higher Dirac operators using Gilkey's invariance theory. That is, we show that the supertrace of the heat kernel of a given geometric operator converges as time goes to zero, and identifies the limit as the Chern-Weil form of the Atiyah-Singer integrand.","sentences":["We prove the local index theorem for the Rarita-Schwinger operator and higher Dirac operators using Gilkey's invariance theory.","That is, we show that the supertrace of the heat kernel of a given geometric operator converges as time goes to zero, and identifies the limit as the Chern-Weil form of the Atiyah-Singer integrand."],"url":"http://arxiv.org/abs/2402.04430v1","category":"math.DG"}
{"created":"2024-02-06 22:06:02","title":"At most one solution to $a^x + b^y = c^z$ for some ranges of $a$, $b$, $c$","abstract":"We consider the number of solutions in positive integers $(x,y,z)$ for the purely exponential Diophantine equation $a^x+b^y =c^z$ (with $\\gcd(a,b)=1$). Apart from a list of known exceptions, a conjecture published in 2016 claims that this equation has at most one solution in positive integers $x$, $y$, and $z$. We show that this is true for some ranges of $a$, $b$, $c$, for instance, when $1 < a,b < 3600$ and $c<10^{10}$. The conjecture also holds for small pairs $(a,b)$ independent of $c$, where $2 \\le a,b \\le 10$ with $\\gcd(a,b)=1$. We show that the Pillai equation $a^x - b^y = r > 0$ has at most one solution (with a known list of exceptions) when $2 \\le a,b \\le 3600$. Finally, the primitive case of the Je\\'smanowicz conjecture holds when $a \\le 10^6$ or when $b \\le 10^6$. This work highlights the power of some ideas of Miyazaki and Pink and the usefulness of a theorem by Scott.","sentences":["We consider the number of solutions in positive integers $(x,y,z)$ for the purely exponential Diophantine equation $a^x+b^y =c^z$ (with $\\gcd(a,b)=1$).","Apart from a list of known exceptions, a conjecture published in 2016 claims that this equation has at most one solution in positive integers $x$, $y$, and $z$. We show that this is true for some ranges of $a$, $b$, $c$, for instance, when $1 < a,b < 3600$ and $c<10^{10}$.","The conjecture also holds for small pairs $(a,b)$ independent of $c$, where $2 \\le a,b \\le 10$ with $\\gcd(a,b)=1$. We show that the Pillai equation $a^x - b^y = r > 0$ has at most one solution (with a known list of exceptions) when $2 \\le a,b \\le 3600$.","Finally, the primitive case of the Je\\'smanowicz conjecture holds when $a \\le 10^6$ or when $b \\le 10^6$.","This work highlights the power of some ideas of Miyazaki and Pink and the usefulness of a theorem by Scott."],"url":"http://arxiv.org/abs/2402.04428v1","category":"math.NT"}
{"created":"2024-02-06 21:07:09","title":"Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques","abstract":"In this work, we focused on deep learning image processing in the context of oral rare diseases, which pose challenges due to limited data availability. A crucial step involves teeth detection, segmentation and numbering in panoramic radiographs. To this end, we used a dataset consisting of 156 panoramic radiographs from individuals with rare oral diseases and labeled by experts. We trained the Detection Transformer (DETR) neural network for teeth detection, segmentation, and numbering the 52 teeth classes. In addition, we used data augmentation techniques, including geometric transformations. Finally, we generated new panoramic images using inpainting techniques with stable diffusion, by removing teeth from a panoramic radiograph and integrating teeth into it. The results showed a mAP exceeding 0,69 for DETR without data augmentation. The mAP was improved to 0,82 when data augmentation techniques are used. Furthermore, we observed promising performances when using new panoramic radiographs generated with inpainting technique, with mAP of 0,76.","sentences":["In this work, we focused on deep learning image processing in the context of oral rare diseases, which pose challenges due to limited data availability.","A crucial step involves teeth detection, segmentation and numbering in panoramic radiographs.","To this end, we used a dataset consisting of 156 panoramic radiographs from individuals with rare oral diseases and labeled by experts.","We trained the Detection Transformer (DETR) neural network for teeth detection, segmentation, and numbering the 52 teeth classes.","In addition, we used data augmentation techniques, including geometric transformations.","Finally, we generated new panoramic images using inpainting techniques with stable diffusion, by removing teeth from a panoramic radiograph and integrating teeth into it.","The results showed a mAP exceeding 0,69 for DETR without data augmentation.","The mAP was improved to 0,82 when data augmentation techniques are used.","Furthermore, we observed promising performances when using new panoramic radiographs generated with inpainting technique, with mAP of 0,76."],"url":"http://arxiv.org/abs/2402.04408v1","category":"cs.CV"}
{"created":"2024-02-06 21:05:27","title":"Interpretable domain knowledge enhanced machine learning framework on axial capacity prediction of circular CFST columns","abstract":"This study introduces a novel machine learning framework, integrating domain knowledge, to accurately predict the bearing capacity of CFSTs, bridging the gap between traditional engineering and machine learning techniques. Utilizing a comprehensive database of 2621 experimental data points on CFSTs, we developed a Domain Knowledge Enhanced Neural Network (DKNN) model. This model incorporates advanced feature engineering techniques, including Pearson correlation, XGBoost, and Random tree algorithms. The DKNN model demonstrated a marked improvement in prediction accuracy, with a Mean Absolute Percentage Error (MAPE) reduction of over 50% compared to existing models. Its robustness was confirmed through extensive performance assessments, maintaining high accuracy even in noisy environments. Furthermore, sensitivity and SHAP analysis were conducted to assess the contribution of each effective parameter to axial load capacity and propose design recommendations for the diameter of cross-section, material strength range and material combination. This research advances CFST predictive modelling, showcasing the potential of integrating machine learning with domain expertise in structural engineering. The DKNN model sets a new benchmark for accuracy and reliability in the field.","sentences":["This study introduces a novel machine learning framework, integrating domain knowledge, to accurately predict the bearing capacity of CFSTs, bridging the gap between traditional engineering and machine learning techniques.","Utilizing a comprehensive database of 2621 experimental data points on CFSTs, we developed a Domain Knowledge Enhanced Neural Network (DKNN) model.","This model incorporates advanced feature engineering techniques, including Pearson correlation, XGBoost, and Random tree algorithms.","The DKNN model demonstrated a marked improvement in prediction accuracy, with a Mean Absolute Percentage Error (MAPE) reduction of over 50% compared to existing models.","Its robustness was confirmed through extensive performance assessments, maintaining high accuracy even in noisy environments.","Furthermore, sensitivity and SHAP analysis were conducted to assess the contribution of each effective parameter to axial load capacity and propose design recommendations for the diameter of cross-section, material strength range and material combination.","This research advances CFST predictive modelling, showcasing the potential of integrating machine learning with domain expertise in structural engineering.","The DKNN model sets a new benchmark for accuracy and reliability in the field."],"url":"http://arxiv.org/abs/2402.04405v1","category":"cs.CE"}
{"created":"2024-02-06 20:51:24","title":"Auto-Encoder Optimized PAM IM/DD Transceivers for Amplified Fiber Links","abstract":"We examine pulse amplitude modulation (PAM) for intensity modulation and direct detection systems. Using a straight-forward, mixed noise model, we optimize the constellations with an autoencoder-based neural network (NN), an improve required signal-to-noise ratio of 4 dB for amplified spontaneous emission (ASE)-limited PAM4 and PAM8, without increasing system complexity. Performance can also be improved in O-band wavelength division multiplexing system with semiconductor optical amplifier amplification and chromatic dispersion. We show via simulation that for such a system operating at 53 Gbaud, we can extend the reach of PAM4 by 10-25 km with an optimized constellation and a NN decoder. We present an experimental validation of 4 dB improvement of an ASE-limited PAM4 at 60 Gbaud using an optimized constellation and a NN decoder.","sentences":["We examine pulse amplitude modulation (PAM) for intensity modulation and direct detection systems.","Using a straight-forward, mixed noise model, we optimize the constellations with an autoencoder-based neural network (NN), an improve required signal-to-noise ratio of 4 dB for amplified spontaneous emission (ASE)-limited PAM4 and PAM8, without increasing system complexity.","Performance can also be improved in O-band wavelength division multiplexing system with semiconductor optical amplifier amplification and chromatic dispersion.","We show via simulation that for such a system operating at 53 Gbaud, we can extend the reach of PAM4 by 10-25 km with an optimized constellation and a NN decoder.","We present an experimental validation of 4 dB improvement of an ASE-limited PAM4 at 60 Gbaud using an optimized constellation and a NN decoder."],"url":"http://arxiv.org/abs/2402.04395v1","category":"eess.SP"}
{"created":"2024-02-06 20:50:19","title":"Total mean curvature surfaces in the product space $\\mathbb{S}^n\\times\\mathbb{R}$ and applications","abstract":"The total mean curvature functional for submanifolds into the Riemannian product space $\\mathbb{S}^n\\times\\mathbb{R}$ is considered and its first variational formula is presented. Later on, two second order differential operators are defined and a nice integral inequality relating both of them is proved. Finally we prove our main result: an integral inequality for closed stationary $\\mathcal{H}$-surfaces in $\\mathbb{S}^n\\times\\mathbb{R}$, characterizing the cases where the equality is attained.","sentences":["The total mean curvature functional for submanifolds into the Riemannian product space $\\mathbb{S}^n\\times\\mathbb{R}$ is considered and its first variational formula is presented.","Later on, two second order differential operators are defined and a nice integral inequality relating both of them is proved.","Finally we prove our main result: an integral inequality for closed stationary $\\mathcal{H}$-surfaces in $\\mathbb{S}^n\\times\\mathbb{R}$, characterizing the cases where the equality is attained."],"url":"http://arxiv.org/abs/2402.04394v1","category":"math.DG"}
{"created":"2024-02-06 20:43:05","title":"Locating the roots of a quadratic equation in one variable through a Line-Circumference (LC) geometric construction in the plane of complex numbers","abstract":"This paper describes a geometrical method for finding the roots $r_1$, $r_2$ of a quadratic equation in one complex variable of the form $x^2+c_1 x+c_2=0$, by means of a Line $L$ and a Circumference $C$ in the complex plane, constructed from known coefficients $c_1$, $c_2$. This Line-Circumference (LC) geometric structure contains the sought roots $r_1$, $r_2$ at the intersections of its component elements $L$ and $C$. Line $L$ in the LC structure is mapped onto Circumference $C$ by a Mobius transformation. The location and inclination angle of Line $L$ can be computed directly from coefficients $c_1$, $c_2$, while Circumference $C$ is constructed by dividing the constant term $c_2$ by each point from Line $L$. This paper describes and develops the technical details for the LC Method, and then shows how the LC Method works through a numerical example. The quadratic LC method described here can be extended to polynomials in one variable of degree greater than two, in order to find initial approximations to their roots. As an additional feature, this paper also studies an interesting property of the rectilinear segments connecting key points in a quadratic LC structure.","sentences":["This paper describes a geometrical method for finding the roots $r_1$, $r_2$ of a quadratic equation in one complex variable of the form $x^2+c_1 x+c_2=0$, by means of a Line $L$ and a Circumference $C$ in the complex plane, constructed from known coefficients $c_1$, $c_2$. This Line-Circumference (LC) geometric structure contains the sought roots $r_1$, $r_2$ at the intersections of its component elements $L$ and $C$.","Line $L$ in the LC structure is mapped onto Circumference $C$ by a Mobius transformation.","The location and inclination angle of Line $L$ can be computed directly from coefficients $c_1$, $c_2$, while Circumference $C$ is constructed by dividing the constant term $c_2$ by each point from Line $L$. This paper describes and develops the technical details for the LC Method, and then shows how the LC Method works through a numerical example.","The quadratic LC method described here can be extended to polynomials in one variable of degree greater than two, in order to find initial approximations to their roots.","As an additional feature, this paper also studies an interesting property of the rectilinear segments connecting key points in a quadratic LC structure."],"url":"http://arxiv.org/abs/2402.04385v1","category":"math.NA"}
{"created":"2024-02-06 20:43:04","title":"Denoising Diffusion Probabilistic Models in Six Simple Steps","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.","sentences":["Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations.","Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear.","The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space.","Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice.","On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow.","In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale.","We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning."],"url":"http://arxiv.org/abs/2402.04384v1","category":"cs.LG"}
{"created":"2024-02-06 20:38:46","title":"Assured LLM-Based Software Engineering","abstract":"In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code   - does not regress the properties of the original code?   - improves the original in a verifiable and measurable way?   To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM's propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.   This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.","sentences":["In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code   - does not regress the properties of the original code?   ","- improves the original in a verifiable and measurable way?   ","To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement.","Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees.","This overcomes the potential problem of LLM's propensity to hallucinate.","It allows us to generate code using LLMs, independently of any human.","The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.   ","This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal."],"url":"http://arxiv.org/abs/2402.04380v1","category":"cs.SE"}
{"created":"2024-02-06 20:24:07","title":"Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data","abstract":"The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset. To prevent leakage of sensitive data, we consider using differentially-private (DP), synthetic training data instead of real training data to train an ML model. A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution. Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions. We perform extensive experimentation alongside our theoretical results.","sentences":["The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset.","To prevent leakage of sensitive data, we consider using differentially-private (DP), synthetic training data instead of real training data to train an ML model.","A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution.","Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions.","We perform extensive experimentation alongside our theoretical results."],"url":"http://arxiv.org/abs/2402.04375v1","category":"cs.LG"}
{"created":"2024-02-06 20:09:52","title":"Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources","abstract":"The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing intermediate features to minimize the distribution gap among multi-agent features. Intensive experiments on the public OPV2V and V2XSet datasets underscore FDA's effectiveness in point cloud-based 3D object detection, presenting it as an invaluable augmentation to existing multi-agent perception systems.","sentences":["The diverse agents in multi-agent perception systems may be from different companies.","Each company might use the identical classic neural network architecture based encoder for feature extraction.","However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system.","The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception.","In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems.","To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception.","FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing intermediate features to minimize the distribution gap among multi-agent features.","Intensive experiments on the public OPV2V and V2XSet datasets underscore FDA's effectiveness in point cloud-based 3D object detection, presenting it as an invaluable augmentation to existing multi-agent perception systems."],"url":"http://arxiv.org/abs/2402.04273v1","category":"cs.CV"}
{"created":"2024-02-06 20:06:07","title":"Exponential Separation Between Powers of Regular and General Resolution Over Parities","abstract":"Proving super-polynomial lower bounds on the size of proofs of unsatisfiability of Boolean formulas using resolution over parities, is an outstanding problem that has received a lot of attention after its introduction by Raz and Tzamaret [Ann. Pure Appl. Log.'08]. Very recently, Efremenko, Garl\\'ik and Itsykson [ECCC'23] proved the first exponential lower bounds on the size of ResLin proofs that were additionally restricted to be bottom-regular. We show that there are formulas for which such regular ResLin proofs of unsatisfiability continue to have exponential size even though there exists short proofs of their unsatisfiability in ordinary, non-regular resolution. This is the first super-polynomial separation between the power of general ResLin and and that of regular ResLin for any natural notion of regularity.   Our argument, while building upon the work of Efremenko et al, uses additional ideas from the literature on lifting theorems.","sentences":["Proving super-polynomial lower bounds on the size of proofs of unsatisfiability of Boolean formulas using resolution over parities, is an outstanding problem that has received a lot of attention after its introduction by Raz and Tzamaret","[Ann.","Pure Appl.","Log.'08].","Very recently, Efremenko, Garl\\'ik and Itsykson","[ECCC'23] proved the first exponential lower bounds on the size of ResLin proofs that were additionally restricted to be bottom-regular.","We show that there are formulas for which such regular ResLin proofs of unsatisfiability continue to have exponential size even though there exists short proofs of their unsatisfiability in ordinary, non-regular resolution.","This is the first super-polynomial separation between the power of general ResLin and and that of regular ResLin for any natural notion of regularity.   ","Our argument, while building upon the work of Efremenko et al, uses additional ideas from the literature on lifting theorems."],"url":"http://arxiv.org/abs/2402.04364v1","category":"cs.CC"}
{"created":"2024-02-06 20:03:35","title":"Neural Networks Learn Statistics of Increasing Complexity","abstract":"The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.","sentences":["The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations.","In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later.","We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs.","Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class.","Code is available at https://github.com/EleutherAI/features-across-time."],"url":"http://arxiv.org/abs/2402.04362v1","category":"cs.LG"}
{"created":"2024-02-06 19:43:52","title":"Building Retrieval Systems for the ClueWeb22-B Corpus","abstract":"The ClueWeb22 dataset containing nearly 10 billion documents was released in 2022 to support academic and industry research. The goal of this project was to build retrieval baselines for the English section of the \"super head\" part (category B) of this dataset. These baselines can then be used by the research community to compare their systems and also to generate data to train/evaluate new retrieval and ranking algorithms. The report covers sparse and dense first stage retrievals as well as neural rerankers that were implemented for this dataset. These systems are available as a service on a Carnegie Mellon University cluster.","sentences":["The ClueWeb22 dataset containing nearly 10 billion documents was released in 2022 to support academic and industry research.","The goal of this project was to build retrieval baselines for the English section of the \"super head\" part (category B) of this dataset.","These baselines can then be used by the research community to compare their systems and also to generate data to train/evaluate new retrieval and ranking algorithms.","The report covers sparse and dense first stage retrievals as well as neural rerankers that were implemented for this dataset.","These systems are available as a service on a Carnegie Mellon University cluster."],"url":"http://arxiv.org/abs/2402.04357v1","category":"cs.IR"}
{"created":"2024-02-06 19:16:00","title":"Spin evolution of neutron stars","abstract":"In this paper we review the basics of magneto-rotational properties of neutron stars focusing on spin-up/spin-down behavior at different evolutionary stages. The main goal is to provide equations for the spin frequency changes in various regimes (radio pulsar, propeller, accretor, etc.). Since presently spin behavior of neutron stars at all stages remains a subject of many uncertainties, we review different suggestions made over the years in the literature.","sentences":["In this paper we review the basics of magneto-rotational properties of neutron stars focusing on spin-up/spin-down behavior at different evolutionary stages.","The main goal is to provide equations for the spin frequency changes in various regimes (radio pulsar, propeller, accretor, etc.).","Since presently spin behavior of neutron stars at all stages remains a subject of many uncertainties, we review different suggestions made over the years in the literature."],"url":"http://arxiv.org/abs/2402.04331v1","category":"astro-ph.HE"}
{"created":"2024-02-06 19:00:01","title":"Cosmological Observatories","abstract":"We study the static patch of de Sitter space in the presence of a timelike boundary. We impose that the conformal class of the induced metric and the trace of the extrinsic curvature, $K$, are fixed at the boundary. We present the thermodynamic structure of de Sitter space subject to these boundary conditions, for static and spherically symmetric configurations. In three spacetime dimensions, and taking $K$ constant on a toroidal Euclidean boundary, we find that the spacetime is thermally stable for all $K$. In four spacetime dimensions, the thermal stability depends on the value of $K$. It is established that for sufficiently large $K$, the de Sitter static patch subject to conformal boundary conditions is thermally stable. This contrasts the Dirichlet problem for which the region encompassing the cosmological horizon has negative specific heat. We present an analysis of the linearised Einstein equations subject to conformal boundary conditions. In the worldline limit of the timelike boundary, the underlying modes are linked to the quasinormal modes of the static patch. In the limit where the timelike boundary approaches the cosmological event horizon, the linearised modes are interpreted in terms of the shear and sound modes of a fluid dynamical system. Additionally, we find modes with a frequency of positive imaginary part. Measured in a local inertial reference frame, and taking the stretched cosmological horizon limit, these modes grow at most polynomially.","sentences":["We study the static patch of de Sitter space in the presence of a timelike boundary.","We impose that the conformal class of the induced metric and the trace of the extrinsic curvature, $K$, are fixed at the boundary.","We present the thermodynamic structure of de Sitter space subject to these boundary conditions, for static and spherically symmetric configurations.","In three spacetime dimensions, and taking $K$ constant on a toroidal Euclidean boundary, we find that the spacetime is thermally stable for all $K$. In four spacetime dimensions, the thermal stability depends on the value of $K$. It is established that for sufficiently large $K$, the de Sitter static patch subject to conformal boundary conditions is thermally stable.","This contrasts the Dirichlet problem for which the region encompassing the cosmological horizon has negative specific heat.","We present an analysis of the linearised Einstein equations subject to conformal boundary conditions.","In the worldline limit of the timelike boundary, the underlying modes are linked to the quasinormal modes of the static patch.","In the limit where the timelike boundary approaches the cosmological event horizon, the linearised modes are interpreted in terms of the shear and sound modes of a fluid dynamical system.","Additionally, we find modes with a frequency of positive imaginary part.","Measured in a local inertial reference frame, and taking the stretched cosmological horizon limit, these modes grow at most polynomially."],"url":"http://arxiv.org/abs/2402.04305v1","category":"hep-th"}
{"created":"2024-02-06 16:02:51","title":"Translating the future: Image-to-image translation for the prediction of future brain metabolism","abstract":"Alzheimer's disease (AD) is a progressive neurodegenerative disorder leading to cognitive decline. [$^{18}$F]-Fluorodeoxyglucose positron emission tomography ([$^{18}$F]-FDG PET) is used to monitor brain metabolism, aiding in the diagnosis and assessment of AD over time. However, the feasibility of multi-time point [$^{18}$F]-FDG PET scans for diagnosis is limited due to radiation exposure, cost, and patient burden. To address this, we have developed a predictive image-to-image translation (I2I) model to forecast future [$^{18}$F]-FDG PET scans using baseline and year-one data. The proposed model employs a convolutional neural network architecture with long-short term memory and was trained on [$^{18}$F]-FDG PET data from 161 individuals from the Alzheimer's Disease Neuroimaging Initiative. Our I2I network showed high accuracy in predicting year-two [18F]-FDG PET scans, with a mean absolute error of 0.031 and a structural similarity index of 0.961. Furthermore, the model successfully predicted PET scans up to seven years post-baseline. Notably, the predicted [$^{18}$F]-FDG PET signal in an AD-susceptible meta-region was highly accurate for individuals with mild cognitive impairment across years. In contrast, a linear model was sufficient for predicting brain metabolism in cognitively normal and dementia subjects. In conclusion, both the I2I network and the linear model could offer valuable prognostic insights, guiding early intervention strategies to preemptively address anticipated declines in brain metabolism and potentially to monitor treatment effects.","sentences":["Alzheimer's disease (AD) is a progressive neurodegenerative disorder leading to cognitive decline.","[$^{18}$F]-Fluorodeoxyglucose positron emission tomography ([$^{18}$F]-FDG PET) is used to monitor brain metabolism, aiding in the diagnosis and assessment of AD over time.","However, the feasibility of multi-time point [$^{18}$F]-FDG PET scans for diagnosis is limited due to radiation exposure, cost, and patient burden.","To address this, we have developed a predictive image-to-image translation (I2I) model to forecast future [$^{18}$F]-FDG PET scans using baseline and year-one data.","The proposed model employs a convolutional neural network architecture with long-short term memory and was trained on [$^{18}$F]-FDG PET data from 161 individuals from the Alzheimer's Disease Neuroimaging Initiative.","Our I2I network showed high accuracy in predicting year-two [18F]-FDG PET scans, with a mean absolute error of 0.031 and a structural similarity index of 0.961.","Furthermore, the model successfully predicted PET scans up to seven years post-baseline.","Notably, the predicted [$^{18}$F]-FDG PET signal in an AD-susceptible meta-region was highly accurate for individuals with mild cognitive impairment across years.","In contrast, a linear model was sufficient for predicting brain metabolism in cognitively normal and dementia subjects.","In conclusion, both the I2I network and the linear model could offer valuable prognostic insights, guiding early intervention strategies to preemptively address anticipated declines in brain metabolism and potentially to monitor treatment effects."],"url":"http://arxiv.org/abs/2402.04299v1","category":"eess.IV"}
{"created":"2024-02-06 14:40:26","title":"LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\\times$ Faster Inference","abstract":"Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into the student MLPs to achieve topology-aware distillation and resistance to over-smoothing. Experiments on eight hypergraph datasets demonstrate that even without hypergraph dependency, the proposed LightHGNNs can still achieve competitive or even better performance than HGNNs and outperform vanilla MLPs by $16.3$ on average. Extensive experiments on three graph datasets further show the average best performance of our LightHGNNs compared with all other methods. Experiments on synthetic hypergraphs with 5.5w vertices indicate LightHGNNs can run $100\\times$ faster than HGNNs, showcasing their ability for latency-sensitive deployments.","sentences":["Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling.","However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment.","In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference.","In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed.","Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity.","LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into the student MLPs to achieve topology-aware distillation and resistance to over-smoothing.","Experiments on eight hypergraph datasets demonstrate that even without hypergraph dependency, the proposed LightHGNNs can still achieve competitive or even better performance than HGNNs and outperform vanilla MLPs by $16.3$ on average.","Extensive experiments on three graph datasets further show the average best performance of our LightHGNNs compared with all other methods.","Experiments on synthetic hypergraphs with 5.5w vertices indicate LightHGNNs can run $100\\times$ faster than HGNNs, showcasing their ability for latency-sensitive deployments."],"url":"http://arxiv.org/abs/2402.04296v1","category":"cs.LG"}
{"created":"2024-02-07 17:07:41","title":"Simulated Overparameterization","abstract":"In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called \"majority kernels\", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and models, even outperforming strong baselines such as combinatorial optimization methods based on submodular optimization.","sentences":["In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP).","SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models.","SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference.","Building upon this framework, we present a novel, architecture agnostic algorithm called \"majority kernels\", which seamlessly integrates with predominant architectures, including Transformer models.","Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks.","Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time.","The proposed approach shows strong performance on a wide variety of datasets and models, even outperforming strong baselines such as combinatorial optimization methods based on submodular optimization."],"url":"http://arxiv.org/abs/2402.05033v1","category":"cs.LG"}
{"created":"2024-02-07 16:09:35","title":"Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults Explore and Learn Smartphone Applications","abstract":"The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications. While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults' smartphone app exploration remains insufficiently explored. Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools. Our research highlights AR's effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process. We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration. Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios.","sentences":["The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications.","While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults' smartphone app exploration remains insufficiently explored.","Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools.","Our research highlights AR's effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process.","We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration.","Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios."],"url":"http://arxiv.org/abs/2402.04991v1","category":"cs.HC"}
{"created":"2024-02-07 14:49:10","title":"Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching","abstract":"Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \\textbf{C}raf\\textbf{T}ing \\textbf{R}ationa\\textbf{L} trajectory (\\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for gradient matching. Theoretically, CTRL can effectively neutralize the impact of accumulated errors on the performance of condensed graphs. We provide extensive experiments on various graph datasets and downstream tasks to support the effectiveness of CTRL. Code is released at https://github.com/NUS-HPC-AI-Lab/CTRL.","sentences":["Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns.","As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set.","Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories.","Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs.","In light of this, we propose a novel graph condensation method named \\textbf{C}raf\\textbf{T}ing \\textbf{R}ationa\\textbf{L} trajectory (\\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for gradient matching.","Theoretically, CTRL can effectively neutralize the impact of accumulated errors on the performance of condensed graphs.","We provide extensive experiments on various graph datasets and downstream tasks to support the effectiveness of CTRL.","Code is released at https://github.com/NUS-HPC-AI-Lab/CTRL."],"url":"http://arxiv.org/abs/2402.04924v1","category":"cs.LG"}
{"created":"2024-02-07 14:47:08","title":"Is Two-shot All You Need? A Label-efficient Approach for Video Segmentation in Breast Ultrasound","abstract":"Breast lesion segmentation from breast ultrasound (BUS) videos could assist in early diagnosis and treatment. Existing video object segmentation (VOS) methods usually require dense annotation, which is often inaccessible for medical datasets. Furthermore, they suffer from accumulative errors and a lack of explicit space-time awareness. In this work, we propose a novel two-shot training paradigm for BUS video segmentation. It not only is able to capture free-range space-time consistency but also utilizes a source-dependent augmentation scheme. This label-efficient learning framework is validated on a challenging in-house BUS video dataset. Results showed that it gained comparable performance to the fully annotated ones given only 1.9% training labels.","sentences":["Breast lesion segmentation from breast ultrasound (BUS) videos could assist in early diagnosis and treatment.","Existing video object segmentation (VOS) methods usually require dense annotation, which is often inaccessible for medical datasets.","Furthermore, they suffer from accumulative errors and a lack of explicit space-time awareness.","In this work, we propose a novel two-shot training paradigm for BUS video segmentation.","It not only is able to capture free-range space-time consistency but also utilizes a source-dependent augmentation scheme.","This label-efficient learning framework is validated on a challenging in-house BUS video dataset.","Results showed that it gained comparable performance to the fully annotated ones given only 1.9% training labels."],"url":"http://arxiv.org/abs/2402.04921v1","category":"eess.IV"}
{"created":"2024-02-07 13:01:43","title":"Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses","abstract":"Understanding preferences, opinions, and sentiment of the workforce is paramount for effective employee lifecycle management. Open-ended survey responses serve as a valuable source of information. This paper proposes a machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch open-ended responses in employee satisfaction surveys. Our approach aims to overcome the inherent noise and variability in these responses, enabling a comprehensive analysis of sentiments that can support employee lifecycle management. Through response clustering we identify six key aspects (salary, schedule, contact, communication, personal attention, agreements), which we validate by domain experts. We compile a dataset of 1,458 Dutch survey responses, revealing label imbalance in aspects and sentiments. We propose few-shot approaches for ABSA based on Dutch BERT models, and compare them against bag-of-words and zero-shot baselines. Our work significantly contributes to the field of ABSA by demonstrating the first successful application of Dutch pre-trained language models to aspect-based sentiment analysis in the domain of human resources (HR).","sentences":["Understanding preferences, opinions, and sentiment of the workforce is paramount for effective employee lifecycle management.","Open-ended survey responses serve as a valuable source of information.","This paper proposes a machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch open-ended responses in employee satisfaction surveys.","Our approach aims to overcome the inherent noise and variability in these responses, enabling a comprehensive analysis of sentiments that can support employee lifecycle management.","Through response clustering we identify six key aspects (salary, schedule, contact, communication, personal attention, agreements), which we validate by domain experts.","We compile a dataset of 1,458 Dutch survey responses, revealing label imbalance in aspects and sentiments.","We propose few-shot approaches for ABSA based on Dutch BERT models, and compare them against bag-of-words and zero-shot baselines.","Our work significantly contributes to the field of ABSA by demonstrating the first successful application of Dutch pre-trained language models to aspect-based sentiment analysis in the domain of human resources (HR)."],"url":"http://arxiv.org/abs/2402.04812v1","category":"cs.CL"}
{"created":"2024-02-07 12:35:31","title":"Scalable Multi-view Clustering via Explicit Kernel Features Maps","abstract":"A growing awareness of multi-view learning as an important component in data science and machine learning is a consequence of the increasing prevalence of multiple views in real-world applications, especially in the context of networks. In this paper we introduce a new scalability framework for multi-view subspace clustering. An efficient optimization strategy is proposed, leveraging kernel feature maps to reduce the computational burden while maintaining good clustering performance. The scalability of the algorithm means that it can be applied to large-scale datasets, including those with millions of data points, using a standard machine, in a few minutes. We conduct extensive experiments on real-world benchmark networks of various sizes in order to evaluate the performance of our algorithm against state-of-the-art multi-view subspace clustering methods and attributed-network multi-view approaches.","sentences":["A growing awareness of multi-view learning as an important component in data science and machine learning is a consequence of the increasing prevalence of multiple views in real-world applications, especially in the context of networks.","In this paper we introduce a new scalability framework for multi-view subspace clustering.","An efficient optimization strategy is proposed, leveraging kernel feature maps to reduce the computational burden while maintaining good clustering performance.","The scalability of the algorithm means that it can be applied to large-scale datasets, including those with millions of data points, using a standard machine, in a few minutes.","We conduct extensive experiments on real-world benchmark networks of various sizes in order to evaluate the performance of our algorithm against state-of-the-art multi-view subspace clustering methods and attributed-network multi-view approaches."],"url":"http://arxiv.org/abs/2402.04794v1","category":"cs.LG"}
{"created":"2024-02-07 11:56:34","title":"A fast score-based search algorithm for maximal ancestral graphs using entropy","abstract":"\\emph{Maximal ancestral graph} (MAGs) is a class of graphical model that extend the famous \\emph{directed acyclic graph} in the presence of latent confounders. Most score-based approaches to learn the unknown MAG from empirical data rely on BIC score which suffers from instability and heavy computations. We propose to use the framework of imsets \\citep{studeny2006probabilistic} to score MAGs using empirical entropy estimation and the newly proposed \\emph{refined Markov property} \\citep{hu2023towards}. Our graphical search procedure is similar to \\citet{claassen2022greedy} but improved from our theoretical results. We show that our search algorithm is polynomial in number of nodes by restricting degree, maximal head size and number of discriminating paths. In simulated experiment, our algorithm shows superior performance compared to other state of art MAG learning algorithms.","sentences":["\\emph{Maximal ancestral graph} (MAGs) is a class of graphical model that extend the famous \\emph{directed acyclic graph} in the presence of latent confounders.","Most score-based approaches to learn the unknown MAG from empirical data rely on BIC score which suffers from instability and heavy computations.","We propose to use the framework of imsets \\citep{studeny2006probabilistic} to score MAGs using empirical entropy estimation and the newly proposed \\emph{refined Markov property} \\citep{hu2023towards}.","Our graphical search procedure is similar to \\citet{claassen2022greedy} but improved from our theoretical results.","We show that our search algorithm is polynomial in number of nodes by restricting degree, maximal head size and number of discriminating paths.","In simulated experiment, our algorithm shows superior performance compared to other state of art MAG learning algorithms."],"url":"http://arxiv.org/abs/2402.04777v1","category":"stat.ML"}
{"created":"2024-02-07 10:55:59","title":"Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers","abstract":"N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\\sim$50\\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80\\%). In this work, we study the effectiveness of existing sparse training recipes at \\textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the flow of gradients towards pruned elements. Our approach improves the model quality by up to 2$\\%$ and 5$\\%$ in vision and language models at high sparsity regime, respectively. We also evaluate the trade-off between model accuracy and training compute cost in terms of FLOPs. At iso-training FLOPs, our method yields better performance compared to conventional sparse training recipes, exhibiting an accuracy improvement of up to 2$\\%$. The source code is available at https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity.","sentences":["N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency.","Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead.","There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\\sim$50\\%).","Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80\\%).","In this work, we study the effectiveness of existing sparse training recipes at \\textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions.","We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes.","To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the flow of gradients towards pruned elements.","Our approach improves the model quality by up to 2$\\%$ and 5$\\%$ in vision and language models at high sparsity regime, respectively.","We also evaluate the trade-off between model accuracy and training compute cost in terms of FLOPs.","At iso-training FLOPs, our method yields better performance compared to conventional sparse training recipes, exhibiting an accuracy improvement of up to 2$\\%$.","The source code is available at https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity."],"url":"http://arxiv.org/abs/2402.04744v1","category":"cs.LG"}
{"created":"2024-02-07 08:38:12","title":"An Over Complete Deep Learning Method for Inverse Problems","abstract":"Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering. Recent machine learning techniques based on proximal and diffusion-based methods have shown promising results. However, as we show in this work, they can also face challenges when applied to some exemplary problems. We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions. The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector. We demonstrate the merit of this approach on several exemplary and common inverse problems.","sentences":["Obtaining meaningful solutions for inverse problems has been a major challenge with many applications in science and engineering.","Recent machine learning techniques based on proximal and diffusion-based methods have shown promising results.","However, as we show in this work, they can also face challenges when applied to some exemplary problems.","We show that similar to previous works on over-complete dictionaries, it is possible to overcome these shortcomings by embedding the solution into higher dimensions.","The novelty of the work proposed is that we jointly design and learn the embedding and the regularizer for the embedding vector.","We demonstrate the merit of this approach on several exemplary and common inverse problems."],"url":"http://arxiv.org/abs/2402.04653v1","category":"cs.LG"}
{"created":"2024-02-07 07:26:49","title":"LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors","abstract":"Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.","sentences":["Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training.","However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells).","This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general.","Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training.","In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment.","Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins."],"url":"http://arxiv.org/abs/2402.04630v1","category":"cs.CV"}
{"created":"2024-02-07 07:12:27","title":"Validity-Preserving Delta Debugging via Generator","abstract":"Reducing test inputs that trigger bugs is crucial for efficient debugging. Delta debugging is the most popular approach for this purpose. When test inputs need to conform to certain specifications, existing delta debugging practice encounters a validity problem: it blindly applies reduction rules, producing a large number of invalid test inputs that do not satisfy the required specifications. This overall diminishing effectiveness and efficiency becomes even more pronounced when the specifications extend beyond syntactical structures. Our key insight is that we should leverage input generators, which are aware of these specifications, to generate valid reduced inputs, rather than straightforwardly performing reduction on test inputs. In this paper, we propose a generator-based delta debugging method, namely GReduce, which derives validity-preserving reducers. Specifically, given a generator and its execution, demonstrating how the bug-inducing test input is generated, GReduce searches for other executions on the generator that yield reduced, valid test inputs. To evaluate the effectiveness, efficiency, and versatility of GReduce, we apply GReduce and the state-of-the-art reducer Perses in three domains: graphs, deep learning models, and JavaScript programs. The results of GReduce are 28.5%, 34.6%, 75.6% in size of those from Perses, and GReduce takes 17.5%, 0.6%, 65.4% time taken by Perses.","sentences":["Reducing test inputs that trigger bugs is crucial for efficient debugging.","Delta debugging is the most popular approach for this purpose.","When test inputs need to conform to certain specifications, existing delta debugging practice encounters a validity problem: it blindly applies reduction rules, producing a large number of invalid test inputs that do not satisfy the required specifications.","This overall diminishing effectiveness and efficiency becomes even more pronounced when the specifications extend beyond syntactical structures.","Our key insight is that we should leverage input generators, which are aware of these specifications, to generate valid reduced inputs, rather than straightforwardly performing reduction on test inputs.","In this paper, we propose a generator-based delta debugging method, namely GReduce, which derives validity-preserving reducers.","Specifically, given a generator and its execution, demonstrating how the bug-inducing test input is generated, GReduce searches for other executions on the generator that yield reduced, valid test inputs.","To evaluate the effectiveness, efficiency, and versatility of GReduce, we apply GReduce and the state-of-the-art reducer Perses in three domains: graphs, deep learning models, and JavaScript programs.","The results of GReduce are 28.5%, 34.6%, 75.6% in size of those from Perses, and GReduce takes 17.5%, 0.6%, 65.4% time taken by Perses."],"url":"http://arxiv.org/abs/2402.04623v1","category":"cs.SE"}
{"created":"2024-02-07 07:07:02","title":"CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients","abstract":"The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable, providing anytime accessibility, saving time, and accommodating diverse literacy levels. Trust was established through expert verification. Broadly, our results could inform future work on designing expert-mediated LLM bots.","sentences":["The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks.","Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information.","Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information.","However, the pressure on experts has led to reduced communication time, impacting information sharing.","To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs).","Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously.","CataractBot features multimodal support and multilingual capabilities.","In an in-the-wild deployment study with 49 participants, CataractBot proved valuable, providing anytime accessibility, saving time, and accommodating diverse literacy levels.","Trust was established through expert verification.","Broadly, our results could inform future work on designing expert-mediated LLM bots."],"url":"http://arxiv.org/abs/2402.04620v1","category":"cs.HC"}
{"created":"2024-02-07 06:30:39","title":"Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces","abstract":"Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\\alpha$ divergences.","sentences":["Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures.","A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients.","Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences.","Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\\alpha$ divergences."],"url":"http://arxiv.org/abs/2402.04613v1","category":"stat.ML"}
{"created":"2024-02-07 05:05:21","title":"Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation","abstract":"Accurate tooth identification and segmentation in Cone Beam Computed Tomography (CBCT) dental images can significantly enhance the efficiency and precision of manual diagnoses performed by dentists. However, existing segmentation methods are mainly developed based on large data volumes training, on which their annotations are extremely time-consuming. Meanwhile, the teeth of each class in CBCT dental images being closely positioned, coupled with subtle inter-class differences, gives rise to the challenge of indistinct boundaries when training model with limited data. To address these challenges, this study aims to propose a tasked-oriented Masked Auto-Encoder paradigm to effectively utilize large amounts of unlabeled data to achieve accurate tooth segmentation with limited labeled data. Specifically, we first construct a self-supervised pre-training framework of masked auto encoder to efficiently utilize unlabeled data to enhance the network performance. Subsequently, we introduce a sparse masked prompt mechanism based on graph attention to incorporate boundary information of the teeth, aiding the network in learning the anatomical structural features of teeth. To the best of our knowledge, we are pioneering the integration of the mask pre-training paradigm into the CBCT tooth segmentation task. Extensive experiments demonstrate both the feasibility of our proposed method and the potential of the boundary prompt mechanism.","sentences":["Accurate tooth identification and segmentation in Cone Beam Computed Tomography (CBCT) dental images can significantly enhance the efficiency and precision of manual diagnoses performed by dentists.","However, existing segmentation methods are mainly developed based on large data volumes training, on which their annotations are extremely time-consuming.","Meanwhile, the teeth of each class in CBCT dental images being closely positioned, coupled with subtle inter-class differences, gives rise to the challenge of indistinct boundaries when training model with limited data.","To address these challenges, this study aims to propose a tasked-oriented Masked Auto-Encoder paradigm to effectively utilize large amounts of unlabeled data to achieve accurate tooth segmentation with limited labeled data.","Specifically, we first construct a self-supervised pre-training framework of masked auto encoder to efficiently utilize unlabeled data to enhance the network performance.","Subsequently, we introduce a sparse masked prompt mechanism based on graph attention to incorporate boundary information of the teeth, aiding the network in learning the anatomical structural features of teeth.","To the best of our knowledge, we are pioneering the integration of the mask pre-training paradigm into the CBCT tooth segmentation task.","Extensive experiments demonstrate both the feasibility of our proposed method and the potential of the boundary prompt mechanism."],"url":"http://arxiv.org/abs/2402.04587v1","category":"cs.CV"}
{"created":"2024-02-07 03:36:41","title":"DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion","abstract":"Human de-occlusion, which aims to infer the appearance of invisible human parts from an occluded image, has great value in many human-related tasks, such as person re-id, and intention inference. To address this task, this paper proposes a dynamic mask-aware transformer (DMAT), which dynamically augments information from human regions and weakens that from occlusion. First, to enhance token representation, we design an expanded convolution head with enlarged kernels, which captures more local valid context and mitigates the influence of surrounding occlusion. To concentrate on the visible human parts, we propose a novel dynamic multi-head human-mask guided attention mechanism through integrating multiple masks, which can prevent the de-occluded regions from assimilating to the background. Besides, a region upsampling strategy is utilized to alleviate the impact of occlusion on interpolated images. During model learning, an amodal loss is developed to further emphasize the recovery effect of human regions, which also refines the model's convergence. Extensive experiments on the AHP dataset demonstrate its superior performance compared to recent state-of-the-art methods.","sentences":["Human de-occlusion, which aims to infer the appearance of invisible human parts from an occluded image, has great value in many human-related tasks, such as person re-id, and intention inference.","To address this task, this paper proposes a dynamic mask-aware transformer (DMAT), which dynamically augments information from human regions and weakens that from occlusion.","First, to enhance token representation, we design an expanded convolution head with enlarged kernels, which captures more local valid context and mitigates the influence of surrounding occlusion.","To concentrate on the visible human parts, we propose a novel dynamic multi-head human-mask guided attention mechanism through integrating multiple masks, which can prevent the de-occluded regions from assimilating to the background.","Besides, a region upsampling strategy is utilized to alleviate the impact of occlusion on interpolated images.","During model learning, an amodal loss is developed to further emphasize the recovery effect of human regions, which also refines the model's convergence.","Extensive experiments on the AHP dataset demonstrate its superior performance compared to recent state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.04558v1","category":"cs.CV"}
{"created":"2024-02-07 03:18:00","title":"Curvature-Informed SGD via General Purpose Lie-Group Preconditioners","abstract":"We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Our proposed approach offers a promising direction for improving the convergence of SGD with low computational overhead. We demonstrate that Preconditioned SGD (PSGD) outperforms SoTA on Vision, NLP, and RL tasks across multiple modern deep-learning architectures. We have provided code for reproducing toy and large scale experiments in this paper.","sentences":["We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm.","Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner.","We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping.","To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups.","The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers.","As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios.","Our proposed approach offers a promising direction for improving the convergence of SGD with low computational overhead.","We demonstrate that Preconditioned SGD (PSGD) outperforms SoTA on Vision, NLP, and RL tasks across multiple modern deep-learning architectures.","We have provided code for reproducing toy and large scale experiments in this paper."],"url":"http://arxiv.org/abs/2402.04553v1","category":"cs.LG"}
{"created":"2024-02-07 03:16:16","title":"Gamma-ray Bursts as Distance Indicators by a Machine Learning Approach","abstract":"Gamma-ray bursts (GRBs) can be probes of the early universe, but currently, only 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known redshifts ($z$) due to observational limitations. To address this, we estimated the GRB redshift (distance) via a supervised machine learning model that uses optical afterglow observed by Swift and ground-based telescopes. The inferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with the observed redshifts, thus proving the reliability of this method. The inferred and observed redshifts allow us to estimate the number of GRBs occurring at a given redshift (GRB rate) to be 7.6-8 $yr^{-1} Gpc^{-1}$ for $1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared this rate with the star formation rate highlighting a discrepancy of a factor of 3 at $z<1$.","sentences":["Gamma-ray bursts (GRBs) can be probes of the early universe, but currently, only 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known redshifts ($z$) due to observational limitations.","To address this, we estimated the GRB redshift (distance) via a supervised machine learning model that uses optical afterglow observed by Swift and ground-based telescopes.","The inferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with the observed redshifts, thus proving the reliability of this method.","The inferred and observed redshifts allow us to estimate the number of GRBs occurring at a given redshift (GRB rate) to be 7.6-8 $yr^{-1} Gpc^{-1}$ for $1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared this rate with the star formation rate highlighting a discrepancy of a factor of 3 at $z<1$."],"url":"http://arxiv.org/abs/2402.04551v1","category":"astro-ph.HE"}
{"created":"2024-02-07 02:53:06","title":"Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers","abstract":"Graph transformers typically lack direct pair-to-pair communication, instead forcing neighboring pairs to exchange information via a common node. We propose the Triplet Graph Transformer (TGT) that enables direct communication between two neighboring pairs in a graph via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning. We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP).","sentences":["Graph transformers typically lack direct pair-to-pair communication, instead forcing neighboring pairs to exchange information via a common node.","We propose the Triplet Graph Transformer (TGT) that enables direct communication between two neighboring pairs in a graph via novel triplet attention and aggregation mechanisms.","TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks.","A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance.","Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE.","We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via transfer learning.","We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP)."],"url":"http://arxiv.org/abs/2402.04538v1","category":"cs.LG"}
{"created":"2024-02-07 00:45:31","title":"The Fine-Grained Complexity of Gradient Computation for Training Large Language Models","abstract":"Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.","sentences":["Large language models (LLMs) have made fundamental contributions over the last a few years.","To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations.","The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation.","In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false.","In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training.","This completely characterizes the fine-grained complexity of every step of LLM training."],"url":"http://arxiv.org/abs/2402.04497v1","category":"cs.LG"}
{"created":"2024-02-07 00:23:20","title":"Incentivized Truthful Communication for Federated Bandits","abstract":"To enhance the efficiency and practicality of federated bandit learning, recent advances have introduced incentives to motivate communication among clients, where a client participates only when the incentive offered by the server outweighs its participation cost. However, existing incentive mechanisms naively assume the clients are truthful: they all report their true cost and thus the higher cost one participating client claims, the more the server has to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming to optimize their own utility by misreporting. To address this issue, we propose an incentive compatible (i.e., truthful) communication protocol, named Truth-FedBan, where the incentive for each participant is independent of its self-reported cost, and reporting the true cost is the only way to achieve the best utility. More importantly, Truth-FedBan still guarantees the sub-linear regret and communication cost without any overheads. In other words, the core conceptual contribution of this paper is, for the first time, demonstrating the possibility of simultaneously achieving incentive compatibility and nearly optimal regret in federated bandit learning. Extensive numerical studies further validate the effectiveness of our proposed solution.","sentences":["To enhance the efficiency and practicality of federated bandit learning, recent advances have introduced incentives to motivate communication among clients, where a client participates only when the incentive offered by the server outweighs its participation cost.","However, existing incentive mechanisms naively assume the clients are truthful: they all report their true cost and thus the higher cost one participating client claims, the more the server has to pay.","Therefore, such mechanisms are vulnerable to strategic clients aiming to optimize their own utility by misreporting.","To address this issue, we propose an incentive compatible (i.e., truthful) communication protocol, named Truth-FedBan, where the incentive for each participant is independent of its self-reported cost, and reporting the true cost is the only way to achieve the best utility.","More importantly, Truth-FedBan still guarantees the sub-linear regret and communication cost without any overheads.","In other words, the core conceptual contribution of this paper is, for the first time, demonstrating the possibility of simultaneously achieving incentive compatibility and nearly optimal regret in federated bandit learning.","Extensive numerical studies further validate the effectiveness of our proposed solution."],"url":"http://arxiv.org/abs/2402.04485v1","category":"cs.LG"}
{"created":"2024-02-06 22:42:28","title":"The Potential of AutoML for Recommender Systems","abstract":"Automated Machine Learning (AutoML) has greatly advanced applications of Machine Learning (ML) including model compression, machine translation, and computer vision. Recommender Systems (RecSys) can be seen as an application of ML. Yet, AutoML has found little attention in the RecSys community; nor has RecSys found notable attention in the AutoML community. Only few and relatively simple Automated Recommender Systems (AutoRecSys) libraries exist that adopt AutoML techniques. However, these libraries are based on student projects and do not offer the features and thorough development of AutoML libraries. We set out to determine how AutoML libraries perform in the scenario of an inexperienced user who wants to implement a recommender system. We compared the predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from 15 libraries, including a mean predictor baseline, on 14 explicit feedback RecSys datasets. To simulate the perspective of an inexperienced user, the algorithms were evaluated with default hyperparameters. We found that AutoML and AutoRecSys libraries performed best. AutoML libraries performed best for six of the 14 datasets (43%), but it was not always the same AutoML library performing best. The single-best library was the AutoRecSys library Auto-Surprise, which performed best on five datasets (36%). On three datasets (21%), AutoML libraries performed poorly, and RecSys libraries with default parameters performed best. Although, while obtaining 50% of all placements in the top five per dataset, RecSys algorithms fall behind AutoML on average. ML algorithms generally performed the worst.","sentences":["Automated Machine Learning (AutoML) has greatly advanced applications of Machine Learning (ML) including model compression, machine translation, and computer vision.","Recommender Systems (RecSys) can be seen as an application of ML.","Yet, AutoML has found little attention in the RecSys community; nor has RecSys found notable attention in the AutoML community.","Only few and relatively simple Automated Recommender Systems (AutoRecSys) libraries exist that adopt AutoML techniques.","However, these libraries are based on student projects and do not offer the features and thorough development of AutoML libraries.","We set out to determine how AutoML libraries perform in the scenario of an inexperienced user who wants to implement a recommender system.","We compared the predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from 15 libraries, including a mean predictor baseline, on 14 explicit feedback RecSys datasets.","To simulate the perspective of an inexperienced user, the algorithms were evaluated with default hyperparameters.","We found that AutoML and AutoRecSys libraries performed best.","AutoML libraries performed best for six of the 14 datasets (43%), but it was not always the same AutoML library performing best.","The single-best library was the AutoRecSys library Auto-Surprise, which performed best on five datasets (36%).","On three datasets (21%), AutoML libraries performed poorly, and RecSys libraries with default parameters performed best.","Although, while obtaining 50% of all placements in the top five per dataset, RecSys algorithms fall behind AutoML on average.","ML algorithms generally performed the worst."],"url":"http://arxiv.org/abs/2402.04453v1","category":"cs.IR"}
{"created":"2024-02-06 22:32:05","title":"Pushing the limits of cell segmentation models for imaging mass cytometry","abstract":"Imaging mass cytometry (IMC) is a relatively new technique for imaging biological tissue at subcellular resolution. In recent years, learning-based segmentation methods have enabled precise quantification of cell type and morphology, but typically rely on large datasets with fully annotated ground truth (GT) labels. This paper explores the effects of imperfect labels on learning-based segmentation models and evaluates the generalisability of these models to different tissue types. Our results show that removing 50% of cell annotations from GT masks only reduces the dice similarity coefficient (DSC) score to 0.874 (from 0.889 achieved by a model trained on fully annotated GT masks). This implies that annotation time can in fact be reduced by at least half without detrimentally affecting performance. Furthermore, training our single-tissue model on imperfect labels only decreases DSC by 0.031 on an unseen tissue type compared to its multi-tissue counterpart, with negligible qualitative differences in segmentation. Additionally, bootstrapping the worst-performing model (with 5% of cell annotations) a total of ten times improves its original DSC score of 0.720 to 0.829. These findings imply that less time and work can be put into the process of producing comparable segmentation models; this includes eliminating the need for multiple IMC tissue types during training, whilst also providing the potential for models with very few labels to improve on themselves. Source code is available on GitHub: https://github.com/kimberley/ISBI2024.","sentences":["Imaging mass cytometry (IMC) is a relatively new technique for imaging biological tissue at subcellular resolution.","In recent years, learning-based segmentation methods have enabled precise quantification of cell type and morphology, but typically rely on large datasets with fully annotated ground truth (GT) labels.","This paper explores the effects of imperfect labels on learning-based segmentation models and evaluates the generalisability of these models to different tissue types.","Our results show that removing 50% of cell annotations from GT masks only reduces the dice similarity coefficient (DSC) score to 0.874 (from 0.889 achieved by a model trained on fully annotated GT masks).","This implies that annotation time can in fact be reduced by at least half without detrimentally affecting performance.","Furthermore, training our single-tissue model on imperfect labels only decreases DSC by 0.031 on an unseen tissue type compared to its multi-tissue counterpart, with negligible qualitative differences in segmentation.","Additionally, bootstrapping the worst-performing model (with 5% of cell annotations) a total of ten times improves its original DSC score of 0.720 to 0.829.","These findings imply that less time and work can be put into the process of producing comparable segmentation models; this includes eliminating the need for multiple IMC tissue types during training, whilst also providing the potential for models with very few labels to improve on themselves.","Source code is available on GitHub: https://github.com/kimberley/ISBI2024."],"url":"http://arxiv.org/abs/2402.04446v1","category":"eess.IV"}
{"created":"2024-02-06 22:15:09","title":"Structured Entity Extraction Using Large Language Models","abstract":"Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.","sentences":["Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text.","This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues.","We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP)","Metric designed to appropriately assess model performance on this task.","Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages.","Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction."],"url":"http://arxiv.org/abs/2402.04437v1","category":"cs.CL"}
{"created":"2024-02-06 22:13:51","title":"Continuous Multidimensional Scaling","abstract":"Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space. As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects. Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects. Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities. But what if $n$ increases? It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space. We present such a reformulation and derive some consequences.","sentences":["Multidimensional scaling (MDS) is the act of embedding proximity information about a set of $n$ objects in $d$-dimensional Euclidean space.","As originally conceived by the psychometric community, MDS was concerned with embedding a fixed set of proximities associated with a fixed set of objects.","Modern concerns, e.g., that arise in developing asymptotic theories for statistical inference on random graphs, more typically involve studying the limiting behavior of a sequence of proximities associated with an increasing set of objects.","Standard results from the theory of point-to-set maps imply that, if $n$ is fixed, then the limit of the embedded structures is the embedded structure of the limiting proximities.","But what if $n$ increases?","It then becomes necessary to reformulate MDS so that the entire sequence of embedding problems can be viewed as a sequence of optimization problems in a fixed space.","We present such a reformulation and derive some consequences."],"url":"http://arxiv.org/abs/2402.04436v1","category":"stat.ML"}
{"created":"2024-02-06 21:38:29","title":"What limits performance of weakly supervised deep learning for chest CT classification?","abstract":"Weakly supervised learning with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels. However, little is known about the limitations of such weakly supervised learning and the effect of these constraints on disease classification performance. In this paper, we test the effects of such weak supervision by examining model tolerance for three conditions. First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data. Second, we assessed the impact of dataset size by varying the amount of training data. Third, we compared performance differences between binary and multi-label classification. Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance. Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencing a plateau in performance at 75% of training data. Last, the binary model outperformed the multilabel model in every disease category. However, such interpretations may be misleading, as the binary model was heavily influenced by co-occurring diseases and may not have learned the specific features of the disease in the image. In conclusion, this study may help the medical imaging community understand the benefits and risks of weak supervision with noisy labels. Such studies demonstrate the need to build diverse, large-scale datasets and to develop explainable and responsible AI.","sentences":["Weakly supervised learning with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels.","However, little is known about the limitations of such weakly supervised learning and the effect of these constraints on disease classification performance.","In this paper, we test the effects of such weak supervision by examining model tolerance for three conditions.","First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data.","Second, we assessed the impact of dataset size by varying the amount of training data.","Third, we compared performance differences between binary and multi-label classification.","Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance.","Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencing a plateau in performance at 75% of training data.","Last, the binary model outperformed the multilabel model in every disease category.","However, such interpretations may be misleading, as the binary model was heavily influenced by co-occurring diseases and may not have learned the specific features of the disease in the image.","In conclusion, this study may help the medical imaging community understand the benefits and risks of weak supervision with noisy labels.","Such studies demonstrate the need to build diverse, large-scale datasets and to develop explainable and responsible AI."],"url":"http://arxiv.org/abs/2402.04419v1","category":"eess.IV"}
{"created":"2024-02-06 21:33:34","title":"Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit","abstract":"We study a robust multi-agent multi-armed bandit problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough. The system's objective is to efficiently ensure the cumulative rewards gained by the honest participants. To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants. This allows various malicious behaviors and the maintenance of participant privacy. More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that requires less information from participants through secure multi-party computation, and design the chain-participant interaction and an incentive mechanism to encourage participants' participation. Notably, we are the first to prove the theoretical guarantee of the proposed algorithms by regret analyses in the context of optimality in blockchains. Unlike existing work that integrates blockchains with learning problems such as federated learning which mainly focuses on numerical optimality, we demonstrate that the regret of honest participants is upper bounded by $log{T}$. This is consistent with the multi-agent multi-armed bandit problem without malicious participants and the robust multi-agent multi-armed bandit problem with purely Byzantine attacks.","sentences":["We study a robust multi-agent multi-armed bandit problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious.","The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough.","The system's objective is to efficiently ensure the cumulative rewards gained by the honest participants.","To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants.","This allows various malicious behaviors and the maintenance of participant privacy.","More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that requires less information from participants through secure multi-party computation, and design the chain-participant interaction and an incentive mechanism to encourage participants' participation.","Notably, we are the first to prove the theoretical guarantee of the proposed algorithms by regret analyses in the context of optimality in blockchains.","Unlike existing work that integrates blockchains with learning problems such as federated learning which mainly focuses on numerical optimality, we demonstrate that the regret of honest participants is upper bounded by $log{T}$. This is consistent with the multi-agent multi-armed bandit problem without malicious participants and the robust multi-agent multi-armed bandit problem with purely Byzantine attacks."],"url":"http://arxiv.org/abs/2402.04417v1","category":"cs.LG"}
{"created":"2024-02-06 21:14:45","title":"Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton","abstract":"This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.","sentences":["This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs).","Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service.","Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM.","This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA.","The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs.","Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent."],"url":"http://arxiv.org/abs/2402.04411v1","category":"cs.CL"}
{"created":"2024-02-06 21:04:57","title":"Edge-Parallel Graph Encoder Embedding","abstract":"New algorithms for embedding graphs have reduced the asymptotic complexity of finding low-dimensional representations. One-Hot Graph Encoder Embedding (GEE) uses a single, linear pass over edges and produces an embedding that converges asymptotically to the spectral embedding. The scaling and performance benefits of this approach have been limited by a serial implementation in an interpreted language. We refactor GEE into a parallel program in the Ligra graph engine that maps functions over the edges of the graph and uses lock-free atomic instrutions to prevent data races. On a graph with 1.8B edges, this results in a 500 times speedup over the original implementation and a 17 times speedup over a just-in-time compiled version.","sentences":["New algorithms for embedding graphs have reduced the asymptotic complexity of finding low-dimensional representations.","One-Hot Graph Encoder Embedding (GEE) uses a single, linear pass over edges and produces an embedding that converges asymptotically to the spectral embedding.","The scaling and performance benefits of this approach have been limited by a serial implementation in an interpreted language.","We refactor GEE into a parallel program in the Ligra graph engine that maps functions over the edges of the graph and uses lock-free atomic instrutions to prevent data races.","On a graph with 1.8B edges, this results in a 500 times speedup over the original implementation and a 17 times speedup over a just-in-time compiled version."],"url":"http://arxiv.org/abs/2402.04403v1","category":"cs.DC"}
{"created":"2024-02-06 20:35:28","title":"Fine-Tuned Language Models Generate Stable Inorganic Materials as Text","abstract":"We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.","sentences":["We propose fine-tuning large language models for generation of stable materials.","While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges.","Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model.","Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation.","Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data."],"url":"http://arxiv.org/abs/2402.04379v1","category":"cs.LG"}
{"created":"2024-02-06 20:31:15","title":"$\\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems","abstract":"Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experiments on different datasets and various machine learning models, including LeNet5, RepVGG, and Vision Transformer (ViT), demonstrate that NeRCC accurately approximates the original predictions in a wide range of stragglers, outperforming the state-of-the-art by up to 23%.","sentences":["Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model.","In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing.","NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points.","We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers.","We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized.","Our extensive experiments on different datasets and various machine learning models, including LeNet5, RepVGG, and Vision Transformer (ViT), demonstrate that NeRCC accurately approximates the original predictions in a wide range of stragglers, outperforming the state-of-the-art by up to 23%."],"url":"http://arxiv.org/abs/2402.04377v1","category":"cs.LG"}
{"created":"2024-02-07 18:56:06","title":"Millimeter-scale freestanding superconducting infinite-layer nickelate membranes","abstract":"Progress in the study of infinite-layer nickelates has always been highly linked to materials advances. In particular, the recent development of superconductivity via hole-doping was predicated on the controlled synthesis of Ni in a very high oxidation state, and subsequent topotactic reduction to a very low oxidation state, currently limited to epitaxial thin films. Here we demonstrate a process to combine these steps with a heterostructure which includes an epitaxial soluble buffer layer, enabling the release of freestanding membranes of (Nd,Sr)NiO2 encapsulated in SrTiO3, which serves as a protective layer. The membranes have comparable structural and electronic properties to that of optimized thin films, and range in lateral dimensions from millimeters to ~100 micron fragments, depending on the degree of strain released with respect to the initial substrate. The changes in the superconducting transition temperature associated with membrane release are quite similar to those reported for substrate and pressure variations, suggestive of a common underlying mechanism. These membranes structures should provide a versatile platform for a range of experimental studies and devices free from substrate constraints.","sentences":["Progress in the study of infinite-layer nickelates has always been highly linked to materials advances.","In particular, the recent development of superconductivity via hole-doping was predicated on the controlled synthesis of Ni in a very high oxidation state, and subsequent topotactic reduction to a very low oxidation state, currently limited to epitaxial thin films.","Here we demonstrate a process to combine these steps with a heterostructure which includes an epitaxial soluble buffer layer, enabling the release of freestanding membranes of (Nd,Sr)NiO2 encapsulated in SrTiO3, which serves as a protective layer.","The membranes have comparable structural and electronic properties to that of optimized thin films, and range in lateral dimensions from millimeters to ~100 micron fragments, depending on the degree of strain released with respect to the initial substrate.","The changes in the superconducting transition temperature associated with membrane release are quite similar to those reported for substrate and pressure variations, suggestive of a common underlying mechanism.","These membranes structures should provide a versatile platform for a range of experimental studies and devices free from substrate constraints."],"url":"http://arxiv.org/abs/2402.05104v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 18:55:15","title":"Large deviations for dynamical Schr\u00f6dinger problems","abstract":"We establish large deviations for dynamical Schr\\\"{o}dinger problems driven by perturbed Brownian motions when the noise parameter tends to zero. Our results show that Schr\\\"{o}dinger bridges charge exponentially small masses outside of the support of the limiting law that agrees with the optimal solution to the dynamical Monge-Kantorovich optimal transport problem. Our proofs build on mixture representations of Schr\\\"{o}dinger bridges and establishing exponential continuity of Brownian bridges with respect to the initial and terminal points.","sentences":["We establish large deviations for dynamical Schr\\\"{o}dinger problems driven by perturbed Brownian motions when the noise parameter tends to zero.","Our results show that Schr\\\"{o}dinger bridges charge exponentially small masses outside of the support of the limiting law that agrees with the optimal solution to the dynamical Monge-Kantorovich optimal transport problem.","Our proofs build on mixture representations of Schr\\\"{o}dinger bridges and establishing exponential continuity of Brownian bridges with respect to the initial and terminal points."],"url":"http://arxiv.org/abs/2402.05100v1","category":"math.PR"}
{"created":"2024-02-07 17:25:44","title":"Non-reversible lifts of reversible diffusion processes and relaxation times","abstract":"We propose a new concept of lifts of reversible diffusion processes and show that various well-known non-reversible Markov processes arising in applications are lifts in this sense of simple reversible diffusions. Furthermore, we introduce a concept of non-asymptotic relaxation times and show that these can at most be reduced by a square root through lifting, generalising a related result in discrete time. Finally, we demonstrate how the recently developed approach to quantitative hypocoercivity based on space-time Poincar\\'e inequalities can be rephrased and simplified in the language of lifts and how it can be applied to find optimal lifts.","sentences":["We propose a new concept of lifts of reversible diffusion processes and show that various well-known non-reversible Markov processes arising in applications are lifts in this sense of simple reversible diffusions.","Furthermore, we introduce a concept of non-asymptotic relaxation times and show that these can at most be reduced by a square root through lifting, generalising a related result in discrete time.","Finally, we demonstrate how the recently developed approach to quantitative hypocoercivity based on space-time Poincar\\'e inequalities can be rephrased and simplified in the language of lifts and how it can be applied to find optimal lifts."],"url":"http://arxiv.org/abs/2402.05041v1","category":"math.PR"}
{"created":"2024-02-07 16:52:58","title":"Characterization and Optimization of Cryogenic Pure CsI Detector for CLOVERS Experiment","abstract":"In this study, we conducted a comprehensive characterization and optimization of a cryogenic pure CsI (pCsI) detector. Achieving a notable light yield of \\SI{35.2}{PE/\\keV_{ee}} and a world-leading energy resolution of \\SI{6.9}{\\%} at \\SI{60}{\\keV}, we utilized a \\SI{2}{\\centi\\metre} cubic crystal coupled with a HAMAMATSU R11065 photomultiplier tube (PMT). Additionally, we measured the scintillation decay time of pCsI, which proved to be significantly faster than that of CsI(Na) at room temperature. Furthermore, we investigated the impact of temperature, surface treatment, and crystal shape on the light yield. Notably, the light yield peaked at approximately \\SI{20}{\\K} and remained stable within the range of \\SI{70}-\\SI{100}{\\K}. We observed that the light yield of polished crystals was approximately 1.5 times greater than that of ground crystals, while the crystal shape exhibited minimal influence on the light yield. These results are crucial for the design of the \\SI{10}{\\kg} pCsI detector for the future CLOVERS (Coherent eLastic neutrinO(V)-nucleus scattERing at China Spallation Neutron Source (CSNS)) experiment.","sentences":["In this study, we conducted a comprehensive characterization and optimization of a cryogenic pure CsI (pCsI) detector.","Achieving a notable light yield of \\SI{35.2}{PE/\\keV_{ee}} and a world-leading energy resolution of \\SI{6.9}{\\%} at \\SI{60}{\\keV}, we utilized a \\SI{2}{\\centi\\metre} cubic crystal coupled with a HAMAMATSU R11065 photomultiplier tube (PMT).","Additionally, we measured the scintillation decay time of pCsI, which proved to be significantly faster than that of CsI(Na) at room temperature.","Furthermore, we investigated the impact of temperature, surface treatment, and crystal shape on the light yield.","Notably, the light yield peaked at approximately \\SI{20}{\\K} and remained stable within the range of \\SI{70}-\\SI{100}{\\K}.","We observed that the light yield of polished crystals was approximately 1.5 times greater than that of ground crystals, while the crystal shape exhibited minimal influence on the light yield.","These results are crucial for the design of the \\SI{10}{\\kg} pCsI detector for the future CLOVERS (Coherent eLastic neutrinO(V)-nucleus scattERing at China Spallation Neutron Source (CSNS))","experiment."],"url":"http://arxiv.org/abs/2402.05026v1","category":"physics.ins-det"}
{"created":"2024-02-07 15:56:03","title":"Tuning HMC parameters with gradients","abstract":"We investigate the effectiveness of tuning HMC parameters using information from the gradients of the HMC acceptance probability with respect to the parameters. In particular, the optimization of the trajectory length and parameters for higher order integrators will be studied in the context of pure gauge and dynamical fermion actions.","sentences":["We investigate the effectiveness of tuning HMC parameters using information from the gradients of the HMC acceptance probability with respect to the parameters.","In particular, the optimization of the trajectory length and parameters for higher order integrators will be studied in the context of pure gauge and dynamical fermion actions."],"url":"http://arxiv.org/abs/2402.04976v1","category":"hep-lat"}
{"created":"2024-02-07 13:25:16","title":"NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering","abstract":"Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: https://nerfemitterpbir.github.io/.","sentences":["Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images.","Here lighting is an important part of achieving faithful light transport simulation.","While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering.","We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter.","By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering.","Project page and video: https://nerfemitterpbir.github.io/."],"url":"http://arxiv.org/abs/2402.04829v1","category":"cs.CV"}
{"created":"2024-02-07 13:20:23","title":"Kinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations","abstract":"Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot. The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers. We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas. We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK). Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs. We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects. We additionally compare our method against existing hand retargeting approaches. Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories.","sentences":["Hand motion capture data is now relatively easy to obtain, even for complicated grasps; however this data is of limited use without the ability to retarget it onto the hands of a specific character or robot.","The target hand may differ dramatically in geometry, number of degrees of freedom (DOFs), or number of fingers.","We present a simple, but effective framework capable of kinematically retargeting multiple human hand-object manipulations from a publicly available dataset to a wide assortment of kinematically and morphologically diverse target hands through the exploitation of contact areas.","We do so by formulating the retarget operation as a non-isometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics (IK).","Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations while providing an intuitive means for artists to specify correspondences with relatively few inputs.","We validate our framework through thirty demonstrations across five different hand shapes and six motions of different objects.","We additionally compare our method against existing hand retargeting approaches.","Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of design choices over full trajectories."],"url":"http://arxiv.org/abs/2402.04820v1","category":"cs.GR"}
{"created":"2024-02-07 11:21:43","title":"Applying Quantum Computing to Solve Multicommodity Network Flow Problem","abstract":"In this paper, the multicommodity network flow (MCNF) problem is formulated as a mixed integer programing model which is known as NP-hard, aiming to optimize the vehicle routing and minimize the total travel cost. We explore the potential of quantum computing, specifically quantum annealing, by comparing its performance in terms of solution quality and efficiency against the traditional method. Our findings indicate that quantum annealing holds significant promise for enhancing computation in large-scale transportation logistics problems.","sentences":["In this paper, the multicommodity network flow (MCNF) problem is formulated as a mixed integer programing model which is known as NP-hard, aiming to optimize the vehicle routing and minimize the total travel cost.","We explore the potential of quantum computing, specifically quantum annealing, by comparing its performance in terms of solution quality and efficiency against the traditional method.","Our findings indicate that quantum annealing holds significant promise for enhancing computation in large-scale transportation logistics problems."],"url":"http://arxiv.org/abs/2402.04758v1","category":"math.OC"}
{"created":"2024-02-07 10:29:58","title":"Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for Transmission with Higher-Order Constellations in the Terahertz Band","abstract":"In this work, we consider Terahertz (THz) communications with low-resolution uniform quantization and spatial oversampling at the receiver side. We compare different analog-to-digital converter (ADC) parametrizations in a fair manner by keeping the ADC power consumption constant. Here, 1-, 2-, and 3-bit quantization is investigated with different oversampling factors. We analytically compute the statistics of the detection variable, and we propose the optimal as well as several suboptimal detection schemes for arbitrary quantization resolutions. Then, we evaluate the symbol error rate (SER) of the different detectors for a 16- and a 64-ary quadrature amplitude modulation (QAM) constellation. The results indicate that there is a noticeable performance degradation of the suboptimal detection schemes compared to the optimal scheme when the constellation size is larger than the number of quantization levels. Furthermore, at low signal-to-noise ratios (SNRs), 1-bit quantization outperforms 2- and 3-bit quantization, respectively, even when employing higher-order constellations. We confirm our analytical results by Monte Carlo simulations. Both a pure line-of-sight (LoS) and a more realistically modeled indoor THz channel are considered. Then, we optimize the input signal constellation with respect to SER for 1-bit quantization. The results show that the minimum SER can be lowered significantly for 16-QAM by increasing the distance between the inner and outer points of the input constellation. For larger constellations, however, the achievable reduction of the minimum SER is much smaller compared to 16-QAM.","sentences":["In this work, we consider Terahertz (THz) communications with low-resolution uniform quantization and spatial oversampling at the receiver side.","We compare different analog-to-digital converter (ADC) parametrizations in a fair manner by keeping the ADC power consumption constant.","Here, 1-, 2-, and 3-bit quantization is investigated with different oversampling factors.","We analytically compute the statistics of the detection variable, and we propose the optimal as well as several suboptimal detection schemes for arbitrary quantization resolutions.","Then, we evaluate the symbol error rate (SER) of the different detectors for a 16- and a 64-ary quadrature amplitude modulation (QAM) constellation.","The results indicate that there is a noticeable performance degradation of the suboptimal detection schemes compared to the optimal scheme when the constellation size is larger than the number of quantization levels.","Furthermore, at low signal-to-noise ratios (SNRs), 1-bit quantization outperforms 2- and 3-bit quantization, respectively, even when employing higher-order constellations.","We confirm our analytical results by Monte Carlo simulations.","Both a pure line-of-sight (LoS) and a more realistically modeled indoor THz channel are considered.","Then, we optimize the input signal constellation with respect to SER for 1-bit quantization.","The results show that the minimum SER can be lowered significantly for 16-QAM by increasing the distance between the inner and outer points of the input constellation.","For larger constellations, however, the achievable reduction of the minimum SER is much smaller compared to 16-QAM."],"url":"http://arxiv.org/abs/2402.04728v1","category":"cs.IT"}
{"created":"2024-02-07 04:07:33","title":"Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution","abstract":"Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses. Thus, optimal prompt construction is essential for maximizing the utility and performance of ChatGPT. However, sub-optimal prompt design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from ChatGPT. Existing studies explore several prompt patterns and strategies to improve the relevance of responses generated by ChatGPT. However, the exploration of constraints that necessitate the submission of multiple prompts is still an unmet attempt. In this study, our contributions are twofold. First, we attempt to uncover gaps in prompt design that demand multiple iterations. In particular, we manually analyze 686 prompts that were submitted to resolve issues related to Java and Python programming languages and identify eleven prompt design gaps (e.g., missing specifications). Such gap exploration can enhance the efficacy of single prompts in ChatGPT. Second, we attempt to reproduce the ChatGPT response by consolidating multiple prompts into a single one. We can completely consolidate prompts with four gaps (e.g., missing context) and partially consolidate prompts with three gaps (e.g., additional functionality). Such an effort provides concrete evidence to users to design more optimal prompts mitigating these gaps. Our study findings and evidence can - (a) save users time, (b) reduce costs, and (c) increase user satisfaction.","sentences":["Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses.","Thus, optimal prompt construction is essential for maximizing the utility and performance of ChatGPT.","However, sub-optimal prompt design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from ChatGPT.","Existing studies explore several prompt patterns and strategies to improve the relevance of responses generated by ChatGPT.","However, the exploration of constraints that necessitate the submission of multiple prompts is still an unmet attempt.","In this study, our contributions are twofold.","First, we attempt to uncover gaps in prompt design that demand multiple iterations.","In particular, we manually analyze 686 prompts that were submitted to resolve issues related to Java and Python programming languages and identify eleven prompt design gaps (e.g., missing specifications).","Such gap exploration can enhance the efficacy of single prompts in ChatGPT.","Second, we attempt to reproduce the ChatGPT response by consolidating multiple prompts into a single one.","We can completely consolidate prompts with four gaps (e.g., missing context) and partially consolidate prompts with three gaps (e.g., additional functionality).","Such an effort provides concrete evidence to users to design more optimal prompts mitigating these gaps.","Our study findings and evidence can - (a) save users time, (b) reduce costs, and (c) increase user satisfaction."],"url":"http://arxiv.org/abs/2402.04568v1","category":"cs.SE"}
{"created":"2024-02-07 03:42:09","title":"Spacecraft Rendezvous Guidance via Factorization-Free Sequential Convex Programming using a First-Order Method","abstract":"We implement a fully factorization-free algorithm for nonconvex, free-final-time trajectory optimization. This algorithm is based on sequential convex programming and utilizes an inverse-free, exact discretization procedure to ensure dynamic feasibility of the converged trajectory and PIPG, a fast, first-order conic optimization algorithm as the subproblem solver. Although PIPG requires the tuning of a hyperparameter to achieve fastest convergence, we show that PIPG can be tuned to a nominal trajectory optimization problem and it is robust to variations in initial condition. We demonstrate this with a monte carlo simulation of the free-final-time rendezvous problem, using Clohessy-Wiltshire dynamics, an impulsive thrust model, and various state and control constraints including a spherical keepout zone.","sentences":["We implement a fully factorization-free algorithm for nonconvex, free-final-time trajectory optimization.","This algorithm is based on sequential convex programming and utilizes an inverse-free, exact discretization procedure to ensure dynamic feasibility of the converged trajectory and PIPG, a fast, first-order conic optimization algorithm as the subproblem solver.","Although PIPG requires the tuning of a hyperparameter to achieve fastest convergence, we show that PIPG can be tuned to a nominal trajectory optimization problem and it is robust to variations in initial condition.","We demonstrate this with a monte carlo simulation of the free-final-time rendezvous problem, using Clohessy-Wiltshire dynamics, an impulsive thrust model, and various state and control constraints including a spherical keepout zone."],"url":"http://arxiv.org/abs/2402.04561v1","category":"math.OC"}
{"created":"2024-02-07 03:01:54","title":"Optimal standoff distance for a highly focused microjet penetrating a soft material","abstract":"A needle-free injector using a highly focused microjet has the potential to minimize the invasiveness of drug delivery. In this study, the jet penetration depth in a soft material-which is a critical parameter for practical needle-free injections-was investigated. We conducted jet penetration experiments by varying the inner diameter of the injection tube and the standoff distance between the meniscus surface and the soft material. Interestingly, the results showed that the penetration depths peaked at certain distances from the meniscus, and the positions shifted further away as the inner diameter was increased. By analyzing the velocity distribution of the microjet, the peak positions of the penetration depth and the maximum velocities were inconsistent due to the effects of the jet shape. To account for this, we introduce the concept of the 'jet pressure impulse', a physical quantity that unifies the velocity and jet shape. However, direct estimation of this parameter from experimental data is challenging due to limitations in spatiotemporal resolution. Therefore, we used numerical simulations to replicate the experimental conditions and calculate the jet pressure impulse. Remarkably, the results show that the jet pressure impulse has peak values, which is consistent with the penetration depth. In addition, there is a correlation between the magnitude of the jet pressure impulse and the penetration depth, highlighting its importance as a key parameter. This study underlines the importance of the jet pressure impulse in controlling the penetration depth of a focused microjet, providing valuable insights for the practical use of needle-free injection techniques.","sentences":["A needle-free injector using a highly focused microjet has the potential to minimize the invasiveness of drug delivery.","In this study, the jet penetration depth in a soft material-which is a critical parameter for practical needle-free injections-was investigated.","We conducted jet penetration experiments by varying the inner diameter of the injection tube and the standoff distance between the meniscus surface and the soft material.","Interestingly, the results showed that the penetration depths peaked at certain distances from the meniscus, and the positions shifted further away as the inner diameter was increased.","By analyzing the velocity distribution of the microjet, the peak positions of the penetration depth and the maximum velocities were inconsistent due to the effects of the jet shape.","To account for this, we introduce the concept of the 'jet pressure impulse', a physical quantity that unifies the velocity and jet shape.","However, direct estimation of this parameter from experimental data is challenging due to limitations in spatiotemporal resolution.","Therefore, we used numerical simulations to replicate the experimental conditions and calculate the jet pressure impulse.","Remarkably, the results show that the jet pressure impulse has peak values, which is consistent with the penetration depth.","In addition, there is a correlation between the magnitude of the jet pressure impulse and the penetration depth, highlighting its importance as a key parameter.","This study underlines the importance of the jet pressure impulse in controlling the penetration depth of a focused microjet, providing valuable insights for the practical use of needle-free injection techniques."],"url":"http://arxiv.org/abs/2402.04543v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 02:50:38","title":"MuNES: Multifloor Navigation Including Elevators and Stairs","abstract":"We propose a scheme called MuNES for single mapping and trajectory planning including elevators and stairs. Optimized multifloor trajectories are important for optimal interfloor movements of robots. However, given two or more options of moving between floors, it is difficult to select the best trajectory because there are no suitable indoor multifloor maps in the existing methods. To solve this problem, MuNES creates a single multifloor map including elevators and stairs by estimating altitude changes based on pressure data. In addition, the proposed method performs floor-based loop detection for faster and more accurate loop closure. The single multifloor map is then voxelized leaving only the parts needed for trajectory planning. An optimal and realistic multifloor trajectory is generated by exploring the voxels using an A* algorithm based on the proposed cost function, which affects realistic factors. We tested this algorithm using data acquired from around a campus and note that a single accurate multifloor map could be created. Furthermore, optimal and realistic multifloor trajectory could be found by selecting the means of motion between floors between elevators and stairs according to factors such as the starting point, ending point, and elevator waiting time. The code and data used in this work are available at https://github.com/donghwijung/MuNES.","sentences":["We propose a scheme called MuNES for single mapping and trajectory planning including elevators and stairs.","Optimized multifloor trajectories are important for optimal interfloor movements of robots.","However, given two or more options of moving between floors, it is difficult to select the best trajectory because there are no suitable indoor multifloor maps in the existing methods.","To solve this problem, MuNES creates a single multifloor map including elevators and stairs by estimating altitude changes based on pressure data.","In addition, the proposed method performs floor-based loop detection for faster and more accurate loop closure.","The single multifloor map is then voxelized leaving only the parts needed for trajectory planning.","An optimal and realistic multifloor trajectory is generated by exploring the voxels using an A* algorithm based on the proposed cost function, which affects realistic factors.","We tested this algorithm using data acquired from around a campus and note that a single accurate multifloor map could be created.","Furthermore, optimal and realistic multifloor trajectory could be found by selecting the means of motion between floors between elevators and stairs according to factors such as the starting point, ending point, and elevator waiting time.","The code and data used in this work are available at https://github.com/donghwijung/MuNES."],"url":"http://arxiv.org/abs/2402.04535v1","category":"cs.RO"}
{"created":"2024-02-07 02:48:03","title":"Minimizing Block Incentive Volatility Through Verkle Tree-Based Dynamic Transaction Storage","abstract":"Transaction fees are a crucial revenue source for miners in public and consortium blockchains. However, while public blockchains have additional revenue streams, transaction fees serve as the primary income for miners in consortium blockchains formed by various financial institutions. These miners allocate different levels of computing resources to process transactions and earn corresponding fees. Nonetheless, relying solely on transaction fees can lead to significant volatility and encourage non-standard mining behaviors, thereby posing threats to the blockchain's security and integrity. Despite previous attempts to mitigate the impact of transaction fees on illicit mining behaviors, a comprehensive solution to this vulnerability is yet to be established. To address this gap, we introduce a novel approach that leverages Dynamic Transaction Storage (DTS) strategies to effectively minimize block incentive volatility. Our solution implements a Verkle tree-based storage mechanism to reduce bandwidth consumption. Moreover, to configure the DTS strategies, we evaluate several optimization algorithms and formulate the challenge as a Vehicle Routing Problem. Our experiments conducted using historical transactions from Bitcoin and remittance data from the Industrial and Commercial Bank of China reveal that the strategy focusing on time-based transaction incorporation priority, while excluding a designated space for small-fee transactions, as discovered by the gradient-based optimizer algorithm, proves most effective in reducing volatility. Hence, the DTS strategy can sustain stable block incentives irrespective of transaction types or user bidding behavior. Furthermore, the inclusion of higher-fee transactions, often smaller in size, can alleviate propagation delays and the occurrence of forks.","sentences":["Transaction fees are a crucial revenue source for miners in public and consortium blockchains.","However, while public blockchains have additional revenue streams, transaction fees serve as the primary income for miners in consortium blockchains formed by various financial institutions.","These miners allocate different levels of computing resources to process transactions and earn corresponding fees.","Nonetheless, relying solely on transaction fees can lead to significant volatility and encourage non-standard mining behaviors, thereby posing threats to the blockchain's security and integrity.","Despite previous attempts to mitigate the impact of transaction fees on illicit mining behaviors, a comprehensive solution to this vulnerability is yet to be established.","To address this gap, we introduce a novel approach that leverages Dynamic Transaction Storage (DTS) strategies to effectively minimize block incentive volatility.","Our solution implements a Verkle tree-based storage mechanism to reduce bandwidth consumption.","Moreover, to configure the DTS strategies, we evaluate several optimization algorithms and formulate the challenge as a Vehicle Routing Problem.","Our experiments conducted using historical transactions from Bitcoin and remittance data from the Industrial and Commercial Bank of China reveal that the strategy focusing on time-based transaction incorporation priority, while excluding a designated space for small-fee transactions, as discovered by the gradient-based optimizer algorithm, proves most effective in reducing volatility.","Hence, the DTS strategy can sustain stable block incentives irrespective of transaction types or user bidding behavior.","Furthermore, the inclusion of higher-fee transactions, often smaller in size, can alleviate propagation delays and the occurrence of forks."],"url":"http://arxiv.org/abs/2402.04533v1","category":"cs.CE"}
{"created":"2024-02-07 01:57:56","title":"BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision","abstract":"Single object tracking (SOT) is a fundamental problem in computer vision, with a wide range of applications, including autonomous driving, augmented reality, and robot navigation. The robustness of SOT faces two main challenges: tiny target and fast motion. These challenges are especially manifested in videos captured by unmanned aerial vehicles (UAV), where the target is usually far away from the camera and often with significant motion relative to the camera. To evaluate the robustness of SOT methods, we propose BioDrone -- the first bionic drone-based visual benchmark for SOT. Unlike existing UAV datasets, BioDrone features videos captured from a flapping-wing UAV system with a major camera shake due to its aerodynamics. BioDrone hence highlights the tracking of tiny targets with drastic changes between consecutive frames, providing a new robust vision benchmark for SOT. To date, BioDrone offers the largest UAV-based SOT benchmark with high-quality fine-grained manual annotations and automatically generates frame-level labels, designed for robust vision analyses. Leveraging our proposed BioDrone, we conduct a systematic evaluation of existing SOT methods, comparing the performance of 20 representative models and studying novel means of optimizing a SOTA method (KeepTrack KeepTrack) for robust SOT. Our evaluation leads to new baselines and insights for robust SOT. Moving forward, we hope that BioDrone will not only serve as a high-quality benchmark for robust SOT, but also invite future research into robust computer vision. The database, toolkits, evaluation server, and baseline results are available at http://biodrone.aitestunion.com.","sentences":["Single object tracking (SOT) is a fundamental problem in computer vision, with a wide range of applications, including autonomous driving, augmented reality, and robot navigation.","The robustness of SOT faces two main challenges: tiny target and fast motion.","These challenges are especially manifested in videos captured by unmanned aerial vehicles (UAV), where the target is usually far away from the camera and often with significant motion relative to the camera.","To evaluate the robustness of SOT methods, we propose BioDrone -- the first bionic drone-based visual benchmark for SOT.","Unlike existing UAV datasets, BioDrone features videos captured from a flapping-wing UAV system with a major camera shake due to its aerodynamics.","BioDrone hence highlights the tracking of tiny targets with drastic changes between consecutive frames, providing a new robust vision benchmark for SOT.","To date, BioDrone offers the largest UAV-based SOT benchmark with high-quality fine-grained manual annotations and automatically generates frame-level labels, designed for robust vision analyses.","Leveraging our proposed BioDrone, we conduct a systematic evaluation of existing SOT methods, comparing the performance of 20 representative models and studying novel means of optimizing a SOTA method (KeepTrack KeepTrack) for robust SOT.","Our evaluation leads to new baselines and insights for robust SOT.","Moving forward, we hope that BioDrone will not only serve as a high-quality benchmark for robust SOT, but also invite future research into robust computer vision.","The database, toolkits, evaluation server, and baseline results are available at http://biodrone.aitestunion.com."],"url":"http://arxiv.org/abs/2402.04519v1","category":"cs.CV"}
{"created":"2024-02-07 01:48:14","title":"Graph-based methods for hyperbolic systems of conservation laws using discontinuous space discretizations, Part I: building blocks","abstract":"We present a graph-based discretization method for solving hyperbolic systems of conservation laws using discontinuous finite elements. The method is based on the convex limiting technique technique introduced by Guermond et al. (SIAM J. Sci. Comput. 40, A3211--A3239, 2018). As such, these methods are mathematically guaranteed to be invariant-set preserving and to satisfy discrete pointwise entropy inequalities. In this paper we extend the theory for the specific case of discontinuous finite elements, incorporating the effect of boundary conditions into the formulation. From a practical point of view, the implementation of these methods is algebraic, meaning, that they operate directly on the stencil of the spatial discretization.   This first paper in a sequence of two papers introduces and verifies essential building blocks for the convex limiting procedure using discontinuous Galerkin discretizations. In particular, we discuss a minimally stabilized high-order discontinuous Galerkin method that exhibits optimal convergence rates comparable to linear stabilization techniques for cell-based methods. In addition, we discuss a proper choice of local bounds for the convex limiting procedure. A follow-up contribution will focus on the high-performance implementation, benchmarking and verification of the method.   We verify convergence rates on a sequence of one- and two-dimensional tests with differing regularity. In particular, we obtain optimal convergence rates for single rarefaction waves. We also propose a simple test in order to verify the implementation of boundary conditions and their convergence rates.","sentences":["We present a graph-based discretization method for solving hyperbolic systems of conservation laws using discontinuous finite elements.","The method is based on the convex limiting technique technique introduced by Guermond et al.","(SIAM J. Sci.","Comput.","40, A3211--A3239, 2018).","As such, these methods are mathematically guaranteed to be invariant-set preserving and to satisfy discrete pointwise entropy inequalities.","In this paper we extend the theory for the specific case of discontinuous finite elements, incorporating the effect of boundary conditions into the formulation.","From a practical point of view, the implementation of these methods is algebraic, meaning, that they operate directly on the stencil of the spatial discretization.   ","This first paper in a sequence of two papers introduces and verifies essential building blocks for the convex limiting procedure using discontinuous Galerkin discretizations.","In particular, we discuss a minimally stabilized high-order discontinuous Galerkin method that exhibits optimal convergence rates comparable to linear stabilization techniques for cell-based methods.","In addition, we discuss a proper choice of local bounds for the convex limiting procedure.","A follow-up contribution will focus on the high-performance implementation, benchmarking and verification of the method.   ","We verify convergence rates on a sequence of one- and two-dimensional tests with differing regularity.","In particular, we obtain optimal convergence rates for single rarefaction waves.","We also propose a simple test in order to verify the implementation of boundary conditions and their convergence rates."],"url":"http://arxiv.org/abs/2402.04514v1","category":"math.NA"}
{"created":"2024-02-07 00:31:24","title":"Inverse Designed WS2 Planar Chiral Metasurface with Geometric Phase","abstract":"Increasing attention is being paid to chiral metasurfaces due to their ability to selectively manipulate right-hand circularly polarized light or left-hand circularly polarized light. The thin nature of metasurfaces, however, poses a challenge in creating a device with effective phase modulation. Plasmonic chiral metasurfaces have attempted to address this issue by increasing light-matter interaction, but they suffer from metallic loss. Dielectric metasurfaces made from high index materials enable phase modulation while being thin. Very few materials, however, have high refractive index and low loss at visible wavelengths. Recently, some 2D materials have been shown to exhibit high refractive index and low loss in the visible wavelengths, positioning them as promising platform for meta-optics. This study introduces and details a planar chiral metasurface with geometric phase composed of WS2 meta-units. By employing adjoint optimization techniques, we achieved broadband circular dichroism.","sentences":["Increasing attention is being paid to chiral metasurfaces due to their ability to selectively manipulate right-hand circularly polarized light or left-hand circularly polarized light.","The thin nature of metasurfaces, however, poses a challenge in creating a device with effective phase modulation.","Plasmonic chiral metasurfaces have attempted to address this issue by increasing light-matter interaction, but they suffer from metallic loss.","Dielectric metasurfaces made from high index materials enable phase modulation while being thin.","Very few materials, however, have high refractive index and low loss at visible wavelengths.","Recently, some 2D materials have been shown to exhibit high refractive index and low loss in the visible wavelengths, positioning them as promising platform for meta-optics.","This study introduces and details a planar chiral metasurface with geometric phase composed of WS2 meta-units.","By employing adjoint optimization techniques, we achieved broadband circular dichroism."],"url":"http://arxiv.org/abs/2402.04490v1","category":"physics.optics"}
{"created":"2024-02-07 00:10:39","title":"MIRT: a simultaneous reconstruction and affine motion compensation technique for four dimensional computed tomography (4DCT)","abstract":"In four-dimensional computed tomography (4DCT), 3D images of moving or deforming samples are reconstructed from a set of 2D projection images. Recent techniques for iterative motion-compensated reconstruction either necessitate a reference acquisition or alternate image reconstruction and motion estimation steps. In these methods, the motion estimation step involves the estimation of either complete deformation vector fields (DVFs) or a limited set of parameters corresponding to the affine motion, including rigid motion or scaling. The majority of these approaches rely on nested iterations, incurring significant computational expenses. Notably, despite the direct benefits of an analytical formulation and a substantial reduction in computational complexity, there has been no exploration into parameterizing DVFs for general affine motion in CT imaging. In this work, we propose the Motion-compensated Iterative Reconstruction Technique (MIRT)- an efficient iterative reconstruction scheme that combines image reconstruction and affine motion estimation in a single update step, based on the analytical gradients of the motion towards both the reconstruction and the affine motion parameters. When most of the state-of-the-art 4DCT methods have not attempted to be tested on real data, results from simulation and real experiments show that our method outperforms the state-of-the-art CT reconstruction with affine motion correction methods in computational feasibility and projection distance. In particular, this allows accurate reconstruction for a proper microscale diamond in the appearance of motion from the practically acquired projection radiographs, which leads to a novel application of 4DCT.","sentences":["In four-dimensional computed tomography (4DCT), 3D images of moving or deforming samples are reconstructed from a set of 2D projection images.","Recent techniques for iterative motion-compensated reconstruction either necessitate a reference acquisition or alternate image reconstruction and motion estimation steps.","In these methods, the motion estimation step involves the estimation of either complete deformation vector fields (DVFs) or a limited set of parameters corresponding to the affine motion, including rigid motion or scaling.","The majority of these approaches rely on nested iterations, incurring significant computational expenses.","Notably, despite the direct benefits of an analytical formulation and a substantial reduction in computational complexity, there has been no exploration into parameterizing DVFs for general affine motion in CT imaging.","In this work, we propose the Motion-compensated Iterative Reconstruction Technique (MIRT)- an efficient iterative reconstruction scheme that combines image reconstruction and affine motion estimation in a single update step, based on the analytical gradients of the motion towards both the reconstruction and the affine motion parameters.","When most of the state-of-the-art 4DCT methods have not attempted to be tested on real data, results from simulation and real experiments show that our method outperforms the state-of-the-art CT reconstruction with affine motion correction methods in computational feasibility and projection distance.","In particular, this allows accurate reconstruction for a proper microscale diamond in the appearance of motion from the practically acquired projection radiographs, which leads to a novel application of 4DCT."],"url":"http://arxiv.org/abs/2402.04480v1","category":"eess.IV"}
{"created":"2024-02-06 23:38:36","title":"Reductive Quantum Phase Estimation","abstract":"Estimating a quantum phase is a necessary task in a wide range of fields of quantum science. To accomplish this task, two well-known methods have been developed in distinct contexts, namely, Ramsey interferometry (RI) in atomic and molecular physics and quantum phase estimation (QPE) in quantum computing. We demonstrate that these canonical examples are instances of a larger class of phase estimation protocols, which we call reductive quantum phase estimation (RQPE) circuits. Here we present an explicit algorithm that allows one to create an RQPE circuit. This circuit distinguishes an arbitrary set of phases with a fewer number of qubits and unitary applications, thereby solving a general class of quantum hypothesis testing to which RI and QPE belong. We further demonstrate a trade-off between measurement precision and phase distinguishability, which allows one to tune the circuit to be optimal for a specific application.","sentences":["Estimating a quantum phase is a necessary task in a wide range of fields of quantum science.","To accomplish this task, two well-known methods have been developed in distinct contexts, namely, Ramsey interferometry (RI) in atomic and molecular physics and quantum phase estimation (QPE) in quantum computing.","We demonstrate that these canonical examples are instances of a larger class of phase estimation protocols, which we call reductive quantum phase estimation (RQPE) circuits.","Here we present an explicit algorithm that allows one to create an RQPE circuit.","This circuit distinguishes an arbitrary set of phases with a fewer number of qubits and unitary applications, thereby solving a general class of quantum hypothesis testing to which RI and QPE belong.","We further demonstrate a trade-off between measurement precision and phase distinguishability, which allows one to tune the circuit to be optimal for a specific application."],"url":"http://arxiv.org/abs/2402.04471v1","category":"quant-ph"}
{"created":"2024-02-06 19:35:58","title":"Fair Interval Scheduling of Indivisible Chores","abstract":"We study the problem of fairly assigning a set of discrete tasks (or chores) among a set of agents with additive valuations. Each chore is associated with a start and finish time, and each agent can perform at most one chore at any given time. The goal is to find a fair and efficient schedule of the chores, where fairness pertains to satisfying envy-freeness up to one chore (EF1) and efficiency pertains to maximality (i.e., no unallocated chore can be feasibly assigned to any agent). Our main result is a polynomial-time algorithm for computing an EF1 and maximal schedule for two agents under monotone valuations when the conflict constraints constitute an arbitrary interval graph. The algorithm uses a coloring technique in interval graphs that may be of independent interest. For an arbitrary number of agents, we provide an algorithm for finding a fair schedule under identical dichotomous valuations when the constraints constitute a path graph. We also show that stronger fairness and efficiency properties, including envy-freeness up to any chore (EFX) along with maximality and EF1 along with Pareto optimality, cannot be achieved.","sentences":["We study the problem of fairly assigning a set of discrete tasks (or chores) among a set of agents with additive valuations.","Each chore is associated with a start and finish time, and each agent can perform at most one chore at any given time.","The goal is to find a fair and efficient schedule of the chores, where fairness pertains to satisfying envy-freeness up to one chore (EF1) and efficiency pertains to maximality (i.e., no unallocated chore can be feasibly assigned to any agent).","Our main result is a polynomial-time algorithm for computing an EF1 and maximal schedule for two agents under monotone valuations when the conflict constraints constitute an arbitrary interval graph.","The algorithm uses a coloring technique in interval graphs that may be of independent interest.","For an arbitrary number of agents, we provide an algorithm for finding a fair schedule under identical dichotomous valuations when the constraints constitute a path graph.","We also show that stronger fairness and efficiency properties, including envy-freeness up to any chore (EFX) along with maximality and EF1 along with Pareto optimality, cannot be achieved."],"url":"http://arxiv.org/abs/2402.04353v1","category":"cs.GT"}
{"created":"2024-02-06 19:29:25","title":"Sensitivity-optimized strongly coupled multicore fiber-based thermometer","abstract":"In this paper, we report on a multicore fiber-based (MCF) temperature sensor that operates in a wide thermal range and that is robustly packaged to withstand harsh environments. To develop the sensor, the fundamentals concerning the effect of temperature on such fibers have been analyzed in detail to predict the most temperature sensitive MCF geometry. Thanks to it, the device, which operates in reflection mode and consists of a short segment of strongly coupled MCF fusion spliced to a standard single mode fiber, shows higher sensitivity than other devices with identical configuration. Regarding its packaging, it consists of an inner ceramic and two outer metallic tubes to provide rigidity and protection against impacts or dirt. The device was calibrated for a thermal range from 25 C to 900 C and a K-type thermocouple was used as reference. Our results suggest that the manufactured optical thermometer is as accurate as the electronic one, reaching a sensitivity up to 29.426 pm/C with the advantage of being passive, compact and easy to fabricate and interrogate. Therefore, we believe this device is appealing for industrial applications that require highly sensitive temperature sensing in very demanding environments, and that the analysis included in this work could be analogously applied to develop sensitivity-optimized devices for other parameters of interest.","sentences":["In this paper, we report on a multicore fiber-based (MCF) temperature sensor that operates in a wide thermal range and that is robustly packaged to withstand harsh environments.","To develop the sensor, the fundamentals concerning the effect of temperature on such fibers have been analyzed in detail to predict the most temperature sensitive MCF geometry.","Thanks to it, the device, which operates in reflection mode and consists of a short segment of strongly coupled MCF fusion spliced to a standard single mode fiber, shows higher sensitivity than other devices with identical configuration.","Regarding its packaging, it consists of an inner ceramic and two outer metallic tubes to provide rigidity and protection against impacts or dirt.","The device was calibrated for a thermal range from 25 C to 900 C and a K-type thermocouple was used as reference.","Our results suggest that the manufactured optical thermometer is as accurate as the electronic one, reaching a sensitivity up to 29.426 pm/C with the advantage of being passive, compact and easy to fabricate and interrogate.","Therefore, we believe this device is appealing for industrial applications that require highly sensitive temperature sensing in very demanding environments, and that the analysis included in this work could be analogously applied to develop sensitivity-optimized devices for other parameters of interest."],"url":"http://arxiv.org/abs/2402.04346v1","category":"physics.optics"}
{"created":"2024-02-06 19:27:48","title":"Does Confidence Calibration Help Conformal Prediction?","abstract":"Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with high probability. Previous works usually employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction. In this work, we first show that post-hoc calibration methods surprisingly lead to larger prediction sets with improved calibration, while over-confidence with small temperatures benefits the conformal prediction performance instead. Theoretically, we prove that high confidence reduces the probability of appending a new class in the prediction set. Inspired by the analysis, we propose a novel method, $\\textbf{Conformal Temperature Scaling}$ (ConfTS), which rectifies the objective through the gap between the threshold and the non-conformity score of the ground-truth label. In this way, the new objective of ConfTS will optimize the temperature value toward an optimal set that satisfies the $\\textit{marginal coverage}$. Experiments demonstrate that our method can effectively improve widely-used conformal prediction methods.","sentences":["Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with high probability.","Previous works usually employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction.","In this work, we first show that post-hoc calibration methods surprisingly lead to larger prediction sets with improved calibration, while over-confidence with small temperatures benefits the conformal prediction performance instead.","Theoretically, we prove that high confidence reduces the probability of appending a new class in the prediction set.","Inspired by the analysis, we propose a novel method, $\\textbf{Conformal Temperature Scaling}$ (ConfTS), which rectifies the objective through the gap between the threshold and the non-conformity score of the ground-truth label.","In this way, the new objective of ConfTS will optimize the temperature value toward an optimal set that satisfies the $\\textit{marginal coverage}$. Experiments demonstrate that our method can effectively improve widely-used conformal prediction methods."],"url":"http://arxiv.org/abs/2402.04344v1","category":"cs.LG"}
{"created":"2024-02-06 19:13:52","title":"Production-Inventory games: a new class of totally balanced combinatorial optimization games","abstract":"In this paper we introduce a new class of cooperative games that arise from production-inventory problems. Several agents have to cover their demand over a finite time horizon and shortages are allowed. Each agent has its own unit production, inventory-holding and backlogging cost. Cooperation among agents is given by sharing production processes and warehouse facilities: agents in a coalition produce with \\ the cheapest production cost and store with the cheapest inventory cost. We prove that the resulting cooperative game is totally balanced and the Owen set reduces to a singleton: the Owen point. Based on this type of allocation we find a population monotonic allocation scheme for this class of games. Finally, we point out the relationship of the Owen point with other well-known allocation rules such as the nucleolus and the Shapley value.","sentences":["In this paper we introduce a new class of cooperative games that arise from production-inventory problems.","Several agents have to cover their demand over a finite time horizon and shortages are allowed.","Each agent has its own unit production, inventory-holding and backlogging cost.","Cooperation among agents is given by sharing production processes and warehouse facilities: agents in a coalition produce with \\ the cheapest production cost and store with the cheapest inventory cost.","We prove that the resulting cooperative game is totally balanced and the Owen set reduces to a singleton: the Owen point.","Based on this type of allocation we find a population monotonic allocation scheme for this class of games.","Finally, we point out the relationship of the Owen point with other well-known allocation rules such as the nucleolus and the Shapley value."],"url":"http://arxiv.org/abs/2402.04328v1","category":"cs.GT"}
{"created":"2024-02-06 19:00:03","title":"Auto from cross: CMB lensing power spectrum without noise bias","abstract":"Upcoming surveys will measure the cosmic microwave background (CMB) weak lensing power spectrum in exquisite detail, allowing for strong constraints on the sum of neutrino masses among other cosmological parameters. Standard CMB lensing power spectrum estimators aim to extract the connected non-Gaussian trispectrum of CMB temperature maps. However, they are generically dominated by a large Gaussian noise bias which thus needs to be subtracted at high accuracy. This is currently done with realistic map simulations of the CMB and noise, whose finite accuracy currently limits our ability to recover the CMB lensing on small-scale. In this paper, we propose a novel estimator which instead avoids this large Gaussian bias. This estimator relies only on the data and avoids the need for bias subtraction with simulations. Thus our bias avoidance method is (1) insensitive to misestimates in simulated CMB and noise models and (2) avoids the large computational cost of standard simulation-based methods like \"realization-dependent $N^{(0)}$\" (${\\rm RDN}^{(0)}$). We show that our estimator is as robust as standard methods in the presence realistic inhomogeneous noise (e.g. from scan strategy) and masking. Moreover, our method can be combined with split-based methods, making it completely insensitive to mode coupling from inhomogeneous atmospheric and detector noise. We derive the corresponding expressions for our estimator when estimating lensing from CMB temperature and polarization. Although in this paper we specifically consider CMB weak lensing power spectrum estimation, we illuminate the relation between our new estimator, ${\\rm RDN}^{(0)}$ subtraction, and general optimal trispectrum estimation. Through this discussion we conclude that our estimator is applicable to analogous problems in other fields which rely on estimating connected trispectra/four-point functions like large-scale structure.","sentences":["Upcoming surveys will measure the cosmic microwave background (CMB) weak lensing power spectrum in exquisite detail, allowing for strong constraints on the sum of neutrino masses among other cosmological parameters.","Standard CMB lensing power spectrum estimators aim to extract the connected non-Gaussian trispectrum of CMB temperature maps.","However, they are generically dominated by a large Gaussian noise bias which thus needs to be subtracted at high accuracy.","This is currently done with realistic map simulations of the CMB and noise, whose finite accuracy currently limits our ability to recover the CMB lensing on small-scale.","In this paper, we propose a novel estimator which instead avoids this large Gaussian bias.","This estimator relies only on the data and avoids the need for bias subtraction with simulations.","Thus our bias avoidance method is (1) insensitive to misestimates in simulated CMB and noise models and (2) avoids the large computational cost of standard simulation-based methods like \"realization-dependent $N^{(0)}$\" (${\\rm RDN}^{(0)}$).","We show that our estimator is as robust as standard methods in the presence realistic inhomogeneous noise (e.g. from scan strategy) and masking.","Moreover, our method can be combined with split-based methods, making it completely insensitive to mode coupling from inhomogeneous atmospheric and detector noise.","We derive the corresponding expressions for our estimator when estimating lensing from CMB temperature and polarization.","Although in this paper we specifically consider CMB weak lensing power spectrum estimation, we illuminate the relation between our new estimator, ${\\rm RDN}^{(0)}$ subtraction, and general optimal trispectrum estimation.","Through this discussion we conclude that our estimator is applicable to analogous problems in other fields which rely on estimating connected trispectra/four-point functions like large-scale structure."],"url":"http://arxiv.org/abs/2402.04309v1","category":"astro-ph.CO"}
{"created":"2024-02-06 16:41:59","title":"Electron Transport Through a 1D Chain of Dopant-Based Quantum Dots","abstract":"Strongly interacting electron systems can provide insight into quantum many-body phenomena, such as Mott insulating behavior and spin liquidity, facilitating semiconductor optimization. The Fermi-Hubbard model is the prototypical model used to study such systems. Recent research, however, has shown that the extended Fermi-Hubbard model, which accounts for long-range interactions, is more accurate, especially for systems far from half-filling. In this study, we use the extended Fermi-Hubbard model to mathematically analyze charge transport through a lattice of quantum dots. One-dimensional chains with spinless electrons and source-drain bias are observed, focusing on the transition between the ground state and the first excited state. Level repulsion decreases the expected energy levels of anticrossings as the hopping onto the chain tends to the hopping within the chain. The distribution of charge density along the chain is characterized in terms of the hopping, nuclear, and Coulomb parameters and novel plasmonic behavior is analyzed. Minor perturbations in electron transport are identified, corresponding to the one-dimensional nature of the observed systems. This research will lead to a better understanding of electron behavior in silicon-doped semiconductors, like the formation of correlation-induced band gaps, and open the door to using the extended Fermi-Hubbard model as a more accurate alternative to study quantum many-body systems.","sentences":["Strongly interacting electron systems can provide insight into quantum many-body phenomena, such as Mott insulating behavior and spin liquidity, facilitating semiconductor optimization.","The Fermi-Hubbard model is the prototypical model used to study such systems.","Recent research, however, has shown that the extended Fermi-Hubbard model, which accounts for long-range interactions, is more accurate, especially for systems far from half-filling.","In this study, we use the extended Fermi-Hubbard model to mathematically analyze charge transport through a lattice of quantum dots.","One-dimensional chains with spinless electrons and source-drain bias are observed, focusing on the transition between the ground state and the first excited state.","Level repulsion decreases the expected energy levels of anticrossings as the hopping onto the chain tends to the hopping within the chain.","The distribution of charge density along the chain is characterized in terms of the hopping, nuclear, and Coulomb parameters and novel plasmonic behavior is analyzed.","Minor perturbations in electron transport are identified, corresponding to the one-dimensional nature of the observed systems.","This research will lead to a better understanding of electron behavior in silicon-doped semiconductors, like the formation of correlation-induced band gaps, and open the door to using the extended Fermi-Hubbard model as a more accurate alternative to study quantum many-body systems."],"url":"http://arxiv.org/abs/2402.04300v1","category":"cond-mat.mes-hall"}
